entity,n-revs
vllm/config.py,711
vllm/engine/arg_utils.py,618
vllm/v1/worker/gpu_model_runner.py,564
.buildkite/test-pipeline.yaml,515
vllm/envs.py,379
vllm/engine/llm_engine.py,334
vllm/entrypoints/openai/api_server.py,318
tests/models/registry.py,310
vllm/utils.py,280
vllm/model_executor/models/registry.py,277
vllm/entrypoints/llm.py,265
vllm/worker/model_runner.py,235
vllm/_custom_ops.py,232
vllm/entrypoints/openai/protocol.py,230
setup.py,221
docs/models/supported_models.md,217
vllm/model_executor/layers/fused_moe/layer.py,216
vllm/entrypoints/openai/serving_chat.py,212
CMakeLists.txt,200
tests/conftest.py,199
vllm/transformers_utils/config.py,190
vllm/model_executor/layers/quantization/fp8.py,190
README.md,187
vllm/v1/core/sched/scheduler.py,180
vllm/engine/async_llm_engine.py,180
vllm/v1/engine/core.py,174
vllm/platforms/cuda.py,172
vllm/model_executor/models/qwen2_vl.py,171
vllm/sequence.py,167
vllm/model_executor/layers/fused_moe/fused_moe.py,167
vllm/model_executor/models/llama.py,163
vllm/entrypoints/openai/serving_engine.py,161
vllm/v1/worker/gpu_worker.py,161
vllm/v1/worker/tpu_model_runner.py,156
docs/source/models/supported_models.rst,153
vllm/model_executor/models/llava.py,152
vllm/platforms/rocm.py,151
vllm/v1/engine/async_llm.py,148
vllm/entrypoints/chat_utils.py,146
vllm/worker/worker.py,143
vllm/platforms/interface.py,139
vllm/attention/layer.py,138
csrc/torch_bindings.cpp,137
pyproject.toml,135
csrc/ops.h,133
vllm/entrypoints/openai/serving_completion.py,131
vllm/core/scheduler.py,131
docs/source/models/supported_models.md,129
vllm/model_executor/models/phi3v.py,123
vllm/model_executor/models/qwen2_5_vl.py,122
vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py,120
vllm/v1/attention/backends/flash_attn.py,120
vllm/distributed/parallel_state.py,118
vllm/v1/engine/processor.py,118
vllm/model_executor/models/deepseek_v2.py,117
vllm/config/model.py,114
vllm/model_executor/models/pixtral.py,114
vllm/model_executor/models/minicpmv.py,114
vllm/v1/spec_decode/eagle.py,113
vllm/model_executor/models/__init__.py,112
vllm/v1/attention/backends/mla/common.py,112
vllm/v1/engine/llm_engine.py,112
vllm/model_executor/model_loader/loader.py,112
vllm/v1/attention/backends/flashinfer.py,112
vllm/model_executor/layers/linear.py,112
vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py,110
vllm/multimodal/processing.py,108
tests/utils.py,106
vllm/model_executor/models/interfaces.py,105
examples/offline_inference/vision_language.py,103
vllm/sampling_params.py,103
vllm/model_executor/models/internvl.py,102
vllm/model_executor/layers/quantization/modelopt.py,102
docker/Dockerfile,101
vllm/utils/__init__.py,100
benchmarks/benchmark_serving.py,100
tests/models/multimodal/processing/test_common.py,100
vllm/platforms/cpu.py,100
requirements/common.txt,99
vllm/v1/engine/core_client.py,99
vllm/model_executor/models/llava_next.py,99
vllm/config/__init__.py,98
vllm/compilation/backends.py,97
vllm/model_executor/models/mixtral.py,96
vllm/model_executor/models/qwen2.py,96
vllm/v1/worker/gpu_input_batch.py,95
Dockerfile,94
vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py,93
vllm/model_executor/models/ultravox.py,93
vllm/model_executor/model_loader/weight_utils.py,90
vllm/platforms/tpu.py,90
requirements-common.txt,89
vllm/platforms/xpu.py,89
vllm/model_executor/models/molmo.py,89
vllm/model_executor/models/utils.py,89
vllm/attention/backends/flash_attn.py,88
vllm/v1/core/kv_cache_utils.py,88
vllm/model_executor/models/qwen.py,88
vllm/model_executor/layers/sampler.py,87
benchmarks/benchmark_throughput.py,87
vllm/v1/core/kv_cache_manager.py,87
vllm/multimodal/registry.py,87
vllm/multimodal/utils.py,87
vllm/model_executor/layers/quantization/utils/fp8_utils.py,87
docs/source/index.rst,86
requirements/test.txt,85
tests/v1/core/test_scheduler.py,84
vllm/model_executor/layers/rotary_embedding.py,84
vllm/model_executor/models/qwen3_vl.py,83
vllm/transformers_utils/configs/__init__.py,83
vllm/model_executor/layers/quantization/gptq_marlin.py,82
vllm/v1/attention/backends/utils.py,82
vllm/config/vllm.py,82
vllm/config/compilation.py,82
vllm/attention/backends/flashinfer.py,82
tests/distributed/test_pipeline_parallel.py,82
vllm/transformers_utils/tokenizer.py,82
vllm/lora/models.py,82
vllm/entrypoints/openai/cli_args.py,82
vllm/model_executor/models/chameleon.py,82
vllm/model_executor/models/fuyu.py,82
vllm/entrypoints/openai/serving_responses.py,80
requirements/test.in,80
vllm/attention/backends/rocm_flash_attn.py,79
vllm/model_executor/models/baichuan.py,79
vllm/v1/executor/multiproc_executor.py,79
vllm/attention/selector.py,79
vllm/model_executor/models/paligemma.py,79
vllm/inputs/preprocess.py,78
vllm/model_executor/models/blip2.py,76
vllm/model_executor/models/llava_onevision.py,75
vllm/spec_decode/spec_decode_worker.py,75
vllm/multimodal/inputs.py,75
vllm/model_executor/models/chatglm.py,75
vllm/model_executor/models/glm4_1v.py,74
vllm/v1/metrics/loggers.py,74
examples/offline_inference/vision_language_multi_image.py,73
vllm/model_executor/models/jamba.py,73
vllm/attention/backends/abstract.py,72
benchmarks/benchmark_latency.py,71
tests/entrypoints/openai/test_serving_chat.py,71
tests/v1/core/test_kv_cache_utils.py,69
tests/v1/worker/test_gpu_model_runner.py,69
vllm/model_executor/models/whisper.py,69
.buildkite/test-amd.yaml,69
vllm/worker/worker_base.py,69
vllm/model_executor/models/qwen2_audio.py,69
vllm/model_executor/models/transformers.py,68
vllm/model_executor/models/mllama.py,68
vllm/model_executor/models/gpt_bigcode.py,67
vllm/worker/cpu_model_runner.py,67
tests/test_config.py,67
.buildkite/release-pipeline.yaml,67
vllm/executor/ray_gpu_executor.py,66
vllm/engine/protocol.py,66
vllm/model_executor/models/gemma.py,66
vllm/lora/layers.py,66
vllm/model_executor/models/gpt2.py,66
vllm/model_executor/layers/quantization/mxfp4.py,66
vllm/executor/executor_base.py,65
vllm/model_executor/models/gpt_j.py,65
vllm/inputs/registry.py,64
vllm/model_executor/models/gemma3_mm.py,64
vllm/v1/request.py,64
vllm/v1/worker/tpu_worker.py,64
vllm/outputs.py,63
tests/quantization/test_compressed_tensors.py,63
vllm/model_executor/models/gpt_neox.py,63
vllm/model_executor/models/falcon.py,63
.pre-commit-config.yaml,63
vllm/model_executor/models/idefics3.py,62
tests/v1/core/test_prefix_caching.py,62
vllm/model_executor/layers/quantization/__init__.py,62
vllm/model_executor/model_loader/utils.py,62
vllm/model_executor/models/bert.py,62
vllm/model_executor/models/internlm2.py,62
vllm/model_executor/models/deepseek_vl2.py,62
vllm/model_executor/models/opt.py,62
vllm/model_executor/models/siglip.py,61
vllm/entrypoints/openai/run_batch.py,61
vllm/model_executor/models/llava_next_video.py,61
vllm/model_executor/models/qwen2_moe.py,61
vllm/entrypoints/openai/serving_embedding.py,61
benchmarks/kernels/benchmark_moe.py,61
vllm/benchmarks/datasets.py,61
vllm/model_executor/models/config.py,61
vllm/model_executor/models/bloom.py,60
.github/CODEOWNERS,60
tests/lora/conftest.py,60
tests/models/test_initialization.py,60
vllm/model_executor/models/qwen2_5_omni_thinker.py,60
vllm/model_executor/layers/activation.py,59
tests/models/decoder_only/vision_language/test_models.py,59
vllm/engine/metrics.py,59
vllm/model_executor/models/gemma2.py,59
vllm/model_executor/models/aria.py,59
tests/entrypoints/test_chat_utils.py,58
tests/models/utils.py,58
vllm/model_executor/models/minicpm.py,58
vllm/benchmarks/serve.py,58
vllm/multimodal/profiling.py,58
tests/models/multimodal/generation/test_common.py,58
vllm/model_executor/models/commandr.py,58
vllm/model_executor/layers/quantization/utils/w8a8_utils.py,58
vllm/__init__.py,57
vllm/v1/attention/backends/triton_attn.py,57
vllm/model_executor/models/mpt.py,56
tests/v1/engine/test_engine_core_client.py,56
tests/v1/kv_connector/unit/test_nixl_connector.py,56
vllm/model_executor/models/clip.py,56
vllm/model_executor/layers/quantization/awq_marlin.py,56
csrc/cache_kernels.cu,56
tests/v1/entrypoints/llm/test_struct_output_generate.py,55
docs/source/conf.py,55
vllm/engine/multiprocessing/client.py,55
vllm/model_executor/layers/fused_moe/modular_kernel.py,55
tests/multimodal/test_processing.py,55
vllm/model_executor/models/phi.py,54
docs/source/serving/openai_compatible_server.md,54
vllm/multimodal/base.py,54
vllm/compilation/decorators.py,54
benchmarks/backend_request_func.py,54
vllm/attention/backends/xformers.py,54
vllm/transformers_utils/tokenizers/mistral.py,53
vllm/v1/worker/utils.py,53
vllm/v1/engine/__init__.py,52
vllm/worker/cpu_worker.py,52
vllm/v1/engine/output_processor.py,52
vllm/model_executor/layers/fused_moe/cutlass_moe.py,51
vllm/model_executor/layers/mamba/mamba_mixer2.py,51
vllm/model_executor/models/qwen3_moe.py,51
vllm/config/parallel.py,51
vllm/model_executor/models/qwen3_next.py,51
tests/v1/engine/test_engine_core.py,51
vllm/model_executor/models/mllama4.py,51
tests/lora/test_layers.py,50
requirements.txt,50
vllm/model_executor/layers/pooler.py,50
vllm/model_executor/models/stablelm.py,49
vllm/model_executor/models/starcoder2.py,49
vllm/worker/tpu_model_runner.py,48
.buildkite/run-cpu-test.sh,48
vllm/entrypoints/api_server.py,48
vllm/forward_context.py,48
tests/basic_correctness/test_basic_correctness.py,48
vllm/v1/attention/backends/flex_attention.py,48
vllm/model_executor/models/olmo.py,48
vllm/model_executor/models/dbrx.py,48
tests/engine/test_arg_utils.py,47
vllm/model_executor/models/phimoe.py,47
vllm/attention/backends/utils.py,47
vllm/platforms/__init__.py,47
vllm/attention/backends/torch_sdpa.py,47
.gitignore,47
vllm/worker/xpu_model_runner.py,47
tests/kernels/test_attention.py,47
format.sh,47
vllm/executor/ray_utils.py,47
vllm/model_executor/models/adapters.py,47
vllm/v1/attention/backends/mla/flashmla.py,46
vllm/model_executor/layers/fused_moe/config.py,46
vllm/model_executor/layers/fused_moe/deep_gemm_moe.py,46
vllm/v1/utils.py,46
vllm/v1/structured_output/__init__.py,46
cmake/cpu_extension.cmake,46
vllm/model_executor/layers/layernorm.py,46
vllm/v1/worker/gpu/model_runner.py,45
vllm/v1/attention/backends/mla/rocm_aiter_mla.py,45
Dockerfile.rocm,45
tests/samplers/test_sampler.py,45
vllm/model_executor/layers/quantization/bitsandbytes.py,45
vllm/v1/outputs.py,45
vllm/utils/deep_gemm.py,45
vllm/model_executor/models/keye.py,45
vllm/model_executor/models/granite.py,45
vllm/model_executor/models/phi4mm.py,45
vllm/model_executor/model_loader.py,45
tests/kernels/test_cache.py,44
.github/mergify.yml,44
vllm/model_executor/layers/quantization/utils/marlin_utils.py,44
vllm/model_executor/models/mistral3.py,44
vllm/v1/metrics/stats.py,44
vllm/model_executor/model_loader/tensorizer.py,44
vllm/engine/multiprocessing/engine.py,44
vllm/model_executor/models/deepseek.py,44
vllm/utils/flashinfer.py,44
vllm/distributed/kv_transfer/kv_connector/v1/base.py,44
tests/kernels/moe/test_moe.py,44
docs/source/getting_started/installation.rst,43
vllm/model_executor/models/vision.py,43
vllm/model_executor/models/orion.py,43
vllm/v1/core/scheduler.py,43
vllm/model_executor/models/mamba.py,43
docs/serving/openai_compatible_server.md,43
vllm/distributed/device_communicators/shm_broadcast.py,43
tests/entrypoints/openai/test_chat.py,43
tests/kernels/utils.py,43
tests/v1/e2e/test_spec_decode.py,43
vllm/executor/gpu_executor.py,43
vllm/v1/attention/backends/pallas.py,43
tests/v1/spec_decode/test_eagle.py,42
vllm/v1/attention/backends/rocm_aiter_fa.py,42
vllm/model_executor/models/arctic.py,42
vllm/lora/utils.py,42
vllm/model_executor/layers/quantization/gguf.py,42
requirements-test.txt,42
tests/entrypoints/openai/test_vision.py,42
vllm/entrypoints/cli/serve.py,42
vllm/inputs/data.py,42
tests/test_utils.py,41
vllm/model_executor/models/qwen3_omni_moe_thinker.py,41
vllm/model_executor/models/nemotron.py,41
vllm/compilation/compiler_interface.py,41
vllm/model_executor/models/qwen_vl.py,41
vllm/lora/worker_manager.py,41
docs/features/tool_calling.md,41
vllm/model_executor/custom_op.py,41
vllm/multimodal/image.py,41
vllm/v1/sample/sampler.py,41
vllm/model_executor/models/gpt_oss.py,40
tests/entrypoints/test_openai_server.py,40
vllm/model_executor/models/modernbert.py,40
vllm/model_executor/models/olmoe.py,40
vllm/executor/ray_distributed_executor.py,40
vllm/entrypoints/openai/serving_score.py,40
vllm/model_executor/models/ernie45_vl.py,40
vllm/entrypoints/openai/serving_tokenization.py,40
vllm/model_executor/models/exaone.py,40
vllm/model_executor/sampling_metadata.py,40
vllm/model_executor/layers/fused_moe/__init__.py,40
vllm/model_executor/models/mixtral_quant.py,39
vllm/model_executor/models/glm4v.py,39
vllm/multimodal/video.py,39
vllm/model_executor/models/granitemoe.py,39
vllm/logger.py,39
vllm/attention/backends/mla/common.py,39
vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py,39
vllm/model_executor/layers/attention.py,39
tests/v1/kv_connector/unit/utils.py,39
vllm/model_executor/models/solar.py,39
vllm/spec_decode/draft_model_runner.py,39
vllm/worker/enc_dec_model_runner.py,39
vllm/model_executor/layers/fused_moe/fused_marlin_moe.py,38
tests/quantization/test_fp8.py,38
vllm/model_executor/models/jais.py,38
tests/core/test_scheduler.py,38
vllm/model_executor/layers/quantization/utils/quant_utils.py,38
vllm/distributed/device_communicators/custom_all_reduce.py,38
vllm/model_executor/models/aya_vision.py,38
vllm/v1/sample/rejection_sampler.py,38
vllm/v1/sample/ops/topk_topp_sampler.py,37
csrc/moe/torch_bindings.cpp,37
examples/offline_inference_vision_language.py,37
vllm/plugins/__init__.py,37
tests/worker/test_model_runner.py,37
docs/usage/v1_guide.md,37
vllm/model_executor/models/bamba.py,37
vllm/worker/tpu_worker.py,37
vllm/distributed/kv_transfer/kv_connector/utils.py,37
vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py,37
tests/multimodal/test_utils.py,37
vllm/model_executor/models/minicpmo.py,37
vllm/executor/cpu_executor.py,37
vllm/model_executor/layers/vocab_parallel_embedding.py,37
vllm/model_executor/layers/quantization/quark/quark_moe.py,37
tests/entrypoints/openai/test_metrics.py,36
vllm/engine/output_processor/multi_step.py,36
vllm/attention/backends/pallas.py,36
vllm/config/speculative.py,36
vllm/model_executor/models/voxtral.py,36
vllm/model_executor/models/roberta.py,36
vllm/v1/core/sched/output.py,36
vllm/v1/kv_cache_interface.py,36
vllm/v1/core/single_type_kv_cache_manager.py,35
examples/offline_inference/audio_language.py,35
tests/compile/piecewise/test_toy_llama.py,35
tests/models/language/generation/test_hybrid.py,35
vllm/worker/neuron_model_runner.py,35
docs/source/models/vlm.rst,35
tests/v1/engine/test_async_llm.py,35
vllm/v1/attention/backends/cpu_attn.py,35
vllm/v1/serial_utils.py,35
docs/features/multimodal_inputs.md,35
vllm/worker/multi_step_model_runner.py,35
tests/compile/test_full_graph.py,34
tests/v1/worker/test_gpu_input_batch.py,34
vllm/model_executor/models/persimmon.py,34
vllm/model_executor/models/gritlm.py,34
vllm/model_executor/guided_decoding/__init__.py,34
vllm/model_executor/models/skyworkr1v.py,34
csrc/attention/attention_kernels.cu,34
vllm/model_executor/layers/quantization/gptq.py,34
tests/v1/attention/utils.py,34
vllm/model_executor/models/llama_eagle3.py,34
tests/kernels/attention/test_attention_selector.py,34
vllm/model_executor/models/nano_nemotron_vl.py,34
tests/compile/test_fusion_attn.py,34
vllm/model_executor/models/minimax_text_01.py,34
vllm/distributed/utils.py,34
vllm/entrypoints/openai/serving_pooling.py,34
vllm/model_executor/models/nemotron_h.py,34
vllm/multimodal/parse.py,33
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py,33
vllm/model_executor/layers/logits_processor.py,33
vllm/model_executor/models/qwen2_rm.py,33
vllm/model_executor/layers/utils.py,33
vllm/spec_decode/multi_step_worker.py,33
vllm/entrypoints/utils.py,33
vllm/model_executor/models/florence2.py,33
vllm/entrypoints/openai/tool_parsers/__init__.py,33
vllm/model_executor/models/glm4_moe.py,33
vllm/distributed/kv_transfer/kv_connector/factory.py,33
vllm/model_executor/models/kimi_vl.py,33
vllm/compilation/pass_manager.py,33
vllm/v1/core/block_pool.py,33
vllm/v1/structured_output/backend_xgrammar.py,33
vllm/model_executor/models/plamo2.py,32
vllm/worker/cache_engine.py,32
tests/lora/test_lora_manager.py,32
vllm/model_executor/layers/quantization/moe_wna16.py,32
vllm/attention/ops/prefix_prefill.py,32
vllm/model_executor/models/zamba2.py,32
tests/entrypoints/openai/test_completion.py,32
tests/v1/tpu/worker/test_tpu_model_runner.py,32
vllm/entrypoints/context.py,32
vllm/model_executor/layers/quantization/awq.py,31
requirements/cpu.txt,31
vllm/model_executor/models/deepseek_mtp.py,31
requirements/nightly_torch_test.txt,31
benchmarks/kernels/benchmark_paged_attention.py,31
vllm/compilation/wrapper.py,31
vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py,31
tests/compile/test_config.py,31
benchmarks/README.md,31
tests/lora/test_llama_tp.py,31
benchmarks/benchmark_serving_structured_output.py,31
vllm/model_executor/layers/fused_moe/fused_batched_moe.py,31
vllm/v1/engine/detokenizer.py,31
vllm/distributed/device_communicators/pynccl.py,31
vllm/v1/executor/abstract.py,31
vllm/model_executor/models/granite_speech.py,31
tests/kernels/test_moe.py,31
vllm/model_executor/layers/batch_invariant.py,31
vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py,31
tests/model_executor/test_enabled_custom_ops.py,31
docs/source/index.md,31
vllm/model_executor/guided_decoding/xgrammar_decoding.py,30
requirements-dev.txt,30
docs/getting_started/installation/cpu.md,30
tests/spec_decode/e2e/conftest.py,30
vllm/model_executor/models/granitemoehybrid.py,30
benchmarks/benchmark_dataset.py,30
tests/v1/attention/test_attention_backends.py,30
vllm/model_executor/models/tarsier.py,30
cmake/utils.cmake,30
tests/models/multimodal/processing/test_tensor_schema.py,30
vllm/compilation/collective_fusion.py,30
vllm/model_executor/models/dots_ocr.py,30
tests/tokenization/test_detokenize.py,30
vllm/v1/attention/backends/mla/cutlass_mla.py,30
vllm/usage/usage_lib.py,30
vllm/v1/engine/utils.py,30
vllm/v1/attention/backends/mla/flashattn_mla.py,30
vllm/model_executor/models/llama4.py,30
.github/workflows/mypy.yaml,30
.buildkite/run-amd-test.sh,30
vllm/attention/ops/paged_attn.py,30
csrc/cpu/torch_bindings.cpp,30
vllm/v1/worker/block_table.py,30
vllm/model_executor/model_loader/bitsandbytes_loader.py,30
tests/compile/test_basic_correctness.py,30
tests/metrics/test_metrics.py,30
vllm/model_executor/layers/quantization/ipex_quant.py,29
vllm/model_executor/weight_utils.py,29
tests/entrypoints/openai/test_response_api_with_harmony.py,29
vllm/v1/worker/cpu_model_runner.py,29
tests/compile/test_fusion.py,29
tests/compile/piecewise/test_simple.py,29
tests/kernels/test_pos_encoding.py,29
vllm/config/cache.py,29
tests/spec_decode/test_spec_decode_worker.py,29
vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py,29
vllm/model_executor/layers/fused_moe/utils.py,29
vllm/worker/model_runner_base.py,29
vllm/model_executor/models/mamba2.py,29
tests/basic_correctness/test_chunked_prefill.py,29
vllm/model_executor/models/ovis.py,29
vllm/lora/punica_wrapper/punica_gpu.py,29
vllm/distributed/device_communicators/cuda_communicator.py,29
csrc/quantization/gptq_marlin/gptq_marlin.cu,29
docker/Dockerfile.cpu,29
vllm/multimodal/__init__.py,29
requirements/tpu.txt,29
vllm/entrypoints/openai/speech_to_text.py,29
vllm/pooling_params.py,29
tests/entrypoints/openai/test_audio.py,28
vllm/model_executor/models/gemma3n_mm.py,28
vllm/model_executor/models/interfaces_base.py,28
vllm/worker/hpu_model_runner.py,28
vllm/model_executor/models/bert_with_rope.py,28
tests/distributed/test_pynccl.py,28
vllm/executor/multiproc_gpu_executor.py,28
vllm/model_executor/models/h2ovl.py,28
tests/kernels/moe/test_deepep_deepgemm_moe.py,28
vllm/benchmarks/throughput.py,28
tests/spec_decode/utils.py,28
tests/async_engine/test_async_llm_engine.py,28
vllm/core/block_manager.py,28
vllm/model_executor/layers/quantization/experts_int8.py,28
tests/models/test_transformers.py,28
vllm/model_executor/layers/rejection_sampler.py,27
vllm/spec_decode/batch_expansion.py,27
tests/basic_correctness/test_cumem.py,27
vllm/inputs/__init__.py,27
tests/models/multimodal/generation/vlm_utils/model_utils.py,27
vllm/model_executor/models/step3_vl.py,27
vllm/model_executor/models/bart.py,27
vllm/model_executor/models/falcon_h1.py,27
vllm/v1/engine/input_processor.py,27
Dockerfile.cpu,27
tests/tensorizer_loader/test_tensorizer.py,27
tests/v1/sample/test_logprobs.py,27
vllm/model_executor/model_loader/default_loader.py,27
csrc/pybind.cpp,27
requirements/docs.txt,27
vllm/model_executor/models/olmo2.py,27
vllm/model_executor/models/intern_vit.py,27
vllm/model_executor/models/gemma3.py,27
.github/workflows/publish.yml,27
vllm/entrypoints/openai/serving_models.py,26
tests/distributed/test_comm_ops.py,26
tests/lora/test_worker.py,26
vllm/config/scheduler.py,26
tests/core/utils.py,26
csrc/cache.h,26
csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu,26
vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py,26
vllm/v1/attention/backends/rocm_attn.py,26
tests/entrypoints/llm/test_guided_generate.py,26
requirements/cuda.txt,26
vllm/transformers_utils/processor.py,26
vllm/model_executor/layers/quantization/compressed_tensors/utils.py,26
tests/models/test_registry.py,26
vllm/platforms/hpu.py,26
benchmarks/benchmark_prefix_caching.py,26
csrc/layernorm_kernels.cu,26
vllm/model_executor/models/interns1.py,26
vllm/attention/backends/blocksparse_attn.py,26
requirements-rocm.txt,26
requirements-cuda.txt,26
docs/models/pooling_models.md,25
vllm/multimodal/audio.py,25
vllm/platforms/neuron.py,25
vllm/model_executor/models/phi3_small.py,25
vllm/compilation/fusion.py,25
vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py,25
cmake/external_projects/vllm_flash_attn.cmake,25
vllm/executor/neuron_executor.py,25
vllm/entrypoints/harmony_utils.py,25
requirements/rocm-test.txt,25
vllm/engine/multiprocessing/__init__.py,25
tests/entrypoints/openai/test_embedding.py,25
tests/v1/engine/test_output_processor.py,25
vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py,25
vllm/core/block/prefix_caching_block.py,25
vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py,25
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py,25
vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py,25
vllm/attention/backends/ipex_attn.py,25
vllm/engine/output_processor/single_step.py,25
vllm/model_executor/layers/mamba/mamba_mixer.py,25
vllm/v1/attention/backends/gdn_attn.py,25
vllm/multimodal/cache.py,25
.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh,25
tests/v1/attention/test_mla_backends.py,25
vllm/v1/attention/backends/mla/flashmla_sparse.py,25
vllm/worker/embedding_model_runner.py,25
tests/kernels/quantization/test_block_fp8.py,25
tests/kernels/test_prefix_prefill.py,25
vllm/inputs/parse.py,25
vllm/v1/core/encoder_cache_manager.py,24
vllm/core/block_manager_v2.py,24
csrc/rocm/attention.cu,24
vllm/_ipex_ops.py,24
tests/v1/core/utils.py,24
vllm/model_executor/layers/quantization/fbgemm_fp8.py,24
vllm/model_executor/models/siglip2navit.py,24
vllm/model_executor/utils.py,24
vllm/distributed/device_communicators/all2all.py,24
vllm/core/block_manager_v1.py,24
vllm/model_executor/models/blip.py,24
docs/requirements-docs.txt,24
tests/models/language/generation/test_common.py,24
tests/utils_/test_utils.py,24
vllm/model_executor/models/minimax_vl_01.py,24
vllm/model_executor/models/xverse.py,24
vllm/model_executor/models/hyperclovax_vision.py,24
vllm/v1/attention/backends/mamba2_attn.py,24
vllm/model_executor/models/qwen3.py,24
vllm/lora/layers/fused_moe.py,24
vllm/engine/ray_utils.py,23
vllm/reasoning/abs_reasoning_parsers.py,23
tests/spec_decode/test_multi_step_worker.py,23
docker/Dockerfile.rocm,23
vllm/v1/worker/lora_model_runner_mixin.py,23
vllm/model_executor/models/bailing_moe.py,23
vllm/v1/core/kv_cache_coordinator.py,23
docs/configuration/optimization.md,23
vllm/model_executor/guided_decoding/outlines_logits_processors.py,23
vllm/compilation/inductor_pass.py,23
vllm/_aiter_ops.py,23
docs/.nav.yml,23
vllm/worker/openvino_model_runner.py,23
tests/samplers/test_logprobs.py,23
vllm/model_executor/models/mlp_speculator.py,23
tests/v1/test_oracle.py,23
docs/features/reasoning_outputs.md,23
tests/models/language/pooling/mteb_utils.py,23
vllm/distributed/device_communicators/base_device_communicator.py,23
vllm/model_executor/layers/quantization/utils/mxfp4_utils.py,23
vllm/model_executor/layers/quantization/utils/flashinfer_utils.py,23
vllm/spec_decode/util.py,23
vllm/model_executor/models/qwen3_vl_moe.py,23
vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py,23
tests/kernels/test_cutlass.py,23
vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py,23
.buildkite/scripts/hardware_ci/run-cpu-test.sh,23
vllm/model_executor/models/mistral.py,23
tests/kernels/moe/test_pplx_moe.py,23
vllm/model_executor/layers/rotary_embedding/mrope.py,23
docker/Dockerfile.rocm_base,23
vllm/model_executor/layers/mamba/ops/causal_conv1d.py,23
vllm/multimodal/hasher.py,23
tests/models/language/pooling/test_embedding.py,23
vllm/v1/structured_output/utils.py,23
tests/models/test_llava.py,23
vllm/model_executor/models/prithvi_geospatial_mae.py,23
tests/kernels/test_activation.py,23
mkdocs.yaml,23
docs/source/models/adding_model.rst,23
tests/quantization/test_bitsandbytes.py,23
tests/distributed/test_custom_all_reduce.py,22
docs/contributing/benchmarks.md,22
vllm/attention/ops/triton_unified_attention.py,22
cacheflow/worker/worker.py,22
vllm/model_executor/models/llama_eagle.py,22
vllm/executor/uniproc_executor.py,22
vllm/model_executor/models/eagle.py,22
vllm/v1/worker/gpu/cudagraph_utils.py,22
tests/models/test_phi3v.py,22
tests/kernels/quant_utils.py,22
vllm/v1/attention/backends/mamba_attn.py,22
benchmarks/cutlass_benchmarks/w8a8_benchmarks.py,22
vllm/entrypoints/renderer.py,22
vllm/v1/structured_output/backend_guidance.py,22
vllm/v1/worker/worker_base.py,22
tests/entrypoints/llm/test_generate.py,22
tests/kernels/moe/test_cutlass_moe.py,22
tests/models/decoder_only/vision_language/vlm_utils/model_utils.py,22
tests/kernels/test_attention_selector.py,22
vllm/model_executor/models/grok1.py,22
vllm/model_executor/layers/quantization/base_config.py,22
vllm/model_executor/models/hunyuan_v1.py,22
vllm/attention/backends/hpu_attn.py,22
examples/offline_inference/data_parallel.py,22
vllm/worker/neuron_worker.py,22
tests/models/test_oot_registration.py,22
vllm/transformers_utils/detokenizer.py,22
tests/models/encoder_decoder/vision_language/test_mllama.py,22
vllm/model_executor/models/idefics2_vision_model.py,22
vllm/model_executor/models/ernie45_moe.py,22
vllm/model_executor/models/paddleocr_vl.py,22
tests/tool_use/utils.py,22
tests/model_executor/test_model_load_with_params.py,22
vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py,21
tests/entrypoints/openai/test_transcription_validation.py,21
docs/source/getting_started/debugging.rst,21
vllm/v1/attention/backends/tree_attn.py,21
vllm/model_executor/models/medusa.py,21
csrc/activation_kernels.cu,21
vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py,21
vllm/v1/spec_decode/ngram_proposer.py,21
vllm/v1/attention/backends/mla/triton_mla.py,21
vllm/model_executor/layers/quantization/quark/quark.py,21
tests/prefix_caching/test_prefix_caching.py,21
.buildkite/test-template.j2,21
vllm/model_executor/models/nemotron_nas.py,21
vllm/model_executor/models/ovis2_5.py,21
docs/contributing/model/multimodal.md,21
csrc/pos_encoding_kernels.cu,21
vllm/core/interfaces.py,21
requirements/rocm.txt,21
docs/deployment/docker.md,21
vllm/model_executor/models/nvlm_d.py,21
tests/v1/sample/test_sampler.py,21
vllm/v1/worker/cpu_worker.py,21
vllm/assets/video.py,21
vllm/worker/xpu_worker.py,21
vllm/model_executor/models/internlm.py,21
vllm/spec_decode/ngram_worker.py,21
vllm/model_executor/models/dots1.py,20
vllm/v1/worker/kv_connector_model_runner_mixin.py,20
tests/compile/test_silu_mul_quant_fusion.py,20
vllm/model_executor/models/nemotron_vl.py,20
tests/models/decoder_only/audio_language/test_ultravox.py,20
docs/README.md,20
tests/basic_correctness/test_preemption.py,20
tests/entrypoints/openai/test_completion_with_function_calling.py,20
vllm/core/block/cpu_gpu_block_allocator.py,20
tests/kernels/test_flash_attn.py,20
tests/v1/tpu/test_basic.py,20
docs/getting_started/quickstart.md,20
csrc/moe/moe_ops.h,20
tests/samplers/test_rejection_sampler.py,20
tests/core/test_chunked_prefill_scheduler.py,20
vllm/model_executor/parameter.py,20
tests/test_inputs.py,20
tests/compile/test_functionalization.py,20
Dockerfile.tpu,20
cacheflow/sequence.py,20
docs/source/getting_started/quickstart.md,20
tests/entrypoints/openai/test_lora_resolvers.py,20
docs/usage/troubleshooting.md,20
cacheflow/master/block_manager.py,20
vllm/distributed/eplb/eplb_state.py,20
tests/distributed/test_sequence_parallel.py,20
vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py,20
tests/weight_loading/models.txt,20
vllm/model_executor/models/minicpm3.py,20
vllm/model_executor/models/gemma3n.py,20
vllm/v1/sample/metadata.py,20
vllm/executor/multiproc_worker_utils.py,20
vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py,20
vllm/model_executor/models/aquila.py,20
vllm/entrypoints/score_utils.py,20
tests/lora/test_quant_model.py,20
vllm/model_executor/layers/quantization/marlin.py,19
vllm/version.py,19
tests/core/block/e2e/test_correctness.py,19
vllm/transformers_utils/utils.py,19
docs/features/lora.md,19
requirements-cpu.txt,19
vllm/model_executor/layers/rotary_embedding/base.py,19
docs/source/getting_started/amd-installation.rst,19
vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py,19
vllm/lora/punica_wrapper/punica_base.py,19
vllm/transformers_utils/configs/eagle.py,19
vllm/core/block/interfaces.py,19
.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py,19
tests/v1/spec_decode/test_ngram.py,19
vllm/triton_utils/importing.py,19
tests/kernels/test_marlin_gemm.py,19
.buildkite/scripts/hardware_ci/run-xpu-test.sh,19
vllm/distributed/communication_op.py,19
tests/kernels/moe/test_pplx_cutlass_moe.py,19
vllm/spec_decode/top1_proposer.py,19
tests/spec_decode/e2e/test_mlp_correctness.py,19
vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py,19
vllm/compilation/vllm_inductor_pass.py,19
.github/workflows/ruff.yml,19
vllm/core/block/naive_block.py,19
vllm/model_executor/models/phi4_multimodal.py,19
tests/kernels/attention/test_flashinfer_trtllm_attention.py,19
vllm/model_executor/layers/quantization/rtn.py,19
tests/v1/sample/test_rejection_sampler.py,19
docs/contributing/profiling.md,19
csrc/punica/bgmv/bgmv_config.h,19
csrc/moe/topk_softmax_kernels.cu,19
vllm/lora/ops/triton_ops/fused_moe_lora_op.py,19
tests/async_engine/test_api_server.py,18
tests/mq_llm_engine/test_error_handling.py,18
vllm/platforms/openvino.py,18
docs/contributing/model/basic.md,18
vllm/distributed/device_communicators/tpu_communicator.py,18
examples/offline_inference/spec_decode.py,18
vllm/model_executor/layers/quantization/torchao.py,18
docs/source/getting_started/quickstart.rst,18
vllm/core/block/block_table.py,18
vllm/compilation/sequence_parallelism.py,18
vllm/model_executor/models/glm4.py,18
vllm/model_executor/models/cohere2_vision.py,18
vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py,18
vllm/v1/attention/backends/xformers.py,18
benchmarks/kernels/benchmark_rope.py,18
benchmarks/kernels/benchmark_machete.py,18
tests/samplers/test_beam_search.py,18
vllm/model_executor/layers/rotary_embedding/common.py,18
tests/core/block/test_prefix_caching_block.py,18
vllm/config/multimodal.py,18
vllm/model_executor/models/granitemoeshared.py,18
vllm/distributed/device_communicators/pynccl_wrapper.py,18
vllm/model_executor/models/exaone4.py,18
vllm/model_executor/layers/mamba/ops/mamba_ssm.py,18
vllm/model_executor/models/deepseek_ocr.py,18
vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py,18
examples/offline_inference_vision_language_multi_image.py,18
vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py,18
tests/entrypoints/openai/test_tokenization.py,18
tests/models/embedding/language/test_embedding.py,18
vllm/entrypoints/launcher.py,18
vllm/engine/metrics_types.py,18
tests/quantization/test_quark.py,18
vllm/lora/request.py,18
vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py,18
tests/lora/test_mixtral.py,18
tests/entrypoints/openai/test_serving_models.py,18
vllm/model_executor/models/internlm2_ve.py,17
tests/kernels/mamba/test_mamba_ssm_ssd.py,17
vllm/v1/engine/coordinator.py,17
vllm/test_utils.py,17
tests/lora/test_chatglm3_tp.py,17
vllm/attention/backends/registry.py,17
tests/kernels/attention/test_cache.py,17
vllm/model_executor/models/lfm2.py,17
tests/core/test_block_manager.py,17
requirements-build.txt,17
vllm/v1/engine/logprobs.py,17
docs/features/structured_outputs.md,17
csrc/quantization/fp8/common.cu,17
docs/contributing/README.md,17
tests/models/test_models.py,17
tests/v1/engine/test_llm_engine.py,17
tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh,17
tests/entrypoints/openai/test_video.py,17
vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py,17
Dockerfile.neuron,17
vllm/model_executor/layers/fused_moe/prepare_finalize.py,17
vllm/benchmarks/lib/endpoint_request_func.py,17
tests/entrypoints/openai/test_chat_template.py,17
tests/worker/test_swap.py,17
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py,17
vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py,17
tests/entrypoints/openai/test_vision_embedding.py,17
docs/mkdocs/hooks/generate_argparse.py,17
tests/test_logger.py,17
tests/kernels/moe/test_deepep_moe.py,17
tools/pre_commit/mypy.py,17
benchmarks/kernels/benchmark_marlin.py,17
docs/source/models/pooling_models.md,17
docs/design/moe_kernel_features.md,17
csrc/cpu/attention.cpp,17
tests/spec_decode/e2e/test_multistep_correctness.py,17
vllm/model_executor/input_metadata.py,17
.buildkite/nightly-benchmarks/benchmark-pipeline.yaml,17
vllm/v1/worker/xpu_worker.py,17
vllm/attention/__init__.py,17
tests/multimodal/test_cache.py,17
vllm/worker/openvino_worker.py,17
vllm/compilation/fix_functionalization.py,17
vllm/model_executor/models/midashenglm.py,17
cacheflow/master/scheduler.py,17
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py,17
tests/distributed/test_utils.py,17
vllm/v1/worker/gpu/input_batch.py,17
vllm/attention/ops/chunked_prefill_paged_decode.py,17
docs/source/contributing/model/multimodal.md,17
csrc/quantization/machete/generate.py,17
docs/source/dev/multimodal/multimodal_index.rst,17
vllm/v1/attention/backends/mla/indexer.py,17
docs/design/plugin_system.md,17
tests/v1/entrypoints/openai/test_completion.py,17
vllm/model_executor/models/glm4_moe_mtp.py,17
vllm/distributed/device_communicators/custom_all_reduce_utils.py,16
vllm/lora/peft_helper.py,16
tests/spec_decode/e2e/test_ngram_correctness.py,16
tests/entrypoints/openai/test_run_batch.py,16
requirements/rocm-build.txt,16
collect_env.py,16
tests/kernels/moe/test_flashinfer.py,16
vllm/model_executor/model_loader/__init__.py,16
vllm/entrypoints/openai/serving_classification.py,16
.buildkite/lm-eval-harness/test_lm_eval_correctness.py,16
vllm/model_executor/layers/quantization/kv_cache.py,16
vllm/compilation/fusion_attn.py,16
requirements-test.in,16
vllm/spec_decode/interfaces.py,16
vllm/attention/backends/placeholder_attn.py,16
vllm/model_executor/models/apertus.py,16
benchmarks/kernels/benchmark_lora.py,16
vllm/entrypoints/openai/serving_transcription.py,16
vllm/v1/attention/backends/mla/flashinfer_mla.py,16
requirements-tpu.txt,16
tests/quantization/test_torchao.py,16
tests/entrypoints/llm/test_encode.py,16
vllm/model_executor/model_loader/gguf_loader.py,16
tests/multi_step/test_correctness_async_llm.py,16
tests/models/test_llava_next.py,16
vllm/tracing.py,16
docs/features/quantization/quark.md,16
vllm/model_executor/guided_decoding/outlines_decoding.py,16
tests/lora/test_punica.py,16
vllm/model_executor/layers/quantization/input_quant_fp8.py,16
vllm/spec_decode/metrics.py,16
vllm/lora/ops/triton_ops/utils.py,16
examples/online_serving/openai_chat_completion_client_for_multimodal.py,16
vllm/model_executor/models/ernie45_vl_moe.py,16
tests/tpu/test_compilation.py,16
csrc/moe/moe_align_sum_kernels.cu,16
vllm/model_executor/layers/quantization/auto_round.py,16
tests/kernels/moe/modular_kernel_tools/common.py,16
vllm/worker/hpu_worker.py,16
.buildkite/scripts/hardware_ci/run-amd-test.sh,16
tests/lora/utils.py,16
vllm/attention/ops/triton_flash_attention.py,16
vllm/scripts.py,16
tests/test_sequence.py,16
requirements/xpu.txt,16
vllm/utils/torch_utils.py,16
vllm/lora/fully_sharded_layers.py,16
vllm/model_executor/models/terratorch.py,16
docs/community/meetups.md,16
tests/kernels/moe/test_batched_moe.py,16
tests/lora/test_qwen2vl.py,16
vllm/lora/punica.py,16
vllm/model_executor/layers/fused_moe/moe_align_block_size.py,15
.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh,15
tests/kernels/moe/test_modular_kernel_combinations.py,15
tests/entrypoints/llm/test_accuracy.py,15
vllm/transformers_utils/tokenizer_group/__init__.py,15
tests/entrypoints/openai/test_basic.py,15
vllm/distributed/kv_events.py,15
vllm/env_override.py,15
vllm/entrypoints/cli/main.py,15
vllm/model_executor/models/deepseek_eagle.py,15
vllm/model_executor/layers/fused_moe/cpu_fused_moe.py,15
vllm/v1/worker/gpu/spec_decode/eagle.py,15
tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py,15
benchmarks/kernels/benchmark_grouped_gemm_cutlass.py,15
tests/test_cache_block_hashing.py,15
vllm/model_executor/models/llama4_eagle.py,15
tests/kernels/test_layernorm.py,15
vllm/model_executor/layers/quantization/squeezellm.py,15
docs/deployment/k8s.md,15
vllm/device_allocator/cumem.py,15
vllm/model_executor/warmup/deep_gemm_warmup.py,15
tests/compile/piecewise/test_full_cudagraph.py,15
docs/source/serving/distributed_serving.rst,15
csrc/cpu/cpu_types_x86.hpp,15
vllm/model_executor/models/step3_text.py,15
vllm/assets/image.py,15
docs/source/models/generative_models.md,15
vllm/model_executor/layers/mla.py,15
vllm/transformers_utils/configs/mistral.py,15
tests/models/language/pooling/test_gte.py,15
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py,15
tests/kernels/moe/test_block_fp8.py,15
vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py,15
tests/v1/spec_decode/test_tree_attention.py,15
docs/source/features/reasoning_outputs.md,15
.github/workflows/lint-and-deploy.yaml,15
vllm/worker/pooling_model_runner.py,15
vllm/v1/engine/mm_input_cache.py,15
.github/workflows/reminder_comment.yml,15
Dockerfile.ppc64le,15
vllm/model_executor/model_loader/tensorizer_loader.py,15
tests/kernels/moe/test_nvfp4_moe.py,15
vllm/model_executor/model_loader/neuron.py,15
vllm/v1/core/sched/interface.py,15
tests/entrypoints/openai/test_serving_engine.py,15
vllm/model_executor/layers/quantization/gptq_marlin_24.py,15
vllm/config/utils.py,15
docs/serving/expert_parallel_deployment.md,15
cacheflow/models/attention.py,15
csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu,15
vllm/reasoning/__init__.py,15
cacheflow/models/opt.py,15
tests/plugins_tests/test_platform_plugins.py,15
docs/mkdocs/hooks/generate_examples.py,15
docs/source/serving/distributed_serving.md,15
tests/lora/test_lora_checkpoints.py,15
tests/compile/test_sequence_parallelism.py,15
.buildkite/run-tpu-v1-test.sh,15
tests/entrypoints/openai/test_score.py,15
docker/Dockerfile.xpu,15
docs/features/spec_decode.md,15
tests/v1/e2e/test_async_scheduling.py,14
tests/kernels/core/test_pos_encoding.py,14
vllm/v1/sample/tpu/metadata.py,14
vllm/model_executor/layers/quantization/ptpc_fp8.py,14
csrc/quantization/compressed_tensors/int8_quant_kernels.cu,14
tests/entrypoints/llm/test_chat.py,14
vllm/entrypoints/responses_utils.py,14
tests/v1/kv_connector/unit/test_multi_connector.py,14
docs/source/getting_started/cpu-installation.rst,14
tests/worker/test_encoder_decoder_model_runner.py,14
tests/lora/test_baichuan.py,14
examples/offline_inference/eagle.py,14
.github/workflows/yapf.yml,14
vllm/model_executor/models/minimax_m2.py,14
tests/quantization/test_register_quantization_config.py,14
tests/compile/test_pass_manager.py,14
tests/spec_decode/e2e/test_compatibility.py,14
docs/source/serving/offline_inference.md,14
tests/v1/engine/test_engine_args.py,14
vllm/model_executor/__init__.py,14
csrc/dispatch_utils.h,14
vllm/lora/ops/triton_ops/lora_shrink_op.py,14
vllm/executor/openvino_executor.py,14
tests/distributed/test_context_parallel.py,14
vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py,14
vllm/model_executor/layers/mamba/mamba_utils.py,14
tests/tracing/test_tracing.py,14
docs/getting_started/installation/README.md,14
vllm/model_executor/models/decilm.py,14
.buildkite/nightly-benchmarks/README.md,14
tests/worker/test_model_input.py,14
tests/kernels/test_int8_quant.py,14
tests/models/language/pooling/test_jina.py,14
vllm/model_executor/models/mimo_mtp.py,14
tests/models/decoder_only/language/test_models.py,14
vllm/v1/attention/backends/short_conv_attn.py,14
vllm/worker/cpu_enc_dec_model_runner.py,14
tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py,14
.buildkite/lm-eval-harness/configs/models-small.txt,14
vllm/reasoning/gptoss_reasoning_parser.py,14
tests/compile/distributed/test_fusions_e2e.py,14
vllm/v1/cudagraph_dispatcher.py,14
tests/kernels/moe/test_deepgemm.py,14
vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py,14
docs/source/serving/multimodal_inputs.md,14
vllm/model_executor/models/lfm2_moe.py,14
tests/entrypoints/openai/test_cli_args.py,14
vllm/model_executor/models/moonvit.py,14
tests/lora/test_add_lora.py,14
vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py,14
docs/models/generative_models.md,14
vllm/v1/structured_output/request.py,14
tests/v1/tpu/test_sampler.py,14
vllm/distributed/device_communicators/xpu_communicator.py,14
Dockerfile.openvino,14
tests/distributed/test_basic_distributed_correctness.py,14
vllm/worker/utils.py,14
tests/lora/test_gemma.py,14
docs/design/metrics.md,14
vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py,14
tests/lora/test_utils.py,14
tests/entrypoints/openai/test_lora_adapters.py,14
tests/v1/test_serial_utils.py,14
vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py,14
tests/compile/test_async_tp.py,14
vllm/model_executor/layers/mamba/linear_attn.py,14
tests/kernels/test_encoder_decoder_attn.py,14
vllm/v1/spec_decode/metrics.py,14
vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py,14
tests/entrypoints/openai/test_serving_responses.py,14
vllm/transformers_utils/s3_utils.py,14
vllm/entrypoints/cli/openai.py,14
vllm/v1/worker/tpu_input_batch.py,14
vllm/model_executor/models/qwen3_next_mtp.py,14
tests/spec_decode/e2e/test_medusa_correctness.py,14
tests/entrypoints/openai/test_prompt_validation.py,14
tests/models/embedding/vision_language/test_llava_next.py,14
docs/source/contributing/overview.md,14
tests/model_executor/test_guided_processors.py,14
vllm/scalar_type.py,14
.buildkite/check-wheel-size.py,14
cacheflow/models/model_utils.py,14
vllm/model_executor/layers/mamba/ops/ssd_combined.py,13
vllm/model_executor/layers/quantization/aqlm.py,13
tests/models/multimodal/generation/vlm_utils/types.py,13
docs/features/nixl_connector_usage.md,13
tests/test_regression.py,13
csrc/moe/marlin_moe_ops.cu,13
tests/kernels/moe/modular_kernel_tools/mk_objects.py,13
vllm/engine/output_processor/stop_checker.py,13
vllm/reasoning/basic_parsers.py,13
vllm/v1/executor/ray_executor.py,13
vllm/lora/layers/base_linear.py,13
tests/compile/backend.py,13
tests/entrypoints/openai/test_completion_with_prompt_embeds.py,13
vllm/distributed/device_communicators/cuda_wrapper.py,13
tests/v1/kv_offload/test_cpu_offloading.py,13
vllm/v1/engine/mm_input_mapper.py,13
docker/Dockerfile.nightly_torch,13
vllm/model_executor/models/phi4mm_audio.py,13
vllm/v1/core/sched/utils.py,13
vllm/entrypoints/openai/rpc/server.py,13
tests/v1/generation/test_batch_invariance.py,13
vllm/compilation/counter.py,13
tests/models/multimodal/processing/test_llava_next.py,13
tests/v1/attention/test_sparse_mla_backends.py,13
tools/profiler/visualize_layerwise_profile.py,13
vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py,13
vllm/v1/worker/gpu/states.py,13
simple_server.py,13
vllm/executor/xpu_executor.py,13
tests/models/multimodal/processing/test_llava_onevision.py,13
vllm/attention/layers/chunked_local_attention.py,13
tests/kernels/attention/test_mha_attn.py,13
docs/features/quantization/README.md,13
tests/entrypoints/openai/test_openai_schema.py,13
vllm/v1/worker/dp_utils.py,13
vllm/model_executor/models/longcat_flash.py,13
cacheflow/sampling_params.py,13
benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py,13
tests/tool_use/test_jamba_tool_parser.py,13
vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py,13
vllm/model_executor/layers/quantization/tpu_int8.py,13
tests/entrypoints/offline_mode/test_offline_mode.py,13
vllm/distributed/kv_transfer/kv_connector/simple_connector.py,13
tools/mypy.sh,13
vllm/model_executor/models/jina_vl.py,13
vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py,13
vllm/entrypoints/openai/rpc/client.py,13
vllm/engine/output_processor/interfaces.py,13
tests/v1/cudagraph/test_cudagraph_mode.py,13
tests/models/decoder_only/vision_language/test_phi3v.py,13
vllm/attention/ops/flashmla.py,13
csrc/moe/marlin_moe_wna16/marlin_template.h,13
tests/spec_decode/e2e/test_integration_dist_tp2.py,13
docs/source/getting_started/tpu-installation.rst,13
examples/offline_inference/basic/embed.py,13
vllm/model_executor/layers/quantization/deepspeedfp.py,13
vllm/model_executor/layers/rotary_embedding/__init__.py,13
tests/models/multimodal/processing/test_internvl.py,13
vllm/compilation/monitor.py,13
vllm/core/evictor.py,13
examples/offline_inference/encoder_decoder_multimodal.py,13
vllm/lora/lora.py,13
vllm/model_executor/parallel_utils/communication_op.py,13
vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py,13
vllm/entrypoints/cli/benchmark/main.py,13
tests/models/language/generation/test_mistral.py,13
vllm/lora/ops/triton_ops/lora_expand_op.py,13
tests/models/multimodal/processing/test_qwen2_vl.py,13
Dockerfile.xpu,13
csrc/rocm/ops.h,13
docs/source/serving/deploying_with_docker.rst,13
vllm/v1/worker/gpu_ubatch_wrapper.py,13
tests/models/test_marlin.py,13
examples/offline_inference/prithvi_geospatial_mae.py,13
vllm/model_executor/models/hunyuan_vision.py,13
tests/quantization/test_lm_head.py,13
tests/multi_step/test_correctness_llm.py,13
tests/async_engine/test_openapi_server_ray.py,13
tests/v1/sample/test_sampling_params_e2e.py,13
tools/ep_kernels/install_python_libraries.sh,13
vllm/model_executor/layers/spec_decode_base_sampler.py,13
tests/entrypoints/llm/test_lazy_outlines.py,13
vllm/attention/ops/ipex_attn.py,13
tests/kernels/test_flashinfer.py,13
tests/test_sharded_state_loader.py,13
vllm/model_executor/models/yi.py,13
tests/entrypoints/conftest.py,13
vllm/attention/backends/rocm_aiter_mla.py,13
tests/spec_decode/e2e/test_eagle_correctness.py,13
tests/kernels/attention/test_attention.py,13
tests/lora/test_long_context.py,13
docs/source/getting_started/installation/gpu/rocm.inc.md,13
cacheflow/worker/cache_engine.py,13
tests/lora/test_minicpmv_tp.py,13
vllm/v1/executor/ray_distributed_executor.py,13
tests/samplers/test_no_bad_words.py,13
vllm/entrypoints/openai/logits_processors.py,13
tests/tpu/test_custom_dispatcher.py,13
tests/basic_correctness/test_cpu_offload.py,13
cacheflow/core/scheduler.py,13
vllm/attention/ops/common.py,12
tests/compile/utils.py,12
examples/offline_inference/basic/score.py,12
tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py,12
vllm/model_executor/parallel_utils/parallel_state.py,12
tests/v1/spec_decode/test_max_len.py,12
tools/pre_commit/check_pickle_imports.py,12
vllm/model_executor/model_loader/runai_streamer_loader.py,12
vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py,12
docs/features/quantization/int8.md,12
csrc/quantization/gptq/q_gemm.cu,12
vllm/model_executor/layers/quantization/gptq_bitblas.py,12
vllm/transformers_utils/tokenizer_group/tokenizer_group.py,12
vllm/v1/engine/parallel_sampling.py,12
vllm/model_executor/layers/quantization/hqq_marlin.py,12
tests/tool_use/test_tool_choice_required.py,12
examples/offline_inference/rlhf.py,12
vllm/model_executor/models/seed_oss.py,12
cacheflow/models/sample.py,12
tests/compile/test_fusion_all_reduce.py,12
docs/source/generate_examples.py,12
vllm/benchmarks/latency.py,12
docs/source/deployment/frameworks/index.md,12
tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py,12
.github/workflows/scripts/build.sh,12
tests/quantization/utils.py,12
vllm/model_executor/layers/quantization/utils/int8_utils.py,12
tests/entrypoints/openai/test_shutdown.py,12
tests/spec_decode/e2e/test_logprobs.py,12
benchmarks/auto_tune/auto_tune.sh,12
.buildkite/scripts/upload-wheels.sh,12
docs/features/quantization/int4.md,12
vllm/attention/utils/fa_utils.py,12
vllm/entrypoints/logger.py,12
tests/models/multimodal/generation/test_whisper.py,12
tests/tool_use/test_qwen3coder_tool_parser.py,12
vllm/model_executor/model_loader/sharded_state_loader.py,12
vllm/model_executor/layers/mamba/short_conv.py,12
csrc/cpu/cache.cpp,12
vllm/model_executor/models/arcee.py,12
tests/entrypoints/openai/test_return_tokens_as_ids.py,12
tests/kernels/moe/utils.py,12
csrc/quantization/gguf/gguf_kernel.cu,12
.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh,12
docs/features/disagg_prefill.md,12
docs/features/README.md,12
vllm/triton_utils/__init__.py,12
tests/kernels/quantization/test_marlin_gemm.py,12
vllm/transformers_utils/configs/ultravox.py,12
vllm/v1/pool/metadata.py,12
tests/core/block/e2e/test_correctness_sliding_window.py,12
csrc/cpu/utils.cpp,12
requirements/cpu-build.txt,12
vllm/distributed/device_communicators/cpu_communicator.py,12
vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py,12
vllm/executor/distributed_gpu_executor.py,12
tests/kernels/moe/test_moe_permute_unpermute.py,12
tests/compile/piecewise/test_multiple_graphs.py,12
tests/models/multimodal/generation/test_pixtral.py,12
benchmarks/benchmark_prioritization.py,12
docs/configuration/conserving_memory.md,12
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py,12
vllm/model_executor/warmup/kernel_warmup.py,12
csrc/rocm/skinny_gemms.cu,12
vllm/connections.py,12
vllm/model_executor/models/aimv2.py,12
csrc/mamba/mamba_ssm/selective_scan_fwd.cu,12
vllm/attention/ops/vit_attn_wrappers.py,12
tests/models/language/pooling/test_gritlm.py,12
requirements/build.txt,12
.github/workflows/pre-commit.yml,12
.buildkite/run-neuron-test.sh,12
docs/source/community/sponsors.md,12
vllm/model_executor/models/minicpm_eagle.py,12
vllm/v1/attention/backends/mamba1_attn.py,12
vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py,12
tests/entrypoints/openai/test_translation_validation.py,12
vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py,12
vllm/v1/executor/uniproc_executor.py,12
tests/models/multimodal/generation/vlm_utils/core.py,12
docs/source/deployment/docker.md,12
vllm/v1/sample/logits_processor/__init__.py,12
tests/models/multimodal/generation/test_qwen2_vl.py,12
vllm/config/pooler.py,12
tests/models/multimodal/processing/test_h2ovl.py,12
vllm/beam_search.py,12
tests/models/test_fp8.py,12
vllm/v1/sample/tpu/sampler.py,12
tests/quantization/test_cpu_offload.py,12
cacheflow/master/server.py,12
.buildkite/run-benchmarks.sh,12
tests/samplers/test_seeded_generate.py,12
vllm/core/block/common.py,12
tests/models/multimodal/processing/test_phi3v.py,12
vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py,12
tests/tool_use/test_xlam_tool_parser.py,12
.github/PULL_REQUEST_TEMPLATE.md,12
vllm/compilation/activation_quant_fusion.py,12
docs/getting_started/installation/gpu/cuda.inc.md,12
vllm/config/observability.py,12
tests/v1/kv_connector/unit/test_offloading_connector.py,12
cacheflow/worker/controller.py,12
vllm/reasoning/mistral_reasoning_parser.py,12
vllm/v1/kv_offload/worker/cpu_gpu.py,12
tests/models/multimodal/processing/test_idefics3.py,12
vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py,12
vllm/model_executor/models/transformers/base.py,11
docs/features/quantization/fp8.md,11
vllm/tokenizers/registry.py,11
tests/distributed/test_shm_broadcast.py,11
vllm/transformers_utils/tokenizer_base.py,11
tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py,11
examples/online_serving/openai_transcription_client.py,11
tests/v1/engine/utils.py,11
vllm/v1/executor/ray_utils.py,11
vllm/transformers_utils/configs/mpt.py,11
.buildkite/run-tpu-test.sh,11
tests/v1/attention/test_attention_splitting.py,11
docs/contributing/ci/update_pytorch_version.md,11
vllm/attention/ops/triton_decode_attention.py,11
vllm/model_executor/layers/quantization/utils/marlin_utils_test.py,11
docs/design/io_processor_plugins.md,11
vllm/compilation/fx_utils.py,11
vllm/model_executor/models/openpangu.py,11
vllm/lora/punica_wrapper/punica_tpu.py,11
tests/mq_llm_engine/utils.py,11
tests/models/decoder_only/language/test_mistral.py,11
benchmarks/multi_turn/benchmark_serving_multi_turn.py,11
examples/online_serving/openai_chat_completion_structured_outputs.py,11
.buildkite/run-cpu-test-ppc64le.sh,11
vllm/compilation/noop_elimination.py,11
.buildkite/run-xpu-test.sh,11
docs/getting_started/installation/intel_gaudi.md,11
vllm/attention/backends/triton_mla.py,11
vllm/transformers_utils/chat_templates/registry.py,11
tests/kernels/test_blocksparse_attention.py,11
vllm/attention/backends/mla/utils.py,11
tests/spec_decode/test_batch_expansion.py,11
vllm/model_executor/layers/resampler.py,11
vllm/transformers_utils/detokenizer_utils.py,11
vllm/compilation/matcher_utils.py,11
vllm/distributed/eplb/rebalance_execute.py,11
vllm/config/lora.py,11
vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py,11
vllm/model_executor/layers/typical_acceptance_sampler.py,11
tests/lora/test_lora_huggingface.py,11
cacheflow/server/llm_server.py,11
csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu,11
tests/kernels/attention/test_rocm_attention_selector.py,11
tests/models/language/pooling/test_reward.py,11
tests/samplers/test_typical_acceptance_sampler.py,11
tests/models/decoder_only/vision_language/test_pixtral.py,11
vllm/v1/spec_decode/utils.py,11
tests/quantization/test_configs.py,11
tests/samplers/test_ignore_eos.py,11
vllm/model_executor/models/mimo.py,11
vllm/attention/ops/rocm_aiter_mla.py,11
vllm/v1/sample/ops/penalties.py,11
vllm/model_executor/layers/fused_moe/moe_pallas.py,11
tests/kernels/quantization/test_cutlass_scaled_mm.py,11
benchmarks/kernels/benchmark_moe_permute_unpermute.py,11
vllm/model_executor/model_loader/openvino.py,11
requirements-openvino.txt,11
docs/mkdocs/stylesheets/extra.css,11
vllm/reasoning/hunyuan_a13b_reasoning_parser.py,11
vllm/v1/worker/gpu/attn_utils.py,11
csrc/custom_all_reduce.cuh,11
tests/models/quantization/test_fp8.py,11
tests/models/language/pooling/embed_utils.py,11
vllm/model_executor/models/keye_vl1_5.py,11
tests/lora/test_phi.py,11
vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py,11
vllm/model_executor/model_loader/neuronx_distributed.py,11
vllm/model_executor/models/phi4mm_utils.py,11
csrc/mamba/causal_conv1d/causal_conv1d.cu,11
tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py,11
tests/async_engine/test_request_tracker.py,11
vllm/compilation/piecewise_backend.py,11
docs/api/README.md,11
vllm/attention/ops/nki_flash_attn.py,11
tests/tool_use/test_seed_oss_tool_parser.py,11
tests/models/decoder_only/language/test_mamba.py,11
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py,11
docs/design/cuda_graphs.md,11
tests/v1/cudagraph/test_cudagraph_dispatch.py,11
.github/workflows/clang-format.yml,11
tests/async_engine/api_server_async_engine.py,11
tests/models/multimodal/processing/test_llama4.py,11
tests/models/test_gptq_marlin.py,11
vllm/attention/layers/cross_attention.py,11
vllm/reasoning/qwen3_reasoning_parser.py,11
tests/tensorizer_loader/conftest.py,11
CONTRIBUTING.md,11
vllm/model_executor/layers/fused_moe/trtllm_moe.py,11
docs/design/fused_moe_modular_kernel.md,11
vllm/executor/tpu_executor.py,11
docs/features/quantization/quantized_kvcache.md,11
tests/entrypoints/llm/test_prompt_validation.py,11
vllm/model_executor/models/ernie_mtp.py,11
tests/entrypoints/openai/test_models.py,11
csrc/cpu/quant.cpp,11
vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py,11
tests/v1/spec_decode/test_mtp.py,11
examples/offline_inference/qwen2_5_omni/only_thinker.py,11
vllm/lora/layers/column_parallel_linear.py,11
.github/workflows/cleanup_pr_body.yml,11
tests/entrypoints/openai/test_rerank.py,11
vllm/transformers_utils/configs/nemotron.py,11
docs/cli/README.md,11
csrc/quantization/awq/gemm_kernels.cu,11
vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py,11
tests/entrypoints/openai/tool_parsers/utils.py,11
tests/v1/structured_output/test_utils.py,11
tests/test_logits_processor.py,11
vllm/attention/backends/dual_chunk_flash_attn.py,11
tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py,11
vllm/model_executor/models/mamba_cache.py,11
vllm/model_executor/models/transformers/multimodal.py,10
vllm/model_executor/layers/quantization/bitblas.py,10
vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py,10
tests/kernels/attention/test_triton_unified_attention.py,10
tests/entrypoints/openai/test_chat_echo.py,10
vllm/assets/audio.py,10
vllm/attention/ops/blocksparse_attention/interface.py,10
vllm/spec_decode/smaller_tp_proposer_worker.py,10
vllm/collect_env.py,10
docs/source/features/quantization/index.md,10
examples/phi3v_example.py,10
vllm/v1/spec_decode/medusa.py,10
vllm/logits_process.py,10
tests/v1/logits_processors/test_custom_offline.py,10
tests/v1/test_async_llm_dp.py,10
tests/models/test_big_models.py,10
vllm/compilation/caching.py,10
vllm/lora/punica_wrapper/punica_selector.py,10
csrc/cpu/cpu_attn_impl.hpp,10
vllm/model_executor/models/phi4flash.py,10
vllm/transformers_utils/tokenizers/__init__.py,10
tests/v1/e2e/test_kv_sharing_fast_prefill.py,10
cacheflow/models/memory_analyzer.py,10
tests/kernels/test_mamba_ssm.py,10
docs/source/features/spec_decode.md,10
tests/encoder_decoder/test_e2e_correctness.py,10
csrc/quantization/cutlass_w8a8/moe/moe_data.cu,10
tests/quantization/test_blackwell_moe.py,10
vllm/v1/kv_offload/cpu.py,10
tests/samplers/test_logits_processor.py,10
vllm/v1/metrics/ray_wrappers.py,10
tests/v1/core/test_specialized_manager.py,10
vllm/v1/attention/backends/linear_attn.py,10
vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py,10
vllm/model_executor/models/telechat2.py,10
vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py,10
csrc/quantization/fp4/nvfp4_quant_kernels.cu,10
tests/models/decoder_only/vision_language/vlm_utils/core.py,10
tests/spec_decode/e2e/test_integration.py,10
examples/offline_inference/mistral-small.py,10
vllm/transformers_utils/processors/__init__.py,10
docs/source/getting_started/installation/gpu/xpu.inc.md,10
vllm/worker/multi_step_worker.py,10
vllm/model_executor/model_loader/base_loader.py,10
examples/offline_inference_with_prefix.py,10
tests/async_engine/test_chat_template.py,10
cmake/external_projects/flashmla.cmake,10
tests/distributed/test_same_node.py,10
vllm/profiler/layerwise_profile.py,10
examples/online_serving/openai_chat_completion_with_reasoning_streaming.py,10
csrc/rocm/torch_bindings.cpp,10
.buildkite/test-template-aws.j2,10
benchmarks/kernels/benchmark_layernorm.py,10
csrc/moe/grouped_topk_kernels.cu,10
examples/offline_inference/save_sharded_state.py,10
tests/entrypoints/openai/correctness/test_lmeval.py,10
csrc/prepare_inputs/advance_step.cu,10
tests/kernels/test_fp8_quant.py,10
use_existing_torch.py,10
docs/source/features/tool_calling.md,10
vllm/transformers_utils/configs/jais.py,10
csrc/reduction_utils.cuh,10
vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py,10
docs/getting_started/installation/gpu.md,10
csrc/layernorm_quant_kernels.cu,10
tests/models/language/pooling/test_token_classification.py,10
docs/serving/distributed_serving.md,10
cacheflow/models/input_metadata.py,10
vllm/model_executor/layers/quantization/quark/utils.py,10
tests/v1/core/test_async_scheduler.py,10
vllm/v1/structured_output/backend_types.py,10
tests/kernels/test_causal_conv1d.py,10
cacheflow/models/llama.py,10
tests/plugins_tests/test_io_processor_plugins.py,10
benchmarks/kernels/benchmark_quant.py,10
vllm/model_executor/layers/mamba/mamba2_metadata.py,10
tests/tokenization/test_mistral_tokenizer.py,10
vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py,10
tests/entrypoints/llm/test_collective_rpc.py,10
tests/tokenization/test_tokenizer_group.py,10
requirements-xpu.txt,10
vllm/model_executor/layers/quantization/neuron_quant.py,10
vllm/compilation/cuda_graph.py,10
csrc/quantization/activation_kernels.cu,10
tests/kernels/moe/test_gpt_oss_triton_kernels.py,10
tests/models/multimodal/processing/test_mllama4.py,10
tests/models/decoder_only/vision_language/test_qwen2_vl.py,10
.buildkite/scripts/generate-nightly-index.py,10
vllm/model_executor/models/qwen2_cls.py,10
tests/spec_decode/test_ngram_worker.py,10
tests/v1/sample/test_topk_topp_sampler.py,10
docs/getting_started/installation/gpu/rocm.inc.md,10
vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py,10
tests/lora/test_lora_functions.py,10
vllm/model_executor/layers/quantization/deepgemm.py,10
tests/tool_use/test_parallel_tool_calls.py,10
benchmarks/benchmark_ngram_proposer.py,10
docs/source/models/lora.rst,10
vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py,10
vllm/lora/layers/vocal_parallel_embedding.py,10
vllm/spec_decode/medusa_worker.py,10
vllm/reasoning/granite_reasoning_parser.py,10
vllm/lora/layers/row_parallel_linear.py,10
tests/kernels/moe/test_moe_align_block_size.py,10
vllm/attention/backends/flashmla.py,10
tests/v1/sample/utils.py,10
benchmarks/kernels/benchmark_w8a8_block_fp8.py,10
tests/kernels/test_flex_attention.py,10
vllm/entrypoints/pooling/score/serving.py,10
tests/models/multimodal/generation/test_granite_speech.py,10
tools/profiler/print_layerwise_table.py,10
vllm/spec_decode/proposer_worker_base.py,10
vllm/executor/ray_xpu_executor.py,10
csrc/quantization/marlin/dense/marlin_cuda_kernel.cu,10
vllm/reasoning/glm4_moe_reasoning_parser.py,10
examples/offline_inference/tpu.py,10
tests/samplers/test_ranks.py,10
docs/design/arch_overview.md,10
tests/kernels/attention/test_prefix_prefill.py,10
tests/tool_use/test_kimi_k2_tool_parser.py,10
vllm/model_executor/layers/quantization/qqq.py,10
examples/llm_engine_example.py,10
tests/multimodal/test_video.py,10
vllm/executor/ray_tpu_executor.py,10
vllm/distributed/device_communicators/symm_mem.py,10
docs/features/compatibility_matrix.md,10
tests/spec_decode/e2e/test_integration_dist_tp4.py,9
tests/v1/executor/test_executor.py,9
.buildkite/run-gh200-test.sh,9
vllm/config/structured_outputs.py,9
tests/models/multimodal/pooling/test_prithvi_mae.py,9
tests/v1/e2e/test_cascade_attention.py,9
vllm/v1/core/sched/async_scheduler.py,9
tests/models/multimodal/generation/vlm_utils/custom_inputs.py,9
tests/lora/test_punica_variation.py,9
tests/models/language/pooling/test_snowflake_arctic_embed.py,9
tests/entrypoints/llm/test_generate_multiple_loras.py,9
tests/distributed/test_pp_cudagraph.py,9
tests/spec_decode/test_dynamic_spec_decode.py,9
vllm/entrypoints/cli/run_batch.py,9
benchmarks/kernels/graph_machete_bench.py,9
examples/llava_example.py,9
vllm/reasoning/olmo3_reasoning_parser.py,9
tests/models/multimodal/processing/test_glm4_1v.py,9
tests/distributed/test_expert_parallel.py,9
vllm/attention/backends/differential_flash_attn.py,9
vllm/lora/lora_weights.py,9
tests/kernels/moe/test_flashinfer_moe.py,9
csrc/quantization/fp4/nvfp4_experts_quant.cu,9
tests/models/embedding/vision_language/test_phi3v.py,9
tests/kernels/attention.py,9
docs/source/models/engine_args.rst,9
cacheflow/entrypoints/llm.py,9
vllm/model_executor/guided_decoding/utils.py,9
tests/models/multimodal/generation/test_phi4mm.py,9
examples/offline_inference/multilora_inference.py,9
tests/kernels/moe/test_mxfp4_moe.py,9
tests/multimodal/test_processor_kwargs.py,9
tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py,9
tests/models/decoder_only/vision_language/vlm_utils/types.py,9
csrc/cpu/cpu_types.hpp,9
tests/compile/test_wrapper.py,9
csrc/cutlass_extensions/vllm_cutlass_library_extension.py,9
tests/kernels/quantization/test_machete_mm.py,9
tests/kernels/quantization/test_rocm_skinny_gemms.py,9
examples/others/tensorize_vllm_model.py,9
tests/spec_decode/e2e/test_seed.py,9
vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py,9
vllm/entrypoints/openai/engine/serving.py,9
vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py,9
examples/offline_inference/profiling.py,9
tests/core/block/e2e/conftest.py,9
docs/source/models/spec_decode.rst,9
tests/spec_decode/test_utils.py,9
tests/compile/test_fusions_e2e.py,9
vllm/block.py,9
examples/offline_inference/profiling_tpu/profiling.py,9
csrc/quantization/squeezellm/quant_cuda_kernel.cu,9
vllm/attention/ops/triton_reshape_and_cache_flash.py,9
tests/v1/tpu/test_pallas.py,9
vllm/utils/gc_utils.py,9
tests/models/multimodal/pooling/test_jinavl_reranker.py,9
tests/models/decoder_only/language/test_gguf.py,9
examples/online_serving/openai_chat_completion_with_reasoning.py,9
docs/deployment/frameworks/haystack.md,9
docs/configuration/tpu.md,9
vllm/transformers_utils/processors/ovis.py,9
vllm/model_executor/layers/lightning_attn.py,9
tests/weight_loading/test_weight_loading.py,9
examples/offline_inference/mlpspeculator.py,9
tests/spec_decode/test_scorer.py,9
vllm/model_executor/parallel_utils/pynccl.py,9
.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh,9
examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py,9
vllm/model_executor/layers/mamba/abstract.py,9
vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py,9
vllm/model_executor/layers/quantization/awq_triton.py,9
docs/assets/contributing/dockerfile-stages-dependency.png,9
csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu,9
vllm/lora/layers/logits_processor.py,9
vllm/engine/output_processor/util.py,9
vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py,9
docs/source/getting_started/troubleshooting.md,9
.readthedocs.yaml,9
vllm/config/kv_transfer.py,9
csrc/moe/marlin_moe_wna16/generate_kernels.py,9
docs/contributing/model/transcription.md,9
vllm/model_executor/models/isaac.py,9
docs/getting_started/installation/gpu.cuda.inc.md,9
csrc/attention/dtype_bfloat16.cuh,9
tests/lora/test_tokenizer_group.py,9
tests/v1/core/test_single_type_kv_cache_manager.py,9
vllm/model_executor/layers/quantization/utils/gptq_utils.py,9
requirements-neuron.txt,9
vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py,9
tests/models/embedding/utils.py,9
tests/models/language/pooling/test_qwen3_reranker.py,9
docs/source/getting_started/installation/gpu/cuda.inc.md,9
vllm/model_executor/models/deepseek_v3.py,9
vllm/model_executor/models/phi_1_5.py,9
examples/offline_inference/basic/chat.py,9
tests/compile/test_decorator.py,9
csrc/moe/marlin_moe_wna16/ops.cu,9
tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py,9
csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp,9
tests/v1/e2e/test_correctness_sliding_window.py,9
tests/models/multimodal/test_mapping.py,9
docs/features/quantization/bitblas.md,9
tests/models/test_vision.py,9
docs/contributing/model/README.md,9
vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py,9
tests/models/test_mistral.py,9
vllm/entrypoints/tool_server.py,9
tests/v1/tpu/test_multimodal.py,9
tools/report_build_time_ninja.py,9
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py,9
docs/contributing/model/registration.md,9
tests/distributed/test_chunked_prefill_distributed.py,9
tests/core/block/test_block_table.py,9
vllm/tokenizers/mistral.py,9
vllm/reasoning/step3_reasoning_parser.py,9
vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py,9
examples/openai_vision_api_client.py,9
tests/v1/entrypoints/conftest.py,9
vllm/reasoning/deepseek_v3_reasoning_parser.py,9
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py,9
.buildkite/generate_index.py,9
csrc/quantization/gptq_marlin/gptq_marlin_repack.cu,9
csrc/moe/moe_permute_unpermute_op.cu,9
benchmark/benchmark_latency.py,9
tests/spec_decode/test_metrics.py,9
docs/design/torch_compile.md,9
vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py,9
vllm/utils/tensor_schema.py,9
docs/deployment/integrations/production-stack.md,9
tests/kernels/moe/parallel_utils.py,9
vllm/model_executor/layers/fused_moe/shared_fused_moe.py,9
vllm/config/load.py,9
vllm/lora/punica_wrapper/utils.py,9
tests/kv_transfer/test_send_recv.py,9
tests/kernels/attention/test_flashmla.py,9
tests/kernels/attention/test_flash_attn.py,9
vllm/distributed/device_communicators/shm_object_storage.py,9
vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py,8
vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py,8
docs/getting_started/installation/cpu/arm.inc.md,8
csrc/quantization/fp8/common.cuh,8
docs/features/quantization/supported_hardware.md,8
tests/engine/test_skip_tokenizer_init.py,8
tests/tool_use/test_chat_completions.py,8
docs/source/serving/compatibility_matrix.rst,8
tests/quantization/test_experts_int8.py,8
vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py,8
tests/v1/determinism/test_batch_invariance.py,8
tests/tool_use/test_tool_calls.py,8
tests/v1/kv_connector/nixl_integration/toy_proxy_server.py,8
docs/features/quantization/auto_awq.md,8
vllm/utils/argparse_utils.py,8
tests/tokenization/test_cached_tokenizer.py,8
vllm/v1/worker/ubatch_utils.py,8
.github/workflows/stale.yml,8
tests/lora/test_peft_helper.py,8
vllm/spec_decode/target_model_runner.py,8
tests/tool_use/test_minimax_tool_parser.py,8
vllm/transformers_utils/__init__.py,8
docs/source/contributing/profiling/profiling_index.md,8
tests/entrypoints/openai/test_root_path.py,8
vllm/distributed/kv_transfer/kv_transfer_state.py,8
tests/models/test_internvl.py,8
vllm/entrypoints/pooling/pooling/serving.py,8
tests/tpu/test_quantization_accuracy.py,8
tests/kv_transfer/test_lookup_buffer.py,8
tests/entrypoints/openai/test_classification.py,8
tests/kernels/mamba/test_mamba_ssm.py,8
vllm/distributed/device_communicators/all_reduce_utils.py,8
vllm/spec_decode/mlp_speculator_worker.py,8
vllm/reasoning/deepseek_r1_reasoning_parser.py,8
vllm/model_executor/models/transformers/moe.py,8
tests/lora/test_punica_ops.py,8
tests/v1/engine/conftest.py,8
benchmarks/cutlass_benchmarks/sparse_benchmarks.py,8
docs/source/features/structured_outputs.md,8
tests/v1/tpu/test_topk_topp_sampler.py,8
vllm/model_executor/models/opencua.py,8
tests/lora/test_llama.py,8
vllm/transformers_utils/runai_utils.py,8
vllm/reasoning/minimax_m2_reasoning_parser.py,8
docs/source/features/quantization/fp8.md,8
vllm/model_executor/guided_decoding/guided_fields.py,8
tests/evals/gsm8k/test_gsm8k_correctness.py,8
vllm/entrypoints/openai/parser/responses_parser.py,8
tests/kernels/core/test_fused_quant_layernorm.py,8
tests/models/embedding/language/test_cls_models.py,8
tests/entrypoints/test_responses_utils.py,8
tests/core/block/test_naive_block.py,8
examples/offline_inference/lora_with_quantization_inference.py,8
csrc/attention/paged_attention_v1.cu,8
docs/getting_started/installation/cpu/x86.inc.md,8
examples/offline_inference/structured_outputs.py,8
tests/v1/kv_connector/unit/test_shared_storage_connector.py,8
vllm/model_executor/models/kimi_linear.py,8
vllm/transformers_utils/configs/mlp_speculator.py,8
docs/source/getting_started/installation/cpu.md,8
.buildkite/run-hpu-test.sh,8
vllm/assets/base.py,8
tests/models/multimodal/generation/vlm_utils/builders.py,8
tests/models/quantization/test_gguf.py,8
tests/kernels/test_block_fp8.py,8
docs/design/multiprocessing.md,8
vllm/model_executor/models/glmasr.py,8
benchmarks/kernels/benchmark_rmsnorm.py,8
tests/tpu/test_moe_pallas.py,8
csrc/quantization/machete/machete_mainloop.cuh,8
csrc/quantization/gptq_marlin/generate_kernels.py,8
docs/design/prefix_caching.md,8
tests/engine/test_multiproc_workers.py,8
docs/source/features/quantization/supported_hardware.md,8
examples/online_serving/pooling/README.md,8
tests/entrypoints/openai/test_oot_registration.py,8
tests/models/language/pooling/test_nomic.py,8
benchmarks/kernels/benchmark_aqlm.py,8
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py,8
benchmarks/benchmark_utils.py,8
tests/lora/test_transfomers_model.py,8
tests/v1/tpu/test_perf.py,8
examples/offline_inference/vision_language_embedding.py,8
tests/reasoning/utils.py,8
MANIFEST.in,8
csrc/core/scalar_type.hpp,8
csrc/attention_kernels.cu,8
vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py,8
vllm/model_executor/models/interns1_vit.py,8
tests/lora/test_fused_moe_lora_kernel.py,8
python_only_dev.py,8
tests/kernels/core/test_activation.py,8
tests/compile/test_aot_compile.py,8
csrc/sampler.cu,8
vllm/lora/ops/sgmv_expand.py,8
vllm/transformers_utils/processors/deepseek_vl2.py,8
tests/kernels/core/test_rotary_embedding.py,8
vllm/model_executor/models/phi3.py,8
tests/plugins_tests/test_scheduler_plugins.py,8
docs/source/features/quantization/auto_awq.md,8
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py,8
.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh,8
vllm/entrypoints/pooling/embed/protocol.py,8
tests/kernels/mamba/test_causal_conv1d.py,8
vllm/lora/layers/base.py,8
tests/kernels/test_gguf.py,8
vllm/model_executor/layers/fused_moe/fused_moe_method_base.py,8
examples/offline_inference/basic/classify.py,8
tests/v1/logits_processors/utils.py,8
tests/distributed/test_eplb_execute.py,8
examples/offline_inference/vision_language_pooling.py,8
tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py,8
tests/kernels/quantization/test_fp8_quant_group.py,8
csrc/quantization/fp8/amd/quant_utils.cuh,8
vllm/entrypoints/openai/tool_parsers/utils.py,8
csrc/quantization/marlin/sparse/common/mma.h,8
tests/distributed/test_multimodal_broadcast.py,8
tests/tokenization/test_tokenizer_registry.py,8
vllm/v1/sample/logits_processor/interface.py,8
.github/ISSUE_TEMPLATE/400-bug-report.yml,8
tests/distributed/test_torchrun_example.py,8
vllm/model_executor/layers/mamba/ops/ssd_state_passing.py,8
examples/offline_inference/llm_engine_example.py,8
vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py,8
tests/v1/kv_offload/test_cpu_gpu.py,8
docker/Dockerfile.ppc64le,8
tests/tpu/lora/test_lora.py,8
docs/deployment/frameworks/autogen.md,8
csrc/cuda_utils.h,8
vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py,8
cacheflow/model_executor/layers/sampler.py,8
examples/offline_inference/encoder_decoder.py,8
tests/models/language/pooling_mteb_test/mteb_utils.py,8
tests/kernels/core/test_layernorm.py,8
docs/source/quantization/supported_hardware.rst,8
examples/online_serving/kv_events_subscriber.py,8
tests/models/quantization/test_bitsandbytes.py,8
server.py,8
vllm/attention/ops/triton_merge_attn_states.py,8
tests/kernels/moe/test_grouped_topk.py,8
csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu,8
vllm/utils/import_utils.py,8
tests/entrypoints/openai/correctness/test_transcription_api_correctness.py,8
docs/source/features/compatibility_matrix.md,8
tests/models/decoder_only/language/test_jamba.py,8
tests/multimodal/test_mapper.py,8
vllm/v1/structured_output/backend_outlines.py,8
tests/entrypoints/openai/test_chunked_prompt.py,8
vllm/core/placeholder_block_space_manager.py,8
tests/v1/metrics/test_ray_metrics.py,8
tests/quantization/test_gptq_dynamic.py,8
vllm/model_executor/models/ouro.py,8
vllm/model_executor/models/glm4_vision_encoder.py,8
vllm/distributed/kv_transfer/kv_lookup_buffer/base.py,8
vllm/model_executor/models/afmoe.py,8
csrc/attention/paged_attention_v2.cu,8
SECURITY.md,8
tests/entrypoints/openai/test_pooling.py,8
docs/source/quantization/fp8.rst,8
vllm/lora/ops/sgmv_expand_slice.py,8
docs/source/serving/integrations.rst,8
docs/deployment/frameworks/skypilot.md,8
docs/deployment/nginx.md,8
vllm/distributed/device_communicators/ray_communicator.py,8
vllm/attention/backends/openvino.py,8
vllm/v1/worker/gpu/sample/sampler.py,8
tests/kernels/attention/test_flashinfer.py,8
tests/compile/test_qk_norm_rope_fusion.py,7
tests/engine/test_executor.py,7
vllm/v1/kv_offload/spec.py,7
csrc/cuda_compat.h,7
vllm/distributed/kv_transfer/kv_connector/base.py,7
.github/workflows/add_label_automerge.yml,7
vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py,7
.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh,7
docs/deployment/frameworks/litellm.md,7
docs/source/getting_started/installation/cpu/x86.inc.md,7
examples/offline_inference_audio_language.py,7
tests/v1/tpu/test_mha_attn.py,7
tests/test_scalartype.py,7
tests/entrypoints/openai/test_async_tokenization.py,7
vllm/transformers_utils/configs/nemotron_h.py,7
examples/offline_inference/disaggregated_prefill.py,7
tests/v1/engine/test_processor_multi_modal_uuids.py,7
docs/source/community/meetups.rst,7
vllm/model_executor/models/module_mapping.py,7
vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py,7
vllm/transformers_utils/configs/chatglm.py,7
tests/tool_use/test_ernie45_moe_tool_parser.py,7
requirements-lint.txt,7
.github/ISSUE_TEMPLATE/400-bug report.yml,7
docs/getting_started/installation/cpu/build.inc.md,7
vllm/model_executor/layers/quantization/utils/machete_utils.py,7
vllm/transformers_utils/configs/nvlm_d.py,7
vllm/model_executor/layers/quantization/utils/bitblas_utils.py,7
vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py,7
docs/source/features/lora.md,7
tests/kernels/attention/test_aiter_flash_attn.py,7
vllm/lora/punica_wrapper/punica_cpu.py,7
tests/v1/determinism/utils.py,7
docs/source/design/v1/metrics.md,7
benchmarks/kernels/benchmark_reshape_and_cache_flash.py,7
examples/online_serving/prithvi_geospatial_mae.py,7
vllm/distributed/kv_transfer/__init__.py,7
tests/models/language/pooling/test_mm_classifier_conversion.py,7
csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu,7
examples/online_serving/ray_serve_deepseek.py,7
.github/workflows/macos-smoke-test.yml,7
vllm/attention/ops/rocm_aiter_paged_attn.py,7
docs/deployment/frameworks/dstack.md,7
tests/engine/test_computed_prefix_blocks.py,7
docs/features/quantization/gguf.md,7
vllm/model_executor/models/smolvlm.py,7
vllm/v1/worker/gpu/sample/penalties.py,7
tests/worker/test_profile.py,7
tests/models/decoder_only/language/test_fp8.py,7
tests/entrypoints/test_renderer.py,7
vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py,7
examples/online_serving/api_client.py,7
tests/detokenizer/test_stop_strings.py,7
tests/core/test_num_computed_tokens_update.py,7
docs/source/getting_started/v1_user_guide.md,7
vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py,7
cacheflow/utils.py,7
tests/entrypoints/openai/test_chat_with_tool_reasoning.py,7
docs/models/hardware_supported_models/tpu.md,7
docs/contributing/model/tests.md,7
vllm/lora/ops/sgmv_shrink.py,7
tests/v1/test_utils.py,7
docs/usage/metrics.md,7
vllm/model_executor/layers/fused_moe/deep_gemm_utils.py,7
docs/getting_started/installation/google_tpu.md,7
csrc/custom_all_reduce_test.cu,7
examples/offline_inference/prithvi_geospatial_mae_io_processor.py,7
vllm/v1/attention/backend.py,7
tests/models/multimodal/pooling/test_radio.py,7
vllm/transformers_utils/configs/arctic.py,7
tests/entrypoints/test_guided_processors.py,7
tests/core/block/test_block_manager_v2.py,7
tests/multimodal/test_image.py,7
vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py,7
tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py,7
vllm/executor/ray_hpu_executor.py,7
.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py,7
tests/benchmarks/test_serve_cli.py,7
docs/source/design/v1/prefix_caching.md,7
vllm/prompt_adapter/utils.py,7
vllm/model_executor/model_loader/tpu.py,7
tests/entrypoints/openai/test_embedding_dimensions.py,7
vllm/tokenizers/__init__.py,7
docs/source/dev/offline_inference/llm_inputs.rst,7
csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh,7
benchmarks/kernels/benchmark_trtllm_decode_attention.py,7
vllm/entrypoints/cli/collect_env.py,7
docs/source/design/arch_overview.md,7
vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py,7
docs/source/community/meetups.md,7
tests/kernels/core/test_mrope.py,7
tests/kernels/moe/test_cutlass_grouped_gemm.py,7
benchmarks/launch_tgi_server.sh,7
tests/weight_loading/models-large.txt,7
benchmarks/benchmark_long_document_qa_throughput.py,7
csrc/quantization/machete/machete_pytorch.cu,7
vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py,7
vllm/distributed/kv_transfer/kv_connector/v1/__init__.py,7
vllm/entrypoints/tool.py,7
tests/kernels/quantization/test_triton_scaled_mm.py,7
vllm/transformers_utils/tokenizer_group.py,7
examples/offline_inference/torchrun_example.py,7
vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py,7
docs/getting_started/installation/gpu/xpu.inc.md,7
tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py,7
tests/entrypoints/openai/test_sleep.py,7
tests/mq_llm_engine/test_load.py,7
vllm/model_executor/models/glm.py,7
tests/models/multimodal/generation/test_voxtral.py,7
vllm/model_executor/models/transformers_pooling.py,7
vllm/model_executor/models/bagel.py,7
docs/configuration/engine_args.md,7
docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md,7
csrc/cutlass_extensions/common.hpp,7
examples/offline_inference/rlhf_utils.py,7
tests/v1/entrypoints/openai/test_multi_api_servers.py,7
tests/models/multimodal/test_tensor_schema.py,7
vllm/model_executor/guided_decoding/guidance_decoding.py,7
tools/check_pickle_imports.py,7
vllm/transformers_utils/configs/dbrx.py,7
vllm/transformers_utils/configs/exaone.py,7
docs/source/contributing/model/basic.md,7
vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py,7
tests/kernels/test_machete_gemm.py,7
csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu,7
vllm/lora/layers/replicated_linear.py,7
vllm/attention/ops/blocksparse_attention/utils.py,7
tests/test_envs.py,7
vllm/model_executor/models/longcat_flash_mtp.py,7
tests/multimodal/test_hasher.py,7
tests/models/multimodal/processing/test_phi4mm.py,7
benchmarks/kernels/benchmark_trtllm_prefill_attention.py,7
docs/source/getting_started/neuron-installation.rst,7
docs/mkdocs/hooks/url_schemes.py,7
vllm/executor/mp_distributed_executor.py,7
docs/deployment/frameworks/cerebrium.md,7
csrc/custom_all_reduce.cu,7
tests/models/multimodal/processing/test_smolvlm.py,7
tests/test_sampling_params.py,7
tests/models/multimodal/processing/test_nemotron_vl.py,7
tests/models/multimodal/processing/test_minimax_vl_01.py,7
vllm/v1/worker/ubatching.py,7
tests/models/multimodal/pooling/test_intern_vit.py,7
vllm/model_executor/layers/fla/ops/fused_recurrent.py,7
tests/lora/test_gptoss_tp.py,7
tests/kernels/test_onednn.py,7
docs/getting_started/installation/cpu.arm.inc.md,7
vllm/lora/punica_wrapper/punica_hpu.py,7
.buildkite/run-openvino-test.sh,7
vllm/lora/layers/utils.py,7
docs/source/contributing/dockerfile/dockerfile.md,7
vllm/lora/ops/triton_ops/kernel_utils.py,7
vllm/worker/cpu_pooling_model_runner.py,7
tests/vllm_test_utils/vllm_test_utils/blame.py,7
vllm/model_executor/models/radio.py,7
.buildkite/nightly-benchmarks/nightly-descriptions.md,7
docs/serving/integrations/langchain.md,7
docs/getting_started/installation/gpu.rocm.inc.md,7
benchmarks/fused_kernels/layernorm_rms_benchmarks.py,7
docs/models/extensions/tensorizer.md,7
tests/models/test_aqlm.py,7
csrc/attention.cpp,7
docs/models/extensions/runai_model_streamer.md,7
csrc/quantization/fp4/nvfp4_quant_entry.cu,7
tests/models/language/pooling/test_scoring.py,7
tests/core/test_scheduler_encoder_decoder.py,7
vllm/lora/ops/triton_ops/__init__.py,7
tests/models/multimodal/generation/vlm_utils/case_filtering.py,7
vllm/transformers_utils/configs/deepseek_vl2.py,7
vllm/attention/layers/encoder_only_attention.py,7
docker/Dockerfile.s390x,7
vllm/plugins/io_processors/__init__.py,7
tests/v1/core/test_scheduler_e2e.py,7
vllm/v1/spec_decode/suffix_decoding.py,7
docs/usage/security.md,7
vllm/executor/msgspec_utils.py,7
tests/prefix_caching/test_disable_sliding_window.py,7
examples/online_serving/openai_chat_embedding_client_for_multimodal.py,7
tests/lora/test_punica_sizes.py,7
tests/tool_use/test_glm4_moe_tool_parser.py,7
tests/models/language/pooling/test_classification.py,7
vllm/v1/worker/gpu/sampler.py,7
vllm/utils/system_utils.py,7
tools/install_nixl_from_source_ubuntu.py,7
csrc/quantization/gptq_marlin/marlin_template.h,7
tests/reasoning/test_mistral_reasoning_parser.py,7
benchmarks/overheads/benchmark_hashing.py,7
csrc/attention/attention_utils.cuh,7
tests/v1/sample/test_logprobs_e2e.py,7
tests/tokenization/test_get_eos.py,7
vllm/spec_decode/mqa_scorer.py,7
vllm/attention/ops/hpu_paged_attn.py,7
tests/quantization/test_rtn.py,7
vllm/adapter_commons/models.py,7
vllm/v1/worker/xpu_model_runner.py,7
tests/utils_/test_argparse_utils.py,7
tests/models/test_paligemma.py,7
csrc/cpu/dnnl_helper.cpp,7
vllm/distributed/eplb/rebalance_algo.py,7
tests/entrypoints/pooling/embed/test_online.py,7
vllm/attention/layers/mm_encoder_attention.py,7
docs/deployment/frameworks/retrieval_augmented_generation.md,7
vllm/attention/backends/cpu_mla.py,7
csrc/moe/marlin_moe_wna16/kernel.h,7
benchmarks/auto_tune/README.md,7
tests/models/language/pooling_mteb_test/test_jina.py,7
vllm/model_executor/pooling_metadata.py,7
tests/models/language/generation/test_gemma.py,7
vllm/model_executor/models/transformers/utils.py,7
csrc/cpu/pos_encoding.cpp,7
tests/speculative_decoding/speculators/test_eagle3.py,7
tests/models/embedding/language/test_gritlm.py,7
vllm/utils/mem_utils.py,7
vllm/model_executor/layers/fla/ops/utils.py,7
cacheflow/__init__.py,7
vllm/lora/ops/triton_ops/lora_kernel_metadata.py,7
docs/features/quantization/gptqmodel.md,7
docs/features/quantization/bnb.md,7
examples/production_monitoring/grafana.json,6
vllm/model_executor/layers/quantization/schema.py,6
tests/quantization/test_ptpc_fp8.py,6
tests/kernels/moe/test_triton_moe_ptpc_fp8.py,6
tools/install_deepgemm.sh,6
docs/source/getting_started/installation/cpu/build.inc.md,6
vllm/entrypoints/cli/__init__.py,6
tests/kernels/moe/test_block_int8.py,6
docs/design/huggingface_integration.md,6
csrc/quantization/gguf/mmvq.cuh,6
tests/distributed/test_pipeline_partition.py,6
vllm/logging_utils/dump_input.py,6
examples/online_serving/openai_chat_completion_client_with_tools.py,6
cacheflow/model_executor/models/gpt2.py,6
tests/kernels/quantization/test_cutlass_w4a8.py,6
vllm/lora/ops/bgmv_expand_slice.py,6
docs/deployment/frameworks/lws.md,6
tests/test_routing_simulator.py,6
examples/online_serving/gradio_openai_chatbot_webserver.py,6
examples/offline_inference/disaggregated-prefill-v1/prefill_example.py,6
tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py,6
vllm/distributed/kv_transfer/kv_connector/v1/metrics.py,6
docs/deployment/frameworks/streamlit.md,6
vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py,6
.buildkite/lm-eval-harness/configs/models-large.txt,6
csrc/cpu/cpu_types_arm.hpp,6
docs/deployment/frameworks/dify.md,6
cacheflow/server/arg_utils.py,6
docs/features/quantization/torchao.md,6
tests/distributed/test_multi_node_assignment.py,6
examples/simple_server.py,6
vllm/core/block/utils.py,6
tests/entrypoints/pooling/pooling/test_online.py,6
tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py,6
tests/models/test_terratorch.py,6
vllm/entrypoints/pooling/embed/serving.py,6
vllm/model_executor/layers/quantization/utils/layer_utils.py,6
examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py,6
tests/engine/output_processor/test_stop_checker.py,6
benchmarks/kernels/utils.py,6
.buildkite/nightly-benchmarks/scripts/wait-for-image.sh,6
tests/model_executor/model_loader/tensorizer_loader/conftest.py,6
vllm/adapter_commons/worker_manager.py,6
tests/tool_use/test_openai_tool_parser.py,6
tests/kernels/quantization/nvfp4_utils.py,6
tests/models/language/generation_ppl_test/ppl_utils.py,6
vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py,6
tests/kernels/test_sampler.py,6
vllm/lora/ops/bgmv_expand.py,6
tests/kernels/attention/test_triton_decode_attention.py,6
csrc/cache.cpp,6
tests/distributed/test_events.py,6
vllm/plugins/io_processors/interface.py,6
requirements/kv_connectors.txt,6
tests/models/decoder_only/vision_language/test_llava_onevision.py,6
examples/offline_inference_neuron.py,6
tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py,6
vllm/distributed/device_communicators/pynccl_allocator.py,6
vllm/model_executor/parallel_utils/custom_all_reduce.py,6
tests/v1/e2e/test_context_length.py,6
tests/kernels/quantization/test_gguf.py,6
tests/kernels/test_awq_marlin.py,6
tests/v1/attention/test_rocm_attention_backends_selection.py,6
vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py,6
examples/online_serving/disaggregated_serving/disagg_proxy_demo.py,6
tests/v1/logits_processors/test_custom_online.py,6
tests/models/multimodal/generation/test_interleaved.py,6
vllm/model_executor/models/nemotron_parse.py,6
examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py,6
tests/engine/test_custom_executor.py,6
vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py,6
vllm/lora/layers/__init__.py,6
docs/design/kernel/paged_attention.md,6
vllm/utils/jsontree.py,6
csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu,6
vllm/adapter_commons/utils.py,6
benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh,6
vllm/model_executor/layers/kda.py,6
tests/kernels/mamba/test_mamba_mixer2.py,6
csrc/attention/mla/sm100_cutlass_mla_kernel.cu,6
tests/engine/test_stop_strings.py,6
tests/models/language/pooling/test_baai.py,6
tests/entrypoints/test_context.py,6
tests/quantization/test_ipex_quant.py,6
tests/quantization/test_modelopt.py,6
vllm/compilation/reshapes.py,6
tests/vllm_test_utils/vllm_test_utils/monitor.py,6
tests/entrypoints/test_api_server_process_manager.py,6
vllm/model_executor/models/teleflm.py,6
benchmarks/kernels/benchmark_cutlass_fp4_moe.py,6
tests/standalone_tests/python_only_compile.sh,6
vllm/benchmarks/sweep/serve_sla.py,6
benchmarks/kernels/bench_per_token_quant_fp8.py,6
vllm/engine/async_timeout.py,6
vllm/v1/attention/backends/rocm_aiter_unified_attn.py,6
tests/kernels/quantization/test_block_int8.py,6
tests/models/language/pooling/test_nomic_max_model_len.py,6
tests/models/multimodal/pooling/test_dse_qwen2_vl.py,6
tests/v1/distributed/test_async_llm_dp.py,6
vllm/v1/sample/logits_processor/builtin.py,6
vllm/v1/sample/ops/bad_words.py,6
vllm/entrypoints/pooling/pooling/protocol.py,6
docs/design/v1/p2p_nccl_connector.md,6
.github/workflows/actionlint.yml,6
docs/source/deployment/k8s.md,6
tests/distributed/conftest.py,6
docs/features/quantization/modelopt.md,6
vllm/profiler/utils.py,6
vllm/prompt_adapter/models.py,6
vllm/model_executor/model_loader/dummy_loader.py,6
vllm/entrypoints/cli/benchmark/base.py,6
tests/quantization/test_auto_round.py,6
examples/offline_inference.py,6
tests/v1/shutdown/test_forward_error.py,6
docs/deployment/frameworks/chatbox.md,6
docs/source/serving/metrics.md,6
tests/weight_loading/run_model_weight_loading_test.sh,6
tests/entrypoints/openai/test_default_mm_loras.py,6
tests/lora/test_minicpmv.py,6
tests/entrypoints/openai/test_chat_error.py,6
docs/deployment/frameworks/helm.md,6
tests/kernels/moe/test_modular_oai_triton_moe.py,6
benchmarks/kernels/benchmark_mixtral_moe.py,6
benchmarks/kernels/benchmark_cutlass_moe_fp8.py,6
tests/kernels/attention/test_cascade_flash_attn.py,6
tests/core/block/test_cpu_gpu_block_allocator.py,6
docs/source/features/quantization/int8.md,6
tests/v1/engine/test_process_multi_modal_uuids.py,6
tests/models/quantization/test_awq.py,6
.github/workflows/issue_autolabel.yml,6
vllm/model_executor/layers/attention_layer_base.py,6
docs/source/design/v1/torch_compile.md,6
vllm/model_executor/layers/fused_moe/moe_torch_iterative.py,6
tests/v1/core/test_encoder_cache_manager.py,6
vllm/v1/metrics/prometheus.py,6
vllm/entrypoints/cli/types.py,6
docs/getting_started/installation/cpu/s390x.inc.md,6
vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py,6
vllm/logging_utils/formatter.py,6
csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu,6
vllm/compilation/multi_output_match.py,6
vllm/logging_utils/__init__.py,6
benchmarks/kernels/benchmark_activation.py,6
docs/features/quantization/inc.md,6
examples/online_serving/openai_chat_completion_client_with_tools_required.py,6
csrc/moe_align_block_size_kernels.cu,6
vllm/compilation/partition_rules.py,6
examples/offline_inference/rlhf_colocate.py,6
tests/neuron/1_core/test_prefix_prefill.py,6
tests/kernels/conftest.py,6
vllm/core/embedding_model_block_manager.py,6
vllm/lora/punica_wrapper/punica_xpu.py,6
tests/models/language/pooling/test_truncation_control.py,6
tests/models/embedding/language/test_scoring.py,6
csrc/moe/marlin_moe_ops.h,6
csrc/quantization/gptq_marlin/awq_marlin_repack.cu,6
cacheflow/model_executor/models/opt.py,6
.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py,6
docs/serving/offline_inference.md,6
examples/offline_inference/neuron_speculation.py,6
.github/ISSUE_TEMPLATE/750-RFC.yml,6
examples/offline_inference/pooling/README.md,6
cacheflow/http_frontend/fastapi_frontend.py,6
docs/deployment/frameworks/anything-llm.md,6
cacheflow/config.py,6
tests/entrypoints/openai/test_skip_tokenizer.py,6
csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu,6
benchmarks/cutlass_benchmarks/weight_shapes.py,6
docs/source/getting_started/openvino-installation.rst,6
vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py,6
docs/serving/data_parallel_deployment.md,6
vllm/model_executor/models/lfm2_vl.py,6
cacheflow/core/server.py,6
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py,6
vllm/transformers_utils/image_processor.py,6
vllm/attention/ops/pallas_kv_cache_update.py,6
vllm/model_executor/layers/ops/sample.py,6
tests/kernels/quantization/test_nvfp4_quant.py,6
docs/contributing/incremental_build.md,6
examples/offline_inference/neuron_eagle.py,6
cacheflow/model_executor/models/gpt_neox.py,6
examples/tensorize_vllm_model.py,6
vllm/model_executor/layers/attention/mla_attention.py,6
tests/models/test_gptq_marlin_24.py,6
csrc/quantization/aqlm/gemm_kernels.cu,6
tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py,6
vllm/model_executor/models/deepencoder.py,6
tests/v1/shutdown/test_startup_error.py,6
docs/source/getting_started/installation/ai_accelerator/tpu.inc.md,6
tests/models/test_minicpmv.py,6
csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu,6
tests/v1/kv_connector/nixl_integration/test_accuracy.py,6
tests/entrypoints/openai/test_encoder_decoder.py,6
vllm/tokenizers/deepseekv32.py,6
tests/entrypoints/pooling/openai/test_embedding.py,6
tests/kernels/test_cutlass_mla_decode.py,6
docs/design/custom_op.md,6
csrc/quantization/fused_kernels/layernorm_utils.cuh,6
tests/mq_llm_engine/test_abort.py,6
tests/entrypoints/openai/test_accuracy.py,6
tests/v1/engine/test_fast_incdec_prefix_err.py,6
docs/getting_started/installation/cpu/apple.inc.md,6
docs/design/debug_vllm_compile.md,6
benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh,6
cacheflow/frontend/fastapi_frontend.py,6
tests/compile/test_noop_elimination.py,6
tests/test_pooling_params.py,6
vllm/model_executor/layers/mamba/ops/ssd_bmm.py,6
csrc/attention/dtype_float32.cuh,6
tests/tool_use/test_chat_completion_request_validations.py,6
examples/offline_inference/embed_matryoshka_fy.py,6
vllm/logprobs.py,6
vllm/v1/worker/gpu/block_table.py,6
vllm/transformers_utils/configs/solar.py,6
tests/models/multimodal/pooling/test_siglip.py,6
.buildkite/nightly-benchmarks/run-benchmarks-suite.sh,6
tests/reasoning/test_base_thinking_reasoning_parser.py,6
benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py,6
vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py,6
docs/source/getting_started/installation/python_env_setup.inc.md,6
vllm/v1/attention/backends/mamba_selectors.py,6
cacheflow/block.py,6
docs/source/design/multiprocessing.md,6
cmake/hipify.py,6
.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh,6
csrc/cuda_utils_kernels.cu,6
vllm/distributed/kv_transfer/kv_pipe/base.py,6
tests/engine/test_short_mm_context.py,6
tests/multimodal/utils.py,6
tests/tool_use/conftest.py,6
tests/entrypoints/openai/test_truncation.py,6
vllm/lora/ops/xla_ops/lora_ops.py,6
csrc/attention/dtype_float16.cuh,6
docs/source/_static/custom.js,6
tests/models/multimodal/generation/test_maverick.py,6
docs/source/serving/run_on_sky.rst,6
csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu,6
vllm/entrypoints/pooling/score/protocol.py,6
tests/neuron/2_core/test_mistral.py,6
docs/source/serving/engine_args.md,6
vllm/benchmarks/sweep/serve.py,6
vllm/lora/model_manager.py,6
vllm/transformers_utils/configs/ovis.py,6
examples/offline_inference/embed_jina_embeddings_v3.py,6
tests/kernels/quantization/test_fp8_quant.py,6
benchmarks/kernels/weight_shapes.py,6
examples/offline_inference/load_sharded_state.py,6
vllm/config/attention.py,6
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py,6
vllm/distributed/device_communicators/quick_all_reduce.py,6
csrc/quantization/fp8/nvidia/quant_utils.cuh,6
vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py,6
tests/models/multimodal/processing/test_transformers.py,6
tests/models/encoder_decoder/vision_language/test_florence2.py,6
vllm/v1/stats/common.py,6
examples/offline_inference/disaggregated-prefill-v1/decode_example.py,6
vllm/adapter_commons/request.py,6
csrc/cpu/shm.cpp,5
examples/offline_inference/chat_with_tools.py,5
vllm/model_executor/layers/attention/backends/flash_attn.py,5
docs/community/sponsors.md,5
docs/source/contributing/model/index.md,5
vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py,5
tests/kernels/test_fused_quant_activation.py,5
tests/kernels/test_top_k_per_row.py,5
tests/models/multimodal/processing/test_mllama.py,5
vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py,5
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-QQQ.yaml,5
vllm/model_executor/models/openpangu_mtp.py,5
benchmarks/benchmark_serving_guided.py,5
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh,5
cacheflow/model_executor/layers/attention.py,5
benchmarks/kernels/benchmark_mrope.py,5
tests/lora/test_lora_bias_e2e.py,5
tests/models/language/pooling_mteb_test/test_gte.py,5
vllm/core/evictor_v2.py,5
tests/kernels/moe/test_ocp_mx_moe.py,5
examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh,5
tests/kernels/test_apply_repetition_penalties.py,5
vllm/v1/attention/backends/mla/aiter_triton_mla.py,5
vllm/model_executor/guided_decoding/guidance_logits_processors.py,5
tests/models/multimodal/generation/test_ultravox.py,5
tests/models/language/pooling_mteb_test/test_mxbai_rerank.py,5
vllm/transformers_utils/configs/qwen3_next.py,5
csrc/fused_qknorm_rope_kernel.cu,5
tests/v1/worker/test_utils.py,5
vllm/model_executor/models/molmo2.py,5
tests/v1/kv_connector/unit/test_kv_connector_lifecyle.py,5
csrc/mamba/mamba_ssm/selective_scan.h,5
vllm/transformers_utils/configs/kimi_vl.py,5
tests/kernels/quantization/test_nvfp4_scaled_mm.py,5
tests/kernels/moe/test_batched_deepgemm.py,5
docs/source/getting_started/installation/cpu/apple.inc.md,5
tests/reasoning/test_deepseekv3_reasoning_parser.py,5
benchmarks/benchmark_guided.py,5
.buildkite/nightly-benchmarks/tests/serving-tests.json,5
tools/enforce_regex_import.py,5
csrc/attention/attention_kernels.cuh,5
.dockerignore,5
tests/compile/silly_attention.py,5
docs/source/models/enabling_multimodal_inputs.rst,5
benchmarks/benchmark_async_llm_server.py,5
csrc/quantization/machete/machete_prepack_launcher.cuh,5
cacheflow/models/utils.py,5
tests/kernels/attention/test_deepgemm_attention.py,5
tests/entrypoints/llm/test_classify.py,5
vllm/v1/worker/gpu/async_utils.py,5
examples/gradio_webserver.py,5
vllm/v1/tokenizer/detokenizer.py,5
tests/kernels/quantization/test_int8_quant.py,5
docs/training/rlhf.md,5
vllm/third_party/pynvml.py,5
examples/online_serving/prometheus_grafana/README.md,5
examples/online_serving/jinaai_rerank_client.py,5
tests/tokenizers_/test_registry.py,5
tools/check_spdx_header.py,5
vllm/multimodal/evs.py,5
examples/multilora_inference.py,5
vllm/distributed/kv_transfer/kv_connector/lmcache_connector.py,5
.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh,5
docs/source/contributing/model/registration.md,5
tests/models/multimodal/generation/test_mllama.py,5
tests/model_executor/weight_utils.py,5
tests/core/test_serialization.py,5
csrc/quantization/awq/dequantize.cuh,5
vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope.py,5
examples/offline_inference/neuron.py,5
.github/scripts/cleanup_pr_body.sh,5
vllm/entrypoints/sagemaker/routes.py,5
tests/models/language/pooling/test_bge_reranker_v2_gemma.py,5
csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp,5
vllm/entrypoints/serve/tokenize/serving.py,5
examples/online_serving/structured_outputs/structured_outputs.py,5
docs/features/custom_logitsprocs.md,5
.buildkite/nightly-benchmarks/nightly-annotation.md,5
tests/models/quantization/test_gptq_marlin_24.py,5
examples/online_serving/openai_pooling_client.py,5
vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py,5
examples/online_serving/openai_chat_completion_client.py,5
csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu,5
vllm/compilation/torch25_custom_graph_pass.py,5
vllm/lora/resolver.py,5
tests/lora/test_lora_allowed_token_ids.py,5
vllm/entrypoints/openai/tool_parsers/minimax_m2_tool_parser.py,5
tests/v1/tpu/test_kv_cache_update_kernel.py,5
benchmarks/kernels/bench_fp8_gemm.py,5
vllm/worker/multi_step_neuronx_distributed_model_runner.py,5
csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh,5
vllm/entrypoints/pooling/classify/serving.py,5
.buildkite/nightly-benchmarks/scripts/download-tokenizer.py,5
vllm/tokenizers/protocol.py,5
vllm/prompt_adapter/request.py,5
vllm/transformers_utils/configs/aquila.py,5
examples/offline_inference/profiling_tpu/README.md,5
vllm/attention/ops/merge_attn_states.py,5
vllm/tokenizers/deepseek_v32_encoding.py,5
vllm/worker/neuronx_distributed_model_runner.py,5
tests/multimodal/test_registry.py,5
vllm/model_executor/models/plamo3.py,5
tests/models/multimodal/generation/test_vit_backend_functionality.py,5
examples/others/lmcache/cpu_offload_lmcache.py,5
benchmarks/kernels/benchmark_silu_mul_fp8_quant.py,5
benchmarks/cutlass_benchmarks/utils.py,5
cacheflow/model_executor/model_loader.py,5
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp,5
examples/online_serving/cohere_rerank_client.py,5
examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py,5
docs/source/getting_started/xpu-installation.rst,5
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh,5
cacheflow/outputs.py,5
docs/serving/parallelism_scaling.md,5
csrc/quantization/gptq_marlin/dequant.h,5
csrc/quantization/fused_kernels/quant_conversions.cuh,5
docs/source/getting_started/installation/ai_accelerator/openvino.inc.md,5
tests/plugins/vllm_add_dummy_platform/setup.py,5
tests/entrypoints/openai/test_token_in_token_out.py,5
vllm/transformers_utils/configs/cohere2.py,5
csrc/quantization/gguf/dequantize.cuh,5
.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc3.json,5
tests/models/embedding/vision_language/test_dse_qwen2_vl.py,5
tools/ep_kernels/README.md,5
tests/compile/distributed/test_sequence_parallelism.py,5
tests/kernels/core/test_fused_qk_norm_rope.py,5
docs/deployment/frameworks/open-webui.md,5
vllm/v1/worker/ubatch_splitting.py,5
benchmarks/kernels/benchmark_reshape_and_cache.py,5
vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py,5
vllm/model_executor/layers/fla/ops/chunk_o.py,5
tests/entrypoints/pooling/classify/test_online.py,5
tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py,5
vllm/transformers_utils/configs/medusa.py,5
vllm/entrypoints/openai/chat_completion/serving.py,5
vllm/entrypoints/openai/engine/protocol.py,5
vllm/entrypoints/openai/utils.py,5
tests/entrypoints/openai/test_transcription_validation_whisper.py,5
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json",5
examples/llava_next_example.py,5
tests/lora/data/long_context_test_data.py,5
tests/v1/tpu/test_spmd_model_weight_loading.py,5
csrc/mamba/causal_conv1d/causal_conv1d.h,5
.buildkite/upload-wheels.sh,5
cacheflow/core/block_manager.py,5
csrc/moe/marlin_kernels/marlin_moe_kernel.h,5
examples/api_client.py,5
tests/models/decoder_only/vision_language/test_h2ovl.py,5
tests/models/quantization/test_nvfp4.py,5
tests/models/decoder_only/vision_language/processing/test_llava_next.py,5
examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py,5
tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py,5
vllm/worker/multi_step_neuron_model_runner.py,5
tests/v1/metrics/test_stats.py,5
tests/v1/structured_output/test_reasoning_structured_output.py,5
docs/design/mm_processing.md,5
tests/engine/test_detokenization.py,5
vllm/model_executor/layers/quantization/compressed_tensors/transform/module.py,5
vllm/model_executor/models/mimo_v2_flash.py,5
tests/tool_use/test_deepseekv31_tool_parser.py,5
examples/offline_inference/basic/generate.py,5
vllm/v1/kv_offload/factory.py,5
examples/offline_inference_vision_language_embedding.py,5
vllm/model_executor/parallel_utils/pynccl_utils.py,5
tests/v1/entrypoints/openai/responses/test_basic.py,5
vllm/entrypoints/anthropic/serving_messages.py,5
tests/kernels/test_awq.py,5
vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark.py,5
.buildkite/test_areas/distributed.yaml,5
docs/source/getting_started/installation.md,5
vllm/entrypoints/pooling/classify/protocol.py,5
cacheflow/model_executor/models/llama.py,5
vllm/model_executor/models/transformers_moe.py,5
tests/engine/output_processor/test_multi_step.py,5
vllm/model_executor/models/fairseq2_llama.py,5
tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py,5
docs/design/v1/metrics.md,5
tests/lora/test_ultravox.py,5
tools/shellcheck.sh,5
vllm/benchmarks/lib/ready_checker.py,5
tests/multimodal/test_inputs.py,5
tests/entrypoints/openai/test_return_token_ids.py,5
vllm/model_executor/layers/quantization/quark/schemes/__init__.py,5
csrc/quantization/gguf/vecdotq.cuh,5
vllm/transformers_utils/gguf_utils.py,5
vllm/transformers_utils/configs/radio.py,5
docs/deployment/frameworks/hf_inference_endpoints.md,5
vllm/distributed/device_communicators/hpu_communicator.py,5
vllm/entrypoints/cli/benchmark/serve.py,5
vllm/model_executor/layers/fla/ops/layernorm_guard.py,5
examples/online_serving/prompt_embed_inference_with_openai_client.py,5
tests/compile/fullgraph/test_full_graph.py,5
docs/getting_started/installation/cpu.x86.inc.md,5
.github/dependabot.yml,5
tests/entrypoints/openai/test_response_api_mcp_tools.py,5
tests/models/multimodal/generation/test_phi4_multimodal.py,5
tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/__init__.py,5
vllm/entrypoints/openai/rpc/__init__.py,5
vllm/model_executor/quantization_utils/awq.py,5
tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py,5
vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py,5
tests/kernels/test_triton_scaled_mm.py,5
vllm/compilation/cuda_piecewise_backend.py,5
vllm/model_executor/layers/pooler/tokwise/poolers.py,5
cacheflow/models/__init__.py,5
tests/distributed/test_kvlayout.py,5
csrc/quantization/gguf/ggml-common.h,5
tools/check_triton_import.py,5
tests/kernels/attention/test_flashmla_sparse.py,5
tests/v1/e2e/test_async_sched_and_preempt.py,5
vllm/model_executor/layers/fla/ops/chunk_delta_h.py,5
tests/v1/entrypoints/openai/responses/test_image.py,5
vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool.py,5
vllm/v1/core/sched/request_queue.py,5
tests/distributed/test_eplb_algo.py,5
vllm/model_executor/models/voxtral_streaming.py,5
tests/v1/shutdown/test_delete.py,5
vllm/model_executor/models/transformers/pooling.py,5
vllm/model_executor/layers/quantized_linear/awq.py,5
vllm/v1/core/scheduler_output.py,5
vllm/reasoning/identity_reasoning_parser.py,5
tests/v1/kv_connector/unit/test_output_aggreagator.py,5
vllm/worker/cpu_embedding_model_runner.py,5
csrc/punica/bgmv/generator.py,5
vllm/lora/ops/torch_ops/lora_ops.py,5
tests/models/test_utils.py,5
docs/design/paged_attention.md,5
tests/models/decoder_only/language/test_hybrid.py,5
vllm/v1/structured_output/backend_lm_format_enforcer.py,5
benchmarks/kernels/benchmark_device_communicators.py,5
tests/compile/test_dynamic_shapes_compilation.py,5
vllm/distributed/tpu_distributed_utils.py,5
csrc/quantization/machete/machete_mm_kernel.cuh,5
csrc/quantization/gptq_allspark/allspark_utils.cuh,5
csrc/cpu/dnnl_helper.h,5
vllm/model_executor/layers/quantization/utils/marlin_utils_test_qqq.py,5
tests/models/language/pooling_mteb_test/test_st_projector.py,5
tests/models/language/pooling/test_mxbai_rerank.py,5
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json",5
vllm/model_executor/guided_decoding.py,5
tests/neuron/test_prefix_prefill.py,5
vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py,5
vllm/v1/sample/logits_processor/state.py,5
tests/tokenization/test_tokenizer.py,5
examples/offline_inference/neuron_int8_quantization.py,5
docs/design/logits_processors.md,5
vllm/v1/worker/gpu/sample/metadata.py,5
csrc/punica/punica_ops.cc,5
vllm/entrypoints/openai/tool_parsers/hunyuan_a13b_tool_parser.py,5
docs/source/deployment/integrations/index.md,5
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json",5
tests/models/language/pooling_mteb_test/test_qwen3_reranker.py,5
.buildkite/scripts/annotate-release.sh,5
tests/v1/entrypoints/openai/test_completion_with_image_embeds.py,5
tests/entrypoints/openai/test_completion_error.py,5
csrc/attention/merge_attn_states.cu,5
vllm/model_executor/models/whisper_utils.py,5
examples/online_serving/openai_embedding_client.py,5
examples/online_serving/openai_completion_client.py,5
tests/v1/entrypoints/openai/test_chat_completion.py,5
.buildkite/nightly-benchmarks/scripts/nightly-annotate.sh,5
.buildkite/nightly-benchmarks/scripts/compare-json-results.py,5
docker/Dockerfile.tpu,5
tests/kernels/attention/test_merge_attn_states.py,5
tests/lora/test_layer_variation.py,5
tests/models/language/pooling/test_intfloat.py,5
tests/models/language/pooling_mteb_test/test_baai.py,5
tests/kernels/test_rocm_attention_selector.py,5
vllm/model_executor/models/audioflamingo3.py,5
tests/models/multimodal/generation/vlm_utils/runners.py,5
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json",5
tests/entrypoints/pooling/score/test_utils.py,5
tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py,5
vllm/model_executor/layers/fused_moe/all2all_utils.py,5
vllm/plugins/lora_resolvers/filesystem_resolver.py,5
benchmarks/kernels/benchmark_bitblas.py,5
docs/source/features/quantization/bnb.md,5
tests/test_embedded_commit.py,5
benchmarks/kernels/benchmark_moe_align_block_size.py,5
vllm/reasoning/holo2_reasoning_parser.py,5
csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu,5
csrc/cpu/dnnl_kernels.cpp,5
vllm/entrypoints/cli/benchmark/throughput.py,5
vllm/model_executor/layers/quantization/utils/__init__.py,5
vllm/model_executor/models/lightonocr.py,5
vllm/entrypoints/openai/parser/harmony_utils.py,5
examples/offline_inference/simple_profiling.py,5
examples/offline_inference/context_extension.py,5
examples/online_serving/gradio_webserver.py,5
tests/kernels/test_cascade_flash_attn.py,5
tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py,5
benchmarks/kernels/benchmark_shapes.py,5
tests/v1/logits_processors/test_correctness.py,5
examples/online_serving/disaggregated_encoder/README.md,5
vllm/transformers_utils/tokenizers/baichuan.py,5
csrc/pos_encoding.cpp,5
tests/v1/distributed/test_eagle_dp.py,5
vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py,5
vllm/distributed/kv_transfer/README.md,5
csrc/sparse/cutlass/sparse_scaled_mm_entry.cu,5
docs/deployment/frameworks/anyscale.md,5
vllm/entrypoints/cli/benchmark/latency.py,5
vllm/entrypoints/openai/responses/serving.py,5
Dockerfile.hpu,5
vllm/transformers_utils/processors/ovis2_5.py,5
vllm/model_executor/layers/fused_moe/routing_simulator.py,5
tests/lora/test_default_mm_loras.py,5
csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu,5
vllm/adapter_commons/layers.py,5
tests/entrypoints/openai/test_tensorizer_entrypoint.py,5
vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py,5
examples/offline_inference/openai_batch/README.md,5
vllm/reasoning/ernie45_reasoning_parser.py,5
vllm/transformers_utils/configs/qwen.py,5
tests/kernels/moe/modular_kernel_tools/parallel_utils.py,5
vllm/model_executor/layers/attention/attention.py,5
vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py,5
.github/ISSUE_TEMPLATE/100-documentation.yml,5
examples/online_serving/openai_cross_encoder_score.py,5
tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh,5
examples/online_serving/streamlit_openai_chatbot_webserver.py,5
.buildkite/nightly-benchmarks/tests/serving-tests-cpu.json,5
docs/source/getting_started/installation/index.md,5
tests/spec_decode/e2e/test_mtp_correctness.py,5
vllm/model_executor/guided_decoding/reasoner/__init__.py,4
vllm/tool_parsers/minimax_m2_tool_parser.py,4
docs/source/design/mm_processing.md,4
requirements-hpu.txt,4
tests/distributed/test_eplb_spec_decode.py,4
docker/Dockerfile.neuron,4
tests/models/multimodal/generation/test_qwen2_5_vl.py,4
vllm/model_executor/layers/fla/ops/cumsum.py,4
examples/offline_inference/qwen_1m.py,4
examples/offline_inference_distributed.py,4
tests/v1/tracing/test_tracing.py,4
docs/source/design/huggingface_integration.rst,4
csrc/cpu/pybind.cpp,4
docs/source/getting_started/tpu-installation.md,4
docs/usage/usage_stats.md,4
tests/models/multimodal/pooling/test_phi3v.py,4
docs/source/getting_started/installation/cpu-x86.md,4
vllm/v1/executor/gpu_executor.py,4
examples/online_serving/multi_instance_data_parallel.py,4
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8.yaml,4
examples/openai_client.py,4
tests/kernels/test_ggml.py,4
tests/models/decoder_only/vision_language/vlm_utils/builders.py,4
RELEASE.md,4
vllm/distributed/eplb/__init__.py,4
tests/kernels/test_triton_flash_attention.py,4
vllm/tasks.py,4
tests/entrypoints/pooling/llm/test_classify.py,4
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_helper.hpp,4
tests/entrypoints/pooling/llm/test_encode.py,4
tests/distributed/test_shm_buffer.py,4
tests/models/decoder_only/vision_language/vlm_utils/runners.py,4
.buildkite/scripts/tpu/run_bm.sh,4
csrc/punica/bgmv/bgmv_impl.cuh,4
docs/source/quantization/auto_awq.rst,4
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh,4
vllm/transformers_utils/configs/falcon.py,4
tests/entrypoints/pooling/llm/test_score.py,4
tests/models/quantization/test_aqlm.py,4
csrc/quantization/fp8/fp8_marlin.cu,4
vllm/transformers_utils/configs/mllama.py,4
csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh,4
csrc/quantization/cutlass_w8a8/scaled_mm_dq_entry.cu,4
tests/test_outputs.py,4
vllm/entrypoints/ssl.py,4
tests/v1/tpu/test_tpu_qkv_linear.py,4
tests/lora/test_moe_lora_align_sum.py,4
docs/getting_started/installation/.nav.yml,4
tests/entrypoints/openai/test_chat_logit_bias_validation.py,4
tests/test_triton_utils.py,4
tests/models/decoder_only/vision_language/processing/test_llava_onevision.py,4
tests/v1/e2e/test_ngram_spec_decode.py,4
.github/ISSUE_TEMPLATE/200-installation.yml,4
examples/offline_inference/batch_llm_inference.py,4
vllm/benchmarks/mm_processor.py,4
vllm/entrypoints/serve/lora/api_router.py,4
tests/kernels/quantization/test_int8_kernel.py,4
tests/kernels/allclose_default.py,4
vllm/transformers_utils/configs/midashenglm.py,4
tests/tool_use/test_mistral_tool_parser.py,4
csrc/quantization/cutlass_w8a8/common.hpp,4
docs/source/api/multimodal/inputs.md,4
vllm/entrypoints/serve/disagg/api_router.py,4
tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt,4
docs/features/sleep_mode.md,4
csrc/quantization/gguf/moe.cuh,4
vllm/model_executor/layers/fla/ops/op.py,4
vllm/v1/spec_decode/metadata.py,4
tests/v1/attention/test_chunked_local_attention.py,4
vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py,4
docs/source/features/quantization/gguf.md,4
tests/core/block/test_block_manager.py,4
.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc2.json,4
tests/entrypoints/openai/test_optional_middleware.py,4
csrc/attention/mla/cutlass_sm100_mla/device/sm100_mla.hpp,4
tests/entrypoints/pooling/openai/test_rerank.py,4
tests/worker/spec_decode/utils.py,4
tests/benchmarks/test_random_dataset.py,4
vllm/v1/executor/utils.py,4
tests/models/test_embedding.py,4
tests/kernels/core/test_uva.py,4
docs/deployment/integrations/llamastack.md,4
tests/reasoning/test_deepseekr1_reasoning_parser.py,4
cacheflow/server/async_llm_server.py,4
tests/entrypoints/openai/correctness/test_mteb_score.py,4
examples/offline_inference/neuron_multimodal.py,4
tests/lora/test_resolver.py,4
benchmarks/multi_turn/README.md,4
examples/online_serving/opentelemetry/dummy_client.py,4
tests/lora/test_olmoe_tp.py,4
.buildkite/scripts/tpu/docker_run_bm.sh,4
vllm/model_executor/layers/quantization/utils/mxfp8_utils.py,4
vllm/model_executor/layers/quantization/fp_quant.py,4
vllm/entrypoints/openai/tool_parsers/longcat_tool_parser.py,4
docs/Makefile,4
vllm/core/policy.py,4
tests/models/language/pooling_mteb_test/test_cross_encoder.py,4
examples/offline_inference/qwen2_5_omni/README.md,4
tests/transformers_utils/test_config.py,4
docs/getting_started/installation/python_env_setup.inc.md,4
cacheflow/model_executor/__init__.py,4
tests/models/test_qwen.py,4
docs/source/getting_started/installation/ai_accelerator/neuron.inc.md,4
vllm/lora/ops/torch_ops/__init__.py,4
docs/source/api/multimodal/index.md,4
vllm/model_executor/layers/fla/ops/solve_tril.py,4
examples/offline_inference/reproducibility.py,4
tests/reasoning/test_qwen3_reasoning_parser.py,4
tests/v1/kv_connector/nixl_integration/tp_config_sweep_accuracy_test.sh,4
vllm/model_executor/models/ernie45.py,4
tests/distributed/test_quick_all_reduce.py,4
docs/benchmarking/dashboard.md,4
docs/usage/README.md,4
.buildkite/nightly-benchmarks/tests/nightly-tests.json,4
csrc/cumem_allocator.cpp,4
vllm/transformers_utils/configs/step3_vl.py,4
docs/source/performance/optimization.md,4
csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu,4
docs/features/automatic_prefix_caching.md,4
tools/check_init_lazy_imports.py,4
tests/multimodal/test_audio.py,4
tests/kernels/attention/test_encoder_decoder_attn.py,4
tests/entrypoints/openai/correctness/test_mteb.py,4
tests/kernels/core/test_permute_cols.py,4
tests/v1/worker/test_worker_memory_snapshot.py,4
tests/v1/metrics/test_engine_logger_apis.py,4
tests/v1/kv_connector/unit/test_backwards_compatibility.py,4
csrc/cpu/dnnl_helper.hpp,4
tests/plugins/vllm_add_dummy_model/setup.py,4
csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh,4
docs/source/features/quantization/gptqmodel.md,4
cacheflow/entrypoints/openai/openai_frontend.py,4
tools/validate_config.py,4
examples/offline_inference/qwen3_reranker.py,4
vllm/model_executor/layers/pooler/seqwise/poolers.py,4
vllm/v1/attention/backends/fa_utils.py,4
tests/entrypoints/openai/test_response_api_parsable_context.py,4
tests/models/decoder_only/language/test_big_models.py,4
examples/fp8/extract_scales.py,4
docs/source/features/disagg_prefill.md,4
tests/engine/test_detokenize.py,4
.github/ISSUE_TEMPLATE/300-usage.yml,4
vllm/model_executor/layers/fla/ops/l2norm.py,4
tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py,4
vllm/model_executor/models/donut.py,4
tests/v1/structured_output/test_backend_guidance.py,4
tests/evals/gsm8k/gsm8k_eval.py,4
csrc/attention/mla/cutlass_mla_kernels.cu,4
tests/neuron/2_core/test_multi_lora.py,4
benchmarks/benchmark_block_pool.py,4
vllm/model_executor/layers/fused_moe/oracle/nvfp4.py,4
tests/transformers_utils/test_utils.py,4
tests/kernels/test_awq_triton.py,4
vllm/transformers_utils/configs/baichuan.py,4
cacheflow/model_executor/utils.py,4
docs/source/models/extensions/index.md,4
tests/models/test_jamba.py,4
benchmarks/kernels/bench_block_fp8_gemm.py,4
tests/kernels/attention/test_mla_decode_cpu.py,4
tests/v1/tpu/test_tpu_int8.py,4
docs/features/prompt_embeds.md,4
tests/distributed/test_expert_placement.py,4
tests/models/decoder_only/language/test_granite.py,4
vllm/transformers_utils/configs/h2ovl.py,4
tests/tokenizers_/test_basic.py,4
vllm/model_executor/layers/quantization/petit.py,4
vllm/distributed/ec_transfer/ec_connector/example_connector.py,4
docs/source/serving/serving_with_langchain.rst,4
tests/models/decoder_only/vision_language/test_qwen.py,4
benchmarks/disagg_benchmarks/round_robin_proxy.py,4
.github/workflows/codespell.yml,4
tests/build_cython.py,4
vllm/prefix.py,4
benchmarks/multi_turn/bench_dataset.py,4
tests/entrypoints/pooling/score/test_online_score.py,4
csrc/quantization/gguf/mmq.cuh,4
tests/models/multimodal/generation/test_florence2.py,4
tests/v1/test_external_lb_dp.py,4
tests/multimodal/test_base.py,4
cacheflow/entrypoints/simple_fastapi_frontend.py,4
tests/models/multimodal/pooling/conftest.py,4
tests/kernels/moe/modular_kernel_tools/cli_args.py,4
vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py,4
tests/models/quantization/test_gptq_marlin.py,4
csrc/core/math.hpp,4
vllm/config/device.py,4
tests/standalone_tests/test_tensor_schema.py,4
vllm/model_executor/models/swin.py,4
csrc/attention/attention_generic.cuh,4
vllm/v1/sample/logits_processor.py,4
tests/multimodal/test_processor.py,4
docs/governance/process.md,4
tests/prompt_adapter/test_multi_adapter_inference.py,4
tests/models/language/pooling/test_pooler_config_init_behaviour.py,4
vllm/worker/multi_step_tpu_worker.py,4
csrc/quantization/machete/machete_mm_launcher.cuh,4
vllm/compilation/base_static_graph.py,4
vllm/model_executor/parallel_utils/tensor_parallel/layers.py,4
vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4.py,4
tests/spec_decode/e2e/test_correctness.py,4
tests/distributed/test_distributed_oot.py,4
tests/v1/determinism/test_online_batch_invariance.py,4
vllm/model_executor/models/rvl.py,4
tests/evals/gsm8k/configs/Qwen1.5-MoE-W4A16-CT.yaml,4
tests/models/decoder_only/vision_language/test_phi4mm.py,4
vllm/v1/worker/gpu/sample/gumbel.py,4
tests/models/encoder_decoder/language/test_bart.py,4
docs/mkdocs/hooks/remove_announcement.py,4
tests/distributed/test_shm_storage.py,4
.buildkite/scripts/hardware_ci/run-cpu-test-arm.sh,4
csrc/cpu/utils.hpp,4
tests/models/decoder_only/vision_language/vlm_utils/case_filtering.py,4
tests/kernels/quantization/test_awq_triton.py,4
tests/entrypoints/openai/conftest.py,4
cacheflow/master/simple_frontend.py,4
vllm/_core_ext.py,4
tests/engine/test_stop_reason.py,4
docs/usage/reproducibility.md,4
examples/online_serving/prometheus_grafana/grafana.json,4
csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu,4
tests/v1/distributed/test_dbo.py,4
vllm/distributed/device_communicators/pynccl_utils.py,4
vllm/model_executor/layers/fla/ops/chunk.py,4
tests/vllm_test_utils/setup.py,4
docs/source/getting_started/installation/gpu-cuda.md,4
vllm/model_executor/parallel_utils/cupy_utils.py,4
tests/test_seed_behavior.py,4
csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c3x.hpp,4
vllm/model_executor/models/constant_size_cache.py,4
vllm/model_executor/quantization_utils/base.py,4
tests/kernels/cache.py,4
vllm/lora/ops/triton_ops/sgmv_expand.py,4
csrc/moe/moe_lora_align_sum_kernels.cu,4
tests/v1/kv_connector/nixl_integration/test_edge_cases.py,4
docs/source/quantization/bnb.rst,4
tests/kernels/quantization/test_silu_mul_nvfp4_quant.py,4
.github/ISSUE_TEMPLATE/450-ci-failure.yml,4
examples/offline_inference_embedding.py,4
vllm/entrypoints/serve/elastic_ep/api_router.py,4
examples/online_serving/structured_outputs/README.md,4
find_cuda_init.py,4
tests/kernels/moe/test_count_expert_num_tokens.py,4
tests/kernels/moe/test_rocm_aiter_topk.py,4
tests/vllm_test_utils/vllm_test_utils/__init__.py,4
vllm/prompt_adapter/worker_manager.py,4
examples/online_serving/openai_embedding_long_text/service.sh,4
vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py,4
tests/v1/test_internal_lb_dp.py,4
tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py,4
vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope.py,4
docs/source/usage/structured_outputs.md,4
tests/runai_model_streamer_test/test_runai_model_streamer_loader.py,4
tests/kernels/pos_encoding.py,4
docs/community/contact_us.md,4
tests/spec_decode/test_memory_usage.py,4
docs/source/getting_started/installation/cpu/arm.inc.md,4
docs/configuration/serve_args.md,4
vllm/v1/kv_offload/lru_manager.py,4
tests/kernels/moe/test_triton_moe_no_act_mul.py,4
tests/model_executor/conftest.py,4
docs/source/design/class_hierarchy.rst,4
vllm/profiler/gpu_profiler.py,4
vllm/transformers_utils/dynamic_module.py,4
vllm/prompt_adapter/layers.py,4
vllm/entrypoints/openai/responses/protocol.py,4
vllm/transformers_utils/configs/yi.py,4
.markdownlint.yaml,4
vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py,4
tests/models/decoder_only/vision_language/test_broadcast.py,4
vllm/model_executor/layers/conv.py,4
tests/models/decoder_only/vision_language/test_awq.py,4
tests/models/language/generation/test_phimoe.py,4
tests/v1/spec_decode/test_acceptance_length.py,4
csrc/quantization/gptq_marlin/kernel.h,4
vllm/compilation/config.py,4
csrc/quantization/fp8/per_token_group_quant.cu,4
vllm/lora/ops/triton_ops/sgmv_shrink.py,4
tests/v1/e2e/test_min_tokens.py,4
tests/kernels/attention/test_cutlass_mla_decode.py,4
tests/v1/core/test_priority_scheduler_random.py,4
tests/entrypoints/openai/test_response_api_simple.py,4
docs/source/api/model/index.md,4
docs/source/dev/profiling/profiling_index.rst,4
vllm/entrypoints/pooling/pooling/api_router.py,4
tests/prompt_adapter/test_pa_lora.py,4
tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/types.py,4
vllm/ray/ray_env.py,4
.buildkite/pyproject.toml,4
.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh,4
tests/engine/test_options.py,4
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8.py,4
.github/ISSUE_TEMPLATE/600-new-model.yml,4
tests/kernels/test_cutlass_2of4_sparse.py,4
cacheflow/server/ray_utils.py,4
tests/v1/test_hybrid_lb_dp.py,4
examples/online_serving/disaggregated_prefill.sh,4
docs/source/getting_started/gaudi-installation.md,4
csrc/quantization/machete/machete_prepacked_layout.cuh,4
vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope.py,4
docs/source/design/kernel/paged_attention.md,4
vllm/model_executor/layers/quantization/utils/petit_utils.py,4
tests/models/language/pooling/test_auto_prefix_cache_support.py,4
vllm/model_executor/layers/triton_kernel/prefix_prefill.py,4
vllm/model_executor/layers/fused_moe/router/router_factory.py,4
csrc/cpu/cpu_attn.cpp,4
docs/source/models/pooling_models.rst,4
examples/offline_inference/prefix_caching.py,4
vllm/model_executor/layers/quantized_linear/__init__.py,4
tests/config/test_mp_reducer.py,4
tests/compile/test_multimodal_compile.py,4
vllm/transformers_utils/configs/telechat2.py,4
tests/models/decoder_only/vision_language/test_internvl.py,4
vllm/transformers_utils/configs/internvl.py,4
tests/entrypoints/pooling/embed/test_online_vision.py,4
tests/kernels/attention/test_lightning_attn.py,4
docs/deployment/integrations/kserve.md,4
.buildkite/performance-benchmarks/README.md,4
vllm/lora/ops/xla_ops/__init__.py,4
csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp,4
tests/reasoning/test_hunyuan_reasoning_parser.py,4
tests/entrypoints/openai/test_gptoss_structural_tags_integration.py,4
vllm/tool_parsers/glm4_moe_tool_parser.py,4
examples/offline_inference_openai.md,4
tests/distributed/test_symm_mem_allreduce.py,4
examples/fp8/quantizer/quantize.py,4
vllm/entrypoints/anthropic/api_server.py,4
tests/config/test_config_with_model.yaml,4
.buildkite/scripts/hardware_ci/run-hpu-test.sh,4
vllm/transformers_utils/configs/speculators/base.py,4
.github/workflows/scripts/pytorch-install.sh,4
tests/v1/entrypoints/openai/responses/conftest.py,4
tests/prompt_adapter/test_bloom.py,4
benchmarks/run_structured_output_benchmark.sh,4
vllm/entrypoints/serve/tokenize/api_router.py,4
cacheflow/frontend/simple_frontend.py,4
tests/models/multimodal/pooling/test_llava_next.py,4
tests/entrypoints/pooling/llm/test_embedding.py,4
vllm/v1/metrics/reader.py,4
tests/entrypoints/test_openai_vision.py,4
vllm/model_executor/neuron_model_loader.py,4
vllm/tool_parsers/abstract_tool_parser.py,4
docs/source/models/performance.rst,4
vllm/entrypoints/pooling/embed/api_router.py,4
tests/plugins/lora_resolvers/test_filesystem_resolver.py,4
csrc/quantization/fp4/nvfp4_utils.cuh,4
vllm/transformers_utils/model_arch_config_convertor.py,4
tests/entrypoints/openai/tool_parsers/test_hunyuan_a13b_tool_parser.py,4
.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py,4
tests/models/test_fuyu.py,4
vllm/lora/ops/bgmv_shrink.py,4
tests/kernels/quantization/test_silu_nvfp4_quant_fusion.py,4
tests/distributed/test_basic_distributed_correctness_enc_dec.py,4
csrc/quantization/gptq_marlin/gptq_marlin.cuh,4
tests/kernels/moe/test_cpu_fused_moe.py,4
tests/models/language/generation/test_bart.py,4
tests/lora/test_jamba.py,4
docs/features/quantization/auto_round.md,4
docs/source/dev/input_processing/model_inputs_index.rst,4
vllm/benchmarks/endpoint_request_func.py,4
benchmark/benchmark_text_completion.py,4
tests/v1/kv_connector/unit/test_output_aggregator.py,4
vllm/model_executor/layers/fused_moe/oracle/unquantized.py,4
vllm/model_executor/models/kanana_v.py,4
tests/models/language/pooling_mteb_test/test_nemotron.py,4
tests/evals/gsm8k/configs/models-blackwell.txt,4
tests/evals/gsm8k/configs/moe-refactor/config-b200.txt,4
requirements/hpu.txt,4
examples/offline_inference/florence2_inference.py,4
tests/kernels/quantization/test_per_token_group_quant.py,4
tests/core/block/test_common.py,4
vllm/model_executor/models/llama_embedding.py,4
vllm/model_executor/guided_logits_processors.py,4
.buildkite/scripts/hardware_ci/run-neuron-test.sh,4
tests/entrypoints/llm/test_gpu_utilization.py,4
tests/distributed/test_ca_buffer_sharing.py,4
tests/core/block/conftest.py,4
benchmarks/disagg_benchmarks/visualize_benchmark_results.py,4
docs/source/serving/serving_with_llamastack.rst,3
tools/profiler/nsys_profile_tools/gputrc2graph.py,3
vllm/model_executor/layers/quantization/utils/allspark_utils.py,3
tests/kernels/quantization/test_cutlass_2of4_sparse.py,3
vllm/entrypoints/openai/reasoning_parsers/__init__.py,3
vllm/utils/platform_utils.py,3
.pylintrc,3
csrc/quantization/gptq_marlin/.gitignore,3
.buildkite/nightly-benchmarks/nightly-pipeline.yaml,3
docs/ci/update_pytorch_version.md,3
vllm/benchmarks/sweep/param_sweep.py,3
tests/entrypoints/sagemaker/conftest.py,3
vllm/model_executor/layers/rotary_embedding/linear_scaling_rope.py,3
csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.cu,3
csrc/cuda_primitives.h,3
tests/v1/attention/test_batch_reordering.py,3
csrc/attention_utils.h,3
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",3
vllm/profiler/__init__.py,3
csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.cu,3
csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.inl,3
docs/source/features/quantization/quantized_kvcache.md,3
docs/serving/distributed_troubleshooting.md,3
tests/plugins_tests/conftest.py,3
docs/source/features/quantization/bitblas.md,3
docs/source/getting_started/debugging.md,3
vllm/model_executor/layers/fused_moe/configs/README,3
vllm/v1/worker/gpu/mm/mrope_utils.py,3
docs/source/serving/usage_stats.md,3
tests/kernels/test_fused_quant_layernorm.py,3
docs/benchmarking/sweeps.md,3
vllm/transformers_utils/config_parser_base.py,3
vllm/compilation/qk_norm_rope_fusion.py,3
vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py,3
csrc/punica/bgmv/bgmv_fp16_fp32_fp16.cu,3
vllm/entrypoints/serve/disagg/protocol.py,3
vllm/reasoning/seedoss_reasoning_parser.py,3
csrc/quantization/cutlass_w8a8/moe/blockwise_scaled_group_mm_sm100.cu,3
docs/models/extensions/fastsafetensor.md,3
examples/offline_inference_with_profiler.py,3
tests/mistral_tool_use/utils.py,3
tests/v1/ec_connector/integration/test_epd_correctness.py,3
docs/configuration/README.md,3
tests/standalone_tests/pytorch_nightly_dependency.sh,3
docs/cli/bench/serve.md,3
vllm/transformers_utils/configs/nemotron_vl.py,3
vllm/benchmarks/sweep/plot.py,3
tests/kernels/quantization/test_hadacore.py,3
tests/compile/piecewise/piecewise_compilation_config.json,3
tests/mq_llm_engine/conftest.py,3
vllm/model_executor/models/neuron/llama.py,3
docs/source/design/plugin_system.rst,3
docs/source/serving/deploying_with_dstack.rst,3
tests/entrypoints/pooling/embed/test_online_long_text.py,3
vllm/utils/serial_utils.py,3
vllm/model_executor/layers/attention/ops/paged_attn.py,3
examples/online_serving/chart-helm/README.md,3
tests/kernels/test_mamba_ssm_ssd.py,3
vllm/transformers_utils/configs/chameleon.py,3
tests/benchmarks/test_latency_cli.py,3
vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py,3
tests/entrypoints/openai/responses/test_harmony.py,3
vllm/model_executor/quantization_utils/__init__.py,3
examples/tool_chat_template_llama3.2_json.jinja,3
.github/workflows/scripts/cuda-install.sh,3
tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh,3
docs/getting_started/installation/ai_accelerator/neuron.inc.md,3
vllm/compilation/levels.py,3
vllm/model_executor/models/iquest_loopcoder.py,3
tests/utils_/test_tensor_schema.py,3
examples/offline_inference/logits_processor/custom.py,3
tools/generate_cmake_presets.py,3
tests/kernels/quantization/test_ggml.py,3
vllm/transformers_utils/configs/skyworkr1v.py,3
.buildkite/scripts/cleanup-nightly-builds.sh,3
docs/serving/integrations/llamaindex.md,3
tests/entrypoints/openai/test_orca_metrics.py,3
examples/aqlm_example.py,3
examples/online_serving/multi-node-serving.sh,3
vllm/compilation/rocm_aiter_fusion.py,3
tests/detokenizer/test_min_tokens.py,3
vllm/model_executor/layers/shared_fused_moe/__init__.py,3
tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py,3
.buildkite/test_areas/entrypoints.yaml,3
vllm/transformers_utils/configs/olmo2.py,3
examples/openai_cross_encoder_score.py,3
examples/lmcache/README.md,3
tests/entrypoints/llm/test_reward.py,3
tests/evals/gsm8k/configs/models-small.txt,3
tests/models/quantization/test_mxfp4.py,3
tests/models/language/pooling_mteb_test/mteb_embed_utils.py,3
tests/lora/test_llm_with_multi_loras.py,3
docs/source/usage/multimodal_inputs.rst,3
tools/update-dockerfile-graph.sh,3
.github/ISSUE_TEMPLATE/700-performance discussion.yml,3
examples/fp8/README.md,3
tests/models/test_pixtral.py,3
.buildkite/lm-eval-harness/configs/Qwen1.5-MoE-W4A16-compressed-tensors.yaml,3
csrc/quantization/w8a8/cutlass/moe/moe_data.cu,3
vllm/distributed/eplb/async_worker.py,3
tests/compile/distributed/test_fusion_all_reduce.py,3
tests/compile/fullgraph/test_multimodal_compile.py,3
tests/test_lazy_torch_compile.py,3
tests/kv_transfer/test_send_recv.sh,3
examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh,3
docs/source/serving/integrations/index.md,3
tests/compile/distributed/test_async_tp.py,3
examples/offline_inference/disaggregated-prefill-v1/README.md,3
.buildkite/scripts/run-benchmarks.sh,3
.buildkite/nightly-benchmarks/performance-benchmarks-descriptions.md,3
tests/entrypoints/pooling/score/test_online_rerank.py,3
vllm/model_executor/layers/fused_moe/oracle/fp8.py,3
vllm/model_executor/models/neuron/mistral.py,3
vllm/entrypoints/openai/serving_rerank.py,3
csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h,3
docs/source/serving/env_vars.rst,3
csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu,3
tests/kernels/moe/modular_kernel_tools/utils.py,3
tests/engine/conftest.py,3
vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py,3
csrc/activation.cpp,3
docs/source/getting_started/installation/gpu/index.md,3
docs/cli/bench/sweep/plot.md,3
csrc/quantization/fp8/amd/hip_float8_impl.h,3
docs/source/models/extensions/tensorizer.md,3
tests/kernels/test_mha_attn.py,3
.buildkite/nightly-benchmarks/tests/genai-perf-tests.json,3
tests/spec_decode/conftest.py,3
benchmarks/kernels/benchmark_per_token_group_quant.py,3
docs/source/deployment/nginx.md,3
examples/others/lmcache/kv_cache_sharing_lmcache_v1.py,3
vllm/transformers_utils/repo_utils.py,3
tests/kernels/quantization/test_gptq.py,3
docs/cli/run-batch.md,3
examples/offline_inference/distributed.py,3
.buildkite/nightly-benchmarks/kickoff-pipeline.sh,3
examples/offline_inference/disaggregated_prefill_lmcache.py,3
vllm/entrypoints/openai/completion/serving.py,3
docs/source/models/extensions/runai_model_streamer.md,3
tests/kernels/test_utils.py,3
examples/online_serving/openai_classification_client.py,3
tests/transformers_utils/test_config_parser_registry.py,3
csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh,3
csrc/quantization/cutlass_w8a8/Epilogues.md,3
vllm/tool_parsers/__init__.py,3
docs/cli/bench/throughput.md,3
tests/kernels/moe/deepep_utils.py,3
vllm/model_executor/models/gemma2_embedding.py,3
vllm/model_executor/models/glmasr_utils.py,3
vllm/lora/layers/qkv_x_parallel_linear.py,3
examples/offline_inference/qwen3_omni/only_thinker.py,3
tests/reasoning/test_seedoss_reasoning_parser.py,3
tests/evals/gsm8k/README.md,3
vllm/model_executor/parallel_utils/layers.py,3
docs/serving/integrations/claude_code.md,3
vllm/model_executor/layers/attention/cross_attention.py,3
docs/getting_started/installation/aws_neuron.md,3
tests/ci_envs.py,3
vllm/ray/lazy_utils.py,3
tests/models/decoder_only/vision_language/test_fuyu.py,3
tests/models/decoder_only/vision_language/mm_processor_kwargs/test_llava_next.py,3
tests/distributed/test_node_count.py,3
docs/source/deployment/frameworks/skypilot.md,3
docs/source/quantization/fp8_e4m3_kvcache.rst,3
docs/source/dev/dockerfile/dockerfile.rst,3
examples/minicpmv_example.py,3
docs/features/disagg_encoder.md,3
tests/test_vllm_port.py,3
vllm/model_executor/models/minimax_cache.py,3
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml,3
docs/cli/bench/sweep/serve_sla.md,3
docs/contributing/dockerfile/dockerfile.md,3
.buildkite/lm-eval-harness/conftest.py,3
tests/utils_/test_async_utils.py,3
tests/kernels/quantization/test_flashinfer_scaled_mm.py,3
csrc/punica/bgmv/bgmv_bf16_fp32_bf16.cu,3
csrc/core/batch_invariant.hpp,3
csrc/quantization/gptq/matrix_view.cuh,3
tests/entrypoints/pooling/correctness/test_mteb_score.py,3
vllm/transformers_utils/configs/deepseek_v3.py,3
vllm/lora/lora_model.py,3
csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.h,3
CODE_OF_CONDUCT.md,3
examples/offline_inference/logits_processor/custom_req_init.py,3
tests/v1/test_stats.py,3
tests/v1/engine/test_preprocess_error_handling.py,3
vllm/transformers_utils/configs/moonvit.py,3
docs/source/deployment/frameworks/helm.md,3
csrc/quantization/vectorization.cuh,3
docs/design/p2p_nccl_connector.md,3
vllm/worker/multi_step_hpu_worker.py,3
tests/kernels/quantization/test_awq.py,3
tests/kernels/test_rotary_embedding.py,3
vllm/distributed/__init__.py,3
vllm/inputs.py,3
docs/configuration/model_resolution.md,3
examples/openai_chat_embedding_client_for_multimodal.py,3
tests/models/test_bart.py,3
docs/source/serving/deploying_with_cerebrium.rst,3
tests/utils_/test_func_utils.py,3
tests/entrypoints/llm/test_init.py,3
tests/models/encoder_decoder/vision_language/test_broadcast.py,3
examples/simple_fastapi_client.py,3
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int.py,3
vllm/v1/worker/gpu/dp_utils.py,3
vllm/compilation/base_piecewise_backend.py,3
tests/evals/gsm8k/configs/moe-refactor/config-h100.txt,3
examples/other/tensorize_vllm_model.py,3
vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/__init__.py,3
cacheflow/entrypoints/fastapi_server.py,3
tests/entrypoints/openai/rpc/test_zmq_client.py,3
benchmarks/pyproject.toml,3
tests/model_executor/model_loader/tensorizer_loader/test_tensorizer.py,3
tests/v1/core/test_reset_prefix_cache_e2e.py,3
.buildkite/test_areas/tool_use.yaml,3
csrc/cutlass_extensions/vllm_collective_builder.cuh,3
tests/worker/conftest.py,3
vllm/transformers_utils/configs/minimax_text_01.py,3
tests/kernels/attention/test_pack_unpack_triton.py,3
tests/models/language/pooling_mteb_test/mteb_score_utils.py,3
docs/models/hardware_supported_models/cpu.md,3
docs/design/v1/prefix_caching.md,3
examples/online_serving/run_cluster.sh,3
.buildkite/scripts/tpu/config_v6e_1.env,3
docs/source/features/quantization/int4.md,3
examples/pooling/score/convert_model_to_seq_cls.py,3
examples/offline_inference/basic/basic.py,3
tests/detokenizer/test_stop_reason.py,3
tests/kernels/test_machete_mm.py,3
benchmarks/structured_schemas/structured_schema_1.json,3
plot/plot_normalized_latency.py,3
.buildkite/nightly-benchmarks/tests/latency-tests-cpu.json,3
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh,3
tests/neuron/1_core/test_rotary_embedding.py,3
vllm/utils/cache.py,3
vllm/utils/network_utils.py,3
docs/getting_started/installation/cpu.apple.inc.md,3
.buildkite/test_areas/misc.yaml,3
docs/cli/serve.md,3
csrc/cpu/cpu_types_vxe.hpp,3
cmake/external_projects/qutlass.cmake,3
.buildkite/lm-eval-harness/configs/DeepSeek-V2-Lite-Chat.yaml,3
vllm/triton_utils/libentry.py,3
tests/entrypoints/openai/tool_parsers/test_gigachat3_tool_parser.py,3
docs/features/custom_arguments.md,3
cacheflow/master/frontend.py,3
docs/cli/chat.md,3
tests/models/language/generation/test_granite.py,3
vllm/v1/worker/gpu/spec_decode/rejection_sample.py,3
csrc/punica/bgmv/bgmv_bf16_bf16_bf16.cu,3
vllm/_bc_linter.py,3
csrc/cpu/activation.cpp,3
tests/entrypoints/test_harmony_utils.py,3
vllm/transformers_utils/configs/dotsocr.py,3
tests/models/decoder_only/language/test_gptq_marlin_24.py,3
examples/offline_inference/pooling/ner.py,3
csrc/quantization/fp8_e5m2_kvcache/quant_utils.cuh,3
vllm/distributed/ec_transfer/ec_connector/factory.py,3
docs/source/serving/deploying_with_k8s.rst,3
tests/models/language/pooling/test_cross_encoder.py,3
tests/kv_transfer/test_disagg.py,3
examples/openai_api_client_for_multimodal.py,3
tests/fastsafetensors_loader/test_fastsafetensors_loader.py,3
benchmarks/auto_tune.sh,3
tests/evals/gsm8k/conftest.py,3
tests/kernels/quantization/test_cutlass_w4a8_moe.py,3
vllm/model_executor/layers/fused_moe/router/custom_routing_router.py,3
csrc/moe/marlin_moe_wna16/.gitignore,3
csrc/cutlass_extensions/torch_utils.hpp,3
examples/online_serving/utils.py,3
tests/async_engine/conftest.py,3
tests/distributed/test_nccl_symm_mem_allreduce.py,3
tests/kv_transfer/disagg_test.py,3
examples/offline_inference/cpu_offload_lmcache.py,3
docs/source/quantization/int8.rst,3
.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-W8A16-compressed-tensors.yaml,3
vllm/v1/engine/exceptions.py,3
tests/tools/test_config_validator.py,3
examples/offline_inference_mlpspeculator.py,3
docs/deployment/frameworks/lobe-chat.md,3
vllm/v1/attention/backends/flash_attn_diffkv.py,3
.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.yaml,3
tests/models/language/generation/test_granitemoehybrid.py,3
vllm/model_executor/layers/fused_moe.py,3
tests/models/test_ultravox.py,3
vllm/distributed/device_communicators/neuron_communicator.py,3
tests/evals/gpt_oss/test_gpqa_correctness.py,3
.github/workflows/scripts/create_release.js,3
vllm/model_executor/models/mistral_large_3_eagle.py,3
vllm/plugins/lora_resolvers/README.md,3
tests/entrypoints/pooling/openai/test_pooling.py,3
csrc/sparse/cutlass/sparse_compressor_c3x.cu,3
tests/neuron/1_core/test_neuron_quant.py,3
docs/source/getting_started/gaudi-installation.rst,3
csrc/punica/bgmv/bgmv_fp32_bf16_bf16.cu,3
vllm/benchmarks/utils.py,3
tests/reasoning/test_glm4_moe_reasoning_parser.py,3
tests/kv_transfer/test_module.py,3
tests/models/language/pooling_mteb_test/test_intfloat.py,3
tools/vllm-tpu/build.sh,3
tests/test_logprobs.py,3
examples/offline_inference/torchrun_dp_example.py,3
tests/neuron/2_core/test_eagle.py,3
tests/models/decoder_only/vision_language/test_llava_next_video.py,3
examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py,3
csrc/quantization/gptq_marlin/marlin.cuh,3
docs/contributing/deprecation_policy.md,3
tests/models/decoder_only/vision_language/test_minicpmv.py,3
.buildkite/scripts/tpu/quantized_v6e_1.env,3
examples/logging_configuration.md,3
examples/online_serving/retrieval_augmented_generation_with_langchain.py,3
vllm/entrypoints/pooling/__init__.py,3
csrc/punica/bgmv/bgmv_fp16_fp16_fp16.cu,3
csrc/type_convert.cuh,3
examples/offline_inference/basic/README.md,3
docs/cli/complete.md,3
vllm/model_executor/models/transformers/causal.py,3
tests/entrypoints/openai/test_protocol.py,3
vllm/vllm_flash_attn/__init__.py,3
tests/v1/generation/test_rms_norm_batch_invariant.py,3
vllm/transformers_utils/configs/starcoder2.py,3
vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope.py,3
csrc/attention/attention_dtypes.h,3
vllm/model_executor/layers/quantization/kernels/__init__.py,3
csrc/sparse/cutlass/sparse_compressor_entry.cu,3
vllm/model_executor/layers/quantization/kernels/machete.py,3
examples/tool_chat_template_phi4_mini.jinja,3
examples/offline_inference/prompt_embed_inference.py,3
vllm/entrypoints/pooling/classify/api_router.py,3
vllm/lora/ops/triton_ops/README_TUNING.md,3
tests/quantization/test_autogptq_marlin_configs.py,3
tests/reasoning/test_olmo3_reasoning_parser.py,3
tests/entrypoints/openai/test_serving_chat_stream_harmony.py,3
Dockerfile.rocm_base,3
examples/offline_inference/gguf_inference.py,3
vllm/model_executor/layers/fla/ops/wy_fast.py,3
tools/ep_kernels/configure_system_drivers.sh,3
.buildkite/nightly-benchmarks/tests/throughput-tests-cpu.json,3
tests/entrypoints/test_ssl_cert_refresher.py,3
csrc/punica/bgmv/bgmv_fp32_fp16_fp16.cu,3
tests/config/test_multimodal_config.py,3
docs/cli/bench/latency.md,3
tests/tokenization/test_do_lower_case.py,3
docs/source/models/adding_model.md,3
docs/source/usage/multimodal_inputs.md,3
vllm/entrypoints/openai/tool_parsers/ernie45_tool_parser.py,3
vllm/distributed/eplb/policy/default.py,3
tests/utils_/test_torch_utils.py,3
docs/source/assets/contributing/dockerfile-stages-dependency.png,3
tests/entrypoints/pooling/openai/test_vision_embedding.py,3
tests/distributed/test_parallel_state.py,3
tests/models/decoder_only/language/test_gptq_marlin.py,3
csrc/quantization/gptq/qdq_4.cuh,3
examples/online_serving/chart-helm/values.yaml,3
.buildkite/nightly-benchmarks/scripts/launch-server.sh,3
csrc/quantization/machete/machete_prepack_kernel.cuh,3
vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils.py,3
tests/neuron/1_core/test_cache.py,3
docs/source/serving/faq.rst,3
docs/source/_templates/sections/header.html,3
vllm/entrypoints/pooling/score/api_router.py,3
examples/production_monitoring/Otel.md,3
tests/models/encoder_decoder/audio_language/test_whisper.py,3
.github/workflows/cancel_fastcheck_when_ready.yml,3
tests/entrypoints/openai/test_vision_embeds.py,3
tests/v1/kv_connector/unit/test_lmcache_integration.py,3
tests/entrypoints/pooling/openai/test_score.py,3
vllm/model_executor/layers/ops/rand.py,3
csrc/moe/dynamic_4bit_int_moe_cpu.cpp,3
tests/core/conftest.py,3
tests/kernels/attention/test_cpu_attn.py,3
vllm/lora/punica_wrapper/__init__.py,3
tests/v1/shutdown/test_processor_error.py,3
vllm/transformers_utils/configs/minimax_vl_01.py,3
docs/source/getting_started/installation/cpu/index.md,3
tests/v1/worker/test_gpu_profiler.py,3
csrc/layernorm.cpp,3
vllm/v1/sample/ops/logprobs.py,3
examples/template_florence2.jinja,3
tests/reasoning/test_granite_reasoning_parser.py,3
examples/online_serving/retrieval_augmented_generation_with_llamaindex.py,3
tests/entrypoints/pooling/openai/test_embedding_dimensions.py,3
tests/models/quantization/test_gptq_bitblas.py,3
cacheflow/decoding.py,3
tests/kernels/test_semi_structured.py,3
tests/kv_transfer/test_lookup_buffer.sh,3
csrc/quantization/gptq_allspark/allspark_repack.cu,3
vllm/distributed/kv_transfer/kv_connector_agent.py,3
tests/entrypoints/pooling/embed/test_offline.py,3
tests/kernels/test_cutlass_moe.py,3
tests/neuron/1_core/test_neuron_model_runner.py,3
tests/kernels/attention/conftest.py,3
.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh,3
csrc/punica/bgmv/vec_dtypes.cuh,3
tests/v1/attention/test_attention_backends_selection.py,3
tests/model_executor/model_loader/fastsafetensors_loader/test_weight_utils.py,3
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",3
csrc/quantization/vectorization_utils.cuh,3
benchmarks/kernels/deepgemm/README.md,3
tests/v1/attention/test_mamba_selectors.py,3
cacheflow/server/tokenizer_utils.py,3
csrc/quantization.cpp,3
vllm/entrypoints/pooling/base/protocol.py,3
tests/config/test_config_generation.py,3
tools/flashinfer-build.sh,3
vllm/v1/worker/gpu/sample/min_p.py,3
examples/online_serving/openai_embedding_matryoshka_fy.py,3
tests/entrypoints/pooling/openai/test_embedding_long_text.py,3
examples/openai_embedding_client.py,3
vllm/model_executor/layers/quantization/qutlass_utils.py,3
vllm/v1/core/specialized_manager.py,3
docs/contributing/ci-failures.md,3
tests/entrypoints/test_openai_chat.py,3
tests/models/decoder_only/language/test_phimoe.py,3
docs/cli/bench/sweep/serve.md,3
tests/compile/fullgraph/test_basic_correctness.py,3
examples/offline_inference/automatic_prefix_caching.py,3
.buildkite/test_areas/attention.yaml,3
vllm/model_executor/layers/pooler/seqwise/heads.py,3
tests/entrypoints/openai/test_lora_lineage.py,3
docs/examples/README.md,3
examples/online_serving/dashboards/perses/performance_statistics.yaml,3
docs/source/performance_benchmark/benchmarks.rst,3
csrc/cpu/layernorm.cpp,3
vllm/model_executor/layers/quantization/utils/marlin_24_perms.py,3
examples/offline_inference/pixtral.py,3
tests/entrypoints/openai/test_messages.py,3
vllm/model_executor/models/na_vit.py,3
tests/v1/kv_offload/test_cpu_manager.py,3
csrc/punica/punica_ops.h,3
tests/engine/test_stop_checker.py,3
mypy.ini,3
vllm/model_executor/layers/quantization/kernels/mixed_precision/conch.py,3
vllm/transformers_utils/configs/lfm2_moe.py,3
tests/kernels/attention/test_flashinfer_trtllm_decode_attention.py,3
docs/source/serving/deploying_with_k8s.md,3
tests/entrypoints/openai/reasoning_parsers/utils.py,3
tests/compile/conftest.py,3
vllm/v1/worker/ec_connector_model_runner_mixin.py,3
docs/source/serving/tensorizer.rst,3
tests/models/language/pooling_mteb_test/test_snowflake_arctic_embed.py,3
vllm/config/profiler.py,3
docs/source/dev/engine/llm_engine.rst,3
docs/source/usage/tool_calling.md,3
tests/v1/test_kv_sharing.py,3
tests/entrypoints/pooling/correctness/test_mteb_embed.py,3
vllm/v1/engine/async_stream.py,3
cacheflow/parallel_utils/tensor_parallel/layers.py,3
examples/production_monitoring/README.md,3
tests/models/decoder_only/vision_language/test_paligemma.py,3
tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh,3
docs/source/features/quantization/fp8_e4m3_kvcache.md,3
tests/lora/test_gptoss.py,3
tests/models/language/pooling_mteb_test/test_nomic.py,3
csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.h,3
vllm/entrypoints/serve/__init__.py,3
vllm/model_executor/layers/quantization/utils/marlin_perms.py,3
.buildkite/lm-eval-harness/run-tests.sh,3
examples/fp8/quantizer/README.md,3
tests/compile/fullgraph/test_full_cudagraph.py,3
docs/design/automatic_prefix_caching.md,3
docs/source/getting_started/installation/ai_accelerator/index.md,3
tests/benchmarks/sweep/test_serve_sla.py,3
tests/models/test_blip2.py,3
examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py,3
csrc/quantization/fp8/fp8_cuda_kernels.cu,3
vllm/entrypoints/openai/chat_completion/api_router.py,3
tests/entrypoints/pooling/llm/test_reward.py,3
docs/source/serving/metrics.rst,3
.buildkite/run-multi-node-test.sh,3
vllm/entrypoints/openai/responses/api_router.py,3
.github/ISSUE_TEMPLATE/500-feature request.yml,3
vllm/utils/hashing.py,3
vllm/triton_utils/custom_cache_manager.py,3
tests/kernels/moe/test_silu_mul_per_token_group_quant_fp8_colmajor.py,3
tests/models/decoder_only/vision_language/test_intern_vit.py,3
tests/v1/test_request.py,3
vllm/model_executor/model_loader/online_quantization.py,3
vllm/entrypoints/serve/disagg/serving.py,3
vllm/model_executor/layers/quantization/inc.py,3
examples/offline_inference/logits_processor/custom_req.py,3
requirements/neuron.txt,3
tests/models/decoder_only/language/test_modelopt.py,3
tests/models/quantization/test_bitblas.py,3
tests/standalone_tests/lazy_imports.py,3
tests/kernels/test_rand.py,3
vllm/model_executor/models/transformers/legacy.py,3
docs/source/usage/spec_decode.md,3
tests/v1/kv_connector/unit/test_config.py,3
tests/v1/core/test_kv_sharing.py,3
tests/entrypoints/llm/test_score.py,3
tests/detokenizer/test_stop_checker.py,3
vllm/worker/spec_decode/multi_step_worker.py,3
.buildkite/download-images.sh,3
vllm/transformers_utils/configs/aria.py,3
docs/source/getting_started/installation/hpu-gaudi.md,3
tests/entrypoints/openai/test_enable_force_include_usage.py,3
csrc/moe/moe_ops.cpp,3
tests/kernels/attention/test_flashinfer_mla_decode.py,3
vllm/model_executor/models/hunyuan_v1_moe.py,3
examples/openai_chatcompletion_client.py,3
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8-compressed-tensors.yaml,3
cacheflow/models/gpt_neox.py,3
tests/entrypoints/pooling/openai/test_classification.py,3
vllm/model_executor/models/exaone_moe.py,3
docs/source/quantization/fp8_e5m2_kvcache.rst,3
tests/v1/kv_connector/unit/test_kv_load_failure_recovery.py,3
vllm/model_executor/guided_decoding/reasoner/deepseek_reasoner.py,3
tests/benchmarks/test_throughput_cli.py,3
vllm/model_executor/layers/fused_moe/fallback.py,3
vllm/tokenizers/hf.py,3
tests/test_version.py,3
docs/source/getting_started/cpu-installation.md,3
vllm/executor/hpu_executor.py,3
tests/models/language/pooling/test_st_projector.py,3
examples/other/logging_configuration.md,3
tests/kernels/core/test_rotary_embedding_mla_cache_fused.py,3
vllm/v1/worker/gpu/sample/logit_bias.py,3
tests/models/test_compressed_tensors.py,3
tests/models/multimodal/pooling/test_clip.py,3
docs/source/serving/deploying_with_kserve.rst,3
examples/online_serving/elastic_ep/serve_deepseek_v2.sh,3
vllm/v1/kv_offload/abstract.py,3
.github/workflows/shellcheck.yml,3
docs/configuration/env_vars.md,3
tests/model_executor/model_loader/test_registry.py,3
tests/neuron/1_core/test_block_table.py,3
tests/detokenizer/conftest.py,3
examples/paligemma_example.py,3
tests/data/test_config.yaml,3
vllm/model_executor/models/jais2.py,3
vllm/transformers_utils/configs/olmo3.py,3
vllm/model_executor/models/eagle2_5_vl.py,3
tests/entrypoints/openai/test_mp_api_server.py,3
vllm/model_executor/layers/pooler/tokwise/heads.py,3
vllm/model_executor/guided_decoding/reasoner/reasoner.py,3
tests/models/quantization/test_modelopt.py,3
vllm/config/kv_events.py,3
docs/source/contributing/vulnerability_management.md,3
csrc/cpu/cpu_types_vsx.hpp,3
vllm/config/speech_to_text.py,3
tests/models/test_chameleon.py,3
vllm/benchmarks/sweep/cli.py,3
tests/models/language/pooling/test_extract_hidden_states.py,3
.github/ISSUE_TEMPLATE/700-performance-discussion.yml,3
examples/pooling/pooling/vision_language_pooling.py,3
vllm/transformers_utils/configs/speculators/algos.py,3
tests/kernels/quantization/test_awq_marlin.py,2
tests/v1/ec_connector/integration/run_epd_correctness_test.sh,2
.buildkite/nightly-benchmarks/sample.yaml,2
examples/tool_chat_template_llama3.1_json.jinja,2
playground/streaming_fastapi_worker.py,2
examples/openai_audio_api_client.py,2
plot/plot_stats.py,2
cacheflow/worker/models/model_utils.py,2
csrc/quantization/fp8/amd/hip_float8.h,2
vllm/distributed/device_communicators/mnnvl_compat.py,2
docs/source/api/model/adapters.md,2
vllm/model_executor/models/motif.py,2
vllm/entrypoints/serve/tokenize/protocol.py,2
csrc/punica/type_convert.h,2
vllm/transformers_utils/chat_templates/template_minicpmv45.jinja,2
tests/kernels/activation.py,2
vllm/model_executor/layers/mamba/ops/layernorm_gated.py,2
tests/v1/shutdown/utils.py,2
examples/offline_inference_pixtral.py,2
vllm/utils/registry.py,2
tests/entrypoints/pooling/classify/test_online_vision.py,2
vllm/entrypoints/openai/translations/speech_to_text.py,2
tests/entrypoints/openai/test_mp_crash.py,2
vllm/model_executor/layers/pooler/tokwise/methods.py,2
tests/kernels/quantization/test_scaled_mm_kernel_selection.py,2
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",2
vllm/tool_parsers/functiongemma_tool_parser.py,2
examples/offline_inference_encoder_decoder.py,2
examples/pooling/score/online_using_template.py,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass-dp-ep.yaml,2
examples/offline_inference/pooling/multi_vector_retrieval.py,2
tests/tracing/__init__.py,2
vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-trtllm.yaml,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-triton.yaml,2
tests/tool_parsers/test_mistral_tool_parser.py,2
examples/online_serving/openai_responses_client_with_tools.py,2
tests/models/test_gguf_download.py,2
vllm/v1/worker/gpu/structured_outputs.py,2
vllm/v1/sample/ops/utils.py,2
vllm/entrypoints/serve/sleep/api_router.py,2
vllm/entrypoints/grpc_server.py,2
docs/source/models/generative_models.rst,2
tests/kernels/quantization/test_mxfp4_qutlass.py,2
tests/models/fixtures/pixtral_chat.pickle,2
docker/Dockerfile.hpu,2
benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py,2
vllm/compilation/compile_context.py,2
vllm/benchmarks/sweep/server.py,2
docs/source/getting_started/arm-installation.rst,2
vllm/utils/collection_utils.py,2
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json",2
vllm/v1/executor/__init__.py,2
examples/offline_inference/kv_load_failure_recovery/prefill_example.py,2
docs/cli/bench/mm_processor.md,2
cacheflow/model_executor/parallel_utils/__init__.py,2
vllm/benchmarks/sweep/sla_sweep.py,2
examples/run_cluster.sh,2
tests/entrypoints/pooling/embed/test_correctness_mteb.py,2
tests/entrypoints/test_server_oot_registration.py,2
vllm/transformers_utils/configs/kimi_linear.py,2
docs/api/vllm/.meta.yml,2
tests/models/multimodal/generation/test_keye.py,2
tests/kernels/test_flashmla.py,2
tests/quantization/reference_mxfp4.py,2
tests/lora/test_punica_ops_sizes.py,2
csrc/punica/bgmv/bgmv_fp32_fp32_fp16.cu,2
examples/online_serving/dashboards/grafana/README.md,2
tests/models/encoder_decoder/vision_language/__init__.py,2
docs/source/assets/design/v1/prefix_caching/example-time-6.png,2
tests/entrypoints/test_llm_generate.py,2
benchmark/benchmark_attention.py,2
docs/source/features/quantization/torchao.md,2
tests/worker/__init__.py,2
docs/source/api/multimodal/profiling.md,2
vllm/distributed/kv_transfer/kv_lookup_buffer/__init__.py,2
vllm/lora/ops/triton_ops/bgmv_shrink.py,2
docs/make.bat,2
tests/core/block/e2e/__init__.py,2
tests/models/language/pooling/test_qwen3_reranker_seq_cls.py,2
vllm/model_executor/models/vlm_base.py,2
csrc/quantization/gptq/qdq_2.cuh,2
csrc/moe/moe_wna16.cu,2
tests/tool_parsers/test_functiongemma_tool_parser.py,2
vllm/logging_utils/log_time.py,2
vllm/benchmarks/startup.py,2
tests/entrypoints/anthropic/__init__.py,2
.buildkite/nightly-benchmarks/scripts/run-trt-nightly.sh,2
tests/kernels/layernorm.py,2
tests/pplx_utils.py,2
tests/evals/gsm8k/__init__.py,2
tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/__init__.py,2
vllm/tokenizers/deepseek_v32.py,2
examples/others/logging_configuration.md,2
Dockerfile.s390x,2
docs/source/getting_started/installation/cpu-apple.md,2
vllm/entrypoints/serve/instrumentator/server_info.py,2
tests/models/language/pooling/test_splade_sparse_pooler.py,2
vllm/model_executor/models/whisper_causal.py,2
tests/entrypoints/pooling/openai/test_truncation.py,2
vllm/transformers_utils/processors/hunyuan_vl_image.py,2
tests/kernels/test_fla_layernorm_guard.py,2
csrc/cpu/sgl-kernels/moe.cpp,2
tests/runai_model_streamer/test_runai_model_streamer_loader.py,2
vllm/executor/utils.py,2
docs/source/usage/compatibility_matrix.md,2
docs/source/design/multimodal/multimodal_index.rst,2
vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope.py,2
tests/kernels/quantization/test_aqlm.py,2
csrc/core/torch_bindings.cpp,2
docs/source/api/multimodal/registry.md,2
examples/offline_inference/chat.py,2
tests/lora/test_transformers_model.py,2
tests/v1/tpu/__init__.py,2
docs/source/contributing/overview.rst,2
examples/others/lmcache/disagg_prefill_lmcache_v0.py,2
tests/models/fixtures/pixtral_chat_engine.json,2
docs/source/serving/deploying_with_docker.md,2
docs/source/quantization/gguf.rst,2
vllm/utils/nccl.py,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-INT8-compressed-tensors-asym.yaml,2
vllm/lora/ops/utils.py,2
tests/v1/tpu/worker/__init__.py,2
csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_fmha_mla_tma_warpspecialized.hpp,2
docs/source/design/huggingface_integration.md,2
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json",2
tests/entrypoints/openai/test_collective_rpc.py,2
tests/models/multimodal/processing/test_qwen3_omni.py,2
csrc/punica/punica_ops.cu,2
benchmarks/kernels/benchmark_trtllm_attention.py,2
tools/install_gdrcopy.sh,2
examples/openai_completion_client.py,2
vllm/transformers_utils/configs/qwen2vl.py,2
docs/source/assets/design/v1/prefix_caching/example-time-4.png,2
vllm/entrypoints/openai/models/serving.py,2
docs/source/usage/usage_stats.md,2
vllm/entrypoints/constants.py,2
csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm100.cu,2
csrc/quantization/gptq/qdq_util.cuh,2
csrc/cuda_utils.cpp,2
tests/model_executor/model_loader/runai_model_streamer/test_runai_utils.py,2
tools/ep_kernels/install_system_drivers.sh,2
docs/source/serving/runai_model_streamer.rst,2
examples/offline_inference/kv_load_failure_recovery/README.md,2
tests/kv_transfer/module_test.py,2
docs/source/assets/design/v1/prefix_caching/example-time-5.png,2
examples/template_paligemma.jinja,2
vllm/v1/sample/tpu/__init__.py,2
tests/model_executor/model_loader/test_sharded_state_loader.py,2
vllm/utils/math_utils.py,2
docs/source/getting_started/installation/gpu-rocm.md,2
.github/FUNDING.yml,2
vllm/model_executor/layers/ops/__init__.py,2
examples/online_serving/openai_translation_client.py,2
tests/kernels/attention/test_blocksparse_attention.py,2
.coveragerc,2
docs/source/dev/engine/engine_index.rst,2
docs/source/autodoc2_docstring_parser.py,2
docs/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md,2
tests/entrypoints/pooling/embed/test_online_dimensions.py,2
tests/models/test_llava_image_embeds.py,2
vllm/model_executor/layers/quantization/kernels/MPLinearKernel.py,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-vllm-cutlass.yaml,2
vllm/model_executor/layers/rotary_embedding/llama3_rope.py,2
docs/source/api/multimodal/parse.md,2
docs/source/assets/design/v1/prefix_caching/example-time-3.png,2
vllm/executor/__init__.py,2
tests/model_executor/model_loader/runai_streamer_loader/conftest.py,2
docs/design/v1/torch_compile.md,2
.buildkite/lm-eval-harness/configs/Mixtral-8x22B-Instruct-v0.1-FP8-Dynamic.yaml,2
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI308X.json",2
tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json,2
.buildkite/scripts/upload-release-wheels.sh,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-fi-cutlass.yaml,2
csrc/cpu/scratchpad_manager.cpp,2
tests/models/encoder_decoder/language/__init__.py,2
.buildkite/test_areas/lm_eval.yaml,2
tests/prefix_caching/__init__.py,2
docs/source/design/arch_overview.rst,2
vllm/model_executor/layers/attention/encoder_only_attention.py,2
tests/v1/determinism/conftest.py,2
tests/kernels/attention/test_triton_prefill_attention.py,2
vllm/entrypoints/serve/profile/api_router.py,2
docs/source/serving/serve_args.md,2
vllm/core/block/__init__.py,2
vllm/benchmarks/lib/utils.py,2
benchmarks/kv_cache/benchmark_block_pool.py,2
tests/lora/test_lora.py,2
vllm/model_executor/parallel_utils/__init__.py,2
tests/v1/kv_connector/kv_load_exception_handling/random_drop_connector.py,2
tools/doc-lint.sh,2
tests/compile/test_compile_ranges.py,2
tests/v1/kv_connector/kv_load_exception_handling/test.sh,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-marlin.yaml,2
csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.cu,2
docs/source/getting_started/xpu-installation.md,2
tests/compile/test_full_graph_multi_gpu.py,2
tests/neuron/1_core/test_layernorm.py,2
tests/models/fixtures/pixtral_chat_engine.pickle,2
examples/template_pixtral_hf.jinja,2
vllm/utils/func.py,2
.buildkite/lm-eval-harness/run-lm-eval-chartqa-vllm-vlm-baseline.sh,2
tests/entrypoints/test_openai_embedding.py,2
tests/mistral_tool_use/test_mistral_tool_calls.py,2
vllm/jsontree.py,2
.buildkite/test_areas/lora.yaml,2
tests/engine/test_multi_step_output_processor.py,2
docs/source/design/multimodal/adding_multimodal_plugin.md,2
vllm/logging/__init__.py,2
examples/offline_inference/basic/reward.py,2
csrc/quantization/hadamard/hadacore/hadamard_transform_cuda.cu,2
docs/source/serving/runai_model_streamer.md,2
vllm/model_executor/layers/attention/mm_encoder_attention.py,2
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI325X.json",2
tests/models/decoder_only/language/test_granitemoe.py,2
vllm/attention/layers/static_sink_attention.py,2
benchmarks/multi_turn/convert_sharegpt_to_openai.py,2
tests/v1/engine/test_detokenizer.py,2
tests/tool_parsers/test_kimi_k2_tool_parser.py,2
examples/offline_inference/openai/openai_batch.md,2
docs/source/models/structured_outputs.rst,2
csrc/cutlass_extensions/gemm/collective/fp8_accumulation.hpp,2
vllm/model_executor/layers/pooler/special.py,2
vllm/entrypoints/openai/serving_tokens.py,2
docs/source/quantization/bnb.md,2
tests/kernels/test_permute_cols.py,2
tests/entrypoints/llm/test_mm_cache_stats.py,2
cacheflow/parallel_utils/parallel_state.py,2
tests/neuron/1_core/test_logits_processor.py,2
vllm/model_executor/layers/quantization/cpu_wna16.py,2
csrc/cutlass_extensions/gemm/collective/collective_builder.hpp,2
tests/spec_decode/e2e/__init__.py,2
tests/model_executor/test_logits_processor.py,2
.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-INT8-compressed-tensors.yaml,2
csrc/quantization/marlin/sparse/common/base.h,2
docs/getting_started/installation/ai_accelerator.md,2
.buildkite/scripts/run-multi-node-test.sh,2
vllm/attention/ops/triton_prefill_attention.py,2
examples/offline_inference/kv_load_failure_recovery/decode_example.py,2
vllm/model_executor/layers/fla/ops/index.py,2
tests/kernels/test_gptq.py,2
examples/offline_inference/kv_load_failure_recovery/rogue_shared_storage_connector.py,2
tests/entrypoints/openai/tool_parsers/conftest.py,2
vllm/model_executor/layers/quantized_linear/squeezellm.py,2
benchmarks/multi_turn/generate_multi_turn.json,2
tests/v1/structured_output/test_gptoss_structural_tags.py,2
vllm/config/renderer.py,2
benchmarks/kernels/bench_int8_gemm.py,2
.buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh,2
tests/evals/gsm8k/configs/Qwen3-0.6B-FP8.yaml,2
docs/source/api/model/interfaces.md,2
vllm/model_executor/models/step_vl.py,2
examples/offline_inference/logits_processor.py,2
examples/offline_inference/reproduciblity.py,2
csrc/launch_bounds_utils.h,2
tests/v1/sample/test_logits_processors.py,2
docs/source/quantization/fp8_e5m2_kv_cache.rst,2
requirements-adag.txt,2
docs/source/deployment/integrations/production-stack.md,2
csrc/cpu/cpu_wna16.cpp,2
tools/install_nixl.sh,2
.buildkite/test_areas/kernels.yaml,2
tools/actionlint.sh,2
.github/workflows/dummy.yml,2
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8.cu,2
tests/mistral_tool_use/conftest.py,2
csrc/quantization/cutlass_w8a8/moe/get_group_starts.cuh,2
docs/source/assets/design/v1/prefix_caching/example-time-7.png,2
examples/gradio_openai_chatbot_webserver.py,2
tests/entrypoints/llm/test_embedding.py,2
docs/source/performance/benchmarks.md,2
tests/models/encoder_decoder/audio_language/__init__.py,2
vllm/mocks/__init__.py,2
csrc/cuda_view.cu,2
tests/tokenizers_/test_mistral.py,2
csrc/punica/bgmv/bgmv_bf16_fp16_bf16.cu,2
.buildkite/nightly-benchmarks/tests/latency-tests.json,2
tests/neuron/test_logits_processor.py,2
examples/template_llava.jinja,2
vllm/model_executor/layers/quantization/utils/mxfp6_utils.py,2
tests/models/language/pooling/test_multilabel_classification_support.py,2
vllm/utils/async_utils.py,2
examples/online_serving/openai_embedding_long_text/README.md,2
tests/evals/gsm8k/configs/DeepSeek-V2-Lite-Instruct-FP8.yaml,2
tests/lora/test_tokenizer.py,2
.github/ISSUE_TEMPLATE/500-feature-request.yml,2
docs/source/deployment/frameworks/open-webui.md,2
docs/source/features/quantization/quark.md,2
tests/entrypoints/openai/correctness/test_mteb_embed.py,2
tests/models/decoder_only/vision_language/test_llava_next.py,2
vllm/benchmarks/serve_multi.py,2
tests/models/embedding/language/test_jina.py,2
.buildkite/scripts/tpu/cleanup_docker.sh,2
tests/entrypoints/test_utils.py,2
.buildkite/nightly-benchmarks/scripts/run-tgi-nightly.sh,2
examples/tool_chat_template_llama3.2_pythonic.jinja,2
tests/model_executor/test_eagle_quantization.py,2
examples/template_deepseek_vl2.jinja,2
vllm/lora/ops/triton_ops/bgmv_expand_slice.py,2
tests/compile/test_full_graph_smoke.py,2
tests/model_executor/test_weight_utils.py,2
examples/offline_inference/basic_with_model_default_sampling.py,2
benchmarks/multi_turn/bench_utils.py,2
csrc/cpu/cpu_attn_macros.h,2
.buildkite/nightly-benchmarks/tests/descriptions.md,2
csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_int8_dispatch.cuh,2
tests/lora/data/__init__.py,2
tests/kernels/moe/test_routing.py,2
tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_custom_ops.py,2
vllm/entrypoints/pooling/score/utils.py,2
tests/kernels/quantization/test_allspark_gemm.py,2
tests/tpu/lora/test_pallas_kernels.py,2
tools/ep_kernels/install_system_libraries.sh,2
.buildkite/performance-benchmarks/tests/serving-tests-cpu.json,2
docs/getting_started/installation/cpu.s390x.inc.md,2
docs/source/serving/deploying_with_triton.rst,2
vllm/core/evictor_v1.py,2
examples/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh,2
tests/neuron/1_core/test_activation.py,2
docs/source/getting_started/arm-installation.md,2
.buildkite/nightly-benchmarks/scripts/run-vllm-nightly.sh,2
.buildkite/lm-eval-harness/configs/Mixtral-8x7B-Instruct-v0.1.yaml,2
examples/pooling/score/qwen3_reranker_online.py,2
.github/ISSUE_TEMPLATE/800-misc discussion.yml,2
tests/v1/e2e/test_async_spec_decode.py,2
vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py,2
examples/tool_chat_template_hermes.jinja,2
vllm/model_executor/layers/fla/ops/kda.py,2
examples/offline_inference/dolphin.py,2
cacheflow/model_executor/memory_analyzer.py,2
tests/runai_model_streamer/test_weight_utils.py,2
docs/source/getting_started/installation/ai_accelerator.md,2
docs/benchmarking/cli.md,2
examples/offline_inference/cli.py,2
.github/workflows/add_label_ready_comment.yml,2
tests/lora/test_qwen3moe_tp.py,2
csrc/quantization/marlin/dense/common/base.h,2
csrc/quantization/gptq/compat.cuh,2
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",2
vllm/lora/ops/xla_ops/pallas.py,2
docs/source/contributing/model/tests.md,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-marlin.yaml,2
examples/online_serving/openai_cross_encoder_score_for_multimodal.py,2
vllm/transformers_utils/processors/bagel.py,2
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",2
tests/models/test_gguf.py,2
examples/tool_chat_template_mistral_parallel.jinja,2
tests/neuron/2_core/test_comm_ops.py,2
vllm/v1/attention/ops/triton_unified_attention.py,2
csrc/punica/punica_pybind.cpp,2
tests/distributed/test_torchrun_example_moe.py,2
examples/offline_inference/embedding.py,2
csrc/cpu/sgl-kernels/vec.h,2
.buildkite/lm-eval-harness/configs/models-large-h100.txt,2
benchmarks/kernels/benchmark_polynorm.py,2
tests/evals/gsm8k/configs/Qwen3-30B-A3B-NVFP4.yaml,2
tests/evals/gsm8k/configs/Llama-3-8B-Instruct-nonuniform-CT.yaml,2
docs/source/getting_started/installation/xpu.md,2
cacheflow/worker/models/__init__.py,2
vllm/mocks/mock_nixl_connector.py,2
csrc/punica/bgmv/bgmv_bf16_fp32_fp16.cu,2
vllm/model_executor/parallel_utils/utils.py,2
docs/design/lora_resolver_plugins.md,2
.buildkite/lm-eval-harness/configs/Qwen2-57B-A14-Instruct.yaml,2
tools/vllm-rocm/pin_rocm_dependencies.py,2
csrc/cpu/cpu_types_scalar.hpp,2
docs/design/torch_compile_multimodal.md,2
docs/getting_started/installation/ai_accelerator/tpu.inc.md,2
.buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep.sh,2
cacheflow/model_executor/models/__init__.py,2
tests/reasoning/test_ernie45_reasoning_parser.py,2
csrc/cpu/micro_gemm/cpu_micro_gemm_amx.hpp,2
"vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json",2
csrc/quantization/gptq_marlin/gptq_marlin_dtypes.cuh,2
vllm/profiler/wrapper.py,2
vllm/compilation/post_cleanup.py,2
vllm/v1/worker/workspace.py,2
csrc/punica/bgmv/bgmv_fp16_fp16_bf16.cu,2
vllm/multimodal/processing/context.py,2
csrc/punica/bgmv/bgmv_bf16_bf16_fp16.cu,2
tests/v1/e2e/test_lora_with_spec_decode.py,2
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json",2
tests/core/block/__init__.py,2
vllm/transformers_utils/configs/afmoe.py,2
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",2
tests/plugins/prithvi_io_processor_plugin/setup.py,2
tests/cuda/test_cuda_context.py,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct-FBGEMM-nonuniform.yaml,2
csrc/quantization/machete/Readme.md,2
docs/contributing/ci/failures.md,2
vllm/v1/stats/__init__.py,2
.buildkite/nightly-benchmarks/tests/throughput-tests.json,2
tools/profiler/nsys_profile_tools/README.md,2
.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8-MM.yaml,2
docs/source/design/input_processing/model_inputs_index.md,2
examples/openai_chat_completion_client_for_multimodal.py,2
tests/models/decoder_only/language/test_aqlm.py,2
examples/tool_chat_template_toolace.jinja,2
tests/evals/gpt_oss/conftest.py,2
tests/v1/metrics/test_metrics_reader.py,2
examples/offline_inference/cpu_offload.py,2
vllm/tool_parsers/kimi_k2_tool_parser.py,2
tests/entrypoints/openai/test_uds.py,2
assets/figures/perf_a10g_n1.png,2
tests/fastsafetensors_loader/test_weight_utils.py,2
assets/figures/perf_a100_n1.png,2
.buildkite/nightly-benchmarks/scripts/launch-trt-server.sh,2
vllm/v1/worker/gpu/sample/logprob.py,2
docker/versions.json,2
tests/models/multimodal/processing/test_gemma3.py,2
cacheflow/parallel_utils/__init__.py,2
docs/usage/faq.md,2
examples/offline_inference_chat.py,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FBGEMM-nonuniform.yaml,2
.buildkite/test_areas/pytorch.yaml,2
csrc/punica/bgmv/bgmv_fp16_fp32_bf16.cu,2
vllm/distributed/kv_transfer/kv_transfer_agent.py,2
vllm/server/launch.py,2
vllm/transformers_utils/configs/granite.py,2
docs/source/getting_started/installation/cpu-arm.md,2
csrc/punica/bgmv/bgmv_fp16_bf16_fp16.cu,2
vllm/model_executor/layers/pooler/common.py,2
tests/tokenizers_/test_detokenize.py,2
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ll.yaml,2
docs/source/serving/deploying_with_nginx.rst,2
rocm_patch/rocm_bf16.patch,2
docs/deployment/integrations/llmaz.md,2
tests/tokenization/__init__.py,2
tests/entrypoints/test_grpc_server.py,2
.github/ISSUE_TEMPLATE/600-new model.yml,2
csrc/cpu/micro_gemm/cpu_micro_gemm_vec.hpp,2
tests/{data => config}/test_config.yaml,2
vllm/lora/ops/triton_ops/v1/__init__.py,2
csrc/prepare_inputs/advance_step.cuh,2
tests/benchmarks/test_param_sweep.py,2
vllm/model_executor/layers/attention/chunked_local_attention.py,2
docs/source/getting_started/amd-installation.md,2
examples/tool_chat_template_llama4_pythonic.jinja,2
vllm/v1/worker/gpu/spec_decode/__init__.py,2
docs/source/models/extensions/fastsafetensor.md,2
vllm/attention/ops/rocm_aiter_mla_sparse.py,2
vllm/entrypoints/anthropic/__init__.py,2
vllm/v1/structured_output/grammar.py,2
docs/cli/bench/sweep/plot_pareto.md,2
.buildkite/scripts/run-prime-rl-test.sh,2
csrc/moe/moe_wna16_utils.h,2
vllm/distributed/kv_transfer/kv_pipe/__init__.py,2
examples/offline_profile.py,2
playground/http_client.py,2
vllm/model_executor/models/vision_siglip_navit.py,2
docs/source/deployment/frameworks/cerebrium.md,2
csrc/punica/bgmv/bgmv_fp32_fp32_bf16.cu,2
requirements/molmo.txt,2
vllm/distributed/eplb/policy/abstract.py,2
tests/entrypoints/openai/test_sparse_tensor_validation.py,2
csrc/moe/permute_unpermute_kernels/dispatch.h,2
vllm/model_executor/layers/fused_moe/router/fused_topk_router.py,2
docs/contributing/vulnerability_management.md,2
tests/v1/kv_offload/test_worker.py,2
tests/evals/gsm8k/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml,2
vllm/spec_decode/__init__.py,2
tests/models/multimodal/generation/conftest.py,2
vllm/distributed/ec_transfer/ec_transfer_state.py,2
vllm/triton_utils/sample.py,2
examples/fuyu_example.py,2
csrc/quantization/marlin/dense/common/mem.h,2
assets/figures/perf_a10g_n3.png,2
docs/source/api/engine/index.md,2
docs/source/dev/kernel/paged_attention.rst,2
vllm/transformers_utils/processors/hunyuan_vl.py,2
docs/source/training/rlhf.md,2
tests/entrypoints/openai/utils.py,2
docs/source/assets/design/v1/prefix_caching/example-time-1.png,2
"vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",2
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16.py,2
vllm/entrypoints/openai/tool_parsers/olmo3_tool_parser.py,2
docs/governance/collaboration.md,2
tests/models/decoder_only/language/test_qwen.py,2
csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.h,2
vllm/model_executor/layers/quantization/kernels/marlin.py,2
examples/offline_inference/arctic.py,2
docs/source/api/offline_inference/index.md,2
vllm/engine/output_processor/__init__.py,2
vllm/v1/kv_offload/backend.py,2
tests/entrypoints/pooling/score/test_correctness_mteb.py,2
csrc/cpu/cpu_attn_amx.hpp,2
tools/ep_kernels/elastic_ep/install_eep_libraries.sh,2
csrc/quickreduce/quick_reduce_impl.cuh,2
cacheflow/model_executor/parallel_utils/tensor_parallel/layers.py,2
examples/online_serving/disaggregated_encoder/disagg_1e1pd_example.sh,2
docs/source/_static/custom.css,2
tests/model_executor/model_loader/fastsafetensors_loader/test_fastsafetensors_loader.py,2
docs/source/serving/deploying_with_kubeai.rst,2
typos.toml,2
tests/utils_/test_gc_utils.py,2
csrc/cub_helpers.h,2
tests/models/decoder_only/vision_language/test_glm4.py,2
Dockerfile.arm,2
tests/kernels/test_triton_decode_attention.py,2
docs/source/getting_started/faq.md,2
docs/source/assets/kernel/v_vec.png,2
docs/source/api/summary.md,2
vllm/v1/kv_offload/worker/worker.py,2
docs/contributing/overview.md,2
csrc/cpu/sgl-kernels/gemm_int8.cpp,2
vllm/model_executor/models/flex_olmo.py,2
tests/v1/engine/test_parallel_sampling.py,2
csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm120.cu,2
.github/workflows/matchers/ruff.json,2
docs/mkdocs/hooks/generate_metrics.py,2
.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-FP8W8.yaml,2
benchmarks/multi_turn/requirements.txt,2
tests/kernels/core/test_opcheck.py,2
tests/v1/entrypoints/openai/responses/test_structured_output.py,2
examples/online_serving/disaggregated_encoder/disagg_1e1p1d_example.sh,2
csrc/punica/LICENSE,2
docs/deployment/frameworks/modal.md,2
.github/workflows/add_label_precommit.yml,2
vllm/entrypoints/openai/translations/protocol.py,2
docs/source/dev/multimodal/adding_multimodal_model.rst,2
tests/models/multimodal/conftest.py,2
docs/source/serving/deploying_with_lws.rst,2
vllm/model_executor/models/glm4_moe_lite.py,2
examples/pooling/score/vision_score_api_online.py,2
tests/detokenizer/test_stop_string_while_stop_model_terminates.py,2
tests/v1/distributed/test_internal_lb_dp.py,2
tests/kernels/test_aqlm.py,2
vllm/model_executor/layers/fused_moe/routed_experts_capturer.py,2
tests/v1/ec_connector/unit/test_ec_example_connector.py,2
tests/kernels/utils_block.py,2
.buildkite/nightly-benchmarks/scripts/plot-nightly-results.py,2
.github/workflows/remove_label_not_ready_comment.yml,2
examples/pooling/score/vision_rerank_api_online.py,2
examples/pooling/score/offline_using_template.py,2
tests/kernels/quantization/test_nvfp4_qutlass.py,2
examples/offline_inference/disaggregated-prefill-v1/run.sh,2
vllm/model_executor/models/glm4_moe_lite_mtp.py,2
examples/online_serving/openai_chat_completion_client_with_tools_xlam.py,2
docs/source/quantization/supported_hardware.md,2
vllm/model_executor/layers/fused_moe/router/base_router.py,2
docs/source/serving/serving_with_llamaindex.rst,2
examples/online_serving/structured_outputs/pyproject.toml,2
vllm/v1/kv_offload/backends/cpu.py,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-nonuniform-compressed-tensors.yaml,2
.buildkite/nightly-benchmarks/scripts/run-lmdeploy-nightly.sh,2
tests/entrypoints/openai/responses/test_function_call_parsing.py,2
tests/kernels/test_shuffle_rows.py,2
examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming.py,2
benchmark/benchmark_cache.py,2
docs/source/getting_started/neuron-installation.md,2
docs/source/automatic_prefix_caching/details.md,2
vllm/engine/tokenizer_utils.py,2
tests/tokenization/test_image_processor.py,2
docs/source/getting_started/examples/examples_index.template.md,2
docs/source/usage/compatibility_matrix.rst,2
examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh,2
docs/source/serving/env_vars.md,2
examples/online_serving/sagemaker-entrypoint.sh,2
tests/tpu/lora/__init__.py,2
examples/offline_inference/scoring.py,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct.yaml,2
docs/source/usage/lora.md,2
tests/core/block/test_block_space_manager.py,2
vllm/entrypoints/openai/chat_utils.py,2
vllm/transformers_utils/processors/ovis2.py,2
tests/quantization/test_cpu_wna16.py,2
docs/design/optimization_levels.md,2
examples/online_serving/openai_embedding_long_text/client.py,2
.buildkite/test_areas/models_basic.yaml,2
examples/offline_inference/whisper.py,2
tests/test_attention_backend_registry.py,2
docs/source/api/model/interfaces_base.md,2
tests/fp8_kv/llama2-7b-fp8-kv/kv_cache_scales.json,2
vllm/v1/attention/ops/triton_prefill_attention.py,2
tests/tool_use/mistral/utils.py,2
vllm/v1/worker/cp_utils.py,2
docs/source/serving/integrations.md,2
docs/source/dev/sampling_params.rst,2
vllm/tool_parsers/openai_tool_parser.py,2
vllm/lora/ops/triton_ops/bgmv_expand.py,2
tests/models/multimodal/generation/test_multimodal_gguf.py,2
csrc/cpu/micro_gemm/cpu_micro_gemm_impl.hpp,2
docs/source/features/automatic_prefix_caching.md,2
examples/lmcache/kv_cache_sharing_lmcache_v1.py,2
tests/entrypoints/openai/test_disable_mp.py,2
examples/online_serving/dashboards/perses/query_statistics.yaml,2
tests/lora/test_chatglm3.py,2
docs/source/getting_started/examples/examples_index.template.rst,2
tests/worker/test_worker.py,2
tests/distributed/test_pynccl_library.py,2
csrc/punica/torch_bindings.cpp,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass-dp-ep.yaml,2
tests/models/test_fp8kv_flashinfer.py,2
examples/offline_inference/aqlm_example.py,2
csrc/quantization/gptq/qdq_8.cuh,2
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-triton.yaml,2
.buildkite/lm-eval-harness/configs/Minitron-4B-Base-FP8.yaml,2
tests/utils_/test_serial_utils.py,2
examples/offline_inference/classification.py,2
vllm/transformers_utils/configs/isaac.py,2
tests/test_test.py,2
docs/deployment/frameworks/bentoml.md,2
vllm/attention/backends/mla/__init__.py,2
vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope.py,2
tests/lora/test_deepseekv2_tp.py,2
tests/entrypoints/openai/parser/test_harmony_utils.py,2
csrc/punica/bgmv/bgmv_fp32_bf16_fp16.cu,2
vllm/model_executor/layers/quantization/utils/ocp_mx_utils.py,2
tests/tensorizer_loader/tensorize_vllm_model_for_testing.py,2
tests/v1/entrypoints/openai/responses/test_stateful.py,2
docs/source/performance/benchmarks.rst,2
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu,2
tests/v1/kv_connector/unit/test_example_connector.py,2
.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_async_eplb.sh,2
.github/ISSUE_TEMPLATE/config.yml,2
csrc/attention/mla/cutlass_mla_entry.cu,2
tests/entrypoints/openai/test_serving_tokens.py,2
tests/tpu/__init__.py,2
.github/workflows/bc-lint.yml,2
docs/source/deployment/security.md,2
.buildkite/test_areas/e2e_integration.yaml,2
csrc/cpu/cpu_attn_neon.hpp,2
vllm/entrypoints/serve/instrumentator/metrics.py,2
.buildkite/lm-eval-harness/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml,2
.github/workflows/pylint.yml,2
vllm/tool_parsers/mistral_tool_parser.py,2
docs/source/dev/offline_inference/offline_index.rst,2
requirements-cuda-arm64.txt,2
tests/runai_model_streamer_test/test_weight_utils.py,2
vllm/entrypoints/openai/chat_completion/protocol.py,2
vllm/v1/worker/mamba_utils.py,2
examples/lora_with_quantization_inference.py,2
vllm/v1/worker/gpu/buffer_utils.py,2
examples/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py,2
docs/source/automatic_prefix_caching/apc.rst,2
test-qwen,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-INT8-compressed-tensors.yaml,2
examples/tool_chat_template_deepseekr1.jinja,2
vllm/model_executor/models/ovis2.py,2
docs/source/serving/deploying_with_cerebrium.md,2
csrc/quantization/marlin/marlin_cuda_kernel.cu,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct.yaml,2
tests/async_engine/__init__.py,2
examples/template_qwen_vl.jinja,2
docs/features/batch_invariance.md,2
docs/source/design/automatic_prefix_caching.md,2
examples/online_serving/dashboards/grafana/performance_statistics.json,2
tests/async_engine/test_merge_async_iterators.py,2
vllm/model_executor/guided_decoding/xgrammar_utils.py,2
vllm/transformers_utils/chat_templates/__init__.py,2
csrc/quantization/gptq_marlin/marlin_dtypes.cuh,2
examples/save_sharded_state.py,2
docs/source/dev/input_processing/input_processing_pipeline.rst,2
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Llama-4-Scout-Fp8-ModelOpt-triton.yaml,2
tests/evals/gsm8k/configs/Llama-3.2-1B-Instruct-INT8-CT.yaml,2
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ll.yaml,2
examples/offline_inference/metrics.py,2
csrc/quantization/machete/machete_collective_builder.cuh,2
tests/async_engine/test_openai_server.py,2
csrc/quickreduce/quick_reduce.h,2
vllm/server/__init__.py,2
docs/source/usage/spec_decode.rst,2
tests/models/decoder_only/vision_language/test_chameleon.py,2
examples/template_baichuan.jinja,2
vllm/attention/ops/blocksparse_attention/__init__.py,2
docs/source/models/enabling_multimodal_inputs.md,2
vllm/transformers_utils/processors/deepseek_ocr.py,2
docs/source/design/input_processing/input_processing_pipeline.md,2
csrc/quantization/marlin/sparse/common/mem.h,2
vllm/lora/ops/ipex_ops/__init__.py,2
vllm/model_executor/layers/pooler/seqwise/methods.py,2
docs/source/design/plugin_system.md,2
vllm/commit_id.py,2
csrc/punica/bgmv/bgmv_fp32_fp16_bf16.cu,2
cacheflow/parallel_utils/tensor_parallel/__init__.py,2
vllm/model_executor/tensorizer_loader.py,2
tests/v1/kv_connector/unit/test_lmcache_connector.py,2
tools/check_repo.sh,2
csrc/reduction_utils.h,2
docs/source/assets/kernel/value.png,2
examples/pyproject.toml,2
.buildkite/lm-eval-harness/configs/Mixtral-8x7B-Instruct-v0.1-FP8.yaml,2
vllm/model_executor/layers/rotary_embedding/xdrope.py,2
docs/getting_started/installation/gpu.xpu.inc.md,2
vllm/entrypoints/openai/completion/protocol.py,2
csrc/cpu/sgl-kernels/common.h,2
csrc/cpu/scratchpad_manager.h,2
vllm/transformers_utils/configs/flex_olmo.py,2
tests/multi_step/__init__.py,2
docs/source/community/blog.md,2
tests/v1/metrics/test_perf_metrics.py,2
csrc/cpu/sgl-kernels/gemm.h,2
tests/v1/test_metrics_reader.py,2
vllm/lora/ops/ipex_ops/lora_ops.py,2
vllm/model_executor/quantization_utils/squeezellm.py,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-vllm-cutlass.yaml,2
vllm/fa_utils.py,2
examples/tool_chat_template_granite.jinja,2
examples/tool_chat_template_mistral3.jinja,2
tests/models/encoder_decoder/__init__.py,2
docs/source/dev/engine/async_llm_engine.rst,2
.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-INT8-compressed-tensors.yaml,2
csrc/punica/bgmv/bgmv_fp16_bf16_bf16.cu,2
vllm/model_executor/layers/attention/__init__.py,2
tests/models/test_intern_vit.py,2
vllm/transformers_utils/configs/olmo.py,2
vllm/transformers_utils/configs/exaone4.py,2
benchmarks/kernels/bench_nvfp4_gemm.py,2
vllm/model_executor/layers/quantization/compressed_tensors/__init__.py,2
docs/source/usage/disagg_prefill.rst,2
benchmarks/benchmark_batch_invariance.py,2
tests/v1/entrypoints/openai/serving_responses/test_image.py,2
vllm/vllm_flash_attn/flash_attn_interface.pyi,2
.buildkite/nightly-benchmarks/run-nightly-suite.sh,2
.yapfignore,2
vllm/model_executor/models/siglip2.py,2
tests/lora/test_punica_ops_variation.py,2
docs/source/dev/offline_inference/offline_index.md,2
.github/workflows/png-lint.yml,2
requirements-mamba.txt,2
vllm/adapter_commons/__init__.py,2
tests/config/draft_model_arch_groundtruth.json,2
tests/kernels/test_fused_moe.py,2
docs/source/serving/deploying_with_dstack.md,2
examples/lmcache/cpu_offload_lmcache_v1.py,2
docs/source/deployment/frameworks/dstack.md,2
tests/entrypoints/openai/responses/test_mcp_tools.py,2
docs/design/dbo.md,2
csrc/cutlass_extensions/cute_utils.cuh,2
vllm/entrypoints/openai/completion/api_router.py,2
csrc/punica/bgmv/bgmv_bf16_fp16_fp16.cu,2
cacheflow/model_executor/layers/activation.py,2
tests/v1/entrypoints/openai/serving_responses/test_function_call.py,2
tests/encoder_decoder/__init__.py,2
docs/deployment/integrations/kubeai.md,2
docs/source/api/multimodal/processing.md,2
csrc/mamba/causal_conv1d/static_switch.h,2
csrc/cutlass_extensions/vllm_numeric_conversion.cuh,2
tests/evals/gpt_oss/__init__.py,2
tests/models/embedding/vision_language/__init__.py,2
vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_connector.py,2
"vllm/model_executor/layers/quantization/utils/configs/N=2112,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",2
tests/v1/kv_connector/unit/test_decode_bench_connector.py,2
tests/models/language/generation/test_mbart.py,2
.buildkite/lm-eval-harness/configs/SparseLlama3.1_2of4_fp8_compressed.yaml,2
csrc/attention/dtype_fp8.cuh,2
vllm/model_executor/models/transformers/__init__.py,2
vllm/model_executor/parallel_utils/tensor_parallel/__init__.py,2
tests/entrypoints/pooling/score/test_offline.py,2
vllm/entrypoints/openai/translations/api_router.py,2
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-cutlass.yaml,2
tests/entrypoints/test_llm_generate_multiple_loras.py,2
.buildkite/scripts/hardware_ci/run-npu-test.sh,2
tests/models/decoder_only/vision_language/mm_processor_kwargs/test_idefics3.py,2
vllm/entrypoints/pooling/embed/conftest.py,2
tests/config/test_config_utils.py,2
csrc/punica/bgmv/bgmv_all.cu,2
tests/kernels/test_mamba_mixer2.py,2
vllm/entrypoints/serve/lora/protocol.py,2
csrc/quantization/w8a8/fp8/per_token_group_quant.cu,2
assets/figures/perf_a100_n3.png,2
tests/detokenizer/test_disable_detokenization.py,2
tests/core/__init__.py,2
vllm/attention/utils/kv_sharing_utils.py,2
vllm/model_executor/layers/fused_moe/router/grouped_topk_router.py,2
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",2
vllm/v1/kv_offload/arc_manager.py,2
docs/cli/.nav.yml,2
csrc/cutlass_extensions/gemm/dispatch_policy.hpp,2
vllm/executor/multiproc_xpu_executor.py,2
vllm/model_executor/layers/attention/static_sink_attention.py,2
tests/v1/ec_connector/integration/README.md,2
examples/online_serving/dashboards/perses/README.md,2
tools/sphinx-lint.sh,2
vllm/distributed/ec_transfer/ec_connector/base.py,2
docs/source/serving/deploying_with_helm.rst,2
csrc/quantization/gptq/qdq_3.cuh,2
docs/models/hardware_supported_models/xpu.md,2
vllm/prompt_adapter/__init__.py,2
docs/deployment/frameworks/triton.md,2
docs/source/serving/deploying_with_bentoml.rst,2
docs/governance/committers.md,2
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",2
benchmarks/kernels/bench_nvfp4_quant.py,2
tests/v1/test_outputs.py,2
docs/source/serving/prompt_embeds.md,2
vllm/entrypoints/{openai/serving_tokenization.py => serve/tokenize/serving.py},1
vllm/{utils.py => utils/__init__.py},1
csrc/quantization/cutlass_w8a8/{scaled_mm_dq_c3x.cu => scaled_mm_c3x.cu},1
tests/models/{ => decoder_only/vision_language}/test_internvl.py,1
tests/entrypoints/pooling/basic/test_truncation.py,1
tests/entrypoints/openai/{test_response_api_parsable_context.py => responses/test_parsable_context.py},1
vllm/lora/ops/{ => triton_ops}/bgmv_shrink.py,1
examples/{openi_example_batch.jsonl => openai_example_batch.jsonl},1
csrc/{quantization/cutlass_w8a8 => cutlass_extensions/epilogue}/broadcast_load_epilogue_c3x.hpp,1
.buildkite/{ => scripts/hardware_ci}/run-xpu-test.sh,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/{source => }/deployment/frameworks/chatbox.md,1
csrc/{cache.cpp => cache.h},1
tests/model_executor/model_loader/runai_model_streamer/test_weight_utils.py,1
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-cutlass.yaml,1
tests/models/language/generation_ppl_test/test_gemma.py,1
tests/v1/shutdown/conftest.py,1
vllm/model_executor/parallel_utils/tensor_parallel/mappings.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/language/generation/{test_models.py => test_common.py},1
vllm/{ => utils}/jsontree.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_A100-SXM4-80GB.json",1
collect_env.py => vllm/collect_env.py,1
tests/weight_loading/models-amd.txt,1
examples/offline_inference/{ => pooling}/prithvi_geospatial_mae.py,1
tests/entrypoints/{ => pooling}/openai/test_pooling.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json",1
tests/kernels/{ => mamba}/test_causal_conv1d.py,1
examples/{production_monitoring => opentelemetry}/dummy_client.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
tests/kernels/{ => quantization}/test_rocm_skinny_gemms.py,1
tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py,1
vllm/entrypoints/dynamic_lora.py,1
docs/source/assets/deployment/dify-settings.png,1
tests/entrypoints/openai/{test_accuracy.py => correctness/test_lmeval.py},1
vllm/distributed/kv_transfer/kv_connector/__init__.py,1
tests/models/{decoder_only/language => quantization}/test_gguf.py,1
tests/{tool_use => tool_parsers}/test_xlam_tool_parser.py,1
examples/chart-helm/templates/_helpers.tpl,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/grouped_mm_c3x.cuh,1
{cacheflow => vllm}/block.py,1
examples/online_serving/{ => pooling}/cohere_rerank_client.py,1
tests/v1/entrypoints/openai/responses/__init__.py,1
vllm/entrypoints/serve/sleep/__init__.py,1
examples/offline_inference/{offline_inference_mlpspeculator.py => mlpspeculator.py},1
examples/pooling/token_classify/{ner_client.py => ner_online.py},1
tests/models/multimodal/{generation => }/conftest.py,1
docs/assets/deployment/hf-inference-endpoints-choose-infra.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/quantization/{supported_hardware.rst => supported_hardware.md},1
examples/offline_inference/{ => pooling}/convert_model_to_seq_cls.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/models/language/{pooling => pooling_mteb_test}/mteb_utils.py,1
examples/offline_inference/{ => pooling}/prithvi_geospatial_mae_io_processor.py,1
"vllm/model_executor/layers/fused_moe/configs/{E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}",1
docs/source/assets/design/v1/metrics/intervals-1.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
examples/online_serving/pooling/{embedding_embed_dtype_client.py => embedding_requests_base64_client.py},1
tests/models/{ => decoder_only/language}/test_gguf.py,1
examples/tool_chat_template_gemma3_pythonic.jinja,1
tests/plugins/lora_resolvers/test_hf_hub_resolver.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=12288,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=AMD_Instinct_MI300X.json",1
tools/generate_nightly_torch_test.py,1
examples/chart-helm/templates/poddisruptionbudget.yaml,1
tests/v1/e2e/{test_async_sched_and_preempt.py => test_async_scheduling.py},1
tests/compile/test_graph_partition.py,1
docs/{source => }/assets/design/v1/metrics/intervals-3.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/{source => }/assets/kernel/value.png,1
tests/v1/kv_connector/unit/test_moriio_connector.py,1
docs/source/api/engine/llm_engine.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/pooling/score/score_api_online.py,1
examples/pooling/score/{openai_reranker.py => rerank_api_online.py},1
docs/{serving => usage}/metrics.md,1
.buildkite/image_build/image_build.sh,1
examples/pooling/embed/openai_chat_embedding_client_for_multimodal.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests-cpu-snc3.json,1
vllm/transformers_utils/configs/ovis2.py,1
vllm/entrypoints/serve/instrumentator/offline_docs.py,1
vllm/model_executor/layers/fused_moe/mori_prepare_finalize.py,1
tests/kernels/moe/__init__.py,1
cacheflow/model_executor/parallel_utils/utils.py,1
vllm/{entrypoints/openai => }/tool_parsers/kimi_k2_tool_parser.py,1
tests/kernels/{ => attention}/test_mla_decode_cpu.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H20.json",1
tests/models/multimodal/generation/test_audioflamingo3.py,1
csrc/quantization/cutlass_w8a8/cutlass_visitor_2x_broadcast_epilogue.hpp,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=384,N=128,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/openai/parser/__init__.py,1
docs/source/assets/deployment/anything-llm-provider.png,1
tests/models/test_phimoe.py,1
tests/models/{decoder_only/language => language/pooling}/__init__.py,1
vllm/{entrypoints/openai => }/tool_parsers/minimax_m2_tool_parser.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/{offline_inference_cli.py => cli.py},1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Channel-marlin.yaml,1
requirements/kv_connectors_rocm.txt,1
tests/kernels/{ => attention}/conftest.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/machete.py,1
tests/{tool_use => tool_parsers}/test_kimi_k2_tool_parser.py,1
tests/v1/kv_connector/unit/__init__.py,1
tests/neuron/{ => 1_core}/test_activation.py,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=1792,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",1
docs/{source => }/contributing/model/basic.md,1
docs/design/{ => v1}/multiprocessing.md,1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8.cu,1
.buildkite/test_areas/models_multimodal.yaml,1
tests/models/language/{pooling => pooling_mteb_test}/test_cross_encoder.py,1
vllm/distributed/kv_transfer/kv_connector/v1/moriio/__init__.py,1
csrc/quantization/{gptq_marlin => marlin}/.gitignore,1
.github/workflows/{pylint.yml => ruff.yml},1
tests/models/decoder_only/audio_language/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/assets/deployment/claude-code-example.png,1
tests/metrics/__init__.py,1
tests/async_engine/test_openapi_server.py,1
examples/{ => online_serving}/chart-helm/templates/configmap.yaml,1
vllm/entrypoints/anthropic/protocol.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3.json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/worker/spec_decode/__init__.py,1
docs/assets/design/{v1 => }/tpu/most_model_len.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/source/getting_started/installation/tpu.md,1
tests/models/{embedding/vision_language => multimodal/pooling}/test_phi3v.py,1
.github/workflows/sphinx-lint.yml,1
vllm/model_executor/layers/quantization/kernels/scaled_mm/flashinfer.py,1
examples/offline_inference_arctic.py,1
docs/source/{serving/deploying_with_kserve.md => deployment/integrations/kserve.md},1
tests/neuron/{ => 1_core}/test_prefix_prefill.py,1
tests/models/decoder_only/language/__init__.py,1
{assets => docs/source/assets}/figures/perf_a100_n3_light.png,1
vllm/config/model_arch.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=1024,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/assets/design/{v1 => }/prefix_caching/overview.png,1
docs/{source => }/features/reasoning_outputs.md,1
vllm/v1/worker/gpu/metrics/logits.py,1
tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/test_runai_utils.py,1
Dockerfile => docker/Dockerfile,1
tests/evals/gsm8k/configs/moe-refactor/config-test.txt,1
tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_qwen2_vl.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/kernels/moe/test_fused_topk.py,1
tests/engine/{output_processor/test_multi_step.py => test_multi_step_output_processor.py},1
examples/offline_inference/{offline_inference_neuron.py => neuron.py},1
examples/pooling/embed/{embedding_requests_bytes_client.py => embedding_requests_bytes_online.py},1
tests/kernels/{ => quantization}/test_marlin_gemm.py,1
tests/entrypoints/openai/reasoning_parsers/test_granite_reasoning_parser.py,1
vllm/entrypoints/serve/instrumentator/health.py,1
tests/models/fixtures/mistral_small_3_chat.json,1
examples/online_serving/kv_events.sh,1
examples/offline_inference/async_llm_streaming.py,1
tests/models/test_bitblas.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/v1/sample/{logits_processor.py => logits_processor/builtin.py},1
examples/{online_serving/pooling => pooling/embed}/openai_chat_embedding_client_for_multimodal.py,1
docs/{source => }/deployment/frameworks/modal.md,1
tests/models/decoder_only/language/test_marlin.py,1
tests/multimodal/media/test_audio.py,1
vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/marlin.py,1
examples/{ => online_serving}/openai_embedding_client.py,1
examples/online_serving/disaggregated_serving/README.md,1
tests/entrypoints/openai/test_responses_function_call_parsing.py,1
tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/test_runai_model_streamer_loader.py,1
tests/multimodal/media/test_video.py,1
vllm/{entrypoints/openai => }/tool_parsers/phi4mini_tool_parser.py,1
examples/{ => online_serving}/opentelemetry/dummy_client.py,1
tests/tokenizers_/{test_cached_tokenizer.py => test_hf.py},1
tests/kernels/{ => core}/test_uva.py,1
vllm/model_executor/models/{transformers_moe.py => transformers/moe.py},1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_L40S.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/deployment/frameworks/modal.md,1
docs/{ => contributing}/ci/update_pytorch_version.md,1
.buildkite/scripts/rerun-test.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=AMD_Instinct_MI350_OAM,dtype=fp8_w8a8.json",1
tests/models/language/{pooling => pooling_mteb_test}/test_nomic.py,1
tests/mq_llm_engine/__init__.py,1
benchmarks/kernels/{benchmark_cutlass_fp4_moe.py => benchmark_cutlass_moe_nvfp4.py},1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c3x_sm100.cu,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
examples/chart-helm/templates/job.yaml,1
docs/cli/json_tip.inc.md,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=640,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/online_serving/chart-helm/values.schema.json,1
tests/kernels/test_triton_unified_attention.py,1
docs/{getting_started => usage}/troubleshooting.md,1
vllm/{transformers_utils => tokenizers}/detokenizer_utils.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
docs/source/assets/kernel/key.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/source/{dev => offline_inference}/sampling_params.rst,1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/{model_executor/parallel_utils => distributed}/communication_op.py,1
tests/core/block/{test_block_manager_v2.py => test_block_manager.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H200.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/multimodal/{processing.py => processing/processor.py},1
tests/kernels/{ => quantization}/test_nvfp4_quant.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3.json",1
examples/offline_inference/kv_load_failure_recovery/run.sh,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/{automatic_prefix_caching/apc.md => features/automatic_prefix_caching.md},1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/kernels/moe/{test_mxfp4_moe.py => test_ocp_mx_moe.py},1
docs/source/getting_started/installation/{gpu-cuda.md => gpu/cuda.inc.md},1
examples/online_serving/token_generation_client.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_H20-3e.json",1
vllm/tool_parsers/granite_tool_parser.py,1
tests/models/{ => decoder_only/vision_language}/test_qwen.py,1
tests/v1/entrypoints/openai/{responses => serving_responses}/test_stateful.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
cacheflow/parallel_utils/tensor_parallel/mappings.py,1
docs/{deployment => usage}/security.md,1
cacheflow/{decoding.py => sampling_params.py},1
tests/{ => model_executor/model_loader}/fastsafetensors_loader/test_fastsafetensors_loader.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/utils/{asyncio.py => async_utils.py},1
{examples => vllm/transformers_utils/chat_templates}/template_fuyu.jinja,1
examples/{ => others}/lmcache/kv_cache_sharing_lmcache_v1.py,1
vllm/tool_parsers/hunyuan_a13b_tool_parser.py,1
.buildkite/performance-benchmarks/tests/serving-tests-arm64-cpu.json,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_A800-SXM4-80GB.json",1
tests/models/language/{pooling => pooling_mteb_test}/test_mxbai_rerank.py,1
vllm/entrypoints/cli/benchmark/sweep.py,1
.github/ISSUE_TEMPLATE/{600-new model.yml => 600-new-model.yml},1
vllm/v1/worker/gpu/__init__.py,1
{cacheflow => vllm}/model_executor/models/llama.py,1
examples/pooling/score/vision_language_reranker.py,1
docs/{source => }/features/automatic_prefix_caching.md,1
{cacheflow => vllm}/core/__init__.py,1
{assets => docs/source/assets}/figures/perf_a10g_n1_light.png,1
vllm/entrypoints/serve/cache/__init__.py,1
vllm/{attention/layers => model_executor/layers/attention}/mm_encoder_attention.py,1
tests/{entrypoints/openai/reasoning_parsers => reasoning}/utils.py,1
tests/models/{encoder_decoder/vision_language => multimodal/generation}/test_florence2.py,1
examples/{ => online_serving}/gradio_openai_chatbot_webserver.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/lora/ops/triton_ops/lora_shrink.py,1
tests/tokenizers_/__init__.py,1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-vllm-cutlass.yaml,1
{cacheflow => vllm}/utils.py,1
vllm/{model_executor/layers => }/attention/backends/xformers.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
examples/online_serving/chart-helm/templates/deployment.yaml,1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_qwen2_vl.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_H100_80GB_HBM3.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/{offline_inference_pixtral.py => pixtral.py},1
.buildkite/nightly-benchmarks/serving-tests.json,1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/v1/tokenizer/__init__.py,1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_qwen.py,1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-marlin.yaml,1
examples/online_serving/elastic_ep/bench.sh,1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8.cu,1
docs/assets/design/hybrid_kv_cache_manager/full_attn.png,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI325X.json",1
tests/v1/{ => distributed}/test_async_llm_dp.py,1
vllm/{v1/attention/backends/mla/common.py => model_executor/layers/attention/mla_attention.py},1
vllm/tool_parsers/llama4_pythonic_tool_parser.py,1
tests/neuron/test_comm_ops.py,1
tests/distributed/test_eplb_fused_moe_layer_dep_nvfp4.py,1
docs/getting_started/installation/{ai_accelerator/hpu-gaudi.inc.md => intel_gaudi.md},1
vllm/{ => v1}/attention/ops/triton_unified_attention.py,1
vllm/tokenizers/grok2.py,1
csrc/quantization/cutlass_w8a8/{scaled_mm_c3x.cuh => c3x/scaled_mm.cuh},1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H200.json",1
tests/spec_decode/__init__.py,1
tests/entrypoints/pooling/classify/test_offline.py,1
cacheflow/{models => model_executor}/input_metadata.py,1
docs/source/serving/run_on_sky.md,1
tests/kernels/{ => core}/test_layernorm.py,1
cacheflow/entrypoints/openai/api_server.py,1
vllm/model_executor/layers/mamba/ops/__init__.py,1
docs/{source => }/assets/deployment/architecture_helm_deployment.png,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/run-performance-benchmarks.sh,1
vllm/model_executor/models/glm_ocr.py,1
csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90_fp8_dispatch.cuh,1
vllm/multimodal/media/audio.py,1
docs/{source/_static/custom.js => mkdocs/javascript/run_llm_widget.js},1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_internvl.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/offline_chat_with_tools.py,1
tests/v1/core/test_kv_cache_metrics.py,1
"vllm/model_executor/layers/fused_moe/configs/{E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/assets/deployment/dp_internal_lb.png,1
vllm/{entrypoints/openai => }/tool_parsers/longcat_tool_parser.py,1
vllm/entrypoints/cli/benchmark/__init__.py,1
{cacheflow => vllm}/model_executor/weight_utils.py,1
tests/entrypoints/pooling/{llm/test_classify.py => classify/test_offline.py},1
cacheflow/entrypoints/openai/{openai_frontend.py => api_server.py},1
examples/{ => online_serving}/chart-helm/.helmignore,1
tests/v1/kv_offload/test_cpu.py,1
tests/models/fixtures/pixtral_chat.json,1
docs/source/assets/deployment/anything-llm-upload-doc.png,1
tests/kernels/{ => quantization}/test_machete_mm.py,1
tests/worker/{test_worker.py => test_model_runner.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
benchmark/trace.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/neuron/{ => 1_core}/test_block_table.py,1
csrc/quantization/{gptq_marlin => marlin}/marlin_template.h,1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
tools/ep_kernels/elastic_ep/eep_nvshmem.patch,1
vllm/entrypoints/pooling/base/__init__.py,1
csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_fmha_mla_reduction.hpp,1
vllm/utils/collections.py,1
docs/source/deployment/frameworks/bentoml.md,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_80GB_HBM3.json",1
examples/template_teleflm.jinja,1
docs/source/{serving => models/extensions}/tensorizer.md,1
docs/source/contributing/deprecation_policy.md,1
docs/{source => }/assets/logos/vllm-logo-text-dark.png,1
docs/source/getting_started/{openvino-installation.md => installation/openvino.md},1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm120_fp8.cu,1
tests/config/base_model_arch_groundtruth.json,1
tests/basic_correctness/__init__.py,1
csrc/cache_kernels_fused.cu,1
docs/{source => }/contributing/model/tests.md,1
tests/v1/{generation => determinism}/test_batch_invariance.py,1
examples/{ => online_serving}/chart-helm/templates/deployment.yaml,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=928,device_name=NVIDIA_H100_80GB_HBM3.json",1
vllm/model_executor/layers/pooler/abstract.py,1
examples/pooling/embed/vision_embedding_offline.py,1
{cacheflow => vllm}/model_executor/input_metadata.py,1
tests/entrypoints/openai/{test_serving_engine.py => test_serving_models.py},1
"vllm/model_executor/layers/fused_moe/configs/E=62,N=256,device_name=AMD_Instinct_MI300X.json",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh,1
tests/v1/{ => distributed}/test_internal_lb_dp.py,1
examples/online_serving/{ => pooling}/openai_cross_encoder_score.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
examples/offline_inference/{offline_inference_arctic.py => arctic.py},1
docs/{source => }/design/plugin_system.md,1
tests/entrypoints/pooling/{openai/test_vision_classification.py => classify/test_online_vision.py},1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_B200,dtype=fp8_w8a8.json",1
tests/entrypoints/{ => pooling}/openai/test_truncation.py,1
cacheflow/entrypoints/__init__.py,1
docs/source/getting_started/{xpu-installation.md => installation/xpu.md},1
tests/entrypoints/test_openai_run_batch.py,1
csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c2x.hpp,1
docs/{source => }/assets/deployment/open_webui.png,1
docs/source/assets/design/v1/prefix_caching/free.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_common.py,1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/types.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
vllm/tool_parsers/jamba_tool_parser.py,1
csrc/cache_kernel.cu,1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_phi3v.py,1
tests/tensorizer/tensorize_vllm_model_for_testing.py,1
tests/quantization/fp_quant.py,1
vllm/v1/sample/__init__.py,1
tests/{ => model_executor}/test_logits_processor.py,1
examples/{ => online_serving}/api_client.py,1
tests/tools/__init__.py,1
tests/{tool_use => tool_parsers}/test_seed_oss_tool_parser.py,1
tests/v1/entrypoints/openai/{responses => serving_responses}/test_structured_output.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/lora/ops/triton_ops/v1/v1_expand.py,1
vllm/distributed/ec_transfer/ec_connector/{shared_storage_connector.py => example_connector.py},1
vllm/entrypoints/sagemaker/__init__.py,1
examples/offline_inference/{openai/openai_batch.md => openai_batch/README.md},1
vllm/grpc/vllm_engine.proto,1
tests/models/embedding/language/test_jina_reranker_v2.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/kernels/{ => attention}/test_flashmla.py,1
csrc/quantization/utils.cuh,1
examples/template_chatml.jinja,1
tests/utils_/test_system_utils.py,1
csrc/quantization/marlin/{ => dense}/LICENSE,1
examples/online_serving/opentelemetry/{Otel.md => README.md},1
tests/{ => models}/quantization/test_bitsandbytes.py,1
examples/tool_chat_template_xlam_llama.jinja,1
vllm/{engine => executor}/ray_utils.py,1
requirements-lint.txt => requirements/lint.txt,1
tests/v1/ec_connector/unit/{test_ec_shared_storage_connector.py => test_ec_example_connector.py},1
cacheflow/{ => model_executor}/models/llama.py,1
.github/ISSUE_TEMPLATE/800-misc-discussion.yml,1
tests/v1/entrypoints/openai/{responses => serving_responses}/__init__.py,1
docs/source/{serving => features}/multimodal_inputs.md,1
docs/{source => }/getting_started/troubleshooting.md,1
csrc/quantization/marlin/sparse/LICENSE,1
tests/benchmarks/sweep/__init__.py,1
examples/{offline_inference/pooling => pooling/plugin}/prithvi_geospatial_mae_io_processor.py,1
docs/requirements-docs.txt => requirements/docs.txt,1
examples/{online_serving => pooling}/pooling/openai_pooling_client.py,1
vllm/v1/{offloading => kv_offload}/mediums.py,1
tools/{ => pre_commit}/validate_config.py,1
docs/source/getting_started/installation/neuron.md,1
docs/assets/design/fused_moe_modular_kernel/fused_moe_batched.png,1
csrc/quantization/{ => w8a8}/fp8/common.cuh,1
examples/{production_monitoring => prometheus_grafana}/prometheus.yaml,1
tests/weight_loading/models-large-amd.txt,1
docs/api/summary.md,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json",1
tests/utils_/test_network_utils.py,1
docs/{source => }/features/spec_decode.md,1
examples/online_serving/chart-helm/templates/job.yaml,1
tests/{config => data}/test_config.yaml,1
examples/{online_serving/pooling => pooling/classify}/openai_classification_client.py,1
examples/{ => other}/fp8/README.md,1
tests/kernels/test_mla_decode_cpu.py,1
csrc/quantization/{ => w8a8}/fp8/nvidia/quant_utils.cuh,1
examples/offline_inference/rlhf_online_quant.py,1
tests/multimodal/test_embedding_shape_validation_unit.py,1
Dockerfile.neuron => docker/Dockerfile.neuron,1
examples/pooling/score/vision_reranker_offline.py,1
tests/{tool_use => tool_parsers}/test_deepseekv31_tool_parser.py,1
tests/kernels/test_rocm_skinny_gemms.py,1
docs/{source => }/deployment/k8s.md,1
"vllm/model_executor/layers/quantization/utils/configs/{N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/api/{summary.md => README.md},1
vllm/v1/sample/ops/__init__.py,1
tests/transformers_utils/__init__.py,1
docs/source/quantization/gguf.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H100_80GB_HBM3.json",1
csrc/quantization/fp8/amd_detail/hip_float8.h,1
tests/models/multimodal/processing/__init__.py,1
vllm/lora/ops/triton_ops/v1/v1_shrink.py,1
"vllm/model_executor/layers/fused_moe/configs/E=60,N=176,device_name=AMD_Instinct_MI300X.json",1
examples/{ => online_serving}/chart-helm/values.yaml,1
patch_xformers-0.0.22.post7.rocm.sh => patch_xformers.rocm.sh,1
tests/{standalone_tests => utils_}/test_tensor_schema.py,1
docs/source/assets/usage/disagg_prefill/overview.jpg,1
examples/{offline_inference/pooling => pooling/token_classify}/ner.py,1
"vllm/model_executor/layers/fused_moe/configs/{E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}",1
vllm/entrypoints/{openai/serving_embedding.py => pooling/embed/serving.py},1
vllm/entrypoints/openai/models/protocol.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
cacheflow/{models => model_executor/layers}/activation.py,1
tests/entrypoints/openai/test_render.py,1
docker/docker-bake.hcl,1
vllm/model_executor/models/lfm2_siglip2.py,1
cacheflow/{models => model_executor}/memory_analyzer.py,1
tests/entrypoints/{test_openai_server.py => openai/test_models.py},1
docs/assets/deployment/hf-inference-endpoints-select-hardware.png,1
docs/source/getting_started/installation/{tpu.md => ai_accelerator/tpu.inc.md},1
tests/kernels/{ => quantization}/test_cutlass_2of4_sparse.py,1
vllm/renderers/registry.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
.buildkite/test_areas/plugins.yaml,1
docs/{serving => configuration}/engine_args.md,1
docs/assets/design/cuda_graphs/executor_runtime.png,1
tests/models/decoder_only/vision_language/test_blip2.py,1
tests/v1/engine/__init__.py,1
docs/source/{usage => features}/spec_decode.md,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_H20-3e.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_B200.json",1
tests/{entrypoints/openai => model_executor}/test_guided_processors.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/{offline_inference_tpu.py => tpu.py},1
tests/quantization/test_gptq_v2.py,1
tests/models/multimodal/{ => processing}/test_tensor_schema.py,1
tests/kernels/{ => mamba}/test_mamba_ssm_ssd.py,1
tests/models/language/pooling/{test_cls_models.py => test_classification.py},1
"vllm/model_executor/layers/fused_moe/configs/E=60,N=352,device_name=AMD_Instinct_MI300X.json",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_entry.cu,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json",1
{cacheflow => vllm}/engine/tokenizer_utils.py,1
docs/{source => }/getting_started/installation/ai_accelerator/tpu.inc.md,1
examples/pooling/score/template/nemotron-rerank.jinja,1
examples/{online_serving/pooling => pooling/token_embed}/multi_vector_retrieval_client.py,1
vllm/core/{block_manager_v2.py => block_manager.py},1
.buildkite/{ => scripts/hardware_ci}/run-hpu-test.sh,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests-cpu-snc2.json,1
tests/models/{ => decoder_only/language}/test_danube3_4b.py,1
docs/getting_started/installation/{gpu/xpu.inc.md => gpu.xpu.inc.md},1
tests/benchmarks/test_bench_startup.py,1
requirements/openvino.txt,1
docs/source/assets/deployment/anything-llm-chat-with-doc.png,1
benchmarks/kernels/cpu/benchmark_cpu_fused_moe.py,1
vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/__init__.py,1
vllm/{executor/ray_distributed_executor.py => v1/executor/ray_executor.py},1
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-fi-cutlass.yaml,1
tests/models/{decoder_only/vision_language => multimodal/generation}/test_interleaved.py,1
{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/__init__.py,1
csrc/cumem_allocator_compat.h,1
{cacheflow => vllm}/engine/arg_utils.py,1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm120_fp8.cu,1
tests/entrypoints/openai/rpc/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/distributed/test_multiproc_executor.py,1
vllm/model_executor/{ => model_loader}/weight_utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=60,N=1408,device_name=AMD_Instinct_MI300X.json",1
docs/assets/deployment/open_webui.png,1
tests/models/embedding/language/__init__.py,1
tests/v1/{ => metrics}/test_metrics_reader.py,1
examples/online_serving/elastic_ep/scale.py,1
docs/{source => }/deployment/integrations/llmaz.md,1
docs/source/assets/{usage => features}/disagg_prefill/abstraction.jpg,1
tests/models/language/pooling/test_head_dtype.py,1
docs/{source => }/deployment/frameworks/retrieval_augmented_generation.md,1
tools/{ => pre_commit}/shellcheck.sh,1
tools/{ => pre_commit}/check_pickle_imports.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/random.py,1
examples/offline_inference/{ray_placement.py => rlhf_colocate.py},1
docs/assets/design/{v1 => }/prefix_caching/example-time-7.png,1
{cacheflow => vllm}/model_executor/models/gpt_neox.py,1
vllm/model_executor/layers/attention/backends/xformers.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=NVIDIA_H200.json",1
tests/entrypoints/openai/test_embedding_long_text.py,1
tests/async_engine/{test_openapi_server_ray.py => test_openapi_server.py},1
examples/production_monitoring/docker-compose.yaml,1
vllm/{entrypoints/openai => }/tool_parsers/utils.py,1
vllm/{config.py => config/__init__.py},1
requirements-openvino.txt => requirements/openvino.txt,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_A100-SXM4-40GB.json",1
{cacheflow => vllm}/logger.py,1
rocm_patch/flashpy_xformers-0.0.23.rocm.patch,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/spec_decode/e2e/test_integration_dist.py,1
tests/v1/e2e/{test_ngram_spec_decode.py => test_spec_decode.py},1
docs/assets/deployment/hf-inference-endpoints-click-deploy-button.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json",1
vllm/entrypoints/{responses_utils.py => openai/responses/utils.py},1
docs/source/assets/deployment/open_webui.png,1
vllm/model_executor/layers/quantization/utils/{format_24.py => marlin_utils_test_24.py},1
docs/source/dev/{sampling_params.rst => sampling_params.md},1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/model_utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
{cacheflow => vllm}/model_executor/utils.py,1
tests/v1/ec_connector/unit/test_ec_shared_storage_connector.py,1
vllm/lora/ops/{ => triton_ops}/sgmv_shrink.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/{source => }/features/quantization/gguf.md,1
examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-prefiller-config.yaml,1
vllm/model_executor/layers/quantization/quark/schemes/{quark_w4a4_mxfp4.py => quark_ocp_mx.py},1
tests/kernels/{ => moe}/test_moe.py,1
vllm/tool_parsers/qwen3xml_tool_parser.py,1
docs/{source => }/getting_started/installation/python_env_setup.inc.md,1
docs/mkdocs/overrides/partials/toc-item.html,1
docs/{source => }/models/extensions/runai_model_streamer.md,1
vllm/{transformers_utils => }/tokenizers/mistral.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/moe_data.cu,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",1
vllm/lora/ops/__init__.py,1
docs/{source => }/assets/design/v1/prefix_caching/free.png,1
tests/v1/distributed/__init__.py,1
vllm/worker/__init__.py,1
docs/source/features/prompt_embeds.md,1
csrc/quantization/cutlass_w8a8/{scaled_mm_c3x_sm90_fp8_dispatch.cuh => c3x/scaled_mm_sm90_fp8_dispatch.cuh},1
tests/async_engine/{test_openai_server.py => test_chat_template.py},1
tests/runai_model_streamer/__init__.py,1
docs/assets/design/{v1 => }/prefix_caching/example-time-3.png,1
vllm/v1/worker/gpu/mm/encoder_runner.py,1
tests/compile/fullgraph/__init__.py,1
benchmarks/kernels/cpu/benchmark_cpu_attn.py,1
{assets => docs/source/assets}/figures/perf_a100_n3_dark.png,1
tests/entrypoints/{openai => pooling}/correctness/test_mteb_embed.py,1
requirements-build.txt => requirements/build.txt,1
vllm/entrypoints/openai/tool_parsers/gigachat3_tool_parser.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H200.json",1
tests/compile/{ => distributed}/test_fusions_e2e.py,1
tests/multimodal/assets/image1.png,1
.buildkite/performance-benchmarks/tests/serving-tests-cpu-snc2.json,1
docs/source/{serving => usage}/usage_stats.md,1
vllm/tool_parsers/deepseekv3_tool_parser.py,1
.buildkite/{ => scripts/hardware_ci}/run-amd-test.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=72,N=768,device_name=AMD_Instinct_MI300X.json",1
vllm/benchmarks/sweep/plot_pareto.py,1
tests/models/{decoder_only/audio_language => multimodal/generation}/test_granite_speech.py,1
.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-FP8-compressed-tensors.yaml,1
cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/random.py,1
vllm/model_executor/models/{prithvi_geospatial_mae.py => terratorch.py},1
vllm/{ => v1}/attention/ops/triton_reshape_and_cache_flash.py,1
examples/pooling/plugin/prithvi_geospatial_mae_offline.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/serving/{serving_with_llamaindex.md => integrations/llamaindex.md},1
examples/{ => offline_inference}/offline_inference_with_prefix.py,1
vllm/attention/layers/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/v1/spec_decode/__init__.py,1
examples/online_serving/pooling/multi_vector_retrieval_client.py,1
vllm/model_executor/layers/quantization/kernels/exllama.py,1
vllm/{ => v1}/attention/ops/paged_attn.py,1
docs/assets/{kernel => design/paged_attention}/q_vecs.png,1
vllm/benchmarks/{ => lib}/utils.py,1
examples/offline_inference/basic.py,1
vllm/{model_executor/parallel_utils => distributed/device_communicators}/__init__.py,1
.buildkite/test_areas/samplers.yaml,1
csrc/cpu/mla_decode.cpp,1
tests/kernels/{ => attention}/test_flashinfer.py,1
.buildkite/performance-benchmarks/tests/latency-tests-hpu.json,1
tests/compile/{ => distributed}/test_fusion_all_reduce.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/getting_started/installation/openvino.md,1
tests/kernels/{ => attention}/test_lightning_attn.py,1
examples/offline_inference/{reproduciblity.py => reproducibility.py},1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=AMD_Instinct_MI355_OAM,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
examples/{ => offline_inference}/lora_with_quantization_inference.py,1
docs/source/serving/serving_with_llamastack.md,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_azp_sm90_int8.cu,1
vllm/utils/{collections.py => collection_utils.py},1
docs/source/{usage => features}/tool_calling.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/models/fixtures/audioflamingo3/expected_results_single.json,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/compare-json-results.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=72,N=768,device_name=NVIDIA_H100_80GB_HBM3.json",1
tests/{tokenization => tokenizers_}/test_cached_tokenizer.py,1
tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/test_runai_utils.py,1
vllm/lora/{models.py => model_manager.py},1
docs/{source => }/models/extensions/tensorizer.md,1
docs/source/serving/deploying_with_lws.md,1
vllm/{entrypoints/openai => }/tool_parsers/olmo3_tool_parser.py,1
examples/offline_inference_neuron_int8_quantization.py,1
examples/chart-helm/templates/secrets.yaml,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
vllm/entrypoints/pooling/classify/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/quantization/int8.md,1
.buildkite/lm-eval-harness/configs/models-large-rocm.txt,1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/entrypoints/serve/rpc/api_router.py,1
.buildkite/performance-benchmarks/tests/serving-tests-cpu-snc3.json,1
tests/v1/entrypoints/openai/{responses => serving_responses}/test_basic.py,1
.github/workflows/doc-lint.yml,1
docs/{source => }/features/quantization/modelopt.md,1
tests/entrypoints/pooling/{correctness/test_mteb_score.py => score/test_correctness_mteb.py},1
docs/{source => }/deployment/frameworks/lobe-chat.md,1
tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py,1
vllm/entrypoints/{openai/serving_classification.py => pooling/classify/serving.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
{cacheflow => vllm}/worker/cache_engine.py,1
vllm/entrypoints/pooling/score/__init__.py,1
vllm/model_executor/models/{ovis2.py => ovis.py},1
.buildkite/test_areas/models_distributed.yaml,1
vllm/{ => v1}/attention/ops/flashmla.py,1
docs/{source => }/community/meetups.md,1
examples/{ => online_serving}/openai_chat_completion_client_with_tools.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => others}/lmcache/cpu_offload_lmcache.py,1
csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh,1
vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/MPLinearKernel.py,1
docs/{source => }/deployment/frameworks/triton.md,1
vllm/v1/offloading/worker/worker.py,1
tests/models/language/generation_ppl_test/__init__.py,1
tests/entrypoints/openai/responses/__init__.py,1
tests/v1/e2e/test_pooling_chunked_prefill.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/{decoder_only/vision_language => multimodal/generation}/test_intern_vit.py,1
docs/source/{ => features}/quantization/fp8.md,1
examples/online_serving/dashboards/README.md,1
cacheflow/model_executor/layers/layernorm.py,1
server.py => cacheflow/master/server.py,1
"vllm/model_executor/layers/fused_moe/configs/E=72,N=384,device_name=NVIDIA_H100_80GB_HBM3.json",1
cacheflow/{ => model_executor}/models/opt.py,1
vllm/v1/attention/ops/paged_attn.py,1
docs/{source => }/design/kernel/paged_attention.md,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/throughput-tests-cpu.json,1
vllm/entrypoints/serve/elastic_ep/__init__.py,1
tests/kernels/{ => attention}/test_flash_attn.py,1
{cacheflow => vllm}/core/scheduler.py,1
docs/features/interleaved_thinking.md,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8.json",1
tests/transformers_utils/test_get_processor_kwargs_from_processor.py,1
docs/mkdocs/overrides/main.html,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8.json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/{entrypoints/openai => }/tool_parsers/abstract_tool_parser.py,1
examples/offline_inference/{offline_inference_distributed.py => distributed.py},1
docs/{source => }/deployment/nginx.md,1
tests/models/fixtures/qwen2_5_math_prm_reward_step.json,1
csrc/attention/{attention_kernels.cu => attention_kernels.cuh},1
examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py,1
tests/lora/{test_punica_sizes.py => test_punica_ops_sizes.py},1
tests/plugins/vllm_add_dummy_stat_logger/dummy_stat_logger/dummy_stat_logger.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H200.json",1
vllm/model_executor/layers/quantization/utils/format_24.py,1
.buildkite/lm-eval-harness/configs/Qwen2.5-1.5B-Instruct.yaml,1
docs/source/serving/serving_with_langchain.md,1
tests/evals/gsm8k/configs/DeepSeek-R1-TP.yaml,1
examples/offline_inference/{offline_inference_openai/offline_inference_openai.md => openai/openai_batch.md},1
cacheflow/{server => engine}/tokenizer_utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/{source => }/serving/usage_stats.md,1
docs/source/assets/figures/perf_a100_n3_dark.png,1
tests/{speculative_decoding/speculators/test_eagle3.py => v1/spec_decode/test_speculators_eagle3.py},1
tests/entrypoints/{ => pooling}/openai/test_embedding_long_text.py,1
examples/{online_serving/pooling => pooling/score}/cohere_rerank_client.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/pooling/score/{jinaai_rerank_client.py => openai_reranker.py},1
docs/source/assets/deployment/dify-chat.png,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm120_fp8_dispatch.cuh,1
csrc/quantization/cutlass_w4a8/w4a8_grouped_mm_entry.cu,1
vllm/model_executor/layers/fused_moe/zero_expert_fused_moe.py,1
.buildkite/test_areas/cuda.yaml,1
docs/{source => }/deployment/frameworks/cerebrium.md,1
tests/neuron/{ => 2_core}/test_comm_ops.py,1
docs/assets/features/disagg_prefill/workflow.png,1
.buildkite/performance-benchmarks/tests/serving-tests-hpu.json,1
examples/chart-helm/templates/hpa.yaml,1
cacheflow/worker/__init__.py,1
.buildkite/test_areas/compile.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/{ => decoder_only/vision_language}/test_llava_next_video.py,1
vllm/entrypoints/openai/orca_metrics.py,1
examples/tool_chat_template_llama4_json.jinja,1
docs/source/getting_started/installation/{cpu-apple.md => cpu/apple.inc.md},1
tests/{tensorizer => tensorizer_loader}/test_tensorizer.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
vllm/tool_parsers/deepseekv32_tool_parser.py,1
tests/entrypoints/pooling/correctness/__init__.py,1
tests/neuron/{ => 1_core}/test_logits_processor.py,1
tests/models/{ => decoder_only/vision_language}/test_llava.py,1
"vllm/model_executor/layers/fused_moe/configs/E=40,N=2560,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml,1
tests/entrypoints/openai/{test_response_api_with_harmony.py => responses/test_harmony.py},1
examples/tool_chat_template_hunyuan_a13b.jinja,1
examples/chart-helm/templates/service.yaml,1
tests/tool_parsers/test_minimax_tool_parser.py,1
vllm/{entrypoints/openai => }/tool_parsers/pythonic_tool_parser.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=3072,device_name=NVIDIA_H20,dtype=fp8_w8a8.json",1
tests/models/language/{pooling => pooling_mteb_test}/test_qwen3_reranker.py,1
vllm/lora/{lora.py => lora_weights.py},1
docs/source/getting_started/installation/{neuron.md => ai_accelerator/neuron.inc.md},1
tests/entrypoints/pooling/{openai/test_score.py => score/test_online_score.py},1
examples/online_serving/{ => pooling}/openai_embedding_matryoshka_fy.py,1
docs/source/getting_started/{installation.md => installation/gpu-cuda.md},1
examples/{ => online_serving}/chart-helm/templates/service.yaml,1
docs/assets/design/{v1 => }/prefix_caching/example-time-4.png,1
tests/v1/{test_utils.py => utils.py},1
tests/multimodal/media/test_image.py,1
docs/source/dev/{pooling_params.rst => pooling_params.md},1
tests/models/language/{pooling => pooling_mteb_test}/test_st_projector.py,1
vllm/entrypoints/serve/rpc/__init__.py,1
tests/kernels/{ => attention}/test_blocksparse_attention.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
.buildkite/{ => scripts}/upload-wheels.sh,1
tests/entrypoints/openai/test_responses_error.py,1
tests/models/multimodal/processing/test_qwen.py,1
Dockerfile.s390x => docker/Dockerfile.s390x,1
vllm/third_party/__init__.py,1
docs/{performance => contributing}/benchmarks.md,1
docs/{source => }/design/automatic_prefix_caching.md,1
tests/v1/entrypoints/openai/serving_responses/conftest.py,1
vllm/multimodal/media/image.py,1
{cacheflow => vllm}/model_executor/layers/layernorm.py,1
tests/entrypoints/pooling/reward/test_offline.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/pooling/score/__init__.py,1
vllm/{entrypoints/openai => }/tool_parsers/ernie45_tool_parser.py,1
benchmarks/disagg_benchmarks/request_queue.py,1
.github/ISSUE_TEMPLATE/{400-bug report.yml => 400-bug-report.yml},1
tests/models/{ => decoder_only/vision_language}/test_fuyu.py,1
.github/ISSUE_TEMPLATE/{500-feature request.yml => 500-feature-request.yml},1
csrc/quantization/{gptq_marlin => marlin}/awq_marlin_repack.cu,1
tests/{tool_use => tool_parsers}/test_glm4_moe_tool_parser.py,1
.buildkite/test_areas/models_language.yaml,1
docs/source/{dev => contributing}/dockerfile/dockerfile.rst,1
Dockerfile.ppc64le => docker/Dockerfile.ppc64le,1
tests/entrypoints/{ => pooling}/openai/test_vision_embedding.py,1
tests/models/decoder_only/vision_language/{ => mm_processor_kwargs}/test_qwen2_vl.py,1
vllm/{entrypoints/openai => }/tool_parsers/deepseekv32_tool_parser.py,1
.github/ISSUE_TEMPLATE/{800-misc discussion.yml => 800-misc-discussion.yml},1
docs/source/dev/offline_inference/{llm_inputs.rst => llm_inputs.md},1
vllm/lora/__init__.py,1
tests/entrypoints/{ => pooling}/llm/test_classify.py,1
tests/models/{embedding/language => language/pooling}/test_jina.py,1
examples/{ => online_serving}/chart-helm/templates/job.yaml,1
"vllm/model_executor/layers/fused_moe/configs/E=40,N=2560,device_name=NVIDIA_H100,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => online_serving}/chart-helm/Chart.yaml,1
.github/workflows/matchers/mypy.json,1
examples/offline_inference/{offline_inference_chat.py => chat.py},1
{cacheflow => vllm}/core/block_manager.py,1
vllm/tool_parsers/qwen3coder_tool_parser.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",1
docs/source/training/trl.md,1
examples/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh,1
tests/compile/piecewise/__init__.py,1
tests/tool_parsers/test_xlam_tool_parser.py,1
examples/online_serving/pooling/ner.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/online_serving/chart-helm/tests/job_test.yaml,1
requirements-neuron.txt => requirements/neuron.txt,1
docs/source/offline_inference/llm.rst,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
cacheflow/server/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=6400,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
csrc/quantization/fp8/{amd_detail => amd}/quant_utils.cuh,1
docs/design/hybrid_kv_cache_manager.md,1
examples/template_vlm2vec.jinja,1
docs/deployment/integrations/kaito.md,1
vllm/v1/attention/ops/flashmla.py,1
examples/pooling/score/template/qwen3_vl_reranker.jinja,1
vllm/v1/core/{ => sched}/scheduler.py,1
assets/figures/perf_a10g_n3_light.png,1
tests/entrypoints/pooling/{openai/test_embedding_dimensions.py => embed/test_online_dimensions.py},1
examples/openai_example_batch.jsonl,1
tests/models/{ => decoder_only/language}/test_models.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
tests/models/language/{pooling => pooling_mteb_test}/test_snowflake_arctic_embed.py,1
tests/kernels/{ => core}/test_fused_quant_layernorm.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/multimodal/media/__init__.py,1
cacheflow/parallel_utils/tensor_parallel/utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
csrc/quantization/{ => w8a8}/fp8/per_token_group_quant.cu,1
vllm/v1/worker/gpu/penalties.py,1
docs/{source => }/features/prompt_embeds.md,1
tests/entrypoints/pooling/openai/test_vision_classification.py,1
{cacheflow => vllm}/worker/worker.py,1
examples/online_serving/openai_responses_client.py,1
vllm/tool_parsers/minimax_tool_parser.py,1
csrc/cpu/cpu_fused_moe.cpp,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
benchmarks/kernels/requirements.txt,1
csrc/cpu/sgl-kernels/gemm.cpp,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
Dockerfile.xpu => docker/Dockerfile.xpu,1
vllm/benchmarks/sweep/__init__.py,1
csrc/quantization/fp8/{amd_detail => amd}/hip_float8_impl.h,1
{cacheflow => vllm}/sampling_params.py,1
tests/{ => model_executor/model_loader}/test_sharded_state_loader.py,1
docs/source/{models => usage}/engine_args.rst,1
DCO,1
tests/models/{decoder_only/audio_language => language/generation}/__init__.py,1
examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh,1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml,1
tests/models/{ => decoder_only/vision_language}/test_minicpmv.py,1
vllm/entrypoints/openai/tool_parsers/deepseekv32_tool_parser.py,1
docs/getting_started/installation/{gpu/cuda.inc.md => gpu.cuda.inc.md},1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H200,dtype=int8_w8a16.json",1
docs/{source => }/assets/logos/vllm-logo-only-light.ico,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/utils_/__init__.py,1
docs/assets/contributing/load-pattern-examples.png,1
csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh,1
"vllm/model_executor/layers/quantization/utils/configs/{N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
cacheflow/{models => model_executor/layers}/layernorm.py,1
vllm/attention/utils/kv_transfer_utils.py,1
docs/{source => }/getting_started/quickstart.md,1
docs/{source => }/getting_started/installation/gpu/rocm.inc.md,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
tests/entrypoints/{ => pooling}/openai/test_embedding.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm89_fp8_dispatch.cuh,1
vllm/{ => v1}/attention/ops/prefix_prefill.py,1
docs/{source => }/features/quantization/torchao.md,1
requirements-cuda.txt => requirements/cuda.txt,1
vllm/lora/ops/{ => triton_ops}/utils.py,1
{cacheflow => vllm}/entrypoints/llm.py,1
"vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/deployment/integrations/kthena.md,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_H200.json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_L40S.json",1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=3072,device_name=NVIDIA_H20.json",1
tests/entrypoints/openai/reasoning_parsers/__init__.py,1
docs/{source => }/getting_started/installation/cpu/apple.inc.md,1
examples/{ => online_serving}/prometheus_grafana/docker-compose.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/quantization/utils/configs/N=8192,K=1536,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
.buildkite/performance-benchmarks/tests/throughput-tests-hpu.json,1
tests/kernels/{ => quantization}/test_awq.py,1
csrc/quantization/{gptq_marlin => marlin}/generate_kernels.py,1
requirements-hpu.txt => requirements/hpu.txt,1
tests/compile/{piecewise => fullgraph}/test_full_cudagraph.py,1
examples/offline_inference/{pixtral.py => mistral-small.py},1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm.yaml,1
tests/models/language/pooling_mteb_test/__init__.py,1
vllm/entrypoints/{ => openai/parser}/harmony_utils.py,1
"vllm/model_executor/layers/fused_moe/configs/E=20,N=1536,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Server_Edition,dtype=fp8_w8a8.json",1
Dockerfile.rocm => docker/Dockerfile.rocm,1
{cacheflow => vllm}/config.py,1
examples/pooling/embed/{embedding_requests_base64_client.py => embedding_requests_base64_online.py},1
tests/models/{ => decoder_only/language}/test_gptq_marlin_24.py,1
requirements-test.txt => requirements/test.txt,1
cacheflow/{worker => }/models/model_utils.py,1
tests/prompts/summary.txt,1
cacheflow/entrypoints/{simple_fastapi_frontend.py => api_server.py},1
examples/offline_inference_classification.py,1
examples/{offline_inference/pooling => pooling/embed}/embed_jina_embeddings_v3.py,1
vllm/{logging => logging_utils}/formatter.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=2560,device_name=NVIDIA_H200.json",1
tests/entrypoints/openai/{test_responses_error.py => responses/test_errors.py},1
vllm/entrypoints/openai/models/__init__.py,1
tests/v1/entrypoints/openai/serving_responses/test_stateful.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
tests/entrypoints/pooling/{openai/test_classification.py => classify/test_online.py},1
vllm/distributed/device_communicators/{custom_all_reduce_utils.py => all_reduce_utils.py},1
tests/kernels/{ => attention}/test_merge_attn_states.py,1
tests/evals/gsm8k/configs/Qwen3-30B-A3B-MXFP4A16.yaml,1
tests/samplers/test_stop_reason.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/other/fp8/extract_scales.py,1
examples/offline_inference/{openai => openai_batch}/openai_example_batch.jsonl,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1536,device_name=NVIDIA_H20,dtype=fp8_w8a8.json",1
vllm/v1/attention/ops/vit_attn_wrappers.py,1
tests/v1/core/test_output.py,1
{assets => docs/source/assets}/figures/perf_a10g_n3_dark.png,1
assets/figures/perf_a100_n3_light.png,1
Dockerfile.arm => docker/Dockerfile.arm,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H100_80GB_HBM3.json",1
examples/offline_inference/{offline_inference_with_default_generation_config.py => basic_with_model_default_sampling.py},1
vllm/model_executor/layers/fused_moe/fused_moe_router.py,1
vllm/core/__init__.py,1
benchmarks/trace.py,1
vllm/{entrypoints/openai => }/tool_parsers/gigachat3_tool_parser.py,1
tests/{tensorizer => tensorizer_loader}/__init__.py,1
tests/models/{decoder_only/language => language/generation}/test_phimoe.py,1
csrc/quantization/cutlass_w8a8/{scaled_mm_c3x_sm90_int8_dispatch.cuh => c3x/scaled_mm_sm90_int8_dispatch.cuh},1
{cacheflow => vllm}/entrypoints/api_server.py,1
gradio_webserver.py,1
docs/source/{design/multimodal/multimodal_index.md => api/multimodal/index.md},1
vllm/v1/metrics/__init__.py,1
examples/offline_inference/ray_placement.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/v1/executor/test_multiproc_executor.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=64,device_name=NVIDIA_A800-SXM4-80GB.json",1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json",1
tests/models/{decoder_only/language => quantization}/test_nvfp4.py,1
docs/source/design/kernel/paged_attention.rst,1
vllm/v1/pool/__init__.py,1
vllm/entrypoints/{openai => }/chat_utils.py,1
.buildkite/nightly-benchmarks/latency-tests.json,1
examples/online_serving/pooling/{ner.py => ner_client.py},1
"vllm/model_executor/layers/fused_moe/configs/E=62,N=512,device_name=NVIDIA_H100_80GB_HBM3.json",1
tests/v1/e2e/test_streaming_input.py,1
vllm/{ => v1}/attention/ops/vit_attn_wrappers.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/tokenizers/{deepseekv32.py => deepseek_v32.py},1
benchmarks/sonnet.txt,1
tests/entrypoints/instrumentator/test_metrics.py,1
csrc/attention/vertical_slash_index.cu,1
tests/models/{ => embedding/language}/test_embedding.py,1
docs/assets/design/{v1 => }/prefix_caching/example-time-5.png,1
vllm/benchmarks/sweep/utils.py,1
tests/v1/{executor/test_multiproc_executor.py => kv_connector/unit/test_output_aggreagator.py},1
examples/{ => offline_inference}/offline_inference_classification.py,1
docs/{source => }/serving/openai_compatible_server.md,1
tests/kernels/{ => mamba}/test_mamba_ssm.py,1
csrc/punica/{punica_ops.cc => punica_ops.cu},1
docs/assets/design/hybrid_kv_cache_manager/memory_layout.png,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
tests/models/decoder_only/language/test_danube3_4b.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H100_80GB_HBM3.json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
docs/{getting_started/v1_user_guide.md => usage/v1_guide.md},1
vllm/entrypoints/openai/chat_completion/stream_harmony.py,1
csrc/quantization/gptq_marlin/{gptq_marlin.cuh => marlin.cuh},1
examples/template_chatglm.jinja,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => offline_inference}/offline_chat_with_tools.py,1
csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm100_fp8.cu,1
examples/production_monitoring/prometheus.yaml,1
tests/entrypoints/{test_server_oot_registration.py => openai/test_oot_registration.py},1
{cacheflow => vllm}/model_executor/layers/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20.json",1
tests/entrypoints/anthropic/test_messages.py,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=320,device_name=NVIDIA_H20-3e.json",1
docs/source/deployment/frameworks/retrieval_augmented_generation.md,1
csrc/quantization/fp8/{amd_detail => amd}/hip_float8.h,1
tests/entrypoints/openai/correctness/__init__.py,1
tests/lora/test_multi_loras_with_tp.py,1
tests/kernels/{activation.py => test_activation.py},1
tests/compile/{ => fullgraph}/test_full_graph.py,1
tools/{ => pre_commit}/check_spdx_header.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/source/{serving/deploying_with_k8s.md => deployment/k8s.md},1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-fi-cutlass.yaml,1
cacheflow/model_executor/parallel_utils/tensor_parallel/__init__.py,1
docker/Dockerfile.arm,1
vllm/{attention/backends/abstract.py => v1/attention/backend.py},1
vllm/{ => v1}/attention/ops/triton_decode_attention.py,1
vllm/renderers/terratorch.py,1
csrc/quantization/{gptq_marlin/gptq_marlin.cu => marlin/marlin.cu},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_phi3v.py,1
examples/pooling/token_embed/{multi_vector_retrieval.py => multi_vector_retrieval_offline.py},1
docs/source/{dev => design}/multimodal/multimodal_index.rst,1
csrc/quantization/cutlass_w8a8/{scaled_mm_dq_entry.cu => scaled_mm_entry.cu},1
csrc/cutlass_extensions/vllm_custom_types.cuh,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X.json",1
csrc/quantization/cutlass_w4a8/w4a8_utils.cu,1
tests/utils_/test_collection_utils.py,1
tests/models/language/pooling/{test_override_pooler_config.py => test_pooler_config_init_behaviour.py},1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H20-3e.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm120_fp8_dispatch.cuh,1
tests/models/{ => decoder_only/language}/test_marlin.py,1
docs/assets/features/disagg_prefill/high_level_design.png,1
tests/entrypoints/pooling/embed/conftest.py,1
docs/source/{models/enabling_multimodal_inputs.md => contributing/model/multimodal.md},1
docs/source/{serving/serving_with_llamastack.md => deployment/integrations/llamastack.md},1
vllm/transformers_utils/{tokenizer_group => }/tokenizer_group.py,1
tests/transformers_utils/{test_get_processor_kwargs_from_processor.py => test_processor.py},1
docs/{source => }/assets/deployment/dify-chat.png,1
docs/{source => }/features/quantization/auto_awq.md,1
.buildkite/performance-benchmarks/scripts/compare-json-results.py,1
docs/source/getting_started/{amd-installation.md => installation/gpu-rocm.md},1
docs/source/dev/offline_inference/{llm.rst => llm.md},1
examples/pooling/token_embed/{multi_vector_retrieval_client.py => multi_vector_retrieval_online.py},1
cacheflow/{master => core}/block_manager.py,1
tests/tool_parsers/test_openai_tool_parser.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/latency-tests.json,1
docs/source/assets/figures/perf_a10g_n1_light.png,1
tests/v1/kv_offload/{test_cpu.py => test_cpu_manager.py},1
tests/{tensorizer => tensorizer_loader}/tensorize_vllm_model_for_testing.py,1
docs/source/assets/design/hierarchy.png,1
{assets => docs/source/assets}/figures/perf_a100_n1_dark.png,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
cacheflow/models/activation.py,1
vllm/entrypoints/serve/disagg/__init__.py,1
tests/kernels/{ => core}/test_permute_cols.py,1
tests/models/{embedding/vision_language => multimodal/pooling}/test_dse_qwen2_vl.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/kernels/test_allspark_gemm.py,1
cacheflow/{ => model_executor}/models/gpt_neox.py,1
vllm/{ => v1}/attention/ops/triton_merge_attn_states.py,1
vllm/v1/attention/backends/{ => mla}/triton_mla.py,1
vllm/v1/offloading/abstract.py,1
benchmarks/disagg_benchmarks/rate_limiter.py,1
examples/{online_serving => pooling/embed}/openai_embedding_long_text/client.py,1
docs/source/assets/logos/vllm-logo-text-dark.png,1
examples/{ => online_serving}/openai_chat_completion_structured_outputs.py,1
examples/{ => offline_inference}/offline_inference_pixtral.py,1
docs/deployment/integrations/llm-d.md,1
examples/template_inkbot.jinja,1
vllm/{ => v1}/attention/backends/registry.py,1
docs/source/{offline_inference => dev}/sampling_params.rst,1
vllm/utils/profiling.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/README.md,1
tests/entrypoints/{openai => instrumentator}/test_metrics.py,1
tests/tensorizer/test_tensorizer.py,1
vllm/{model_executor/parallel_utils => distributed}/utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => offline_inference}/offline_inference_with_default_generation_config.py,1
.github/workflows/matchers/actionlint.json,1
vllm/{entrypoints/openai => }/tool_parsers/seed_oss_tool_parser.py,1
tests/v1/logits_processors/__init__.py,1
examples/{ => online_serving}/sagemaker-entrypoint.sh,1
examples/{ => online_serving}/chart-helm/templates/secrets.yaml,1
vllm/model_executor/layers/fused_moe/{routing_simulator.py => router/routing_simulator_router.py},1
cacheflow/{models/sample.py => model_executor/layers/sampler.py},1
vllm/lora/ops/triton_ops/{v1/v1_shrink.py => lora_shrink.py},1
vllm/entrypoints/{ => mcp}/tool_server.py,1
vllm/multimodal/processing/processor.py,1
docs/source/{usage => features}/multimodal_inputs.md,1
vllm/entrypoints/openai/responses/context.py,1
cacheflow/entrypoints/openai/protocol.py,1
tests/kernels/core/test_apply_rotary_emb.py,1
examples/{ => offline_inference}/florence2_inference.py,1
vllm/model_executor/layers/fused_moe/{ => router}/fused_moe_router.py,1
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-marlin.yaml,1
vllm/tool_parsers/hermes_tool_parser.py,1
tests/models/test_gptq_bitblas.py,1
cacheflow/parallel_utils/tensor_parallel/random.py,1
docs/assets/{kernel => design/paged_attention}/key.png,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/grouped_mm_c3x_sm90.cu,1
"vllm/model_executor/layers/quantization/utils/configs/{N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/seed_parameter_behavior.md,1
tests/models/{encoder_decoder/language => language/generation}/test_bart.py,1
vllm/lora/ops/{ => triton_ops}/bgmv_expand.py,1
docs/{source => }/assets/design/v1/metrics/intervals-2.png,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
docs/source/serving/serving_with_llamaindex.md,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H200,dtype=int8_w8a16.json",1
docs/source/deployment/frameworks/lws.md,1
docs/{serving => configuration}/serve_args.md,1
docs/source/serving/deploying_with_kubeai.md,1
docs/source/{usage => serving}/usage_stats.md,1
vllm/v1/attention/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_B200,dtype=fp8_w8a8.json",1
csrc/quantization/fp4/nvfp4_scaled_mm_sm120_kernels.cu,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/entrypoints/{dynamic_lora.py => serve/lora/api_router.py},1
vllm/{attention/layers => model_executor/layers/attention}/cross_attention.py,1
vllm/multimodal/media/video.py,1
cacheflow/{worker => }/models/opt.py,1
tests/{ => model_executor/model_loader}/fastsafetensors_loader/test_weight_utils.py,1
benchmarks/auto_tune/batch_auto_tune.sh,1
tests/models/{embedding/language => language/pooling}/test_cls_models.py,1
examples/{ => offline_inference}/offline_inference_embedding.py,1
examples/pooling/plugin/prithvi_geospatial_mae_client.py,1
tests/spec_decode/e2e/{test_integration_dist.py => test_integration_dist_tp4.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
examples/{ => offline_inference}/offline_profile.py,1
{cacheflow => vllm}/model_executor/parallel_utils/__init__.py,1
vllm/v1/worker/gpu/sample/states.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
rocm_patch/flashpy_xformers-0.0.22.post7.rocm.patch,1
examples/{ => online_serving}/chart-helm/templates/custom-objects.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm90_fp8.cu,1
tests/neuron/test_layernorm.py,1
test_cli_client.py,1
docs/source/{usage => features}/disagg_prefill.md,1
tests/models/{embedding/language => language/pooling}/test_snowflake_arctic_embed.py,1
tests/models/{encoder_decoder/vision_language => multimodal/generation}/test_mllama.py,1
docs/source/{ => dev}/offline_inference/llm.rst,1
examples/{ => online_serving}/openai_cross_encoder_score.py,1
examples/offline_inference/pooling/embed_matryoshka_fy.py,1
docs/source/{serving/deploying_with_helm.md => deployment/frameworks/helm.md},1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_A100-SXM4-80GB.json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_A100-SXM4-80GB.json",1
docs/{source => }/deployment/frameworks/streamlit.md,1
vllm/entrypoints/anthropic/api_router.py,1
tests/evals/gsm8k/configs/moe-refactor/{Qwen3-30B-A3B-Fp8-CT-Block-vllm-cutlass.yaml => Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml},1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/skip_loading_weights_in_engine_init.py,1
tests/entrypoints/pooling/{openai/test_vision_embedding.py => embed/test_online_vision.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/template_alpaca.jinja,1
requirements-cpu.txt => requirements/cpu.txt,1
{assets => docs/source/assets}/figures/perf_a10g_n3_light.png,1
docs/source/usage/{faq.rst => faq.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/assets/{kernel => design/paged_attention}/k_vecs.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
examples/{ => offline_inference}/offline_inference_neuron_int8_quantization.py,1
examples/openai_pooling_client.py,1
tests/tool_parsers/test_ernie45_moe_tool_parser.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_sm100.cu,1
examples/offline_inference_structured_outputs.py,1
docs/{source => }/features/lora.md,1
docs/source/getting_started/{cpu-installation.md => installation/cpu-x86.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/v1/{ => core}/test_kv_sharing.py,1
tests/utils_/test_mem_utils.py,1
vllm/v1/attention/ops/rocm_aiter_mla_sparse.py,1
tests/models/decoder_only/vision_language/test_llava.py,1
tests/neuron/test_cache.py,1
tests/models/language/pooling_mteb_test/{mteb_utils.py => mteb_score_utils.py},1
vllm/{ => v1}/attention/ops/common.py,1
docs/source/serving/{serving_with_langchain.md => integrations/langchain.md},1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/builders.py,1
vllm/v1/worker/gpu/README.md,1
tests/kernels/test_lightning_attn.py,1
examples/gguf_inference.py,1
examples/online_serving/{disagg_examples => disaggregated_serving}/disagg_proxy_demo.py,1
tests/entrypoints/openai/{test_skip_tokenizer.py => test_vision_embeds.py},1
vllm/entrypoints/openai/{serving_completion.py => completion/serving.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI325X.json",1
csrc/{ => core}/registration.h,1
tests/entrypoints/pooling/openai/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/{ => pooling}/embed_matryoshka_fy.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_B200.json",1
tests/rocm/aiter/test_grouped_quant.py,1
vllm/entrypoints/openai/completion/__init__.py,1
examples/tool_chat_template_qwen3coder.jinja,1
docs/{source => }/design/v1/prefix_caching.md,1
tests/models/{decoder_only/audio_language => multimodal/generation}/test_ultravox.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
.buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep_eplb.sh,1
docs/source/{serving/deploying_with_cerebrium.md => deployment/frameworks/cerebrium.md},1
docs/{source => }/design/mm_processing.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
.buildkite/scripts/cache-rocm-base-wheels.sh,1
tests/kernels/{ => attention}/test_prefix_prefill.py,1
examples/pooling/score/openai_cross_encoder_score_for_multimodal.py,1
tests/kernels/{ => quantization}/test_allspark_gemm.py,1
examples/chart-helm/ct.yaml,1
docs/{source => }/assets/deployment/dify-create-chatbot.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
examples/chart-helm/values.schema.json,1
tests/entrypoints/{test_openai_completion.py => openai/test_completion.py},1
.buildkite/{ => scripts/hardware_ci}/run-tpu-v1-test.sh,1
tests/models/language/pooling/test_bge_m3.py,1
csrc/quantization/marlin/LICENSE,1
examples/pooling/token_classify/{ner.py => ner_offline.py},1
tests/v1/cudagraph/__init__.py,1
examples/pooling/embed/openai_embedding_long_text/service.sh,1
csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_mla_tile_scheduler.hpp,1
.buildkite/nightly-benchmarks/{ => tests}/serving-tests.json,1
docs/getting_started/installation/{gpu/rocm.inc.md => gpu.rocm.inc.md},1
tests/models/{ => decoder_only/vision_language}/test_llava_next.py,1
csrc/attention/dtype_fp8_e5m2.cuh,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_int8.cu,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
examples/offline_inference/convert_model_to_seq_cls.py,1
tests/v1/streaming_input/test_gpu_model_runner_streaming.py,1
docs/source/assets/design/v1/metrics/intervals-2.png,1
tests/evals/gsm8k/configs/DeepSeek-R1-DP.yaml,1
examples/{offline_inference/pooling => pooling/embed}/embed_matryoshka_fy.py,1
csrc/core/exception.hpp,1
requirements-rocm-build.txt,1
vllm/entrypoints/openai/{serving_responses.py => responses/serving.py},1
cmake/external_projects/triton_kernels.cmake,1
tests/neuron/test_rotary_embedding.py,1
"vllm/model_executor/layers/fused_moe/configs/E=32,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/fastsafetensors_loader/__init__.py,1
docs/{source => }/models/extensions/fastsafetensor.md,1
docs/{usage => configuration}/env_vars.md,1
cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/__init__.py,1
tests/compile/fullgraph/test_simple.py,1
vllm/entrypoints/openai/responses/__init__.py,1
tests/kernels/test_block_int8.py,1
vllm/{entrypoints/openai => }/tool_parsers/granite_tool_parser.py,1
docs/{source => }/design/v1/metrics.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Channel-vllm-cutlass.yaml,1
vllm/tool_parsers/granite_20b_fc_tool_parser.py,1
tests/model_executor/model_loader/runai_streamer_loader/test_runai_model_streamer_s3.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H200.json",1
cacheflow/worker/models/opt.py,1
vllm/v1/kv_offload/backends/__init__.py,1
docs/source/dev/multimodal/adding_multimodal_plugin.rst,1
docs/assets/design/{v1 => }/prefix_caching/example-time-6.png,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1408,device_name=NVIDIA_B200.json",1
docs/source/{models => usage}/spec_decode.rst,1
tests/v1/distributed/test_external_lb_dp.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/{mistral_tool_use => tool_use/mistral}/test_mistral_tool_calls.py,1
.buildkite/{ => scripts/hardware_ci}/run-gh200-test.sh,1
vllm/executor/{ray_gpu_executor.py => ray_distributed_executor.py},1
docs/{source => }/assets/deployment/chatbox-settings.png,1
tests/v1/{offloading => kv_offload}/test_worker.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=384,device_name=NVIDIA_H20,dtype=fp8_w8a8.json",1
examples/{ => offline_inference}/multilora_inference.py,1
.buildkite/image_build/image_build.yaml,1
requirements-xpu.txt => requirements/xpu.txt,1
tests/entrypoints/pooling/{openai/test_embedding_long_text.py => embed/test_online_long_text.py},1
vllm/v1/kv_offload/worker/__init__.py,1
docs/{source => }/deployment/frameworks/open-webui.md,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=bf16.json",1
docs/source/serving/{deploying_with_helm.rst => deploying_with_helm.md},1
docs/source/assets/{dev => contributing}/dockerfile-stages-dependency.png,1
vllm/entrypoints/serve/profile/__init__.py,1
vllm/v1/attention/backends/{mamba_attn.py => mamba2_attn.py},1
examples/{ => offline_inference}/offline_inference_vision_language_embedding.py,1
vllm/config/ec_transfer.py,1
examples/{ => other}/fp8/quantizer/README.md,1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-trtllm.yaml,1
csrc/quantization/{compressed_tensors/int8_quant_kernels.cu => w8a8/int8/scaled_quant.cu},1
examples/pooling/embed/openai_embedding_long_text/README.md,1
docs/source/usage/env_vars.rst,1
examples/{offline_inference/pooling => pooling/token_embed}/multi_vector_retrieval.py,1
docs/source/automatic_prefix_caching/apc.md,1
"vllm/model_executor/layers/fused_moe/configs/{E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}",1
tests/entrypoints/pooling/basic/test_encode.py,1
docs/source/{ => features}/quantization/gguf.md,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/kernels/{ => quantization}/test_nvfp4_scaled_mm.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",1
tests/multimodal/test_sparse_tensor_validation_unit.py,1
tests/models/language/{pooling => pooling_mteb_test}/test_bge_reranker_v2_gemma.py,1
docs/{source => }/getting_started/installation/gpu/xpu.inc.md,1
tests/v1/offloading/test_worker.py,1
tests/entrypoints/pooling/{correctness/test_mteb_embed.py => embed/test_correctness_mteb.py},1
tests/entrypoints/instrumentator/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
docs/source/usage/faq.rst,1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/entrypoints/openai/models/api_router.py,1
tests/entrypoints/{test_llm_encode.py => llm/test_encode.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X.json",1
vllm/model_executor/models/step1.py,1
vllm/tool_parsers/utils.py,1
vllm/model_executor/models/gemma3n_audio_utils.py,1
.buildkite/{ => scripts/hardware_ci}/run-cpu-test-ppc64le.sh,1
examples/offline_inference/{ => pooling}/qwen3_reranker.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1344,device_name=NVIDIA_A100-SXM4-80GB.json",1
examples/online_serving/{ => pooling}/openai_cross_encoder_score_for_multimodal.py,1
examples/{online_serving/pooling => pooling/token_classify}/ner_client.py,1
tests/tool_use/mistral/test_mistral_tool_calls.py,1
examples/template_qwen_vl_chat.jinja => vllm/transformers_utils/chat_templates/template_chatml.jinja,1
docs/{source => }/getting_started/installation/cpu/s390x.inc.md,1
cacheflow/master/{frontend.py => simple_frontend.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json",1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8.cu,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
tests/models/embedding/language/test_snowflake_arctic_embed.py,1
examples/chart-helm/values.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
vllm/v1/attention/backends/{rocm_attn.py => triton_attn.py},1
tests/models/{ => encoder_decoder/language}/test_bart.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-BF16-triton.yaml,1
tests/{entrypoints/openai/rpc => mq_llm_engine}/__init__.py,1
vllm/tool_parsers/phi4mini_tool_parser.py,1
tests/benchmarks/__init__.py,1
tests/models/decoder_only/language/{test_jamba.py => test_hybrid.py},1
tests/models/{decoder_only/vision_language => multimodal/generation}/test_phi4mm.py,1
csrc/quantization/{gptq_marlin => marlin}/marlin_int4_fp8_preprocess.cu,1
examples/pooling/score/{qwen3_reranker.py => offline_reranker.py},1
tests/lora/__init__.py,1
docs/source/usage/{performance.rst => performance.md},1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm100_fp8.cu,1
docs/source/usage/disagg_prefill.md,1
{cacheflow => vllm}/model_executor/model_loader.py,1
tests/kernels/{ => core}/test_mrope.py,1
cacheflow/models/gpt2.py,1
examples/{ => offline_inference}/offline_inference_vision_language.py,1
vllm/lora/ops/triton_ops/v1/v1_kernel_metadata.py,1
docs/source/serving/architecture_helm_deployment.png,1
tests/entrypoints/{test_openai_chat.py => openai/test_chat.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/{ => decoder_only/vision_language}/test_chameleon.py,1
examples/online_serving/{ => disaggregated_serving}/kv_events.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=2560,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
docs/source/assets/kernel/q_vecs.png,1
examples/template_dse_qwen2_vl.jinja,1
vllm/model_executor/layers/fused_moe/router/__init__.py,1
tools/png-lint.sh,1
examples/{ => online_serving}/prometheus_grafana/README.md,1
examples/{ => offline_inference}/offline_inference_chat.py,1
tests/models/{ => decoder_only/language}/test_jamba.py,1
examples/chart-helm/.helmignore,1
tests/models/multimodal/generation/test_nemotron_parse.py,1
vllm/renderers/hf.py,1
tests/models/{ => decoder_only/language}/test_aqlm.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
examples/{online_serving/pooling => pooling/embed}/embedding_requests_bytes_client.py,1
vllm/model_executor/layers/{triton_kernel => attention/ops}/prefix_prefill.py,1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm.yaml,1
tests/models/{ => decoder_only/language}/test_gptq_marlin.py,1
examples/{ => offline_inference/offline_inference_openai}/offline_inference_openai.md,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=AMD_Instinct_MI300X.json",1
docs/{source => }/assets/design/v1/prefix_caching/example-time-3.png,1
examples/pooling/score/{cohere_rerank_online.py => cohere_rerank_client.py},1
vllm/{entrypoints/openai => }/tool_parsers/minimax_tool_parser.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c3x_sm90.cu,1
"vllm/model_executor/layers/fused_moe/configs/E=129,N=704,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/getting_started/installation/device.template.md,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI325X.json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
tests/entrypoints/{ => pooling}/llm/test_embedding.py,1
examples/{ => offline_inference}/offline_inference_neuron.py,1
vllm/utils/{async_utils.py => asyncio.py},1
tests/{test_lazy_torch_compile.py => standalone_tests/lazy_torch_compile.py},1
examples/{ => other}/logging_configuration.md,1
vllm/model_executor/models/glm_ocr_mtp.py,1
docs/assets/deployment/hf-inference-endpoints-select-model.png,1
tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_idefics3.py,1
csrc/registration.h,1
vllm/transformers_utils/processors/{ovis2.py => ovis.py},1
examples/template_blip2.jinja,1
vllm/distributed/ec_transfer/__init__.py,1
tests/utils_/test_jsontree.py,1
docs/source/{serving/deploying_with_kubeai.md => deployment/integrations/kubeai.md},1
tests/compile/{piecewise => fullgraph}/test_multiple_graphs.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
cacheflow/model_executor/parallel_utils/tensor_parallel/random.py,1
tools/{ => pre_commit}/check_triton_import.py,1
tests/transformers_utils/test_repo_utils.py,1
tests/benchmarks/test_plot_filters.py,1
examples/online_serving/disagg_xpyd/disagg_prefill_proxy_xpyd.py,1
docs/{source => }/training/rlhf.md,1
tests/kernels/test_nvfp4_quant.py,1
tests/entrypoints/openai/correctness/{test_mteb.py => test_mteb_embed.py},1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
examples/offline_inference/{offline_inference_whisper.py => whisper.py},1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
docs/source/assets/deployment/chatbox-settings.png,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=512,device_name=NVIDIA_H100_80GB_HBM3.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
csrc/quantization/{gptq_marlin => marlin}/marlin_mma.h,1
vllm/v1/offloading/mediums.py,1
vllm/v1/worker/gpu/kv_connector.py,1
tests/kernels/test_cache_kernels.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/cutlass_gemm_caller.cuh,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
tests/v1/core/__init__.py,1
tests/entrypoints/pooling/{llm/test_embedding.py => embed/test_offline.py},1
vllm/model_executor/models/{phi_1_5.py => phi.py},1
vllm/renderers/grok2.py,1
vllm/{entrypoints/openai => }/tool_parsers/step3_tool_parser.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H100_80GB_HBM3.json",1
docs/assets/{kernel => design/paged_attention}/logits_vec.png,1
examples/offline_inference/{offline_inference_structured_outputs.py => structured_outputs.py},1
docs/getting_started/installation/{ai_accelerator/neuron.inc.md => aws_neuron.md},1
csrc/quantization/cutlass_w8a8/{scaled_mm_c2x_sm89_dispatch.cuh => scaled_mm_c2x_sm89_fp8_dispatch.cuh},1
tests/distributed/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H200.json",1
docs/source/{dev => contributing}/profiling/profiling_index.rst,1
tests/{tool_use => tool_parsers}/test_mistral_tool_parser.py,1
tests/models/{embedding/language => quantization}/__init__.py,1
docs/source/assets/{usage => features}/disagg_prefill/overview.jpg,1
vllm/logging_utils/lazy.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/test_openai_completion.py,1
tests/tool_parsers/test_qwen3coder_tool_parser.py,1
examples/other/fp8/quantizer/README.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/online_serving/disagg_examples/disagg_proxy_demo.py,1
examples/florence2_inference.py,1
{cacheflow => vllm}/model_executor/parallel_utils/parallel_state.py,1
docs/assets/design/hybrid_kv_cache_manager/basic_grouping_example.png,1
"vllm/model_executor/layers/fused_moe/configs/E=384,N=256,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/serving/deploying_with_bentoml.md,1
tests/distributed/eplb_utils.py,1
docs/source/assets/figures/perf_a100_n3_light.png,1
docs/source/api/offline_inference/llm_inputs.md,1
tests/{engine => detokenizer}/test_stop_reason.py,1
examples/{ => offline_inference}/offline_inference_vision_language_multi_image.py,1
examples/offline_inference/{offline_profile.py => profiling.py},1
tests/multi_step/{test_correctness.py => test_correctness_async_llm.py},1
tests/models/language/{pooling => pooling_mteb_test}/test_gte.py,1
vllm/core/{evictor_v2.py => evictor.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI325X.json",1
tests/{mistral_tool_use => tool_use/mistral}/conftest.py,1
tests/evals/gsm8k/configs/Qwen3-Next-80B-A3B-NVFP4-EP2.yaml,1
vllm/v1/attention/ops/pallas_kv_cache_update.py,1
cacheflow/model_executor/parallel_utils/tensor_parallel/utils.py,1
docs/{source => }/assets/design/v1/prefix_caching/example-time-7.png,1
tests/models/{decoder_only/language => language/generation}/test_mistral.py,1
.buildkite/test_areas/expert_parallelism.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
csrc/core/registration.h,1
tests/{mistral_tool_use => tool_use/mistral}/__init__.py,1
tests/entrypoints/llm/__init__.py,1
examples/{ => online_serving}/gradio_webserver.py,1
examples/{ => online_serving}/chart-helm/templates/_helpers.tpl,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/pooling/score/using_template_online.py,1
docs/source/usage/{engine_args.rst => engine_args.md},1
docs/source/serving/deploying_with_kserve.md,1
docs/{source => }/deployment/frameworks/skypilot.md,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/nightly-tests.json,1
"vllm/model_executor/layers/fused_moe/configs/E=72,N=384,device_name=AMD_Instinct_MI300X.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=NVIDIA_B300_SXM6_AC,dtype=fp8_w8a8.json",1
tests/entrypoints/pooling/{openai => basic}/test_truncation.py,1
cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/mappings.py,1
vllm/lora/ops/triton_ops/lora_expand.py,1
tests/v1/engine/{test_processor_multi_modal_uuids.py => test_process_multi_modal_uuids.py},1
tests/{worker => }/spec_decode/utils.py,1
csrc/quantization/{gptq_marlin => marlin}/marlin.cuh,1
vllm/transformers_utils/chat_templates/template_deepseek_ocr.jinja,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/assets/design/v1/metrics/intervals-3.png,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H200.json",1
csrc/cutlass_extensions/common.cpp,1
examples/pooling/score/{cohere_rerank_client.py => cohere_rerank_online.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/v1/engine/test_init_error_messaging.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_fp8_dispatch.cuh,1
tests/entrypoints/openai/test_embedding_shape_validation.py,1
docs/{source => }/features/quantization/quantized_kvcache.md,1
tests/multimodal/__init__.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/get_group_starts.cuh,1
examples/offline_inference/{logits_processor.py => logits_processor/custom.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X.json",1
.github/workflows/matchers/markdownlint.json,1
examples/{ => online_serving}/chart-helm/templates/pvc.yaml,1
examples/online_serving/{ => pooling}/openai_embedding_client.py,1
tests/v1/e2e/test_mamba_prefix_cache.py,1
examples/{template_vlm2vec.jinja => template_vlm2vec_phi3v.jinja},1
docs/{source => }/deployment/frameworks/anything-llm.md,1
{cacheflow => vllm}/model_executor/models/gpt2.py,1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm.yaml,1
docs/{source => }/features/multimodal_inputs.md,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_H20-3e.json",1
docs/{source => }/getting_started/installation/cpu.md,1
vllm/entrypoints/serve/instrumentator/static/swagger-ui-bundle.js,1
docs/source/{usage/performance.md => performance/optimization.md},1
vllm/tool_parsers/deepseekv31_tool_parser.py,1
tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh,1
.buildkite/scripts/push-nightly-builds.sh,1
.buildkite/scripts/{upload-wheels.sh => upload-nightly-wheels.sh},1
tests/lora/{test_chatglm3.py => test_chatglm3_tp.py},1
csrc/quantization/cutlass_w4a8/w4a8_utils.cuh,1
vllm/transformers_utils/configs/speculators/__init__.py,1
docs/{source => }/features/structured_outputs.md,1
examples/online_serving/{ => pooling}/jinaai_rerank_client.py,1
vllm/{attention/layers => model_executor/layers/attention}/static_sink_attention.py,1
"vllm/model_executor/layers/fused_moe/configs/E=384,N=128,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm100_fp8_dispatch.cuh,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X.json",1
{assets => docs/source/assets}/figures/perf_a100_n1_light.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
docs/{source => }/getting_started/faq.md,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x.cu,1
tests/{tool_use => tool_parsers}/test_openai_tool_parser.py,1
tests/{entrypoints/openai/reasoning_parsers => reasoning}/__init__.py,1
cacheflow/master/policy.py,1
.buildkite/{ => scripts/hardware_ci}/run-neuron-test.sh,1
examples/online_serving/pooling/embedding_embed_dtype_client.py,1
docs/design/{v1 => }/prefix_caching.md,1
examples/pooling/score/offline_reranker.py,1
tests/models/decoder_only/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/online_serving/{ => pooling}/openai_classification_client.py,1
tests/entrypoints/openai/responses/conftest.py,1
docs/source/{features => serving}/multimodal_inputs.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/entrypoints/openai/reasoning_parsers/granite_reasoning_parser.py,1
tests/kernels/{ => attention}/test_attention.py,1
examples/{ => online_serving}/openai_chat_embedding_client_for_multimodal.py,1
examples/pooling/embed/{openai_chat_embedding_client_for_multimodal.py => vision_embedding_online.py},1
docs/assets/deployment/hf-inference-endpoints-new-endpoint.png,1
tests/model_executor/model_loader/__init__.py,1
tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_internvl.py,1
vllm/entrypoints/{openai/serving_tokens.py => serve/disagg/serving.py},1
tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-fi-cutlass.yaml,1
tests/renderers/test_mistral.py,1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/__init__.py,1
tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/test_weight_utils.py,1
vllm/v1/core/kv_cache_metrics.py,1
docs/{contributing/benchmarks.md => benchmarking/cli.md},1
vllm/{entrypoints/openai/reasoning_parsers => reasoning}/deepseek_r1_reasoning_parser.py,1
tests/kernels/{ => attention}/test_mha_attn.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutedsl-deepep-ll.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/sagemaker-entrypoint.sh,1
tests/v1/utils.py,1
tests/kv_transfer/{disagg_test.py => test_disagg.py},1
examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-decoder-config.yaml,1
tests/models/language/generation/conftest.py,1
vllm/benchmarks/sweep/startup.py,1
tests/kernels/{ => core}/test_pos_encoding.py,1
tests/models/language/pooling/test_all_pooling_plus_chunked_prefill.py,1
vllm/renderers/deepseek_v32.py,1
examples/{online_serving/pooling => pooling/embed}/openai_embedding_matryoshka_fy.py,1
examples/online_serving/pooling/cohere_rerank_client.py,1
tools/profiler/nsys_profile_tools/images/html_tbl.png,1
vllm/lora/ops/triton_ops/{v1/v1_kernel_metadata.py => lora_kernel_metadata.py},1
docs/assets/deployment/hf-inference-endpoints-locate-deploy-button.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/vllm_flash_attn/.gitkeep,1
docs/source/quantization/auto_awq.md,1
.shellcheckrc,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3200,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
cacheflow/{server => engine}/arg_utils.py,1
csrc/quantization/{gptq_marlin => marlin}/marlin_dtypes.cuh,1
tests/models/{decoder_only/vision_language => quantization}/test_awq.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
rocm_patch/commonpy_xformers-0.0.22.post7.rocm.patch,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/performance-benchmarks-descriptions.md,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=2688,device_name=NVIDIA_A100-SXM4-80GB.json",1
docs/source/assets/logos/vllm-logo-only-light.ico,1
tests/{ => kernels/moe}/test_routing_simulator.py,1
vllm/model_executor/layers/fla/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_B200.json",1
tests/standalone_tests/{lazy_torch_compile.py => lazy_imports.py},1
tests/multimodal/{test_processor.py => test_mapper.py},1
tests/entrypoints/pooling/reward/__init__.py,1
tests/models/embedding/language/{test_jina_reranker_v2.py => test_jina.py},1
vllm/{entrypoints/openai => }/tool_parsers/glm4_moe_tool_parser.py,1
vllm/tool_parsers/ernie45_tool_parser.py,1
tests/entrypoints/{ => pooling}/openai/test_score.py,1
examples/tool_chat_template_functiongemma.jinja,1
vllm/{attention/layers => model_executor/layers/attention}/chunked_local_attention.py,1
tests/quantization/test_mixed_precision.py,1
tests/models/{ => quantization}/test_bitblas.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/{runai_model_streamer => runai_model_streamer_test}/test_weight_utils.py,1
tests/kernels/moe/modular_kernel_tools/__init__.py,1
vllm/{ => v1}/attention/ops/triton_prefill_attention.py,1
docs/source/{dev => design}/input_processing/input_processing_pipeline.rst,1
vllm/v1/worker/gpu/sample/prompt_logprob.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/{source => }/serving/integrations/llamaindex.md,1
vllm/{entrypoints/openai => }/tool_parsers/llama4_pythonic_tool_parser.py,1
docs/assets/{kernel => design/paged_attention}/value.png,1
docs/source/getting_started/openvino-installation.md,1
tests/{tokenization => tokenizers_}/test_detokenize.py,1
examples/{online_serving/pooling => pooling/score}/openai_cross_encoder_score.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=51200,K=5120,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/renderers/test_hf.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/offline_inference/{offline_inference_vision_language_embedding.py => vision_language_embedding.py},1
vllm/{entrypoints/openai => }/tool_parsers/deepseekv31_tool_parser.py,1
tests/kernels/{ => attention}/test_encoder_decoder_attn.py,1
tests/{tool_use => tool_parsers}/test_qwen3coder_tool_parser.py,1
tests/models/{decoder_only/vision_language/processing => multimodal}/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=768,device_name=NVIDIA_H20.json",1
examples/offline_inference/pooling/embed_jina_embeddings_v3.py,1
examples/online_serving/opentelemetry/README.md,1
csrc/quantization/per_token_group_quant_8bit.h,1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutedsl-deepep-ll.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/{N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/entrypoints/{ => pooling}/llm/test_reward.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/pooling/embed/embedding_requests_bytes_client.py,1
docs/{source => }/assets/logos/vllm-logo-only-light.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
.github/workflows/{sphinx-lint.yml => doc-lint.yml},1
{cacheflow => vllm}/engine/ray_utils.py,1
tests/entrypoints/{ => pooling}/openai/test_classification.py,1
vllm/plugins/lora_resolvers/hf_hub_resolver.py,1
docs/source/quantization/fp8_e5m2_kvcache.md,1
examples/{online_serving/pooling => pooling/score}/jinaai_rerank_client.py,1
tests/models/multimodal/generation/test_voxtral_streaming.py,1
docs/{source => }/features/quantization/int4.md,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
docs/source/{serving/run_on_sky.md => deployment/frameworks/skypilot.md},1
docs/source/{serving/deploying_with_lws.md => deployment/frameworks/lws.md},1
.buildkite/scripts/annotate-rocm-release.sh,1
docs/source/{dev => design}/input_processing/model_inputs_index.rst,1
cacheflow/model_executor/layers/__init__.py,1
csrc/quantization/cutlass_w4a8/get_group_starts.cuh,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/chart-helm/templates/pvc.yaml,1
vllm/v1/worker/gpu/metrics/__init__.py,1
examples/online_serving/chart-helm/tests/deployment_test.yaml,1
tests/entrypoints/openai/{test_response_api_mcp_tools.py => responses/test_mcp_tools.py},1
tests/v1/structured_output/__init__.py,1
vllm/{attention/backends => model_executor/layers/attention}/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8.json",1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-triton.yaml,1
tools/{ => pre_commit}/generate_nightly_torch_test.py,1
tests/samplers/__init__.py,1
tests/models/{ => decoder_only/vision_language}/test_phi3v.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/{offline_inference_embedding.py => embedding.py},1
vllm/{entrypoints/openai => }/tool_parsers/granite_20b_fc_tool_parser.py,1
docs/{source => }/features/quantization/gptqmodel.md,1
examples/pooling/score/{vision_language_reranker.py => vision_reranker_offline.py},1
docs/{source => }/contributing/vulnerability_management.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/{source => }/assets/design/v1/prefix_caching/example-time-4.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
.gemini/config.yaml,1
docs/source/{serving/deploying_with_dstack.md => deployment/frameworks/dstack.md},1
examples/{online_serving => pooling/embed}/openai_embedding_long_text/service.sh,1
docs/{source => }/getting_started/installation/ai_accelerator/neuron.inc.md,1
vllm/logging/formatter.py,1
examples/{online_serving/pooling => pooling/embed}/embedding_requests_base64_client.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py,1
examples/{ => others}/lmcache/README.md,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/{production_monitoring => prometheus_grafana}/grafana.json,1
vllm/model_executor/parallel_utils/README.md,1
vllm/model_executor/models/bee.py,1
"vllm/model_executor/layers/fused_moe/configs/{E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}",1
vllm/utils/nvtx_pytorch_hooks.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=768,device_name=NVIDIA_H100_PCIe,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/v1/kv_connector/nixl_integration/{tp_config_sweep_accuracy_test.sh => config_sweep_accuracy_test.sh},1
rocm_patch/commonpy_xformers-0.0.23.rocm.patch,1
examples/{openai_api_client_for_multimodal.py => openai_chat_completion_client_for_multimodal.py},1
tests/multi_step/test_correctness.py,1
tests/standalone_tests/lazy_torch_compile.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
tests/models/decoder_only/vision_language/vlm_utils/__init__.py,1
examples/{ => offline_inference}/offline_inference.py,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=640,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/fixtures/audioflamingo3/expected_results_batched.json,1
vllm/{entrypoints/openai => }/tool_parsers/openai_tool_parser.py,1
examples/tool_chat_template_minimax_m1.jinja,1
docs/{source => }/assets/logos/vllm-logo-text-light.png,1
csrc/cpu/cpu_attn_vec.hpp,1
tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-triton.yaml,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_int8_dispatch.cuh,1
examples/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-prefiller-config.yaml,1
tests/v1/executor/__init__.py,1
tests/kernels/test_triton_moe_ptpc_fp8.py,1
vllm/{model_executor/parallel_utils => distributed}/parallel_state.py,1
docs/source/{serving => assets/deployment}/architecture_helm_deployment.png,1
tests/models/{decoder_only/language => language/generation}/test_granite.py,1
docs/source/design/multimodal/adding_multimodal_plugin.rst,1
examples/{ => online_serving}/prometheus_grafana/grafana.json,1
docs/source/assets/deployment/chatbox-chat.png,1
.buildkite/nightly-benchmarks/{ => tests}/throughput-tests.json,1
examples/pooling/embed/{openai_embedding_matryoshka_fy.py => openai_embedding_matryoshka_fy_client.py},1
vllm/model_executor/guided_decoding/{xgrammar_utils.py => utils.py},1
tests/utils_/test_import_utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/{engine/tokenizer_utils.py => transformers_utils/tokenizer.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
.buildkite/test_areas/basic_correctness.yaml,1
csrc/quantization/cutlass_w8a8/{scaled_mm_c3x.cu => scaled_mm_c3x_sm90.cu},1
vllm/entrypoints/openai/{serving_chat.py => chat_completion/serving.py},1
examples/pooling/classify/classification_online.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=768,device_name=NVIDIA_H20,dtype=fp8_w8a8.json",1
tests/{engine => tokenization}/test_detokenize.py,1
tests/kernels/{test_cutlass.py => quantization/test_cutlass_scaled_mm.py},1
tests/compile/{piecewise => fullgraph}/test_toy_llama.py,1
docs/{source => }/assets/deployment/chatbox-chat.png,1
vllm/tool_parsers/llama_tool_parser.py,1
examples/template_falcon_180b.jinja,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/{source => }/assets/deployment/dify-settings.png,1
vllm/entrypoints/serve/rlhf/__init__.py,1
docs/{source => }/assets/kernel/key.png,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/mappings.py,1
patch_xformers-0.0.22.post7.rocm.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/kernels/{ => attention}/test_cache.py,1
tests/v1/streaming_input/test_async_llm_streaming.py,1
tests/models/{ => decoder_only/language}/test_phimoe.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/multimodal/processing/test_audioflamingo3.py,1
vllm/{ => vllm_flash_attn}/fa_utils.py,1
vllm/v1/{offloading => kv_offload}/abstract.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/kernels/{ => attention}/test_triton_unified_attention.py,1
examples/chart-helm/lintconf.yaml,1
csrc/quantization/{gptq_marlin => marlin}/dequant.h,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/tool_parsers/pythonic_tool_parser.py,1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_idefics3.py,1
.buildkite/lm-eval-harness/configs/models-mm-small.txt,1
docs/source/usage/structured_outputs.rst,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_fp8_dispatch.cuh,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{offline_inference/pooling/prithvi_geospatial_mae.py => pooling/plugin/prithvi_geospatial_mae_offline.py},1
tests/models/{encoder_decoder/audio_language => multimodal/generation}/test_whisper.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/{serving => usage}/env_vars.rst,1
tests/entrypoints/pooling/llm/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
tests/renderers/__init__.py,1
vllm/entrypoints/{ => openai/responses}/context.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
examples/{ => offline_inference}/offline_inference_whisper.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20-3e.json",1
examples/offline_inference/{vision_language_embedding.py => vision_language_pooling.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
vllm/benchmarks/lib/__init__.py,1
docs/source/getting_started/installation/{xpu.md => gpu/xpu.inc.md},1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",1
tests/neuron/{ => 1_core}/test_cache.py,1
vllm/model_executor/layers/triton_kernel/__init__.py,1
vllm/{ => v1}/executor/uniproc_executor.py,1
examples/{offline_inference => pooling/pooling}/vision_language_pooling.py,1
docs/source/quantization/{fp8_e5m2_kv_cache.rst => fp8_e5m2_kvcache.rst},1
vllm/v1/core/__init__.py,1
vllm/v1/worker/gpu/lora_utils.py,1
docs/{source => }/models/pooling_models.md,1
docs/source/design/{arch_overview.rst => arch_overview.md},1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H200.json",1
examples/template_fuyu.jinja,1
vllm/{inputs.py => inputs/data.py},1
examples/offline_inference/{offline_inference_audio_language.py => audio_language.py},1
docs/contributing/{overview.md => README.md},1
vllm/compilation/__init__.py,1
tests/compile/README.md,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm100_fp8.cu,1
tests/kernels/helion/test_helion_available.py,1
.buildkite/performance-benchmarks/tests/latency-tests-arm64-cpu.json,1
docs/{source => }/assets/deployment/anything-llm-upload-doc.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_H100.json",1
examples/{ => offline_inference}/offline_inference_structured_outputs.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/assets/design/debug_vllm_compile/design_diagram.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/benchmarking/README.md,1
docs/{source => }/contributing/model/registration.md,1
vllm/{model_executor/parallel_utils => distributed/device_communicators}/custom_all_reduce.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",1
examples/production_monitoring/dummy_client.py,1
requirements.txt => requirements-common.txt,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/design/{kernel => }/paged_attention.md,1
docs/source/dev/sampling_params.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/cpu/{cpu_attn_macros.h => cpu_arch_macros.h},1
examples/tool_chat_template_granite_20b_fc.jinja,1
docs/source/{usage => features}/structured_outputs.md,1
examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => online_serving}/openai_pooling_client.py,1
cacheflow/{models/utils.py => model_executor/weight_utils.py},1
docs/{source => }/features/quantization/bnb.md,1
.buildkite/lm-eval-harness/configs/models-mm-large-h100.txt,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/assets/design/{v1 => }/metrics/intervals-1.png,1
examples/others/lmcache/README.md,1
tests/{mistral_tool_use => tool_use/mistral}/utils.py,1
vllm/utils/{functools.py => func_utils.py},1
tests/lora/{test_transfomers_model.py => test_transformers_model.py},1
docs/source/usage/performance.md,1
docs/source/{dev/engine/engine_index.md => api/engine/index.md},1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8.cu,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_fp8.cu,1
docs/assets/design/hybrid_kv_cache_manager/overview.png,1
"vllm/model_executor/layers/fused_moe/configs/E=32,N=1408,device_name=NVIDIA_B200.json",1
examples/{simple_server.py => llmserver_example.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
cacheflow/frontend/utils.py,1
docs/assets/design/hybrid_kv_cache_manager/sw_attn.png,1
tests/mistral_tool_use/__init__.py,1
docs/{source => }/community/sponsors.md,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json",1
tests/entrypoints/pooling/{llm/test_score.py => score/test_offline.py},1
tests/__init__.py,1
examples/{production_monitoring => opentelemetry}/Otel.md,1
cacheflow/{ => model_executor}/parallel_utils/parallel_state.py,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json",1
docs/source/getting_started/installation/{hpu-gaudi.md => ai_accelerator/hpu-gaudi.inc.md},1
docs/source/{usage => features}/compatibility_matrix.md,1
.buildkite/scripts/hardware_ci/run-cpu-test-s390x.sh,1
tests/models/{ => decoder_only/vision_language}/test_paligemma.py,1
vllm/model_executor/models/{siglip2.py => lfm2_siglip2.py},1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml,1
vllm/{entrypoints/openai => }/tool_parsers/hunyuan_a13b_tool_parser.py,1
tests/utils_/test_cache.py,1
tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py,1
docs/{source => }/design/huggingface_integration.md,1
examples/chart-helm/templates/deployment.yaml,1
tests/utils_/{test_collections.py => test_collection_utils.py},1
vllm/renderers/protocol.py,1
tests/{engine/output_processor => detokenizer}/test_stop_checker.py,1
csrc/quantization/cutlass_w8a8/{scaled_mm_dq_c2x.cu => scaled_mm_c2x.cu},1
docs/source/{models => usage}/structured_outputs.rst,1
tests/evals/gsm8k/configs/Qwen3-Next-FP8-EP2.yaml,1
vllm/model_executor/layers/quantization/compressed_tensors/transform/__init__.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm120_fp8.cu,1
"vllm/model_executor/layers/quantization/utils/configs/{N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
vllm/{entrypoints/openai => }/tool_parsers/mistral_tool_parser.py,1
.buildkite/scripts/scheduled_integration_test/{qwen30b_a3b_fp8_block_ep.sh => qwen30b_a3b_fp8_block_ep_eplb.sh},1
cacheflow/{master => frontend}/simple_frontend.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_A100-SXM4-80GB.json",1
docs/source/{dev/offline_inference/offline_index.md => api/offline_inference/index.md},1
examples/{simple_fastapi_client.py => api_client.py},1
csrc/quantization/gguf/moe_vec.cuh,1
"vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/entrypoints/{ => mcp}/tool.py,1
tests/entrypoints/{openai => }/conftest.py,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
tests/entrypoints/{ => pooling}/llm/test_encode.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
vllm/worker/{cpu_embedding_model_runner.py => cpu_pooling_model_runner.py},1
csrc/quantization/marlin/{ => dense}/marlin_cuda_kernel.cu,1
tests/{worker => }/spec_decode/test_multi_step_worker.py,1
tests/kernels/{ => quantization}/test_int8_quant.py,1
docs/{source => }/serving/metrics.md,1
tests/models/{decoder_only/vision_language => multimodal/generation}/test_pixtral.py,1
cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/utils.py,1
docs/{source => }/features/quantization/quark.md,1
cacheflow/model_executor/parallel_utils/parallel_state.py,1
docs/source/{usage => features}/lora.md,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_H200.json",1
vllm/multimodal/processing/__init__.py,1
docs/{source => }/serving/offline_inference.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/kernels/{ => quantization}/test_int8_kernel.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
examples/{online_serving/pooling/prithvi_geospatial_mae.py => pooling/plugin/prithvi_geospatial_mae_client.py},1
docs/source/features/quantization/modelopt.md,1
tests/v1/ec_connector/integration/hato.jpg,1
tests/kernels/moe/{deepep_utils.py => utils.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",1
docs/serving/engine_args.md,1
vllm/v1/spec_decode/draft_model.py,1
docs/{source => }/features/quantization/fp8.md,1
tests/v1/spec_decode/__init__.py,1
tests/models/{decoder_only/vision_language/test_models.py => multimodal/generation/test_common.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/{ => online_serving}/openai_chat_completion_client.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_A100-SXM4-80GB.json",1
vllm/lora/ops/triton_ops/{v1/v1_expand.py => lora_expand.py},1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H200.json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/quantization/fp8_e4m3_kvcache.md,1
Dockerfile.hpu => docker/Dockerfile.hpu,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
{cacheflow => vllm}/entrypoints/__init__.py,1
examples/pooling/score/template/bge-reranker-v2-gemma.jinja,1
vllm/{attention/layers => v1/attention/ops}/__init__.py,1
examples/pooling/score/template/qwen3_reranker.jinja,1
cacheflow/entrypoints/api_server.py,1
examples/{ => offline_inference}/offline_inference_arctic.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
csrc/sparse/cutlass/sparse_compressor_c3x.cuh,1
tests/evals/gsm8k/configs/models-h200.txt,1
examples/{offline_inference/pooling => pooling/score}/qwen3_reranker.py,1
docs/{source => }/assets/design/v1/prefix_caching/overview.png,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H100_80GB_HBM3.json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H200.json",1
tests/entrypoints/openai/test_guided_processors.py,1
{vllm/v1/tokenizer => tests/v1}/__init__.py,1
vllm/distributed/ec_transfer/ec_connector/__init__.py,1
examples/chart-helm/templates/configmap.yaml,1
tests/models/{ => decoder_only/vision_language}/test_blip2.py,1
tests/entrypoints/rpc/__init__.py,1
requirements-common.txt => requirements/common.txt,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
tests/compile/{piecewise => distributed}/__init__.py,1
vllm/profiler/{gpu_profiler.py => wrapper.py},1
docs/features/mooncake_connector_usage.md,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/w8a8/cutlass/moe/blockwise_scaled_group_mm_sm100.cu,1
tests/reasoning/test_gptoss_reasoning_parser.py,1
vllm/multimodal/{profiling.py => processing/dummy_inputs.py},1
tests/utils_/test_hashing.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/{entrypoints/openai => }/tool_parsers/jamba_tool_parser.py,1
assets/figures/perf_a100_n1_light.png,1
docs/source/dev/engine/{async_llm_engine.rst => async_llm_engine.md},1
examples/offline_inference_tpu.py,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=640,device_name=NVIDIA_H100,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/tool_parsers/glm47_moe_tool_parser.py,1
tests/kernels/{ => attention}/test_attention_selector.py,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
tests/config/test_model_arch_config.py,1
tests/kernels/{ => moe}/test_triton_moe_ptpc_fp8.py,1
examples/{ => offline_inference}/offline_inference_tpu.py,1
cacheflow/{entrypoints/fastapi_server.py => server/async_llm_server.py},1
tests/models/{ => decoder_only/language}/test_fp8.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/models/language/pooling/test_override_pooler_config.py,1
cacheflow/http_frontend/test_cli_client.py,1
csrc/cpu/sgl-kernels/gemm_fp8.cpp,1
cacheflow/{ => model_executor}/parallel_utils/README.md,1
cacheflow/{server/async_llm_server.py => engine/async_llm_engine.py},1
vllm/model_executor/layers/{ => fused_moe}/fused_moe.py,1
tests/models/__init__.py,1
tests/prompts/example.txt,1
vllm/third_party/flashmla/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=8192,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/deployment/frameworks/litellm.md,1
docs/source/design/multimodal/{multimodal_index.rst => multimodal_index.md},1
tests/utils_/test_collections.py,1
tests/kernels/{attention.py => test_attention.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/models/multimodal/{generation => pooling}/test_intern_vit.py,1
tests/v1/entrypoints/openai/{responses => serving_responses}/test_image.py,1
csrc/quantization/machete/machete_interleaving_utils.cuh,1
tests/engine/__init__.py,1
docs/source/contributing/profiling/profiling_index.rst,1
csrc/{moe_align_block_size_kernels.cu => moe/moe_align_sum_kernels.cu},1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/{source => }/features/tool_calling.md,1
vllm/entrypoints/openai/{ => engine}/protocol.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2112,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
{benchmark => benchmarks}/trace.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",1
vllm/plugins/lora_resolvers/__init__.py,1
docs/getting_started/installation/{cpu/x86.inc.md => cpu.x86.inc.md},1
docs/mkdocs/javascript/mathjax.js,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
tests/models/decoder_only/vision_language/mm_processor_kwargs/test_internvl.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/kernels/{ => quantization}/test_aqlm.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json",1
tests/kernels/{pos_encoding.py => test_pos_encoding.py},1
csrc/quantization/w8a8/int8/per_token_group_quant.cu,1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/entrypoints/serve/cache/api_router.py,1
vllm/entrypoints/openai/translations/__init__.py,1
docs/source/{serving/deploying_with_triton.md => deployment/frameworks/triton.md},1
vllm/{ => v1}/attention/ops/pallas_kv_cache_update.py,1
docs/source/api/{params.md => inference_params.md},1
vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/exllama.py,1
tests/entrypoints/{ => pooling}/openai/test_rerank.py,1
vllm/model_executor/layers/pooler/tokwise/__init__.py,1
codecov.yml,1
docs/source/design/input_processing/model_inputs_index.rst,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/model_executor/models/{hunyuan_v1_moe.py => hunyuan_v1.py},1
examples/{ => online_serving}/openai_chat_completion_client_for_multimodal.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
requirements-rocm-build.txt => requirements/rocm-build.txt,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_B200.json",1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H200.json",1
tests/tool_use/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/fused_moe/configs/E=62,N=256,device_name=NVIDIA_H100_80GB_HBM3.json",1
.buildkite/test_areas/engine.yaml,1
vllm/{entrypoints/openai => }/tool_parsers/internlm2_tool_parser.py,1
docs/{getting_started => usage}/faq.md,1
.buildkite/lm-eval-harness/configs/Minitron-4B-Base.yaml,1
cacheflow/parallel_utils/README.md,1
examples/offline_inference/{offline_inference_with_profiler.py => simple_profiling.py},1
tests/models/{embedding/language => language/pooling}/test_scoring.py,1
docs/{source => }/assets/design/v1/prefix_caching/example-time-5.png,1
tests/plugins_tests/test_stats_logger_plugins.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=5120,K=8192,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/model_executor/{guided_decoding.py => guided_decoding/outlines_decoding.py},1
docs/source/assets/figures/perf_a100_n1_dark.png,1
tests/neuron/test_block_table.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=8192,K=1536,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/getting_started/installation/{cpu-x86.md => cpu/index.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/offline_mode/__init__.py,1
docs/contributing/ci/nightly_builds.md,1
docs/design/{v1 => }/multiprocessing.md,1
.buildkite/test_areas/quantization.yaml,1
vllm/{entrypoints/openai => }/tool_parsers/qwen3coder_tool_parser.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/entrypoints/openai/responses/test_errors.py,1
tests/v1/distributed/test_hybrid_lb_dp.py,1
tests/entrypoints/{anthropic => openai}/test_messages.py,1
vllm/worker/{embedding_model_runner.py => pooling_model_runner.py},1
docs/source/quantization/fp8.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/{serving => models/extensions}/runai_model_streamer.md,1
tests/v1/engine/{test_detokenizer.py => test_output_processor.py},1
examples/{online_serving/pooling => pooling/score}/openai_cross_encoder_score_for_multimodal.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=384,device_name=NVIDIA_H20.json",1
csrc/quickreduce/base.h,1
docs/source/{dev/multimodal/adding_multimodal_model.rst => models/enabling_multimodal_inputs.rst},1
vllm/core/{embedding_model_block_manager.py => placeholder_block_space_manager.py},1
docs/source/assets/dev/dockerfile-stages-dependency.png,1
csrc/quantization/cutlass_w8a8/{cutlass_visitor_2x_broadcast_epilogue.hpp => broadcast_load_epilogue_c2x.hpp},1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/core.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/genai-perf-tests.json,1
tests/{tool_use => tool_parsers}/test_minimax_tool_parser.py,1
docs/source/dev/pooling_params.rst,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json",1
tests/kernels/{ => quantization}/test_awq_marlin.py,1
tests/models/{ => decoder_only/vision_language}/test_pixtral.py,1
examples/{ => online_serving}/prometheus_grafana/prometheus.yaml,1
vllm/tool_parsers/step3_tool_parser.py,1
Dockerfile.rocm_base => docker/Dockerfile.rocm_base,1
examples/offline_inference/{offline_chat_with_tools.py => chat_with_tools.py},1
docs/source/{usage => getting_started}/faq.md,1
vllm/model_executor/models/exaone_moe_mtp.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/launch-server.sh,1
tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/test_weight_utils.py,1
tests/neuron/test_activation.py,1
tests/v1/streaming_input/__init__.py,1
vllm/model_executor/layers/mamba/__init__.py,1
docs/source/assets/usage/disagg_prefill/abstraction.jpg,1
csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90_int8_dispatch.cuh,1
docs/source/{serving => usage}/compatibility_matrix.rst,1
tests/kernels/{ => attention}/test_rocm_attention_selector.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=96,device_name=NVIDIA_H20.json",1
tests/kernels/{ => attention}/test_cutlass_mla_decode.py,1
tests/models/{embedding => multimodal/pooling}/__init__.py,1
docs/source/getting_started/installation/{cpu/index.md => cpu.md},1
examples/{production_monitoring => prometheus_grafana}/docker-compose.yaml,1
examples/tool_chat_template_deepseekv31.jinja,1
tests/kernels/{ => attention}/test_triton_decode_attention.py,1
vllm/entrypoints/openai/{serving_engine.py => engine/serving.py},1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H20.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
{cacheflow => vllm}/engine/llm_engine.py,1
tests/v1/{sample/test_logits_processors.py => logits_processors/test_correctness.py},1
examples/{ => offline_inference}/offline_inference_distributed.py,1
tests/models/{embedding/vision_language => multimodal/pooling}/test_llava_next.py,1
"vllm/model_executor/layers/fused_moe/configs/E=384,N=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/assets/deployment/dp_external_lb.png,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
assets/figures/perf_a10g_n3_dark.png,1
examples/{llmserver_example.py => llm_engine_example.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/{source => }/deployment/frameworks/bentoml.md,1
.git-blame-ignore-revs,1
vllm/grpc/compile_protos.py,1
.buildkite/scripts/hardware_ci/run-gh200-test.sh,1
.buildkite/lm-eval-harness/configs/models-large-hopper.txt,1
vllm/model_executor/{neuron_model_loader.py => model_loader/neuron.py},1
tests/{tool_use => tool_parsers}/test_jamba_tool_parser.py,1
tests/entrypoints/{openai => sleep}/test_sleep.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/{dev => api}/engine/async_llm_engine.md,1
examples/chart-helm/templates/custom-objects.yaml,1
docs/source/assets/logos/vllm-logo-text-light.png,1
tests/models/{decoder_only/language => quantization}/test_fp8.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/assets/deployment/streamlit-chat.png,1
benchmarks/benchmark_hash.py,1
{cacheflow => vllm}/entrypoints/openai/__init__.py,1
vllm/model_executor/layers/quantization/utils/configs/README.md,1
docs/{source => }/deployment/integrations/kubeai.md,1
docs/{source => }/assets/features/disagg_prefill/abstraction.jpg,1
vllm/utils/mem_constants.py,1
docs/source/serving/deploying_with_helm.md,1
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-trtllm.yaml,1
tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-triton.yaml,1
vllm/v1/core/sched/__init__.py,1
tests/entrypoints/{test_openai_run_batch.py => openai/test_run_batch.py},1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/api/{README.md => summary.md},1
vllm/entrypoints/{openai/serving_pooling.py => pooling/pooling/serving.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/source/{usage => serving}/env_vars.md,1
examples/{ => offline_inference}/offline_inference_mlpspeculator.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI325X.json",1
examples/template_falcon.jinja,1
tests/models/embedding/__init__.py,1
tests/kernels/test_uva.py,1
docs/source/features/quantization/fp8_e5m2_kvcache.md,1
cacheflow/http_frontend/test_cli_client.py => test_cli_client.py,1
cacheflow/{worker => }/models/__init__.py,1
docs/serving/{distributed_serving.md => parallelism_scaling.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
tests/kv_transfer/{module_test.py => test_module.py},1
csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm80_dispatch.cuh,1
examples/tool_chat_template_xlam_qwen.jinja,1
tests/v1/determinism/test_rms_norm_batch_invariant.py,1
examples/{offline_inference/pooling => pooling/score}/convert_model_to_seq_cls.py,1
examples/lmcache/disagg_prefill_lmcache_v0.py,1
assets/figures/perf_a100_n3_dark.png,1
"vllm/model_executor/layers/fused_moe/configs/E=60,N=704,device_name=AMD_Instinct_MI300X.json",1
vllm/tool_parsers/seed_oss_tool_parser.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/utils.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/assets/design/{v1 => }/prefix_caching/example-time-1.png,1
tests/model_executor/test_qwen3_omni.py,1
tests/v1/spec_decode/test_speculators_eagle3.py,1
tests/kernels/{ => quantization}/test_gptq.py,1
examples/offline_inference_cli.py,1
.buildkite/test_areas/benchmarks.yaml,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json",1
examples/{other => others}/logging_configuration.md,1
tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/__init__.py,1
docs/{source => }/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md,1
docs/source/{models => usage}/lora.rst,1
docs/{source => }/performance/optimization.md,1
docs/{serving/seed_parameter_behavior.md => usage/reproducibility.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/{source => }/design/multiprocessing.md,1
examples/{ => offline_inference}/offline_inference_audio_language.py,1
"vllm/model_executor/layers/fused_moe/configs/E=62,N=512,device_name=AMD_Instinct_MI300X.json",1
{cacheflow => vllm}/worker/__init__.py,1
examples/{ => others}/lmcache/disagg_prefill_lmcache_v0.py,1
tests/models/test_llava_next_video.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/v1/attention/backends/__init__.py,1
tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/test_runai_model_streamer_loader.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json",1
vllm/entrypoints/openai/translations/serving.py,1
docs/{source => }/assets/kernel/logits_vec.png,1
.buildkite/lm-eval-harness/configs/Qwen2.5-VL-7B-Instruct.yaml,1
tests/lora/{test_qwen2vl.py => test_qwenvl.py},1
tests/compile/{ => fullgraph}/test_multimodal_compile.py,1
requirements-dev.txt => requirements/dev.txt,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/blockwise_scaled_group_mm_sm100.cu,1
docs/{source => }/features/disagg_prefill.md,1
tests/v1/{ => distributed}/test_hybrid_lb_dp.py,1
tests/models/language/generation/test_grok.py,1
docs/{source => }/deployment/frameworks/dstack.md,1
tests/v1/{generation => determinism}/test_rms_norm_batch_invariant.py,1
docs/{source => }/assets/design/v1/metrics/intervals-1.png,1
vllm/distributed/kv_transfer/{kv_transfer_agent.py => kv_connector_agent.py},1
docs/source/deployment/integrations/llamastack.md,1
examples/{offline_inference/cpu_offload_lmcache.py => lmcache/cpu_offload_lmcache_v0.py},1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_A800-SXM4-80GB.json",1
cacheflow/parallel_utils/utils.py,1
examples/pooling/embed/{embed_jina_embeddings_v3.py => embed_jina_embeddings_v3_offline.py},1
examples/offline_inference/{offline_inference_scoring.py => scoring.py},1
docs/source/{dev => design}/kernel/paged_attention.rst,1
docs/source/dev/engine/engine_index.md,1
tests/v1/sample/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=768,device_name=NVIDIA_B300_SXM6_AC,dtype=fp8_w8a8.json",1
vllm/v1/metrics/perf.py,1
examples/online_serving/pooling/embedding_requests_bytes_client.py,1
examples/{other => others}/tensorize_vllm_model.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
benchmarks/{ => auto_tune}/auto_tune.sh,1
docs/source/dev/engine/{llm_engine.rst => llm_engine.md},1
{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/layers.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_H200.json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_A100-SXM4-40GB.json",1
tests/benchmarks/test_random_multimodal_dataset_video.py,1
tests/{ => model_executor/model_loader}/fastsafetensors_loader/__init__.py,1
docs/{source => }/getting_started/installation/device.template.md,1
tests/kernels.py,1
tests/v1/streaming_input/test_scheduler_streaming.py,1
vllm/{entrypoints/openai/reasoning_parsers => reasoning}/abs_reasoning_parsers.py,1
tests/v1/entrypoints/llm/__init__.py,1
tools/profiler/nsys_profile_tools/images/html.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/entrypoints/serve/tokenize/__init__.py,1
vllm/tool_parsers/gigachat3_tool_parser.py,1
vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_engine.py,1
tests/kernels/{ => quantization}/test_block_fp8.py,1
tests/v1/kv_connector/unit/test_cache_pollution_prevention.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3.json",1
examples/other/fp8/README.md,1
docs/assets/design/fused_moe_modular_kernel/fused_experts_blocks.png,1
docs/source/getting_started/{tpu-installation.md => installation/tpu.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/source/{ => features}/quantization/bnb.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/v1/kv_offload/__init__.py,1
vllm/model_executor/layers/{attention.py => attention/backends/xformers.py},1
tests/entrypoints/pooling/{llm/test_reward.py => reward/test_offline.py},1
examples/openai_chat_completion_client_with_tools.py,1
vllm/model_executor/{tensorizer_loader.py => model_loader/tensorizer.py},1
cacheflow/{server => engine}/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ht.yaml,1
vllm/device_allocator/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=32,N=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=928,device_name=NVIDIA_L40S.json",1
vllm/{entrypoints/openai/reasoning_parsers => reasoning}/granite_reasoning_parser.py,1
docs/{source => }/assets/deployment/anything-llm-provider.png,1
{examples => vllm/transformers_utils/chat_templates}/template_blip2.jinja,1
.buildkite/{ => scripts}/run-multi-node-test.sh,1
docs/{ => serving}/seed_parameter_behavior.md,1
docs/deployment/integrations/kuberay.md,1
docs/{source => }/design/arch_overview.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/offline_inference/{offline_inference_with_prefix.py => prefix_caching.py},1
tests/models/test_modelopt.py,1
examples/chart-helm/Chart.yaml,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_helper.hpp,1
cacheflow/models/layernorm.py,1
csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_dispatch.cuh,1
vllm/{ => v1}/attention/ops/chunked_prefill_paged_decode.py,1
vllm/v1/worker/gpu/mm/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H20-3e.json",1
vllm/{model_executor/layers => }/attention/ops/__init__.py,1
docs/source/usage/lora.rst,1
docs/source/{dev => api}/engine/llm_engine.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/neuron/{ => 1_core}/test_rotary_embedding.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh,1
docs/assets/features/disagg_encoder/disagg_encoder_flow.png,1
docs/{source => }/assets/kernel/v_vec.png,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json",1
docs/{source => }/getting_started/v1_user_guide.md,1
docs/features/{compatibility_matrix.md => README.md},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
cacheflow/{ => model_executor}/parallel_utils/utils.py,1
docs/{source => }/contributing/deprecation_policy.md,1
docs/{source => }/features/quantization/int8.md,1
vllm/model_executor/layers/{shared_fused_moe => fused_moe}/shared_fused_moe.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI325X.json",1
tests/entrypoints/{test_llm_generate.py => llm/test_generate.py},1
tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/__init__.py,1
docs/source/assets/figures/perf_a10g_n3_light.png,1
"vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/layers.py,1
examples/{ => online_serving}/chart-helm/lintconf.yaml,1
vllm/{model_executor/layers => }/attention/ops/prefix_prefill.py,1
vllm/{entrypoints/openai => }/tool_parsers/qwen3xml_tool_parser.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H200.json",1
docs/source/getting_started/installation/cpu/s390x.inc.md,1
tests/{runai_model_streamer => runai_model_streamer_test}/test_runai_model_streamer_loader.py,1
docs/contributing/{ci-failures.md => ci/failures.md},1
vllm/v1/worker/gpu/sample/__init__.py,1
docs/source/{usage => serving}/engine_args.md,1
tests/kernels/{ => moe}/test_cutlass_moe.py,1
cacheflow/model_executor/parallel_utils/tensor_parallel/mappings.py,1
vllm/model_executor/layers/quantization/quark/__init__.py,1
vllm/entrypoints/openai/chat_completion/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/{ => v1}/attention/ops/rocm_aiter_mla_sparse.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=1024,device_name=AMD_Instinct_MI325X,block_shape=[128,128].json",1
vllm/model_executor/layers/quantization/kernels/scaled_mm/rocm.py,1
csrc/quantization/{ => w8a8}/fp8/amd/quant_utils.cuh,1
tests/multimodal/assets/rgba.png,1
requirements-tpu.txt => requirements/tpu.txt,1
.buildkite/image_build/image_build_cpu_arm64.sh,1
examples/offline_inference/{ => basic}/basic.py,1
docs/design/{v1 => }/p2p_nccl_connector.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/pooling/{llm => classify}/__init__.py,1
csrc/quantization/w8a8/cutlass/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh,1
examples/offline_inference/{offline_inference_classification.py => classification.py},1
vllm/tool_parsers/xlam_tool_parser.py,1
{cacheflow => vllm}/core/policy.py,1
tests/engine/{test_custom_executor.py => test_executor.py},1
tests/entrypoints/{openai => rpc}/test_collective_rpc.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
cacheflow/{ => model_executor}/models/gpt2.py,1
tests/entrypoints/{test_openai_vision.py => openai/test_vision.py},1
tools/install_torchcodec_rocm.sh,1
csrc/quantization/w8a8/int8/scaled_quant.cu,1
docs/{source => }/getting_started/installation/gpu/cuda.inc.md,1
tests/entrypoints/pooling/{openai/test_embedding.py => embed/test_online.py},1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests.json,1
vllm/model_executor/layers/{triton_kernel => attention/backends}/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/kernels/test_mrope.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H200.json",1
tests/{ => model_executor/model_loader}/tensorizer_loader/__init__.py,1
csrc/attention/{attention_dtypes.cuh => attention_dtypes.h},1
vllm/attention/ops/__init__.py,1
tests/v1/kv_connector/__init__.py,1
vllm/distributed/kv_transfer/disagg_prefill_workflow.jpg,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/getting_started/installation/{gpu/index.md => gpu.md},1
tests/entrypoints/{ => openai}/test_guided_processors.py,1
docs/assets/design/{v1 => }/metrics/intervals-3.png,1
vllm/entrypoints/anthropic/{serving_messages.py => serving.py},1
tests/multimodal/assets/corrupted.mp4,1
"vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml,1
tests/models/decoder_only/vision_language/processing/test_qwen2_vl.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/{source => }/deployment/frameworks/litellm.md,1
tests/kernels/{ => core}/test_rotary_embedding.py,1
tests/multimodal/media/test_base.py,1
vllm/v1/worker/gpu/{ => sample}/penalties.py,1
tests/models/language/pooling/conftest.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/assets/figures/perf_a100_n1_light.png,1
.github/ISSUE_TEMPLATE/{700-performance discussion.yml => 700-performance-discussion.yml},1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_llava_next.py,1
tests/distributed/test_eplb_fused_moe_layer.py,1
examples/{ => other}/tensorize_vllm_model.py,1
docs/{source => }/assets/features/disagg_prefill/overview.jpg,1
vllm/entrypoints/serve/instrumentator/static/swagger-ui.css,1
vllm/{entrypoints/openai/reasoning_parsers => reasoning}/__init__.py,1
examples/offline_inference/{offline_inference_vision_language.py => vision_language.py},1
tests/entrypoints/sagemaker/__init__.py,1
vllm/entrypoints/openai/parser/__init__.py,1
vllm/compilation/{cuda_piecewise_backend.py => piecewise_backend.py},1
tests/entrypoints/openai/__init__.py,1
csrc/{cache_kernel.cu => cache_kernels.cu},1
vllm/entrypoints/pooling/embed/__init__.py,1
tests/entrypoints/pooling/{openai => embed}/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
vllm/entrypoints/mcp/__init__.py,1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8_dispatch.cuh,1
tests/v1/{ => distributed}/test_external_lb_dp.py,1
examples/{ => offline_inference}/offline_inference_with_profiler.py,1
docs/source/{serving => usage}/faq.rst,1
docs/{source => }/features/quantization/bitblas.md,1
cacheflow/{server/llm_server.py => engine/llm_engine.py},1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=bf16.json",1
docs/source/{ => features}/quantization/fp8_e4m3_kvcache.md,1
tests/models/test_danube3_4b.py,1
.buildkite/scripts/ci-clean-log.sh,1
benchmarks/kernels/benchmark_fused_collective.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/assets/figures/perf_a10g_n3_dark.png,1
tests/tool_parsers/test_glm4_moe_tool_parser.py,1
cacheflow/entrypoints/openai/__init__.py,1
vllm/model_executor/layers/sparse_attn_indexer.py,1
examples/offline_inference/{ => pooling}/embed_jina_embeddings_v3.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/compile/{ => distributed}/test_async_tp.py,1
vllm/benchmarks/{ => lib}/endpoint_request_func.py,1
vllm/entrypoints/{score_utils.py => pooling/score/utils.py},1
vllm/{model_executor/parallel_utils => distributed/device_communicators}/pynccl_utils.py,1
vllm/{attention/layers => model_executor/layers/attention}/encoder_only_attention.py,1
vllm/{entrypoints/openai => }/tool_parsers/hermes_tool_parser.py,1
.buildkite/scripts/trigger-ci-build.sh,1
docs/{serving => usage}/usage_stats.md,1
docs/{source => }/deployment/security.md,1
examples/online_serving/opentelemetry/Otel.md,1
tests/tool_parsers/test_jamba_tool_parser.py,1
tests/entrypoints/openai/tool_parsers/test_openai_tool_parser.py,1
examples/online_serving/openai_responses_client_with_mcp_tools.py,1
tests/{tokenization/test_mistral_tokenizer.py => tokenizers_/test_mistral.py},1
vllm/model_executor/warmup/__init__.py,1
tests/models/decoder_only/vision_language/mm_processor_kwargs/__init__.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/throughput-tests.json,1
csrc/quantization/cutlass_w8a8/c3x/scaled_mm_azp_sm90_int8.cu,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json",1
csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm100_fp8_dispatch.cuh,1
tests/models/{embedding/language => language/pooling}/test_gritlm.py,1
vllm/{model_executor/layers => }/attention/backends/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
examples/{production_monitoring => prometheus_grafana}/README.md,1
tests/entrypoints/pooling/__init__.py,1
examples/offline_inference_whisper.py,1
docs/{source => }/assets/design/hierarchy.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
docs/serving/serve_args.md,1
csrc/cutlass_extensions/epilogue/broadcast_load_epilogue_array_c3x.hpp,1
docs/source/assets/logos/vllm-logo-only-light.png,1
examples/pooling/embed/{embed_matryoshka_fy.py => embed_matryoshka_fy_offline.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H20-3e.json",1
examples/{ => online_serving}/chart-helm/templates/hpa.yaml,1
examples/pooling/token_embed/{jina_embeddings_v4.py => jina_embeddings_v4_offline.py},1
tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-fi-cutlass.yaml,1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/runners.py,1
docs/source/getting_started/installation/{ai_accelerator/index.md => ai_accelerator.md},1
vllm/lora/ops/{ => triton_ops}/bgmv_expand_slice.py,1
docs/source/serving/deploying_with_triton.md,1
tests/entrypoints/pooling/{openai/test_rerank.py => score/test_online_rerank.py},1
.clang-format,1
csrc/quantization/{ => w8a8}/fp8/common.cu,1
.buildkite/ci_config.yaml,1
docs/source/deployment/frameworks/lobe-chat.md,1
"vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_H100,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/{source => }/deployment/frameworks/dify.md,1
tests/kernels/{ => quantization}/test_fp8_quant.py,1
tests/kernels/{ => core}/test_activation.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
examples/{ => online_serving}/chart-helm/values.schema.json,1
tests/entrypoints/{openai => pooling}/correctness/test_mteb_score.py,1
Dockerfile.cpu => docker/Dockerfile.cpu,1
tests/v1/e2e/__init__.py,1
tests/multimodal/assets/image2.png,1
assets/figures/perf_a10g_n1_light.png,1
docs/source/serving/deploying_with_nginx.md,1
csrc/quantization/{gptq_marlin => marlin}/gptq_marlin_repack.cu,1
vllm/model_executor/layers/fused_moe/oracle/__init__.py,1
tests/tool_parsers/__init__.py,1
docs/source/models/{supported_models.rst => supported_models.md},1
tests/models/{decoder_only => language}/__init__.py,1
vllm/transformers_utils/configs/bagel.py,1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml,1
tests/models/decoder_only/language/test_nvfp4.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/model_executor/layers/attention/ops/__init__.py,1
examples/{ => other}/fp8/quantizer/quantize.py,1
docs/design/{v1 => }/torch_compile.md,1
tests/entrypoints/pooling/{correctness => basic}/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
docs/source/contributing/{overview.rst => overview.md},1
tests/{engine/test_detokenization.py => detokenizer/test_disable_detokenization.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/latency-tests-cpu.json,1
gradio_webserver.py => examples/gradio_webserver.py,1
tests/entrypoints/{ => pooling}/llm/test_score.py,1
tests/models/{embedding/language => language/pooling}/test_embedding.py,1
tests/models/{decoder_only/vision_language => multimodal/generation}/test_qwen2_vl.py,1
examples/online_serving/{disagg_xpyd/disagg_prefill_proxy_xpyd.py => disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py},1
examples/pooling/embed/openai_embedding_long_text/client.py,1
docs/source/{automatic_prefix_caching/details.md => design/automatic_prefix_caching.md},1
csrc/attention/attention_dtypes.cuh,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/quantization/utils/configs/{N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=NVIDIA_H200.json",1
benchmarks/benchmark_text_completion.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
.buildkite/performance-benchmarks/tests/throughput-tests-arm64-cpu.json,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
vllm/utils/counter.py,1
docs/source/getting_started/{neuron-installation.md => installation/neuron.md},1
docs/{source => }/assets/deployment/anything-llm-chat-with-doc.png,1
vllm/v1/{offloading => kv_offload}/worker/worker.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
tests/{worker => }/spec_decode/__init__.py,1
tools/profiler/nsys_profile_tools/images/csv1.png,1
docs/source/{ => features}/quantization/int8.md,1
tests/models/{ => quantization}/test_gptq_bitblas.py,1
docs/assets/design/{v1 => }/metrics/intervals-2.png,1
.buildkite/image_build/image_build_cpu.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_H200.json",1
"vllm/model_executor/layers/fused_moe/configs/E=40,N=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8.json",1
docs/source/deployment/frameworks/dify.md,1
docs/{source => }/getting_started/installation/cpu/build.inc.md,1
cacheflow/http_frontend/gradio_webserver.py,1
vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu.py,1
examples/{ => offline_inference}/llm_engine_example.py,1
vllm/exceptions.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/neuron/{ => 1_core}/test_layernorm.py,1
csrc/cpu/cpu_attn_vec16.hpp,1
examples/openai_chat_completion_structured_outputs.py,1
tests/benchmarks/{ => sweep}/test_param_sweep.py,1
.buildkite/{ => scripts/hardware_ci}/run-cpu-test.sh,1
tests/{ => model_executor/model_loader}/tensorizer_loader/test_tensorizer.py,1
docs/source/{serving => features}/prompt_embeds.md,1
examples/lmcache/cpu_offload_lmcache.py,1
vllm/{vllm_flash_attn => attention/utils}/fa_utils.py,1
tests/{ => utils_}/test_utils.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
docs/assets/deployment/hf-inference-endpoints-catalog.png,1
tests/model_executor/model_loader/runai_model_streamer/test_runai_model_streamer_loader.py,1
tests/entrypoints/{test_openai_embedding.py => openai/test_embedding.py},1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/custom_inputs.py,1
examples/{online_serving => pooling/embed}/openai_embedding_long_text/README.md,1
vllm/model_executor/models/mistral_large_3.py,1
examples/online_serving/dashboards/grafana/query_statistics.json,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm.cuh,1
.buildkite/lm-eval-harness/configs/Qwen3-235B-A22B-Instruct-2507-FP8.yaml,1
vllm/model_executor/{guided_logits_processors.py => guided_decoding/outlines_logits_processors.py},1
vllm/core/{block_manager.py => block_manager_v1.py},1
tests/{runai_model_streamer => runai_model_streamer_test}/__init__.py,1
csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cuh,1
docs/assets/design/fused_moe_modular_kernel/fused_moe_non_batched.png,1
vllm/executor/{multiproc_gpu_executor.py => mp_distributed_executor.py},1
tests/models/{ => decoder_only/language}/test_granite.py,1
examples/online_serving/chart-helm/templates/_helpers.tpl,1
tests/models/{decoder_only/vision_language => multimodal}/processing/test_llava_onevision.py,1
vllm/model_executor/layers/pooler/seqwise/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json",1
vllm/entrypoints/openai/{serving_models.py => models/serving.py},1
docs/source/deployment/frameworks/chatbox.md,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=NVIDIA_H100,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
.github/.bc-linter.yml,1
docs/source/{models => usage}/performance.rst,1
docs/{source => }/assets/deployment/anything-llm-chat-without-doc.png,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=2688,device_name=NVIDIA_H100_80GB_HBM3.json",1
tests/tensorizer/__init__.py,1
docs/source/assets/deployment/anything-llm-chat-without-doc.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/v1/kv_connector/unit/{test_shared_storage_connector.py => test_example_connector.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tools/{ => pre_commit}/update-dockerfile-graph.sh,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/entrypoints/cli/benchmark/startup.py,1
examples/offline_inference/{offline_inference_vision_language_multi_image.py => vision_language_multi_image.py},1
docs/{source => }/serving/integrations/langchain.md,1
examples/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-decoder-config.yaml,1
docs/{source => }/assets/kernel/k_vecs.png,1
examples/pooling/token_embed/jina_embeddings_v4.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json",1
tests/lora/{test_multi_loras_with_tp.py => test_llm_with_multi_loras.py},1
tests/multimodal/media/__init__.py,1
docs/source/getting_started/installation/gpu.md,1
tests/reasoning/test_minimax_m2_reasoning_parser.py,1
docs/{source => }/assets/design/arch_overview/llm_engine.excalidraw.png,1
tests/models/language/generation_ppl_test/test_qwen.py,1
examples/offline_inference/kv_load_failure_recovery/{rogue_shared_storage_connector.py => load_recovery_example_connector.py},1
tests/worker/spec_decode/test_multi_step_worker.py,1
csrc/cutlass_extensions/vllm_type_utils.cuh,1
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-CT-vllm-cutlass.yaml,1
examples/online_serving/{ => pooling}/prithvi_geospatial_mae.py,1
docs/source/getting_started/{arm-installation.md => installation/cpu-arm.md},1
vllm/entrypoints/serve/lora/__init__.py,1
tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_qwen.py,1
examples/{ => online_serving}/disaggregated_prefill.sh,1
cacheflow/http_frontend/gradio_webserver.py => gradio_webserver.py,1
cacheflow/core/__init__.py,1
vllm/entrypoints/cli/benchmark/mm_processor.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=2560,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
csrc/permute_cols.cu,1
tests/kernels/quantization/test_fp8_min_max_helper.py,1
csrc/quantization/fp8/{fp8_cuda_kernels.cu => common.cu},1
{cacheflow => vllm}/outputs.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/tool_parsers/internlm2_tool_parser.py,1
tests/kernels/{ => quantization}/test_block_int8.py,1
vllm/model_executor/layers/quantization/kernels/scaled_mm/pytorch.py,1
.buildkite/{ => scripts}/run-benchmarks.sh,1
tests/kernels/test_nvfp4_scaled_mm.py,1
tests/models/language/{pooling => pooling_mteb_test}/test_intfloat.py,1
tests/models/quantization/test_gpt_oss_attn_quantization.py,1
docs/source/getting_started/{debugging.md => troubleshooting.md},1
tests/tool_parsers/test_seed_oss_tool_parser.py,1
examples/online_serving/{ => pooling}/openai_chat_embedding_client_for_multimodal.py,1
docs/source/assets/design/arch_overview/entrypoints.excalidraw.png,1
tests/{entrypoints/openai/reasoning_parsers => reasoning}/test_granite_reasoning_parser.py,1
examples/pooling/score/using_template_offline.py,1
vllm/model_inspection.py,1
tools/{ => pre_commit}/png-lint.sh,1
{cacheflow => vllm}/model_executor/layers/activation.py,1
tests/models/decoder_only/audio_language/test_granite_speech.py,1
docs/assets/design/fused_moe_modular_kernel/prepare_and_finalize_blocks.png,1
.buildkite/performance-benchmarks/performance-benchmarks-descriptions.md,1
examples/pooling/pooling/{openai_pooling_client.py => pooling_online.py},1
docs/source/{serving/deploying_with_nginx.md => deployment/nginx.md},1
{cacheflow => vllm}/engine/__init__.py,1
tests/models/language/pooling/test_multi_vector_retrieval.py,1
csrc/cpu/sgl-kernels/moe_fp8.cpp,1
docs/{source => }/performance/benchmarks.md,1
docs/source/api/offline_inference/llm.md,1
docs/getting_started/installation/{cpu/apple.inc.md => cpu.apple.inc.md},1
tests/tool_use/mistral/conftest.py,1
csrc/quantization/marlin/dense/LICENSE,1
vllm/{model_executor/parallel_utils => distributed/device_communicators}/pynccl.py,1
examples/offline_inference/{offline_inference.py => basic.py},1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-marlin.yaml,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
csrc/quantization/{ => w8a8}/per_token_group_quant_8bit.h,1
cacheflow/{master => core}/server.py,1
tests/models/{decoder_only/vision_language => multimodal/generation}/__init__.py,1
docs/source/getting_started/installation/{gpu-rocm.md => gpu/rocm.inc.md},1
examples/{online_serving/pooling => pooling/embed}/openai_embedding_client.py,1
tests/v1/kv_connector/unit/test_error_propagation.py,1
tests/plugins/vllm_add_dummy_stat_logger/setup.py,1
{cacheflow => vllm}/model_executor/parallel_utils/README.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/{gptq_marlin => marlin}/kernel.h,1
tests/compile/{ => fullgraph}/test_basic_correctness.py,1
vllm/model_executor/layers/pooler/__init__.py,1
csrc/custom_quickreduce.cu,1
docs/source/dev/pooling_params.md,1
docs/source/deployment/frameworks/anything-llm.md,1
vllm/multimodal/{ => media}/base.py,1
tests/system_messages/sonnet3.5_nov2024.txt,1
"vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
benchmarks/kernels/{benchmark_trtllm_attention.py => benchmark_trtllm_decode_attention.py},1
tests/{entrypoints/openai/reasoning_parsers => reasoning}/test_deepseekr1_reasoning_parser.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/model_executor/layers/pooler/activations.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_B200.json",1
vllm/model_executor/layers/quantization/kernels/mixed_precision/cpu.py,1
vllm/distributed/eplb/policy/__init__.py,1
tests/models/decoder_only/vision_language/test_interleaved.py,1
tests/engine/output_processor/__init__.py,1
vllm/lora/ops/{ => triton_ops}/sgmv_expand.py,1
"vllm/model_executor/layers/quantization/utils/configs/{N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/template_chameleon.jinja => vllm/transformers_utils/chat_templates/template_basic.jinja,1
csrc/cpu/float_convert.hpp,1
tests/spec_decode/e2e/{test_correctness.py => test_multistep_correctness.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=8192,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/design/multimodal/multimodal_index.md,1
tests/v1/kv_connector/unit/{test_output_aggreagator.py => test_output_aggregator.py},1
docs/source/usage/env_vars.md,1
docs/assets/design/cuda_graphs/wrapper_flow.png,1
tests/kernels/{ => quantization}/test_awq_triton.py,1
docs/source/assets/figures/perf_a10g_n1_dark.png,1
tests/models/decoder_only/vision_language/__init__.py,1
docs/{source => }/deployment/frameworks/lws.md,1
tests/models/{ => decoder_only/language}/test_modelopt.py,1
vllm/transformers_utils/configs/tarsier2.py,1
examples/openi_example_batch.jsonl,1
examples/offline_inference/llm_engine_reset_kv.py,1
docs/source/{ => features}/quantization/auto_awq.md,1
docs/getting_started/installation/{cpu/s390x.inc.md => cpu.s390x.inc.md},1
vllm/entrypoints/serve/rlhf/api_router.py,1
examples/lmcache/{cpu_offload_lmcache_v0.py => cpu_offload_lmcache.py},1
cacheflow/{models/model_utils.py => model_executor/model_loader.py},1
benchmarks/kernels/benchmark_2d_silu_mul_fp8_quant.py,1
{benchmark => benchmarks}/benchmark_text_completion.py,1
benchmarks/kernels/bench_nvfp4_qutlass.py,1
tests/v1/entrypoints/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=10240,K=5120,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/assets/deployment/dify-create-chatbot.png,1
tests/models/{embedding/language => language/pooling}/test_truncation_control.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8.json",1
examples/tool_chat_template_mistral.jinja,1
vllm/entrypoints/openai/{serving_chat_stream_harmony.py => chat_completion/stream_harmony.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/models/{decoder_only/language => language/generation}/test_hybrid.py,1
docs/assets/deployment/hf-inference-endpoints-configure-container.png,1
tests/v1/core/{test_specialized_manager.py => test_single_type_kv_cache_manager.py},1
.buildkite/scripts/cherry-pick-from-milestone.sh,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI325X.json",1
.buildkite/test_areas/weight_loading.yaml,1
vllm/{entrypoints/openai => }/tool_parsers/deepseekv3_tool_parser.py,1
tests/v1/entrypoints/openai/{responses => serving_responses}/conftest.py,1
vllm/entrypoints/serve/instrumentator/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=352,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
examples/template_qwen_vl_chat.jinja,1
tests/lora/{test_punica_variation.py => test_punica_ops_variation.py},1
examples/online_serving/chart-helm/tests/pvc_test.yaml,1
docs/source/assets/design/arch_overview/llm_engine.excalidraw.png,1
examples/pooling/score/{openai_cross_encoder_score.py => score_api_online.py},1
"vllm/model_executor/layers/quantization/utils/configs/{N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/{kernels.py => kernels/cache.py},1
benchmarks/kernels/bench_mxfp4_qutlass.py,1
tests/runai_model_streamer_test/test_runai_utils.py,1
vllm/entrypoints/openai/engine/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/{E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}",1
examples/offline_inference/{offline_inference_neuron_int8_quantization.py => neuron_int8_quantization.py},1
cacheflow/{server => engine}/ray_utils.py,1
tests/kernels/{ => quantization}/test_gguf.py,1
vllm/distributed/kv_transfer/kv_connector/v1/p2p/__init__.py,1
docs/assets/design/{v1 => }/prefix_caching/free.png,1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=AMD_Instinct_MI350_OAM,dtype=fp8_w8a8.json",1
examples/{ => offline_inference}/save_sharded_state.py,1
docs/{source => }/training/trl.md,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1344,device_name=NVIDIA_H100_80GB_HBM3.json",1
docs/assets/design/cuda_graphs/current_design.png,1
tests/entrypoints/pooling/pooling/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml,1
.buildkite/scripts/upload-rocm-wheels.sh,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c3x_sm120.cu,1
tests/kernels/test_int8_kernel.py,1
examples/pooling/classify/openai_classification_client.py,1
tests/{samplers => engine}/test_stop_reason.py,1
docs/training/trl.md,1
.buildkite/nightly-benchmarks/{run-benchmarks-suite.sh => scripts/run-performance-benchmarks.sh},1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=AMD_Instinct_MI300X.json",1
docs/source/{serving/deploying_with_docker.md => deployment/docker.md},1
examples/disaggregated_prefill.sh,1
examples/other/fp8/quantizer/quantize.py,1
tests/entrypoints/{test_llm_generate_multiple_loras.py => llm/test_generate_multiple_loras.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/compile/__init__.py,1
docs/source/api/params.md,1
docs/source/design/input_processing/input_processing_pipeline.rst,1
examples/tool_chat_template_deepseekv3.jinja,1
examples/{ => offline_inference}/offline_inference_encoder_decoder.py,1
docs/getting_started/installation/{ai_accelerator/tpu.inc.md => google_tpu.md},1
tests/models/test_granite.py,1
patch_xformers.rocm.sh,1
tests/entrypoints/openai/{test_response_api_simple.py => responses/test_simple.py},1
cacheflow/model_executor/weight_utils.py,1
tests/models/language/{pooling => pooling_mteb_test}/test_jina.py,1
vllm/tool_parsers/olmo3_tool_parser.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm75_dispatch.cuh,1
vllm/model_executor/layers/fla/ops/__init__.py,1
csrc/quantization/cutlass_w8a8/moe/{grouped_mm_c3x.cu => grouped_mm_c3x_sm90.cu},1
docs/source/deployment/integrations/llmaz.md,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/Epilogues.md,1
vllm/v1/core/{specialized_manager.py => single_type_kv_cache_manager.py},1
docs/source/{dev => api}/offline_inference/llm.md,1
vllm/model_executor/parallel_utils/{tensor_parallel => }/utils.py,1
tests/v1/worker/__init__.py,1
vllm/v1/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=62,N=128,device_name=AMD_Instinct_MI300X.json",1
tests/models/{ => decoder_only/audio_language}/test_ultravox.py,1
tests/models/{ => decoder_only/vision_language}/test_intern_vit.py,1
docs/source/deployment/frameworks/streamlit.md,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm80_dispatch.cuh,1
tests/entrypoints/pooling/{llm => basic}/test_encode.py,1
tests/model_executor/__init__.py,1
docs/source/getting_started/{gaudi-installation.md => installation/hpu-gaudi.md},1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_kernels.hpp,1
docs/{source => }/assets/design/arch_overview/entrypoints.excalidraw.png,1
docs/getting_started/installation/{cpu/arm.inc.md => cpu.arm.inc.md},1
docs/assets/{kernel => design/paged_attention}/v_vec.png,1
examples/{ => online_serving}/opentelemetry/Otel.md,1
tests/entrypoints/{ => pooling}/openai/test_embedding_dimensions.py,1
examples/{ => offline_inference/offline_inference_openai}/openai_example_batch.jsonl,1
tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ht.yaml,1
docs/source/{dev => api}/offline_inference/llm_inputs.md,1
vllm/v1/core/{scheduler_output.py => sched/output.py},1
vllm/model_executor/parallel_utils/tensor_parallel/random.py,1
tests/kernels/{ => quantization}/test_ggml.py,1
docs/assets/design/cuda_graphs/previous_design.png,1
"vllm/model_executor/layers/quantization/utils/configs/N=5120,K=25600,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/{source => }/contributing/overview.md,1
examples/{ => online_serving}/chart-helm/templates/poddisruptionbudget.yaml,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests-cpu.json,1
docs/{source => }/assets/kernel/q_vecs.png,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm89_int8_dispatch.cuh,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/entrypoints/openai/{ => translations}/speech_to_text.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=896,device_name=NVIDIA_H20.json",1
vllm/grpc/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json",1
examples/template_vlm2vec_qwen2vl.jinja,1
tests/models/{ => decoder_only/vision_language}/test_llava_image_embeds.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/online_serving/{ => pooling}/openai_pooling_client.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=8192,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => online_serving}/chart-helm/ct.yaml,1
docs/assets/design/debug_vllm_compile/tlparse_inductor.png,1
tests/models/decoder_only/vision_language/processing/test_phi3v.py,1
{cacheflow => vllm}/engine/async_llm_engine.py,1
docs/{source => }/contributing/dockerfile/dockerfile.md,1
csrc/quantization/fp8/amd_detail/quant_utils.cuh,1
tests/reasoning/test_holo2_reasoning_parser.py,1
.buildkite/lm-eval-harness/configs/{Minitron-4B-Base.yaml => Minitron-4B-Base-FP8.yaml},1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/serving/integrations/langchain.md,1
"vllm/model_executor/layers/fused_moe/configs/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json",1
examples/{ => offline_inference}/offline_inference_scoring.py,1
tests/kernels/test_merge_attn_states.py,1
Dockerfile.tpu => docker/Dockerfile.tpu,1
"vllm/model_executor/layers/fused_moe/configs/E=72,N=192,device_name=AMD_Instinct_MI300X.json",1
requirements-test.in => requirements/test.in,1
csrc/quantization/gptq_marlin/marlin_mma.h,1
docs/source/{dev => design}/multimodal/adding_multimodal_plugin.rst,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
examples/{ => online_serving}/run_cluster.sh,1
tools/profiler/nsys_profile_tools/vllm_engine_model.json,1
vllm/{entrypoints/openai => }/tool_parsers/xlam_tool_parser.py,1
docs/{source => }/assets/design/v1/prefix_caching/example-time-1.png,1
benchmarks/{benchmark_serving_guided.py => benchmark_serving_structured_output.py},1
{examples => vllm/transformers_utils/chat_templates}/template_deepseek_vl2.jinja,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/grouped_mm_c3x_sm100.cu,1
tests/models/language/{pooling => pooling_mteb_test}/test_baai.py,1
docs/{source => }/deployment/integrations/llamastack.md,1
benchmarks/benchmark_prefix_block_hash.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_B300_SXM6_AC,dtype=fp8_w8a8.json",1
docs/{source => }/assets/deployment/streamlit-chat.png,1
tests/utils_/{test_utils.py => test_argparse_utils.py},1
cacheflow/{master => core}/scheduler.py,1
LICENSE,1
vllm/lora/ops/triton_ops/{lora_shrink.py => lora_shrink_op.py},1
docs/source/assets/kernel/logits_vec.png,1
docs/assets/design/v1/tpu/most_model_len.png,1
docs/source/{ => features}/quantization/supported_hardware.md,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json",1
vllm/v1/worker/gpu/sample/output.py,1
examples/pooling/score/qwen3_reranker_offline.py,1
"vllm/model_executor/layers/fused_moe/configs/E=384,N=128,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
assets/figures/perf_a100_n1_dark.png,1
docs/source/{models/vlm.rst => usage/multimodal_inputs.rst},1
vllm/py.typed,1
tests/models/{decoder_only/language => quantization}/test_aqlm.py,1
vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_mxfp4.py,1
vllm/lora/ops/triton_ops/{lora_expand.py => lora_expand_op.py},1
cacheflow/{frontend/utils.py => server/tokenizer_utils.py},1
{cacheflow => vllm}/sequence.py,1
benchmarks/kernels/benchmark_mla_k_concat.py,1
docs/mkdocs/javascript/edit_and_feedback.js,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/entrypoints/sleep/__init__.py,1
tests/models/decoder_only/vision_language/{test_internvl.py => test_awq.py},1
.buildkite/nightly-benchmarks/throughput-tests.json,1
examples/{offline_inference/disaggregated_prefill_lmcache.py => lmcache/disagg_prefill_lmcache_v0.py},1
examples/template_chatglm2.jinja,1
tests/rocm/aiter/test_mla_fp8_support_check.py,1
vllm/utils/{func.py => functools.py},1
csrc/mamba/mamba_ssm/static_switch.h,1
"vllm/model_executor/layers/quantization/utils/configs/{N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/kernels/{ => attention}/test_cascade_flash_attn.py,1
tests/v1/kv_connector/unit/test_invalid_blocks_correctness.py,1
tests/reasoning/test_minimax_m2_append_reasoning_parser.py,1
vllm/v1/engine/{mm_input_mapper.py => mm_input_cache.py},1
docs/{source => }/assets/design/v1/prefix_caching/example-time-6.png,1
docs/source/assets/kernel/k_vecs.png,1
"vllm/model_executor/layers/quantization/utils/configs/{N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
vllm/distributed/kv_transfer/kv_connector/v1/{shared_storage_connector.py => example_connector.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/api/inference_params.md,1
tests/compile/{piecewise => fullgraph}/test_simple.py,1
vllm/core/{evictor.py => evictor_v1.py},1
tools/{ => pre_commit}/check_init_lazy_imports.py,1
tests/config/test_config.yaml,1
"vllm/model_executor/layers/fused_moe/configs/{E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
docs/assets/{kernel => design/paged_attention}/query.png,1
examples/template_chameleon.jinja,1
{cacheflow => vllm}/entrypoints/openai/protocol.py,1
tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/case_filtering.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
.buildkite/image_build/image_build_hpu.sh,1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
vllm/v1/attention/backends/mla/__init__.py,1
vllm/{entrypoints/openai => }/tool_parsers/llama_tool_parser.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
csrc/attention/{dtype_fp8_e5m2.cuh => dtype_fp8.cuh},1
vllm/renderers/__init__.py,1
tests/kernels/{layernorm.py => test_layernorm.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/transformers_utils/configs/{ovis2.py => ovis.py},1
csrc/quantization/gptq_marlin/{gptq_marlin_dtypes.cuh => marlin_dtypes.cuh},1
tests/v1/engine/test_abort_final_step.py,1
examples/cpu_offload.py,1
docs/{source => }/serving/distributed_serving.md,1
docs/{source => }/assets/kernel/query.png,1
docs/design/{v1 => }/metrics.md,1
docs/source/getting_started/{installation/index.md => installation.md},1
tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1344,device_name=NVIDIA_A100-SXM4-40GB.json",1
"vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/{performance => configuration}/optimization.md,1
examples/{ => offline_inference}/cpu_offload.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_B200.json",1
vllm/usage/__init__.py,1
examples/online_serving/disaggregated_encoder/disagg_epd_proxy.py,1
tests/models/{decoder_only/language => quantization}/test_gptq_marlin.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/{decoder_only/language => quantization}/test_gptq_marlin_24.py,1
docs/cli/.meta.yml,1
tests/{ => model_executor/model_loader}/tensorizer_loader/conftest.py,1
.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/convert-results-json-to-markdown.py,1
{cacheflow => vllm}/entrypoints/openai/api_server.py,1
docs/{source => }/deployment/integrations/kserve.md,1
csrc/quantization/fp8/amd_detail/hip_float8_impl.h,1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
examples/{ => online_serving}/openai_completion_client.py,1
examples/offline_inference/{offline_inference_encoder_decoder.py => encoder_decoder.py},1
tools/{ => pre_commit}/enforce_regex_import.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/api/engine/async_llm_engine.md,1
vllm/entrypoints/openai/serving_chat_stream_harmony.py,1
vllm/attention/utils/__init__.py,1
cacheflow/model_executor/input_metadata.py,1
docs/source/contributing/dockerfile/dockerfile.rst,1
tests/entrypoints/openai/{test_responses_function_call_parsing.py => responses/test_function_call_parsing.py},1
examples/{ => other}/fp8/extract_scales.py,1
docs/{source => }/deployment/integrations/production-stack.md,1
.github/workflows/scripts/env.sh,1
docs/source/getting_started/installation/{openvino.md => ai_accelerator/openvino.inc.md},1
cacheflow/{http_frontend => frontend}/fastapi_frontend.py,1
{assets => docs/source/assets}/figures/perf_a10g_n1_dark.png,1
examples/{ => offline_inference}/offline_inference_cli.py,1
tests/{async_engine => entrypoints/openai}/test_chat_template.py,1
tests/models/{ => decoder_only/language}/test_mistral.py,1
vllm/{ => v1}/executor/ray_utils.py,1
tests/{metrics => v1/tracing}/__init__.py,1
docs/assets/design/debug_vllm_compile/dynamic_shapes.png,1
tests/entrypoints/openai/tool_parsers/__init__.py,1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x.cuh,1
examples/{openai_client.py => openai_completion_client.py},1
.buildkite/nightly-benchmarks/{ => tests}/latency-tests.json,1
tests/plugins/lora_resolvers/__init__.py,1
cacheflow/{models => model_executor/layers}/attention.py,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json",1
examples/offline_inference_with_default_generation_config.py,1
"vllm/model_executor/layers/fused_moe/configs/E=40,N=2560,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_A100-SXM4-80GB.json",1
"vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}",1
csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh,1
tests/models/{decoder_only/language => language/generation}/test_models.py,1
tests/multimodal/{test_base.py => test_inputs.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
docs/mkdocs/javascript/slack_and_forum.js,1
.buildkite/nightly-benchmarks/{tests/descriptions.md => performance-benchmarks-descriptions.md},1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8.json",1
examples/pooling/score/template/mxbai_rerank_v2.jinja,1
"vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json",1
{benchmark => benchmarks}/benchmark_latency.py,1
examples/tool_chat_template_glm4.jinja,1
examples/{ => offline_inference}/gguf_inference.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json",1
vllm/entrypoints/openai/{serving_transcription.py => translations/serving.py},1
vllm/transformers_utils/configs/hunyuan_vl.py,1
{cacheflow => vllm}/model_executor/layers/sampler.py,1
"vllm/model_executor/layers/fused_moe/configs/E=16,N=800,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json",1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",1
docs/source/assets/kernel/query.png,1
tests/compile/{ => distributed}/test_sequence_parallelism.py,1
vllm/assets/__init__.py,1
vllm/model_executor/layers/quantization/compressed_tensors/schemes/{compressed_tensors_w4a16.py => compressed_tensors_wNa16.py},1
tests/entrypoints/{ => openai/parser}/test_harmony_utils.py,1
benchmarks/kernels/benchmark_fused_topk.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/models/{ => decoder_only/language}/test_big_models.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml,1
vllm/v1/executor/{gpu_executor.py => uniproc_executor.py},1
tools/generate_versions_json.py,1
"vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json",1
tests/tool_use/test_minimax_m2_tool_parser.py,1
cacheflow/{master => core}/policy.py,1
vllm/v1/worker/__init__.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/{attention/utils => v1/attention/backends}/fa_utils.py,1
examples/pooling/plugin/{prithvi_geospatial_mae_client.py => prithvi_geospatial_mae_online.py},1
csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90.cu,1
.buildkite/test_areas/model_executor.yaml,1
vllm/v1/attention/backends/triton_mla.py,1
.buildkite/lm-eval-harness/configs/models-small-rocm.txt,1
"vllm/model_executor/layers/fused_moe/configs/{E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}",1
tests/models/embedding/language/test_truncation_control.py,1
vllm/entrypoints/{openai/serving_score.py => pooling/score/serving.py},1
"vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/{source => }/models/generative_models.md,1
tests/quantization/__init__.py,1
docs/source/{serving/deploying_with_bentoml.md => deployment/frameworks/bentoml.md},1
tests/models/language/generation_ppl_test/test_gpt.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json",1
.github/scale-config.yml,1
csrc/quantization/w8a8/fp8/common.cu,1
"vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_H100_80GB_HBM3.json",1
vllm/benchmarks/__init__.py,1
tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm.yaml,1
tests/{tool_use => tool_parsers}/test_ernie45_moe_tool_parser.py,1
vllm/renderers/mistral.py,1
docs/{source/contributing/profiling/profiling_index.md => contributing/profiling.md},1
{cacheflow => vllm}/model_executor/layers/attention.py,1
examples/{ => offline_inference}/aqlm_example.py,1
csrc/{quantization/cutlass_w8a8 => cutlass_extensions/epilogue}/broadcast_load_epilogue_c2x.hpp,1
cacheflow/logger.py,1
examples/tool_chat_template_internlm2_tool.jinja,1
docs/{serving => usage}/env_vars.md,1
docs/serving/env_vars.md,1
vllm/{ => v1}/attention/selector.py,1
docs/serving/context_parallel_deployment.md,1
"vllm/model_executor/layers/quantization/utils/configs/N=12288,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/model_executor/layers/quantization/compressed_tensors/transform/utils.py,1
tests/model_executor/{weight_utils.py => test_weight_utils.py},1
vllm/entrypoints/pooling/pooling/__init__.py,1
tests/models/decoder_only/vision_language/test_llava_image_embeds.py,1
{cacheflow => vllm}/model_executor/models/opt.py,1
vllm/entrypoints/serve/elastic_ep/middleware.py,1
docs/source/serving/integrations/llamaindex.md,1
assets/figures/perf_a10g_n1_dark.png,1
docs/assets/deployment/hf-inference-endpoints-create-endpoint.png,1
vllm/{ => v1}/attention/ops/merge_attn_states.py,1
vllm/distributed/ec_transfer/ec_connector/shared_storage_connector.py,1
docs/source/assets/design/v1/prefix_caching/overview.png,1
tests/models/{decoder_only/language => quantization}/test_modelopt.py,1
csrc/quantization/gptq_marlin/marlin_int4_fp8_preprocess.cu,1
tests/compile/fusion_test_utils.py,1
docs/source/serving/tensorizer.md,1
tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-triton.yaml,1
"vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",1
"vllm/model_executor/layers/quantization/utils/configs/N=24576,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json",1
vllm/ray/__init__.py,1
tests/{engine/output_processor => detokenizer}/__init__.py,1
tests/kernels/{ => mamba}/test_mamba_mixer2.py,1
"vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json",1
docs/source/{ => features}/quantization/fp8_e5m2_kvcache.md,1
rocm_patch/{commonpy_xformers-0.0.22.post7.rocm.patch => commonpy_xformers-0.0.23.rocm.patch},1
tests/kernels/moe/test_cutedsl_moe.py,1
tests/kernels/__init__.py,1
"vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_A800-SXM4-80GB.json",1
tests/entrypoints/pooling/{openai/test_pooling.py => pooling/test_online.py},1
csrc/cpu/sgl-kernels/moe_int8.cpp,1
requirements-rocm.txt => requirements/rocm.txt,1
examples/{openai_chatcompletion_client.py => openai_chat_completion_client.py},1
examples/offline_inference/{offline_inference_openai => openai}/openai_example_batch.jsonl,1
tests/kernels/{cache.py => test_cache.py},1
tests/kernels/{ => quantization}/test_triton_scaled_mm.py,1
rocm_patch/{flashpy_xformers-0.0.22.post7.rocm.patch => flashpy_xformers-0.0.23.rocm.patch},1
tests/entrypoints/test_llm_encode.py,1
examples/offline_inference_scoring.py,1
docs/{source => }/design/v1/torch_compile.md,1
