[c25dbee40] Cyrus Leung 2026-01-27 [Model] Bump transformers version for test registry (#33100)
14	14	tests/models/registry.py
1	1	tests/models/test_transformers.py
1	1	tests/v1/e2e/test_spec_decode.py
2	2	vllm/model_executor/models/transformers/base.py
1	1	vllm/model_executor/models/transformers/moe.py
1	1	vllm/transformers_utils/config.py

[19ab0f7ce] Nicolò Lucchesi 2026-01-26 [Bugfix] Fix Voxtral streaming slot_mapping (#33073)
40	0	vllm/model_executor/models/whisper_causal.py

[67fe677c5] danielafrimi 2026-01-26  [FIX] Always support TP > 4 for FP4 Gemm (#31099)
26	4	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
21	2	vllm/model_executor/layers/quantization/modelopt.py
67	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[d56afd45f] Andy Lo 2026-01-26 Remove unused logic in `models/mistral.py` (#33095)
0	8	vllm/model_executor/models/mistral.py

[a2393ed49] Chauncey 2026-01-26 [CI] Fix AssertionError: MCP tool call not found in output_messages (#33093)
1	1	tests/entrypoints/openai/responses/test_harmony.py

[be6931ee2] Pleaplusone 2026-01-26 [ROCm][Bugfix] Fix ptpc scale load issue for fused shared expert path in deepseek mtp (#33018)
11	8	vllm/model_executor/models/deepseek_mtp.py

[9ef3b718d] Chauncey 2026-01-26 [Bugfix] Fix Can't instantiate abstract class DeepseekV32IndexerBackend (#33052)
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[bb17e8f11] Yuxuan Zhang 2026-01-26 [GLM-OCR] GLM-OCR with MTP Support (#33005)
1	0	docs/models/supported_models.md
38	0	examples/offline_inference/vision_language.py
14	0	tests/models/multimodal/generation/test_common.py
13	0	tests/models/multimodal/generation/test_vit_backend_functionality.py
1	0	tests/models/multimodal/processing/test_common.py
11	1	tests/models/registry.py
12	0	vllm/config/speculative.py
99	2	vllm/model_executor/models/glm4.py
3	2	vllm/model_executor/models/glm4_1v.py
389	0	vllm/model_executor/models/glm_ocr.py
285	0	vllm/model_executor/models/glm_ocr_mtp.py
4	2	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/model_arch_config_convertor.py
2	1	vllm/v1/spec_decode/eagle.py

[dcd80206b] Cyrus Leung 2026-01-26 [Chore] Update type annotation of `input_ids` in model forward (#33063)
1	1	docs/contributing/model/basic.md
1	1	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
2	2	vllm/model_executor/models/afmoe.py
1	1	vllm/model_executor/models/apertus.py
1	1	vllm/model_executor/models/arcee.py
2	2	vllm/model_executor/models/arctic.py
1	1	vllm/model_executor/models/aria.py
1	1	vllm/model_executor/models/audioflamingo3.py
1	1	vllm/model_executor/models/aya_vision.py
1	1	vllm/model_executor/models/bagel.py
2	2	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bailing_moe.py
2	2	vllm/model_executor/models/bamba.py
1	1	vllm/model_executor/models/bert_with_rope.py
1	1	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/bloom.py
1	1	vllm/model_executor/models/chameleon.py
2	2	vllm/model_executor/models/chatglm.py
1	1	vllm/model_executor/models/cohere2_vision.py
2	2	vllm/model_executor/models/commandr.py
2	2	vllm/model_executor/models/dbrx.py
1	1	vllm/model_executor/models/deepseek_mtp.py
1	1	vllm/model_executor/models/deepseek_ocr.py
2	2	vllm/model_executor/models/deepseek_v2.py
1	1	vllm/model_executor/models/deepseek_vl2.py
2	2	vllm/model_executor/models/dots1.py
1	1	vllm/model_executor/models/dots_ocr.py
1	2	vllm/model_executor/models/eagle2_5_vl.py
2	2	vllm/model_executor/models/ernie45_moe.py
1	1	vllm/model_executor/models/ernie45_vl.py
2	2	vllm/model_executor/models/ernie45_vl_moe.py
1	1	vllm/model_executor/models/ernie_mtp.py
1	1	vllm/model_executor/models/exaone.py
1	1	vllm/model_executor/models/exaone4.py
1	1	vllm/model_executor/models/exaone_moe.py
1	1	vllm/model_executor/models/falcon.py
2	2	vllm/model_executor/models/falcon_h1.py
1	1	vllm/model_executor/models/fuyu.py
2	2	vllm/model_executor/models/gemma.py
1	1	vllm/model_executor/models/gemma2.py
1	1	vllm/model_executor/models/gemma3.py
1	1	vllm/model_executor/models/gemma3_mm.py
4	4	vllm/model_executor/models/gemma3n.py
1	1	vllm/model_executor/models/gemma3n_mm.py
1	1	vllm/model_executor/models/glm4.py
1	1	vllm/model_executor/models/glm4_1v.py
2	2	vllm/model_executor/models/glm4_moe.py
2	2	vllm/model_executor/models/glm4_moe_lite.py
1	1	vllm/model_executor/models/glm4_moe_lite_mtp.py
1	1	vllm/model_executor/models/glm4_moe_mtp.py
1	1	vllm/model_executor/models/glm4v.py
1	1	vllm/model_executor/models/glmasr.py
3	3	vllm/model_executor/models/gpt2.py
2	2	vllm/model_executor/models/gpt_bigcode.py
2	2	vllm/model_executor/models/gpt_j.py
2	2	vllm/model_executor/models/gpt_neox.py
2	2	vllm/model_executor/models/gpt_oss.py
1	1	vllm/model_executor/models/granite.py
1	1	vllm/model_executor/models/granite_speech.py
2	2	vllm/model_executor/models/granitemoe.py
2	2	vllm/model_executor/models/granitemoehybrid.py
2	2	vllm/model_executor/models/granitemoeshared.py
2	2	vllm/model_executor/models/grok1.py
1	1	vllm/model_executor/models/hunyuan_v1.py
1	1	vllm/model_executor/models/hunyuan_vision.py
1	1	vllm/model_executor/models/hyperclovax_vision.py
2	2	vllm/model_executor/models/idefics3.py
4	0	vllm/model_executor/models/interfaces.py
3	3	vllm/model_executor/models/internlm2.py
1	1	vllm/model_executor/models/internlm2_ve.py
1	1	vllm/model_executor/models/interns1.py
1	1	vllm/model_executor/models/internvl.py
2	2	vllm/model_executor/models/iquest_loopcoder.py
1	1	vllm/model_executor/models/isaac.py
2	2	vllm/model_executor/models/jais.py
1	1	vllm/model_executor/models/jais2.py
2	2	vllm/model_executor/models/jamba.py
1	1	vllm/model_executor/models/jina_vl.py
1	1	vllm/model_executor/models/kanana_v.py
1	1	vllm/model_executor/models/keye.py
1	1	vllm/model_executor/models/kimi_linear.py
1	1	vllm/model_executor/models/kimi_vl.py
2	2	vllm/model_executor/models/lfm2.py
2	2	vllm/model_executor/models/lfm2_moe.py
1	1	vllm/model_executor/models/lfm2_vl.py
1	1	vllm/model_executor/models/llama.py
1	1	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/llava_next.py
1	1	vllm/model_executor/models/llava_next_video.py
1	1	vllm/model_executor/models/llava_onevision.py
2	2	vllm/model_executor/models/longcat_flash.py
1	1	vllm/model_executor/models/longcat_flash_mtp.py
2	2	vllm/model_executor/models/mamba.py
2	2	vllm/model_executor/models/mamba2.py
1	1	vllm/model_executor/models/midashenglm.py
1	1	vllm/model_executor/models/mimo.py
1	1	vllm/model_executor/models/mimo_mtp.py
2	2	vllm/model_executor/models/mimo_v2_flash.py
2	2	vllm/model_executor/models/minicpm.py
1	1	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/minimax_m2.py
1	1	vllm/model_executor/models/minimax_text_01.py
1	1	vllm/model_executor/models/minimax_vl_01.py
1	1	vllm/model_executor/models/mistral3.py
2	2	vllm/model_executor/models/mixtral.py
1	1	vllm/model_executor/models/mllama4.py
4	5	vllm/model_executor/models/modernbert.py
1	1	vllm/model_executor/models/molmo.py
1	1	vllm/model_executor/models/molmo2.py
2	2	vllm/model_executor/models/mpt.py
1	1	vllm/model_executor/models/nano_nemotron_vl.py
1	1	vllm/model_executor/models/nemotron.py
2	2	vllm/model_executor/models/nemotron_h.py
1	1	vllm/model_executor/models/nemotron_nas.py
2	2	vllm/model_executor/models/nemotron_parse.py
1	1	vllm/model_executor/models/nemotron_vl.py
2	2	vllm/model_executor/models/olmo.py
2	2	vllm/model_executor/models/olmo2.py
2	2	vllm/model_executor/models/olmoe.py
2	2	vllm/model_executor/models/openpangu.py
1	1	vllm/model_executor/models/openpangu_mtp.py
3	3	vllm/model_executor/models/opt.py
2	2	vllm/model_executor/models/orion.py
2	2	vllm/model_executor/models/ouro.py
1	1	vllm/model_executor/models/ovis.py
1	1	vllm/model_executor/models/ovis2_5.py
1	1	vllm/model_executor/models/paddleocr_vl.py
1	1	vllm/model_executor/models/paligemma.py
2	2	vllm/model_executor/models/persimmon.py
2	2	vllm/model_executor/models/phi.py
1	1	vllm/model_executor/models/phi3v.py
1	1	vllm/model_executor/models/phi4mm.py
2	2	vllm/model_executor/models/phimoe.py
1	1	vllm/model_executor/models/pixtral.py
2	2	vllm/model_executor/models/plamo2.py
2	2	vllm/model_executor/models/plamo3.py
2	2	vllm/model_executor/models/qwen.py
2	2	vllm/model_executor/models/qwen2.py
1	1	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
1	1	vllm/model_executor/models/qwen2_audio.py
2	2	vllm/model_executor/models/qwen2_moe.py
1	1	vllm/model_executor/models/qwen2_rm.py
1	1	vllm/model_executor/models/qwen2_vl.py
1	1	vllm/model_executor/models/qwen3.py
2	2	vllm/model_executor/models/qwen3_moe.py
2	2	vllm/model_executor/models/qwen3_next.py
1	1	vllm/model_executor/models/qwen3_next_mtp.py
2	2	vllm/model_executor/models/qwen3_omni_moe_thinker.py
2	2	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/qwen3_vl_moe.py
1	1	vllm/model_executor/models/qwen_vl.py
2	2	vllm/model_executor/models/seed_oss.py
1	1	vllm/model_executor/models/skyworkr1v.py
1	1	vllm/model_executor/models/solar.py
2	2	vllm/model_executor/models/stablelm.py
2	2	vllm/model_executor/models/starcoder2.py
2	2	vllm/model_executor/models/step3_text.py
1	1	vllm/model_executor/models/step3_vl.py
1	1	vllm/model_executor/models/tarsier.py
1	1	vllm/model_executor/models/ultravox.py
1	1	vllm/model_executor/models/voxtral.py
1	1	vllm/model_executor/models/voxtral_streaming.py
2	2	vllm/model_executor/models/zamba2.py

[f4a0921c9] danisereb 2026-01-26 [Performance] Tune Mamba selective scan kernel for B200 (#32873)
5	0	vllm/model_executor/layers/mamba/mamba_mixer2.py
21	11	vllm/model_executor/layers/mamba/ops/mamba_ssm.py

[208c56256] VihaanThat 2026-01-26 [Feature] Add LoRA support for Gemma3 vision components (#32764)
38	0	vllm/model_executor/models/gemma3_mm.py

[9ac818a55] Alex Brooks 2026-01-26 [Misc] HF Hub LoRA Resolver (#20320)
1	1	.buildkite/test-pipeline.yaml
1	1	docs/design/lora_resolver_plugins.md
6	4	docs/features/lora.md
1	0	pyproject.toml
107	0	tests/plugins/lora_resolvers/test_hf_hub_resolver.py
8	0	vllm/envs.py
13	3	vllm/plugins/lora_resolvers/filesystem_resolver.py
143	0	vllm/plugins/lora_resolvers/hf_hub_resolver.py

[6ca2c91b9] Itay Etelis 2026-01-26 [Model] Use mm_position to compute mrope positions for Qwen3-Omni (#33010)
56	3	examples/offline_inference/qwen3_omni/only_thinker.py
237	295	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[e33192b26] cwazai 2026-01-26 [lora/moe] Improve fused MoE‑LoRA kernel indexing and memory access (#32770)
23	7	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[61274bdef] Cyrus Leung 2026-01-26 [Doc] Further update multi-modal impl doc (#33065)
65	20	docs/contributing/model/multimodal.md

[b40db4dfe] ltd0924 2026-01-26 [StepVL] add step vl offline example (#33054)
27	0	examples/offline_inference/vision_language.py
27	0	examples/offline_inference/vision_language_multi_image.py

[11b556878] Cyrus Leung 2026-01-26 [Refactor] Use data parser for matching data items to multi-modal UUIDs (#32955)
139	117	docs/features/multimodal_inputs.md
4	2	examples/pooling/plugin/prithvi_geospatial_mae_offline.py
103	23	tests/entrypoints/openai/test_vision_embeds.py
36	18	tests/entrypoints/test_chat_utils.py
6	9	tests/models/multimodal/pooling/test_prithvi_mae.py
4	2	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
26	63	tests/v1/engine/test_process_multi_modal_uuids.py
224	176	vllm/entrypoints/chat_utils.py
0	52	vllm/entrypoints/llm.py
8	4	vllm/entrypoints/pooling/score/utils.py
8	23	vllm/model_executor/models/qwen3_vl.py
40	40	vllm/model_executor/models/terratorch.py
1	1	vllm/utils/__init__.py
103	75	vllm/v1/engine/input_processor.py

[ee484b3f4] Danielle Robinson 2026-01-25 Set splitk=1 for fused-moe-lora expand kernel (#32882)
1	1	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[a9b53dd43] Woosuk Kwon 2026-01-25 [Model Runner V2] Add LoRAState to consolidate lora logic (#33062)
47	0	vllm/v1/worker/gpu/lora_utils.py
6	2	vllm/v1/worker/gpu/model_runner.py
0	40	vllm/v1/worker/gpu/states.py

[254db42ed] Robert Shaw 2026-01-26 [Tests] Remove Duplicates (#33032)
0	8	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass-dp-ep.yaml
0	8	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass-dp-ep.yaml
0	2	tests/evals/gsm8k/configs/moe-refactor/config-b200.txt

[105d10457] ltd0924 2026-01-26 [StepVL] support close img patch (#32923)
10	5	vllm/model_executor/models/step3_vl.py

[566cdb6cf] Lucas Wilkinson 2026-01-25 [CI] Fix MHA attention test failure (AttributeError when model_config is None in ViT attention backend) (#33033)
4	2	vllm/model_executor/models/vision.py

[2f0d3ba74] Woosuk Kwon 2026-01-25 [Model Runner V2] Minor simplification for finish_requests (#33048)
6	7	vllm/v1/worker/gpu/model_runner.py

[edf927bc9] Woosuk Kwon 2026-01-25 [Model Runner V2] Fix slot_mapping after #25954 (#33046)
7	7	vllm/v1/worker/gpu/cudagraph_utils.py
3	0	vllm/v1/worker/gpu/input_batch.py
11	9	vllm/v1/worker/gpu/model_runner.py
16	4	vllm/v1/worker/gpu/spec_decode/eagle.py
3	3	vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py

[22aeb4300] Andreas Karatzas 2026-01-25 [Bugfix][VLM] Fix transformers backend embed_multimodal for Qwen2.5-VL profiling (#32969)
38	13	vllm/model_executor/models/transformers/multimodal.py

[a698e8e7a] Itay Etelis 2026-01-25 [Model] Use mm_position to compute mrope positions for Qwen2.5-Omni (#32772)
26	0	examples/offline_inference/qwen2_5_omni/only_thinker.py
351	198	vllm/model_executor/models/qwen2_5_omni_thinker.py
9	3	vllm/v1/worker/gpu_model_runner.py

[151e5451c] zhanqiuhu 2026-01-25 [Doc] Add Qwen2.5 models to batch invariance tested models (#33016)
1	0	docs/features/batch_invariance.md

[73b243463] Jee Jee Li 2026-01-25 [BugFix]  Add env variable to control PDL in LoRA (#32836)
5	0	vllm/envs.py
6	1	vllm/lora/ops/triton_ops/utils.py

[7e67df557] JJJYmmm 2026-01-25 [Bugfix] fix encoder cache hang in Qwen3VL (#32684)
2	17	tests/models/multimodal/processing/test_tensor_schema.py
8	3	vllm/model_executor/models/qwen2_vl.py
44	23	vllm/model_executor/models/qwen3_vl.py

[ff6c1da4e] 7. Sun 2026-01-25 [Docs] Fix Apple silicon include path in CPU installation docs (#32977)
1	1	docs/getting_started/installation/cpu.md

[fcb9df99b] Roberto L. Castro 2026-01-25 [Perf][Kernel] Optimize FP4 quantization kernels (SM100F) (#32520)
55	22	benchmarks/kernels/bench_nvfp4_quant.py
2	1	csrc/ops.h
59	20	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
4	4	csrc/quantization/fp4/nvfp4_experts_quant.cu
6	3	csrc/quantization/fp4/nvfp4_quant_entry.cu
141	47	csrc/quantization/fp4/nvfp4_quant_kernels.cu
171	25	csrc/quantization/fp4/nvfp4_utils.cuh
2	1	csrc/torch_bindings.cpp
6	2	tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py
28	0	tests/kernels/quantization/test_nvfp4_quant.py
18	13	vllm/_custom_ops.py
1	0	vllm/compilation/activation_quant_fusion.py
2	0	vllm/compilation/collective_fusion.py
1	0	vllm/compilation/fusion_attn.py
1	4	vllm/model_executor/layers/fused_moe/utils.py
4	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
3	1	vllm/model_executor/layers/quantization/modelopt.py
5	8	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[1ebdff412] TJian 2026-01-25 [DOC] [ROCm] Update doc for v0.14.1 (#32998)
2	2	docs/getting_started/installation/gpu.rocm.inc.md

[91601ff47] Joshua Deng 2026-01-24 [Feature] add session based streaming input support to v1 (#28973)
23	7	tests/v1/core/test_scheduler.py
656	0	tests/v1/e2e/test_streaming_input.py
0	0	tests/v1/streaming_input/__init__.py
171	0	tests/v1/streaming_input/test_async_llm_streaming.py
210	0	tests/v1/streaming_input/test_gpu_model_runner_streaming.py
575	0	tests/v1/streaming_input/test_scheduler_streaming.py
1	0	tests/v1/test_request.py
10	0	vllm/outputs.py
112	13	vllm/v1/core/sched/scheduler.py
1	0	vllm/v1/engine/__init__.py
162	11	vllm/v1/engine/async_llm.py
2	0	vllm/v1/engine/input_processor.py
131	28	vllm/v1/engine/output_processor.py
10	2	vllm/v1/engine/utils.py
47	3	vllm/v1/request.py
41	0	vllm/v1/worker/gpu_model_runner.py

[d4dbb7af6] yugong333 2026-01-24 Using max_loras + 1 to construct grid in fused_moe_lora (#32277)
8	3	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[203d0bc0c] Maryam Tahhan 2026-01-24 [CPU] Improve CPU Docker build  (#30953)
12	0	cmake/cpu_extension.cmake
50	11	docker/Dockerfile.cpu
61	6	docs/getting_started/installation/cpu.x86.inc.md

[17ab54de8] Fadi Arafeh 2026-01-24 [CPU Backend][BugFix] Fix failing Darwin pipelines (#33002)
2	1	.github/workflows/macos-smoke-test.yml

[cd775bdbe] 7. Sun 2026-01-24 [Tests] Replace flaky sleep with polling in test_background_cancel (#32986)
19	6	tests/v1/entrypoints/openai/serving_responses/test_stateful.py

[da5e7b12b] Lucas Wilkinson 2026-01-24 [MLA] Fuse cat and qaunt for fp8 kv-cache (#32950)
41	20	vllm/model_executor/layers/attention/mla_attention.py

[719ac592e] Louie Tsai 2026-01-24 Update CPU doc according to feedback (#32963)
2	2	docs/benchmarking/dashboard.md
1	1	docs/models/hardware_supported_models/cpu.md
1	1	docs/models/hardware_supported_models/xpu.md

[1209b784f] Hiroken. 2026-01-24 [Bugfix]: resolve torch.compile cache conflict between mm_encoder_tp_modes (#32842)
2	1	vllm/config/multimodal.py
6	0	vllm/config/vllm.py

[5fa0f6efa] Lukas Geiger 2026-01-24 [EncoderCacheManager] Remove unnecessary copy (#32800)
1	1	vllm/v1/core/encoder_cache_manager.py

[bc0d291bf] david guan 2026-01-24 feat: Complete LoRA support for MiniMaxM2 Fixes #32736 (#32763)
1	1	docs/models/supported_models.md
10	2	vllm/model_executor/models/minimax_m2.py

[9ad7f89f5] Isotr0py 2026-01-24 [Models]: Make Multimodal config implicit in ViT implementation (#31972)
0	9	vllm/model_executor/layers/attention/mm_encoder_attention.py
4	24	vllm/model_executor/models/clip.py
0	3	vllm/model_executor/models/deepencoder.py
0	1	vllm/model_executor/models/deepseek_ocr.py
5	34	vllm/model_executor/models/dots_ocr.py
0	1	vllm/model_executor/models/eagle2_5_vl.py
1	12	vllm/model_executor/models/ernie45_vl.py
5	30	vllm/model_executor/models/glm4_1v.py
5	26	vllm/model_executor/models/hunyuan_vision.py
1	6	vllm/model_executor/models/hyperclovax_vision.py
4	18	vllm/model_executor/models/isaac.py
4	16	vllm/model_executor/models/keye.py
0	1	vllm/model_executor/models/kimi_vl.py
3	22	vllm/model_executor/models/lfm2_siglip2.py
0	1	vllm/model_executor/models/lfm2_vl.py
0	1	vllm/model_executor/models/lightonocr.py
1	6	vllm/model_executor/models/llava.py
0	1	vllm/model_executor/models/llava_next.py
0	1	vllm/model_executor/models/llava_next_video.py
0	1	vllm/model_executor/models/llava_onevision.py
0	1	vllm/model_executor/models/minimax_vl_01.py
1	4	vllm/model_executor/models/mistral3.py
3	15	vllm/model_executor/models/moonvit.py
0	1	vllm/model_executor/models/opencua.py
1	7	vllm/model_executor/models/ovis2_5.py
1	19	vllm/model_executor/models/paddleocr_vl.py
1	6	vllm/model_executor/models/phi3v.py
4	20	vllm/model_executor/models/pixtral.py
0	1	vllm/model_executor/models/qwen2_5_omni_thinker.py
5	33	vllm/model_executor/models/qwen2_5_vl.py
6	36	vllm/model_executor/models/qwen2_vl.py
2	26	vllm/model_executor/models/qwen3_omni_moe_thinker.py
4	25	vllm/model_executor/models/qwen3_vl.py
0	1	vllm/model_executor/models/qwen3_vl_moe.py
4	29	vllm/model_executor/models/siglip.py
4	24	vllm/model_executor/models/siglip2navit.py
1	6	vllm/model_executor/models/tarsier.py
48	2	vllm/model_executor/models/vision.py

[6450b536a] Hiroken. 2026-01-24 [Bugfix] Fix E2E latency calculation and add warmup support in mm_processor benchmark (#32646)
46	13	vllm/benchmarks/mm_processor.py

[0f19427db] 7. Sun 2026-01-24 [Perf] Cache exc.errors() result in validation exception handler (#32984)
4	3	vllm/entrypoints/openai/api_server.py

[51931c5c9] Cyrus Leung 2026-01-24 [UX] Deduplicate sampling parameter startup logs (#32953)
11	9	vllm/config/model.py
0	8	vllm/entrypoints/openai/chat_completion/serving.py
2	9	vllm/entrypoints/openai/completion/serving.py
1	8	vllm/entrypoints/openai/responses/serving.py

[06b557ecd] Reagan Lee 2026-01-24 feat(benchmark): add encoder forward pass benchmarking to mm-processor (#31655)
95	28	vllm/benchmarks/mm_processor.py
81	10	vllm/multimodal/processing/context.py
11	7	vllm/multimodal/processing/processor.py
112	14	vllm/v1/worker/gpu_model_runner.py
4	0	vllm/v1/worker/gpu_worker.py

[81c2a889c] Roger Wang 2026-01-23 [Doc] Ignore typo check on doc (#32999)
2	1	pyproject.toml

[8edaf3857] Isotr0py 2026-01-24 [Models] Add `SharedFusedMoE` support to Qwen3MoE (#32082)
56	16	vllm/model_executor/models/qwen3_moe.py

[5c86a8980] Roy Wang 2026-01-24 [docs] Update governance process links (#32995)
49	28	docs/governance/process.md

[0ccecf883] 7. Sun 2026-01-24 [Tests] Standardize RNG seed utility across test files (#32982)
3	14	tests/kernels/test_flex_attention.py
4	1	tests/utils.py
4	5	tests/v1/logits_processors/test_custom_offline.py
2	0	vllm/utils/torch_utils.py

[0b9a735e1] 7. Sun 2026-01-24 [Tests] Clarify pytest skip reasons with actionable context (#32981)
2	2	tests/samplers/test_beam_search.py
5	1	tests/v1/sample/test_topk_topp_sampler.py

[14d03b8dd] 7. Sun 2026-01-24 [Perf] Cache xpu_get_mem_info() result to avoid duplicate calls (#32983)
2	1	vllm/v1/worker/xpu_worker.py

[d0cbac582] Michael Goin 2026-01-23 [Dev UX] Add auto-detection for VLLM_PRECOMPILED_WHEEL_VARIANT during install (#32948)
1	1	docs/getting_started/installation/gpu.cuda.inc.md
50	5	setup.py

[c0d820457] ruizcrp 2026-01-24 Auth_token added in documentation as it is required (#32988)
2	0	docs/serving/integrations/claude_code.md

[97ef11dd3] monajafi-amd 2026-01-23 [ROCm][ViT] Enable Flash Attention Triton backend on RDNA3/RDNA4 (#32944)
34	1	vllm/platforms/rocm.py

[ecc3dd66c] Xin Yang 2026-01-23 [Bugfix] Fix FusedMoE LoRA kernel offs_token out of bound value (#32279)
4	2	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[7e1f10d56] Joe Runde 2026-01-23 [Core][Bugfix] allow graceful worker termination (#32965)
18	5	vllm/v1/executor/multiproc_executor.py

[a28b94e6e] ElizaWszola 2026-01-24 [Performance] Split FlashAttn attention and cache update (#25954)
5	0	tests/v1/attention/test_attention_backends.py
12	0	tests/v1/attention/utils.py
1	1	tests/v1/kv_connector/unit/test_decode_bench_connector.py
8	0	tests/v1/kv_connector/unit/test_nixl_connector.py
4	1	tests/v1/kv_connector/unit/test_offloading_connector.py
9	0	tests/v1/spec_decode/test_tree_attention.py
85	16	vllm/attention/layer.py
5	0	vllm/forward_context.py
48	5	vllm/model_executor/layers/attention/cross_attention.py
2	1	vllm/utils/torch_utils.py
3	0	vllm/v1/attention/backend.py
45	26	vllm/v1/attention/backends/flash_attn.py
50	0	vllm/v1/spec_decode/eagle.py
3	0	vllm/v1/spec_decode/medusa.py
4	0	vllm/v1/spec_decode/ngram_proposer.py
5	0	vllm/v1/spec_decode/suffix_decoding.py
12	0	vllm/v1/worker/gpu/attn_utils.py
12	4	vllm/v1/worker/gpu/cudagraph_utils.py
10	0	vllm/v1/worker/gpu/model_runner.py
127	14	vllm/v1/worker/gpu_model_runner.py
8	0	vllm/v1/worker/gpu_ubatch_wrapper.py

[0118cdcc0] dolpm 2026-01-23 [fix] add VLLM_OBJECT_STORAGE_SHM_BUFFER_NAME to compile factors (#32912)
37	0	tests/config/test_config_utils.py
1	0	vllm/envs.py

[136c499f6] Shengqi Chen 2026-01-23 [CI] fix version comparsion and exclusion patterns in upload-release-wheels.sh (#32971)
6	5	.buildkite/scripts/upload-release-wheels.sh

[ebd0a17e0] joninco 2026-01-23 [Bugfix] Fix missing is_layer_skipped check for FusedMoE in AWQConfig (#32935)
3	2	vllm/model_executor/layers/quantization/awq.py

[37c9859fa] Wentao Ye 2026-01-23 [Refactor] Clean up unused variables & func (#32692)
0	3	vllm/entrypoints/openai/engine/protocol.py
0	4	vllm/model_executor/layers/quantization/utils/petit_utils.py
0	20	vllm/model_executor/models/glmasr_utils.py
0	2	vllm/model_executor/models/phi4mm_audio.py
0	1	vllm/platforms/rocm.py

[4561f1398] Michael Goin 2026-01-23 [Refactor] Rename `gptq_marlin` to `marlin` to match MoE (#32952)
9	9	CMakeLists.txt
1	1	benchmarks/kernels/benchmark_machete.py
5	5	benchmarks/kernels/benchmark_marlin.py
2	2	csrc/moe/marlin_moe_wna16/kernel.h
4	4	csrc/moe/marlin_moe_wna16/marlin_template.h
1	1	csrc/quantization/gptq_allspark/allspark_utils.cuh
0	0	csrc/quantization/{gptq_marlin => marlin}/.gitignore
0	0	csrc/quantization/{gptq_marlin => marlin}/awq_marlin_repack.cu
0	0	csrc/quantization/{gptq_marlin => marlin}/dequant.h
0	0	csrc/quantization/{gptq_marlin => marlin}/generate_kernels.py
0	0	csrc/quantization/{gptq_marlin => marlin}/gptq_marlin_repack.cu
0	0	csrc/quantization/{gptq_marlin => marlin}/kernel.h
3	3	csrc/quantization/{gptq_marlin/gptq_marlin.cu => marlin/marlin.cu}
0	0	csrc/quantization/{gptq_marlin => marlin}/marlin.cuh
0	0	csrc/quantization/{gptq_marlin => marlin}/marlin_dtypes.cuh
0	0	csrc/quantization/{gptq_marlin => marlin}/marlin_int4_fp8_preprocess.cu
0	0	csrc/quantization/{gptq_marlin => marlin}/marlin_mma.h
0	0	csrc/quantization/{gptq_marlin => marlin}/marlin_template.h
2	2	csrc/torch_bindings.cpp
5	5	tests/kernels/quantization/test_marlin_gemm.py
4	4	vllm/_custom_ops.py
2	2	vllm/model_executor/layers/quantization/utils/marlin_utils.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[6cc6d92be] rasmith 2026-01-23 [CI][AMD][BugFix] Update wvSplitK (and other skinny_gemm wrappers) to ensure tensors passed will be made contiguous for the kernel (#32831)
7	0	csrc/rocm/skinny_gemms.cu
16	0	vllm/_custom_ops.py

[dfab5f376] Wentao Ye 2026-01-23 [Bug] Fix benchmark script `moe_permute_unpermute` (#32949)
3	5	benchmarks/kernels/benchmark_moe_permute_unpermute.py

[586a57ad7] Markus / Mark 2026-01-23 fix: Add glm4_moe_lite to MLA detection (#32614)
23	4	vllm/model_executor/layers/attention/mla_attention.py
19	6	vllm/platforms/cuda.py
2	0	vllm/transformers_utils/model_arch_config_convertor.py
26	0	vllm/v1/attention/backends/mla/flashinfer_mla.py

[3a4145950] Lucas Wilkinson 2026-01-23 [cudagraphs] Refactor cudagraph capture loop (#32946)
62	0	tests/v1/cudagraph/test_cudagraph_dispatch.py
23	0	vllm/v1/cudagraph_dispatcher.py
32	59	vllm/v1/worker/gpu_model_runner.py

[8518b3044] Nick Hill 2026-01-23 [Model Runner V2] Add KV Connector support (#32742)
2	1	vllm/v1/worker/gpu/attn_utils.py
125	0	vllm/v1/worker/gpu/kv_connector.py
28	13	vllm/v1/worker/gpu/model_runner.py
4	2	vllm/v1/worker/gpu_worker.py

[2d6b53715] Matthew Bonanni 2026-01-23 [Bugfix][CI] Fix pre-commit (#32956)
1	2	vllm/v1/worker/mamba_utils.py

[68b0a6c1b] Orion Reblitz-Richardson 2026-01-23 [CI][torch nightlies] Use main Dockerfile with flags for nightly torch tests (#30443)
146	19	docker/Dockerfile
8	0	docker/Dockerfile.nightly_torch
-	-	docs/assets/contributing/dockerfile-stages-dependency.png
49	13	use_existing_torch.py

[5206e5e28] Harry Huang 2026-01-24 [V1][Hybrid] Mamba Prefix Caching with align mode (#30877)
18	7	tests/v1/core/test_single_type_kv_cache_manager.py
764	0	tests/v1/e2e/test_mamba_prefix_cache.py
10	0	vllm/config/cache.py
11	0	vllm/config/vllm.py
6	0	vllm/engine/arg_utils.py
1	0	vllm/model_executor/layers/mamba/abstract.py
3	3	vllm/model_executor/layers/mamba/mamba_mixer.py
6	6	vllm/model_executor/layers/mamba/mamba_mixer2.py
95	0	vllm/model_executor/layers/mamba/mamba_utils.py
6	0	vllm/model_executor/models/bamba.py
55	20	vllm/model_executor/models/config.py
6	0	vllm/model_executor/models/falcon_h1.py
6	0	vllm/model_executor/models/granitemoehybrid.py
14	0	vllm/model_executor/models/interfaces.py
6	0	vllm/model_executor/models/jamba.py
10	0	vllm/model_executor/models/kimi_linear.py
11	4	vllm/model_executor/models/lfm2.py
6	0	vllm/model_executor/models/lfm2_moe.py
6	0	vllm/model_executor/models/mamba.py
6	0	vllm/model_executor/models/mamba2.py
6	0	vllm/model_executor/models/minimax_text_01.py
4	0	vllm/model_executor/models/nano_nemotron_vl.py
6	0	vllm/model_executor/models/nemotron_h.py
6	0	vllm/model_executor/models/plamo2.py
11	3	vllm/model_executor/models/qwen3_next.py
5	3	vllm/model_executor/models/qwen3_next_mtp.py
6	0	vllm/model_executor/models/zamba2.py
11	4	vllm/v1/attention/backends/gdn_attn.py
10	2	vllm/v1/attention/backends/linear_attn.py
27	8	vllm/v1/attention/backends/mamba_attn.py
38	0	vllm/v1/attention/backends/utils.py
2	1	vllm/v1/core/block_pool.py
11	1	vllm/v1/core/kv_cache_coordinator.py
7	2	vllm/v1/core/kv_cache_manager.py
76	1	vllm/v1/core/sched/scheduler.py
171	32	vllm/v1/core/single_type_kv_cache_manager.py
8	2	vllm/v1/kv_cache_interface.py
22	23	vllm/v1/worker/block_table.py
15	0	vllm/v1/worker/cp_utils.py
2	2	vllm/v1/worker/gpu_input_batch.py
53	5	vllm/v1/worker/gpu_model_runner.py
232	0	vllm/v1/worker/mamba_utils.py

[fec9da0af] Matteo Fari 2026-01-23 [Model] Enable LoRA support for internvl2 (#32397)
16	3	vllm/model_executor/models/internvl.py

[bbbd696af] Luka Govedič 2026-01-23 [torch.compile][CI] Add back attn fusion on hopper/ada (#32940)
4	5	tests/compile/test_fusion_attn.py

[9b77bb790] sangbumlikeagod 2026-01-24 [Frontend] add logprob, compression_rate to 'verbose_json' features (#31059)
1	1	docs/serving/openai_compatible_server.md
2	0	tests/entrypoints/openai/test_transcription_validation_whisper.py
4	4	vllm/entrypoints/openai/translations/protocol.py
25	9	vllm/entrypoints/openai/translations/speech_to_text.py

[305e53ade] Matt 2026-01-23 [Hardware][AMD][CI][Bugfix] Fix Kernels Attention Cache test (#32904)
1	1	tests/kernels/attention/test_cache.py

[1cb4341fb] Mark McLoughlin 2026-01-23 [ROCm][PD] Remove unused moriio connector proxy code (#32939)
0	22	examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py

[1fb648bf1] baonudesifeizhai 2026-01-23 [Bugfix] Fix FP8 MoE EP Weight Loading for ModelOpt Llama4 (#32886)
21	1	vllm/model_executor/models/llama4.py

[7e2230975] Nicolò Lucchesi 2026-01-23 [Misc] Postpone torch_profiler deprecation (#32867)
1	1	vllm/config/profiler.py

[90c200793] Xin Yang 2026-01-23 [Bugfix] Disable tma_aligned_scales in test_fusions_e2e (#32916)
3	0	tests/compile/distributed/test_fusions_e2e.py
5	0	vllm/envs.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[d95d65076] Raushan Turganbay 2026-01-23 [Bugfix] Fix getting vision features in Transformer Multimodal backend (#32933)
9	0	vllm/model_executor/models/transformers/multimodal.py

[13d8746c5] tianshu-Michael-yu 2026-01-23 [Feature]: Remove DtoH Copy for lfm2_vl On Default Stream (#32815)
89	92	vllm/model_executor/models/{siglip2.py => lfm2_siglip2.py}
129	51	vllm/model_executor/models/lfm2_vl.py
24	4	vllm/v1/attention/backends/gdn_attn.py
10	7	vllm/v1/attention/backends/mamba_attn.py
8	4	vllm/v1/attention/backends/utils.py

[10e94c84f] Fadi Arafeh 2026-01-23 [CPU][Feat] Update PyTorch to v2.10 for CPU Backend (#32869)
1	1	docker/Dockerfile.cpu
2	3	requirements/cpu-build.txt
2	2	requirements/cpu.txt

[243e78c20] Isotr0py 2026-01-23 [Benchmark][Bugfix] Fix race condtion when starting server for sweep benchmark (#32927)
13	0	vllm/benchmarks/sweep/serve.py
24	0	vllm/benchmarks/sweep/server.py

[aac0b817f] Fadi Arafeh 2026-01-23 [CPU Backend][BugFix] Fix failing CPU MoE test (#32876)
1	0	tests/kernels/moe/test_moe.py

[05f3d714d] wang.yuqi 2026-01-23 [Frontend][3/n] Make pooling entrypoints request schema consensus | EmbedRequest & ClassifyRequest (#32905)
100	17	docs/serving/openai_compatible_server.md
2	2	examples/pooling/score/convert_model_to_seq_cls.py
73	18	tests/entrypoints/pooling/classify/test_online_vision.py
12	0	tests/entrypoints/test_utils.py
7	3	vllm/entrypoints/openai/engine/serving.py
85	6	vllm/entrypoints/pooling/base/protocol.py
8	50	vllm/entrypoints/pooling/classify/protocol.py
8	69	vllm/entrypoints/pooling/embed/protocol.py
21	52	vllm/entrypoints/pooling/pooling/protocol.py
6	40	vllm/entrypoints/pooling/score/protocol.py
4	4	vllm/pooling_params.py

[3f3f89529] Patrick von Platen 2026-01-23 [Voxtral] Add new streaming arch (#32861)
97	0	tests/models/multimodal/generation/test_voxtral_streaming.py
4	1	vllm/model_executor/models/llama.py
108	0	vllm/model_executor/models/mistral.py
39	4	vllm/model_executor/models/voxtral.py
21	4	vllm/model_executor/models/voxtral_streaming.py
31	72	vllm/model_executor/models/whisper.py
465	0	vllm/model_executor/models/whisper_causal.py
0	233	vllm/model_executor/models/whisper_utils.py
3	0	vllm/transformers_utils/configs/mistral.py

[5da4c7d78] Li, Jiang 2026-01-23 [CI/Build][CPU] Fix failed pooling tests and macos smoke test (#32907)
1	1	csrc/cpu/torch_bindings.cpp
6	0	vllm/model_executor/layers/utils.py

[160c6fa38] Nicolò Lucchesi 2026-01-23 [Misc] Add `get_name` to missing AttentionBackends (#32698)
4	0	vllm/v1/attention/backends/gdn_attn.py
4	0	vllm/v1/attention/backends/linear_attn.py
4	0	vllm/v1/attention/backends/mamba1_attn.py
8	1	vllm/v1/attention/backends/mamba2_attn.py
4	0	vllm/v1/attention/backends/mla/indexer.py
4	0	vllm/v1/attention/backends/short_conv_attn.py

[a8eb1182f] Andreas Karatzas 2026-01-23 [CI][Models] Add VLM Support for Sequence Classification Conversion (#32885)
39	15	vllm/model_executor/layers/layernorm.py
113	23	vllm/model_executor/models/adapters.py
3	1	vllm/v1/attention/backends/triton_attn.py

[fa6e599a6] Karan Bansal 2026-01-23 [Bugfix] Fix _CPU_MOE_ACT AssertionError when vLLM config not set (#32777)
2	7	tests/kernels/moe/test_cpu_fused_moe.py
24	19	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py

[7ef587375] Wentao Ye 2026-01-22 [CI] Fix mypy for `vllm/v1/structured_output` (#32722)
1	1	tools/pre_commit/mypy.py
2	2	vllm/reasoning/abs_reasoning_parsers.py
2	2	vllm/reasoning/basic_parsers.py
1	1	vllm/reasoning/deepseek_v3_reasoning_parser.py
1	1	vllm/reasoning/gptoss_reasoning_parser.py
1	1	vllm/reasoning/holo2_reasoning_parser.py
1	1	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
2	2	vllm/reasoning/identity_reasoning_parser.py
1	1	vllm/reasoning/minimax_m2_reasoning_parser.py
2	1	vllm/reasoning/mistral_reasoning_parser.py
1	1	vllm/reasoning/olmo3_reasoning_parser.py
2	2	vllm/reasoning/step3_reasoning_parser.py
6	4	vllm/v1/attention/backends/fa_utils.py
1	1	vllm/v1/attention/backends/mla/aiter_triton_mla.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
3	2	vllm/v1/structured_output/__init__.py
3	0	vllm/v1/structured_output/backend_guidance.py
1	1	vllm/v1/structured_output/backend_xgrammar.py

[5e4e0e51f] Luka Govedič 2026-01-22 [torch.compile] Compile `CustomOp.forward_native` for `SiluAndMul` and `QuantFP8` to avoid raw torch ops inside opaque custom ops (#32806)
2	1	tests/compile/test_silu_mul_quant_fusion.py
1	1	tests/kernels/core/test_activation.py
1	0	vllm/compilation/matcher_utils.py
42	7	vllm/model_executor/custom_op.py
2	2	vllm/model_executor/layers/activation.py
3	1	vllm/model_executor/layers/quantization/input_quant_fp8.py
1	1	vllm/model_executor/layers/rotary_embedding/common.py

[f61c9da71] Rishabh Saini 2026-01-22 [BugFix] deepseek_v32_encoding: Replace asserts with proper exceptions (#32884)
39	28	vllm/tokenizers/deepseek_v32_encoding.py

[7fe255889] Nick Hill 2026-01-22 [Misc] Log vLLM logo when starting server (#32796)
3	2	vllm/entrypoints/grpc_server.py
2	1	vllm/entrypoints/openai/api_server.py
29	1	vllm/entrypoints/utils.py
3	0	vllm/envs.py
13	0	vllm/logger.py

[dc917cceb] bnellnm 2026-01-22 [MoE Refactor] Move `select_experts` from `FusedMoEQuantMethod` -> `FusedMoE` (#31996)
12	16	tests/kernels/moe/test_moe.py
0	8	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
15	5	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
4	13	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
61	22	vllm/model_executor/layers/fused_moe/layer.py
0	5	vllm/model_executor/layers/fused_moe/router/base_router.py
3	1	vllm/model_executor/layers/fused_moe/router/custom_routing_router.py
4	1	vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py
5	20	vllm/model_executor/layers/fused_moe/router/router_factory.py
26	46	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
2	9	vllm/model_executor/layers/quantization/awq_marlin.py
2	7	vllm/model_executor/layers/quantization/bitsandbytes.py
106	126	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	8	vllm/model_executor/layers/quantization/experts_int8.py
64	59	vllm/model_executor/layers/quantization/fp8.py
2	7	vllm/model_executor/layers/quantization/gguf.py
2	9	vllm/model_executor/layers/quantization/gptq_marlin.py
5	5	vllm/model_executor/layers/quantization/ipex_quant.py
64	53	vllm/model_executor/layers/quantization/modelopt.py
2	7	vllm/model_executor/layers/quantization/moe_wna16.py
108	80	vllm/model_executor/layers/quantization/mxfp4.py
6	23	vllm/model_executor/layers/quantization/quark/quark_moe.py

[fc56f4a07] Fadi Arafeh 2026-01-22 [BugFix] Fix invalid flashinfer_fused_moe_blockscale_fp8 op registration (#32855)
1	1	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py

[d08b356ee] Xin Yang 2026-01-22 [Perf] Create TMA-aligned input scale tensor for DeepGemm on Hopper (#32619)
3	3	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
3	0	tests/kernels/quant_utils.py
30	9	tests/kernels/quantization/test_block_fp8.py
6	2	tests/kernels/quantization/test_per_token_group_quant.py
5	0	vllm/model_executor/layers/quantization/input_quant_fp8.py
23	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py
5	0	vllm/utils/deep_gemm.py

[f74481018] Wentao Ye 2026-01-22 [Refactor] Remove unused tpu files (#32610)
0	0	vllm/v1/sample/tpu/__init__.py
0	120	vllm/v1/sample/tpu/metadata.py
0	215	vllm/v1/sample/tpu/sampler.py
0	18	vllm/v1/worker/tpu_worker.py

[44f08af3a] Eldar Kurtić 2026-01-22 Add llmcompressor fp8 kv-cache quant (per-tensor and per-attn_head) (#30141)
33	12	csrc/cache_kernels.cu
139	114	docs/features/quantization/quantized_kvcache.md
34	13	tests/kernels/attention/test_cache.py
21	3	tests/quantization/test_compressed_tensors.py
37	36	vllm/attention/layer.py
168	22	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
4	1	vllm/model_executor/layers/quantization/input_quant_fp8.py
47	24	vllm/model_executor/layers/quantization/utils/quant_utils.py
29	2	vllm/model_executor/model_loader/weight_utils.py
1	1	vllm/model_executor/models/apertus.py
1	1	vllm/model_executor/models/arcee.py
2	2	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/llama_eagle.py
2	2	vllm/model_executor/models/llama_eagle3.py
1	1	vllm/model_executor/models/nemotron_h.py
1	1	vllm/model_executor/models/nemotron_nas.py
1	0	vllm/v1/attention/backend.py
15	6	vllm/v1/attention/backends/flash_attn.py

[955b43a5a] Matthew Bonanni 2026-01-22 [Bugfix][Attention] Explicitly report support for kv_cache_dtype bfloat16 (#32795)
6	2	vllm/model_executor/layers/quantization/kv_cache.py
3	2	vllm/platforms/cpu.py
2	2	vllm/v1/attention/backend.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	0	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/flex_attention.py
1	0	vllm/v1/attention/backends/mla/cutlass_mla.py
4	1	vllm/v1/attention/backends/mla/flashattn_mla.py
1	0	vllm/v1/attention/backends/mla/flashinfer_mla.py
1	0	vllm/v1/attention/backends/mla/flashmla.py
5	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
4	1	vllm/v1/attention/backends/mla/triton_mla.py
1	0	vllm/v1/attention/backends/triton_attn.py

[744ef3048] Fadi Arafeh 2026-01-22 [CPU Backend] [Perf] Accelerate tensor-parallel/data-parallel inference across NUMA domains on Arm (#32792)
6	0	cmake/cpu_extension.cmake
99	1	csrc/cpu/cpu_types_arm.hpp
50	2	csrc/cpu/shm.cpp
2	2	csrc/cpu/torch_bindings.cpp
4	1	vllm/distributed/device_communicators/cpu_communicator.py
3	0	vllm/v1/worker/cpu_worker.py

[300622e60] Matthew Bonanni 2026-01-22 [CI][Attention] Add more CI dependencies for attention tests (#32487)
4	0	.buildkite/test-amd.yaml
4	0	.buildkite/test-pipeline.yaml
4	0	.buildkite/test_areas/attention.yaml

[69d09fdd6] RickyChen / 陳昭儒 2026-01-23 [Feature] Add --ssl-ciphers CLI argument for TLS cipher control (#30937)
1	0	vllm/entrypoints/openai/api_server.py
3	0	vllm/entrypoints/openai/cli_args.py

[3a63be0fa] David Ramon Prados 2026-01-22 Support custom URI schemes and trace handlers for profiler (#32393)
34	0	tests/v1/worker/test_gpu_profiler.py
19	11	vllm/config/profiler.py
21	8	vllm/profiler/wrapper.py

[803e3f3f6] Tyler Michael Smith 2026-01-22 [UX] Default api_server_count to dp_size if not specified (#32525)
57	6	vllm/entrypoints/cli/serve.py
3	2	vllm/entrypoints/openai/cli_args.py

[70917b1c5] Vadim Gimpelson 2026-01-22 [MISC] Add .cursor to .gitignore (#32868)
3	0	.gitignore

[c517d8c93] Matt 2026-01-22 [Hardware][AMD][CI][Bugfix] Fix regressions from deprecated env vars (#32837)
2	2	.buildkite/test-amd.yaml
6	1	tests/v1/kv_connector/nixl_integration/config_sweep_accuracy_test.sh
6	3	vllm/platforms/rocm.py

[fc37187a5] Xu Jinyang 2026-01-23 [Bugfix] ModelScope is supported when downloading LORA models. (#32844)
21	6	vllm/lora/utils.py

[ff365eea9] Maximilien de Bayser 2026-01-22 Support bge-m3 sparse embeddings and colbert embeddings (#14526)
38	0	docs/models/pooling_models.md
14	0	tests/models/language/pooling/embed_utils.py
170	0	tests/models/language/pooling/test_bge_m3.py
3	13	tests/models/language/pooling/test_gritlm.py
1	0	tests/models/registry.py
46	1	vllm/model_executor/layers/pooler/special.py
11	3	vllm/model_executor/layers/pooler/tokwise/poolers.py
1	0	vllm/model_executor/models/registry.py
109	2	vllm/model_executor/models/roberta.py

[444e2e7e1] Isotr0py 2026-01-22 [Misc] Bump opencv-python dependecy version to 4.13 (#32668)
1	1	requirements/common.txt
3	3	requirements/nightly_torch_test.txt
3	3	requirements/test.in
23	8	requirements/test.txt
5	1	tests/entrypoints/openai/test_translation_validation.py

[bc14663e6] Nick Hill 2026-01-22 [Cleanup] Move scheduler `get_routed_experts` logic to separate method (#32706)
31	23	vllm/v1/core/sched/scheduler.py

[654a71fc3] Richard Zou 2026-01-22 [torch.compile] Improve Cold Start for MoEs (#32805)
5	1	tests/kernels/moe/test_moe.py
34	1	vllm/forward_context.py
32	7	vllm/model_executor/layers/fused_moe/layer.py

[15e302dfc] Lucas Kabela 2026-01-22 [Misc][BE] Turn on strict type coverage for vllm/compilation (#31756)
7	0	pyproject.toml
1	1	tests/compile/test_pass_manager.py
37	3	tools/pre_commit/mypy.py
1	1	vllm/compilation/backends.py
12	11	vllm/compilation/caching.py
15	13	vllm/compilation/collective_fusion.py
6	6	vllm/compilation/compiler_interface.py
17	17	vllm/compilation/decorators.py
13	9	vllm/compilation/matcher_utils.py
8	6	vllm/compilation/piecewise_backend.py
4	1	vllm/compilation/sequence_parallelism.py

[d117a4d1a] Cyrus Leung 2026-01-22 [Frontend] Introduce Renderer for processing chat messages (using `ModelConfig`) (#30200)
2	0	.buildkite/test-amd.yaml
2	0	.buildkite/test-pipeline.yaml
2	0	.buildkite/test_areas/misc.yaml
2	1	docs/features/reasoning_outputs.md
0	156	tests/entrypoints/openai/test_chat_template.py
58	58	tests/entrypoints/openai/test_serving_chat.py
0	71	tests/entrypoints/openai/test_serving_engine.py
1	0	tests/entrypoints/openai/test_serving_models.py
2	0	tests/entrypoints/openai/test_serving_responses.py
4	4	tests/entrypoints/pooling/score/test_utils.py
24	468	tests/entrypoints/test_chat_utils.py
0	0	tests/renderers/__init__.py
537	0	tests/renderers/test_hf.py
100	0	tests/renderers/test_mistral.py
2	3	tests/test_inputs.py
1	1	tests/v1/engine/test_llm_engine.py
11	10	tests/v1/engine/test_process_multi_modal_uuids.py
1	0	tools/pre_commit/mypy.py
5	6	vllm/engine/protocol.py
27	508	vllm/entrypoints/chat_utils.py
14	54	vllm/entrypoints/llm.py
2	4	vllm/entrypoints/openai/api_server.py
8	8	vllm/entrypoints/openai/chat_completion/serving.py
3	11	vllm/entrypoints/openai/completion/serving.py
54	109	vllm/entrypoints/openai/engine/serving.py
1	0	vllm/entrypoints/openai/models/serving.py
4	1	vllm/entrypoints/openai/responses/context.py
8	5	vllm/entrypoints/openai/responses/serving.py
2	6	vllm/entrypoints/pooling/__init__.py
2	4	vllm/entrypoints/pooling/classify/serving.py
2	4	vllm/entrypoints/pooling/embed/serving.py
2	7	vllm/entrypoints/pooling/pooling/serving.py
4	2	vllm/entrypoints/pooling/score/serving.py
5	4	vllm/entrypoints/pooling/score/utils.py
5	7	vllm/entrypoints/serve/tokenize/serving.py
0	43	vllm/entrypoints/utils.py
7	8	vllm/inputs/preprocess.py
7	0	vllm/renderers/__init__.py
119	0	vllm/renderers/deepseek_v32.py
119	0	vllm/renderers/grok2.py
600	0	vllm/renderers/hf.py
147	0	vllm/renderers/mistral.py
48	0	vllm/renderers/protocol.py
88	0	vllm/renderers/registry.py
85	0	vllm/renderers/terratorch.py
8	10	vllm/v1/engine/async_llm.py
9	3	vllm/v1/engine/input_processor.py
7	9	vllm/v1/engine/llm_engine.py

[421012b63] Or Ozeri 2026-01-22 OffloadingConnector: Support kernel_block_size != block_size (#30692)
1	0	csrc/cache.h
1	4	csrc/cache_kernels.cu
2	1	csrc/torch_bindings.cpp
29	6	tests/kernels/attention/test_cache.py
51	45	tests/v1/kv_offload/test_cpu_gpu.py
25	2	vllm/_custom_ops.py
47	56	vllm/v1/kv_offload/worker/cpu_gpu.py

[841d53aaa] Chauncey 2026-01-22 [Frontend] add prompt_cache_key for openresponses (#32824)
8	0	vllm/entrypoints/openai/responses/protocol.py

[1752262e9] Shengqi Chen 2026-01-22 [CI] refactor release pipeline config into groups (#32833)
277	276	.buildkite/release-pipeline.yaml

[ea6102b85] Nicolò Lucchesi 2026-01-22 [Bugfix] Fix Whisper/encoder-decoder GPU memory leak (#32789)
43	0	tests/models/multimodal/generation/test_whisper.py
11	5	vllm/v1/core/encoder_cache_manager.py

[328cbb277] wang.yuqi 2026-01-22 [Frontend][2/n] Make pooling entrypoints request schema consensus | ChatRequest (#32574)
1	1	docs/design/io_processor_plugins.md
2	2	docs/models/pooling_models.md
1	1	docs/models/supported_models.md
1	1	docs/serving/openai_compatible_server.md
0	0	examples/pooling/embed/{embed_jina_embeddings_v3.py => embed_jina_embeddings_v3_offline.py}
0	0	examples/pooling/embed/{embed_matryoshka_fy.py => embed_matryoshka_fy_offline.py}
18	12	examples/pooling/embed/{embedding_requests_base64_client.py => embedding_requests_base64_online.py}
11	7	examples/pooling/embed/{embedding_requests_bytes_client.py => embedding_requests_bytes_online.py}
0	0	examples/pooling/embed/{openai_embedding_matryoshka_fy.py => openai_embedding_matryoshka_fy_client.py}
0	0	examples/pooling/plugin/{prithvi_geospatial_mae_client.py => prithvi_geospatial_mae_online.py}
10	7	examples/pooling/pooling/{openai_pooling_client.py => pooling_online.py}
0	0	examples/pooling/token_classify/{ner.py => ner_offline.py}
0	0	examples/pooling/token_classify/{ner_client.py => ner_online.py}
0	0	examples/pooling/token_embed/{jina_embeddings_v4.py => jina_embeddings_v4_offline.py}
0	0	examples/pooling/token_embed/{multi_vector_retrieval.py => multi_vector_retrieval_offline.py}
0	0	examples/pooling/token_embed/{multi_vector_retrieval_client.py => multi_vector_retrieval_online.py}
148	2	tests/entrypoints/pooling/classify/test_online.py
124	60	tests/entrypoints/pooling/embed/test_online.py
66	3	tests/entrypoints/pooling/pooling/test_online.py
66	2	vllm/entrypoints/pooling/base/protocol.py
2	42	vllm/entrypoints/pooling/classify/protocol.py
2	2	vllm/entrypoints/pooling/classify/serving.py
2	59	vllm/entrypoints/pooling/embed/protocol.py
2	4	vllm/entrypoints/pooling/pooling/serving.py

[64e3d67ac] liranschour 2026-01-22 Enable Cross layers KV cache layout at NIXL Connector (#30207)
9	0	docs/features/nixl_connector_usage.md
9	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
178	47	tests/v1/kv_connector/unit/test_nixl_connector.py
39	2	vllm/distributed/kv_transfer/kv_connector/utils.py
73	38	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[098b2d66f] Nick Hill 2026-01-22 [Benchmark] Don't default to `temperature==0` in `vllm bench serve` (#32723)
0	3	vllm/benchmarks/lib/endpoint_request_func.py
7	3	vllm/benchmarks/serve.py

[8ebf271bb] Isotr0py 2026-01-22 [Misc] Replace urllib's `urlparse` with urllib3's `parse_url` (#32746)
2	2	vllm/connections.py
2	2	vllm/envs.py
15	10	vllm/multimodal/utils.py
4	2	vllm/utils/network_utils.py

[49a126226] Alex Sun 2026-01-22 [AMD][ROCm] MoRI EP: a high-performance all2all backend (#28664)
1	1	tests/kernels/moe/modular_kernel_tools/cli_args.py
19	1	tests/kernels/moe/modular_kernel_tools/common.py
48	1	tests/kernels/moe/modular_kernel_tools/mk_objects.py
12	0	vllm/_aiter_ops.py
3	0	vllm/config/parallel.py
94	1	vllm/distributed/device_communicators/all2all.py
4	0	vllm/distributed/device_communicators/cuda_communicator.py
3	0	vllm/envs.py
4	0	vllm/model_executor/layers/fused_moe/__init__.py
34	1	vllm/model_executor/layers/fused_moe/all2all_utils.py
8	0	vllm/model_executor/layers/fused_moe/config.py
13	0	vllm/model_executor/layers/fused_moe/layer.py
121	0	vllm/model_executor/layers/fused_moe/mori_prepare_finalize.py
17	4	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
6	0	vllm/platforms/rocm.py
10	0	vllm/utils/import_utils.py

[2b8a38b6d] Cyrus Leung 2026-01-22 [Model] Extend `collect_children` and `no_init_weights` contexts (#32757)
18	2	tests/utils_/test_collection_utils.py
2	15	vllm/model_executor/model_loader/utils.py
81	89	vllm/model_executor/models/adapters.py
4	4	vllm/model_executor/models/bagel.py
14	6	vllm/model_executor/models/chameleon.py
1	4	vllm/model_executor/models/gemma3_mm.py
11	9	vllm/model_executor/models/glm4v.py
15	7	vllm/model_executor/models/idefics3.py
106	65	vllm/model_executor/models/interfaces.py
12	4	vllm/model_executor/models/internlm2.py
4	2	vllm/model_executor/models/minicpmv.py
2	7	vllm/model_executor/models/mllama4.py
2	2	vllm/model_executor/models/nano_nemotron_vl.py
2	2	vllm/model_executor/models/paddleocr_vl.py
3	2	vllm/model_executor/models/paligemma.py
8	7	vllm/model_executor/models/qwen3_vl_moe.py
11	9	vllm/model_executor/models/qwen_vl.py
110	13	vllm/model_executor/models/utils.py
6	4	vllm/model_executor/models/whisper.py
30	2	vllm/utils/collection_utils.py

[1bf1a34b1] Kebe 2026-01-22 [bench] add start_times field to vllm bench serve json result (#32667)
2	0	vllm/benchmarks/serve.py

[a81029983] Andreas Karatzas 2026-01-22 [ROCm][CI][Docs] Add comment explaining TRITON_ATTN fallback for ROCm (#32835)
2	0	tests/v1/spec_decode/test_acceptance_length.py

[eb1629da2] Andreas Karatzas 2026-01-21 [ROCm][CI] Fix AITER test flakiness by using explicit attention backend (#32346)
1	1	.buildkite/test-amd.yaml
5	1	tests/models/language/generation/test_common.py
4	4	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI325X.json

[019e2c3b7] Micah Williamson 2026-01-21 [ROCm][CI] Lower Acceptance Len Threshold For test_draft_model_quantization (#32731)
1	1	tests/v1/e2e/test_spec_decode.py

[f5fdec8ce] Huy Do 2026-01-21 Upgrade transformers-4.57.5 (#32287)
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
1	1	requirements/test.txt
22	0	tests/models/multimodal/pooling/test_jinavl_reranker.py

[1579c9b5f] Patrick von Platen 2026-01-22 [Llama.py -> mistral.py] Extract mistral-only relevant code into separate file (#32780)
5	114	vllm/model_executor/models/llama.py
242	0	vllm/model_executor/models/mistral.py
1	1	vllm/model_executor/models/registry.py

[889722f3b] Lucas Wilkinson 2026-01-21 [FlashMLA] Update FlashMLA to expose new arguments (#32810)
3	0	.gitignore
19	2	cmake/external_projects/flashmla.cmake
10	0	setup.py
0	1	tests/v1/attention/test_sparse_mla_backends.py
1	0	vllm/third_party/flashmla/__init__.py
45	50	vllm/v1/attention/backends/mla/flashmla.py
8	29	vllm/v1/attention/backends/mla/flashmla_sparse.py
47	135	vllm/v1/attention/ops/flashmla.py

[49d965385] Divakar Verma 2026-01-21 [ROCm][CI] fix get_valid_backends (#32787)
8	2	tests/v1/spec_decode/test_acceptance_length.py

[a1d82466e] Ifta khairul Alam Adil 2026-01-22 [Docs] Remove outdated async_scheduling limitation with speculative decoding (#32775)
0	3	vllm/config/scheduler.py

[24a163ed7] Lucain 2026-01-22 Cleanup some huggingface_hub-related stuff (#32788)
2	12	vllm/lora/utils.py
2	13	vllm/transformers_utils/config.py
5	25	vllm/transformers_utils/repo_utils.py

[378385b90] knlnguyen1802 2026-01-22 [EC Connector] Optimize remote cache check in scheduler (#32585)
24	16	tests/v1/core/test_scheduler.py
31	17	tests/v1/ec_connector/unit/test_ec_example_connector.py
7	7	vllm/distributed/ec_transfer/ec_connector/base.py
7	10	vllm/distributed/ec_transfer/ec_connector/example_connector.py
7	7	vllm/v1/core/sched/scheduler.py

[c5487e2b9] Matt 2026-01-21 [Bugfix] Fix potential EAGLE spec decode segfault during graph capture (#32818)
8	4	vllm/v1/spec_decode/eagle.py

[6437ff1fb] Wentao Ye 2026-01-21 [Deprecation] Remove deprecated environment variables (#32812)
2	2	.buildkite/test-amd.yaml
1	1	tests/v1/spec_decode/test_acceptance_length.py
0	46	vllm/config/attention.py
0	67	vllm/envs.py
4	1	vllm/platforms/rocm.py
0	1	vllm/usage/usage_lib.py

[5e00b561c] Woosuk Kwon 2026-01-21 [Model Runner V2] Do not error on attention backends (#32820)
0	10	vllm/v1/worker/gpu/model_runner.py

[408195ec5] Woosuk Kwon 2026-01-21 [Model Runner V2] Refactor Prompt Logprobs (#32811)
17	102	vllm/v1/worker/gpu/model_runner.py
0	29	vllm/v1/worker/gpu/sample/logprob.py
212	0	vllm/v1/worker/gpu/sample/prompt_logprob.py
1	11	vllm/v1/worker/gpu/states.py

[63227accf] Xin Yang 2026-01-21 [Kernel] Add topk_sigmoid kernel (#31246)
99	0	benchmarks/kernels/benchmark_fused_topk.py
7	1	csrc/moe/moe_ops.h
242	101	csrc/moe/topk_softmax_kernels.cu
9	1	csrc/moe/torch_bindings.cpp
137	0	tests/kernels/moe/test_fused_topk.py
17	3	tests/model_executor/test_enabled_custom_ops.py
39	0	vllm/_aiter_ops.py
25	1	vllm/_custom_ops.py
2	2	vllm/model_executor/layers/fused_moe/config.py
97	1	vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py
50	8	vllm/model_executor/layers/fused_moe/router/fused_topk_router.py
1	5	vllm/model_executor/layers/fused_moe/router/router_factory.py
0	3	vllm/model_executor/models/minimax_m2.py

[e675dda67] Yanan Cao 2026-01-21 [Misc] Add Helion version check to collect_env (#32797)
2	0	vllm/collect_env.py

[24dc30f7f] Nick Hill 2026-01-21 [ModelRunner V2] Don't pin reused flashinfer tensors (#32799)
6	1	vllm/v1/attention/backends/flashinfer.py

[180fba653] Divakar Verma 2026-01-21 [ROCm] fix import for on_gfx9 (#32783)
8	1	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
8	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[f99953986] danisereb 2026-01-21 Add missing import of fused_topk to benchmark_moe (#32784)
20	16	benchmarks/kernels/benchmark_moe.py

[e1da249c9] Woosuk Kwon 2026-01-21 [Model Runner V2] Minor refactor for `compute_slot_mappings` (#32794)
22	18	vllm/v1/worker/gpu/block_table.py
3	1	vllm/v1/worker/gpu/model_runner.py
8	3	vllm/v1/worker/gpu/spec_decode/eagle.py

[9b693d023] Nick Hill 2026-01-21 [Misc] Omit "disable NCCL for DP sync" startup log when not applicable (#32707)
8	5	vllm/config/vllm.py

[808d6fd7b] elvischenv 2026-01-22 Bump Flashinfer to v0.6.1 (#30993)
1	1	docker/Dockerfile
2	3	docker/Dockerfile.nightly_torch
1	1	docker/versions.json
1	1	requirements/cuda.txt
0	26	tests/kernels/moe/test_ocp_mx_moe.py
1	0	tests/v1/sample/test_topk_topp_sampler.py
0	7	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
0	1	vllm/model_executor/layers/fused_moe/trtllm_moe.py
1	2	vllm/model_executor/layers/quantization/mxfp4.py
0	2	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
0	24	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
13	5	vllm/v1/attention/backends/flashinfer.py

[1861ae8aa] whx 2026-01-22 [PluggableLayer][1/N] Define PluggableLayer (Fix ci) (#32744)
5	5	benchmarks/kernels/benchmark_activation.py
0	9	docs/design/custom_op.md
2	2	tests/kernels/utils.py
5	5	tests/model_executor/test_enabled_custom_ops.py
2	2	vllm/config/compilation.py
86	14	vllm/model_executor/custom_op.py
8	11	vllm/model_executor/layers/mla.py

[4e31b7f22] Robert Shaw 2026-01-21 [Quantization][Deprecation] Remove RTN (#32697)
0	39	tests/quantization/test_rtn.py
0	4	vllm/model_executor/layers/quantization/__init__.py
0	626	vllm/model_executor/layers/quantization/rtn.py
0	61	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[6c20e89c0] Pleaplusone 2026-01-21 [ROCm][Deepseekv3.2] Refactor Sparse Indexer as CustomOp (#29287)
12	0	vllm/_aiter_ops.py
1	0	vllm/config/compilation.py
318	0	vllm/model_executor/layers/sparse_attn_indexer.py
14	233	vllm/model_executor/models/deepseek_v2.py
3	0	vllm/platforms/rocm.py
6	0	vllm/v1/attention/backends/mla/indexer.py
110	10	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
518	80	vllm/v1/attention/ops/rocm_aiter_mla_sparse.py

[85f55c943] Robert Shaw 2026-01-21 [Quantization][Deprecation] Deprecate HQQ (#32681)
0	9	csrc/quantization/gptq_marlin/generate_kernels.py
0	87	tests/kernels/quantization/test_marlin_gemm.py
0	1	tests/weight_loading/models.txt
0	3	vllm/lora/layers/base_linear.py
0	3	vllm/lora/layers/utils.py
0	1	vllm/model_executor/layers/linear.py
0	4	vllm/model_executor/layers/quantization/__init__.py
0	372	vllm/model_executor/layers/quantization/hqq_marlin.py

[cea3c754c] Robert Shaw 2026-01-21 [Quantization][Deprecation] Remove `DeepSpeedFp8` (#32679)
0	2	examples/offline_inference/load_sharded_state.py
0	2	examples/offline_inference/save_sharded_state.py
0	4	vllm/model_executor/layers/quantization/__init__.py
0	218	vllm/model_executor/layers/quantization/deepspeedfp.py
19	58	vllm/model_executor/models/arctic.py

[42135d689] Robert Shaw 2026-01-21 [MoE Refactor] Oracle Select FP8+NVFP4 Kernels In Priority (#32414)
40	0	.buildkite/test-pipeline.yaml
7	5	benchmarks/kernels/benchmark_cutlass_moe_fp8.py
3	4	benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py
13	12	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
29	1	benchmarks/kernels/benchmark_moe.py
2	2	docs/design/moe_kernel_features.md
3	3	tests/compile/test_fusion_attn.py
3	3	tests/compile/test_silu_mul_quant_fusion.py
3	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Llama-4-Scout-Fp8-ModelOpt-triton.yaml
0	1	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ll.yaml
0	1	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ll.yaml
2	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-triton.yaml
0	2	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-cutlass.yaml
0	2	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-trtllm.yaml
0	2	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-marlin.yaml
1	1	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-triton.yaml
0	2	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-fi-cutlass.yaml
0	2	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-marlin.yaml
1	1	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml
2	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-vllm-cutlass.yaml
2	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-vllm-cutlass.yaml
5	0	tests/kernels/moe/modular_kernel_tools/common.py
17	75	tests/kernels/moe/modular_kernel_tools/mk_objects.py
3	0	tests/kernels/moe/test_batched_deepgemm.py
39	3	tests/kernels/moe/test_block_fp8.py
16	13	tests/kernels/moe/test_cutlass_moe.py
6	5	tests/kernels/moe/test_deepep_deepgemm_moe.py
11	2	tests/kernels/moe/test_deepep_moe.py
26	13	tests/kernels/moe/test_deepgemm.py
25	20	tests/kernels/moe/test_flashinfer.py
29	5	tests/kernels/moe/test_flashinfer_moe.py
4	2	tests/kernels/moe/test_modular_oai_triton_moe.py
3	3	tests/kernels/moe/test_moe.py
2	3	tests/kernels/moe/test_nvfp4_moe.py
26	21	tests/kernels/moe/test_pplx_cutlass_moe.py
2	0	tests/kernels/moe/test_pplx_moe.py
0	7	tests/kernels/moe/test_routing.py
13	3	tests/kernels/moe/test_triton_moe_no_act_mul.py
36	1	tests/kernels/moe/utils.py
2	1	tools/vllm-rocm/pin_rocm_dependencies.py
3	3	vllm/compilation/activation_quant_fusion.py
2	2	vllm/compilation/fusion.py
2	2	vllm/compilation/fusion_attn.py
2	2	vllm/compilation/matcher_utils.py
48	13	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
34	1	vllm/model_executor/layers/fused_moe/config.py
164	91	vllm/model_executor/layers/fused_moe/cutlass_moe.py
39	93	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
70	8	vllm/model_executor/layers/fused_moe/fallback.py
50	41	vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py
93	106	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
105	1	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
121	26	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
80	46	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
120	77	vllm/model_executor/layers/fused_moe/fused_moe.py
47	28	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
36	34	vllm/model_executor/layers/fused_moe/layer.py
115	6	vllm/model_executor/layers/fused_moe/modular_kernel.py
350	186	vllm/model_executor/layers/fused_moe/oracle/fp8.py
258	91	vllm/model_executor/layers/fused_moe/oracle/nvfp4.py
9	8	vllm/model_executor/layers/fused_moe/oracle/unquantized.py
52	10	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
5	0	vllm/model_executor/layers/fused_moe/router/custom_routing_router.py
4	6	vllm/model_executor/layers/fused_moe/router/grouped_topk_router.py
0	3	vllm/model_executor/layers/fused_moe/router/router_factory.py
14	8	vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py
14	4	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
47	11	vllm/model_executor/layers/fused_moe/trtllm_moe.py
6	3	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
2	0	vllm/model_executor/layers/quantization/awq_marlin.py
194	185	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
51	104	vllm/model_executor/layers/quantization/fp8.py
2	0	vllm/model_executor/layers/quantization/gptq_marlin.py
64	42	vllm/model_executor/layers/quantization/modelopt.py
4	3	vllm/model_executor/layers/quantization/mxfp4.py
84	38	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
0	31	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
26	12	vllm/model_executor/layers/quantization/utils/quant_utils.py
0	2	vllm/model_executor/models/qwen3_moe.py
0	2	vllm/model_executor/models/qwen3_next.py
4	0	vllm/model_executor/warmup/deep_gemm_warmup.py
2	2	vllm/v1/attention/backends/flashinfer.py

[e14467be4] Divakar Verma 2026-01-21 [bugfix] Aria model (#32727)
16	1	vllm/model_executor/models/aria.py

[7727ce35c] Kim Hee Su 2026-01-21 [Model] Add Eagle2.5-8B Vision-Language Model support   (#32456)
1	0	docs/models/supported_models.md
35	0	examples/offline_inference/vision_language.py
3	0	tests/models/registry.py
475	0	vllm/model_executor/models/eagle2_5_vl.py
4	0	vllm/model_executor/models/registry.py

[6bb2bc71e] Yanwen Lin 2026-01-21 [Bugfix] Force using spawn multiprocess method when it's the WSL platform (#32749)
4	0	vllm/utils/system_utils.py

[c80f92c14] Lucas Kabela 2026-01-20 [Documentation] Fix typo in `docs/design/torch_compile_multimodal.md` (#32741)
5	5	docs/design/torch_compile_multimodal.md

[f23fb5a7c] RickyChen / 陳昭儒 2026-01-21 [Bugfix] Support HF sharded weights for Mistral3/Pixtral models (#32673)
14	8	vllm/model_executor/models/pixtral.py

[360aa93f8] Paco Xu 2026-01-21 [Docs] Fix GitHub handle in governance process (#32582)
2	2	docs/governance/process.md

[27ca95b3c] Netanel Haber 2026-01-21 [Bugfix] Fix Nemotron-Nano-v2-vlm static resolution (#32682)
3	1	vllm/model_executor/models/nano_nemotron_vl.py

[b4f64e5b0] Lucas Wilkinson 2026-01-20 Update FlashMLA (#32491)
40	10	cmake/external_projects/flashmla.cmake
1	1	tests/kernels/attention/test_flashmla_sparse.py
64	11	tests/v1/attention/test_sparse_mla_backends.py
64	20	vllm/v1/attention/backends/mla/flashmla_sparse.py

[7ab80a8e3] shanjiaz 2026-01-20 Added qwen3 vision language moe support for speculative decoding (#32048)
8	0	vllm/model_executor/models/qwen3_vl_moe.py
9	1	vllm/v1/spec_decode/eagle.py

[0900cedb3] gopalsarda 2026-01-20 Enable Eagle3 speculative decoding for Pixtral (LlavaForConditionalGeneration) (#32542)
9	1	vllm/model_executor/models/llava.py

[6f067b1fb] Nick Hill 2026-01-20 [Cleanup] Remove unused `KVConnectorModelRunnerMixin` methods (#32077)
2	2	tests/v1/kv_connector/unit/test_nixl_connector.py
0	30	vllm/v1/worker/kv_connector_model_runner_mixin.py

[27b81e010] Alex Brooks 2026-01-20 [Bugfix] Fix Granite Vision / Don't use Siglip Pooling Head Nested Models by Default  (#32299)
8	0	tests/models/multimodal/generation/test_common.py
4	2	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	0	tests/models/registry.py
55	13	vllm/model_executor/models/siglip.py
15	1	vllm/model_executor/models/vision.py

[7013e9ac8] Or Ozeri 2026-01-21 OffloadingConnector: Prevent redundant loads (#29087)
67	1	tests/v1/kv_connector/unit/test_offloading_connector.py
36	3	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
4	2	vllm/v1/kv_offload/abstract.py
1	1	vllm/v1/kv_offload/arc_manager.py
1	1	vllm/v1/kv_offload/lru_manager.py

[c78ee240b] Robert Shaw 2026-01-20 Revert "[PluggableLayer][1/N] Define PluggableLayer" (#32725)
9	0	docs/design/custom_op.md
5	5	tests/model_executor/test_enabled_custom_ops.py
14	86	vllm/model_executor/custom_op.py
11	8	vllm/model_executor/layers/mla.py

[d2389c126] Vasiliy Kuznetsov 2026-01-20 fp8 online quant: split out Fp8OnlineLinearMethod (#32189)
4	1	tests/quantization/test_fp8.py
139	109	vllm/model_executor/layers/quantization/fp8.py

[22375f8d1] Micah Williamson 2026-01-20 [ROCm][CI] Remove DS async eplb accuracy test from AMD CI (#32717)
0	11	.buildkite/test-amd.yaml

[9b67338b7] TJian 2026-01-21 [Bugfix] Suppress log on non-ROCm platform (#32703)
5	4	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py

[226134080] Lucas Wilkinson 2026-01-20 [Misc] Remove pad_for_cudagraphs from config (#30143)
34	2	tests/compile/test_config.py
9	1	tests/models/language/generation/test_hybrid.py
4	6	tests/v1/cudagraph/test_cudagraph_dispatch.py
1	1	vllm/compilation/piecewise_backend.py
1	1	vllm/compilation/sequence_parallelism.py
0	47	vllm/config/compilation.py
55	5	vllm/v1/cudagraph_dispatcher.py
43	64	vllm/v1/spec_decode/eagle.py
10	9	vllm/v1/worker/gpu_model_runner.py

[86c69dc54] Shinichi Hemmi 2026-01-21 [Bugfix] Fix byte fallback handling when using outlines  (#31391)
6	1	vllm/v1/structured_output/backend_outlines.py
3	1	vllm/v1/structured_output/utils.py

[7c5dedc24] dolpm 2026-01-20 [AOT compilation] support torch.compile inductor artifacts in VllmCompiledFunction (#25205)
415	2	tests/compile/test_aot_compile.py
2	0	tools/pre_commit/check_pickle_imports.py
228	60	vllm/compilation/backends.py
327	8	vllm/compilation/caching.py
41	7	vllm/compilation/compiler_interface.py
36	16	vllm/compilation/decorators.py
112	20	vllm/compilation/piecewise_backend.py
8	0	vllm/envs.py

[193069d12] Cyrus Leung 2026-01-21 [5/N] Initialize MM components in context managers (Q-Z) (#32695)
13	15	vllm/model_executor/models/qwen2_audio.py
52	45	vllm/model_executor/models/qwen3_omni_moe_thinker.py
14	6	vllm/model_executor/models/qwen3_vl.py
0	1	vllm/model_executor/models/radio.py
14	15	vllm/model_executor/models/siglip.py
18	21	vllm/model_executor/models/skyworkr1v.py
37	36	vllm/model_executor/models/tarsier.py
14	13	vllm/model_executor/models/ultravox.py
15	15	vllm/model_executor/models/voxtral.py

[f0feb1cf8] Rahul Tuli 2026-01-21 Test: added acceptance length tests (#32030)
2	0	tests/v1/spec_decode/__init__.py
277	0	tests/v1/spec_decode/test_acceptance_length.py

[09194b90a] Cyrus Leung 2026-01-21 [Doc] Update docs for MM model development with context usage (#32691)
19	27	docs/contributing/model/multimodal.md
1	1	examples/online_serving/disaggregated_encoder/README.md

[9ab4388cd] Woosuk Kwon 2026-01-20 [Model Runner V2] Support FLASHINFER_MLA backend (#32709)
1	1	vllm/v1/worker/gpu/attn_utils.py
1	1	vllm/v1/worker/gpu/model_runner.py

[04a9e064d] JJJYmmm 2026-01-21 [Bugfix] fix the ima issue of qwen-vit (#32687)
1	0	vllm/model_executor/models/qwen2_5_vl.py
2	2	vllm/model_executor/models/qwen3_vl.py

[c025263dd] TJian 2026-01-21 [Doc] [ROCm] Update ROCm getting started doc (#32580)
80	15	docs/getting_started/installation/gpu.rocm.inc.md
16	27	docs/getting_started/quickstart.md

[6c97b9b9b] Wentao Ye 2026-01-20 [Perf] Only clone when needed for `moe_permute` (#32273)
4	3	csrc/moe/moe_permute_unpermute_op.cu
1	1	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
1	1	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h

[4ca62a0db] whx 2026-01-21 [PluggableLayer][1/N] Define PluggableLayer (#32331)
0	9	docs/design/custom_op.md
5	5	tests/model_executor/test_enabled_custom_ops.py
86	14	vllm/model_executor/custom_op.py
8	11	vllm/model_executor/layers/mla.py

[7901109ea] linhaifeng 2026-01-21 [Bugfix] Fix Off-by-one error in _num_tokens_to_min_blocks calculation (#32603)
2	2	tests/kernels/utils.py

[13f6630a9] YiSheng5 2026-01-20 [XPU]Support AgRsAll2AllManager on XPU device (#32654)
130	7	vllm/distributed/device_communicators/xpu_communicator.py

[fda3f03eb] Cyrus Leung 2026-01-20 [4/N] Initialize MM components in context managers (M-P) (#32663)
2	8	vllm/model_executor/models/aria.py
0	8	vllm/model_executor/models/dots_ocr.py
4	5	vllm/model_executor/models/interfaces.py
0	1	vllm/model_executor/models/interns1.py
0	1	vllm/model_executor/models/internvl.py
21	24	vllm/model_executor/models/midashenglm.py
5	3	vllm/model_executor/models/minicpmo.py
20	21	vllm/model_executor/models/minicpmv.py
27	36	vllm/model_executor/models/minimax_vl_01.py
14	12	vllm/model_executor/models/molmo.py
13	13	vllm/model_executor/models/molmo2.py
31	31	vllm/model_executor/models/nano_nemotron_vl.py
11	12	vllm/model_executor/models/nemotron_parse.py
13	19	vllm/model_executor/models/nemotron_vl.py
16	18	vllm/model_executor/models/ovis.py
24	17	vllm/model_executor/models/ovis2_5.py
23	36	vllm/model_executor/models/paddleocr_vl.py
20	22	vllm/model_executor/models/paligemma.py
24	29	vllm/model_executor/models/phi3v.py
14	13	vllm/model_executor/models/phi4mm.py
6	6	vllm/model_executor/models/qwen3_vl_moe.py
0	1	vllm/model_executor/models/skyworkr1v.py
0	8	vllm/model_executor/models/step3_vl.py
1	8	vllm/model_executor/models/tarsier.py

[bb9172030] 杨朱 · Kiki 2026-01-20 [Metrics] Complete removal of deprecated vllm:time_per_output_token_seconds metric (#32661)
1	1	docs/design/metrics.md
8	8	examples/online_serving/dashboards/grafana/performance_statistics.json
10	10	examples/online_serving/dashboards/perses/performance_statistics.yaml
1	7	tests/entrypoints/instrumentator/test_metrics.py
0	39	vllm/v1/metrics/loggers.py

[c4e5bdf61] Chauncey 2026-01-20 [Bugfix] Fix the  fp8_mqa_logits dim mismatch (#32652)
1	1	vllm/model_executor/models/deepseek_v2.py
2	2	vllm/utils/deep_gemm.py

[7f1bcd18f] Cyrus Leung 2026-01-20 [3/N] Initialize MM components in context managers (I-L) (#32650)
13	18	vllm/model_executor/models/interns1.py
16	19	vllm/model_executor/models/internvl.py
18	25	vllm/model_executor/models/isaac.py
15	13	vllm/model_executor/models/kanana_v.py
21	23	vllm/model_executor/models/keye.py
27	188	vllm/model_executor/models/kimi_vl.py
25	27	vllm/model_executor/models/lfm2_vl.py
23	25	vllm/model_executor/models/llava_next.py
24	28	vllm/model_executor/models/llava_next_video.py
20	22	vllm/model_executor/models/llava_onevision.py

[8be263c3f] Walter Beller-Morales 2026-01-20 [Core] Cleanup shm based object store on engine shutdown (#32429)
1	1	tests/distributed/test_shm_buffer.py
1	1	tests/distributed/test_shm_storage.py
13	3	vllm/distributed/device_communicators/shm_object_storage.py
27	2	vllm/envs.py
8	0	vllm/multimodal/cache.py
3	0	vllm/v1/engine/async_llm.py
4	0	vllm/v1/engine/input_processor.py

[e1a34c3a5] Cyrus Leung 2026-01-20 [2/N] Initialize MM components in context managers (E-H) (#32641)
0	2	vllm/model_executor/models/aria.py
0	1	vllm/model_executor/models/aya_vision.py
0	2	vllm/model_executor/models/cohere2_vision.py
21	23	vllm/model_executor/models/ernie45_vl.py
15	18	vllm/model_executor/models/fuyu.py
20	23	vllm/model_executor/models/gemma3_mm.py
28	30	vllm/model_executor/models/gemma3n_mm.py
15	16	vllm/model_executor/models/glm4_1v.py
19	21	vllm/model_executor/models/glmasr.py
21	22	vllm/model_executor/models/granite_speech.py
11	15	vllm/model_executor/models/hunyuan_vision.py
13	18	vllm/model_executor/models/hyperclovax_vision.py

[148117ea2] vllmellm 2026-01-20 [Refactor] Make FP8 Linear Ops use kernel abstraction (#27814)
5	0	.buildkite/lm-eval-harness/configs/models-small-rocm.txt
17	27	tests/compile/distributed/test_fusion_all_reduce.py
17	26	tests/compile/distributed/test_sequence_parallelism.py
19	22	tests/compile/test_functionalization.py
182	146	tests/compile/test_fusion.py
20	21	tests/compile/test_fusion_attn.py
44	27	tests/compile/test_silu_mul_quant_fusion.py
24	22	tests/kernels/quantization/test_scaled_mm_kernel_selection.py
1	1	tests/quantization/test_compressed_tensors.py
127	0	tests/utils.py
1	1	vllm/_aiter_ops.py
33	30	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
10	23	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
12	16	vllm/model_executor/layers/quantization/fbgemm_fp8.py
21	26	vllm/model_executor/layers/quantization/fp8.py
144	33	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
183	30	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
17	36	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
33	38	vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py
61	35	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
57	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/flashinfer.py
221	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/pytorch.py
117	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/rocm.py
23	18	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
18	19	vllm/model_executor/layers/quantization/modelopt.py
10	13	vllm/model_executor/layers/quantization/ptpc_fp8.py
28	19	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
10	22	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
3	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
0	378	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[e9c83cdc5] Woosuk Kwon 2026-01-19 [Model Runner V2] Skip kernel launch for penalties & logit_bias (#32634)
18	1	vllm/v1/worker/gpu/sample/logit_bias.py
15	2	vllm/v1/worker/gpu/sample/penalties.py
2	2	vllm/v1/worker/gpu/sample/sampler.py

[b75e85ded] Cyrus Leung 2026-01-20 [1/N] Initialize MM components in context managers (A-D) (#32632)
21	32	vllm/model_executor/models/aria.py
13	15	vllm/model_executor/models/audioflamingo3.py
17	18	vllm/model_executor/models/aya_vision.py
36	43	vllm/model_executor/models/bagel.py
24	30	vllm/model_executor/models/blip2.py
23	24	vllm/model_executor/models/clip.py
17	18	vllm/model_executor/models/cohere2_vision.py
40	41	vllm/model_executor/models/deepseek_ocr.py
29	30	vllm/model_executor/models/deepseek_vl2.py
15	15	vllm/model_executor/models/dots_ocr.py
4	1	vllm/model_executor/models/interfaces.py

[4753f3bf6] Cyrus Leung 2026-01-20 [Model] Use context managers for encoder- and LM-only mode (#32605)
1	1	examples/online_serving/disaggregated_encoder/README.md
12	0	vllm/config/model.py
6	0	vllm/config/multimodal.py
5	0	vllm/engine/arg_utils.py
1	8	vllm/model_executor/model_loader/utils.py
0	61	vllm/model_executor/models/adapters.py
109	13	vllm/model_executor/models/interfaces.py
9	22	vllm/model_executor/models/llava.py
8	18	vllm/model_executor/models/mistral3.py
17	18	vllm/model_executor/models/mllama4.py
8	9	vllm/model_executor/models/opencua.py
7	18	vllm/model_executor/models/pixtral.py
12	31	vllm/model_executor/models/qwen2_5_omni_thinker.py
9	26	vllm/model_executor/models/qwen2_5_vl.py
8	17	vllm/model_executor/models/qwen2_vl.py
25	43	vllm/model_executor/models/qwen3_vl.py
25	28	vllm/model_executor/models/qwen3_vl_moe.py
10	27	vllm/model_executor/models/step3_vl.py
8	9	vllm/model_executor/models/step_vl.py
6	2	vllm/model_executor/models/utils.py
6	4	vllm/v1/worker/gpu_model_runner.py

[6c01ffb89] Woosuk Kwon 2026-01-19 [Model Runner V2] Decouple temperature from penalties (#32629)
45	1	vllm/v1/worker/gpu/sample/gumbel.py
31	50	vllm/v1/worker/gpu/sample/penalties.py
6	5	vllm/v1/worker/gpu/sample/sampler.py

[7b7cdce96] Woosuk Kwon 2026-01-19 [Model Runner V2] Refactor get_cudagraph_and_dp_padding (#32625)
40	5	vllm/v1/worker/gpu/dp_utils.py
14	62	vllm/v1/worker/gpu/model_runner.py

[12dab78f4] Jackmin801 2026-01-19 [Feat] allow inplace loading lora (#31326)
18	0	docs/features/lora.md
16	0	tests/entrypoints/conftest.py
76	0	tests/entrypoints/openai/test_lora_adapters.py
12	0	tests/lora/conftest.py
111	2	tests/lora/test_llm_with_multi_loras.py
13	4	vllm/entrypoints/openai/models/serving.py
1	0	vllm/entrypoints/serve/lora/api_router.py
1	0	vllm/entrypoints/serve/lora/protocol.py
6	0	vllm/lora/request.py
8	1	vllm/lora/worker_manager.py

[05dc4bfab] Woosuk Kwon 2026-01-19 [Model Runner V2] Initialized communication buffer for DP (#32624)
7	0	vllm/v1/worker/gpu/model_runner.py

[1a1fc3bbc] Matthew Bonanni 2026-01-19 [Attention][MLA] Make FLASHINFER_MLA the default MLA backend on Blackwell, and TRTLLM the default prefill (#32615)
1	1	.buildkite/test-amd.yaml
1	1	.buildkite/test-pipeline.yaml
1	1	.buildkite/test_areas/attention.yaml
1	1	vllm/config/attention.py
11	10	vllm/model_executor/layers/attention/mla_attention.py
4	4	vllm/platforms/cuda.py

[43fada536] Woosuk Kwon 2026-01-19 [Model Runner V2] Refactor `dummy_run` (#32533)
2	3	vllm/v1/worker/gpu/cudagraph_utils.py
31	46	vllm/v1/worker/gpu/model_runner.py
1	6	vllm/v1/worker/gpu_worker.py

[4a5299c93] Tomas Ruiz 2026-01-19 feat: spec decode with draft models (#24322)
17	2	examples/offline_inference/spec_decode.py
1	1	examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py
306	9	tests/v1/e2e/test_spec_decode.py
35	0	tests/v1/worker/test_utils.py
10	9	vllm/benchmarks/datasets.py
6	0	vllm/benchmarks/lib/ready_checker.py
4	0	vllm/config/parallel.py
33	6	vllm/config/speculative.py
23	13	vllm/config/vllm.py
0	15	vllm/engine/arg_utils.py
7	2	vllm/model_executor/model_loader/__init__.py
2	2	vllm/model_executor/model_loader/base_loader.py
2	2	vllm/model_executor/model_loader/gguf_loader.py
4	3	vllm/model_executor/model_loader/tensorizer_loader.py
11	1	vllm/v1/attention/backend.py
32	0	vllm/v1/attention/backends/utils.py
2	0	vllm/v1/core/sched/scheduler.py
271	0	vllm/v1/spec_decode/draft_model.py
98	38	vllm/v1/spec_decode/eagle.py
31	10	vllm/v1/worker/gpu_model_runner.py
2	2	vllm/v1/worker/utils.py

[73f2a81c7] lon 2026-01-19 docs: prefix caching seems quite outdated (#28784)
7	2	docs/design/prefix_caching.md

[735033171] jiahanc 2026-01-19 [BugFix] Fix TRT-LLM NVFP4 DP/EP (#32349)
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml
1	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt
1	8	vllm/model_executor/layers/fused_moe/layer.py
11	3	vllm/model_executor/layers/quantization/modelopt.py

[9d1e611f0] Yanan Cao 2026-01-19 [CI] Add Helion as an optional dependency (#32482)
11	0	.buildkite/test-amd.yaml
10	0	.buildkite/test-pipeline.yaml
2	0	setup.py
45	0	tests/kernels/helion/test_helion_available.py
15	0	vllm/utils/import_utils.py

[0727cc9ec] Vadim Gimpelson 2026-01-19 [BUGFIX] Fix `test_mla_backends.py`. Scale MLA projection weights to prevent numerical instability  (#32529)
8	0	tests/v1/attention/test_mla_backends.py

[a0490be8f] qli88 2026-01-19 [CI][amd] Revert NIXL connector change to avoid crash (#32570)
2	2	.buildkite/test-amd.yaml
1	1	docker/Dockerfile.rocm
5	5	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[cd3ac5b79] Netanel Haber 2026-01-19 support dynamic resolution image encoding for Nemotron Nano VL (#32121)
6	2	vllm/model_executor/models/intern_vit.py
544	141	vllm/model_executor/models/nano_nemotron_vl.py
205	21	vllm/model_executor/models/radio.py

[2636d7625] Jee Jee Li 2026-01-20 [Misc] Remove unused ModelKeys (#32608)
1	38	vllm/model_executor/models/module_mapping.py

[aa7f37ccf] danisereb 2026-01-19 Add support for LoRA adapters in Nemotron-H models (#30802)
297	0	tests/lora/test_layers.py
2	0	vllm/lora/layers/__init__.py
85	4	vllm/lora/layers/column_parallel_linear.py
25	17	vllm/lora/layers/fused_moe.py
26	3	vllm/lora/lora_weights.py
47	3	vllm/lora/model_manager.py
6	0	vllm/lora/utils.py
5	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
1	0	vllm/model_executor/models/interfaces.py
3	0	vllm/model_executor/models/nemotron_h.py

[c88860d75] wang.yuqi 2026-01-19 [Frontend] Score entrypoint support data_1 & data_2 and queries & documents as inputs (#32577)
15	15	docs/serving/openai_compatible_server.md
5	5	examples/offline_inference/basic/score.py
2	2	examples/offline_inference/openai_batch/README.md
0	0	examples/pooling/score/{cohere_rerank_online.py => cohere_rerank_client.py}
2	2	examples/pooling/score/qwen3_reranker_online.py
18	12	examples/pooling/score/score_api_online.py
20	2	examples/pooling/score/vision_rerank_api_online.py
22	4	examples/pooling/score/vision_score_api_online.py
2	2	tests/entrypoints/openai/test_run_batch.py
2	2	tests/entrypoints/pooling/classify/test_online.py
4	4	tests/entrypoints/pooling/score/test_offline.py
95	37	tests/entrypoints/pooling/score/test_online_score.py
2	2	tests/models/language/pooling_mteb_test/mteb_score_utils.py
9	2	vllm/entrypoints/openai/engine/serving.py
1	1	vllm/entrypoints/openai/run_batch.py
38	5	vllm/entrypoints/pooling/score/protocol.py
16	16	vllm/entrypoints/pooling/score/serving.py

[758df5afe] Nicolò Lucchesi 2026-01-19 [NIXL][Metrics] Track `nixl_num_kv_expired_reqs` metric in Prometheus (#32340)
31	6	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[cdd03d25d] Daniel Mescheder 2026-01-19 [CI/Build] Fix dependency conflict between model-hosting-container-standards and starlette (#32560)
5	2	requirements/test.txt

[74c583bc5] Nicolò Lucchesi 2026-01-19 [Core] Whisper support `torch.compile` (#30385)
3	1	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py
7	0	vllm/compilation/decorators.py
7	0	vllm/forward_context.py
2	0	vllm/model_executor/models/whisper.py
8	0	vllm/v1/worker/gpu_model_runner.py

[c0a350ca7] Andreas Karatzas 2026-01-19 [ROCm][CI] Add ROCm attention backend support for EAGLE DP tests (#32363)
28	6	tests/v1/distributed/test_eagle_dp.py

[71832ba71] Yuxuan Zhang 2026-01-19 [GLM-4.7] GLM Model support for GLM-Lite (#31386)
1	0	benchmarks/kernels/benchmark_moe.py
1	0	benchmarks/kernels/benchmark_moe_permute_unpermute.py
1	0	docs/features/tool_calling.md
10	0	tests/models/registry.py
12	0	vllm/config/speculative.py
642	0	vllm/model_executor/models/glm4_moe_lite.py
464	0	vllm/model_executor/models/glm4_moe_lite_mtp.py
2	1	vllm/model_executor/models/glm4_moe_mtp.py
2	0	vllm/model_executor/models/registry.py

[11bbf86f6] Matt 2026-01-19 [CI][Hardware][AMD] Fix test_rotary_embedding_mla_cache_fused (#32408)
12	3	tests/kernels/core/test_rotary_embedding_mla_cache_fused.py

[3c8740aac] Hyunkyun Moon 2026-01-19 [Frontend] Add render endpoints for prompt preprocessing (#32473)
226	0	tests/entrypoints/openai/test_render.py
31	0	vllm/entrypoints/openai/chat_completion/api_router.py
39	14	vllm/entrypoints/openai/chat_completion/serving.py
30	0	vllm/entrypoints/openai/completion/api_router.py
47	28	vllm/entrypoints/openai/completion/serving.py

[7518a3dc6] Alex Brooks 2026-01-18 [CI/Build] Use Common Event Map Fixture in Harmony / MCP Server Tests (#32531)
30	0	tests/entrypoints/openai/responses/conftest.py
6	27	tests/entrypoints/openai/responses/test_harmony.py
4	12	tests/entrypoints/openai/responses/test_mcp_tools.py

[976af2f31] honglyua 2026-01-19 [BugFix] Fix embed_input_ids argument error of QwenVLForConditionalGeneration (#32462)
2	0	vllm/model_executor/models/qwen_vl.py

[9a1f16da1] Woosuk Kwon 2026-01-18 [Model Runner V2] Refactor `update_states` (#32562)
24	17	vllm/v1/worker/gpu/model_runner.py

[bb1848cd6] Woosuk Kwon 2026-01-18 [Model Runner V2] Support VLM (#32546)
7	0	vllm/v1/worker/gpu/cudagraph_utils.py
3	3	vllm/v1/worker/gpu/input_batch.py
184	0	vllm/v1/worker/gpu/mm/encoder_runner.py
3	5	vllm/v1/worker/gpu/mm/mrope_utils.py
66	4	vllm/v1/worker/gpu/model_runner.py
0	3	vllm/v1/worker/gpu/spec_decode/eagle.py

[6101a26dc] Vadim Gimpelson 2026-01-19 [BUGFIX]  Fix degenerate strides in TRTLLM query tensors for FlashInfer backend. Fixes issue #32353 (#32417)
10	4	vllm/v1/attention/backends/flashinfer.py

[f5d174003] Iryna Boiko 2026-01-18 [Bugfix] Add OOT backend option (#32471)
4	0	vllm/model_executor/layers/fused_moe/oracle/unquantized.py

[eebc58df0] Wentao Ye 2026-01-18 [Refactor] Remove unused cutlass moe problem size function (#32047)
0	6	csrc/ops.h
0	20	csrc/quantization/w8a8/cutlass/moe/moe_data.cu
0	27	csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu
0	13	csrc/torch_bindings.cpp
0	35	vllm/_custom_ops.py

[16de822c7] Wentao Ye 2026-01-18 [Refactor] Remove unused file `pallas_kv_cache_update.py` (#32433)
0	130	vllm/v1/attention/ops/pallas_kv_cache_update.py

[5480c6b1f] Deming 2026-01-19 [Doc] Correct comment for _jobs dict in OffloadingConnectorWorker (#32556)
1	1	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py

[ba29ab441] Andrey Khalyavin 2026-01-18 Use the same memory for workspace13 and fused_output. (#31531)
8	6	vllm/model_executor/layers/fused_moe/modular_kernel.py

[afc362260] Robert Shaw 2026-01-18 [CI] Move Distributed Tests from H200 -> H100 (#32555)
5	4	.buildkite/test-pipeline.yaml

[327a02d8d] bnellnm 2026-01-18 [MoE Refactor] Separate Router into OO Classes (#30623)
1	1	tests/kernels/moe/modular_kernel_tools/common.py
1	1	tests/kernels/moe/test_batched_moe.py
4	2	tests/kernels/moe/test_block_fp8.py
1	1	tests/kernels/moe/test_cutlass_moe.py
1	1	tests/kernels/moe/test_flashinfer_moe.py
1	1	tests/kernels/moe/test_grouped_topk.py
3	1	tests/kernels/moe/test_moe.py
1	1	tests/kernels/moe/test_moe_permute_unpermute.py
1	1	tests/kernels/moe/test_nvfp4_moe.py
1	1	tests/kernels/moe/test_pplx_cutlass_moe.py
499	0	tests/kernels/moe/test_routing.py
38	34	tests/{ => kernels/moe}/test_routing_simulator.py
1	1	tests/model_executor/test_enabled_custom_ops.py
9	0	vllm/distributed/eplb/eplb_state.py
9	5	vllm/model_executor/layers/fused_moe/__init__.py
5	1	vllm/model_executor/layers/fused_moe/config.py
0	375	vllm/model_executor/layers/fused_moe/fused_moe.py
3	3	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
3	1	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
60	224	vllm/model_executor/layers/fused_moe/layer.py
2	0	vllm/model_executor/layers/fused_moe/router/__init__.py
245	0	vllm/model_executor/layers/fused_moe/router/base_router.py
53	0	vllm/model_executor/layers/fused_moe/router/custom_routing_router.py
0	0	vllm/model_executor/layers/fused_moe/{ => router}/fused_moe_router.py
88	0	vllm/model_executor/layers/fused_moe/router/fused_topk_bias_router.py
118	0	vllm/model_executor/layers/fused_moe/router/fused_topk_router.py
353	0	vllm/model_executor/layers/fused_moe/router/grouped_topk_router.py
178	0	vllm/model_executor/layers/fused_moe/router/router_factory.py
46	9	vllm/model_executor/layers/fused_moe/{routing_simulator.py => router/routing_simulator_router.py}
9	7	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
1	1	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/experts_int8.py
1	1	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/gguf.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	2	vllm/model_executor/layers/quantization/ipex_quant.py
1	1	vllm/model_executor/layers/quantization/modelopt.py
1	1	vllm/model_executor/layers/quantization/moe_wna16.py
4	4	vllm/model_executor/layers/quantization/mxfp4.py
1	1	vllm/model_executor/layers/quantization/quark/quark_moe.py
1	1	vllm/model_executor/layers/quantization/rtn.py
1	0	vllm/model_executor/models/ernie45_moe.py
2	0	vllm/model_executor/models/ernie45_vl_moe.py

[2f03035a6] tjp_zju 2026-01-18 "refactor: refactor_repeated_interfaces" (#32486)
5	8	vllm/_custom_ops.py
1	1	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
0	4	vllm/model_executor/layers/quantization/fp_quant.py
4	7	vllm/model_executor/layers/quantization/qutlass_utils.py
1	5	vllm/model_executor/model_loader/bitsandbytes_loader.py
3	8	vllm/model_executor/models/aya_vision.py
3	16	vllm/model_executor/models/llava.py
3	16	vllm/model_executor/models/mistral3.py
8	8	vllm/model_executor/models/tarsier.py
13	0	vllm/model_executor/models/utils.py
2	4	vllm/model_executor/warmup/deep_gemm_warmup.py

[38bf2ffb2] Isotr0py 2026-01-18 [Bugfix] Fix GLM-ASR audio encoder RoPE dim (#32540)
28	28	examples/offline_inference/audio_language.py
12	2	vllm/model_executor/models/glmasr.py

[c826c72a9] Li Xie 2026-01-18 [Model] Support Step1 Model (#32511)
2	1	docs/models/supported_models.md
3	0	tests/models/registry.py
4	1	tests/models/test_initialization.py
11	1	vllm/attention/layer.py
1	0	vllm/model_executor/models/registry.py
415	0	vllm/model_executor/models/step1.py
4	0	vllm/v1/attention/backend.py
7	1	vllm/v1/attention/backends/triton_attn.py
25	2	vllm/v1/attention/ops/triton_unified_attention.py

[fe36bf5e8] Canlin Guo 2026-01-18 [Model] Remove the unnecessary dtype conversion in MiniCPM (#32523)
0	3	vllm/model_executor/models/minicpm.py

[963dc0b86] Woosuk Kwon 2026-01-17 [Model Runner V2] Minor optimization for eagle input processing (#32535)
2	8	vllm/v1/worker/gpu/model_runner.py
10	6	vllm/v1/worker/gpu/spec_decode/eagle.py

[8cc26acd8] Isotr0py 2026-01-18 [Performance] Improve Triton prefill attention kernel's performance  (#32403)
2	2	tests/models/language/pooling/test_token_classification.py
4	0	vllm/utils/math_utils.py
27	46	vllm/v1/attention/ops/triton_prefill_attention.py

[4a6af8813] Robert Shaw 2026-01-17 [MoE Refactor] Move Test Impl into Test Dirs (#32129)
0	1	docs/design/moe_kernel_features.md
58	3	tests/kernels/moe/test_moe.py
0	60	vllm/model_executor/layers/fused_moe/moe_torch_iterative.py

[4147910f1] Woosuk Kwon 2026-01-17 [Model Runner V2] Move mrope_positions buffer to MRopeState (#32532)
7	4	vllm/v1/worker/gpu/cudagraph_utils.py
2	17	vllm/v1/worker/gpu/input_batch.py
17	3	vllm/v1/worker/gpu/mm/mrope_utils.py
23	10	vllm/v1/worker/gpu/model_runner.py

[3055232ba] Karan Bansal 2026-01-18 [Feature] Add FIPS 140-3 compliant hash algorithm option for multimodal hashing (#32386)
12	0	vllm/envs.py
34	3	vllm/multimodal/hasher.py

[965765aef] Shengqi Chen 2026-01-18 [build] fix cu130 related release pipeline steps and publish as nightly image (#32522)
23	14	.buildkite/release-pipeline.yaml
9	2	.buildkite/scripts/cleanup-nightly-builds.sh
36	0	.buildkite/scripts/push-nightly-builds.sh

[9e078d058] Mritunjay Kumar Sharma 2026-01-17 [CI/Build][Docker] Add centralized version manifest for Docker builds (#31492)
7	0	.pre-commit-config.yaml
33	6	docker/Dockerfile
92	0	docker/versions.json
139	0	tools/generate_versions_json.py

[2b99f210f] Guofang.Tang 2026-01-17 [Misc] Fix typo: seperator -> separator in flashmla_sparse.py (#32411)
7	7	vllm/v1/attention/backends/mla/flashmla_sparse.py

[1646fea67] Kim Hee Su 2026-01-17 [Model] Molmo2: Enable quantized weight mapping for vision backbone (#32385)
23	8	vllm/model_executor/models/molmo2.py

[d3317bbba] Paul Pak 2026-01-16 [Models] Lfm2Moe: minor name changes for resolving lora conflicts (#29063)
6	2	vllm/model_executor/models/lfm2.py
6	2	vllm/model_executor/models/lfm2_moe.py

[8e61425ee] Shengqi Chen 2026-01-17 [CI] Implement uploading to PyPI and GitHub in the release pipeline, enable release image building for CUDA 13.0 (#31032)
81	23	.buildkite/release-pipeline.yaml
0	0	.buildkite/scripts/{upload-wheels.sh => upload-nightly-wheels.sh}
103	0	.buildkite/scripts/upload-release-wheels.sh

[2e7c89e70] Matthew Bonanni 2026-01-16 Revert "[Attention][MLA] Make `FLASHINFER_MLA` the default MLA backen… (#32484)
1	1	vllm/config/attention.py
10	11	vllm/model_executor/layers/attention/mla_attention.py
4	4	vllm/platforms/cuda.py

[037a6487a] vanshil shah 2026-01-16 apply _validate_input to MistralTokenizer token-id chat prompts (#32448)
95	0	tests/entrypoints/openai/test_serving_chat.py
5	3	vllm/entrypoints/openai/engine/serving.py

[5a3050a08] Simon Mo 2026-01-16 [Docs][Governance] Add @robertshaw2-redhat to lead maintainers group (#32498)
1	1	docs/governance/process.md

[484e22bc1] Chenyaaang 2026-01-16 [TPU][Core] Enable Pipeline Parallelism on TPU backend (#28506)
30	13	vllm/v1/executor/multiproc_executor.py
4	1	vllm/v1/executor/ray_utils.py

[ca2128808] Lucas Wilkinson 2026-01-16 [CI] Fix OOM in Hopper Fusion E2E Tests (H100) (#32489)
2	1	.buildkite/test-pipeline.yaml

[4c82b6fac] Andrew Xia 2026-01-16 [responsesAPI] allow tuning include_stop_str_in_output (#32383)
2	0	vllm/entrypoints/openai/responses/protocol.py

[a884bc62d] Xin Yang 2026-01-16 [LoRA] Update LoRA expand kernel heuristic (#32425)
1	1	vllm/lora/ops/triton_ops/utils.py

[7a1030431] Hashem Hashemi 2026-01-16 Atomics Reduce Counting Optimization for SplitK Skinny GEMMs. (#29843)
4	0	csrc/rocm/ops.h
545	9	csrc/rocm/skinny_gemms.cu
6	0	csrc/rocm/torch_bindings.cpp
53	0	tests/kernels/quantization/test_rocm_skinny_gemms.py
6	0	vllm/_custom_ops.py
21	1	vllm/model_executor/layers/utils.py

[9fd918e51] Wentao Ye 2026-01-16 [CI] Update deepgemm to newer version (#32479)
1	1	tools/install_deepgemm.sh

[c9a533079] Ilya Markov 2026-01-16 [EPLB][BugFix]Possible deadlock fix (#32418)
17	2	vllm/distributed/eplb/eplb_state.py

[6ca4f400d] rasmith 2026-01-16 [CI][AMD] Skip test_permute_cols since the kernel is not used and not built for ROCm (#32444)
3	0	tests/kernels/core/test_permute_cols.py

[180e981d5] Cyrus Leung 2026-01-16 [Chore] Replace swish with silu (#32459)
9	45	vllm/model_executor/models/phi4mm_utils.py

[b84c426a8] Micah Williamson 2026-01-16 [ROCm][CI] Skip Qwen3-30B-A3B-MXFP4A16 Eval Test On Non-CUDA Platforms (#32460)
11	0	tests/evals/gsm8k/test_gsm8k_correctness.py

[b66b0d6ab] Rabi Mishra 2026-01-16 fix(rocm): Enable non-gated MoE (is_act_and_mul=False) support on ROCm (#32244)
8	4	vllm/model_executor/layers/fused_moe/layer.py
7	0	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[03da3b52e] Hongxin Xu 2026-01-16 [Bugfix] Refactor to support DP parallel in R3 (#32306)
33	21	vllm/model_executor/layers/fused_moe/routed_experts_capturer.py
1	2	vllm/v1/core/sched/scheduler.py
1	3	vllm/v1/worker/gpu_model_runner.py

[14ce52424] Lucas Wilkinson 2026-01-15 [CI] Breakup h200 tests (#30499)
53	4	.buildkite/test-pipeline.yaml
15	283	tests/compile/distributed/test_fusions_e2e.py
208	0	tests/compile/fusion_test_utils.py
113	8	tests/compile/test_fusion_attn.py
1	1	vllm/env_override.py

[4ae77dfd4] wang.yuqi 2026-01-16 [Frontend][1/n] Make pooling entrypoints request schema consensus | CompletionRequest  (#32395)
1	1	docs/serving/openai_compatible_server.md
67	0	examples/pooling/classify/classification_online.py
0	53	examples/pooling/classify/openai_classification_client.py
1	1	examples/pooling/pooling/vision_language_pooling.py
1	1	examples/pooling/score/vision_reranker_offline.py
86	49	tests/entrypoints/pooling/classify/test_online.py
233	221	tests/entrypoints/pooling/embed/test_online.py
49	57	tests/entrypoints/pooling/pooling/test_online.py
19	5	tests/entrypoints/pooling/score/test_online_rerank.py
5	5	tests/entrypoints/pooling/score/test_utils.py
1	1	tests/models/multimodal/pooling/test_jinavl_reranker.py
1	1	vllm/entrypoints/llm.py
4	58	vllm/entrypoints/openai/api_server.py
88	0	vllm/entrypoints/pooling/__init__.py
0	0	vllm/entrypoints/pooling/base/__init__.py
46	0	vllm/entrypoints/pooling/base/protocol.py
7	51	vllm/entrypoints/pooling/classify/protocol.py
9	50	vllm/entrypoints/pooling/embed/protocol.py
2	10	vllm/entrypoints/pooling/pooling/protocol.py
8	29	vllm/entrypoints/pooling/score/protocol.py
1	1	vllm/entrypoints/pooling/score/serving.py
0	0	vllm/entrypoints/{score_utils.py => pooling/score/utils.py}

[73f635a75] XiongfeiWei 2026-01-15 [Bug] Add TPU backend option (#32438)
4	0	vllm/model_executor/layers/fused_moe/oracle/unquantized.py

[35bf5d08e] cjackal 2026-01-16 [bugfix] Fix online serving crash when text type response_format is received (#26822)
19	0	tests/entrypoints/openai/test_chat.py
14	7	vllm/entrypoints/openai/chat_completion/protocol.py
14	7	vllm/entrypoints/openai/completion/protocol.py
13	10	vllm/entrypoints/openai/responses/serving.py
5	0	vllm/sampling_params.py
3	2	vllm/tool_parsers/abstract_tool_parser.py

[5de6dd066] Kebe 2026-01-16 [Bugfix] [DeepSeek-V3.2] fix sparse_attn_indexer padding (#32175)
9	2	vllm/model_executor/models/deepseek_v2.py

[709502558] ltd0924 2026-01-16 [Model] Add Step3vl 10b (#32329)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py
549	0	vllm/model_executor/models/step_vl.py

[46f8a982b] Micah Williamson 2026-01-15 [ROCm][CI] Enable AITER Unified Attention On ROCm For gpt-oss Test (#32431)
15	2	tests/entrypoints/openai/test_serving_chat.py

[bcf2333cd] Matthew Bonanni 2026-01-15 [CI] Fix LM Eval Large Models (H100) (#32423)
3	1	vllm/model_executor/layers/quantization/input_quant_fp8.py

[83239ff19] Michael Goin 2026-01-15 Add thread_n=64 support to Marlin MoE (#32360)
1	1	csrc/moe/marlin_moe_wna16/generate_kernels.py
4	2	csrc/moe/marlin_moe_wna16/ops.cu
3	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[c277fbdf3] TomerBN-Nvidia 2026-01-16 [Feat] Support non-gated MoE with Marlin, NVFP4 CUTLASS, FP8, INT8, compressed-tensors (#32257)
1	1	tests/kernels/moe/test_cutlass_moe.py
80	0	tests/kernels/moe/test_moe.py
2	0	vllm/envs.py
26	18	vllm/model_executor/layers/fused_moe/cutlass_moe.py
16	24	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
3	22	vllm/model_executor/layers/fused_moe/layer.py
8	3	vllm/model_executor/layers/fused_moe/oracle/fp8.py
1	0	vllm/model_executor/layers/fused_moe/oracle/nvfp4.py
2	1	vllm/model_executor/layers/fused_moe/utils.py
0	2	vllm/model_executor/layers/quantization/awq_marlin.py
45	27	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	0	vllm/model_executor/layers/quantization/fp8.py
0	2	vllm/model_executor/layers/quantization/gptq_marlin.py
19	17	vllm/model_executor/layers/quantization/modelopt.py
16	0	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
1	8	vllm/model_executor/layers/quantization/utils/marlin_utils.py
5	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py

[aca5c5148] Wentao Ye 2026-01-15 [Refactor] Remove unused file (#32422)
0	28	vllm/entrypoints/pooling/embed/conftest.py

[31c29257c] Yongye Zhu 2026-01-15 [MoE Refactor][17/N] Apply Refactor to Bf16 (#31827)
5	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-BF16-triton.yaml
1	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt
7	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-fi-cutlass.yaml
6	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-BF16-triton.yaml
7	0	tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-fi-cutlass.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-BF16-triton.yaml
7	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-fi-cutlass.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-BF16-triton.yaml
4	0	tests/evals/gsm8k/configs/moe-refactor/config-b200.txt
2	0	tests/evals/gsm8k/configs/moe-refactor/config-h100.txt
161	0	vllm/model_executor/layers/fused_moe/oracle/unquantized.py
47	87	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[8c11001ba] Aleksandr Malyshev 2026-01-15 [ROCM] DSfp4 mla projection gemms weight dynamic quantization (#32238)
30	0	vllm/_aiter_ops.py
6	0	vllm/envs.py
47	80	vllm/model_executor/layers/attention/mla_attention.py
14	0	vllm/model_executor/layers/quantization/quark/utils.py

[bd292be0c] Richard Zou 2026-01-15 [BugFix] Python file source reading can fail on UnicodeDecodeError (#32416)
1	1	vllm/compilation/backends.py

[41c544f78] TJian 2026-01-16 [ROCm] [CI] [Release] Rocm wheel pipeline with sccache (#32264)
362	0	.buildkite/release-pipeline.yaml
74	0	.buildkite/scripts/annotate-rocm-release.sh
140	0	.buildkite/scripts/cache-rocm-base-wheels.sh
69	9	.buildkite/scripts/generate-nightly-index.py
151	0	.buildkite/scripts/upload-rocm-wheels.sh
150	4	docker/Dockerfile.rocm
104	2	docker/Dockerfile.rocm_base
2	0	requirements/rocm-test.txt
0	1	requirements/rocm.txt
221	0	tools/vllm-rocm/pin_rocm_dependencies.py

[1be5a7357] Michael Goin 2026-01-15 [UX] Use kv_offloading_backend=native by default (#32421)
18	1	tests/v1/kv_connector/unit/test_config.py
5	5	vllm/config/cache.py
4	6	vllm/config/vllm.py
1	3	vllm/engine/arg_utils.py

[c36ba69bd] Lucas Wilkinson 2026-01-15 [BugFix] Fix `assert x_s.shape[-1] == x_q.shape[-1] // group_shape[1]` in Blackwell Quantized MoE Test (#32362)
2	2	vllm/model_executor/layers/quantization/utils/quant_utils.py

[047413375] Matthias Gehre 2026-01-15 [Attention][AMD] Make flash-attn optional (#30361)
7	5	vllm/v1/attention/backends/fa_utils.py

[74e4bb1c5] smit kadvani 2026-01-15 fixing podman build issue (#32131)
2	0	docker/Dockerfile.rocm

[b34474bf2] Wentao Ye 2026-01-15 [Feature] Support async scheduling + PP (#32359)
3	0	tests/v1/core/utils.py
1	13	vllm/config/vllm.py
7	0	vllm/v1/core/sched/scheduler.py
3	3	vllm/v1/executor/multiproc_executor.py
2	3	vllm/v1/executor/ray_executor.py

[6218034dd] Woosuk Kwon 2026-01-15 [Model Runner V2] Support FlashInfer backend & Fix CUDA Graph bug [1/2] (#32348)
9	5	vllm/v1/worker/gpu/cudagraph_utils.py
8	2	vllm/v1/worker/gpu/model_runner.py

[77c16df31] Pleaplusone 2026-01-16 [ROCm][Bugfix] Disable hip sampler to fix deepseek's accuracy issue on ROCm (#32413)
4	0	vllm/v1/sample/ops/topk_topp_sampler.py

[130d6c951] Pleaplusone 2026-01-15 [ROCm][Perf] Enable shuffle kv cache layout and assembly paged attention kernel for `AiterFlashAttentionBackend` (#29887)
7	0	vllm/_aiter_ops.py
5	0	vllm/envs.py
299	84	vllm/v1/attention/backends/rocm_aiter_fa.py

[361dfdc9d] Dipika Sikka 2026-01-15 [Quant] Support MXFP4 W4A16 for compressed-tensors MoE models  (#32285)
5	0	tests/evals/gsm8k/configs/Qwen3-30B-A3B-MXFP4A16.yaml
1	0	tests/evals/gsm8k/configs/models-small.txt
11	0	vllm/model_executor/layers/fused_moe/oracle/nvfp4.py
139	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[8ebfacaa7] Matthew Bonanni 2026-01-15 [Attention][MLA] Make `FLASHINFER_MLA` the default MLA backend on Blackwell, and TRTLLM the default prefill (#32339)
1	1	vllm/config/attention.py
11	10	vllm/model_executor/layers/attention/mla_attention.py
4	4	vllm/platforms/cuda.py

[b89275d01] brian033 2026-01-15 [ROCm] Improve error handling while loading quantized model on gfx120… (#31715)
5	1	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py

[28459785f] Cyrus Leung 2026-01-15 [3/N] Group together media-related code (#32406)
1	1	tests/conftest.py
1	2	tests/entrypoints/openai/test_sparse_tensor_validation.py
1	1	tests/entrypoints/openai/test_vision.py
1	1	tests/entrypoints/pooling/embed/test_online_vision.py
0	0	tests/multimodal/media/__init__.py
73	0	tests/multimodal/media/test_audio.py
45	0	tests/multimodal/media/test_base.py
133	0	tests/multimodal/media/test_image.py
237	0	tests/multimodal/media/test_video.py
0	56	tests/multimodal/test_audio.py
1	153	tests/multimodal/test_image.py
3	220	tests/multimodal/test_video.py
1	1	tools/pre_commit/check_pickle_imports.py
0	77	vllm/multimodal/audio.py
1	1	vllm/multimodal/hasher.py
0	119	vllm/multimodal/image.py
1	1	vllm/multimodal/inputs.py
16	0	vllm/multimodal/media/__init__.py
89	0	vllm/multimodal/media/audio.py
7	6	vllm/multimodal/{ => media}/base.py
124	0	vllm/multimodal/media/image.py
89	0	vllm/multimodal/media/video.py
1	1	vllm/multimodal/parse.py
8	4	vllm/multimodal/utils.py
0	81	vllm/multimodal/video.py

[8853a50af] rasmith 2026-01-15 [CI][BugFix][AMD][FP8] Fix test_rms_norm so it runs correctly on ROCm (#32372)
7	5	tests/kernels/core/test_fused_quant_layernorm.py

[c5891b543] Douglas Lehr 2026-01-15 [ROCM] Add ROCm image build to release pipeline (#31995)
18	0	.buildkite/release-pipeline.yaml
7	0	.buildkite/scripts/annotate-release.sh
4	0	docker/Dockerfile.rocm

[707b44cc2] Chauncey 2026-01-15 [Refactor] [11/N] to simplify the mcp architecture (#32396)
1	1	tests/entrypoints/openai/responses/test_mcp_tools.py
1	1	tests/entrypoints/openai/test_gptoss_structural_tags_integration.py
2	2	tests/entrypoints/openai/test_serving_responses.py
3	3	tests/entrypoints/test_context.py
1	1	tests/entrypoints/test_responses_utils.py
1	1	tests/v1/structured_output/test_gptoss_structural_tags.py
2	0	vllm/entrypoints/mcp/__init__.py
4	4	vllm/entrypoints/{ => mcp}/tool.py
1	1	vllm/entrypoints/{ => mcp}/tool_server.py
1	1	vllm/entrypoints/openai/api_server.py
9	9	vllm/entrypoints/openai/engine/serving.py
3	3	vllm/entrypoints/{ => openai/responses}/context.py
9	9	vllm/entrypoints/openai/responses/serving.py
0	0	vllm/entrypoints/{responses_utils.py => openai/responses/utils.py}
1	1	vllm/reasoning/abs_reasoning_parsers.py
1	1	vllm/reasoning/gptoss_reasoning_parser.py

[3a4e10c84] rongfu.leng 2026-01-15 [Benchmark] [Feature] add vllm bench sweep startup command (#32337)
57	0	docs/benchmarking/sweeps.md
3	0	vllm/benchmarks/sweep/cli.py
405	0	vllm/benchmarks/sweep/startup.py

[cbbae38f9] Cyrus Leung 2026-01-15 [2/N] Move cache factories to MM registry (#32382)
4	6	tests/multimodal/test_cache.py
3	4	tests/v1/engine/test_process_multi_modal_uuids.py
10	10	vllm/lora/model_manager.py
0	109	vllm/multimodal/cache.py
93	3	vllm/multimodal/registry.py
2	3	vllm/v1/engine/core.py
1	2	vllm/v1/engine/input_processor.py
1	5	vllm/v1/worker/gpu_model_runner.py
5	7	vllm/v1/worker/utils.py
5	5	vllm/v1/worker/worker_base.py

[cdba4c74b] Cyrus Leung 2026-01-15 [Model] Avoid token selection in SigLIP pooling head (#32389)
3	3	vllm/model_executor/models/siglip.py

[a52d1396a] seeksky 2026-01-15 fix: avoid crash on zero-arg tool calls in glm4 parser (#32321)
7	1	vllm/tool_parsers/glm4_moe_tool_parser.py

[1e584823f] dtc 2026-01-15 [Bugfix] Strengthen the check of X-data-parallel-rank in Hybrid LB mode (#32314)
1	0	tests/v1/engine/test_engine_core_client.py
8	0	vllm/config/parallel.py
2	4	vllm/entrypoints/cli/serve.py
1	3	vllm/v1/engine/coordinator.py
1	6	vllm/v1/engine/core_client.py
6	5	vllm/v1/engine/input_processor.py
1	4	vllm/v1/engine/utils.py

[4c1c501a7] Chauncey 2026-01-15 [Refactor] [10/N] to simplify the vLLM openai completion serving architecture (#32369)
2	1	tests/entrypoints/openai/test_chat_error.py
1	1	tests/entrypoints/openai/test_cli_args.py
5	3	tests/entrypoints/openai/test_completion_error.py
1	3	tests/entrypoints/openai/test_gptoss_structural_tags_integration.py
5	3	tests/entrypoints/openai/test_lora_resolvers.py
1	1	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/entrypoints/openai/test_serving_engine.py
2	1	tests/entrypoints/openai/test_serving_models.py
2	1	tests/v1/engine/test_async_llm.py
2	0	vllm/entrypoints/anthropic/__init__.py
92	0	vllm/entrypoints/anthropic/api_router.py
1	1	vllm/entrypoints/anthropic/{serving_messages.py => serving.py}
24	141	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/chat_completion/serving.py
1	1	vllm/entrypoints/openai/cli_args.py
2	0	vllm/entrypoints/openai/completion/__init__.py
76	0	vllm/entrypoints/openai/completion/api_router.py
463	0	vllm/entrypoints/openai/completion/protocol.py
5	3	vllm/entrypoints/openai/{serving_completion.py => completion/serving.py}
1	430	vllm/entrypoints/openai/engine/protocol.py
5	3	vllm/entrypoints/openai/engine/serving.py
0	0	vllm/entrypoints/openai/models/__init__.py
29	0	vllm/entrypoints/openai/models/api_router.py
18	0	vllm/entrypoints/openai/models/protocol.py
1	14	vllm/entrypoints/openai/{serving_models.py => models/serving.py}
2	2	vllm/entrypoints/openai/responses/serving.py
2	1	vllm/entrypoints/openai/run_batch.py
3	8	vllm/entrypoints/openai/translations/api_router.py
1	1	vllm/entrypoints/openai/translations/serving.py
1	1	vllm/entrypoints/openai/translations/speech_to_text.py
1	1	vllm/entrypoints/pooling/classify/serving.py
1	1	vllm/entrypoints/pooling/embed/serving.py
1	1	vllm/entrypoints/pooling/pooling/serving.py
1	1	vllm/entrypoints/pooling/score/serving.py
9	5	vllm/entrypoints/sagemaker/routes.py
1	1	vllm/entrypoints/serve/disagg/api_router.py
1	1	vllm/entrypoints/serve/disagg/protocol.py
1	1	vllm/entrypoints/serve/disagg/serving.py
1	1	vllm/entrypoints/serve/elastic_ep/api_router.py
3	2	vllm/entrypoints/serve/lora/api_router.py
1	1	vllm/entrypoints/serve/tokenize/api_router.py
1	1	vllm/entrypoints/serve/tokenize/serving.py
5	3	vllm/entrypoints/utils.py

[ae1eba6a9] Andreas Karatzas 2026-01-15 [ROCm][CI] Pin transformers 4.57.3 to fix jina test failures (#32350)
3	1	requirements/rocm-test.txt

[e9ec2a72d] Ofir Zafrir 2026-01-15 [Bugfix] Fix stale `common_attn_metadata.max_seq_len` in speculative decoding with Eagle (#32312)
6	0	vllm/v1/spec_decode/eagle.py

[2c9b4cf5b] Lucas Wilkinson 2026-01-14 [BugFix] Fix DeepSeek-V3.1 + DeepGEMM incompatible scale shapes (#32361)
3	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[9d7ae3fcd] Ning Xie 2026-01-15 [code clean] remove duplicate check (#32376)
1	1	vllm/multimodal/registry.py
1	4	vllm/v1/engine/async_llm.py
1	4	vllm/v1/engine/llm_engine.py

[3c2685645] rasmith 2026-01-14 [CI][AMD][Quantization][BugFix] Fix fp8 max in quant_utils.py and update test_fp8_quant.::test_static_fp8_quant_group_2d to use correct fp8 dtype and adjust atol/rtol (#32201)
2	2	tests/kernels/quantization/test_fp8_quant.py
2	1	vllm/model_executor/layers/quantization/utils/quant_utils.py

[773d7073a] Micah Williamson 2026-01-14 [ROCm][CI] Disable async scheduling on ROCm for test_structured_output[meta-llama/Meta-Llama-3.1-8B-Instruct-xgrammar-auto-speculative_config9] (#32355)
5	0	tests/v1/entrypoints/llm/test_struct_output_generate.py

[edadca109] kzwrime 2026-01-15 [Bugfix] Add CpuCommunicator.dispatch and combine to fix DP+MoE inference (#31867)
4	1	vllm/distributed/device_communicators/base_device_communicator.py
41	0	vllm/distributed/device_communicators/cpu_communicator.py

[d86fc23bd] Li Wang 2026-01-15 [Misc] Remove redundant line (#32366)
0	1	vllm/attention/layer.py

[375e5984f] Shiyan Deng 2026-01-14 Support configure skip_special_tokens in openai response api (#32345)
2	0	vllm/entrypoints/openai/responses/protocol.py

[19b251fe3] baonudesifeizhai 2026-01-14 Fix optional parameter parsing in MiniMax M2 tool parser #32278 (#32342)
2	5	vllm/tool_parsers/minimax_m2_tool_parser.py

[15422ed3f] Ryan Rock 2026-01-14 [CI/Build][Hardware][AMD] Fix v1/shutdown (#31997)
26	0	tests/v1/shutdown/conftest.py
18	2	tests/v1/shutdown/test_forward_error.py
20	1	tests/v1/shutdown/test_startup_error.py

[8471b27df] dolpm 2026-01-14 [compile] raise on compile_size implicit padding (#32343)
65	0	tests/compile/test_config.py
14	0	vllm/config/compilation.py

[66652e808] Lumosis 2026-01-14 [BugFix] Assign page_size_padded when unifying kv cache spec. (#32283)
97	1	tests/v1/core/test_kv_cache_utils.py
2	0	vllm/v1/core/kv_cache_utils.py

[e27078ea8] vllmellm 2026-01-15 [Bugfix][ROCm][performance] Resolve the performance regression issue of the Qwen3-Next-80B-A3B-Thinking under rocm_atten (#32336)
10	1	vllm/v1/attention/backends/rocm_attn.py

[d084e9fca] Aleksandr Samarin 2026-01-14 [MODEL] Fix handling of multiple channels for gpt-oss with speculative decoding  (#26291)
335	275	tests/entrypoints/openai/test_serving_chat.py
171	27	tests/entrypoints/openai/test_serving_chat_stream_harmony.py
26	11	vllm/entrypoints/openai/chat_completion/serving.py
118	48	vllm/entrypoints/openai/chat_completion/stream_harmony.py

[3a612322e] qli88 2026-01-14 [CI] Move rixl/ucx from Dockerfile.rocm_base to Dockerfile.rocm (#32295)
76	0	docker/Dockerfile.rocm
0	111	docker/Dockerfile.rocm_base

[9ea07b41d] Cyrus Leung 2026-01-14 [1/N] Reorganize multimodal processing code (#32327)
0	4	docs/api/README.md
4	6	docs/contributing/model/multimodal.md
1	1	docs/design/mm_processing.md
2	2	tests/multimodal/test_processing.py
1	1	vllm/benchmarks/mm_processor.py
1	1	vllm/model_executor/models/aria.py
1	1	vllm/model_executor/models/audioflamingo3.py
6	3	vllm/model_executor/models/aya_vision.py
1	1	vllm/model_executor/models/bagel.py
1	1	vllm/model_executor/models/blip2.py
1	1	vllm/model_executor/models/chameleon.py
1	1	vllm/model_executor/models/clip.py
6	3	vllm/model_executor/models/cohere2_vision.py
1	1	vllm/model_executor/models/deepseek_ocr.py
2	2	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/ernie45_vl.py
1	1	vllm/model_executor/models/fuyu.py
2	2	vllm/model_executor/models/gemma3_mm.py
2	2	vllm/model_executor/models/gemma3n_mm.py
1	1	vllm/model_executor/models/glm4_1v.py
1	1	vllm/model_executor/models/glm4v.py
1	1	vllm/model_executor/models/glmasr.py
1	1	vllm/model_executor/models/granite_speech.py
1	1	vllm/model_executor/models/h2ovl.py
1	1	vllm/model_executor/models/hunyuan_vision.py
1	1	vllm/model_executor/models/hyperclovax_vision.py
2	3	vllm/model_executor/models/idefics3.py
1	1	vllm/model_executor/models/interns1.py
1	1	vllm/model_executor/models/internvl.py
1	1	vllm/model_executor/models/isaac.py
1	1	vllm/model_executor/models/kanana_v.py
1	1	vllm/model_executor/models/keye.py
1	1	vllm/model_executor/models/kimi_vl.py
1	1	vllm/model_executor/models/lfm2_vl.py
1	1	vllm/model_executor/models/lightonocr.py
1	1	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/llava_next_video.py
1	1	vllm/model_executor/models/midashenglm.py
2	2	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/mistral3.py
1	1	vllm/model_executor/models/mllama4.py
1	1	vllm/model_executor/models/molmo.py
1	1	vllm/model_executor/models/molmo2.py
2	2	vllm/model_executor/models/nano_nemotron_vl.py
3	3	vllm/model_executor/models/nemotron_parse.py
1	1	vllm/model_executor/models/ovis.py
1	1	vllm/model_executor/models/ovis2_5.py
1	1	vllm/model_executor/models/paddleocr_vl.py
1	1	vllm/model_executor/models/paligemma.py
2	2	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/phi4mm.py
2	2	vllm/model_executor/models/pixtral.py
2	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	1	vllm/model_executor/models/qwen2_audio.py
1	1	vllm/model_executor/models/qwen2_vl.py
1	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py
1	1	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/qwen_vl.py
1	1	vllm/model_executor/models/siglip.py
1	1	vllm/model_executor/models/skyworkr1v.py
1	1	vllm/model_executor/models/step3_vl.py
1	1	vllm/model_executor/models/tarsier.py
1	1	vllm/model_executor/models/terratorch.py
5	2	vllm/model_executor/models/transformers/multimodal.py
1	1	vllm/model_executor/models/ultravox.py
2	2	vllm/model_executor/models/voxtral.py
2	2	vllm/model_executor/models/voxtral_streaming.py
1	1	vllm/model_executor/models/whisper.py
1	1	vllm/multimodal/cache.py
9	5	vllm/multimodal/inputs.py
27	0	vllm/multimodal/processing/__init__.py
558	0	vllm/multimodal/processing/context.py
4	8	vllm/multimodal/{profiling.py => processing/dummy_inputs.py}
16	556	vllm/multimodal/{processing.py => processing/processor.py}
2	2	vllm/multimodal/registry.py
1	1	vllm/v1/engine/input_processor.py

[552b26293] Ning Xie 2026-01-14 rename tokenize serving api request id prefix to tokenize (#32328)
2	2	vllm/entrypoints/serve/tokenize/protocol.py
2	2	vllm/entrypoints/serve/tokenize/serving.py

[00e6402d5] Chauncey 2026-01-14 [Frontend] track responsesAPI server_load (#32323)
4	0	vllm/entrypoints/openai/api_server.py
4	0	vllm/entrypoints/openai/responses/api_router.py

[ce0946249] Shanshan Shen 2026-01-14 [Misc] Make mem utils can be reused by other platforms (#32322)
12	8	vllm/platforms/interface.py
9	13	vllm/utils/mem_utils.py
0	4	vllm/v1/worker/gpu_worker.py

[3f28174c6] Cyrus Leung 2026-01-14 [Frontend] Standardize use of `create_error_response` (#32319)
1	7	vllm/entrypoints/openai/api_server.py
4	4	vllm/entrypoints/openai/chat_completion/api_router.py
8	3	vllm/entrypoints/openai/engine/serving.py
4	10	vllm/entrypoints/openai/responses/api_router.py
6	1	vllm/entrypoints/openai/serving_models.py
3	5	vllm/entrypoints/pooling/classify/api_router.py
2	4	vllm/entrypoints/pooling/embed/api_router.py
3	4	vllm/entrypoints/pooling/pooling/api_router.py
5	7	vllm/entrypoints/pooling/score/api_router.py
2	3	vllm/entrypoints/serve/disagg/api_router.py
1	7	vllm/entrypoints/serve/tokenize/api_router.py
23	12	vllm/entrypoints/utils.py

[769d0629e] Chauncey 2026-01-14 [Refactor] [9/N] to simplify the vLLM openai translations  serving ar chitecture (#32313)
8	101	vllm/entrypoints/openai/api_server.py
0	518	vllm/entrypoints/openai/engine/protocol.py
5	3	vllm/entrypoints/openai/engine/serving.py
2	0	vllm/entrypoints/openai/translations/__init__.py
122	0	vllm/entrypoints/openai/translations/api_router.py
545	0	vllm/entrypoints/openai/translations/protocol.py
4	2	vllm/entrypoints/openai/{serving_transcription.py => translations/serving.py}
6	4	vllm/entrypoints/openai/{ => translations}/speech_to_text.py

[90db5b31e] Cyrus Leung 2026-01-14 [Refactor] Move top-level dummy data generation to registry (#32310)
5	18	tests/models/multimodal/processing/test_mllama4.py
3	3	tests/multimodal/test_processing.py
1	1	vllm/multimodal/processing.py
9	77	vllm/multimodal/profiling.py
31	27	vllm/multimodal/registry.py
8	6	vllm/v1/worker/gpu_model_runner.py

[b8199f604] Roger Wang 2026-01-13 [Model] Re-implement Qwen3Omni Audio Encoder (#32167)
428	29	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[7e6f12381] sangho.lee 2026-01-13 Add Molmo2 multimodal model support (#30997)
1	0	docs/models/supported_models.md
32	0	examples/offline_inference/vision_language.py
38	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
13	0	tests/models/registry.py
1	0	tests/models/test_initialization.py
1	0	vllm/config/model.py
2793	0	vllm/model_executor/models/molmo2.py
1	0	vllm/model_executor/models/registry.py
15	0	vllm/multimodal/processing.py
319	1	vllm/multimodal/video.py

[9312a6c03] Chauncey 2026-01-14 [Refactor] [8/N] to simplify the vLLM openai responsesapi_serving architecture (#32260)
2	2	tests/entrypoints/openai/responses/test_function_call_parsing.py
1	1	tests/entrypoints/openai/test_protocol.py
3	2	tests/entrypoints/openai/test_serving_responses.py
5	3	vllm/entrypoints/context.py
8	111	vllm/entrypoints/openai/api_server.py
1	531	vllm/entrypoints/openai/engine/protocol.py
4	2	vllm/entrypoints/openai/engine/serving.py
1	1	vllm/entrypoints/openai/parser/harmony_utils.py
1	1	vllm/entrypoints/openai/parser/responses_parser.py
2	0	vllm/entrypoints/openai/responses/__init__.py
143	0	vllm/entrypoints/openai/responses/api_router.py
554	0	vllm/entrypoints/openai/responses/protocol.py
14	13	vllm/entrypoints/openai/{serving_responses.py => responses/serving.py}
2	4	vllm/entrypoints/responses_utils.py
2	0	vllm/reasoning/abs_reasoning_parsers.py
1	1	vllm/reasoning/basic_parsers.py
2	0	vllm/reasoning/minimax_m2_reasoning_parser.py
1	1	vllm/reasoning/mistral_reasoning_parser.py
2	0	vllm/reasoning/olmo3_reasoning_parser.py
3	1	vllm/reasoning/qwen3_reasoning_parser.py
2	0	vllm/tool_parsers/abstract_tool_parser.py

[6388b5005] Michael Goin 2026-01-14 [Docs] Add docs about OOT Quantization Plugins (#32035)
157	0	docs/features/quantization/README.md
1	0	vllm/model_executor/layers/quantization/__init__.py

[048bb5972] Hongxia Yang 2026-01-14 AMD CI Test - unskip moe_sum test and moe_align_block_size tests (#32039)
0	1	tests/kernels/moe/test_moe.py
0	3	tests/kernels/moe/test_moe_align_block_size.py
11	0	vllm/platforms/rocm.py

[793363805] Angela Yi 2026-01-13 [misc] Remove is_torch_equal_or_newer(2.4) cases (#32296)
1	2	vllm/compilation/decorators.py
23	25	vllm/distributed/parallel_state.py
0	25	vllm/utils/torch_utils.py
0	2	vllm/v1/worker/gpu_model_runner.py

[6b176095e] David 2026-01-14 [Build] Relax anthropic version pin from ==0.71.0 to >=0.71.0 (#32289)
1	1	requirements/common.txt

[9d0d7f48d] Andreas Karatzas 2026-01-14 [ROCm][CI] Handle missing vision_config in Isaac model attention patch (#32281)
23	1	tests/models/multimodal/generation/vlm_utils/model_utils.py

[50632adc5] Yi Liu 2026-01-14 Consolidate Intel Quantization Toolkit Integration in vLLM (#31716)
14	15	docs/features/quantization/README.md
0	103	docs/features/quantization/auto_round.md
72	33	docs/features/quantization/inc.md
1	3	tests/quantization/test_auto_round.py
1	0	vllm/config/model.py
0	5	vllm/engine/arg_utils.py
1	4	vllm/model_executor/layers/quantization/__init__.py
0	454	vllm/model_executor/layers/quantization/auto_round.py
440	41	vllm/model_executor/layers/quantization/inc.py
1	1	vllm/model_executor/model_loader/weight_utils.py

[6fa6e7ef0] Micah Williamson 2026-01-13 [ROCm][CI] Disable Async Scheduling For Qwen3-Next-80B-A3B-Instruct MTP Async EPLB Accuracy Test (#32275)
5	1	.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh

[90c083690] Woosuk Kwon 2026-01-13 [Model Runner V2] Refactor Sampler (#32245)
36	25	vllm/v1/worker/gpu/model_runner.py
0	79	vllm/v1/worker/gpu/sample/metadata.py
103	11	vllm/v1/worker/gpu/sample/penalties.py
70	22	vllm/v1/worker/gpu/sample/sampler.py
74	0	vllm/v1/worker/gpu/sample/states.py
6	4	vllm/v1/worker/gpu/spec_decode/eagle.py
0	128	vllm/v1/worker/gpu/states.py

[8ef50d9a6] Roberto L. Castro 2026-01-14 [Kernel][Performance] Enable smaller Scaling Factor tiling for NVFP4 small-batch decoding (#30885)
3	1	.buildkite/test-pipeline.yaml
24	2	tests/kernels/quantization/nvfp4_utils.py
26	8	tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py
26	0	tests/models/quantization/test_nvfp4.py
30	16	vllm/_custom_ops.py
6	1	vllm/envs.py
7	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
1	1	vllm/model_executor/layers/quantization/modelopt.py
54	1	vllm/utils/flashinfer.py

[2a60ac91d] emricksini-h 2026-01-13 [Improvement] Persist CUDA compat libraries paths to prevent reset on `apt-get` (#30784)
4	4	docker/Dockerfile

[9e65bb4ef] Michael Goin 2026-01-13 Add mergify label job for "bug" in PR titles (#31980)
12	0	.github/mergify.yml

[0db574b18] Simon Mo 2026-01-13 [Build] Add scripts for cherry-picking and trigger build (#32282)
242	0	.buildkite/scripts/cherry-pick-from-milestone.sh
227	0	.buildkite/scripts/trigger-ci-build.sh

[2f4a71daf] HappyAmazonian 2026-01-13 [Misc] Add In-Container restart capability through supervisord for sagemaker entrypoint (#28502)
1	1	examples/online_serving/sagemaker-entrypoint.sh
1	1	requirements/common.txt

[69f8a0ea3] Rabi Mishra 2026-01-14 fix(rocm): Use refresh_env_variables() for rocm_aiter_ops in test_moe (#31711)
6	10	tests/kernels/moe/test_moe.py

[f28125d87] Wentao Ye 2026-01-13 [Perf] Optimize grouped topk kernel, 1.2%~2% E2E Throughput improvement (#32058)
181	283	csrc/moe/grouped_topk_kernels.cu
5	0	tests/models/utils.py

[46f8c6b72] Dmitry Tokarev 2026-01-13 Fix CUDA 13 wheel installation doc (#32276)
2	1	docs/getting_started/installation/gpu.cuda.inc.md

[af54d2e2d] Andrew Xia 2026-01-13 [responseAPI] support partial message generation (#32100)
280	0	tests/entrypoints/test_responses_utils.py
10	0	vllm/entrypoints/openai/serving_responses.py
47	0	vllm/entrypoints/responses_utils.py

[6beef12b9] Sage Moore 2026-01-13 [EPLB][Cleanup] Remove `is_async_enabled` from `EplbModelState` (#32050)
4	4	vllm/distributed/eplb/async_worker.py
4	16	vllm/distributed/eplb/eplb_state.py

[ab74b2a27] Mark McLoughlin 2026-01-13 [Trivial] Remove duplicate enable_mfu_metrics (#32246)
0	3	vllm/config/observability.py

[2263d44b6] Matthew Bonanni 2026-01-13 [4/N][Attention] Move MLA common to model_executor (#32060)
1	1	tests/v1/attention/test_mla_backends.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
0	2	vllm/logging_utils/formatter.py
0	0	vllm/{v1/attention/backends/mla/common.py => model_executor/layers/attention/mla_attention.py}
6	6	vllm/v1/attention/backends/mla/cutlass_mla.py
8	8	vllm/v1/attention/backends/mla/flashattn_mla.py
7	7	vllm/v1/attention/backends/mla/flashinfer_mla.py
8	8	vllm/v1/attention/backends/mla/flashmla.py
4	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
2	2	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
4	1	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
5	5	vllm/v1/attention/backends/mla/triton_mla.py
3	1	vllm/v1/spec_decode/eagle.py

[4f3676e72] Mathis Felardos 2026-01-13 nixl_connector: export UCX_MEM_MMAP_HOOK_MODE=none to avoid a UCX memory leak (#32181)
17	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[510265472] Martin Hickey 2026-01-13 [BugFix] [KVConnector] Fix KV events for LMCache connector (#32169)
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py

[4f02cb2ea] Chauncey 2026-01-13 [Refactor] [7/N] to simplify the vLLM lora serving architecture (#32251)
3	1	tests/entrypoints/openai/test_serving_models.py
0	10	vllm/entrypoints/openai/engine/protocol.py
3	1	vllm/entrypoints/openai/serving_models.py
3	1	vllm/entrypoints/serve/lora/api_router.py
14	0	vllm/entrypoints/serve/lora/protocol.py

[252c01101] Cyrus Leung 2026-01-13 [Refactor] Remove `MultiModalProfiler` (#32254)
7	11	tests/models/multimodal/processing/test_mllama4.py
2	4	tests/multimodal/test_processing.py
14	68	vllm/multimodal/profiling.py
33	17	vllm/multimodal/registry.py

[98f60e5ac] Matthew Bonanni 2026-01-13 [6/N][Attention] Move utils to more appropriate locations (#32215)
4	2	tests/v1/attention/test_attention_splitting.py
1	1	vllm/model_executor/layers/attention/chunked_local_attention.py
0	2	vllm/model_executor/layers/attention/cross_attention.py
0	2	vllm/model_executor/layers/attention/encoder_only_attention.py
0	2	vllm/model_executor/layers/attention/static_sink_attention.py
1	3	vllm/model_executor/models/whisper_utils.py
25	1	vllm/v1/attention/backend.py
1	159	vllm/v1/attention/backends/utils.py
1	1	vllm/v1/worker/gpu/cudagraph_utils.py
1	1	vllm/v1/worker/gpu/spec_decode/eagle.py
1	1	vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py
1	1	vllm/v1/worker/gpu_model_runner.py
131	0	vllm/v1/worker/ubatch_utils.py
1	2	vllm/v1/worker/utils.py

[fefce4980] Chauncey 2026-01-13 [Refactor] [6/N] to simplify the vLLM openai chat_completion serving architecture (#32240)
2	2	tests/entrypoints/openai/responses/test_errors.py
2	2	tests/entrypoints/openai/responses/test_function_call_parsing.py
3	2	tests/entrypoints/openai/test_chat_error.py
1	1	tests/entrypoints/openai/test_chat_template.py
1	1	tests/entrypoints/openai/test_completion_error.py
1	1	tests/entrypoints/openai/test_gptoss_structural_tags_integration.py
1	1	tests/entrypoints/openai/test_lora_resolvers.py
4	1	tests/entrypoints/openai/test_protocol.py
13	11	tests/entrypoints/openai/test_serving_chat.py
2	2	tests/entrypoints/openai/test_serving_chat_stream_harmony.py
1	1	tests/entrypoints/openai/test_serving_engine.py
1	1	tests/entrypoints/openai/test_serving_models.py
1	1	tests/entrypoints/openai/test_serving_responses.py
1	1	tests/entrypoints/openai/tool_parsers/test_gigachat3_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_hunyuan_a13b_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
2	2	tests/entrypoints/openai/tool_parsers/utils.py
3	1	tests/entrypoints/openai/utils.py
1	1	tests/reasoning/test_base_thinking_reasoning_parser.py
2	1	tests/reasoning/test_deepseekv3_reasoning_parser.py
2	2	tests/reasoning/utils.py
2	2	tests/tool_parsers/test_ernie45_moe_tool_parser.py
1	1	tests/tool_parsers/test_functiongemma_tool_parser.py
1	1	tests/tool_parsers/test_glm4_moe_tool_parser.py
1	1	tests/tool_parsers/test_jamba_tool_parser.py
1	1	tests/tool_parsers/test_kimi_k2_tool_parser.py
3	1	tests/tool_parsers/test_minimax_tool_parser.py
1	1	tests/tool_parsers/test_mistral_tool_parser.py
1	1	tests/tool_parsers/test_openai_tool_parser.py
3	1	tests/tool_parsers/test_qwen3coder_tool_parser.py
3	1	tests/tool_parsers/test_seed_oss_tool_parser.py
2	2	tests/tool_parsers/test_xlam_tool_parser.py
1	1	tests/tool_use/test_chat_completion_request_validations.py
2	2	tests/tool_use/test_tool_choice_required.py
4	2	tests/v1/engine/test_async_llm.py
4	2	vllm/entrypoints/anthropic/serving_messages.py
6	6	vllm/entrypoints/context.py
11	48	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/chat_completion/__init__.py
77	0	vllm/entrypoints/openai/chat_completion/api_router.py
654	0	vllm/entrypoints/openai/chat_completion/protocol.py
16	14	vllm/entrypoints/openai/{serving_chat.py => chat_completion/serving.py}
1	1	vllm/entrypoints/openai/{serving_chat_stream_harmony.py => chat_completion/stream_harmony.py}
2	0	vllm/entrypoints/openai/engine/__init__.py
89	847	vllm/entrypoints/openai/{ => engine}/protocol.py
9	5	vllm/entrypoints/openai/{serving_engine.py => engine/serving.py}
2	2	vllm/entrypoints/openai/parser/harmony_utils.py
4	1	vllm/entrypoints/openai/parser/responses_parser.py
4	2	vllm/entrypoints/openai/run_batch.py
2	2	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_models.py
14	14	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/entrypoints/openai/serving_transcription.py
2	2	vllm/entrypoints/openai/speech_to_text.py
1	1	vllm/entrypoints/openai/utils.py
1	1	vllm/entrypoints/pooling/classify/api_router.py
1	1	vllm/entrypoints/pooling/classify/protocol.py
4	2	vllm/entrypoints/pooling/classify/serving.py
1	1	vllm/entrypoints/pooling/embed/api_router.py
1	1	vllm/entrypoints/pooling/embed/protocol.py
2	2	vllm/entrypoints/pooling/embed/serving.py
1	1	vllm/entrypoints/pooling/pooling/api_router.py
1	1	vllm/entrypoints/pooling/pooling/protocol.py
2	2	vllm/entrypoints/pooling/pooling/serving.py
1	1	vllm/entrypoints/pooling/score/api_router.py
1	1	vllm/entrypoints/pooling/score/protocol.py
2	2	vllm/entrypoints/pooling/score/serving.py
1	1	vllm/entrypoints/responses_utils.py
7	3	vllm/entrypoints/sagemaker/routes.py
1	1	vllm/entrypoints/serve/disagg/api_router.py
2	2	vllm/entrypoints/serve/disagg/protocol.py
4	2	vllm/entrypoints/serve/disagg/serving.py
1	1	vllm/entrypoints/serve/elastic_ep/api_router.py
1	1	vllm/entrypoints/serve/lora/api_router.py
4	2	vllm/entrypoints/serve/tokenize/api_router.py
139	0	vllm/entrypoints/serve/tokenize/protocol.py
7	5	vllm/entrypoints/serve/tokenize/serving.py
4	2	vllm/entrypoints/utils.py
3	1	vllm/reasoning/abs_reasoning_parsers.py
4	2	vllm/reasoning/basic_parsers.py
1	1	vllm/reasoning/deepseek_r1_reasoning_parser.py
4	1	vllm/reasoning/deepseek_v3_reasoning_parser.py
4	1	vllm/reasoning/ernie45_reasoning_parser.py
4	1	vllm/reasoning/gptoss_reasoning_parser.py
4	1	vllm/reasoning/granite_reasoning_parser.py
4	1	vllm/reasoning/holo2_reasoning_parser.py
4	1	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
4	1	vllm/reasoning/identity_reasoning_parser.py
3	1	vllm/reasoning/minimax_m2_reasoning_parser.py
3	1	vllm/reasoning/mistral_reasoning_parser.py
3	2	vllm/reasoning/olmo3_reasoning_parser.py
4	2	vllm/reasoning/qwen3_reasoning_parser.py
4	1	vllm/reasoning/step3_reasoning_parser.py
1	1	vllm/tokenizers/mistral.py
2	2	vllm/tool_parsers/abstract_tool_parser.py
3	1	vllm/tool_parsers/deepseekv31_tool_parser.py
3	1	vllm/tool_parsers/deepseekv32_tool_parser.py
3	1	vllm/tool_parsers/deepseekv3_tool_parser.py
3	1	vllm/tool_parsers/ernie45_tool_parser.py
3	1	vllm/tool_parsers/functiongemma_tool_parser.py
3	1	vllm/tool_parsers/gigachat3_tool_parser.py
3	1	vllm/tool_parsers/glm4_moe_tool_parser.py
3	1	vllm/tool_parsers/granite_20b_fc_tool_parser.py
3	1	vllm/tool_parsers/granite_tool_parser.py
3	1	vllm/tool_parsers/hermes_tool_parser.py
3	1	vllm/tool_parsers/hunyuan_a13b_tool_parser.py
3	1	vllm/tool_parsers/internlm2_tool_parser.py
3	1	vllm/tool_parsers/jamba_tool_parser.py
3	1	vllm/tool_parsers/kimi_k2_tool_parser.py
3	1	vllm/tool_parsers/llama4_pythonic_tool_parser.py
3	1	vllm/tool_parsers/llama_tool_parser.py
3	1	vllm/tool_parsers/minimax_m2_tool_parser.py
3	1	vllm/tool_parsers/minimax_tool_parser.py
3	1	vllm/tool_parsers/mistral_tool_parser.py
3	1	vllm/tool_parsers/olmo3_tool_parser.py
4	2	vllm/tool_parsers/openai_tool_parser.py
3	1	vllm/tool_parsers/phi4mini_tool_parser.py
3	1	vllm/tool_parsers/pythonic_tool_parser.py
3	1	vllm/tool_parsers/qwen3coder_tool_parser.py
3	1	vllm/tool_parsers/qwen3xml_tool_parser.py
3	1	vllm/tool_parsers/seed_oss_tool_parser.py
3	1	vllm/tool_parsers/step3_tool_parser.py
1	1	vllm/tool_parsers/utils.py
4	3	vllm/tool_parsers/xlam_tool_parser.py

[a5bbbd2f2] Mickaël Seznec 2026-01-13 [Quantization] fix: overflow with static per-tensor scaling (#29867)
61	2	vllm/model_executor/layers/quantization/utils/quant_utils.py
10	54	vllm/v1/attention/backends/mla/common.py

[8c8653b67] Nicolò Lucchesi 2026-01-13 [Docs] Nixl Usage recommend `fail` kv_load_failure_policy (#32198)
16	6	docs/features/nixl_connector_usage.md

[232214b2a] Cyrus Leung 2026-01-13 [Bugfix] Replace `PoolingParams.normalize` with `use_activation` (#32243)
1	1	examples/pooling/embed/openai_embedding_long_text/README.md
2	2	examples/pooling/embed/openai_embedding_long_text/client.py
1	1	examples/pooling/embed/openai_embedding_long_text/service.sh
3	1	tests/entrypoints/pooling/embed/test_offline.py
1	1	tests/entrypoints/pooling/embed/test_online_long_text.py
4	13	tests/entrypoints/pooling/score/test_online_score.py
2	2	tests/model_executor/test_model_load_with_params.py
4	4	tests/models/language/pooling/test_pooler_config_init_behaviour.py
1	1	tests/test_config.py
11	11	tests/test_pooling_params.py
16	9	vllm/config/pooler.py
2	2	vllm/entrypoints/pooling/embed/protocol.py
0	2	vllm/entrypoints/pooling/pooling/protocol.py
1	1	vllm/model_executor/layers/pooler/seqwise/heads.py
2	2	vllm/model_executor/layers/pooler/seqwise/poolers.py
3	3	vllm/model_executor/layers/pooler/tokwise/heads.py
2	2	vllm/model_executor/layers/pooler/tokwise/poolers.py
1	1	vllm/model_executor/models/bert.py
2	1	vllm/model_executor/models/modernbert.py
8	9	vllm/pooling_params.py
1	1	vllm/transformers_utils/config.py

[eb28e8068] Cyrus Leung 2026-01-13 [Refactor] Remove `get_encoder_dummy_data` (#32241)
4	4	vllm/model_executor/models/nemotron_parse.py
4	4	vllm/model_executor/models/whisper.py
4	4	vllm/multimodal/processing.py
1	25	vllm/multimodal/profiling.py
0	38	vllm/multimodal/registry.py
8	7	vllm/v1/engine/input_processor.py

[542a4059b] YunzhuLu 2026-01-13 [Model] Use mm_position to compute mrope positions for Qwen2-VL/2.5-VL (#32126)
57	95	vllm/model_executor/models/qwen2_5_vl.py
56	95	vllm/model_executor/models/qwen2_vl.py

[df7e12715] Andreas Karatzas 2026-01-13 [ROCm][CI] Fix engine core client tests for ROCm spawn multiprocessing (#32061)
181	70	tests/v1/engine/test_engine_core_client.py

[44c34f22d] Roy Wang 2026-01-13 [Doc] Update installation from source command (#32239)
8	2	docs/contributing/README.md

[80221e188] Xingyu Liu 2026-01-12 [BugFix]Fix eagle draft_model_config and add tests (#31753)
3	3	tests/config/draft_model_arch_groundtruth.json
22	0	tests/test_config.py
14	0	vllm/config/speculative.py
1	1	vllm/transformers_utils/model_arch_config_convertor.py

[5e714f7ff] Andreas Karatzas 2026-01-13 [ROCm][CI] Fix HuggingFace flash_attention_2 accuracy issue in Isaac vision encoder (#32233)
19	0	tests/models/multimodal/conftest.py
8	0	tests/models/multimodal/generation/vlm_utils/model_utils.py

[11b6af528] Andreas Karatzas 2026-01-12 [ROCm][Bugfix] Fix Mamba batched decode producing incorrect output (#32099)
7	5	vllm/model_executor/layers/mamba/mamba_mixer.py
8	0	vllm/model_executor/models/plamo2.py

[2a719e086] Wentao Ye 2026-01-12 [Perf] Optimize requests abort (#32211)
4	3	vllm/v1/engine/async_llm.py

[f243abc92] Andrew Bennett 2026-01-12 Fix various typos found in `docs` (#32212)
1	1	docs/contributing/deprecation_policy.md
1	1	docs/contributing/model/basic.md
1	1	docs/deployment/frameworks/cerebrium.md
1	1	docs/deployment/frameworks/hf_inference_endpoints.md
1	1	docs/deployment/integrations/production-stack.md
2	2	docs/design/custom_op.md
1	1	docs/design/fused_moe_modular_kernel.md
1	1	docs/design/logits_processors.md
1	1	docs/features/disagg_encoder.md
1	1	docs/features/disagg_prefill.md
1	1	docs/features/quantization/inc.md
1	1	docs/features/spec_decode.md
4	4	docs/features/structured_outputs.md
1	1	docs/getting_started/installation/cpu.arm.inc.md
1	1	docs/getting_started/quickstart.md
2	2	docs/governance/collaboration.md
1	1	docs/models/extensions/fastsafetensor.md
1	1	docs/models/generative_models.md
1	1	docs/serving/openai_compatible_server.md
1	1	docs/usage/troubleshooting.md
1	1	docs/usage/v1_guide.md

[60b77e146] Sanghoon Yoon 2026-01-13 [Frontend] Add `reasoning_effort` to `OpenAIServing._preprocess_chat()` (#31956)
5	1	vllm/entrypoints/openai/serving_chat.py
8	0	vllm/entrypoints/openai/serving_responses.py

[15b33ff06] cjackal 2026-01-13 [Misc] improve warning/assert messages (#32226)
2	2	vllm/compilation/compiler_interface.py
4	4	vllm/config/compilation.py
1	1	vllm/config/model.py
10	10	vllm/config/vllm.py
1	1	vllm/lora/ops/triton_ops/utils.py
1	1	vllm/v1/attention/backends/fa_utils.py

[c6bb5b560] Nick Hill 2026-01-12 [BugFix] Fix engine crash caused by chat tools + response_format (#32127)
42	0	tests/tool_use/test_chat_completions.py
1	0	vllm/tool_parsers/abstract_tool_parser.py
4	0	vllm/v1/engine/input_processor.py

[9273a427b] Nick Hill 2026-01-12 [Misc] Allow enabling NCCL for DP sync when async scheduling (#32197)
13	3	vllm/config/parallel.py
1	3	vllm/config/scheduler.py
11	9	vllm/config/vllm.py
1	1	vllm/engine/arg_utils.py

[78d13ea9d] Cyrus Leung 2026-01-13 [Model] Handle `trust_remote_code` for transformers backend (#32194)
2	0	vllm/model_executor/models/registry.py
12	1	vllm/transformers_utils/dynamic_module.py

[a307ac073] Andrew Xia 2026-01-12 [responsesAPI] add unit test for optional function tool call id (#32036)
118	0	tests/entrypoints/test_responses_utils.py

[a28d9f447] Divakar Verma 2026-01-12 [ROCm][CI] Handle pytest status code 5 when a shard isn't allocated any tests  (#32040)
10	1	.buildkite/scripts/hardware_ci/run-amd-test.sh
1	1	.buildkite/test-amd.yaml

[629584bfc] xuebwang-amd 2026-01-13 [Kernel][MoE] fix computation order of MoE weight multiplication and improve flow (#31962)
24	9	vllm/model_executor/layers/fused_moe/fused_moe.py

[0a7dd2375] Woosuk Kwon 2026-01-12 [Model Runner V2] Add support for M-RoPE (#32143)
6	1	vllm/v1/worker/gpu/cudagraph_utils.py
21	2	vllm/v1/worker/gpu/input_batch.py
0	0	vllm/v1/worker/gpu/mm/__init__.py
127	0	vllm/v1/worker/gpu/mm/mrope_utils.py
49	4	vllm/v1/worker/gpu/model_runner.py

[dec28688c] Woosuk Kwon 2026-01-12 [Model Runner V2] Minor refactor for logit_bias (#32209)
54	26	vllm/v1/worker/gpu/sample/logit_bias.py

[9f430c94b] Vadim Gimpelson 2026-01-13 [BUGFIX] Add missed remaping of the names of fp8 kv-scale (#32199)
7	0	vllm/model_executor/models/qwen3_next.py

[f8bd8394e] Nicolò Lucchesi 2026-01-12 [NIXL][Bugfix] Failure logging overhaul + early metadata free on failure (#32031)
146	0	tests/v1/kv_connector/unit/test_nixl_connector.py
88	28	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[ca81811bf] Woosuk Kwon 2026-01-12 [Model Runner V2] Support logit_bias, allowed_token_ids, min_tokens (#32163)
11	5	vllm/v1/worker/gpu/buffer_utils.py
242	0	vllm/v1/worker/gpu/sample/logit_bias.py

[ad8818bb5] Lucas Kabela 2026-01-12 [Misc][BE] Type coverage for vllm/compilation [3/3] (#31748)
33	28	vllm/compilation/activation_quant_fusion.py
111	102	vllm/compilation/collective_fusion.py
58	38	vllm/compilation/fusion.py
24	20	vllm/compilation/fusion_attn.py
1	1	vllm/compilation/inductor_pass.py
23	22	vllm/compilation/matcher_utils.py
16	10	vllm/compilation/qk_norm_rope_fusion.py
38	36	vllm/compilation/rocm_aiter_fusion.py
22	18	vllm/compilation/sequence_parallelism.py
4	4	vllm/distributed/parallel_state.py
3	1	vllm/model_executor/layers/rotary_embedding/__init__.py

[08e8e99ce] Nicolò Lucchesi 2026-01-12 [Misc] Change log level for batch queue log (#32192)
1	1	vllm/v1/engine/core.py

[2be765b68] Or Ozeri 2026-01-12 [BugFix] scheduler: Fix ordering preserving of skipped requests (#32173)
25	0	tests/v1/core/test_scheduler.py
6	15	vllm/v1/core/sched/request_queue.py

[16abe6b85] Roger Wang 2026-01-12 [Misc] Set default torch num threads for input processing (#31879)
1	15	vllm/model_executor/models/internvl.py
11	1	vllm/v1/engine/input_processor.py

[1eb61ab34] Ilya Markov 2026-01-12 [Refactor] EPLB rebalance algo to NumPy (#30697)
16	15	tests/distributed/test_eplb_algo.py
1	1	tests/distributed/test_eplb_execute.py
111	114	vllm/distributed/eplb/policy/default.py

[3d962d72a] Kyungmin Lee 2026-01-13 [BugFix] fix FusedMoE.make_expert_params_mapping in EXAONE-MoE (#32196)
1	0	vllm/model_executor/models/exaone_moe.py

[20228cb85] Matthew Bonanni 2026-01-12 [3/N][Attention] Move AttentionMetadata-related code from utils.py to backend.py (#32054)
1	1	docs/design/cuda_graphs.md
1	2	tests/v1/attention/test_attention_backends.py
1	1	tests/v1/attention/test_mla_backends.py
3	3	tests/v1/attention/utils.py
1	1	tests/v1/e2e/test_async_spec_decode.py
1	1	tests/v1/spec_decode/test_tree_attention.py
4	2	vllm/model_executor/layers/attention/chunked_local_attention.py
1	1	vllm/model_executor/layers/attention/cross_attention.py
1	1	vllm/model_executor/layers/attention/encoder_only_attention.py
1	1	vllm/model_executor/layers/attention/static_sink_attention.py
1	1	vllm/model_executor/models/whisper_utils.py
287	0	vllm/v1/attention/backend.py
2	2	vllm/v1/attention/backends/cpu_attn.py
3	1	vllm/v1/attention/backends/flash_attn.py
3	3	vllm/v1/attention/backends/flashinfer.py
2	4	vllm/v1/attention/backends/flex_attention.py
5	3	vllm/v1/attention/backends/gdn_attn.py
3	3	vllm/v1/attention/backends/linear_attn.py
1	4	vllm/v1/attention/backends/mamba2_attn.py
4	2	vllm/v1/attention/backends/mamba_attn.py
2	2	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/cutlass_mla.py
1	1	vllm/v1/attention/backends/mla/flashattn_mla.py
2	1	vllm/v1/attention/backends/mla/flashinfer_mla.py
6	2	vllm/v1/attention/backends/mla/flashmla.py
3	3	vllm/v1/attention/backends/mla/flashmla_sparse.py
3	3	vllm/v1/attention/backends/mla/indexer.py
1	2	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
3	5	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
3	3	vllm/v1/attention/backends/rocm_aiter_fa.py
3	5	vllm/v1/attention/backends/rocm_attn.py
2	2	vllm/v1/attention/backends/tree_attn.py
3	5	vllm/v1/attention/backends/triton_attn.py
6	289	vllm/v1/attention/backends/utils.py
4	4	vllm/v1/spec_decode/eagle.py
2	2	vllm/v1/worker/gpu/attn_utils.py
3	3	vllm/v1/worker/gpu_model_runner.py

[7c0d3c515] Cyrus Leung 2026-01-13 [Benchmark] Share data between SLA runs (#32184)
68	10	tests/benchmarks/sweep/test_serve_sla.py
40	15	vllm/benchmarks/sweep/serve_sla.py

[5b6810741] Nicolò Lucchesi 2026-01-12 [Misc][PD] Fix `get_attn_backend` usage in transfer connectors (#31988)
1	1	tests/v1/kv_connector/unit/test_nixl_connector.py
26	2	vllm/distributed/kv_transfer/kv_connector/utils.py
7	9	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py
5	8	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[8fb2c135b] Asaf Joseph Gardin 2026-01-12 [Bugfix] Fix stale SSM state for new Mamba requests scheduled as decode (#32118)
21	0	tests/v1/attention/test_batch_reordering.py
3	3	vllm/v1/attention/backends/utils.py

[8863c2b25] Cyrus Leung 2026-01-13 [Model] Standardize pooling heads (#32148)
6	1	vllm/model_executor/layers/pooler/common.py
41	47	vllm/model_executor/layers/pooler/seqwise/heads.py
25	4	vllm/model_executor/layers/pooler/seqwise/poolers.py
23	32	vllm/model_executor/layers/pooler/tokwise/heads.py
25	4	vllm/model_executor/layers/pooler/tokwise/poolers.py
25	24	vllm/model_executor/models/bert.py
1	4	vllm/model_executor/models/bert_with_rope.py
11	14	vllm/model_executor/models/gritlm.py
25	19	vllm/model_executor/models/modernbert.py

[3f72639d3] danielafrimi 2026-01-12 [FIX] Add NO_MUL activation support for modular kernel path (#31528)
201	0	tests/kernels/moe/test_triton_moe_no_act_mul.py
3	1	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
16	4	vllm/model_executor/layers/fused_moe/cutlass_moe.py
12	6	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/fallback.py
1	0	vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py
1	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
9	3	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	0	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
17	34	vllm/model_executor/layers/fused_moe/fused_moe.py
18	13	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
29	10	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
3	0	vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py
3	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/trtllm_moe.py
50	0	vllm/model_executor/layers/fused_moe/utils.py

[6bc9c8473] Jaehyun An 2026-01-13 [MODEL] New model support for kakaocorp/kanana-1.5-v-3b-instruct (#29384)
1	0	docs/models/supported_models.md
28	0	examples/offline_inference/vision_language.py
4	0	tests/models/registry.py
756	0	vllm/model_executor/models/kanana_v.py
1	0	vllm/model_executor/models/registry.py

[63ed2409e] Kyungmin Lee 2026-01-13 Add K-EXAONE-236B-A23B (#31621)
1	0	docs/models/supported_models.md
8	0	tests/models/registry.py
10	0	vllm/config/speculative.py
2	0	vllm/model_executor/models/exaone4.py
578	0	vllm/model_executor/models/exaone_moe.py
255	0	vllm/model_executor/models/exaone_moe_mtp.py
2	0	vllm/model_executor/models/registry.py

[95e53d907] Andy Zhang 2026-01-13 doc: Update model references in supported_models.md (#32188)
2	2	docs/models/supported_models.md

[0346396e9] TJian 2026-01-12 [ROCm] [Bugfix] Fix order of mori build in Dockerfile.rocm_base (#32179)
21	13	docker/Dockerfile.rocm_base

[e68b0dad8] Andy Zhang 2026-01-12 doc: Update model name for Qwen3-Coder in documentation (#32185)
1	1	docs/features/tool_calling.md

[9cddbdba6] Or Ozeri 2026-01-12 OffloadingConnector: Add cpu_bytes_to_use configuration (#24498)
2	2	docs/features/disagg_prefill.md
2	3	tests/v1/kv_connector/unit/test_config.py
3	2	tests/v1/kv_connector/unit/test_offloading_connector.py
1	1	tests/v1/kv_offload/test_cpu_offloading.py
1	5	vllm/config/vllm.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
31	8	vllm/v1/kv_offload/cpu.py
3	1	vllm/v1/kv_offload/factory.py
5	1	vllm/v1/kv_offload/spec.py

[49e6b86c9] Hongxin Xu 2026-01-12 [Feature] Support recording expert indices for rollout router replay (#28284)
2	0	vllm/config/model.py
1	0	vllm/config/vllm.py
6	0	vllm/engine/arg_utils.py
3	0	vllm/entrypoints/llm.py
22	0	vllm/model_executor/layers/fused_moe/layer.py
324	0	vllm/model_executor/layers/fused_moe/routed_experts_capturer.py
3	0	vllm/outputs.py
50	1	vllm/v1/core/sched/scheduler.py
2	1	vllm/v1/engine/__init__.py
9	1	vllm/v1/engine/output_processor.py
41	0	vllm/v1/worker/gpu_model_runner.py

[0565f1fde] dtc 2026-01-12 [P/D] Refactor mooncake connector sender thread using async coroutines (#31573)
173	174	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py

[9dbe1fe96] Isotr0py 2026-01-12 [Bugfix] Fix missing scale passing for encoder Triton Attention implementation  (#32149)
0	8	examples/offline_inference/basic/embed.py
0	8	examples/offline_inference/basic/score.py
1	0	vllm/v1/attention/backends/triton_attn.py
12	11	vllm/v1/attention/ops/triton_prefill_attention.py

[a5f89ae29] RickyChen / 陳昭儒 2026-01-12 [Doc] Add documentation for offline API docs feature (#32134)
8	0	docs/serving/openai_compatible_server.md

[05e898123] Jee Jee Li 2026-01-12 [Doc] Improve LoRA docs (#32159)
12	15	docs/features/lora.md

[899541bdb] XlKsyt 2026-01-12 [doc] fix broken links (#32158)
7	21	docs/design/paged_attention.md

[d7b2e5709] daniel-salib 2026-01-12 [Frontend] Fix Flaky MCP Streaming Test (#32153)
1	1	tests/entrypoints/openai/responses/test_harmony.py

[5e034f2e3] Andika Rachman 2026-01-12 [cpu][bench] Add Fused MoE Micro Benchmark for CPU Backend (#32092)
175	0	benchmarks/kernels/cpu/benchmark_cpu_fused_moe.py

[22970c162] Nicolò Lucchesi 2026-01-12 [Misc] Disable default `--ready-check-timeout-sec` extra call in vllm bench (#30975)
2	3	vllm/benchmarks/serve.py

[600aaab8d] Cyrus Leung 2026-01-12 [Model] Remove incorrect `SupportsPP` from MTP models (#32150)
1	2	vllm/model_executor/models/deepseek_mtp.py
1	2	vllm/model_executor/models/ernie_mtp.py
1	2	vllm/model_executor/models/glm4_moe_mtp.py
1	2	vllm/model_executor/models/longcat_flash_mtp.py
1	2	vllm/model_executor/models/openpangu_mtp.py
1	5	vllm/model_executor/models/qwen3_next_mtp.py

[60446cd68] wang.yuqi 2026-01-12 [Model] Improve multimodal pooling examples (#32085)
7	4	docs/serving/openai_compatible_server.md
93	0	examples/pooling/embed/vision_embedding_offline.py
130	5	examples/pooling/embed/{openai_chat_embedding_client_for_multimodal.py => vision_embedding_online.py}
0	0	examples/pooling/score/{cohere_rerank_client.py => cohere_rerank_online.py}
0	60	examples/pooling/score/openai_cross_encoder_score_for_multimodal.py
0	0	examples/pooling/score/{openai_reranker.py => rerank_api_online.py}
0	0	examples/pooling/score/{openai_cross_encoder_score.py => score_api_online.py}
80	0	examples/pooling/score/vision_rerank_api_online.py
0	0	examples/pooling/score/{vision_language_reranker.py => vision_reranker_offline.py}
71	0	examples/pooling/score/vision_score_api_online.py

[9101dc756] Cyrus Leung 2026-01-12 [Model] Avoid hardcoding pooling type (#32119)
10	4	vllm/model_executor/models/bert.py
9	1	vllm/model_executor/models/bert_with_rope.py
9	4	vllm/model_executor/models/gritlm.py
12	4	vllm/model_executor/models/modernbert.py
1	3	vllm/model_executor/models/roberta.py
6	6	vllm/model_executor/models/transformers/pooling.py

[025a32f9e] Woosuk Kwon 2026-01-11 [Model Runner V2] Remove async barrier (#32083)
18	34	vllm/v1/worker/gpu/block_table.py
218	0	vllm/v1/worker/gpu/buffer_utils.py
9	6	vllm/v1/worker/gpu/cudagraph_utils.py
57	25	vllm/v1/worker/gpu/input_batch.py
108	111	vllm/v1/worker/gpu/model_runner.py
12	7	vllm/v1/worker/gpu/sample/gumbel.py
14	127	vllm/v1/worker/gpu/sample/metadata.py
9	2	vllm/v1/worker/gpu/sample/min_p.py
7	7	vllm/v1/worker/gpu/sample/penalties.py
2	1	vllm/v1/worker/gpu/sample/sampler.py
27	23	vllm/v1/worker/gpu/spec_decode/eagle.py
51	85	vllm/v1/worker/gpu/states.py
58	34	vllm/v1/worker/gpu/structured_outputs.py

[19504ac07] Woosuk Kwon 2026-01-11 [Model Runner V2] Skip building deprecated fields in attn metadata (#32132)
1	7	vllm/v1/worker/gpu/attn_utils.py
3	6	vllm/v1/worker/gpu/cudagraph_utils.py
0	4	vllm/v1/worker/gpu/input_batch.py
2	18	vllm/v1/worker/gpu/model_runner.py
1	4	vllm/v1/worker/gpu/spec_decode/eagle.py

[3df619ac9] Jiangyun Zhu 2026-01-11 [CI] fix `test_concat_and_cache_mla_rope_fused` (#32117)
3	2	tests/kernels/core/test_rotary_embedding_mla_cache_fused.py

[d74132ca3] Ning Xie 2026-01-11 fix offline inference chat response prompt (#32088)
9	7	examples/offline_inference/context_extension.py
5	2	examples/offline_inference/spec_decode.py

[a34abc49b] maang 2026-01-11 [FixBug] Improve exception string in `tensorizer.py` (#31680)
4	1	vllm/model_executor/model_loader/tensorizer.py

[d70249e2e] rongfu.leng 2026-01-11 [Misc] fix this log format not space (#32112)
1	1	vllm/v1/engine/core_client.py

[a37453211] Cyrus Leung 2026-01-11 [CI/Build] Separate out flaky responses API tests (#32110)
16	2	.buildkite/test-amd.yaml
13	1	.buildkite/test-pipeline.yaml
9	2	.buildkite/test_areas/entrypoints.yaml
0	0	tests/entrypoints/openai/responses/__init__.py
0	0	tests/entrypoints/openai/{test_responses_error.py => responses/test_errors.py}
0	0	tests/entrypoints/openai/{test_responses_function_call_parsing.py => responses/test_function_call_parsing.py}
1	1	tests/entrypoints/openai/{test_response_api_with_harmony.py => responses/test_harmony.py}
1	1	tests/entrypoints/openai/{test_response_api_mcp_tools.py => responses/test_mcp_tools.py}
1	1	tests/entrypoints/openai/{test_response_api_parsable_context.py => responses/test_parsable_context.py}
1	1	tests/entrypoints/openai/{test_response_api_simple.py => responses/test_simple.py}

[cee7436a2] Isotr0py 2026-01-11 [Misc] Make `scipy` as optional audio/benchmark dependency  (#32096)
0	1	requirements/common.txt
2	1	setup.py
8	5	vllm/multimodal/audio.py

[4c16ba617] Or Ozeri 2026-01-11 [KVConnector] OffloadingConnector: Fix bug in handling of preemptions (#29870)
175	64	tests/v1/kv_connector/unit/test_offloading_connector.py
12	0	tests/v1/kv_offload/test_worker.py
10	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
24	0	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
10	0	vllm/v1/kv_offload/worker/cpu_gpu.py
19	0	vllm/v1/kv_offload/worker/worker.py
5	0	vllm/v1/worker/gpu_model_runner.py

[bde57ab2e] Matt 2026-01-11 [Hardware][AMD][CI][Bugfix] Fix AMD Quantization test group (#31713)
1	1	.buildkite/test-amd.yaml
7	1	tests/quantization/test_compressed_tensors.py
31	6	tests/quantization/test_configs.py
1	1	tests/quantization/test_cpu_offload.py
12	4	tests/quantization/test_fp8.py
5	1	tests/quantization/test_gptq_dynamic.py
12	25	tests/quantization/test_ptpc_fp8.py
5	0	tests/quantization/utils.py
4	0	vllm/model_executor/layers/quantization/__init__.py
17	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
17	13	vllm/model_executor/layers/quantization/ptpc_fp8.py
2	0	vllm/platforms/rocm.py

[9103ed169] Fadi Arafeh 2026-01-11 [CPU][BugFix] Disable AOT Compile for CPU (#32037)
6	1	vllm/envs.py

[46eb30f51] Laith Sakka 2026-01-10 make assume_32_bit_indexing configurable (#32044)
3	1	vllm/compilation/decorators.py
6	0	vllm/config/compilation.py

[0dd63639b] Andy Liu 2026-01-10 [MTP][GLM][Bugfix] Fixed .weight_scale loading logic that dropped MTP prediction accuracy with fp8+mtp (#32101)
6	5	vllm/model_executor/models/glm4_moe_mtp.py

[ef96fa3f1] Cyrus Leung 2026-01-11 [Benchmark][2/2] Use spline interpolation to tune SLA variables (#32095)
4	4	docs/benchmarking/sweeps.md
135	97	tests/benchmarks/sweep/test_serve_sla.py
70	140	vllm/benchmarks/sweep/serve_sla.py

[2a4dbe24e] Or Ozeri 2026-01-11 [BugFix] Wait for compute before offloading KV to CPU (#31341)
19	13	tests/v1/kv_connector/unit/test_offloading_connector.py
17	6	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
8	11	vllm/v1/kv_offload/worker/cpu_gpu.py

[8020a6040] RickyChen / 陳昭儒 2026-01-11 [Bugfix] Fix Qwen3-VL-Reranker model loading for sequence classification (#32089)
15	11	vllm/model_executor/models/adapters.py
3	1	vllm/model_executor/models/config.py

[e15a5ff07] Vadim Gimpelson 2026-01-11 [MISC] Add strict contiguity check for FlashInfer attention tensors (#32008)
30	0	vllm/utils/torch_utils.py
11	10	vllm/v1/attention/backends/flashinfer.py

[6ea001cfb] Vensen 2026-01-11 [Bugfix][Quantization] Ensure input contiguity in per_token_quant_int8 (#31637)
8	5	vllm/model_executor/layers/quantization/utils/int8_utils.py

[1c46dea00] shyeh25 2026-01-11 Revert "[Kernels][FI] Skip trtllm attention when num_kv_heads=1 (#308… (#31617)
0	35	tests/kernels/attention/test_flashinfer_trtllm_attention.py
1	21	vllm/utils/flashinfer.py

[028599739] Or Ozeri 2026-01-10 [BugFix] scheduler: Fix resuming of preempted requests after async load (#31583)
21	2	tests/v1/core/test_scheduler.py
6	1	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/request.py

[d1fd802fa] gnovack 2026-01-10 fused_moe_kernel - cast accumulator after applying router weights (#32002)
5	8	vllm/model_executor/layers/fused_moe/fused_moe.py

[543c23be7] Xin Yang 2026-01-10 [LoRA][Perf] Improve FusedMoE LoRA performance for small rank (#32019)
14	9	vllm/lora/layers/fused_moe.py
33	0	vllm/lora/layers/utils.py
14	2	vllm/lora/ops/triton_ops/utils.py

[b8bf5c45b] jvlunteren 2026-01-10 [Kernel] Optimize Sliding Window Attention in 3D Triton Kernel (#31984)
26	3	vllm/v1/attention/ops/triton_unified_attention.py

[e6c6f2c79] Michael Goin 2026-01-10 [Quant] Support MXFP4 W4A16 for compressed-tensors dense models (#31926)
23	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
106	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_mxfp4.py

[07286ec5a] Jeremy Teboul 2026-01-10 [Bugfix] Fix integer overflow in Gemma3n audio processing (#31657)
141	1	tests/models/multimodal/processing/test_gemma3.py
57	0	vllm/model_executor/models/gemma3n_audio_utils.py
41	31	vllm/model_executor/models/gemma3n_mm.py

[14fc7a68c] Ning Xie 2026-01-10 [Bugfix] fix offline chat output prompt (#32076)
13	7	examples/offline_inference/basic/chat.py

[5f2385a4c] Cyrus Leung 2026-01-10 [Benchmark][1/2] Generalize SLA criterion validation from binary flags to margins (#32075)
0	0	tests/benchmarks/sweep/__init__.py
0	0	tests/benchmarks/{ => sweep}/test_param_sweep.py
202	0	tests/benchmarks/sweep/test_serve_sla.py
2	1	vllm/benchmarks/sweep/param_sweep.py
28	16	vllm/benchmarks/sweep/serve_sla.py
21	15	vllm/benchmarks/sweep/sla_sweep.py

[a01a1c0d6] Frelam 2026-01-10 [Bugfix] fix encoder cache leak of waiting requests in scheduler to solve stuck in CPU scheduling (#31857)
5	0	vllm/v1/core/sched/scheduler.py

[da6709c9f] Lucas Wilkinson 2026-01-10 [Misc] Delay deprecation of CommonAttentionMetadata properties (#32074)
3	3	vllm/v1/attention/backends/utils.py

[d83becd50] Andreas Karatzas 2026-01-09 [ROCm][CI] Fix flaky `test_function_calling_with_stream` and reduce schema test examples (#32063)
1	1	tests/entrypoints/openai/test_openai_schema.py
13	9	tests/entrypoints/openai/test_response_api_with_harmony.py

[0c9614876] roikoren755 2026-01-10 Update modelopt KV cache quantization resolution to new scheme (#31895)
21	2	vllm/utils/torch_utils.py

[583a90e00] Cyrus Leung 2026-01-10 [Refactor] Separate sequence and token pooling types (#32026)
4	2	tests/model_executor/test_model_load_with_params.py
1	1	tests/models/language/pooling/test_embedding.py
1	1	tests/models/language/pooling/test_mm_classifier_conversion.py
5	2	tests/models/language/pooling_mteb_test/mteb_embed_utils.py
5	2	tests/models/language/pooling_mteb_test/mteb_score_utils.py
4	4	tests/models/language/pooling_mteb_test/test_baai.py
1	1	tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py
2	2	tests/models/language/pooling_mteb_test/test_cross_encoder.py
7	7	tests/models/language/pooling_mteb_test/test_gte.py
2	2	tests/models/language/pooling_mteb_test/test_intfloat.py
2	2	tests/models/language/pooling_mteb_test/test_jina.py
1	1	tests/models/language/pooling_mteb_test/test_mxbai_rerank.py
2	2	tests/models/language/pooling_mteb_test/test_nemotron.py
2	2	tests/models/language/pooling_mteb_test/test_nomic.py
1	1	tests/models/language/pooling_mteb_test/test_qwen3_reranker.py
5	5	tests/models/language/pooling_mteb_test/test_snowflake_arctic_embed.py
2	2	tests/models/language/pooling_mteb_test/test_st_projector.py
3	1	tests/models/utils.py
49	37	tests/test_config.py
4	4	tests/test_pooling_params.py
55	39	vllm/config/model.py
59	7	vllm/config/pooler.py
1	1	vllm/entrypoints/llm.py
4	4	vllm/model_executor/layers/pooler/seqwise/methods.py
2	2	vllm/model_executor/layers/pooler/seqwise/poolers.py
2	4	vllm/model_executor/layers/pooler/tokwise/methods.py
2	2	vllm/model_executor/layers/pooler/tokwise/poolers.py
5	5	vllm/model_executor/models/bert.py
2	2	vllm/model_executor/models/bert_with_rope.py
3	3	vllm/model_executor/models/clip.py
5	4	vllm/model_executor/models/config.py
1	1	vllm/model_executor/models/gritlm.py
33	9	vllm/model_executor/models/interfaces_base.py
1	1	vllm/model_executor/models/internlm2.py
3	3	vllm/model_executor/models/modernbert.py
2	2	vllm/model_executor/models/qwen2_rm.py
9	5	vllm/model_executor/models/registry.py
2	2	vllm/model_executor/models/roberta.py
3	3	vllm/model_executor/models/siglip.py
1	1	vllm/pooling_params.py
2	2	vllm/tasks.py
24	21	vllm/transformers_utils/config.py

[52d428295] maang 2026-01-10 [Core] Refactor ColumnParallelLinear: remove unused parameter and optimize forward (#31939)
4	10	vllm/model_executor/layers/linear.py

[c60578de0] Kevin McKay 2026-01-09 [Bugfix][Hardware][AMD] Use dynamic WARP_SIZE in sampler vectorized_process (#31295)
7	4	csrc/sampler.cu

[80fead8bf] PatrykSaffer 2026-01-10 Fuse RoPE and MLA KV-cache write (#25774)
1	0	CMakeLists.txt
7	0	csrc/cache.h
279	0	csrc/cache_kernels_fused.cu
16	0	csrc/torch_bindings.cpp
161	0	tests/kernels/core/test_rotary_embedding_mla_cache_fused.py
26	0	vllm/_custom_ops.py

[e45946bd9] Akshat Shrivastava 2026-01-09 feature/issac 0.2 (#31550)
4	1	tests/models/multimodal/generation/test_common.py
1	0	tests/models/registry.py
23	1	vllm/model_executor/models/isaac.py
15	1	vllm/transformers_utils/configs/isaac.py

[ea6d067a2] Lucas Kabela 2026-01-09 [Misc][LLaMa4] Compile LLaMa Vision Encoder (#30709)
37	0	tests/compile/fullgraph/test_multimodal_compile.py
3	2	vllm/config/compilation.py
3	3	vllm/model_executor/layers/attention/mm_encoder_attention.py
5	2	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
5	1	vllm/model_executor/models/llama.py
29	9	vllm/model_executor/models/mllama4.py
3	3	vllm/v1/attention/ops/vit_attn_wrappers.py

[abd922428] Ning Xie 2026-01-10 resolve pydantic error in startup benchmark (#31348)
19	0	tests/benchmarks/test_bench_startup.py
2	7	vllm/benchmarks/startup.py

[4dc0d606b] Kevin McKay 2026-01-09 [Bugfix] Narrow broad exceptions in compilation backends (#31616)
1	1	vllm/compilation/backends.py

[ac0675ff6] Micah Williamson 2026-01-09 [CI] Allow Deprecated Quantization For LM Eval Tests (#32065)
1	0	.buildkite/lm-eval-harness/test_lm_eval_correctness.py

[e18464a57] Wentao Ye 2026-01-09 [Perf] Optimize async scheduling placeholder using empty (#32056)
4	1	vllm/v1/engine/output_processor.py

[1963245ed] Russell Bryant 2026-01-09 [Core] Use weights_only=True with torch.load (#32045)
1	1	vllm/model_executor/model_loader/tensorizer.py

[030890197] Matthew Bonanni 2026-01-09 [2/N][Attention] Fix pre-commit errors (#32052)
0	2	tools/pre_commit/mypy.py
4	8	vllm/v1/attention/backends/fa_utils.py
2	6	vllm/v1/attention/ops/paged_attn.py

[aaf4b70aa] Lucas Kabela 2026-01-09 [Misc][BE] Type coverage for vllm/compilation [2/3] (#31744)
6	6	vllm/compilation/backends.py
16	7	vllm/compilation/caching.py
12	10	vllm/compilation/cuda_graph.py
42	20	vllm/compilation/decorators.py
5	5	vllm/compilation/fix_functionalization.py
3	0	vllm/compilation/inductor_pass.py
2	2	vllm/compilation/noop_elimination.py
19	12	vllm/compilation/pass_manager.py
29	15	vllm/compilation/piecewise_backend.py
23	13	vllm/compilation/wrapper.py
3	0	vllm/config/compilation.py
1	1	vllm/distributed/device_communicators/pynccl_allocator.py

[3adffd5b9] Nick Hill 2026-01-09 [Misc] Enable async scheduling by default with spec decoding (#31998)
27	22	vllm/config/vllm.py

[97ba96fbe] zhrrr 2026-01-10 [perf][async] support non cpu sync get logprob tensors for spec (#31336)
8	0	vllm/v1/outputs.py
25	14	vllm/v1/sample/rejection_sampler.py
15	15	vllm/v1/worker/gpu_model_runner.py

[94578127a] Chendi.Xue 2026-01-09 [NIXL] refine decoder side post process for heterogeneous BlockSize and kv_layout (#30275)
78	0	vllm/distributed/kv_transfer/kv_connector/utils.py
61	87	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[2612ba928] Matthew Bonanni 2026-01-09 [1/N][Attention] Restructure attention: move files (#31916)
1	1	.buildkite/test-amd.yaml
1	1	.buildkite/test-pipeline.yaml
1	1	.buildkite/test_areas/kernels.yaml
4	4	.github/CODEOWNERS
2	2	.github/mergify.yml
3	3	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
1	1	docs/contributing/model/basic.md
1	1	docs/design/custom_op.md
2	2	docs/design/plugin_system.md
1	1	examples/offline_inference/basic/embed.py
1	1	examples/offline_inference/basic/score.py
1	1	tests/compile/fullgraph/test_full_cudagraph.py
1	1	tests/compile/fullgraph/test_full_graph.py
2	2	tests/compile/test_fusion_attn.py
1	1	tests/compile/test_qk_norm_rope_fusion.py
1	1	tests/config/test_multimodal_config.py
1	1	tests/engine/test_arg_utils.py
1	1	tests/kernels/attention/test_aiter_flash_attn.py
2	2	tests/kernels/attention/test_attention.py
3	3	tests/kernels/attention/test_attention_selector.py
1	1	tests/kernels/attention/test_cache.py
2	2	tests/kernels/attention/test_flashmla.py
3	3	tests/kernels/attention/test_flashmla_sparse.py
2	2	tests/kernels/attention/test_merge_attn_states.py
3	3	tests/kernels/attention/test_mha_attn.py
1	1	tests/kernels/attention/test_pack_unpack_triton.py
4	2	tests/kernels/attention/test_prefix_prefill.py
3	3	tests/kernels/attention/test_rocm_attention_selector.py
1	1	tests/kernels/attention/test_triton_decode_attention.py
1	1	tests/kernels/attention/test_triton_prefill_attention.py
1	1	tests/kernels/attention/test_triton_unified_attention.py
1	1	tests/kernels/utils.py
1	1	tests/models/multimodal/generation/test_vit_backend_functionality.py
1	1	tests/models/quantization/test_fp8.py
2	2	tests/test_attention_backend_registry.py
2	2	tests/v1/attention/test_attention_backends.py
3	3	tests/v1/attention/test_mla_backends.py
2	2	tests/v1/attention/test_rocm_attention_backends_selection.py
1	1	tests/v1/attention/test_sparse_mla_backends.py
2	2	tests/v1/attention/utils.py
1	1	tests/v1/determinism/utils.py
1	1	tests/v1/kv_connector/unit/test_backwards_compatibility.py
1	1	tests/v1/spec_decode/test_eagle.py
1	1	tests/v1/spec_decode/test_mtp.py
2	2	tests/v1/spec_decode/test_tree_attention.py
2	2	tests/v1/worker/test_gpu_model_runner.py
3	1	tools/pre_commit/mypy.py
7	7	vllm/attention/layer.py
0	0	vllm/attention/ops/__init__.py
1	1	vllm/config/attention.py
1	1	vllm/config/model.py
2	2	vllm/config/multimodal.py
1	1	vllm/distributed/kv_transfer/kv_connector/utils.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/example_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/envs.py
1	1	vllm/forward_context.py
0	0	vllm/{attention/backends => model_executor/layers/attention}/__init__.py
2	2	vllm/{attention/layers => model_executor/layers/attention}/chunked_local_attention.py
6	6	vllm/{attention/layers => model_executor/layers/attention}/cross_attention.py
5	5	vllm/{attention/layers => model_executor/layers/attention}/encoder_only_attention.py
6	6	vllm/{attention/layers => model_executor/layers/attention}/mm_encoder_attention.py
9	9	vllm/{attention/layers => model_executor/layers/attention}/static_sink_attention.py
1	1	vllm/model_executor/layers/attention_layer_base.py
1	1	vllm/model_executor/layers/batch_invariant.py
1	1	vllm/model_executor/layers/kda.py
2	2	vllm/model_executor/layers/mamba/abstract.py
1	1	vllm/model_executor/layers/mamba/linear_attn.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	1	vllm/model_executor/layers/mamba/short_conv.py
1	1	vllm/model_executor/models/afmoe.py
1	1	vllm/model_executor/models/aimv2.py
4	2	vllm/model_executor/models/apertus.py
3	1	vllm/model_executor/models/bert.py
3	1	vllm/model_executor/models/bert_with_rope.py
1	1	vllm/model_executor/models/blip.py
1	1	vllm/model_executor/models/clip.py
1	1	vllm/model_executor/models/config.py
1	1	vllm/model_executor/models/deepencoder.py
6	4	vllm/model_executor/models/deepseek_v2.py
4	4	vllm/model_executor/models/dots_ocr.py
4	4	vllm/model_executor/models/ernie45_vl.py
4	2	vllm/model_executor/models/gemma3.py
1	1	vllm/model_executor/models/glm4.py
4	4	vllm/model_executor/models/glm4_1v.py
1	1	vllm/model_executor/models/glm4v.py
1	1	vllm/model_executor/models/glmasr.py
1	1	vllm/model_executor/models/gpt_oss.py
1	1	vllm/model_executor/models/hunyuan_v1.py
2	2	vllm/model_executor/models/hunyuan_vision.py
1	1	vllm/model_executor/models/idefics2_vision_model.py
1	1	vllm/model_executor/models/intern_vit.py
1	1	vllm/model_executor/models/interns1_vit.py
1	1	vllm/model_executor/models/iquest_loopcoder.py
1	1	vllm/model_executor/models/isaac.py
3	3	vllm/model_executor/models/keye.py
4	2	vllm/model_executor/models/llama.py
3	1	vllm/model_executor/models/llama4.py
1	1	vllm/model_executor/models/mimo_v2_flash.py
1	1	vllm/model_executor/models/minimax_text_01.py
1	1	vllm/model_executor/models/mllama4.py
3	1	vllm/model_executor/models/modernbert.py
1	1	vllm/model_executor/models/molmo.py
1	1	vllm/model_executor/models/moonvit.py
1	1	vllm/model_executor/models/nemotron_nas.py
1	1	vllm/model_executor/models/nemotron_parse.py
3	1	vllm/model_executor/models/openpangu.py
1	1	vllm/model_executor/models/ouro.py
4	4	vllm/model_executor/models/paddleocr_vl.py
1	1	vllm/model_executor/models/plamo2.py
4	2	vllm/model_executor/models/qwen2.py
2	2	vllm/model_executor/models/qwen2_5_vl.py
2	2	vllm/model_executor/models/qwen2_vl.py
1	1	vllm/model_executor/models/qwen3.py
1	1	vllm/model_executor/models/qwen3_next.py
1	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py
1	1	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/seed_oss.py
4	2	vllm/model_executor/models/siglip.py
1	1	vllm/model_executor/models/siglip2.py
1	1	vllm/model_executor/models/siglip2navit.py
1	1	vllm/model_executor/models/step3_vl.py
4	2	vllm/model_executor/models/transformers/base.py
1	1	vllm/model_executor/models/vision.py
5	5	vllm/model_executor/models/whisper.py
5	5	vllm/model_executor/models/whisper_utils.py
2	2	vllm/platforms/cpu.py
4	4	vllm/platforms/cuda.py
2	2	vllm/platforms/interface.py
2	2	vllm/platforms/rocm.py
2	2	vllm/platforms/xpu.py
0	0	vllm/{attention/backends/abstract.py => v1/attention/backend.py}
4	4	vllm/v1/attention/backends/cpu_attn.py
0	0	vllm/{attention/utils => v1/attention/backends}/fa_utils.py
6	6	vllm/v1/attention/backends/flash_attn.py
4	4	vllm/v1/attention/backends/flash_attn_diffkv.py
8	8	vllm/v1/attention/backends/flashinfer.py
6	6	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/attention/backends/gdn_attn.py
1	1	vllm/v1/attention/backends/linear_attn.py
1	1	vllm/v1/attention/backends/mamba1_attn.py
1	1	vllm/v1/attention/backends/mamba2_attn.py
9	9	vllm/v1/attention/backends/mla/common.py
4	4	vllm/v1/attention/backends/mla/cutlass_mla.py
9	9	vllm/v1/attention/backends/mla/flashattn_mla.py
4	4	vllm/v1/attention/backends/mla/flashinfer_mla.py
8	8	vllm/v1/attention/backends/mla/flashmla.py
11	11	vllm/v1/attention/backends/mla/flashmla_sparse.py
4	4	vllm/v1/attention/backends/mla/indexer.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
3	3	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
6	6	vllm/v1/attention/backends/mla/triton_mla.py
1	1	vllm/{ => v1}/attention/backends/registry.py
7	7	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/rocm_aiter_unified_attn.py
13	11	vllm/v1/attention/backends/rocm_attn.py
1	1	vllm/v1/attention/backends/short_conv_attn.py
4	4	vllm/v1/attention/backends/tree_attn.py
11	11	vllm/v1/attention/backends/triton_attn.py
5	5	vllm/v1/attention/backends/utils.py
0	0	vllm/{attention/layers => v1/attention/ops}/__init__.py
0	0	vllm/{ => v1}/attention/ops/chunked_prefill_paged_decode.py
0	0	vllm/{ => v1}/attention/ops/common.py
0	0	vllm/{ => v1}/attention/ops/flashmla.py
1	1	vllm/{ => v1}/attention/ops/merge_attn_states.py
0	0	vllm/{ => v1}/attention/ops/paged_attn.py
0	0	vllm/{ => v1}/attention/ops/pallas_kv_cache_update.py
0	0	vllm/{ => v1}/attention/ops/prefix_prefill.py
0	0	vllm/{ => v1}/attention/ops/rocm_aiter_mla_sparse.py
0	0	vllm/{ => v1}/attention/ops/triton_decode_attention.py
0	0	vllm/{ => v1}/attention/ops/triton_merge_attn_states.py
0	0	vllm/{ => v1}/attention/ops/triton_prefill_attention.py
0	0	vllm/{ => v1}/attention/ops/triton_reshape_and_cache_flash.py
0	0	vllm/{ => v1}/attention/ops/triton_unified_attention.py
1	1	vllm/{ => v1}/attention/ops/vit_attn_wrappers.py
5	5	vllm/{ => v1}/attention/selector.py
1	1	vllm/v1/kv_offload/cpu.py
1	1	vllm/v1/kv_offload/spec.py
1	1	vllm/v1/kv_offload/worker/cpu_gpu.py
1	1	vllm/v1/spec_decode/eagle.py
1	1	vllm/v1/worker/gpu/attn_utils.py
6	6	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/kv_connector_model_runner_mixin.py
1	1	vllm/v1/worker/utils.py

[1f8b7c536] Andrew Xia 2026-01-09 [responsesAPI] fix incomplete_messages for simple/parsable context (#31836)
15	0	tests/entrypoints/openai/test_response_api_parsable_context.py
15	0	tests/entrypoints/openai/test_response_api_simple.py
6	0	vllm/entrypoints/openai/parser/responses_parser.py
8	0	vllm/entrypoints/openai/serving_responses.py

[0a0aa0774] Lucas Wilkinson 2026-01-09 [Quant] Make static quant support all group shapes (#30833)
4	2	csrc/ops.h
178	22	csrc/quantization/w8a8/fp8/common.cu
5	2	csrc/torch_bindings.cpp
103	2	tests/kernels/quantization/test_fp8_quant.py
13	4	vllm/_custom_ops.py
17	11	vllm/model_executor/layers/quantization/input_quant_fp8.py
19	4	vllm/model_executor/layers/quantization/utils/quant_utils.py

[f9e2a75a1] jiahanc 2026-01-09 [fix] add cutedsl to global sf (#32001)
1	0	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[a4d5d663e] Runkai Tao 2026-01-09 Add unpermute-aware fused MoE path and small-batch fallback (#29354)
114	0	tests/kernels/moe/test_moe.py
62	21	vllm/model_executor/layers/fused_moe/fused_moe.py

[657e9c0e1] Jeremy Teboul 2026-01-09 [Fix] Introduce audio channels spec (#31595)
38	0	docs/features/multimodal_inputs.md
4	185	tests/models/multimodal/processing/test_qwen3_omni.py
503	0	tests/multimodal/test_audio.py
5	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
8	1	vllm/model_executor/models/qwen2_audio.py
8	1	vllm/model_executor/models/ultravox.py
8	1	vllm/model_executor/models/whisper.py
132	0	vllm/multimodal/audio.py
11	1	vllm/multimodal/parse.py

[308feab33] Wentao Ye 2026-01-09 [Perf] Optimize cutlass moe problem size calculation, 5.3% E2E Throughput improvement, 2.2% TTFT improvement (#31830)
5	0	csrc/ops.h
96	12	csrc/quantization/w8a8/cutlass/moe/moe_data.cu
24	0	csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu
11	0	csrc/torch_bindings.cpp
19	0	vllm/_custom_ops.py
17	51	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[28ae32a5d] Wentao Ye 2026-01-09 [Refactor] Remove numpy split in async scheduling (#32034)
6	12	vllm/v1/engine/async_llm.py

[f32c629eb] Andrew Xia 2026-01-09 [Frontend][gpt-oss] Allow system message to overwrite model identity (#31737)
89	0	tests/entrypoints/openai/test_response_api_with_harmony.py
3	8	vllm/entrypoints/openai/parser/harmony_utils.py
19	1	vllm/entrypoints/openai/serving_responses.py

[cd4a95e3a] Yifan Qiao 2026-01-09 [Feat][Core] Support multiple KV cache groups in Hybrid KV Coordinator (#31707)
235	4	tests/v1/core/test_prefix_caching.py
89	123	vllm/v1/core/kv_cache_coordinator.py

[d5ec6c056] Michael Goin 2026-01-09 [UX] Add vLLM model inspection view (#29450)
16	0	vllm/entrypoints/llm.py
7	0	vllm/envs.py
1	1	vllm/model_executor/layers/rotary_embedding/common.py
14	0	vllm/model_executor/model_loader/base_loader.py
136	0	vllm/model_inspection.py
6	0	vllm/v1/worker/worker_base.py

[08d954f03] Shanshan Shen 2026-01-10 [Doc] Add developer guide for CustomOp (#30886)
318	0	docs/design/custom_op.md
3	0	vllm/attention/layers/mm_encoder_attention.py
2	1	vllm/config/compilation.py
6	3	vllm/model_executor/custom_op.py
33	0	vllm/model_executor/layers/activation.py
6	0	vllm/model_executor/layers/conv.py
3	0	vllm/model_executor/layers/fused_moe/fused_moe.py
3	0	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
3	0	vllm/model_executor/layers/fused_moe/layer.py
3	0	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
9	0	vllm/model_executor/layers/layernorm.py
9	0	vllm/model_executor/layers/linear.py
3	0	vllm/model_executor/layers/logits_processor.py
3	0	vllm/model_executor/layers/mamba/mamba_mixer.py
6	0	vllm/model_executor/layers/mamba/mamba_mixer2.py
3	0	vllm/model_executor/layers/mamba/short_conv.py
3	0	vllm/model_executor/layers/mla.py
3	0	vllm/model_executor/layers/quantization/input_quant_fp8.py
3	0	vllm/model_executor/layers/rotary_embedding/base.py
3	0	vllm/model_executor/layers/rotary_embedding/common.py
3	0	vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py
6	0	vllm/model_executor/layers/vocab_parallel_embedding.py
4	1	vllm/model_executor/models/plamo2.py
3	0	vllm/model_executor/models/transformers/moe.py

[ac9f9330e] Kevin Šuc 2026-01-09 Rename --exclude-log-deltas to --enable-log-deltas (#32020)
1	1	vllm/entrypoints/openai/api_server.py
5	6	vllm/entrypoints/openai/cli_args.py
3	3	vllm/entrypoints/openai/serving_chat.py

[2d0c5b630] Isotr0py 2026-01-09 [Doc] Remove hardcoded Whisper in example openai translation client (#32027)
12	6	examples/online_serving/openai_translation_client.py

[34cd32fe3] Michael Goin 2026-01-09 [Perf][Kernel] Fused SiLU+Mul+Quant kernel for NVFP4 cutlass_moe (#31832)
6	0	csrc/ops.h
0	31	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
136	57	csrc/quantization/fp4/nvfp4_experts_quant.cu
24	0	csrc/quantization/fp4/nvfp4_quant_entry.cu
30	0	csrc/quantization/fp4/nvfp4_utils.cuh
9	0	csrc/torch_bindings.cpp
68	3	vllm/_custom_ops.py
5	4	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[8e27663b6] R3hankhan 2026-01-09 [CPU] Add head sizes 80 and 112 with vec16 fallback (#31968)
3	0	csrc/cpu/cpu_attn.cpp
1	1	csrc/cpu/cpu_attn_amx.hpp
1	1	csrc/cpu/cpu_attn_neon.hpp
7	3	vllm/v1/attention/backends/cpu_attn.py

[7cdf7e2fe] maang 2026-01-09 [Model] Remove redundant None check in DeepSeekOCR image input processing (#32016)
10	13	vllm/model_executor/models/deepseek_ocr.py

[bbf80ede4] Adolfo Victoria 2026-01-09 Fix type error (#31999)
3	1	vllm/entrypoints/responses_utils.py

[4505849b3] inkcherry 2026-01-09 [ROCm][PD] add moriio kv connector. (#29304)
18	1	docker/Dockerfile.rocm_base
17	2	docs/getting_started/installation/gpu.rocm.inc.md
320	0	examples/online_serving/disaggregated_serving/moriio_toy_proxy_server.py
545	0	tests/v1/kv_connector/unit/test_moriio_connector.py
6	0	vllm/distributed/kv_transfer/kv_connector/factory.py
0	0	vllm/distributed/kv_transfer/kv_connector/v1/moriio/__init__.py
321	0	vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_common.py
1515	0	vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_connector.py
609	0	vllm/distributed/kv_transfer/kv_connector/v1/moriio/moriio_engine.py
18	0	vllm/envs.py

[db07433ce] Roger Wang 2026-01-09 [Misc] Skip hashing kwargs if value is `None` (#32025)
3	0	vllm/multimodal/hasher.py

[e02706d2d] Andreas Karatzas 2026-01-09 [ROCm][CI][V1] Fix `nixl_connector` test failure and achieve CUDA parity in `test_async_scheduling` (#32000)
3	23	tests/v1/e2e/test_async_scheduling.py
20	16	tests/v1/kv_connector/unit/test_nixl_connector.py
5	0	vllm/v1/spec_decode/eagle.py

[b474782ad] Sophie du Couédic 2026-01-09 [Feature][Benchmarks] Custom dataset: read output length from dataset (#31881)
26	5	vllm/benchmarks/datasets.py

[55212c140] Bofeng Xue 2026-01-09 fix: remove duplicate engine_id check in nixl_connector (#31948)
0	7	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[e7b68f4d6] Xin Yang 2026-01-09 [Bugfix] Fix Triton FusedMoE LoRA (#30585)
1	0	pyproject.toml
47	34	tests/lora/test_gptoss_tp.py
5	3	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py

[1a19e9cd8] vllmellm 2026-01-09 [Bugfix][ROCm]Fix Qwen3-Next-80B-A3B-Thinking inference and optimize non-standard block size (544) support under rocm_atten (#31380)
33	2	tests/kernels/attention/test_prefix_prefill.py
83	28	vllm/attention/ops/chunked_prefill_paged_decode.py
80	32	vllm/attention/ops/prefix_prefill.py
52	12	vllm/attention/ops/triton_reshape_and_cache_flash.py
35	10	vllm/v1/attention/backends/rocm_attn.py

[c8ed39b9d] Cyrus Leung 2026-01-09 [Model] Reorganize pooling layers (#31973)
1	1	.github/CODEOWNERS
2	1	tests/model_executor/test_model_load_with_params.py
2	7	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
4	0	vllm/config/pooler.py
0	845	vllm/model_executor/layers/pooler.py
5	0	vllm/model_executor/layers/pooler/__init__.py
39	0	vllm/model_executor/layers/pooler/abstract.py
162	0	vllm/model_executor/layers/pooler/activations.py
27	0	vllm/model_executor/layers/pooler/common.py
45	0	vllm/model_executor/layers/pooler/seqwise/__init__.py
157	0	vllm/model_executor/layers/pooler/seqwise/heads.py
93	0	vllm/model_executor/layers/pooler/seqwise/methods.py
106	0	vllm/model_executor/layers/pooler/seqwise/poolers.py
128	0	vllm/model_executor/layers/pooler/special.py
39	0	vllm/model_executor/layers/pooler/tokwise/__init__.py
142	0	vllm/model_executor/layers/pooler/tokwise/heads.py
124	0	vllm/model_executor/layers/pooler/tokwise/methods.py
106	0	vllm/model_executor/layers/pooler/tokwise/poolers.py
5	23	vllm/model_executor/models/adapters.py
28	57	vllm/model_executor/models/bert.py
5	15	vllm/model_executor/models/bert_with_rope.py
2	7	vllm/model_executor/models/clip.py
2	14	vllm/model_executor/models/gpt2.py
18	29	vllm/model_executor/models/gritlm.py
2	4	vllm/model_executor/models/internlm2.py
2	14	vllm/model_executor/models/jamba.py
2	14	vllm/model_executor/models/jina_vl.py
20	51	vllm/model_executor/models/modernbert.py
4	7	vllm/model_executor/models/qwen2_rm.py
6	18	vllm/model_executor/models/roberta.py
2	7	vllm/model_executor/models/siglip.py
2	2	vllm/model_executor/models/terratorch.py
7	24	vllm/model_executor/models/transformers/pooling.py
1	3	vllm/v1/outputs.py

[020732800] Andreas Karatzas 2026-01-09 [Bugfix] Fix OpenAPI schema test failures (#31921)
5	1	tests/entrypoints/openai/test_openai_schema.py
6	2	vllm/entrypoints/openai/protocol.py

[dc77cb712] Alex Brooks 2026-01-09 [Bugfix] Fix Var Length Batched Padding in Granite Speech (#31906)
8	3	vllm/model_executor/models/granite_speech.py

[bde38c11d] gnovack 2026-01-08 fix lora moe sharding when rank < max_lora_rank (#31994)
0	1	tests/lora/test_gptoss_tp.py
6	7	vllm/lora/layers/fused_moe.py

[707b240d7] Xin Yang 2026-01-08 [Bugfix] Fix FusedMoE LoRA w2_output_size (#31949)
1	1	vllm/lora/layers/fused_moe.py

[29ce48221] Nick Hill 2026-01-08 [Cleanup] Remove obsolete spec decoding compatibility logic (#32003)
0	3	benchmarks/benchmark_ngram_proposer.py
45	13	tests/v1/sample/test_logprobs.py
0	20	tests/v1/spec_decode/test_ngram.py
0	10	vllm/v1/spec_decode/ngram_proposer.py
0	6	vllm/v1/spec_decode/suffix_decoding.py
0	14	vllm/v1/spec_decode/utils.py
0	7	vllm/v1/worker/gpu_input_batch.py
0	2	vllm/v1/worker/gpu_model_runner.py

[7a05d2dc6] TJian 2026-01-09 [CI] [ROCm] Fix `tests/entrypoints/test_grpc_server.py` on ROCm (#31970)
1	1	requirements/common.txt
1	1	requirements/rocm-test.txt
1	0	requirements/rocm.txt
14	1	setup.py
1	1	tests/entrypoints/test_grpc_server.py

[a1648c404] Divakar Verma 2026-01-08 [ROCm][CI] Fix test_token_classification.py::test_bert_models (#31993)
12	4	tests/models/language/pooling/test_token_classification.py

[e2d49ec2a] RioS 2026-01-09 [Bugfix] missing tokens occur in harmony streaming (#30437)
6	0	vllm/entrypoints/context.py
7	7	vllm/entrypoints/openai/serving_responses.py

[8413868da] Xin Yang 2026-01-08 [Bugfix] Fix typo in FusedMoE LoRA reshape comment (#31992)
1	1	vllm/lora/model_manager.py

[8ff4a9956] zhrrr 2026-01-09 [Async][Feat] support apply penalty or bad_words for async + spec (#30495)
12	0	tests/v1/e2e/test_async_scheduling.py
0	17	vllm/v1/engine/input_processor.py
34	3	vllm/v1/worker/gpu_input_batch.py
24	14	vllm/v1/worker/gpu_model_runner.py

[a4ec0c559] daniel-salib 2026-01-08 [Frontend] Add MCP tool streaming support to Responses API (#31761)
285	194	tests/entrypoints/openai/test_response_api_mcp_tools.py
233	1	tests/entrypoints/openai/test_response_api_with_harmony.py
869	434	vllm/entrypoints/openai/serving_responses.py

[0fa8dd24d] Robert Shaw 2026-01-08 [Bugfix] Fix Typo from NVFP4 Refactor (#31977)
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutedsl-deepep-ll.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutedsl-deepep-ll.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml
4	2	tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt
1	0	tests/evals/gsm8k/configs/moe-refactor/config-test.txt
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[6ebe34d6f] Max Hu 2026-01-08 [Feature] Add iteration level logging and enhance nvtx marker (#31193)
6	0	vllm/config/observability.py
8	0	vllm/engine/arg_utils.py
15	0	vllm/v1/core/sched/output.py
39	3	vllm/v1/engine/core.py
52	0	vllm/v1/utils.py
18	7	vllm/v1/worker/gpu_worker.py

[11cec296d] Nick Hill 2026-01-08 [BugFix] Add spec-decode-incompatible request param validation (#31982)
0	1	tests/v1/distributed/test_eagle_dp.py
12	2	vllm/v1/engine/input_processor.py

[5825bbc1f] Robert Shaw 2026-01-08 [Quantization] Deprecate Long Tail of Schemes (#31688)
4	1	tests/compile/fullgraph/test_full_graph.py
4	1	tests/models/quantization/test_gptq_marlin_24.py
3	1	tests/quantization/test_auto_round.py
5	1	tests/quantization/test_experts_int8.py
5	1	tests/quantization/test_rtn.py
17	0	vllm/config/model.py
6	0	vllm/engine/arg_utils.py
17	0	vllm/model_executor/layers/quantization/__init__.py

[d62cfe546] Yongye Zhu 2026-01-08 [MoE Refactoring][Bugfix]Wrap WNA16 Triton kernel into mk and change compressed tensor kernel selection (#31752)
2	0	vllm/model_executor/layers/fused_moe/__init__.py
144	2	vllm/model_executor/layers/fused_moe/fused_moe.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[6cdf015c3] Lucas Wilkinson 2026-01-08 [Misc] Fix `Current vLLM config is not set.` warnings, assert to avoid issues in the future (#31747)
29	24	tests/compile/distributed/test_async_tp.py
1	1	tests/compile/test_config.py
11	0	tests/conftest.py
1	1	tests/kernels/attention/test_flashinfer_trtllm_attention.py
3	1	tests/kernels/attention/test_mha_attn.py
2	0	tests/kernels/core/test_activation.py
1	0	tests/kernels/core/test_fused_qk_norm_rope.py
1	0	tests/kernels/core/test_fused_quant_layernorm.py
1	0	tests/kernels/core/test_layernorm.py
2	0	tests/kernels/core/test_mrope.py
2	1	tests/kernels/core/test_pos_encoding.py
1	0	tests/kernels/core/test_rotary_embedding.py
2	6	tests/kernels/moe/test_cpu_fused_moe.py
6	1	tests/kernels/moe/test_moe.py
10	3	tests/kernels/quantization/test_fp8_quant_group.py
1	1	tests/kernels/quantization/test_int8_kernel.py
1	0	tests/kernels/quantization/test_silu_mul_nvfp4_quant.py
1	0	tests/kernels/test_fused_quant_activation.py
2	2	tests/lora/conftest.py
8	5	tests/lora/test_layers.py
13	7	tests/lora/test_lora_manager.py
1	1	tests/model_executor/test_eagle_quantization.py
3	1	tests/models/multimodal/pooling/test_intern_vit.py
3	1	tests/models/multimodal/pooling/test_radio.py
9	9	tests/models/multimodal/processing/test_tensor_schema.py
1	1	tests/plugins_tests/test_platform_plugins.py
1	0	tests/quantization/test_fp8.py
27	6	tests/utils.py
1	1	tests/v1/attention/test_attention_backends.py
6	1	tests/v1/attention/test_attention_backends_selection.py
5	1	tests/v1/attention/test_mla_backends.py
6	1	tests/v1/attention/test_sparse_mla_backends.py
12	6	tests/v1/determinism/test_rms_norm_batch_invariant.py
93	83	tests/v1/kv_connector/unit/test_nixl_connector.py
1	0	tests/v1/kv_offload/test_cpu_gpu.py
16	15	tests/v1/worker/test_gpu_model_runner.py
8	8	tests/v1/worker/test_utils.py
6	3	vllm/attention/utils/fa_utils.py
2	0	vllm/config/__init__.py
12	7	vllm/config/vllm.py
2	2	vllm/distributed/device_communicators/base_device_communicator.py
2	2	vllm/distributed/device_communicators/quick_all_reduce.py
6	6	vllm/distributed/parallel_state.py
23	5	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
16	13	vllm/model_executor/layers/fused_moe/layer.py
7	6	vllm/model_executor/models/keye.py
8	6	vllm/model_executor/models/siglip2navit.py
4	2	vllm/v1/worker/gpu_worker.py

[5d3b6097a] Dipika Sikka 2026-01-08 [Compressed-Tensors] Simplify NVFP4 Conditions, enable marlin support for NVFP4A16 MoEs (#30881)
11	38	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
31	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[e74698c27] bnellnm 2026-01-08 [Misc][Refactor] Add FusedMoERouter object (#30519)
1	1	tests/test_routing_simulator.py
4	0	vllm/model_executor/layers/fused_moe/__init__.py
4	0	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
3	1	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
40	0	vllm/model_executor/layers/fused_moe/fused_moe_router.py
25	3	vllm/model_executor/layers/fused_moe/layer.py
7	1	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
3	1	vllm/model_executor/layers/quantization/awq_marlin.py
7	2	vllm/model_executor/layers/quantization/bitsandbytes.py
14	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
3	1	vllm/model_executor/layers/quantization/experts_int8.py
3	1	vllm/model_executor/layers/quantization/fp8.py
7	2	vllm/model_executor/layers/quantization/gguf.py
3	1	vllm/model_executor/layers/quantization/gptq_marlin.py
4	0	vllm/model_executor/layers/quantization/ipex_quant.py
15	8	vllm/model_executor/layers/quantization/modelopt.py
3	1	vllm/model_executor/layers/quantization/moe_wna16.py
6	3	vllm/model_executor/layers/quantization/mxfp4.py
6	2	vllm/model_executor/layers/quantization/quark/quark_moe.py
7	2	vllm/model_executor/layers/quantization/rtn.py

[aa125ecf0] Cyrus Leung 2026-01-09 [Frontend] Improve error message (#31987)
10	0	tests/entrypoints/test_utils.py
4	4	vllm/entrypoints/openai/api_server.py
6	0	vllm/entrypoints/utils.py

[f16bfbe5b] Lucas Kabela 2026-01-08 [Documentation][torch.compile] Add documentation for torch.compile + multimodal encoders (#31627)
111	0	docs/design/torch_compile_multimodal.md

[87e07a6b4] Michael Goin 2026-01-08 Revert "feat(moe): Add is_act_and_mul=False support for Triton MoE kernels" (#31978)
0	129	tests/kernels/moe/test_triton_moe_no_act_mul.py
0	9	vllm/model_executor/layers/fused_moe/config.py
3	10	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	7	vllm/model_executor/layers/fused_moe/fused_moe.py
2	8	vllm/model_executor/layers/fused_moe/layer.py
2	23	vllm/model_executor/layers/fused_moe/modular_kernel.py
0	5	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[750824324] Woosuk Kwon 2026-01-08 [Model Runner V2] Simplify BlockTables with UVA (#31965)
46	122	vllm/v1/worker/gpu/block_table.py
6	32	vllm/v1/worker/gpu/model_runner.py

[83e1c76db] Nicolò Lucchesi 2026-01-08 [CI][ROCm] Fix NIXL tests on ROCm (#31728)
15	1	.buildkite/test-amd.yaml
4	4	.buildkite/test-pipeline.yaml
1	1	.buildkite/test_areas/distributed.yaml

[a563866b4] Nishidha Panpaliya 2026-01-08 Fix ijson build for Power. (#31702)
5	5	docker/Dockerfile.ppc64le

[a3d909ad2] Nick Hill 2026-01-08 [Misc] Tidy up some spec decode logic in GPUModelRunner (#31591)
4	0	vllm/v1/worker/gpu/model_runner.py
46	49	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py

[49568d5cf] Jee Jee Li 2026-01-09 [Doc] Improve MM models LoRA notes (#31979)
1	23	docs/models/supported_models.md

[b8112c1d8] danisereb 2026-01-08 [Bugfix] Fix vllm serve failure with Nemotron Nano V3 FP8 (#31960)
4	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[eaba8ece7] Chauncey 2026-01-08 [Bugfix]: Fix Step3ReasoningParser missing is_reasoning_end_streaming (#31969)
6	0	vllm/reasoning/step3_reasoning_parser.py

[fe86be66c] yxing-bj 2026-01-08 [Model] Support IQuestCoder model (#31575)
2	0	docs/models/supported_models.md
6	0	tests/models/registry.py
595	0	vllm/model_executor/models/iquest_loopcoder.py
2	0	vllm/model_executor/models/registry.py

[1da3a5441] Chauncey 2026-01-08 [Docs]: update claude code url (#31971)
1	1	docs/serving/integrations/claude_code.md

[72c068b8e] TJian 2026-01-08 [CI] [Bugfix] Fix unbounded variable in `run-multi-node-test.sh` (#31967)
1	1	.buildkite/scripts/run-multi-node-test.sh
1	0	.buildkite/test-pipeline.yaml

[7645bc524] Mary 2026-01-08 [OpenAI] Fix tool_choice=required streaming when output has trailing extra data (#31610)
33	0	tests/tool_use/test_tool_choice_required.py
8	2	vllm/entrypoints/openai/serving_chat.py

[1123a8789] Ce Zhao 2026-01-08 [Model] Enable LoRA support for Pixtral (#31724)
1	1	docs/models/supported_models.md
29	2	vllm/model_executor/models/pixtral.py

[03fd76c57] tianshu-Michael-yu 2026-01-08 [Model] Add LFM2-VL model support (#31758)
1	0	docs/models/supported_models.md
33	1	examples/offline_inference/vision_language.py
4	0	tests/models/registry.py
732	0	vllm/model_executor/models/lfm2_vl.py
1	0	vllm/model_executor/models/registry.py
495	0	vllm/model_executor/models/siglip2.py

[59d260f5e] Bijaya Dangol 2026-01-08 [Model] Add Grok-2  (#31847)
4	0	docs/models/supported_models.md
43	0	tests/models/language/generation/test_grok.py
1	0	tests/models/registry.py
5	0	tests/tokenizers_/test_basic.py
267	19	vllm/model_executor/models/grok1.py
2	1	vllm/model_executor/models/registry.py
443	0	vllm/tokenizers/grok2.py
12	0	vllm/tokenizers/registry.py

[18d4e481d] Patrick von Platen 2026-01-08 [Voxtral] Fix speech transcription api (#31388)
3	2	vllm/config/speech_to_text.py
13	1	vllm/entrypoints/openai/speech_to_text.py
16	8	vllm/model_executor/models/voxtral.py
73	12	vllm/model_executor/models/voxtral_streaming.py
9	4	vllm/model_executor/models/whisper.py

[2972a0547] Isotr0py 2026-01-08 [MM Encoder]: Make MMEncoderAttention's `scale` takes effect properly  (#31950)
2	0	vllm/attention/layers/mm_encoder_attention.py
21	8	vllm/attention/ops/vit_attn_wrappers.py
1	0	vllm/model_executor/models/dots_ocr.py
1	0	vllm/model_executor/models/ernie45_vl.py
1	0	vllm/model_executor/models/glm4_1v.py
1	0	vllm/model_executor/models/glmasr.py
1	0	vllm/model_executor/models/isaac.py
1	0	vllm/model_executor/models/moonvit.py
1	0	vllm/model_executor/models/paddleocr_vl.py
1	0	vllm/model_executor/models/qwen2_5_vl.py
1	0	vllm/model_executor/models/qwen2_vl.py

[5576227bc] Cyrus Leung 2026-01-08 [Model] Standardize common vision encoders (#31947)
57	12	vllm/model_executor/models/clip.py
3	0	vllm/model_executor/models/deepencoder.py
1	0	vllm/model_executor/models/deepseek_ocr.py
9	10	vllm/model_executor/models/hyperclovax_vision.py
1	1	vllm/model_executor/models/isaac.py
2	0	vllm/model_executor/models/keye.py
3	1	vllm/model_executor/models/lightonocr.py
7	2	vllm/model_executor/models/llava.py
3	1	vllm/model_executor/models/llava_next.py
3	1	vllm/model_executor/models/llava_next_video.py
2	1	vllm/model_executor/models/llava_onevision.py
2	1	vllm/model_executor/models/minimax_vl_01.py
5	2	vllm/model_executor/models/mistral3.py
2	42	vllm/model_executor/models/paddleocr_vl.py
31	31	vllm/model_executor/models/phi3v.py
38	7	vllm/model_executor/models/pixtral.py
73	55	vllm/model_executor/models/siglip.py
3	5	vllm/model_executor/models/siglip2navit.py
9	2	vllm/model_executor/models/tarsier.py

[d1b6fe007] Cyrus Leung 2026-01-08 [Chore] Further cleanup pooler (#31951)
3	8	tests/model_executor/test_model_load_with_params.py
1	2	tests/test_config.py
1	2	vllm/config/pooler.py
38	44	vllm/model_executor/layers/pooler.py
1	2	vllm/model_executor/models/bert.py
1	2	vllm/model_executor/models/modernbert.py
2	2	vllm/v1/outputs.py

[04a49669d] omer-dayan 2026-01-08 RayLLM Bugfix - Preserve obj store URL for multi engine_config creation (#30803)
8	0	vllm/config/model.py
3	0	vllm/engine/arg_utils.py
2	2	vllm/model_executor/model_loader/runai_streamer_loader.py
2	2	vllm/model_executor/model_loader/sharded_state_loader.py

[96fcd3c26] BingjiaWang 2026-01-08 [Misc] Support qwen3-next lora (#31719)
7	1	vllm/model_executor/models/qwen3_next.py

[1f214290d] DevByteAI 2026-01-08 fix(compile): apply partition wrapper when loading AOT cached functions (#31536)
95	0	tests/compile/test_aot_compile.py
8	3	vllm/compilation/decorators.py

[8cbdc7eb9] Ryan Rock 2026-01-08 [CI/Build] Enable test_kv_cache_events_dp for AMD (#31834)
5	2	tests/v1/engine/test_engine_core_client.py

[b634e619b] Lumosis 2026-01-08 Decouple page_size_bytes calculation in AttentionSpec for TPU/RPA Compatibility. (#31635)
3	1	tests/v1/core/test_kv_sharing.py
32	9	tests/v1/core/test_prefix_caching.py
7	1	tests/v1/core/test_scheduler.py
7	1	tests/v1/core/utils.py
7	1	tests/v1/kv_connector/unit/utils.py
19	7	vllm/v1/kv_cache_interface.py

[eac3b96ec] Isotr0py 2026-01-08 [Models] Allow converting Qwen3-VL into Reranker model (#31890)
9	0	docs/models/supported_models.md
31	0	examples/pooling/pooling/vision_language_pooling.py
23	0	examples/pooling/score/template/qwen3_vl_reranker.jinja
172	0	examples/pooling/score/vision_language_reranker.py
9	0	tests/models/registry.py
2	0	vllm/entrypoints/score_utils.py
36	13	vllm/model_executor/models/adapters.py
5	0	vllm/model_executor/models/config.py

[573a1d111] Zhiwei 2026-01-08 [ROCm]Skip test_torchao.py::test_pre_quantized_model on CDNA3 arch (#31905)
6	0	tests/quantization/test_torchao.py

[33156f56e] Shang Wang 2026-01-08 [docker] A follow-up patch to fix #30913: `[docker] install cuda13 version of lmcache and nixl` (#31775)
2	0	docker/Dockerfile

[107cf8e92] Rabi Mishra 2026-01-08  fix(rocm): Add get_supported_kernel_block_sizes() to ROCM_ATTN (#31712)
8	0	vllm/v1/attention/backends/rocm_attn.py

[63baa28cf] Zyyeric 2026-01-08 [Model] Enable LoRA support for tower and connector in GLM4-V (#31652)
14	0	vllm/model_executor/models/glm4_1v.py

[e5173d3ba] Andy Liu 2026-01-07 [Bugfix] Remove the num_hidden_layers override for glm4_moe (#31745)
0	1	vllm/config/speculative.py

[d3235cb50] prashanth058 2026-01-08 [Fix] Enable mm_processor_cache with vision LoRA (#31927)
39	0	tests/multimodal/test_cache.py
0	13	vllm/engine/arg_utils.py
7	2	vllm/multimodal/cache.py
3	0	vllm/multimodal/inputs.py
3	1	vllm/v1/engine/input_processor.py

[287b37cda] Nick Hill 2026-01-07 [BugFix] Fix spec decoding edge case bugs (#31944)
1	0	vllm/v1/core/sched/scheduler.py
26	0	vllm/v1/worker/gpu_input_batch.py
22	46	vllm/v1/worker/gpu_model_runner.py

[791b2fc30] Chang Su 2026-01-07 [grpc] Support gRPC server entrypoint (#30190)
5	0	.gitignore
3	1	mkdocs.yaml
5	0	pyproject.toml
2	0	requirements/build.txt
3	1	requirements/common.txt
3	2	requirements/test.txt
71	2	setup.py
428	0	tests/entrypoints/test_grpc_server.py
531	0	vllm/entrypoints/grpc_server.py
17	0	vllm/grpc/__init__.py
94	0	vllm/grpc/compile_protos.py
195	0	vllm/grpc/vllm_engine.proto

[be6a81f31] Lucas Wilkinson 2026-01-08 [chore] Update FA commit (#30460)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[2ab441bef] Ronald 2026-01-08 [platform] add dp_metadata arg to set_additional_forward_context (#31942)
1	0	vllm/forward_context.py

[9572f74f1] ShaanveerS 2026-01-08 [Model] Enable LoRA support for tower and connector in DotsOCR (#31825)
1	1	docs/models/supported_models.md
8	0	vllm/model_executor/models/dots_ocr.py

[5f2a473ff] Andreas Karatzas 2026-01-08 [ROCm][CI] v1 cpu offloading attention backend fix (#31833)
4	2	tests/v1/kv_offload/test_cpu_offloading.py

[6b2a672e4] Michael Goin 2026-01-08 [Doc] Add Claude code usage example (#31188)
-	-	docs/assets/deployment/claude-code-example.png
75	0	docs/serving/integrations/claude_code.md

[f1b1bea5c] rasmith 2026-01-07 [CI][BugFix][AMD] Actually skip tests marked @pytest.mark.skip_v1 (#31873)
1	2	.buildkite/test-amd.yaml

[cddbc2b4b] Charlie Fu 2026-01-07 [ROCm][CI] Add rocm support for run-multi-node-test.sh (#31922)
21	3	.buildkite/scripts/run-multi-node-test.sh

[087a13896] Andreas Karatzas 2026-01-07 [ROCm][CI] Fix attention backend test flakiness from uninitialized KV cache memory (#31928)
1	1	tests/v1/attention/test_attention_backends.py

[c4041f37a] Andreas Karatzas 2026-01-07 [ROCm][LoRA] Fix MoE accuracy regression by preserving float32 router weight scaling (#31931)
8	4	vllm/model_executor/layers/fused_moe/fused_moe.py

[a79079fee] Richard Zou 2026-01-07 [BugFix] Fix flakiness in test_eagle_dp for PyTorch 2.10 (#31915)
3	1	tests/v1/distributed/test_eagle_dp.py

[9f6dcb71a] Robert Shaw 2026-01-07 [MoE Refactor][16/N] Apply Refactor to NVFP4 (#31692)
34	19	benchmarks/kernels/benchmark_cutlass_moe_nvfp4.py
1	1	docs/design/moe_kernel_features.md
20	10	tests/kernels/moe/test_nvfp4_moe.py
0	2	vllm/model_executor/layers/fused_moe/__init__.py
23	0	vllm/model_executor/layers/fused_moe/config.py
0	62	vllm/model_executor/layers/fused_moe/cutlass_moe.py
0	39	vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py
7	11	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
5	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	9	vllm/model_executor/layers/fused_moe/layer.py
280	0	vllm/model_executor/layers/fused_moe/oracle/nvfp4.py
115	240	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
87	285	vllm/model_executor/layers/quantization/modelopt.py
106	4	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
101	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py

[8dd2419fa] Andreas Karatzas 2026-01-07 [CI] Skip Qwen-VL in multimodal processing tests due to flaky external dependency (#31932)
5	0	tests/models/multimodal/processing/test_common.py

[39d82005f] Rabi Mishra 2026-01-08 fix(rocm): add early return in get_flash_attn_version for ROCm (#31286)
3	0	vllm/attention/utils/fa_utils.py

[25eef3dc2] Rabi Mishra 2026-01-08 feat(moe): Add is_act_and_mul=False support for Triton MoE kernels (#31645)
129	0	tests/kernels/moe/test_triton_moe_no_act_mul.py
9	0	vllm/model_executor/layers/fused_moe/config.py
10	3	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
7	2	vllm/model_executor/layers/fused_moe/fused_moe.py
8	2	vllm/model_executor/layers/fused_moe/layer.py
23	2	vllm/model_executor/layers/fused_moe/modular_kernel.py
5	0	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[0d7667419] Matthew Bonanni 2026-01-07 [0/N][Attention] Fix miscellaneous pre-commit issues (#31924)
1	1	vllm/attention/layers/static_sink_attention.py
5	2	vllm/attention/ops/flashmla.py
6	2	vllm/attention/ops/paged_attn.py
2	2	vllm/attention/ops/prefix_prefill.py
3	3	vllm/attention/ops/rocm_aiter_mla_sparse.py
1	4	vllm/attention/ops/triton_decode_attention.py
3	3	vllm/attention/ops/triton_prefill_attention.py
14	8	vllm/attention/utils/fa_utils.py

[5dcd7ef1f] Robert Shaw 2026-01-07 [MoE Refactor][15/N] Apply Refactor to Fp8 (#31415)
7	0	.buildkite/test-pipeline.yaml
24	92	benchmarks/kernels/benchmark_cutlass_moe_fp8.py
37	53	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
0	2	benchmarks/kernels/benchmark_moe.py
1	1	docs/design/moe_kernel_features.md
5	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Llama-4-Scout-Fp8-ModelOpt-triton.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ht.yaml
9	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm-deepep-ll.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ht.yaml
9	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm-deepep-ll.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor-dp-ep/config-b200.txt
5	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-CT-vllm-cutlass.yaml
0	0	tests/evals/gsm8k/configs/moe-refactor/{Qwen3-30B-A3B-Fp8-CT-Block-vllm-cutlass.yaml => Qwen3-30B-A3B-Fp8-CT-Block-triton.yaml}
1	0	tests/evals/gsm8k/configs/moe-refactor/config-b200.txt
1	1	tests/evals/gsm8k/configs/moe-refactor/config-h100.txt
1	0	tests/evals/gsm8k/test_gsm8k_correctness.py
46	26	tests/kernels/moe/test_cutlass_moe.py
37	12	tests/kernels/moe/test_flashinfer.py
12	7	tests/quantization/test_fp8.py
0	2	vllm/model_executor/layers/fused_moe/__init__.py
17	145	vllm/model_executor/layers/fused_moe/cutlass_moe.py
126	0	vllm/model_executor/layers/fused_moe/fallback.py
5	5	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
2	0	vllm/model_executor/layers/fused_moe/oracle/__init__.py
358	0	vllm/model_executor/layers/fused_moe/oracle/fp8.py
75	0	vllm/model_executor/layers/fused_moe/triton_cutlass_moe.py
16	107	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
135	355	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
112	384	vllm/model_executor/layers/quantization/fp8.py
147	264	vllm/model_executor/layers/quantization/modelopt.py
3	4	vllm/model_executor/layers/quantization/quark/quark_moe.py
119	53	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
71	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py
16	17	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[ffc0a2798] Elvir Crnčević 2026-01-07 Add back missing DeepEP LL params (#31911)
2	0	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py

[10ef65ede] Nick Hill 2026-01-07 [BugFix] Fix bad words with speculative decoding (#31908)
13	14	tests/v1/sample/test_rejection_sampler.py
13	8	vllm/v1/sample/ops/bad_words.py

[6170d47d2] Ilya Markov 2026-01-07 [EPLB] Optimize EPLB with numpy (#29499)
140	0	tests/distributed/test_eplb_algo.py
9	8	tests/distributed/test_eplb_execute.py
12	0	vllm/config/parallel.py
1	1	vllm/distributed/eplb/async_worker.py
60	32	vllm/distributed/eplb/eplb_state.py
4	1	vllm/distributed/eplb/policy/abstract.py
124	12	vllm/distributed/eplb/policy/default.py
388	218	vllm/distributed/eplb/rebalance_execute.py

[0ada960a2] Xin Yang 2026-01-07 [Kernel] Support bias type in grouped_topk kernel (#31781)
95	63	csrc/moe/grouped_topk_kernels.cu
7	7	tests/kernels/moe/test_grouped_topk.py
2	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[c907d2215] Ning Xie 2026-01-08 [refactor] refactor memory constants usage (#31865)
0	1	tests/basic_correctness/test_cumem.py
5	4	vllm/config/cache.py
3	2	vllm/multimodal/cache.py
3	3	vllm/platforms/cpu.py
7	3	vllm/utils/mem_utils.py
5	5	vllm/v1/core/kv_cache_utils.py
3	4	vllm/v1/worker/gpu/model_runner.py
3	4	vllm/v1/worker/gpu_model_runner.py
4	4	vllm/v1/worker/gpu_worker.py

[f347ac6c3] Michael Goin 2026-01-07 [Perf] Fuse stride preparation for NVFP4 cutlass_moe (#31837)
30	18	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu

[05f47bd8d] Festus Ayobami Owumi 2026-01-07 [Doc] Fix: Correct vLLM announcing blog post link in docs (#31868)
1	1	docs/README.md

[bf184a662] roikoren755 2026-01-07 Enable quantized attention in NemotronH models (#31898)
3	0	vllm/model_executor/model_loader/weight_utils.py
1	0	vllm/model_executor/models/nemotron_h.py

[30399cc72] Jee Jee Li 2026-01-08 UX: add vLLM env info in '/server_info' (#31899)
19	3	vllm/entrypoints/serve/instrumentator/server_info.py

[b89443b8d] Kfir Toledo 2026-01-07 [KVConnector]: Enable Cross-layers KV cache layout for MultiConnector (#30761)
45	0	tests/v1/kv_connector/unit/test_multi_connector.py
8	8	vllm/distributed/kv_transfer/kv_connector/v1/base.py
14	1	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
4	2	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py

[1d9e9ae8a] Marko Rosenmueller 2026-01-07 [Bugfix]: prevent leaking tokens in crash log (#30751)
25	0	vllm/v1/core/sched/output.py

[b7036c87a] Cyrus Leung 2026-01-08 [Refactor] Clean up pooler modules (#31897)
102	81	vllm/model_executor/layers/pooler.py
19	14	vllm/model_executor/models/bert.py
17	9	vllm/model_executor/models/gritlm.py
18	13	vllm/model_executor/models/modernbert.py
4	2	vllm/v1/outputs.py
6	0	vllm/v1/pool/metadata.py
1	1	vllm/v1/worker/gpu_model_runner.py

[cc6dafaef] Kate Cheng 2026-01-07 [Perf][Kernels] Enable FlashInfer DeepGEMM swapAB on SM90 (for W8A8 Linear Op) (#29213)
51	0	tests/kernels/quantization/test_block_fp8.py
6	0	vllm/envs.py
143	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py
57	0	vllm/utils/flashinfer.py

[1ab055efe] R3hankhan 2026-01-07 [OpenAI] Extend VLLMValidationError to additional validation parameters (#31870)
1	1	vllm/entrypoints/openai/api_server.py
1	30	vllm/entrypoints/openai/protocol.py
4	4	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/entrypoints/renderer.py
36	0	vllm/exceptions.py
31	12	vllm/sampling_params.py
13	6	vllm/v1/engine/input_processor.py

[b665bbc2d] Cyrus Leung 2026-01-07 [Chore] Migrate V0 attention utils (#31891)
1	1	tests/kernels/mamba/test_causal_conv1d.py
1	1	tests/kernels/mamba/test_mamba_ssm.py
0	33	vllm/attention/backends/utils.py
1	1	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
1	1	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
1	1	vllm/v1/attention/backends/gdn_attn.py
22	2	vllm/v1/attention/backends/mla/common.py
1	2	vllm/v1/attention/backends/mla/flashmla_sparse.py
1	4	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
1	1	vllm/v1/worker/gpu/block_table.py

[974138751] Jared Wen 2026-01-07 [Refactor] GLM-ASR Modeling (#31779)
644	36	vllm/model_executor/models/glmasr.py
28	5	vllm/model_executor/models/glmasr_utils.py

[41cfa5063] vllmellm 2026-01-07 [ROCm][AITER] fix wrong argument passed to  AITER `flash_attn_varlen_func` (#31880)
1	1	vllm/v1/attention/backends/mla/aiter_triton_mla.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[d111bc53a] Andy Liu 2026-01-07 [Bugfix][MTP] Fix GLM4 MoE fp8 loading with MTP on (#31757)
6	1	vllm/model_executor/models/glm4_moe_mtp.py

[0790f0769] BlankR 2026-01-07 [Misc] Improve error messages for unsupported types and parameters (#30593)
3	1	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
4	1	vllm/attention/ops/chunked_prefill_paged_decode.py
1	1	vllm/config/lora.py
4	1	vllm/distributed/device_communicators/pynccl_wrapper.py
4	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py
6	6	vllm/model_executor/layers/quantization/auto_round.py
4	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
4	1	vllm/model_executor/layers/quantization/mxfp4.py
5	1	vllm/model_executor/models/ernie45_vl.py
3	1	vllm/model_executor/models/granite_speech.py
2	1	vllm/model_executor/models/minimax_text_01.py

[1f33e38e8] maang 2026-01-07 [Model] Cleanup: Remove redundant manual definition of `make_empty_intermediate_tensors` in GLM-4-MoE (#31869)
0	14	vllm/model_executor/models/glm4_moe.py

[59fe6f298] sihao_li 2026-01-07 [XPU]fallback to TRITON_ATTN on xpu when use float32 dtype (#31762)
7	0	vllm/platforms/xpu.py

[e7596371a] weiyu 2026-01-07 [Refactor][TPU] Remove torch_xla path and use tpu-inference (#30808)
0	1	docs/design/moe_kernel_features.md
0	0	tests/tpu/__init__.py
0	0	tests/tpu/lora/__init__.py
0	139	tests/tpu/lora/test_lora.py
0	86	tests/tpu/test_compilation.py
0	34	tests/tpu/test_custom_dispatcher.py
0	88	tests/tpu/test_moe_pallas.py
0	52	tests/tpu/test_quantization_accuracy.py
0	0	tests/v1/tpu/__init__.py
0	177	tests/v1/tpu/test_basic.py
0	78	tests/v1/tpu/test_kv_cache_update_kernel.py
0	95	tests/v1/tpu/test_mha_attn.py
0	76	tests/v1/tpu/test_multimodal.py
0	100	tests/v1/tpu/test_pallas.py
0	150	tests/v1/tpu/test_perf.py
0	105	tests/v1/tpu/test_sampler.py
0	78	tests/v1/tpu/test_spmd_model_weight_loading.py
0	149	tests/v1/tpu/test_topk_topp_sampler.py
0	78	tests/v1/tpu/test_tpu_int8.py
0	93	tests/v1/tpu/test_tpu_qkv_linear.py
0	0	tests/v1/tpu/worker/__init__.py
0	587	tests/v1/tpu/worker/test_tpu_model_runner.py
0	1	vllm/attention/backends/registry.py
0	25	vllm/attention/layers/mm_encoder_attention.py
0	99	vllm/distributed/device_communicators/tpu_communicator.py
1	5	vllm/distributed/kv_transfer/kv_connector/utils.py
0	1	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py
1	7	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
0	188	vllm/distributed/tpu_distributed_utils.py
0	6	vllm/lora/ops/xla_ops/__init__.py
0	141	vllm/lora/ops/xla_ops/lora_ops.py
0	358	vllm/lora/punica_wrapper/punica_tpu.py
0	6	vllm/model_executor/layers/fused_moe/layer.py
0	83	vllm/model_executor/layers/fused_moe/moe_pallas.py
1	51	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
0	3	vllm/model_executor/layers/quantization/__init__.py
0	4	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
0	106	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
0	139	vllm/model_executor/layers/quantization/tpu_int8.py
0	17	vllm/model_executor/model_loader/default_loader.py
0	118	vllm/model_executor/model_loader/tpu.py
3	278	vllm/platforms/tpu.py
1	17	vllm/usage/usage_lib.py
0	436	vllm/v1/attention/backends/pallas.py
0	2196	vllm/v1/worker/tpu_model_runner.py
2	334	vllm/v1/worker/tpu_worker.py

[0dd5dee9b] xuebwang-amd 2026-01-07 [Bugfix][Kernel] fix bias adding in triton kernel implemented fused moe (#31676)
8	5	vllm/model_executor/layers/fused_moe/fused_moe.py

[4614c5a53] Kevin McKay 2026-01-07 [Bugfix][Hardware][AMD] Consolidate FP8 min/max values helper function (#31106)
11	25	tests/kernels/quant_utils.py
65	0	tests/kernels/quantization/test_fp8_min_max_helper.py
5	6	vllm/model_executor/layers/quantization/input_quant_fp8.py
5	7	vllm/model_executor/layers/quantization/utils/fp8_utils.py
11	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
5	1	vllm/utils/deep_gemm.py

[482914849] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2026-01-07 [BugFix] LoRA: Support loading base_layer of experts (#31104)
10	3	vllm/model_executor/layers/fused_moe/layer.py
1	0	vllm/model_executor/models/afmoe.py
1	0	vllm/model_executor/models/bailing_moe.py
1	0	vllm/model_executor/models/deepseek_eagle.py
1	0	vllm/model_executor/models/deepseek_mtp.py
2	0	vllm/model_executor/models/deepseek_v2.py
1	0	vllm/model_executor/models/dots1.py
1	0	vllm/model_executor/models/ernie45_moe.py
1	0	vllm/model_executor/models/ernie45_vl_moe.py
1	0	vllm/model_executor/models/glm4_moe.py
1	0	vllm/model_executor/models/glm4_moe_mtp.py
1	0	vllm/model_executor/models/gpt_oss.py
1	0	vllm/model_executor/models/granitemoe.py
1	0	vllm/model_executor/models/grok1.py
1	0	vllm/model_executor/models/hunyuan_v1.py
1	0	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/kimi_linear.py
1	0	vllm/model_executor/models/kimi_vl.py
1	0	vllm/model_executor/models/lfm2_moe.py
2	0	vllm/model_executor/models/llama4.py
1	0	vllm/model_executor/models/longcat_flash.py
1	0	vllm/model_executor/models/mimo_v2_flash.py
1	0	vllm/model_executor/models/minimax_m2.py
1	0	vllm/model_executor/models/mixtral.py
1	0	vllm/model_executor/models/mllama4.py
1	0	vllm/model_executor/models/nemotron_h.py
1	0	vllm/model_executor/models/olmoe.py
1	0	vllm/model_executor/models/openpangu.py
1	0	vllm/model_executor/models/openpangu_mtp.py
1	0	vllm/model_executor/models/phimoe.py
1	0	vllm/model_executor/models/qwen2_moe.py
1	0	vllm/model_executor/models/qwen3_moe.py
1	0	vllm/model_executor/models/qwen3_next.py
1	0	vllm/model_executor/models/qwen3_next_mtp.py
1	0	vllm/model_executor/models/transformers/moe.py

[efeaac92f] tianshu-Michael-yu 2026-01-06 [Bugfix] Fix race condition in async-scheduling for vlm model (#31841)
14	3	vllm/v1/worker/gpu_model_runner.py

[55caa6051] tjp_zju 2026-01-07 refactor: find_loaded_library (#31866)
1	27	vllm/device_allocator/cumem.py
1	27	vllm/distributed/device_communicators/cuda_wrapper.py
27	0	vllm/utils/system_utils.py

[c7a79d41a] Lucas Wilkinson 2026-01-07 [Attention][3/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31850)
2	2	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/rocm_attn.py

[6409004b2] vllmellm 2026-01-07 [ROCm][AITER] bugfix accuracy regression in ROCM_AITER_TRITON_MLA backend (#31816)
2	10	vllm/v1/attention/backends/mla/aiter_triton_mla.py

[aafd4d235] Cyrus Leung 2026-01-07 [Chore] Try remove `init_cached_hf_modules` (#31786)
1	5	tests/model_executor/model_loader/runai_streamer_loader/conftest.py
1	1	tests/model_executor/model_loader/tensorizer_loader/conftest.py
0	11	vllm/utils/import_utils.py
1	3	vllm/v1/executor/multiproc_executor.py
3	6	vllm/v1/executor/ray_executor.py
1	1	vllm/v1/executor/uniproc_executor.py
0	6	vllm/v1/worker/gpu_worker.py
0	6	vllm/v1/worker/tpu_worker.py
23	34	vllm/v1/worker/worker_base.py

[0a2c2dc3f] Jack Yang 2026-01-06 fixed mypy warnings for files vllm/v1/attention with TEMPORARY workaround (#31465)
1	1	tools/pre_commit/mypy.py
9	3	vllm/attention/backends/abstract.py
3	1	vllm/model_executor/layers/attention_layer_base.py
34	8	vllm/v1/attention/backends/flash_attn.py
9	1	vllm/v1/attention/backends/flash_attn_diffkv.py
6	5	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/flex_attention.py
11	4	vllm/v1/attention/backends/gdn_attn.py
3	2	vllm/v1/attention/backends/mamba2_attn.py
12	6	vllm/v1/attention/backends/mamba_attn.py
1	1	vllm/v1/attention/backends/mla/aiter_triton_mla.py
21	12	vllm/v1/attention/backends/mla/common.py
9	2	vllm/v1/attention/backends/mla/flashattn_mla.py
4	2	vllm/v1/attention/backends/mla/flashmla_sparse.py
2	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
4	3	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
3	1	vllm/v1/attention/backends/tree_attn.py
7	2	vllm/v1/attention/backends/utils.py

[f09c5feb7] Tyler Michael Smith 2026-01-06 Change warning in get_current_vllm_config to report caller's line number (#31855)
3	1	vllm/config/vllm.py

[1b8af957f] Cyrus Leung 2026-01-07 [Doc] Update release docs (#31799)
15	32	RELEASE.md

[a051525e0] Ce Zhao 2026-01-06 [Model] Enable LoRA support for PaliGemma (#31656)
1	1	docs/models/supported_models.md
23	2	vllm/model_executor/models/paligemma.py

[5b833be49] Yihua Cheng 2026-01-06 [1/2][lmcache connector] clean up lmcache multi-process adapter  (#31838)
9	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py
14	6	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py

[873480d13] Lucas Kabela 2026-01-06 [Misc][BE] Type coverage for vllm/compilation [1/3] (#31554)
13	13	vllm/compilation/backends.py
7	7	vllm/compilation/collective_fusion.py
24	24	vllm/compilation/compiler_interface.py
3	1	vllm/compilation/counter.py
1	0	vllm/compilation/cuda_graph.py
3	2	vllm/compilation/fx_utils.py
15	11	vllm/compilation/inductor_pass.py
4	4	vllm/compilation/monitor.py
4	1	vllm/compilation/partition_rules.py
7	7	vllm/compilation/sequence_parallelism.py
3	3	vllm/compilation/torch25_custom_graph_pass.py
19	12	vllm/compilation/vllm_inductor_pass.py

[6f351548b] vSeamar 2026-01-06 [Frontend] Implement robust video frame recovery for corrupted videos (#29197)
25	0	docs/features/multimodal_inputs.md
209	0	tests/multimodal/test_video.py
1	1	vllm/benchmarks/datasets.py
186	14	vllm/multimodal/video.py

[364a8bc6d] Andreas Karatzas 2026-01-06 [ROCm][CI] Fix plugin tests (2 GPUs) failures on ROCm and removing `VLLM_FLOAT32_MATMUL_PRECISION` from all ROCm tests (#31829)
3	9	.buildkite/test-amd.yaml
2	0	requirements/rocm-test.txt

[9a1d20a89] Angela Yi 2026-01-06 [CI] Add warmup run in test_fusion_attn (#31183)
16	8	tests/compile/test_fusion_attn.py

[309a8f66e] Cyrus Leung 2026-01-07 [Bugfix] Handle mistral tokenizer in get_hf_processor (#31817)
7	1	vllm/multimodal/processing.py

[e5d427e93] Andreas Karatzas 2026-01-06 [ROCm][CI] Pinning timm lib version to fix ImportError in Multi-Modal Tests (Nemotron) (#31835)
2	0	requirements/rocm-test.txt

[2a42ae790] Andreas Karatzas 2026-01-06 [ROCm][CI] Fix ModernBERT token classification test numerical accuracy on ROCm (#31820)
29	0	tests/models/language/pooling/conftest.py

[d49899732] Matthew Bonanni 2026-01-06 [Spec Decode][UX] Add acceptance stats to `vllm bench serve` report (#31739)
155	0	vllm/benchmarks/serve.py

[dba95378a] Elvir Crnčević 2026-01-06 Report error log after vllm bench serve (#31808)
6	0	vllm/benchmarks/serve.py

[ada6f91d5] Nikhil G 2026-01-06 Fix RecursionError in MediaWithBytes unpickling (#31191)
33	0	tests/multimodal/test_image.py
1	0	tools/pre_commit/check_pickle_imports.py
5	1	vllm/multimodal/base.py

[8becf146b] Li, Jiang 2026-01-07 [Quantization][Refactor] Move CPU GPTQ kernel into MP linear (#31801)
1	0	tests/quantization/test_cpu_wna16.py
0	1	vllm/config/model.py
1	3	vllm/model_executor/layers/quantization/__init__.py
0	326	vllm/model_executor/layers/quantization/cpu_wna16.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
4	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
126	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/cpu.py
8	1	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py
30	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[c07163663] Charlie Fu 2026-01-06 [ROCm][CI] Fix tests/compile unit tests (#28895)
20	11	tests/compile/fullgraph/test_basic_correctness.py
5	0	tests/compile/fullgraph/test_full_cudagraph.py
5	2	tests/compile/test_noop_elimination.py

[f7008ce1c] Benjamin Chislett 2026-01-06 [Perf] Async Scheduling + Speculative Decoding + Structured Outputs (#29821)
11	3	tests/v1/e2e/test_async_scheduling.py
3	0	vllm/v1/core/sched/async_scheduler.py
20	1	vllm/v1/core/sched/interface.py
7	0	vllm/v1/core/sched/output.py
54	7	vllm/v1/core/sched/scheduler.py
12	0	vllm/v1/engine/core.py
1	2	vllm/v1/engine/input_processor.py
76	41	vllm/v1/worker/gpu_model_runner.py

[4e67a8f61] Yakine Tahtah 2026-01-06 [Bugfix] Fix GLM-4 MoE router logits dtype for data parallel chunking (#31055)
6	0	vllm/model_executor/layers/fused_moe/config.py
6	1	vllm/model_executor/layers/fused_moe/layer.py
1	0	vllm/model_executor/models/glm4_moe.py

[142c4d173] Masataro Asai 2026-01-06 make 500: InternalServerError more informative (#20610)
11	3	vllm/v1/engine/async_llm.py

[6f5e65338] Ning Xie 2026-01-07 [Log] add log about gpu worker init snapshot and requested memory (#29493)
20	4	vllm/utils/mem_utils.py
30	30	vllm/v1/worker/gpu_worker.py
9	8	vllm/v1/worker/utils.py

[22dffca98] Vadim Gimpelson 2026-01-06 [PERF] Speed-up of GDN attention decode part (Qwen3-Next) (#31722)
1	1	vllm/model_executor/layers/fla/ops/fused_recurrent.py

[4c73be14e] Lucas Wilkinson 2026-01-06 [Attention][2/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31774)
1	3	vllm/v1/attention/backends/gdn_attn.py
4	1	vllm/v1/attention/backends/mamba2_attn.py
8	10	vllm/v1/attention/backends/mamba_attn.py

[2f4bdee61] Jinzhen Lin 2026-01-07 [Quantization][MoE] remove unused ep logic from moe marlin (#31571)
14	14	csrc/moe/marlin_moe_wna16/kernel.h
2	26	csrc/moe/marlin_moe_wna16/marlin_template.h
14	14	csrc/moe/marlin_moe_wna16/ops.cu
1	1	csrc/moe/torch_bindings.cpp
0	3	vllm/_custom_ops.py
0	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py

[28c94770a] roikoren755 2026-01-06 [NemotronH] Use ReplicatedLinear for fc1_latent_proj (#31807)
1	5	vllm/model_executor/models/nemotron_h.py

[af8fd7305] Robert Shaw 2026-01-06 [MoE Refactor][14/N] Clean Up FI Quant Config Smuggling (#31593)
12	7	tests/kernels/moe/test_flashinfer.py
7	4	vllm/model_executor/layers/fused_moe/config.py
4	4	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
4	3	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
74	10	vllm/model_executor/layers/quantization/fp8.py
38	13	vllm/model_executor/layers/quantization/modelopt.py
34	43	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[d3e477c01] Robert Shaw 2026-01-06 [MoE Refactor] Add Temporary Integration Tests - H100/B200 (#31759)
16	0	.buildkite/test-pipeline.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-cutlass.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-fi-trtllm.yaml
7	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-marlin.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Llama-4-Scout-Fp8-ModelOpt-triton.yaml
9	0	tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-fi-cutlass.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Mixtral-8x7B-Fp8-AutoFp8-triton.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-deepgemm.yaml
10	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-cutlass.yaml
10	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-fi-trtllm.yaml
9	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-marlin.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-AutoFp8-triton.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-deepgemm.yaml
10	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-fi-cutlass.yaml
9	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-marlin.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Block-vllm-cutlass.yaml
7	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Channel-marlin.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-Fp8-CT-Channel-vllm-cutlass.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass-dp-ep.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-cutlass.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-fi-trtllm.yaml
7	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-marlin.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-CT-vllm-cutlass.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass-dp-ep.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-cutlass.yaml
8	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-fi-trtllm.yaml
7	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-marlin.yaml
5	0	tests/evals/gsm8k/configs/moe-refactor/Qwen3-30B-A3B-NvFp4-ModelOpt-vllm-cutlass.yaml
12	0	tests/evals/gsm8k/configs/moe-refactor/config-b200.txt
13	0	tests/evals/gsm8k/configs/moe-refactor/config-h100.txt

[02809af1e] Isotr0py 2026-01-06 [Bugfix]: Fix cross attention backend selection for Turing GPU (#31806)
9	5	vllm/attention/layers/cross_attention.py

[cbd4690a0] Jee Jee Li 2026-01-06 [LoRA]Disable linear LoRA  kernel PDL (#31777)
1	1	docs/features/lora.md
7	3	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
3	3	vllm/lora/ops/triton_ops/lora_expand_op.py
3	3	vllm/lora/ops/triton_ops/lora_shrink_op.py

[96860af65] wang.yuqi 2026-01-06 [Model] rename use_pad_token to use_sep_token (#31784)
7	7	examples/pooling/score/convert_model_to_seq_cls.py
8	8	tests/entrypoints/pooling/score/test_utils.py
12	4	vllm/config/model.py
3	3	vllm/entrypoints/score_utils.py
3	3	vllm/model_executor/models/adapters.py

[0202971a4] Chauncey 2026-01-06 [Frontend] Support GLM-4.5 / GLM-4.7 with enable_thinking: false (#31788)
3	3	vllm/reasoning/deepseek_v3_reasoning_parser.py
4	3	vllm/reasoning/glm4_moe_reasoning_parser.py
4	3	vllm/reasoning/holo2_reasoning_parser.py

[2c1a4f248] Jzz1943 2026-01-06 [Bugfix]: avoid overriding audio/text kwargs (Qwen3-Omni) (#31790)
8	6	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[644482487] Cyrus Leung 2026-01-06 [Misc] Implement `TokenizerLike.convert_tokens_to_ids` (#31796)
11	1	vllm/tokenizers/deepseek_v32.py
10	1	vllm/tokenizers/mistral.py
10	1	vllm/tokenizers/protocol.py

[bf0f3a463] kzwrime 2026-01-06 [Bugfix] Fix torch.compile error for DP + MoE on CPU Backend (#31650)
2	2	vllm/model_executor/layers/fused_moe/layer.py

[e0327c9db] Lucas Wilkinson 2026-01-06 [Attention][1/n] Remove usage of deprecated `seq_lens_cpu` and `num_computed_tokens_cpu` CommonAttentionMetadata properties (#31773)
2	2	tests/v1/attention/test_attention_backends.py
2	2	tests/v1/attention/test_mla_backends.py
1	1	tests/v1/attention/test_sparse_mla_backends.py
3	1	vllm/v1/attention/backends/flashinfer.py
1	3	vllm/v1/attention/backends/flex_attention.py
3	1	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
1	1	vllm/v1/attention/backends/triton_attn.py
9	0	vllm/v1/attention/backends/utils.py

[14df02b4e] Cyrus Leung 2026-01-06 [Chore] Cleanup `mem_utils.py` (#31793)
27	17	vllm/utils/mem_utils.py

[6ebb66cce] BlankR 2026-01-06 [Doc] Fix format of multimodal_inputs.md (#31800)
45	41	docs/features/multimodal_inputs.md

[43d384bab] wang.yuqi 2026-01-06 [CI] Increase the MTEB_EMBED_TOL threshold to 5e-4. (#31797)
2	2	tests/models/language/pooling_mteb_test/mteb_embed_utils.py
0	1	tests/models/language/pooling_mteb_test/test_baai.py
0	2	tests/models/language/pooling_mteb_test/test_gte.py
0	1	tests/models/language/pooling_mteb_test/test_jina.py
0	1	tests/models/language/pooling_mteb_test/test_st_projector.py

[db318326a] Cyrus Leung 2026-01-06 [Misc] Use `deprecated` for `seed_everything` (#31780)
2	3	benchmarks/kernels/benchmark_activation.py
2	3	benchmarks/kernels/benchmark_layernorm.py
3	2	benchmarks/kernels/benchmark_moe.py
3	2	benchmarks/kernels/benchmark_moe_permute_unpermute.py
2	2	benchmarks/kernels/benchmark_mrope.py
2	1	benchmarks/kernels/benchmark_paged_attention.py
2	3	benchmarks/kernels/benchmark_quant.py
2	2	benchmarks/kernels/benchmark_reshape_and_cache.py
2	2	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
2	2	benchmarks/kernels/benchmark_silu_mul_fp8_quant.py
1	1	docs/design/plugin_system.md
5	4	vllm/platforms/interface.py

[799b5721f] Fadi Arafeh 2026-01-06 [cpu][bench] Add CPU paged attention benchmarks (#31720)
272	0	benchmarks/kernels/cpu/benchmark_cpu_attn.py

[97ca4c3b6] Cyrus Leung 2026-01-06 [Chore] Remove more V0 dead code from `sequence.py` (#31783)
1	2	vllm/outputs.py
0	34	vllm/sequence.py

[ee2e69d6c] Isotr0py 2026-01-06 [Bugfix][CI/Build] Fix failing pooling models test due to Triton kernel accuracy diff (#31776)
1	1	tests/models/language/pooling/test_token_classification.py

[7101e0851] Isotr0py 2026-01-06 [Models]: Use `MMEncoderAttention` for MoonViT (#31738)
1	1	vllm/model_executor/models/kimi_vl.py
71	157	vllm/model_executor/models/moonvit.py

[e9717801b] vllmellm 2026-01-06 [Bugfix][ROCm] Fix Unsupported attention metadata type for speculative decoding in `eagle.py` (#31714)
6	2	vllm/v1/spec_decode/eagle.py

[da71d4441] Cyrus Leung 2026-01-06 [Doc] Show that `use_audio_in_video` is supported in docs (#30837)
0	3	docs/models/supported_models.md
0	1	examples/offline_inference/qwen2_5_omni/README.md
0	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
0	2	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[1fb0209bb] Kevin McKay 2026-01-06 [Bugfix][Hardware][AMD] Fix exception types in AITER MLA FP8 check (#31177)
118	0	tests/rocm/aiter/test_mla_fp8_support_check.py
11	1	vllm/_aiter_ops.py

[81323ea22] Robert Shaw 2026-01-05 [CI] Fix CPU MM PRocessor Test (#31764)
1	1	requirements/test.in

[e1cd7a5fa] Michael Goin 2026-01-05 [Bugfix] Add init_workspace_manager to moe kernel benchmarks (#31042)
5	0	benchmarks/kernels/benchmark_cutlass_moe_fp8.py
5	0	benchmarks/kernels/{benchmark_cutlass_fp4_moe.py => benchmark_cutlass_moe_nvfp4.py}
5	0	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py

[a68e703c3] Michael Goin 2026-01-05 [UX] Add `-ep` shorthand for `--enable-expert-parallel` (#30890)
3	1	vllm/engine/arg_utils.py

[cd1245a18] maang 2026-01-06 [Cleanup] Remove redundant `decoder_layer_type` assignment in `Qwen2` (#31760)
0	2	vllm/model_executor/models/qwen2.py

[ffec81542] Wentao Ye 2026-01-05 [Perf] Optimize additional `fill(0)` in cutlass moe, 2.9% E2E throughput improvement, 10.8% TTFT improvement (#31754)
10	10	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[d386ab141] maang 2026-01-06 [Docs] Improve malformed exception caused by backslash line continuations (#31694)
3	3	vllm/config/compilation.py
1	2	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	2	vllm/model_executor/model_loader/weight_utils.py
2	2	vllm/model_executor/models/llama.py
3	4	vllm/model_executor/models/phi4mm.py
2	2	vllm/multimodal/registry.py
1	2	vllm/utils/argparse_utils.py

[ccb309a96] Michael Goin 2026-01-05 Revert "[CI Failure] Disable B200 tests while runner is broken" (#31750)
0	4	.buildkite/test-pipeline.yaml

[2f4e6548e] John Calderon 2026-01-05 [Bugfix] vLLM produces invalid UTF-8 tokens and “�” (#28874)
44	12	tests/v1/engine/test_output_processor.py
418	0	tests/v1/sample/test_logprobs.py
73	17	vllm/v1/engine/logprobs.py

[3c98c2d21] Seiji Eicher 2026-01-05 [CI/Build] Allow user to configure NVSHMEM version via ENV or command line (#30732)
3	1	docker/Dockerfile
21	1	tools/ep_kernels/install_python_libraries.sh

[951302989] Michael Goin 2026-01-05 [Bugfix] Properly apply v_scale for mimo_v2_flash (#31175)
10	13	vllm/model_executor/models/mimo_v2_flash.py

[f6c0009af] Robert Shaw 2026-01-05 [Bugfix] Fix Broken ModelOpt NVFP4 MoE (#31742)
5	13	vllm/model_executor/layers/fused_moe/all2all_utils.py
3	1	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
9	1	vllm/model_executor/layers/quantization/fp8.py
15	0	vllm/model_executor/layers/quantization/modelopt.py

[776ca1e18] Yongye Zhu 2026-01-05 [MoE Refactor] Aiter Experts for BF16 MoE (#31542)
30	39	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[af9a7ec25] Wentao Ye 2026-01-05 [Bug] Revert torch warning fix (#31585)
1	1	tests/v1/e2e/test_async_scheduling.py
4	6	vllm/envs.py
1	1	vllm/v1/worker/gpu_worker.py

[276e03b92] Matthew Bonanni 2026-01-05 [CI][DeepSeek] Add nightly DeepSeek R1 `lm_eval` tests on H200 (#30356)
8	0	.buildkite/test-pipeline.yaml
11	0	tests/evals/gsm8k/configs/DeepSeek-R1-DP.yaml
11	0	tests/evals/gsm8k/configs/DeepSeek-R1-TP.yaml
2	0	tests/evals/gsm8k/configs/models-h200.txt
1	1	tests/evals/gsm8k/test_gsm8k_correctness.py

[32f4e4db0] Nick Hill 2026-01-05 [Cleanup] Remove deprecated fields from CachedRequestData class (#31734)
12	14	tests/v1/core/test_scheduler.py
0	16	vllm/v1/core/sched/output.py

[ee2129182] amitz-nv 2026-01-05 [Model] Nemotron Parse 1.1 Support (#30864)
2	1	requirements/test.in
3	1	requirements/test.txt
2	1	tests/conftest.py
89	0	tests/models/multimodal/generation/test_nemotron_parse.py
10	8	tests/models/multimodal/pooling/test_radio.py
1	0	tests/models/multimodal/processing/test_common.py
3	0	tests/models/registry.py
5	2	vllm/assets/image.py
2	7	vllm/model_executor/models/nano_nemotron_vl.py
958	0	vllm/model_executor/models/nemotron_parse.py
23	5	vllm/model_executor/models/radio.py
4	0	vllm/model_executor/models/registry.py
15	6	vllm/transformers_utils/configs/radio.py

[af1b07b0c] Qidong Su 2026-01-05 [docker] install cuda13 version of lmcache and nixl (#30913)
20	1	docker/Dockerfile

[c77a993cc] gnovack 2026-01-05 pin lora_b moe weights on cpu (#31317)
0	11	vllm/lora/layers/fused_moe.py
89	54	vllm/lora/model_manager.py

[fdcc5176b] Roberto L. Castro 2026-01-05 [BugFix] Fix architecture flags to prevent issues on SM103 (#31150)
8	3	cmake/external_projects/qutlass.cmake
5	2	tools/flashinfer-build.sh

[5708297e4] Wang Kunpeng 2026-01-06 [Misc][Model][Refactor] Pass the prefix into Linear layers (#31669)
12	2	vllm/model_executor/layers/mamba/mamba_mixer.py
15	5	vllm/model_executor/models/aria.py
1	1	vllm/model_executor/models/gpt_neox.py
1	0	vllm/model_executor/models/hunyuan_v1.py
1	0	vllm/model_executor/models/jamba.py
14	4	vllm/model_executor/models/jina_vl.py
4	0	vllm/model_executor/models/minicpm.py
1	0	vllm/model_executor/models/minicpm_eagle.py
1	0	vllm/model_executor/models/mistral_large_3_eagle.py
25	8	vllm/model_executor/models/modernbert.py
50	9	vllm/model_executor/models/molmo.py
5	1	vllm/model_executor/models/olmoe.py
1	0	vllm/model_executor/models/phimoe.py
15	3	vllm/model_executor/models/qwen.py
1	0	vllm/model_executor/models/qwen3_next.py
26	6	vllm/model_executor/models/qwen_vl.py
8	1	vllm/model_executor/models/zamba2.py

[02dbb933c] baonudesifeizhai 2026-01-05 Fix GLM-4.6v flash tool calling in transformers 5.x (#31622)
54	0	examples/tool_chat_template_glm4.jinja
14	0	vllm/tool_parsers/glm4_moe_tool_parser.py

[51e38a8e3] Isotr0py 2026-01-06 [Misc] Enable Paligemma's PrefixLM attention mask computation (#31725)
0	4	tests/models/multimodal/generation/test_common.py
1	3	vllm/config/model.py

[d8e38d493] Or Ozeri 2026-01-05 Triton Attention: Support cross-layers blocks (#30687)
1	3	tests/v1/kv_offload/test_cpu_offloading.py
13	0	vllm/v1/attention/backends/triton_attn.py

[21156ff19] kzwrime 2026-01-06 [Bugfix] Add missing extra_tensors arg to DeviceCommunicatorBase.disp… (#31644)
1	0	vllm/distributed/device_communicators/base_device_communicator.py
8	5	vllm/distributed/device_communicators/xpu_communicator.py

[c455b771f] RickyChen / 陳昭儒 2026-01-06 [Bugfix][CPU] Fix RotaryEmbedding fallback causing gibberish with --enforce-eager (#31643)
3	2	vllm/model_executor/custom_op.py
22	0	vllm/model_executor/layers/rotary_embedding/base.py

[eefa713a6] Michael Goin 2026-01-05 [CI Failure] Disable B200 tests while runner is broken (#31732)
4	1	.buildkite/test-pipeline.yaml

[79ed460dd] Kevin Šuc 2026-01-05 [Frontend] [Doc] Exclude log deltas feature (#30322)
1	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/entrypoints/openai/cli_args.py
3	1	vllm/entrypoints/openai/serving_chat.py

[6aa5b18e1] Isotr0py 2026-01-06 [v1] Add encoder-only/cross attention support to Triton Attention backend (#31406)
225	0	tests/kernels/attention/test_triton_prefill_attention.py
1	1	tests/models/multimodal/generation/test_whisper.py
57	0	tests/v1/attention/test_attention_backends.py
271	0	vllm/attention/ops/triton_prefill_attention.py
0	9	vllm/platforms/rocm.py
73	4	vllm/v1/attention/backends/triton_attn.py

[911d38ed9] wang.yuqi 2026-01-05 [Model] Let more models to support the score template. (#31335)
19	17	docs/models/supported_models.md
93	19	examples/pooling/score/convert_model_to_seq_cls.py
0	89	examples/pooling/score/offline_reranker.py
0	27	examples/pooling/score/offline_using_template.py
0	46	examples/pooling/score/online_using_template.py
104	0	examples/pooling/score/qwen3_reranker_offline.py
80	0	examples/pooling/score/qwen3_reranker_online.py
3	0	examples/pooling/score/template/bge-reranker-v2-gemma.jinja
8	0	examples/pooling/score/template/mxbai_rerank_v2.jinja
11	0	examples/pooling/score/template/qwen3_reranker.jinja
159	0	examples/pooling/score/using_template_offline.py
75	0	examples/pooling/score/using_template_online.py
6	2	tests/conftest.py
69	26	tests/models/language/pooling_mteb_test/mteb_score_utils.py
2	4	tests/models/language/pooling_mteb_test/test_baai.py
29	40	tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py
5	6	tests/models/language/pooling_mteb_test/test_cross_encoder.py
2	4	tests/models/language/pooling_mteb_test/test_gte.py
2	4	tests/models/language/pooling_mteb_test/test_jina.py
45	23	tests/models/language/pooling_mteb_test/test_mxbai_rerank.py
2	4	tests/models/language/pooling_mteb_test/test_nemotron.py
52	25	tests/models/language/pooling_mteb_test/test_qwen3_reranker.py
1	1	vllm/model_executor/models/config.py

[caaa482ac] zzzzwwjj 2026-01-05 [platform] Support additional forward context for OOT (#31674)
18	3	vllm/forward_context.py
7	0	vllm/platforms/interface.py

[b471aad41] Yihua Cheng 2026-01-05 [KVconnector][LMCache] remove the import of legacy LMCache code (#31704)
3	13	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils.py

[d5503ca7f] Jee Jee Li 2026-01-05 [LoRA] LoRA PDL improvement (#31660)
4	2	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[a2ad15c07] Qiping Pan 2026-01-05 [Model] Enable LoRA support for BLIP2 (#31620)
1	1	docs/models/supported_models.md
46	3	vllm/model_executor/models/blip2.py

[3133c192a] Tres 2026-01-05 [ROCM] Reorder arguments and rename parameters for rope_cached_thd_positions_2c_fwd_inplace (#29993)
4	4	vllm/_aiter_ops.py

[76fd458aa] wang.yuqi 2026-01-05 [CI] Bump sentence-transformer from 3.2.1 to 5.2.0 (#31664)
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
2	2	requirements/test.txt

[e2701cc52] cjackal 2026-01-05 [Frontend] [Bugfix] respect server-level default chat template kwargs in reasoning parser (#31581)
12	2	vllm/entrypoints/openai/serving_chat.py
16	3	vllm/entrypoints/openai/serving_engine.py

[fe8a9fbd2] Tyler Michael Smith 2026-01-04 [Bugfix] Fix EPLB state logging error (#31455)
1	1	vllm/distributed/eplb/eplb_state.py

[98b8b3aba] Ning Xie 2026-01-05 [log] enable max_log_len trim only when needed (#31482)
17	15	vllm/entrypoints/logger.py

[346e56455] CHENYUE 2026-01-05 Add chat prefix completion feature to DeepSeek v3.2 (#31147)
9	5	vllm/tokenizers/deepseek_v32_encoding.py

[8be6432bd] wang.yuqi 2026-01-05 [CI Failure] Fix NomicBert max_model_len validation (#31662)
0	6	vllm/config/vllm.py
33	20	vllm/model_executor/models/config.py

[43e3f8e4a] Nick Hill 2026-01-04 [Misc] Various code simplifications (#31666)
2	7	vllm/v1/core/sched/async_scheduler.py
1	4	vllm/v1/core/sched/interface.py
7	23	vllm/v1/core/sched/scheduler.py
10	23	vllm/v1/spec_decode/eagle.py
18	37	vllm/v1/structured_output/__init__.py
24	36	vllm/v1/structured_output/utils.py
4	5	vllm/v1/worker/gpu_model_runner.py

[bb4337b34] wangxiyuan 2026-01-05 [Platform] Deprecate seed_everything (#31659)
1	0	docs/design/plugin_system.md
2	1	tests/compile/distributed/test_async_tp.py
2	1	tests/compile/distributed/test_fusion_all_reduce.py
2	1	tests/compile/distributed/test_sequence_parallelism.py
2	1	tests/kernels/attention/test_aiter_flash_attn.py
2	1	tests/kernels/attention/test_attention.py
9	8	tests/kernels/attention/test_cache.py
3	2	tests/kernels/attention/test_cascade_flash_attn.py
2	1	tests/kernels/attention/test_cpu_attn.py
2	1	tests/kernels/attention/test_flash_attn.py
5	4	tests/kernels/attention/test_flashinfer.py
3	2	tests/kernels/attention/test_flashinfer_trtllm_attention.py
4	4	tests/kernels/attention/test_lightning_attn.py
3	2	tests/kernels/attention/test_mha_attn.py
3	3	tests/kernels/attention/test_prefix_prefill.py
2	1	tests/kernels/attention/test_triton_unified_attention.py
3	3	tests/kernels/core/test_activation.py
2	1	tests/kernels/core/test_fused_qk_norm_rope.py
3	3	tests/kernels/core/test_layernorm.py
2	1	tests/kernels/core/test_mrope.py
2	2	tests/kernels/core/test_pos_encoding.py
4	4	tests/kernels/mamba/test_causal_conv1d.py
2	2	tests/kernels/mamba/test_mamba_mixer2.py
6	6	tests/kernels/mamba/test_mamba_ssm.py
2	2	tests/kernels/mamba/test_mamba_ssm_ssd.py
2	2	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
2	2	tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py
3	2	tests/kernels/moe/test_batched_moe.py
2	1	tests/kernels/moe/test_cpu_fused_moe.py
4	3	tests/kernels/moe/test_cutlass_moe.py
4	4	tests/kernels/moe/test_deepep_deepgemm_moe.py
3	3	tests/kernels/moe/test_deepep_moe.py
3	2	tests/kernels/moe/test_flashinfer.py
2	1	tests/kernels/moe/test_flashinfer_moe.py
2	1	tests/kernels/moe/test_grouped_topk.py
2	2	tests/kernels/moe/test_modular_kernel_combinations.py
2	1	tests/kernels/moe/test_modular_oai_triton_moe.py
2	1	tests/kernels/moe/test_moe.py
2	1	tests/kernels/moe/test_moe_align_block_size.py
3	2	tests/kernels/moe/test_moe_permute_unpermute.py
2	1	tests/kernels/moe/test_nvfp4_moe.py
2	1	tests/kernels/moe/test_pplx_cutlass_moe.py
7	7	tests/kernels/moe/test_pplx_moe.py
2	1	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
2	1	tests/kernels/moe/test_silu_mul_per_token_group_quant_fp8_colmajor.py
3	3	tests/kernels/quantization/test_awq_triton.py
3	2	tests/kernels/quantization/test_cutlass_w4a8_moe.py
2	1	tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py
2	1	tests/kernels/quantization/test_flashinfer_scaled_mm.py
4	4	tests/kernels/quantization/test_fp8_quant.py
4	4	tests/kernels/quantization/test_fp8_quant_group.py
4	4	tests/kernels/quantization/test_gguf.py
5	5	tests/kernels/quantization/test_int8_quant.py
2	1	tests/kernels/quantization/test_mxfp4_qutlass.py
3	2	tests/kernels/quantization/test_nvfp4_quant.py
2	1	tests/kernels/quantization/test_nvfp4_qutlass.py
2	1	tests/kernels/quantization/test_nvfp4_scaled_mm.py
2	1	tests/kernels/quantization/test_silu_mul_nvfp4_quant.py
2	1	tests/kernels/quantization/test_triton_scaled_mm.py
3	2	tests/kernels/test_apply_repetition_penalties.py
8	8	tests/kernels/test_fla_layernorm_guard.py
4	4	tests/lora/test_fused_moe_lora_kernel.py
1	1	tests/lora/test_layers.py
3	3	tests/lora/test_punica_ops.py
4	3	tests/models/test_vision.py
6	2	tests/v1/attention/test_attention_backends.py
2	1	tests/v1/kv_offload/test_cpu_gpu.py
2	1	tests/v1/tpu/test_mha_attn.py
2	1	tests/v1/worker/test_gpu_model_runner.py
0	2	vllm/model_executor/__init__.py
0	6	vllm/model_executor/utils.py
4	0	vllm/platforms/interface.py
10	5	vllm/utils/torch_utils.py
1	1	vllm/v1/worker/cpu_worker.py
1	1	vllm/v1/worker/gpu_worker.py
1	2	vllm/v1/worker/tpu_worker.py
1	1	vllm/v1/worker/xpu_worker.py

[367856de1] Isotr0py 2026-01-05 [CI/Build] Revive skipped reward models e2e test (#31665)
1	0	tests/models/fixtures/qwen2_5_math_prm_reward_step.json
63	4	tests/models/language/pooling/test_reward.py

[da436f868] Nick Hill 2026-01-04 [Minor] Small pooler output processing optimization (#31667)
8	11	vllm/v1/worker/gpu_model_runner.py

[f099cd557] Jee Jee Li 2026-01-05 [Bugfix] Fix  AttributeError: 'Stream' object has no attribute 'dp_size' (#31663)
0	1	vllm/lora/layers/fused_moe.py

[f2b6dfd23] Andreas Karatzas 2026-01-04 [ROCm][CI] Fix language generation test accuracy by disabling HF flash_sdp and mem_efficient_sdp (#31597)
28	0	tests/models/language/generation/conftest.py

[89f1f2531] Andreas Karatzas 2026-01-04 [CI] Skip Phi-MoE test due to old API util (#31632)
2	2	.buildkite/test-amd.yaml
13	0	tests/models/language/generation/test_phimoe.py

[b53b89fdb] Nick Hill 2026-01-04 [BugFix] Async scheduling: handle model forward errors more cleanly (#31611)
9	16	vllm/v1/engine/core.py

[6522721d1] Ning Xie 2026-01-05 [misc] Sort uvicorn log level description according to verbosity (#31137)
1	1	vllm/entrypoints/openai/cli_args.py

[0d4044edd] Yuxuan Zhang 2026-01-04 fix no think of GLM-4.5 / GLM-4.7 (#31449)
4	163	vllm/reasoning/glm4_moe_reasoning_parser.py

[41ab17973] Reagan Lee 2026-01-03 [Docs] Fix argparse include path for mm-processor benchmark (#31654)
1	1	docs/cli/bench/mm_processor.md

[268b1c55a] Robert Shaw 2026-01-03 [MoE Refactor][13/N] Convert FI to Use PFNoEP (#31533)
13	3	vllm/model_executor/layers/fused_moe/all2all_utils.py
17	5	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
0	1	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
2	2	vllm/model_executor/layers/fused_moe/layer.py
5	75	vllm/model_executor/layers/fused_moe/modular_kernel.py
40	73	vllm/model_executor/layers/quantization/fp8.py
1	9	vllm/model_executor/layers/quantization/modelopt.py

[4f9ce35af] Andreas Karatzas 2026-01-03 [CI][Bugfix] Fix token counting in chunked prefill compl test (#31630)
4	1	tests/entrypoints/openai/test_chunked_prompt.py

[97a01308e] jeremyteboul 2026-01-02 Improve HF qwen3_omni: preserve audio_sample_rate in kwargs restructuring (#29255)
285	0	tests/models/multimodal/processing/test_qwen3_omni.py
2	3	tests/multimodal/test_processing.py
25	0	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[0eee877f6] Xingyu Liu 2026-01-02 [Core] Parse vLLM engine required fields from hf_config to model_arch_config (#28454)
359	0	tests/config/base_model_arch_groundtruth.json
87	0	tests/config/draft_model_arch_groundtruth.json
152	0	tests/config/test_model_arch_config.py
5	1	tests/models/utils.py
1	0	tests/test_config.py
10	0	tests/v1/metrics/test_perf_metrics.py
44	286	vllm/config/model.py
57	0	vllm/config/model_arch.py
3	0	vllm/config/speculative.py
1	0	vllm/config/vllm.py
402	0	vllm/transformers_utils/model_arch_config_convertor.py

[a0e9ee83c] Alfred 2026-01-03 [Benchmark] Fix OOM during MoE kernel tuning for large models (#31604)
55	1	benchmarks/kernels/benchmark_moe.py

[a3f2f4094] Yongye Zhu 2026-01-02 [MoE Refactor] Explicit construct mk for flashinfer bf16 kernel (#31504)
32	36	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[5a468ff7c] Yongye Zhu 2026-01-02 [MoE Refactor] Split `invoke_fused_moe_kernel` (#31050)
266	102	vllm/model_executor/layers/fused_moe/fused_moe.py

[6ef770df7] Andreas Karatzas 2026-01-02 [MoE] Fix output_shape calculation in Attention layer to handle 3D query inputs (#31596)
4	1	vllm/attention/layer.py
13	1	vllm/model_executor/layers/quantization/fp8.py

[bd877162e] Nick Hill 2026-01-02 [BugFix] Support online dense model DP without overhead (#30739)
48	5	tests/test_config.py
1	0	tests/v1/engine/test_engine_core_client.py
1	1	vllm/compilation/backends.py
1	1	vllm/compilation/decorators.py
6	7	vllm/config/model.py
15	0	vllm/config/parallel.py
33	8	vllm/config/vllm.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
22	15	vllm/distributed/parallel_state.py
1	0	vllm/engine/arg_utils.py
1	0	vllm/forward_context.py
1	1	vllm/v1/core/sched/scheduler.py
62	43	vllm/v1/engine/coordinator.py
94	23	vllm/v1/engine/core.py
1	1	vllm/v1/engine/core_client.py
2	1	vllm/v1/engine/llm_engine.py
37	19	vllm/v1/engine/utils.py
9	9	vllm/v1/worker/gpu/model_runner.py
7	9	vllm/v1/worker/gpu_worker.py

[08f425bad] Xinyu Chen 2026-01-02 CustomOp: test forward dispatch for grouped_topk (#31530)
13	1	tests/kernels/moe/test_grouped_topk.py

[a01f2faed] labAxiaoming 2026-01-02 Add multimodal input method in the documentation (#31601)
33	0	docs/features/multimodal_inputs.md
63	0	examples/online_serving/openai_chat_completion_client_for_multimodal.py

[cc410e864] Kyuyeun Kim 2026-01-01 [Bugfix] Fix weight_loader v1 block scale (#31103)
40	27	vllm/model_executor/layers/linear.py

[825c2dc13] Kevin McKay 2026-01-01 [Bugfix][Hardware][AMD] Fix last_page_len calculation in AITER MLA decode (#31282)
12	9	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[1f43c121d] Vaibhav Sourirajan 2026-01-02 Remove unused `use_marlin` variable in `Mxfp4MoEMethod` (#31549)
0	1	vllm/model_executor/layers/quantization/mxfp4.py

[ca179d0f6] Tmn07 2026-01-02 [Bugfix] Fix activation quantization for compressed-tensors W4A16 (#31572)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py

[013b54088] Andreas Karatzas 2026-01-01 [ROCm][CI] Fix ModernBERT token classification test (#31612)
18	7	tests/models/language/pooling/test_token_classification.py

[5ac55eb30] Jay Hemnani 2026-01-01 [Model] Enable LoRA support for tower and connector in LLaVA (#31513)
1	1	docs/models/supported_models.md
36	2	vllm/model_executor/models/llava.py

[ea53ca5e8] Benjamin Chislett 2026-01-01 [Bugfix] Fix block size used in EAGLE slot mapping (#31540)
8	9	vllm/v1/spec_decode/eagle.py

[27864a851] zhima771 2026-01-02 feat: support LoRA for DeepSeek-OCR(Language Model part) (#31569)
1	1	docs/models/supported_models.md
13	1	vllm/model_executor/models/deepseek_ocr.py

[5cc487663] Andreas Karatzas 2026-01-01 [ROCm][CI] Fix failure in Language Models Tests (Extra Standard) by reducing agent pool size (#31553)
2	1	.buildkite/test-amd.yaml

[5fff44064] Kevin McKay 2026-01-01 [Bugfix] Replace BaseException with specific exceptions in FLA utils (#31590)
1	1	vllm/model_executor/layers/fla/ops/utils.py

[1f5b7c41c] Reagan Lee 2026-01-01 Add Multimodal Processor Benchmark  (#29105)
9	0	docs/cli/bench/mm_processor.md
2	0	docs/mkdocs/hooks/generate_argparse.py
100	71	vllm/benchmarks/datasets.py
363	0	vllm/benchmarks/mm_processor.py
167	32	vllm/benchmarks/throughput.py
8	0	vllm/config/observability.py
2	0	vllm/engine/arg_utils.py
4	0	vllm/entrypoints/cli/__init__.py
21	0	vllm/entrypoints/cli/benchmark/mm_processor.py
4	1	vllm/inputs/preprocess.py
252	37	vllm/multimodal/processing.py
26	8	vllm/multimodal/registry.py
10	6	vllm/v1/engine/input_processor.py

[adcf682fc] Ekagra Ranjan 2025-12-31 [Audio] Improve Audio Inference Scripts (offline/online) (#29279)
31	18	examples/offline_inference/audio_language.py
82	14	examples/online_serving/openai_transcription_client.py

[21de6d4b0] Andreas Karatzas 2025-12-31 [CI][Bugfix] Fix token counting in chunked prefill streaming test (#31565)
4	1	tests/entrypoints/openai/test_chunked_prompt.py

[6c2cfb62f] Nick Hill 2025-12-31 [BugFix] Fix async scheduling for pooling models (#31584)
5	2	vllm/v1/executor/ray_utils.py
7	18	vllm/v1/outputs.py
20	28	vllm/v1/pool/metadata.py
1	5	vllm/v1/worker/gpu/model_runner.py
92	35	vllm/v1/worker/gpu_model_runner.py
4	2	vllm/v1/worker/gpu_worker.py
2	2	vllm/v1/worker/worker_base.py

[d8da76f3b] Fanjiang Ye 2025-12-31 [Bugfix] Fix BAGEL online serving for text and image understanding (#31546)
7	0	vllm/model_executor/models/bagel.py
8	3	vllm/transformers_utils/processors/bagel.py

[d722e9e61] baonudesifeizhai 2025-12-31 Add GLM-ASR multimodal support  (#31436)
3	2	docs/models/supported_models.md
29	0	examples/offline_inference/audio_language.py
14	0	tests/models/multimodal/processing/test_common.py
5	0	tests/models/registry.py
545	0	vllm/model_executor/models/glmasr.py
165	0	vllm/model_executor/models/glmasr_utils.py
1	0	vllm/model_executor/models/registry.py
2	0	vllm/transformers_utils/config.py

[cf16342d4] Andreas Karatzas 2025-12-31 [ROCm][CI] Update MiniCPM model test: MiniCPM3-4B to MiniCPM4.1-8B and simplify attention backend testing (#31551)
16	7	tests/models/language/generation/test_common.py
3	0	tests/models/registry.py

[357d435c5] Wentao Ye 2025-12-31 [Bug] Fix log issue with `\n` (#31390)
2	2	vllm/v1/attention/backends/mla/flashmla_sparse.py

[108a2728f] danisereb 2025-12-31 Add get_expert_mapping to NemotronHModel (for LoRA support) (#31539)
14	10	vllm/model_executor/models/nemotron_h.py

[578c8f51f] TJian 2025-12-31 [CI] [Critical] [CUDA] Fix duplicated test name (#31562)
1	1	.buildkite/test-pipeline.yaml

[b4bb5f312] maang-h 2025-12-31 [Core] Remove unused `num_tokens` parameter from `_init_model_kwargs` (#31517)
6	6	vllm/v1/worker/gpu_model_runner.py

[70e1acefc] SameerAsal 2025-12-30 [BugFix] Fix NUMA node validation in CPU platform (#31520)
1	1	vllm/platforms/cpu.py

[84f6cd741] Qiu 2025-12-31 [Mics] add pcp basic support to MoE model (#31003)
8	0	vllm/model_executor/layers/fused_moe/config.py

[ecd49ce7e] B-201 2025-12-31 [Fix] Align fused moe lora_b shape with peft (#31534)
1	1	docs/models/supported_models.md
3	3	tests/lora/test_gptoss_tp.py
5	5	vllm/lora/layers/fused_moe.py

[e1ee11b2a] Amr Mahdi 2025-12-30 Add docker buildx bake configuration (#31477)
76	0	docker/docker-bake.hcl

[04147dcfa] vintipandey 2025-12-30 [Bugfix]Fix pooling model always disabled due to incorrect PP rank check (#31505)
1	1	vllm/v1/worker/gpu_model_runner.py

[07728bf5c] JartX 2025-12-30 [BugFix]  add select_gemm_impl on CompressedTensorsWNA16MoEMethod to support LoRA (#31453)
23	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[3f52fa5aa] yt0428 2025-12-31 [Model] Add support for openPangu moe model (#28775)
1	0	docs/models/supported_models.md
5	0	tests/models/registry.py
3	0	vllm/attention/backends/registry.py
11	4	vllm/attention/layer.py
254	0	vllm/attention/layers/static_sink_attention.py
171	0	vllm/attention/ops/triton_reshape_and_cache_flash.py
329	3	vllm/model_executor/models/openpangu.py
1	0	vllm/model_executor/models/registry.py
269	0	vllm/v1/attention/backends/flash_attn_diffkv.py
26	0	vllm/v1/core/single_type_kv_cache_manager.py
66	0	vllm/v1/kv_cache_interface.py

[715759610] Li, Jiang 2025-12-30 [CPU] Disable async schedule on CPU (#31525)
2	0	vllm/platforms/cpu.py

[ab1af6aa3] Nicolò Lucchesi 2025-12-30 [CI][NIXL] Split DPEP tests (#31491)
12	1	.buildkite/test-pipeline.yaml
13	3	tests/v1/kv_connector/nixl_integration/{tp_config_sweep_accuracy_test.sh => config_sweep_accuracy_test.sh}

[1a834df2d] Pleaplusone 2025-12-30 [ROCm][Bugfix] Fix accuracy issue on fmoe when `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` enabled (#31523)
18	0	vllm/platforms/rocm.py

[51085c2ae] Kevin 2025-12-29 [Frontend] add continue_final_message parameter to /embeddings endpoint (#31497)
10	1	vllm/entrypoints/pooling/embed/protocol.py
1	1	vllm/entrypoints/pooling/embed/serving.py

[3d973764c] Roger Feng 2025-12-30 [xpu] [bugfix] upgrade to latest oneccl in dockerfile (#31522)
6	2	docker/Dockerfile.xpu

[3b312fb79] Nick Hill 2025-12-29 [Minor] Various small code cleanups/simplifications (#31508)
4	4	vllm/config/model.py
0	1	vllm/entrypoints/openai/serving_responses.py
2	5	vllm/entrypoints/renderer.py
1	5	vllm/inputs/preprocess.py
2	11	vllm/multimodal/inputs.py
2	4	vllm/v1/engine/core.py
1	1	vllm/v1/engine/input_processor.py
2	7	vllm/v1/engine/output_processor.py
1	1	vllm/v1/executor/multiproc_executor.py
1	2	vllm/v1/request.py
8	6	vllm/v1/structured_output/__init__.py
3	6	vllm/v1/structured_output/request.py
1	1	vllm/v1/worker/gpu_input_batch.py
1	1	vllm/v1/worker/gpu_model_runner.py

[f84bf7d79] ZT-AIA 2025-12-30 Add Loraconfig parameter to  get_punica_wrapper function (#31408)
10	10	tests/lora/test_layers.py
4	4	vllm/lora/model_manager.py
2	1	vllm/lora/punica_wrapper/punica_gpu.py

[99dcf5dcc] Roy Wang 2025-12-30 Migrate meetups & sponsors [2/N] (#31500)
3	90	README.md
1	42	docs/community/meetups.md
1	40	docs/community/sponsors.md

[dc837bc23] Hojin Yang 2025-12-30 feat(frontend): add --default-chat-template-kwargs CLI argument (#31343)
36	0	docs/features/reasoning_outputs.md
33	0	tests/entrypoints/openai/test_cli_args.py
1	0	vllm/entrypoints/openai/api_server.py
10	1	vllm/entrypoints/openai/cli_args.py
3	0	vllm/entrypoints/openai/run_batch.py
5	1	vllm/entrypoints/openai/serving_chat.py
3	0	vllm/entrypoints/openai/serving_engine.py

[e54ee3ea3] Nick Hill 2025-12-29 [Core] Deduplicate generate/encode logic in `AsyncLLM` (#31510)
32	48	vllm/v1/engine/async_llm.py

[358bfd315] wangln19 2025-12-30 fix: update kimi k2 tool parser logic (#31207)
192	191	tests/tool_parsers/test_kimi_k2_tool_parser.py
19	11	vllm/tool_parsers/kimi_k2_tool_parser.py

[39512aba7] Sage 2025-12-30 [Prefix Cache] Include lora_name in BlockStored event for deterministic KV-cache reconstruction (#27577)
6	0	examples/online_serving/kv_events_subscriber.py
1	0	tests/v1/engine/test_engine_core_client.py
30	0	tests/v1/kv_connector/unit/test_lmcache_connector.py
1	0	tests/v1/kv_connector/unit/test_offloading_connector.py
6	0	vllm/distributed/kv_events.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
3	0	vllm/v1/core/block_pool.py

[0f35429a0] qli88 2025-12-29 [CI]Test Group 'NixlConnector PD accuracy tests' is fixed (#31460)
2	2	.buildkite/test-amd.yaml
136	1	docker/Dockerfile.rocm_base
7	1	docs/features/nixl_connector_usage.md
2	0	requirements/kv_connectors_rocm.txt
10	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[d63b96967] Alexei-V-Ivanov-AMD 2025-12-29 [CI/ROCm] Fixing "V1 Test attention (H100)" test group. (#31187)
29	9	tests/v1/attention/test_attention_backends.py
39	10	tests/v1/attention/test_rocm_attention_backends_selection.py
4	0	tests/v1/attention/test_sparse_mla_backends.py

[56f516254] Robert Shaw 2025-12-29 [Bugfix][ROCm] Fix Static Quant Issue (#31502)
4	1	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
10	13	vllm/model_executor/layers/quantization/fp8.py

[9152a30d8] Robert Shaw 2025-12-29 [MoE Refactor][12/N] Marlin Fp8 MoE Pure Function (#31499)
18	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
21	21	vllm/model_executor/layers/quantization/fp8.py
20	4	vllm/model_executor/layers/quantization/quark/quark_moe.py
33	46	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[c2ff33cc8] Nick Hill 2025-12-29 [Core] Enable async scheduling by default (#27614)
6	5	vllm/config/scheduler.py
25	9	vllm/config/vllm.py

[b12cb3839] chunxiaozheng 2025-12-30 implements register kv caches in lmcache connector (#31397)
16	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
10	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py

[5bc664110] Roger Young 2025-12-30 Optimize QKNorm for MiniMax-M2/M2.1 (#31493)
22	0	vllm/model_executor/layers/mamba/linear_attn.py
3	2	vllm/model_executor/models/minimax_m2.py

[b3a2bdf1a] RickyChen / 陳昭儒 2025-12-30 [Feature] Add offline FastAPI documentation support for air-gapped environments (#30184)
2	2	pyproject.toml
2	0	setup.py
2	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/entrypoints/openai/cli_args.py
6	1	vllm/entrypoints/serve/__init__.py
50	0	vllm/entrypoints/serve/instrumentator/offline_docs.py
2	0	vllm/entrypoints/serve/instrumentator/static/swagger-ui-bundle.js
3	0	vllm/entrypoints/serve/instrumentator/static/swagger-ui.css

[e37e7349e] Harry Mellor 2025-12-29 Replace `nn.ConvNd` with vLLM's `ConvNdLayer` for Transformers modeling backend (#31498)
3	0	vllm/model_executor/models/transformers/base.py
40	0	vllm/model_executor/models/transformers/utils.py

[b5d2d71d2] Roy Wang 2025-12-29 Migrate doc to website: Hardware Plugins (1/N) (#31496)
2	14	docs/getting_started/installation/README.md

[decc24476] Harry Mellor 2025-12-29 [Docs] Use relative `md` links instead of absolute `html` links for cross referencing (#31494)
2	2	docs/deployment/docker.md
1	1	docs/design/debug_vllm_compile.md
1	1	docs/features/custom_logitsprocs.md
4	4	docs/getting_started/installation/cpu.md
4	6	docs/mkdocs/hooks/url_schemes.py

[9c884faa9] amittell 2025-12-29 [Bugfix] Preserve tool call id/type/name in streaming finish chunk (#31438)
139	0	tests/entrypoints/openai/test_serving_chat.py
31	9	vllm/entrypoints/openai/serving_chat.py

[48d5ca4e8] Chauncey 2025-12-29 [CI] fix test_chat_truncation_content_not_null test (#31488)
8	2	tests/entrypoints/openai/test_response_api_with_harmony.py

[bf73a3e4d] twj 2025-12-29 [Bugfix][Frontend] Fix Jina reranker multimodal input compatibility (#31445)
313	137	tests/models/multimodal/pooling/test_jinavl_reranker.py
3	1	vllm/entrypoints/score_utils.py

[3ecfdc377] Andreas Karatzas 2025-12-29 [ROCm][GPTQ][Bugfix] Fix GPTQ GEMM kernel output zeroing race condition (#30719)
1	27	csrc/quantization/gptq/q_gemm.cu

[45c1ca1ca] Andreas Karatzas 2025-12-29 [ROCm][CI] Skip DeepGemm-dependent test on ROCm platform (#31462)
4	0	tests/kernels/moe/test_silu_mul_per_token_group_quant_fp8_colmajor.py

[17347daaa] Li, Jiang 2025-12-29 [CI/Build][CPU] Update CPU CI test cases (#31466)
1	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh

[b9793e6a8] Mamy Ratsimbazafy 2025-12-28 Add Fused MoE Triton kernels for GLM-4.5-Air, GLM-4.5v, GLM-4.6v on 2x RTX Pro 6000 (#31407)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=129,N=704,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8.json

[0b6b70105] Jzz1943 2025-12-29 [Model] Add tuned triton fused_moe configs for Qwen3Moe on B200 (#31448)
147	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_B200.json

[094fcce25] Nick Hill 2025-12-28 [BugFix] Re-fix async multimodal cpu tensor race condition (#31373)
112	114	vllm/v1/worker/gpu_model_runner.py

[573dd0e6f] Andreas Karatzas 2025-12-28 [ROCm] Migrate xgrammar to upstream release (#31327)
1	1	requirements/common.txt
0	4	requirements/rocm-test.txt

[f70368867] Andreas Karatzas 2025-12-28 [ROCm][CI] Add TorchCodec source build for transcription tests (#31323)
4	1	.buildkite/test-amd.yaml
8	0	docker/Dockerfile.rocm
90	0	tools/install_torchcodec_rocm.sh

[96142f209] Andreas Karatzas 2025-12-27 [ROCm][CI] Added perceptron lib in requirements for isaac multi-modal test (#31441)
2	0	requirements/rocm-test.txt

[62def07d6] Boyuan Feng 2025-12-27 [BugFix] register quant scale tensors as buffer (#31395)
82	16	vllm/attention/layer.py

[b326598e9] yitingdc 2025-12-28 add tip for VLLM_USE_PRECOMPILED arg to reduce docker build time (#31385)
9	0	docs/deployment/docker.md

[727c41f3f] Robert Shaw 2025-12-27 [MoE Refactor][10/N] Cleanup Fp8 Process Weights After Loading (#31169)
150	194	vllm/model_executor/layers/quantization/fp8.py

[2f12cd32c] Boyuan Feng 2025-12-27 [BugFix] Fix cache issue in compilation_config (#31376)
42	0	tests/compile/test_config.py
5	0	vllm/config/vllm.py

[40a875622] Isotr0py 2025-12-27 [Chore]: Remove HF format Phi4-MM examples (#31405)
0	32	examples/offline_inference/audio_language.py
0	36	examples/offline_inference/vision_language.py
0	35	examples/offline_inference/vision_language_multi_image.py

[3d024985a] Isotr0py 2025-12-27 [CI/Build] Ignore max transformers version for more common tests (#31401)
5	1	tests/models/multimodal/processing/test_common.py
5	1	tests/models/multimodal/processing/test_tensor_schema.py
5	1	tests/models/test_registry.py
5	1	tests/models/utils.py

[8711b2167] baonudesifeizhai 2025-12-26 Fix/get raw stream patch #30905 (#30912)
14	0	tests/compile/fullgraph/test_full_graph.py
27	1	tests/compile/test_config.py
24	0	vllm/env_override.py

[52bf06651] Yifan Qiao 2025-12-26 [Core][Hybrid allocator + connector] Support hybrid allocator + kv cache connector (#30166)
62	8	tests/v1/core/test_single_type_kv_cache_manager.py
13	10	vllm/v1/core/block_pool.py
35	11	vllm/v1/core/kv_cache_coordinator.py
108	42	vllm/v1/core/kv_cache_manager.py
12	4	vllm/v1/core/sched/scheduler.py
122	31	vllm/v1/core/single_type_kv_cache_manager.py
1	1	vllm/v1/worker/gpu_worker.py
7	2	vllm/v1/worker/tpu_worker.py

[5326c8980] Kunshang Ji 2025-12-27 [XPU][CI]skip test_preprocess_error_handling due to fork/spawn issue (#31381)
3	2	tests/v1/engine/test_preprocess_error_handling.py

[87f1b8ca2] Xinyu Chen 2025-12-27 CustomOp: Unify aiter impl into GroupedTopk (#31221)
31	0	vllm/model_executor/layers/fused_moe/fused_moe.py
9	26	vllm/model_executor/layers/fused_moe/layer.py

[887e900b7] rongfu.leng 2025-12-26 [Docs] Add profiler user docs for http request (#31370)
23	0	docs/contributing/profiling.md

[48e744976] Patrick von Platen 2025-12-26 [Mistral common] Ensure all functions are imported from the top & only use public methods (#31138)
1	1	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
1	1	requirements/test.txt
20	53	vllm/tokenizers/mistral.py

[ce1eafd1a] Jee Jee Li 2025-12-26 [Core] Initialize LoRA support for tower and connector in multi-modal models (#26674)
4	0	docs/features/lora.md
25	0	tests/lora/conftest.py
21	7	tests/lora/test_lora_manager.py
125	4	tests/lora/{test_qwen2vl.py => test_qwenvl.py}
6	0	vllm/config/lora.py
19	0	vllm/engine/arg_utils.py
2	1	vllm/lora/layers/__init__.py
8	1	vllm/lora/layers/base_linear.py
8	0	vllm/lora/layers/utils.py
174	41	vllm/lora/model_manager.py
10	0	vllm/lora/worker_manager.py
18	0	vllm/model_executor/models/idefics3.py
18	0	vllm/model_executor/models/interfaces.py
20	0	vllm/model_executor/models/qwen2_5_vl.py
19	0	vllm/model_executor/models/qwen2_vl.py
21	1	vllm/model_executor/models/qwen3_vl.py
22	1	vllm/v1/engine/input_processor.py
79	6	vllm/v1/worker/gpu_model_runner.py
27	14	vllm/v1/worker/lora_model_runner_mixin.py
8	3	vllm/v1/worker/tpu_model_runner.py

[0b544e647] Harry Mellor 2025-12-26 [Docs] Fix some snippets (#31378)
1	1	docs/cli/bench/latency.md
1	1	docs/cli/bench/serve.md
1	1	docs/cli/bench/sweep/plot.md
1	1	docs/cli/bench/sweep/plot_pareto.md
1	1	docs/cli/bench/sweep/serve.md
1	1	docs/cli/bench/sweep/serve_sla.md
1	1	docs/cli/bench/throughput.md
1	1	docs/cli/chat.md
1	1	docs/cli/complete.md
1	1	docs/cli/run-batch.md
1	1	docs/cli/serve.md
2	2	docs/configuration/engine_args.md
1	1	docs/mkdocs/hooks/generate_argparse.py
3	3	docs/mkdocs/hooks/generate_metrics.py
3	3	docs/usage/metrics.md

[c3666f56f] Jee Jee Li 2025-12-26 [Misc] Fix Qwen2-MoE shared_expert_gate (#31339)
0	1	vllm/lora/request.py
8	2	vllm/model_executor/models/qwen2_moe.py

[c79dbfa9a] Andreas Karatzas 2025-12-25 [CI] Fix flaky vision beam search test with flexible semantic validation (#31324)
50	48	tests/entrypoints/openai/test_vision.py

[9ee05cbe7] Shinichi Hemmi 2025-12-26 Support LoRA and GPTQModel for PLaMo 2/3  (#31322)
2	2	docs/models/supported_models.md
25	8	vllm/model_executor/models/plamo2.py
4	7	vllm/model_executor/models/plamo3.py

[3b8f31b36] Ning Xie 2025-12-26 [benchmark] use model card root instead of id (#31329)
6	6	vllm/benchmarks/serve.py

[2cd94259c] Isotr0py 2025-12-26 [CI/Build] Ignore max transformers version skipping for initialization tests (#30619)
52	28	tests/models/registry.py
5	1	tests/models/test_initialization.py

[b7165d53c] oscardev256 2025-12-25 Feature/isaac 0.1 (#28367)
1	0	docs/models/supported_models.md
2	0	requirements/test.in
19	2	requirements/test.txt
25	0	tests/models/multimodal/generation/test_common.py
177	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
4	0	tests/models/registry.py
1480	0	vllm/model_executor/models/isaac.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
4	0	vllm/transformers_utils/configs/__init__.py
86	0	vllm/transformers_utils/configs/isaac.py

[81786c877] Nick Hill 2025-12-25 [BugFix] Fix async scheduling + reasoning with struct output (#31332)
6	2	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	0	tests/v1/structured_output/test_reasoning_structured_output.py
2	1	vllm/v1/structured_output/__init__.py

[f1531d9f2] Stan Wozniak 2025-12-25 [Hybrid] Mamba2 prefix cache blocks freeing for running requests (#28047)
8	0	vllm/v1/core/single_type_kv_cache_manager.py

[2d6001f49] SongHe 2025-12-25 [Model][Ernie4.5-VL] Support video metadata for timestamp rendering (#31274)
2	1	tests/models/multimodal/processing/test_common.py
80	4	vllm/model_executor/models/ernie45_vl.py

[030fc4491] Amir Samani 2025-12-25 use the same stream for cuda graph catpure and replay for NCCL (#29207)
1	1	benchmarks/kernels/benchmark_device_communicators.py
9	21	tests/utils_/test_torch_utils.py
6	2	vllm/compilation/cuda_graph.py
7	3	vllm/utils/torch_utils.py

[2532f437e] Isotr0py 2025-12-25 [Doc] Add troubleshooting for Triton PTX error about undefined gpu-name (#31338)
26	0	docs/usage/troubleshooting.md

[f15185fbd] Louie Tsai 2025-12-25 [Benchmark Suite] improve cpu Benchmark Suite tests and comparison report for 0.12.0 (#30994)
3	16	.buildkite/performance-benchmarks/README.md
574	205	.buildkite/performance-benchmarks/scripts/compare-json-results.py
40	3	.buildkite/performance-benchmarks/tests/serving-tests-cpu.json
52	1	docs/benchmarking/dashboard.md

[ba25a6599] Mark Gatere 2025-12-25 [Frontend] add FunctionGemma tool parser support (#31218)
24	0	docs/features/tool_calling.md
54	0	examples/tool_chat_template_functiongemma.jinja
154	0	tests/tool_parsers/test_functiongemma_tool_parser.py
4	0	vllm/tool_parsers/__init__.py
321	0	vllm/tool_parsers/functiongemma_tool_parser.py

[42826bbcc] Amith KK 2025-12-25 [Doc] Add tool call parser documentation for GPT-OSS models (#31212)
9	0	docs/features/tool_calling.md

[254f6b986] Richard Zou 2025-12-24 [Bugfix] Fix eagle dp tests on A100 (#31241)
7	1	tests/v1/distributed/test_eagle_dp.py

[bc5ef333e] Michael Goin 2025-12-24 [Perf] Add skip_clone to SamplingParams for internal request handling (#31041)
2	1	vllm/entrypoints/api_server.py
4	1	vllm/entrypoints/llm.py
5	0	vllm/entrypoints/openai/protocol.py
1	0	vllm/entrypoints/openai/speech_to_text.py
13	0	vllm/sampling_params.py

[09dc7c690] Cyrus Leung 2025-12-25 [Chore][1/2] Drop `v0.14` deprecations (#31285)
0	1	docs/api/README.md
1	1	tests/entrypoints/openai/test_chat_error.py
1	1	tests/entrypoints/openai/test_completion_error.py
2	2	tests/entrypoints/openai/test_lora_resolvers.py
3	4	vllm/entrypoints/context.py
5	6	vllm/entrypoints/openai/parser/responses_parser.py
1	1	vllm/entrypoints/openai/serving_models.py
0	33	vllm/entrypoints/openai/tool_parsers/__init__.py
0	34	vllm/lora/request.py
2	41	vllm/model_executor/models/interfaces.py
0	2	vllm/multimodal/__init__.py
8	64	vllm/multimodal/inputs.py
1	14	vllm/multimodal/utils.py
0	2	vllm/tokenizers/__init__.py
1	8	vllm/tokenizers/registry.py
2	110	vllm/transformers_utils/tokenizer.py
0	33	vllm/transformers_utils/tokenizer_base.py
0	30	vllm/utils/__init__.py
0	9	vllm/v1/engine/async_llm.py
1	9	vllm/v1/engine/llm_engine.py
0	20	vllm/v1/engine/processor.py
0	2	vllm/v1/worker/gpu_model_runner.py

[506eb0f45] ゆり 2025-12-25 [Bugfix] Remove dead `block_quant_to_tensor_quant` function (#31294)
1	19	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[5d9308968] Ning Xie 2025-12-24 [cli] complete vllm cli help message (#31226)
1	0	vllm/entrypoints/cli/benchmark/main.py
5	1	vllm/entrypoints/cli/serve.py

[66c988744] Kevin McKay 2025-12-24 [Bugfix][Hardware][AMD] Fix FP8 dtype in silu_mul quantization (#31179)
8	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[1ff67df18] wang.yuqi 2025-12-24 [CI] Reorganization pooling_mteb_test (#31265)
1	1	tests/entrypoints/pooling/embed/test_correctness_mteb.py
1	1	tests/entrypoints/pooling/score/test_correctness_mteb.py
4	5	tests/entrypoints/pooling/score/test_utils.py
228	0	tests/models/language/pooling_mteb_test/mteb_embed_utils.py
25	197	tests/models/language/pooling_mteb_test/{mteb_utils.py => mteb_score_utils.py}
36	32	tests/models/language/pooling_mteb_test/test_baai.py
8	6	tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py
11	5	tests/models/language/pooling_mteb_test/test_cross_encoder.py
45	25	tests/models/language/pooling_mteb_test/test_gte.py
17	13	tests/models/language/pooling_mteb_test/test_intfloat.py
12	5	tests/models/language/pooling_mteb_test/test_jina.py
8	4	tests/models/language/pooling_mteb_test/test_mxbai_rerank.py
16	6	tests/models/language/pooling_mteb_test/test_nemotron.py
14	6	tests/models/language/pooling_mteb_test/test_nomic.py
8	4	tests/models/language/pooling_mteb_test/test_qwen3_reranker.py
30	10	tests/models/language/pooling_mteb_test/test_snowflake_arctic_embed.py
11	5	tests/models/language/pooling_mteb_test/test_st_projector.py
5	22	tests/models/utils.py

[7cd288a4b] skaraban3807 2025-12-24 [PERF] Add interleaved memory allocation to NUMA module (#30800)
38	13	csrc/cpu/utils.cpp

[d20180733] Cyrus Leung 2025-12-24 [Chore] Bump `lm-eval` version (#31264)
1	1	.buildkite/lm-eval-harness/run-lm-eval-chartqa-vllm-vlm-baseline.sh
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
1	1	.buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
1	1	docs/features/quantization/fp8.md
1	1	docs/features/quantization/int4.md
1	1	docs/features/quantization/int8.md
1	1	docs/features/quantization/quark.md
1	1	requirements/nightly_torch_test.txt
1	1	requirements/rocm-test.txt
1	2	requirements/test.in
1	1	requirements/test.txt

[aa3868ecf] Cyrus Leung 2025-12-24 [Chore] Remove unused `noqa`s (#31263)
1	1	tests/conftest.py
1	1	tests/entrypoints/openai/test_async_tokenization.py
1	1	tests/entrypoints/openai/test_chat.py
1	1	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
2	2	tests/entrypoints/openai/test_completion_with_function_calling.py
1	1	tests/entrypoints/openai/test_default_mm_loras.py
1	1	tests/entrypoints/openai/test_enable_force_include_usage.py
1	1	tests/entrypoints/openai/test_messages.py
1	1	tests/entrypoints/openai/test_return_tokens_as_ids.py
1	1	tests/models/multimodal/generation/test_qwen2_vl.py
1	1	tests/v1/kv_connector/unit/test_example_connector.py
1	4	vllm/distributed/ec_transfer/ec_connector/example_connector.py
1	1	vllm/entrypoints/serve/elastic_ep/api_router.py
1	2	vllm/model_executor/model_loader/bitsandbytes_loader.py
0	1	vllm/model_executor/model_loader/runai_streamer_loader.py
1	3	vllm/v1/worker/ec_connector_model_runner_mixin.py
1	3	vllm/v1/worker/kv_connector_model_runner_mixin.py

[7adeb4bfa] Cyrus Leung 2025-12-24 [Bugfix] Fix `max_model_len="auto"` handling (#31260)
1	1	vllm/config/model.py
26	15	vllm/engine/arg_utils.py
52	66	vllm/v1/core/kv_cache_utils.py

[bd89ce16d] wang.yuqi 2025-12-24 [Model] Introduce verify_and_update_model_config for VerifyAndUpdateConfig. (#31131)
18	1	vllm/config/model.py
29	26	vllm/model_executor/models/config.py
0	3	vllm/model_executor/models/llama.py

[b41aeb346] Pleaplusone 2025-12-24 [Bugfix][ROCm] Fix load issue on deepseek quark quantization when shared expert enabled (#31261)
11	8	vllm/model_executor/models/deepseek_v2.py

[ddfac7034] Ryan Rock 2025-12-24 [CI/Build] Ignore data_parallel_size_local (#30281)
1	0	vllm/config/parallel.py

[6559d9679] Micah Williamson 2025-12-24 [ROCm][CI] Set TORCH_NCCL_BLOCKING_WAIT Distributed Tests On ROCm (#31259)
9	2	.buildkite/test-amd.yaml

[1c74150bc] kliuae 2025-12-24 [ROCm][CI] Fix "Distributed Tests (H200)" Test (#31227)
1	1	.buildkite/test-amd.yaml

[0247a91e0] Andreas Karatzas 2025-12-24 [ROCm][CI] Fix entrypoints tests and Python-only installation test on ROCm (#28979)
126	26	setup.py
24	0	tests/entrypoints/openai/conftest.py
5	5	tests/entrypoints/openai/test_chat.py
1	0	tests/entrypoints/openai/test_optional_middleware.py
6	1	tests/entrypoints/openai/test_response_api_with_harmony.py
25	4	tests/entrypoints/openai/test_serving_tokens.py
23	11	tests/entrypoints/openai/test_shutdown.py
16	4	tests/entrypoints/openai/test_transcription_validation.py
25	6	tests/entrypoints/openai/test_translation_validation.py
16	1	tests/entrypoints/openai/test_video.py
40	2	tests/entrypoints/openai/test_vision.py
1	0	tests/entrypoints/openai/test_vision_embeds.py
7	5	tests/entrypoints/pooling/basic/test_encode.py
4	5	tests/entrypoints/pooling/basic/test_truncation.py
28	0	tests/entrypoints/pooling/embed/conftest.py
4	5	tests/entrypoints/pooling/embed/test_correctness_mteb.py
7	5	tests/entrypoints/pooling/embed/test_offline.py
13	5	tests/entrypoints/pooling/embed/test_online.py
4	5	tests/entrypoints/pooling/embed/test_online_dimensions.py
4	5	tests/entrypoints/pooling/embed/test_online_long_text.py
4	5	tests/entrypoints/pooling/score/test_correctness_mteb.py
7	5	tests/entrypoints/pooling/score/test_offline.py
4	5	tests/entrypoints/pooling/score/test_online_rerank.py
4	5	tests/entrypoints/pooling/score/test_online_score.py
6	1	tests/standalone_tests/pytorch_nightly_dependency.sh
28	0	vllm/entrypoints/pooling/embed/conftest.py

[8ee90c83f] Michael Goin 2025-12-24 Add `--max-model-len auto` to auto-fit context to available memory (#29431)
10	0	tests/engine/test_arg_utils.py
57	0	tests/v1/core/test_kv_cache_utils.py
8	4	vllm/config/model.py
7	0	vllm/engine/arg_utils.py
209	29	vllm/v1/core/kv_cache_utils.py
11	0	vllm/v1/engine/core.py
13	0	vllm/v1/worker/gpu_worker.py

[d7e05ac74] Nick Cao 2025-12-23 [docker] Fix downloading sccache on aarch64 platform (#30070)
9	3	docker/Dockerfile

[471ddb99a] sihao_li 2025-12-24 [XPU] Remove distributed_executor_backend check  (#30760)
0	27	vllm/platforms/xpu.py

[bb24592d1] Xiong Wang 2025-12-24 [Qwen3-Omni] fixed _get_feat_extract_output_lengths function (#31007)
8	12	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[369f47aa0] Matthew Bonanni 2025-12-24 [DeepSeek v3.2] Remove unnecessary syncwarps (#31047)
1	6	csrc/cache_kernels.cu

[dabff12ed] zejunchen-zejun 2025-12-24 [Bugfix][ROCm][Dynamo][DS 3.1][FP8] fix unsupported hasattr call when Dynamo tracing for ROCm device (#31149)
44	13	vllm/_aiter_ops.py

[3bb956192] Ming Yang 2025-12-23 Revert "[bench] Support common prefix len config (for decode-only bench)" (#31240)
0	1	vllm/benchmarks/datasets.py
0	6	vllm/benchmarks/serve.py

[3ce791ac7] Micah Williamson 2025-12-23 [ROCm][CI] Set VLLM_FLOAT32_MATMUL_PRECISION="tf32" For terratorch Tests In AMD CI (#31242)
11	3	.buildkite/test-amd.yaml
1	1	tests/models/test_terratorch.py

[e42894f5b] Andreas Karatzas 2025-12-23 [ROCm][CI][Bugfix] Fix Siglip2 rotary embedding dispatch and InternVL video test tolerance (#31235)
1	0	tests/models/multimodal/generation/test_common.py
3	1	vllm/model_executor/models/siglip2navit.py

[76e6a9519] Wentao Ye 2025-12-23 [Bug] Fix `Number of dimensions of tensors must match.` for Deepseek V3.2 (#31160)
6	3	vllm/model_executor/models/deepseek_v2.py

[8b59753cd] Chao Lei 2025-12-24 [P/D] Mooncake connector support more protocols (#30133)
7	1	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py

[538e830ca] Chen Zhang 2025-12-23 [KVEvent] User request.block_hash for parent block_hash (#30544)
63	0	tests/v1/core/test_prefix_caching.py
1	3	vllm/v1/core/block_pool.py

[4ed11105d] rongfu.leng 2025-12-24 [Misc] Remove unused custom ops `copy_blocks` and `copy_blocks_mla` (#30967)
0	10	csrc/cache.h
0	88	csrc/cache_kernels.cu
0	10	csrc/torch_bindings.cpp
0	154	tests/kernels/attention/test_cache.py
0	12	vllm/_custom_ops.py
0	12	vllm/_ipex_ops.py

[dd424571c] Cyrus Leung 2025-12-24 [Bugfix] Enable `dynamic_dims` for different embeds shape (#31223)
1	1	vllm/model_executor/models/audioflamingo3.py
1	1	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/qwen2_audio.py

[ca6a95ba2] Cyrus Leung 2025-12-24 [Chore] Simplify logic of `_execute_mm_encoder` (#31222)
21	25	vllm/v1/worker/gpu_model_runner.py

[bc0a5a0c0] Vadim Gimpelson 2025-12-24 [CI] Add Qwen3-Next-FP8 to Blackwell model tests (#31049)
11	0	tests/evals/gsm8k/configs/Qwen3-Next-FP8-EP2.yaml
1	0	tests/evals/gsm8k/configs/models-blackwell.txt
1	0	tests/evals/gsm8k/test_gsm8k_correctness.py
1	0	tests/utils.py

[bfa2c0bbb] Andreas Karatzas 2025-12-23 [ROCm][Bugfix] Fix RuntimeError in MMEncoderAttention by replacing .view() with .reshape() (#31203)
1	1	tests/models/multimodal/conftest.py
2	2	vllm/attention/layers/mm_encoder_attention.py

[f79006860] Mark McLoughlin 2025-12-23 [Core] Add a random suffix to frontend-provided request IDs (#27987)
2	2	tests/v1/engine/test_async_llm.py
7	1	tests/v1/engine/test_engine_core.py
7	1	tests/v1/engine/test_engine_core_client.py
1	0	tests/v1/engine/test_fast_incdec_prefix_err.py
82	44	tests/v1/engine/test_output_processor.py
37	56	tests/v1/engine/test_parallel_sampling.py
15	5	tests/v1/engine/test_process_multi_modal_uuids.py
7	1	tests/v1/engine/utils.py
35	6	tests/v1/kv_connector/unit/test_nixl_connector.py
2	2	vllm/entrypoints/llm.py
6	0	vllm/v1/engine/__init__.py
27	11	vllm/v1/engine/async_llm.py
14	1	vllm/v1/engine/input_processor.py
11	3	vllm/v1/engine/llm_engine.py
66	17	vllm/v1/engine/output_processor.py
9	4	vllm/v1/engine/parallel_sampling.py

[34916ae37] Asaf Joseph Gardin 2025-12-23 [Mamba] - Consolidate Mambas Attention Logic (#28133)
1	5	vllm/model_executor/layers/mamba/short_conv.py
7	138	vllm/v1/attention/backends/mamba1_attn.py
100	220	vllm/v1/attention/backends/mamba2_attn.py
191	1	vllm/v1/attention/backends/mamba_attn.py
6	84	vllm/v1/attention/backends/short_conv_attn.py

[0736f901e] Yuan Tang 2025-12-23 docs: Add llm-d integration to the website (#31234)
1	1	docs/deployment/integrations/kserve.md
5	0	docs/deployment/integrations/llm-d.md
1	0	docs/deployment/k8s.md

[c016c95b4] Harry Mellor 2025-12-23 Use helper function instead of looping through attribute names (#29788)
8	11	vllm/config/model.py
7	3	vllm/config/utils.py

[1339878e1] Harry Mellor 2025-12-23 Only patch `original_max_position_embeddings` for Transformers v4 (#31214)
7	5	vllm/transformers_utils/config.py

[b94f80ffb] danielafrimi 2025-12-23 [FIX] FP4 quantization kernel padding initialization bug (#31097)
28	18	csrc/quantization/fp4/nvfp4_quant_kernels.cu

[38c361f99] Joachim Studnia 2025-12-23 Fix edge case Mistral tool parser (#30724)
34	2	tests/tool_parsers/test_mistral_tool_parser.py
81	54	vllm/tool_parsers/mistral_tool_parser.py

[bb62dda2c] Cyrus Leung 2025-12-23 [Misc] Introduce `encode_*_url` utility function (#31208)
11	5	tests/entrypoints/openai/test_audio.py
7	11	tests/entrypoints/openai/test_video.py
8	12	tests/entrypoints/openai/test_vision.py
1	9	tests/entrypoints/pooling/embed/test_online_vision.py
6	9	tests/entrypoints/test_chat_utils.py
2	5	tests/models/multimodal/generation/test_keye.py
4	9	tests/models/multimodal/generation/test_vit_backend_functionality.py
3	7	tests/v1/ec_connector/integration/test_epd_correctness.py
5	5	tests/v1/entrypoints/openai/serving_responses/test_image.py
2	2	tests/v1/kv_connector/unit/test_example_connector.py
10	15	tests/v1/tpu/test_multimodal.py
7	2	vllm/multimodal/audio.py
14	1	vllm/multimodal/image.py
54	4	vllm/multimodal/utils.py

[3faa8bee5] Patrick von Platen 2025-12-23 adapt voxtral (#31095)
1	0	tests/models/multimodal/generation/test_voxtral.py
5	0	tests/models/registry.py
4	0	vllm/config/model.py
10	0	vllm/model_executor/models/interfaces.py
4	0	vllm/model_executor/models/registry.py
29	11	vllm/model_executor/models/voxtral.py
243	0	vllm/model_executor/models/voxtral_streaming.py
91	81	vllm/model_executor/models/whisper.py
299	0	vllm/model_executor/models/whisper_utils.py
31	3	vllm/transformers_utils/configs/mistral.py
9	0	vllm/v1/attention/backends/utils.py
14	4	vllm/v1/worker/gpu_model_runner.py

[b10d47e0e] Harry Mellor 2025-12-23 Add util function for checking nesting of rope parameters (#31146)
2	4	vllm/config/model.py
2	2	vllm/model_executor/models/transformers/utils.py
19	2	vllm/transformers_utils/config.py

[769f27e70] R3hankhan 2025-12-23 [OpenAI] Add parameter metadata to validation errors (#30134)
11	0	vllm/entrypoints/openai/api_server.py
91	17	vllm/entrypoints/openai/protocol.py
5	9	vllm/entrypoints/openai/serving_chat.py
18	8	vllm/entrypoints/openai/serving_completion.py
62	15	vllm/entrypoints/openai/serving_engine.py
18	8	vllm/entrypoints/openai/serving_responses.py
19	11	vllm/entrypoints/openai/speech_to_text.py
8	4	vllm/entrypoints/renderer.py

[23daef548] Jakub Zakrzewski 2025-12-23 [Frontend] Support using chat template as custom score template for reranking models (#30550)
9	2	docs/models/supported_models.md
15	0	docs/serving/openai_compatible_server.md
27	0	examples/pooling/score/offline_using_template.py
46	0	examples/pooling/score/online_using_template.py
3	0	examples/pooling/score/template/nemotron-rerank.jinja
352	0	tests/entrypoints/pooling/score/test_utils.py
40	24	tests/models/language/pooling_mteb_test/mteb_utils.py
42	0	tests/models/language/pooling_mteb_test/test_nemotron.py
6	0	tests/models/registry.py
1	0	tests/models/utils.py
10	1	vllm/entrypoints/chat_utils.py
11	0	vllm/entrypoints/llm.py
1	0	vllm/entrypoints/openai/api_server.py
1	0	vllm/entrypoints/openai/run_batch.py
3	0	vllm/entrypoints/pooling/score/serving.py
46	16	vllm/entrypoints/score_utils.py
22	0	vllm/model_executor/models/config.py
22	1	vllm/model_executor/models/llama.py
6	2	vllm/model_executor/models/registry.py

[27c6c2f98] Jee Jee Li 2025-12-23 [Bugfix] Fix MoE LoRA bin/pt loading (#31161)
1	26	vllm/lora/lora_model.py
0	34	vllm/lora/utils.py

[73cfb7a72] Weida Hong 2025-12-23 Correct position of docstring of class attributes (#31209)
1	1	vllm/forward_context.py
12	9	vllm/v1/kv_cache_interface.py

[f32cfd7d9] vllmellm 2025-12-23 [ROCm][FEAT] Support AITER RMSNorm quantization fusion pass  (#26575)
189	127	tests/compile/test_fusion.py
144	13	vllm/_aiter_ops.py
103	11	vllm/compilation/matcher_utils.py
4	2	vllm/compilation/pass_manager.py
223	66	vllm/compilation/rocm_aiter_fusion.py

[6b16fff01] Jee Jee Li 2025-12-23 [Bugfix] Fix Jais2ForCausalLM (#31198)
4	25	vllm/model_executor/models/jais2.py

[f1c2c2013] Yan Ma 2025-12-23 [XPU] decrease IGC_ForceOCLSIMDWidth for speculative decoding triton-xpu kernel compilation (#30538)
6	1	docker/Dockerfile.xpu
1	1	docs/features/README.md
3	1	vllm/platforms/xpu.py

[8cef13768] Cyrus Leung 2025-12-23 [Chore] Update more locations to use `attention_config.backend` (#31153)
1	1	benchmarks/benchmark_batch_invariance.py
2	1	tests/compile/distributed/test_fusions_e2e.py

[a37328fc5] quanliu 2025-12-23 [Feature] Batch invariant: Lora (#30097)
8	2	vllm/lora/ops/triton_ops/utils.py

[3e1026235] Pavani Majety 2025-12-22 Revert "[SM100] Enable fp8 compute for prefill MLA (#30746)" (#31197)
3	4	tests/v1/attention/test_mla_backends.py
1	0	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
14	112	vllm/v1/attention/backends/mla/common.py

[612d5ffda] Angela Yi 2025-12-22 [ci] Fix Pytorch compilation test oom in 2.10 (#31194)
1	0	tests/compile/test_dynamic_shapes_compilation.py

[78e5e62bb] Divakar Verma 2025-12-22 [AMD][CI] fix v1/engine test_preprocess_error_handling (#31192)
6	0	tests/v1/engine/test_preprocess_error_handling.py

[b57b96738] Robert Shaw 2025-12-22 [MoE Refactor][7/N] AITER MK (#31102)
5	1	vllm/model_executor/layers/fused_moe/fused_moe.py
9	0	vllm/model_executor/layers/fused_moe/prepare_finalize.py
79	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
51	65	vllm/model_executor/layers/quantization/fp8.py

[6d518ffba] Michael Goin 2025-12-22 [CI Failure] Disable mosaicml/mpt-7b and databricks/dbrx-instruct tests (#31182)
1	1	docs/serving/integrations/langchain.md
6	2	tests/models/registry.py
2	1	tests/tokenizers_/test_detokenize.py

[85aff45e2] Benjamin Chislett 2025-12-22 [Perf] Remove blocking copy in GDN Attention (#31167)
1	1	vllm/v1/attention/backends/gdn_attn.py

[5312a7284] Wentao Ye 2025-12-22 [Bug] Fix `'CutlassMLAImpl' object has no attribute '_workspace_buffer'` (#31173)
7	3	vllm/v1/attention/backends/mla/common.py

[de7174765] Lucas Wilkinson 2025-12-22 [SpecDecode] Simplified alternative padded-speculation acceptance rate fix (#29845)
8	2	tests/v1/spec_decode/test_eagle.py
15	9	vllm/v1/attention/backends/mla/common.py
2	3	vllm/v1/attention/backends/mla/flashattn_mla.py
1	1	vllm/v1/attention/backends/mla/flashmla.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
23	3	vllm/v1/spec_decode/eagle.py
2	0	vllm/v1/spec_decode/utils.py
10	6	vllm/v1/worker/gpu_model_runner.py

[958635405] Michael Goin 2025-12-22 [Doc] Add vllm-metal to hardware plugin documentation (#31174)
1	0	docs/getting_started/installation/README.md
3	0	docs/getting_started/installation/cpu.apple.inc.md

[b10f41c89] Pavani Majety 2025-12-22 [SM100] Enable fp8 compute for prefill MLA (#30746)
4	3	tests/v1/attention/test_mla_backends.py
0	1	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
113	14	vllm/v1/attention/backends/mla/common.py

[7b926e890] Yongye Zhu 2025-12-22 [MoE Refactor][9/N] Use modular kernel for unquantized Triton MoE (#31052)
7	0	tests/kernels/moe/test_moe.py
15	7	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py

[ab3a85fd6] Gregory Shtrasberg 2025-12-22 [ROCm][CI/Build] Fix triton version to one that has triton_kernels required for gpt-oss to run (#31159)
2	2	docker/Dockerfile.rocm_base

[8dd0db687] Boyuan Feng 2025-12-22 [UX] improve profiler error message (#31125)
6	1	vllm/v1/worker/gpu_worker.py

[022f3cea5] TJian 2025-12-23 [ROCm] [Critical]: Remove unused variable (#31156)
0	1	csrc/fused_qknorm_rope_kernel.cu

[a5bc77c25] Micah Williamson 2025-12-22 [AMD][CI] Add "V1 Test e2e + engine" to mi325_8 Agent Pool (#31040)
3	1	.buildkite/test-amd.yaml

[b1c3f96ae] Nicolò Lucchesi 2025-12-22 [CI][Bugfix] Fix `entrypoints/openai/test_audio.py` (#31151)
1	1	tests/entrypoints/openai/test_audio.py

[8f8f469b1] dengyunyang 2025-12-22 [BugFix] skip language model in Encoder (#30242)
2	0	examples/online_serving/disaggregated_encoder/README.md
1	1	vllm/config/model.py
8	1	vllm/model_executor/model_loader/utils.py
61	0	vllm/model_executor/models/adapters.py
12	0	vllm/model_executor/models/interfaces.py
9	1	vllm/model_executor/models/qwen2_5_vl.py
8	0	vllm/model_executor/models/qwen3_vl.py
15	0	vllm/v1/worker/gpu_model_runner.py

[2cf91c2ea] Shengqi Chen 2025-12-22 [CI] add polling for precompiled wheel in python_only_compile.sh, fix index generation for releases (#30781)
10	2	.buildkite/scripts/generate-nightly-index.py
2	1	.buildkite/scripts/upload-wheels.sh
20	8	tests/standalone_tests/python_only_compile.sh

[bd6d5a747] AlonKejzman 2025-12-22 [gpt-oss] Fix harmony parser in streaming responses (#30205)
5	0	vllm/entrypoints/openai/serving_chat.py

[256a33ecb] Li Wang 2025-12-22 [Model] Fix bagel failed to run (#31132)
1	1	vllm/model_executor/models/bagel.py

[c02a2705f] Roger Young 2025-12-22 Update MiniMax-M2 ToolCall and add MiniMax-M2.1 in Docs (#31083)
1	1	docs/models/supported_models.md
166	47	vllm/tool_parsers/minimax_m2_tool_parser.py

[cf8eed7be] Kevin McKay 2025-12-21 [Bugfix][ROCm] Fix typo: is_linear_fp8_enaled -> is_linear_fp8_enabled (#31109)
1	1	vllm/_aiter_ops.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	1	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/input_quant_fp8.py
1	1	vllm/platforms/rocm.py

[44ae85f72] Kevin McKay 2025-12-21 [Misc] Fix typo: 'occured' -> 'occurred' (#31120)
[14c3e6ade] Kevin McKay 2025-12-21 [Misc] Fix spelling typos in model comments (#31117)
1	1	vllm/model_executor/models/config.py
1	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[42b42824a] Kevin McKay 2025-12-21 [Misc] Fix grammar errors in comments and messages (#31115)
3	3	tests/quantization/test_compressed_tensors.py
1	1	vllm/attention/ops/merge_attn_states.py

[ec58c10ce] Kevin McKay 2025-12-21 [Misc] Fix quantization-related typos (#31116)
5	5	tests/kernels/moe/modular_kernel_tools/common.py
1	1	tests/quantization/test_fp8.py
1	1	vllm/utils/deep_gemm.py

[8c084de59] Kevin McKay 2025-12-21 [Misc] Fix spelling typos in comments (#31114)
1	1	.buildkite/scripts/generate-nightly-index.py
1	1	tests/models/multimodal/processing/test_tensor_schema.py
1	1	vllm/reasoning/mistral_reasoning_parser.py

[19cc9468f] CedricHuang 2025-12-22 [Feature]: Support NVIDIA ModelOpt HF FP8 variants FP8_PER_CHANNEL_PER_TOKEN and FP8_PB_WO  in vLLM (#30957)
31	0	docs/features/quantization/modelopt.md
141	0	tests/quantization/test_modelopt.py
12	6	vllm/config/model.py
2	0	vllm/model_executor/layers/linear.py
251	9	vllm/model_executor/layers/quantization/modelopt.py

[097978a15] Jee Jee Li 2025-12-22 [Kernel] Enable fused_qknorm_rope_kernel supports partial rope (#30821)
62	53	csrc/fused_qknorm_rope_kernel.cu
5	2	tests/kernels/core/test_fused_qk_norm_rope.py

[7e065eba5] Lucas Wilkinson 2025-12-21 [CI] Fix "2 Node Tests (4 GPUs in total)" (#31090)
2	2	.buildkite/test-amd.yaml
2	2	.buildkite/test-pipeline.yaml
1	1	.buildkite/test_areas/distributed.yaml
46	22	examples/offline_inference/data_parallel.py

[9d701e90d] Steve Westerhouse 2025-12-21 [Doc] Clarify FP8 KV cache computation workflow (#31071)
21	21	docs/design/paged_attention.md
10	0	docs/features/quantization/quantized_kvcache.md

[06d490282] Michael Goin 2025-12-21 [NVFP4][Perf] Tune NVFP4 input quant kernel for small batch size (#30897)
177	0	benchmarks/kernels/bench_nvfp4_quant.py
4	1	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
14	17	csrc/quantization/fp4/nvfp4_experts_quant.cu
20	42	csrc/quantization/fp4/nvfp4_quant_kernels.cu
28	37	csrc/quantization/fp4/nvfp4_utils.cuh

[b471092d3] Robert Shaw 2025-12-21 [MoE Refactor][4/N] Marlin Fp8 Mk (#31036)
4	0	tests/quantization/test_fp8.py
30	2	vllm/model_executor/layers/fused_moe/config.py
17	21	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
34	40	vllm/model_executor/layers/quantization/fp8.py

[93cabc417] Ameen Patel 2025-12-21 ci: add nvidia-smi warmup before Prime-RL integration test (#31093)
1	0	.buildkite/test-pipeline.yaml

[bb80f69bc] Chauncey 2025-12-21 add aarnphm and chaunceyjiang to the new tool_parser directory (#31088)
1	0	.github/CODEOWNERS

[3e92b2b7a] 汪志鹏 2025-12-21 [BugFix]fix gpt-oss v1/completions response bug (#30608)
21	0	tests/entrypoints/openai/test_response_api_with_harmony.py
5	10	tests/entrypoints/openai/test_serving_chat.py
5	4	vllm/entrypoints/openai/serving_chat.py
9	0	vllm/tool_parsers/openai_tool_parser.py

[7c73ceb58] Jinzhen Lin 2025-12-21 [Quantization] add marlin w4a8/w8a8 check (#31061)
12	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py
12	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
4	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[ae0770fa6] Lucas Wilkinson 2025-12-20 [CI] Fix H200 Distributed test (#31054)
3	3	.buildkite/test-amd.yaml
3	3	.buildkite/test-pipeline.yaml
2	2	.buildkite/test_areas/distributed.yaml
43	121	examples/offline_inference/data_parallel.py

[ee52d9901] Jinzhen Lin 2025-12-21 [Quantization] support logical_widths for fp8 marlin (#30962)
10	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[54c892438] baonudesifeizhai 2025-12-20 [MoE Refactor][5/N] Isolate zero expert to LongCatFlash (#28891)
1	1	tests/test_routing_simulator.py
4	0	vllm/model_executor/layers/fused_moe/__init__.py
2	8	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
6	47	vllm/model_executor/layers/fused_moe/layer.py
2	8	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
189	0	vllm/model_executor/layers/fused_moe/zero_expert_fused_moe.py
1	1	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
6	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/experts_int8.py
2	10	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/gguf.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	2	vllm/model_executor/layers/quantization/modelopt.py
1	1	vllm/model_executor/layers/quantization/moe_wna16.py
2	2	vllm/model_executor/layers/quantization/mxfp4.py
3	3	vllm/model_executor/layers/quantization/quark/quark_moe.py
1	1	vllm/model_executor/layers/quantization/rtn.py
37	14	vllm/model_executor/models/longcat_flash.py

[560ae9638] Yan Ma 2025-12-20 [XPU] enable fp8 online streaming quantization  (#30944)
12	2	vllm/model_executor/layers/quantization/fp8.py
17	105	vllm/model_executor/layers/quantization/ipex_quant.py

[1501a4070] Jeffrey Wang 2025-12-20 [Bugfix] Read truncate_prompt_tokens from pooling_params in AsyncLLM.encode() (#31013)
5	1	vllm/engine/protocol.py
15	1	vllm/v1/engine/async_llm.py

[ff2168bca] Lucas Wilkinson 2025-12-19 [CI] FIx `fixture 'siglip_attention_config' not found` (#31053)
18	0	tests/models/multimodal/pooling/conftest.py

[0be149524] Gregory Shtrasberg 2025-12-19 [ROCm][CI/Build] Update ROCm dockerfiles (#30991)
6	0	docker/Dockerfile.rocm
5	5	docker/Dockerfile.rocm_base

[d52c5096d] zejunchen-zejun 2025-12-20 [Bugfix] fix the alias bug of AttentionBackendEnum when register CUSTOM attention backend to vllm (#30869)
169	0	tests/test_attention_backend_registry.py
4	2	vllm/attention/backends/registry.py

[8a7a41437] Yuxuan Zhang 2025-12-20 GLM-4.7 Tool Parser and Doc Update (#30876)
8	1	docs/features/tool_calling.md
1	1	docs/models/supported_models.md
2	1	vllm/model_executor/models/glm4_moe.py
4	0	vllm/tool_parsers/__init__.py
23	0	vllm/tool_parsers/glm47_moe_tool_parser.py

[95befecc1] Robert Shaw 2025-12-19 [MoE Refactor][2/N] Use Modular Kernels for Fp8 (#30825)
89	49	vllm/model_executor/layers/quantization/fp8.py

[4cf942989] Wentao Ye 2025-12-19 [Bug] Fix `error 'Dynamo failed to run FX node with fake tensors` for Deepseek V3.2 (#31046)
5	2	vllm/model_executor/models/deepseek_v2.py

[83a317f65] Robert Shaw 2025-12-19 [MoE Refactor][3/N] Deprecate cutlass block quant fp8 (b200) (#30990)
0	18	CMakeLists.txt
0	373	csrc/quantization/w8a8/cutlass/moe/blockwise_scaled_group_mm_sm100.cu
0	7	csrc/torch_bindings.cpp
0	92	tests/kernels/moe/test_cutlass_grouped_gemm.py
0	14	vllm/_custom_ops.py
1	157	vllm/model_executor/layers/fused_moe/cutlass_moe.py
0	23	vllm/model_executor/layers/fused_moe/fused_moe.py
2	20	vllm/model_executor/layers/quantization/fp8.py

[5f6477d1d] Lucas Wilkinson 2025-12-19 [BugFix] Fix TypeError: unhashable type: 'dict' when serving deepseek32 (#30924)
4	1	vllm/v1/worker/gpu_model_runner.py

[3bd8335bd] Wentao Ye 2025-12-19 [Refactor] Refactor for `DeepGemmQuantScaleFMT` using cache (#30898)
2	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py
27	7	vllm/utils/deep_gemm.py

[1ab521353] Seiji Eicher 2025-12-19 Make engine core client handshake timeout configurable  (#27444)
92	1	tests/v1/engine/test_engine_core_client.py
6	0	vllm/envs.py
7	2	vllm/v1/engine/core_client.py

[969bbc7c6] Zhonghua Deng 2025-12-20 [Model] Add MiMo-V2-Flash support (#30836)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
2	0	vllm/config/__init__.py
5	0	vllm/config/model.py
49	13	vllm/model_executor/layers/linear.py
8	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
720	0	vllm/model_executor/models/mimo_v2_flash.py
1	0	vllm/model_executor/models/registry.py

[268a972c6] Andrey Talman 2025-12-19 Update Pytorch version update docs (#30982)
9	14	docs/contributing/ci/update_pytorch_version.md

[5fbfa8d9e] Jinzhen Lin 2025-12-19 [Quantization] fix marlin w8a8 check (#30961)
3	6	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[23a1946e3] Shanshan Shen 2025-12-19 [CustomOp][Refactor] Extract common methods for ApplyRotaryEmb CustomOp (#31021)
35	28	vllm/model_executor/layers/rotary_embedding/common.py

[b5545d9d5] Thomas Parnell 2025-12-19 [Bugfix] [Kernel] Triton attention kernels: mask out V blocks that fall outside sliding window (#30887)
12	0	vllm/attention/ops/triton_unified_attention.py

[bd2b52fc2] Nishidha Panpaliya 2025-12-19 [CPU][Bugfix] Fix ppc64le CPU build (#30871)
2	0	csrc/cpu/utils.hpp

[420ba2dbb] Li, Jiang 2025-12-19 Enable aarch64 CPU performance benchmarks (#26494)
5	4	.buildkite/performance-benchmarks/README.md
14	10	.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh
26	0	.buildkite/performance-benchmarks/tests/latency-tests-arm64-cpu.json
130	0	.buildkite/performance-benchmarks/tests/serving-tests-arm64-cpu.json
27	0	.buildkite/performance-benchmarks/tests/throughput-tests-arm64-cpu.json
11	4	docs/benchmarking/dashboard.md

[455949675] Marko Rosenmueller 2025-12-19 [Frontend][Bug] allow tool calls in analysis channel (#28139)
212	0	tests/entrypoints/openai/test_serving_chat_stream_harmony.py
14	58	vllm/entrypoints/openai/serving_chat.py
101	0	vllm/entrypoints/openai/serving_chat_stream_harmony.py

[086b96339] lif 2025-12-19 [Bugfix] Add validation for tool requests when tool_parser is unavailable (#30613)
67	0	tests/entrypoints/openai/test_serving_chat.py
22	9	vllm/entrypoints/openai/serving_chat.py

[9187de9fa] Jinzhen Lin 2025-12-19 [Quantization] enable compressed-tensors marlin support for turing (2) (#31008)
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[ac1c93427] Isotr0py 2025-12-19 [Bugfix] Fix incorrect tiles creation for mm prefix triton attention (#30974)
10	4	vllm/attention/ops/triton_unified_attention.py

[4924ac582] Wenqi Glantz 2025-12-19 Add hidden dimension validation for multimodal embedding inputs (#30968)
223	0	tests/entrypoints/openai/test_embedding_shape_validation.py
249	0	tests/multimodal/test_embedding_shape_validation_unit.py
108	9	vllm/multimodal/parse.py
9	1	vllm/multimodal/processing.py

[096b25c9e] Li, Jiang 2025-12-19 [Doc][CPU] Fix index link for CPU regular release wheels (#31015)
2	2	docs/getting_started/installation/cpu.arm.inc.md
2	2	docs/getting_started/installation/cpu.x86.inc.md

[de08b8f61] Jinzhen Lin 2025-12-19 [Quantization] enable compressed-tensors marlin support for turing (#31000)
1	1	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py

[2ac85a454] Nick Hill 2025-12-18 [BugFix] Fix logprobs with spec decode and modified logits (#30846)
23	10	tests/v1/sample/test_logprobs.py
7	1	vllm/v1/sample/rejection_sampler.py

[7b43db210] Andreas Karatzas 2025-12-18 [ROCm][CI][Bugfix] Multi-Modal Model Support Fixes and Attention Backend Improvements (#30270)
16	5	.buildkite/test-amd.yaml
3	6	tests/models/multimodal/{generation => }/conftest.py
30	2	tests/models/multimodal/generation/test_common.py
1	1	tests/models/multimodal/generation/test_granite_speech.py
0	18	tests/models/multimodal/pooling/conftest.py
31	1	vllm/model_executor/models/transformers/multimodal.py
28	14	vllm/platforms/rocm.py

[6a09612b2] PlatinumGod 2025-12-19 [Bugfix] Fix tool_choice="none" being ignored by GPT-OSS/harmony models (#30867)
77	1	tests/entrypoints/openai/test_serving_chat.py
9	3	vllm/entrypoints/openai/serving_chat.py

[45c0526ac] Nick Hill 2025-12-18 [BugFix] Handle errors when preprocessing added requests (#30895)
4	1	.buildkite/test-pipeline.yaml
56	0	tests/v1/engine/test_preprocess_error_handling.py
33	2	vllm/v1/engine/core.py

[d6b3d39b6] Benjamin Chislett 2025-12-18 [Cleanup] Refactor FlashInferMetadataBuilder (#29128)
403	269	vllm/v1/attention/backends/flashinfer.py

[6ca74bc11] Chendi.Xue 2025-12-18 [NIXL][BUG FIX] Fix both failing issue and accuracy issue with nixl + host_buffer on CUDA (#30419)
21	0	vllm/distributed/kv_transfer/kv_connector/utils.py
30	15	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
2	21	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py

[19c583398] Harry Mellor 2025-12-18 Check for truthy `rope_parameters` not the existence of it (#30983)
5	5	vllm/transformers_utils/config.py

[b0b77c465] Nick Hill 2025-12-18 [BugFix] Fix spec decode + structured outputs + preemption edge case (#30916)
5	1	vllm/v1/worker/gpu_model_runner.py

[634a14bd7] Kayvan Mivehnejad 2025-12-18 Strengthen input validation and tests for 'parse_raw_prompts’. (#30652)
11	10	vllm/inputs/parse.py

[24b65eff0] Chen Zhang 2025-12-18 [BugFix] Spec decode with VLLM_ENABLE_V1_MULTIPROCESSING=0 (#30319)
2	1	vllm/v1/engine/core_client.py

[41b6f9200] Elizabeth Thomas 2025-12-18 Remove all2all backend envvar (#30363)
1	1	.buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep_eplb.sh
1	1	.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh
1	1	.buildkite/test-amd.yaml
1	1	.buildkite/test-pipeline.yaml
1	1	.buildkite/test_areas/distributed.yaml
1	1	docs/design/moe_kernel_features.md
1	1	examples/online_serving/elastic_ep/serve_deepseek_v2.sh
4	1	tests/v1/cudagraph/test_cudagraph_dispatch.py
3	5	vllm/config/compilation.py
22	27	vllm/config/parallel.py
1	1	vllm/engine/arg_utils.py
3	2	vllm/envs.py

[97000a2be] Wentao Ye 2025-12-18 [Bug] Fix compressed tensor not using deepgemm (#30820)
0	1	vllm/model_executor/layers/fused_moe/fused_moe.py
10	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[d2dc5dfc6] Isotr0py 2025-12-19 [Bugfix] Remove `tile_size=64` for mm_prefix triton attention (#30973)
0	7	vllm/attention/ops/triton_unified_attention.py

[b8c477c11] navmarri14 2025-12-18 tuned fused configs for B300 (#30629)
147	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_B300_SXM6_AC,dtype=fp8_w8a8.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=NVIDIA_B300_SXM6_AC,dtype=fp8_w8a8.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=768,device_name=NVIDIA_B300_SXM6_AC,dtype=fp8_w8a8.json

[53ad423f2] jiahanc 2025-12-18 [Perf] enable flashinfer rotary_embedding custom ops in DeepSeek rotary (#30729)
4	1	vllm/model_executor/layers/rotary_embedding/base.py
20	1	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py

[889f8bb25] wz1qqx 2025-12-19 [BugFix]Reclaim resources to prevent memory leaks when use LMCacheMPConnector (#30745)
8	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py
16	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py

[058926d48] Fanli Lin 2025-12-19 [XPU] allow custom workers (e.g. vllm-omni workers) to be used on XPU (#30935)
4	1	vllm/platforms/xpu.py

[700a5ad6c] Isotr0py 2025-12-19 [MM Encoder]: Migrate legacy ViT `MultiHeadAttention` to new `MMEncoderAttention` interface (#30684)
3	2	tests/kernels/attention/test_attention.py
67	11	tests/kernels/attention/test_mha_attn.py
3	3	tests/v1/tpu/test_mha_attn.py
0	132	vllm/attention/layer.py
27	63	vllm/attention/layers/mm_encoder_attention.py
39	14	vllm/attention/ops/vit_attn_wrappers.py
2	2	vllm/model_executor/models/aimv2.py
2	2	vllm/model_executor/models/blip.py
6	5	vllm/model_executor/models/clip.py
2	2	vllm/model_executor/models/deepencoder.py
2	2	vllm/model_executor/models/glm4v.py
2	2	vllm/model_executor/models/hunyuan_vision.py
4	4	vllm/model_executor/models/idefics2_vision_model.py
2	2	vllm/model_executor/models/intern_vit.py
4	4	vllm/model_executor/models/interns1_vit.py
2	2	vllm/model_executor/models/mllama4.py
3	2	vllm/model_executor/models/molmo.py
5	5	vllm/model_executor/models/siglip.py
4	4	vllm/model_executor/models/step3_vl.py
3	3	vllm/model_executor/models/whisper.py

[62be3670c] Alec 2025-12-18 [BugFix] Add sleep to fix tight loop and release GIL (#29476)
7	0	vllm/v1/engine/core.py

[500f26e6d] inkcherry 2025-12-19 [Bugfix] fix DP-aware routing in OpenAI API requests (#29002)
1	0	tests/entrypoints/openai/test_chat_error.py
1	0	tests/entrypoints/openai/test_completion_error.py
1	0	tests/entrypoints/openai/test_serving_chat.py
61	0	tests/v1/engine/test_async_llm.py
1	0	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/entrypoints/openai/serving_completion.py
2	0	vllm/entrypoints/openai/serving_engine.py

[686cbaac6] Nick Hill 2025-12-18 [Cleanup] Remove unused ModelRunner V1 `InputBatch.num_tokens` field (#30218)
9	19	vllm/v1/worker/gpu_input_batch.py
0	5	vllm/v1/worker/gpu_model_runner.py
1	10	vllm/v1/worker/tpu_input_batch.py
2	2	vllm/v1/worker/tpu_model_runner.py

[f4ee2c3d9] Vasiliy Kuznetsov 2025-12-18 fix fp8 online quantization streaming with tp > 1 (#30900)
33	8	vllm/model_executor/layers/quantization/fp8.py

[9a5e96523] Xin Yang 2025-12-18 [LoRA] Set default MXFP4 LoRA backend to Marlin (#30598)
5	5	vllm/model_executor/layers/quantization/mxfp4.py

[326e7c310] wzyrrr 2025-12-19 [Doc] Add Sophgo TPU Support (#30949)
1	0	docs/getting_started/installation/README.md

[0db5439de] Lucas Kabela 2025-12-18 [Bugfix][torch2.10] Fix test_qwen2_5_vl_compilation with 2.10 RC (#30822)
4	3	vllm/compilation/backends.py
8	2	vllm/compilation/caching.py
1	2	vllm/compilation/piecewise_backend.py

[28d15ab56] sarathc-cerebras 2025-12-18 adds jais 2 support (#30188)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
529	0	vllm/model_executor/models/jais2.py
1	0	vllm/model_executor/models/registry.py

[662875823] Wentao Ye 2025-12-18 [Bug] Fix batch invariant in torch 2.10 (#30907)
20	24	vllm/model_executor/layers/batch_invariant.py

[eee600c34] zhrrr 2025-12-18 [Misc] support nsys profile for bench latency (#29776)
14	12	vllm/benchmarks/latency.py

[100f93d2b] Michael Goin 2025-12-18 Filter safetensors files to download if .safetensors.index.json exists (#30537)
29	6	vllm/model_executor/model_loader/weight_utils.py

[96bf50a2c] vllmellm 2025-12-18 [ROCm] Serving Fails on Radeon Due to AITER Dtype Import  (#30952)
18	12	vllm/_aiter_ops.py

[f90d3636e] Li, Jiang 2025-12-18 [Bugfix][CPU] Fix Mac CPU build (#30955)
1	1	csrc/cpu/utils.cpp

[8372be282] Ming Yang 2025-12-18 [moe] Use enable_chunking func (to support disabling chunking) (#29935)
2	2	vllm/model_executor/layers/fused_moe/modular_kernel.py

[8da6ae49c] Andreas Karatzas 2025-12-18 [ROCm][Bugfix] Fix `fa_version` argument error in `flash_attn_maxseqlen_wrapper` for ROCm without aiter (#30909)
5	4	vllm/attention/ops/vit_attn_wrappers.py

[30bb19a76] Lucas Wilkinson 2025-12-18 [BugFix] Partial revert of #29558 (DeepEP HT + PIECEWISE CG support) (#30910)
0	38	tests/compile/test_config.py
14	36	vllm/config/compilation.py

[aa7e83605] Chauncey 2025-12-18 [Bugfix] Fix Unicode issues in GLM-4 tool calling (#30920)
2	1	vllm/tool_parsers/glm4_moe_tool_parser.py

[be2ad5f92] Andreas Karatzas 2025-12-18 [ROCm][Bugfix] fix(structured_output): Skip guidance backend for schemas with patternProperties (#30730)
19	2	vllm/v1/engine/input_processor.py
26	0	vllm/v1/structured_output/backend_guidance.py

[a85724bd6] wangxiyuan 2025-12-18 [Platform] Let EPD work with non-cuda platform (#30225)
4	1	vllm/distributed/ec_transfer/ec_connector/example_connector.py

[11a89cf95] Yifan Qiao 2025-12-17 [Fix][FlexAttention] return max logical block index to handle reused blocks (#30915)
30	1	tests/kernels/test_flex_attention.py
12	3	vllm/v1/attention/backends/flex_attention.py

[e3ab93c89] Li, Jiang 2025-12-18 [CPU] Refactor CPU fused MOE (#30531)
1	0	.buildkite/scripts/hardware_ci/run-cpu-test.sh
2	2	cmake/cpu_extension.cmake
5	5	csrc/cpu/{cpu_attn_macros.h => cpu_arch_macros.h}
9	32	csrc/cpu/cpu_attn_impl.hpp
727	0	csrc/cpu/cpu_fused_moe.cpp
8	0	csrc/cpu/cpu_types_x86.hpp
9	9	csrc/cpu/cpu_wna16.cpp
6	6	csrc/cpu/dnnl_helper.cpp
33	0	csrc/cpu/micro_gemm/cpu_micro_gemm_amx.hpp
38	0	csrc/cpu/micro_gemm/cpu_micro_gemm_impl.hpp
19	0	csrc/cpu/micro_gemm/cpu_micro_gemm_vec.hpp
0	23	csrc/cpu/scratchpad_manager.cpp
0	31	csrc/cpu/scratchpad_manager.h
24	0	csrc/cpu/torch_bindings.cpp
23	1	csrc/cpu/utils.cpp
67	22	csrc/cpu/utils.hpp
3	1	docker/Dockerfile.cpu
1	1	requirements/cpu-build.txt
2	0	requirements/cpu.txt
172	0	tests/kernels/moe/test_cpu_fused_moe.py
36	0	vllm/_custom_ops.py
205	71	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
4	2	vllm/model_executor/layers/fused_moe/layer.py

[fc2ae6d61] Nathan Price 2025-12-18 fix: add warmup for audio preprocessing (#30706)
126	1	vllm/entrypoints/openai/speech_to_text.py

[ec965569d] Yihua Cheng 2025-12-17 [KV connector][LMCache] Only record the cuda event when there are request to store/load (#30814)
1	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py
39	17	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py

[82dc338ad] Divakar Verma 2025-12-17 [AMD][CI] fix lm eval ci arg (#30911)
3	3	.buildkite/test-amd.yaml

[717ac33d9] Vadim Gimpelson 2025-12-18 [PERF] Qwen3-next. Add fp8 cutlass MoE tuned configs. `chmod -x *MI308X.json` (#29553)
147	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
0	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI308X.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
147	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json

[cfb7e5551] Li, Jiang 2025-12-18 [Doc][CPU] Update CPU doc (#30765)
2	2	docker/Dockerfile.cpu
31	3	docs/getting_started/installation/cpu.arm.inc.md
6	5	docs/getting_started/installation/cpu.md
66	5	docs/getting_started/installation/cpu.x86.inc.md
1	1	docs/getting_started/installation/python_env_setup.inc.md

[b166ef20e] zzhxxx 2025-12-18 [refactor] Add prefix support to embed_tokens in DeepSeek MTP (#30788)
1	0	vllm/model_executor/models/deepseek_mtp.py

[5f2f3fba1] Zhengxu Chen 2025-12-17 [compile] Fix CI for test_gpt2_cache_hit (#30902)
10	1	tests/compile/test_aot_compile.py
5	5	vllm/config/compilation.py

[4a8412f77] Matthew Bonanni 2025-12-17 [UX] Reduce DeepGEMM warmup log output to single progress bar (#30903)
99	42	vllm/model_executor/warmup/deep_gemm_warmup.py

[0c738b58b] Bowen Bao 2025-12-17 [Quantization] Support Quark int4-fp8 w4a8 for MoE (#30071)
43	0	vllm/model_executor/layers/quantization/quark/quark.py
158	2	vllm/model_executor/layers/quantization/quark/quark_moe.py

[5a3adf581] gnovack 2025-12-17 fused_moe_lora PDL improvements (#30716)
18	12	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[6fe588765] Isotr0py 2025-12-18 [Chore] Remove v0 dead code for Qwen2.5-omni (#30883)
0	22	vllm/model_executor/models/qwen2_5_omni_thinker.py

[bc3700e0c] Nicolò Lucchesi 2025-12-18 [NIXL] Support P tensor-parallel-size > D tensor-parallel-size (#27274)
3	0	tests/v1/kv_connector/nixl_integration/tp_config_sweep_accuracy_test.sh
223	22	tests/v1/kv_connector/unit/test_nixl_connector.py
40	22	vllm/distributed/kv_transfer/kv_connector/utils.py
290	168	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[fd8afdf38] Micah Williamson 2025-12-17 [ROCm][CI] Reduce Flakiness For test_async_scheduling Using ROCM_ATTN With FP32 (#30811)
2	10	tests/v1/e2e/test_async_scheduling.py
5	1	vllm/v1/attention/backends/rocm_attn.py

[a0b782f9c] SungMinCho 2025-12-17 [Metrics] Model FLOPs Utilization estimation (#30738)
897	0	tests/v1/metrics/test_perf_metrics.py
3	0	vllm/config/observability.py
6	0	vllm/engine/arg_utils.py
5	0	vllm/envs.py
12	1	vllm/v1/core/sched/scheduler.py
16	1	vllm/v1/metrics/loggers.py
1244	0	vllm/v1/metrics/perf.py
3	0	vllm/v1/metrics/stats.py

[ed2897f33] Rafael Vasquez 2025-12-17 [CI][Feature] Adds auto-rebase PR rule (#30875)
12	0	.github/mergify.yml

[74a1ac38b] Isotr0py 2025-12-18 [v1] Add PrefixLM support to TritonAttention backend (#30386)
99	34	tests/models/multimodal/generation/test_multimodal_gguf.py
143	21	vllm/attention/ops/triton_unified_attention.py
0	69	vllm/model_executor/models/gemma3.py
39	0	vllm/v1/attention/backends/triton_attn.py

[05a83dc6e] Nathan Price 2025-12-17 feat(api): Eager chat template warmup to eliminate first-request latency (#30700)
3	0	vllm/entrypoints/openai/api_server.py
49	0	vllm/entrypoints/openai/serving_chat.py

[e3fc374a9] Varun Sundar Rabindranath 2025-12-17 [BugFix] Workspace allocation during profile run : DeepEPHighThroughput + DeepGEMM  (#30899)
4	1	vllm/model_executor/layers/fused_moe/modular_kernel.py

[e06d0bf0a] Andrey Talman 2025-12-17 2.9.1 PyTorch release update (#28495)
1	1	.buildkite/test-amd.yaml
1	1	.buildkite/test-pipeline.yaml
2	2	CMakeLists.txt
1	1	pyproject.toml
1	1	requirements/build.txt
3	3	requirements/cuda.txt
4	4	requirements/rocm-build.txt
3	3	requirements/test.in
4	4	requirements/test.txt
1	1	vllm/model_executor/layers/conv.py

[e3a0f21e6] Xunzhuo 2025-12-18 [docs]: add ecosystem projects sr in docs/governance (#30844)
1	0	docs/governance/committers.md

[7eb6cb6c1] Matthew Bonanni 2025-12-17 [Attention] Update tests to remove deprecated env vars (#30563)
1	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh
41	48	tests/basic_correctness/test_basic_correctness.py
6	3	tests/compile/distributed/test_fusions_e2e.py
40	42	tests/compile/fullgraph/test_basic_correctness.py
3	10	tests/compile/fullgraph/test_full_cudagraph.py
3	4	tests/compile/fullgraph/test_full_graph.py
1	3	tests/distributed/test_context_parallel.py
12	14	tests/distributed/test_pp_cudagraph.py
134	1	tests/engine/test_arg_utils.py
4	9	tests/entrypoints/openai/test_serving_chat.py
27	25	tests/kernels/attention/test_attention_selector.py
39	21	tests/kernels/attention/test_rocm_attention_selector.py
44	51	tests/kernels/test_flex_attention.py
9	3	tests/models/multimodal/generation/test_granite_speech.py
9	15	tests/models/multimodal/pooling/conftest.py
8	0	tests/models/multimodal/pooling/test_siglip.py
2	1	tests/models/quantization/test_fp8.py
7	5	tests/models/test_initialization.py
3	9	tests/v1/attention/test_rocm_attention_backends_selection.py
22	25	tests/v1/attention/utils.py
7	26	tests/v1/cudagraph/test_cudagraph_mode.py
13	12	tests/v1/determinism/test_batch_invariance.py
2	3	tests/v1/determinism/test_online_batch_invariance.py
13	9	tests/v1/e2e/test_async_scheduling.py
16	17	tests/v1/e2e/test_cascade_attention.py
24	19	tests/v1/e2e/test_spec_decode.py
20	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
6	6	tests/v1/kv_connector/nixl_integration/tp_config_sweep_accuracy_test.sh
2	4	tests/v1/kv_connector/unit/test_nixl_connector.py
4	0	tests/v1/kv_connector/unit/utils.py
7	8	tests/v1/kv_offload/test_cpu_offloading.py
12	7	tests/v1/spec_decode/test_eagle.py
40	45	tests/v1/spec_decode/test_max_len.py
1	1	vllm/v1/attention/backends/rocm_attn.py

[9ca8cb38f] Nicolò Lucchesi 2025-12-17 [CI][Bugfix] Fix flaky `tests/entrypoints/openai/test_audio.py::test_chat_streaming_audio` (#30878)
3	1	tests/entrypoints/openai/test_audio.py

[2497228ad] Cyrus Leung 2025-12-17 [Chore] Factor out logic for requesting initial memory (#30868)
27	4	vllm/utils/mem_utils.py
4	16	vllm/v1/worker/gpu_worker.py
25	1	vllm/v1/worker/utils.py

[196cdc322] KimHyemin 2025-12-18 [Model] Gemma3: Support untied word embeddings (#30827)
15	4	vllm/model_executor/models/gemma3.py

[b7b6a60ac] 高鑫崧 2025-12-17 Adapt the old parameter enable_thinking in chat_template_kwargs (#30852)
2	0	vllm/reasoning/deepseek_v3_reasoning_parser.py
2	0	vllm/tokenizers/deepseek_v32.py

[9e67c4ce9] rongfu.leng 2025-12-17 [Docs] fix function name (#30748)
1	1	docs/design/plugin_system.md

[6e9dbcc50] Jialin Ouyang 2025-12-17 [Fix] uniform decode batch check (#30747)
84	0	tests/v1/worker/test_gpu_model_runner.py
37	8	vllm/v1/worker/gpu_model_runner.py

[6482e3895] Hank_ 2025-12-17 chores: adjust the attn register param order (#30688)
1	1	vllm/attention/backends/registry.py

[fb980eb2f] Harry Mellor 2025-12-17 Fix lazy import (#30858)
6	4	vllm/v1/structured_output/utils.py

[84896fda2] baoqian426 2025-12-17 [Bugfix] deepseek-V3.2 self.weights_proj has no bias (#30841)
5	1	vllm/model_executor/models/deepseek_v2.py

[4bf6c2366] Kevin H. Luu 2025-12-17 [ci] Sync test areas yaml file with test-pipeline (#30862)
1	18	.buildkite/test_areas/e2e_integration.yaml
2	2	.buildkite/test_areas/lm_eval.yaml
2	0	.buildkite/test_areas/lora.yaml
2	0	.buildkite/test_areas/models_basic.yaml
3	1	.buildkite/test_areas/pytorch.yaml

[9ad5b2171] Chauncey 2025-12-17 [Refactor] [4/N] Move VLLM_SERVER_DEV endpoints into the serve directory (#30749)
0	1	.buildkite/scripts/hardware_ci/run-amd-test.sh
22	15	.buildkite/test-amd.yaml
20	14	.buildkite/test-pipeline.yaml
19	4	.buildkite/test_areas/entrypoints.yaml
0	13	.buildkite/test_areas/tool_use.yaml
0	0	tests/entrypoints/instrumentator/__init__.py
2	3	tests/entrypoints/{openai => instrumentator}/test_metrics.py
0	0	tests/entrypoints/rpc/__init__.py
1	1	tests/entrypoints/{openai => rpc}/test_collective_rpc.py
0	0	tests/entrypoints/sleep/__init__.py
1	1	tests/entrypoints/{openai => sleep}/test_sleep.py
3	95	vllm/entrypoints/openai/api_server.py
29	0	vllm/entrypoints/serve/__init__.py
0	0	vllm/entrypoints/serve/cache/__init__.py
61	0	vllm/entrypoints/serve/cache/api_router.py
40	0	vllm/entrypoints/serve/instrumentator/server_info.py
0	0	vllm/entrypoints/serve/rpc/__init__.py
61	0	vllm/entrypoints/serve/rpc/api_router.py
0	4	vllm/entrypoints/serve/sleep/api_router.py

[f284d7bd0] Wentao Ye 2025-12-17 [Bug] Fix AttributeError: 'ColumnParallelLinear' object has no attribute `weight_scale_inv` (#30823)
5	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[53cd7f868] Zhengxu Chen 2025-12-17 [compile] Recompile graph module during Dynamo cache loading. (#30743)
1	0	vllm/compilation/caching.py

[7b966ae2b] danielafrimi 2025-12-17 [Fix]Load kv-cache dtype from hf_quant_config.json automatically (fix for reverted PR) (#30785)
8	1	vllm/engine/arg_utils.py
75	0	vllm/utils/torch_utils.py

[9db1db594] Zhengxu Chen 2025-12-17 [compile] Ignore VLLM_FORCE_AOT_LOAD from cache factors (#30809)
1	0	vllm/envs.py

[177c391db] Zhengxu Chen 2025-12-17 [compile] Disable aot when eager backend is used. (#30810)
9	2	vllm/compilation/decorators.py

[519ef9a91] Michael Goin 2025-12-17 [UX] Make `vllm bench serve` discover model by default and use --input-len (#30816)
3	6	tests/benchmarks/test_serve_cli.py
76	7	vllm/benchmarks/serve.py

[a10015228] Ye (Charlotte) Qi 2025-12-17 [Kernels][FI] Skip trtllm attention when num_kv_heads=1 (#30842)
35	0	tests/kernels/attention/test_flashinfer_trtllm_attention.py
21	1	vllm/utils/flashinfer.py

[4c054d89a] Andrew Xia 2025-12-17 [Doc][ResponsesAPI] add documentation (#30840)
27	0	docs/serving/openai_compatible_server.md
14	4	vllm/entrypoints/openai/protocol.py

[f4e884f22] Sheng Lin 2025-12-17 [NIXL][Bugfix] Fix NIXL/RDMA registration failure over CuMemAllocator (#29569)
10	0	csrc/cumem_allocator.cpp

[3b1d440ed] Xinyu Chen 2025-12-17 CustomOp: grouped topk (#29575)
6	4	tests/kernels/moe/test_grouped_topk.py
2	2	vllm/model_executor/layers/fused_moe/__init__.py
52	0	vllm/model_executor/layers/fused_moe/fused_moe.py
15	8	vllm/model_executor/layers/fused_moe/layer.py

[a9e15c21e] Asaf Joseph Gardin 2025-12-17 [Mamba] Removed disable cascade attn in MambaModelConfig (#30712)
0	6	vllm/model_executor/models/config.py

[20fda4315] Robin 2025-12-17 [Bugfix][Frontend] Prevent IndexError in MiniMax M2 tool parser during streaming extraction (#30555)
119	0	tests/tool_use/test_minimax_m2_tool_parser.py
18	4	vllm/tool_parsers/minimax_m2_tool_parser.py

[4f735babb] Yan Ma 2025-12-17 [XPU] fix broken fp8 online quantization for XPU platform (#30831)
35	0	vllm/model_executor/layers/quantization/ipex_quant.py

[0cd535364] Li, Jiang 2025-12-17 [Bugfix][CPU] Fix CPU backend ROPE dispatch for VL models (#30829)
9	0	vllm/model_executor/layers/rotary_embedding/common.py

[d4d275173] Michael Goin 2025-12-17 Update note comment for flashinfer attention warmup (#30711)
3	4	vllm/model_executor/warmup/kernel_warmup.py

[009a77382] shanjiaz 2025-12-17 bump up compressed tensors version to 0.13.0 (#30799)
1	1	requirements/common.txt

[44d3b1df3] Cyrus Leung 2025-12-17 [CI/Build] Fix compatibility between #30244 and #30396 (#30787)
3	1	tests/compile/distributed/test_fusions_e2e.py

[bb5ac1fe3] Fadi Arafeh 2025-12-17 [CPU] Add action to automatically label CPU related PRs (#30678)
14	0	.github/mergify.yml

[811cdf519] Michael Goin 2025-12-16 Update model-hosting-container-standards to 0.1.10 (#30815)
2	2	requirements/common.txt

[f5db6385a] Grzegorz K. Karch 2025-12-17 Fix nemotron_nas intermediate_size computation (#30795)
7	4	vllm/model_executor/models/nemotron_nas.py

[c0a88df7f] Amr Mahdi 2025-12-17 [docker] Allow kv_connectors install to fail on arm64 (#30806)
1	1	docker/Dockerfile

[e087fbc39] Nicolò Lucchesi 2025-12-17 [MM] Pass FA version in ViT Attn (#30756)
6	0	vllm/attention/layers/mm_encoder_attention.py
8	1	vllm/attention/ops/vit_attn_wrappers.py

[e80455ca8] Michael Goin 2025-12-16 Replace deprecated enable_fusion with fuse_norm_quant in test_rms_group_quant (#30817)
[2410132bb] TJian 2025-12-17 [ROCm] [Bugfix] Fix torch sdpa hallucination (#30789)
8	0	vllm/attention/ops/vit_attn_wrappers.py

[0a1ab1e56] Michael Goin 2025-12-16 [Perf][Kernels] Vectorize `csrc/activations_kernels.cu` (#29512)
2	2	benchmarks/kernels/benchmark_activation.py
174	36	csrc/activation_kernels.cu

[b6ec077e0] Wentao Ye 2025-12-16 [CI] Skip ci failure test (#30804)
3	1	tests/compile/distributed/test_fusions_e2e.py

[ce96857fd] Jinzhen Lin 2025-12-17 [Kernel][Quantization][MoE] add marlin kernel support for turing (sm75) (#29901)
76	31	CMakeLists.txt
1	0	csrc/moe/marlin_moe_wna16/.gitignore
76	56	csrc/moe/marlin_moe_wna16/generate_kernels.py
55	153	csrc/moe/marlin_moe_wna16/marlin_template.h
30	24	csrc/moe/marlin_moe_wna16/ops.cu
1	0	csrc/quantization/gptq_marlin/.gitignore
1	1	csrc/quantization/gptq_marlin/dequant.h
76	56	csrc/quantization/gptq_marlin/generate_kernels.py
39	29	csrc/quantization/gptq_marlin/gptq_marlin.cu
60	14	csrc/quantization/gptq_marlin/marlin.cuh
269	0	csrc/quantization/gptq_marlin/marlin_mma.h
40	144	csrc/quantization/gptq_marlin/marlin_template.h
1	1	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
1	1	vllm/model_executor/layers/quantization/modelopt.py

[eaa82a709] Daniel Cámpora 2025-12-16 [Bugfix][DSV32] Fix overflow in topk. (#30754)
10	8	csrc/sampler.cu

[f5f51e593] Roger Wang 2025-12-16 [Core][MM] Optimize encoder cache manager by operating with embeddings only (#30475)
2	2	tests/models/multimodal/processing/test_mllama4.py
92	0	tests/multimodal/test_utils.py
74	5	tests/v1/core/test_encoder_cache_manager.py
1	1	tests/v1/ec_connector/unit/test_ec_example_connector.py
1	1	vllm/distributed/ec_transfer/ec_connector/example_connector.py
2	6	vllm/model_executor/models/qwen3_vl.py
35	4	vllm/multimodal/inputs.py
7	25	vllm/multimodal/profiling.py
1	1	vllm/multimodal/registry.py
43	37	vllm/v1/core/encoder_cache_manager.py
26	9	vllm/v1/core/sched/scheduler.py
3	3	vllm/v1/request.py
13	36	vllm/v1/worker/gpu_model_runner.py
6	0	vllm/v1/worker/utils.py

[9fec0e13d] Lucas Wilkinson 2025-12-16 [Attention] Cache attention metadata builds across hybrid KV-cache groups (#29627)
1	1	tests/v1/attention/test_chunked_local_attention.py
12	4	vllm/attention/layers/chunked_local_attention.py
2	2	vllm/envs.py
13	0	vllm/v1/attention/backends/flash_attn.py
27	0	vllm/v1/attention/backends/mamba2_attn.py
27	5	vllm/v1/attention/backends/utils.py
23	1	vllm/v1/worker/gpu_model_runner.py

[254a7f8fd] jiahanc 2025-12-16 [Perf] Do FP4 quant before All gather on flashinfer trtllmgen MOE  (#30014)
24	5	vllm/distributed/device_communicators/all2all.py
6	1	vllm/distributed/device_communicators/base_device_communicator.py
11	5	vllm/distributed/device_communicators/cuda_communicator.py
10	3	vllm/distributed/parallel_state.py
12	0	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
39	2	vllm/model_executor/layers/fused_moe/layer.py
24	1	vllm/model_executor/layers/quantization/modelopt.py
22	14	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
17	0	vllm/utils/flashinfer.py

[f21f5ea38] Wentao Ye 2025-12-16 [Refactor] Small refactor for group topk (#30562)
10	3	csrc/moe/grouped_topk_kernels.cu
0	1	tests/v1/determinism/test_batch_invariance.py

[ca702a14d] Nicolò Lucchesi 2025-12-16 [Frontend] Add `max-completion-token` option to transcription/translation endpoints (#30769)
32	0	tests/entrypoints/openai/test_transcription_validation_whisper.py
33	0	tests/entrypoints/openai/test_translation_validation.py
6	0	vllm/entrypoints/openai/protocol.py
8	2	vllm/entrypoints/openai/speech_to_text.py

[10ee1c64c] Michael Goin 2025-12-16 [CI] Generalize gsm8k test args and add Qwen3-Next MTP B200 test (#30723)
2	2	.buildkite/test-pipeline.yaml
9	4	tests/evals/gsm8k/README.md
1	2	tests/evals/gsm8k/configs/DeepSeek-V2-Lite-Instruct-FP8.yaml
1	1	tests/evals/gsm8k/configs/Llama-3-8B-Instruct-nonuniform-CT.yaml
1	1	tests/evals/gsm8k/configs/Llama-3.2-1B-Instruct-INT8-CT.yaml
1	1	tests/evals/gsm8k/configs/Qwen1.5-MoE-W4A16-CT.yaml
1	1	tests/evals/gsm8k/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
1	1	tests/evals/gsm8k/configs/Qwen3-0.6B-FP8.yaml
1	2	tests/evals/gsm8k/configs/Qwen3-30B-A3B-NVFP4.yaml
12	0	tests/evals/gsm8k/configs/Qwen3-Next-80B-A3B-NVFP4-EP2.yaml
1	0	tests/evals/gsm8k/configs/models-blackwell.txt
3	5	tests/evals/gsm8k/conftest.py
41	29	tests/evals/gsm8k/test_gsm8k_correctness.py
3	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[66c3537e5] Mark McLoughlin 2025-12-16 [Docs][API] Remove warning about LoRARequest being internal-only (#30774)
0	5	vllm/lora/request.py

[e1625498f] Harry Mellor 2025-12-16 Update where `bytes_to_unicode` is imported from (#30771)
4	6	vllm/v1/structured_output/utils.py

[0b0acc758] Harry Mellor 2025-12-16 Remove `head_mask` from Ultravox and Swin (#30764)
1	15	vllm/model_executor/models/swin.py
15	2	vllm/model_executor/models/ultravox.py

[af506fd76] Harry Mellor 2025-12-16 Fix instantiation of `HfHubHTTPError` in LoRA test (#30768)
5	2	tests/lora/test_utils.py

[ce12b407f] Ming Yang 2025-12-16 [TRTLLM] Remove the MoE GEMM weight name change (#30713)
4	12	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
4	12	vllm/model_executor/layers/quantization/modelopt.py
8	16	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[59bd5f6a7] Wentao Ye 2025-12-16 [Feat] Enable eplb with default all2all backend (#30559)
4	4	vllm/model_executor/layers/fused_moe/shared_fused_moe.py

[00a8d7628] Lucas Wilkinson 2025-12-16 [BugFix] Fix memory spike in workspace allocation (#30744)
2	0	.buildkite/test-pipeline.yaml
11	3	vllm/v1/worker/workspace.py

[4de08ad69] Isotr0py 2025-12-16 [CI/Build] Skip broken ViT backend functionality test tempoarily (#30782)
1	0	tests/models/multimodal/generation/test_vit_backend_functionality.py

[75eb302a2] Nicolò Lucchesi 2025-12-16 [Bugfix] Whisper fix number of allocated CrossAttn blocks per-request (#30772)
11	11	vllm/v1/core/sched/scheduler.py

[9dbbc59b1] Pleaplusone 2025-12-16 [ROCm][MTP] Support MTP for AITER MLA backend (#28624)
15	9	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[104003dc7] Boyuan Feng 2025-12-16 update piecewise cudagraph warning when splitting_ops=[] (#30728)
1	1	vllm/config/compilation.py

[d0fb57292] TJian 2025-12-16 [ROCm] [AITER] [DOC] Add usage description about check functions in `_aiter_ops` (#30586)
88	15	vllm/_aiter_ops.py
0	3	vllm/platforms/rocm.py

[6f15ac5de] Harry Mellor 2025-12-16 Don'e assume `position_embedding_type` will be present for BERT and RoBERTa models (#30770)
3	1	vllm/model_executor/models/bert.py
5	11	vllm/model_executor/models/roberta.py

[676db55ee] Junru Shen 2025-12-16 [Bugfix] Fix prefix_repetition routing in bench throughput (#29663)
4	1	vllm/benchmarks/throughput.py

[0e391e757] Jee Jee Li 2025-12-16 [Bugfix] Fix RequestOutput miss lora_request (#30636)
5	1	tests/lora/test_gptoss_tp.py
8	1	tests/lora/test_llama_tp.py
6	5	vllm/v1/engine/output_processor.py

[0d0c929f2] Andrew Xia 2025-12-16 [responsesAPI][8] input/output messages for ResponsesParser (#30158)
6	0	tests/entrypoints/openai/test_response_api_parsable_context.py
28	0	vllm/entrypoints/context.py
37	1	vllm/entrypoints/openai/parser/responses_parser.py
3	10	vllm/entrypoints/openai/serving_responses.py
0	33	vllm/entrypoints/responses_utils.py

[e94384bba] Isotr0py 2025-12-16 [Bugfix] Fix broken ViT attention selection for Blackwell device (#30731)
2	8	vllm/model_executor/models/vision.py

[b9ff4f2a8] jiangkuaixue123 2025-12-16 [feature] extend DBO to XBO (#30120)
1	0	tests/v1/attention/test_attention_splitting.py
10	0	vllm/config/parallel.py
5	2	vllm/config/vllm.py
6	0	vllm/engine/arg_utils.py
9	5	vllm/v1/attention/backends/utils.py
4	4	vllm/v1/worker/dp_utils.py
26	7	vllm/v1/worker/gpu_model_runner.py
19	16	vllm/v1/worker/gpu_ubatch_wrapper.py
37	34	vllm/v1/worker/ubatch_utils.py
16	5	vllm/v1/worker/ubatching.py

[c881db364] Boyuan Feng 2025-12-15 improve lazy import test (#30733)
6	25	tests/standalone_tests/lazy_imports.py

[3bd9c4915] Shanshan Shen 2025-12-16 [CustomOp] Extract ApplyRotaryEmb as CustomOp and unify the dispatch logic (#29873)
203	0	tests/kernels/core/test_apply_rotary_emb.py
17	3	vllm/model_executor/layers/rotary_embedding/base.py
153	71	vllm/model_executor/layers/rotary_embedding/common.py
10	3	vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope.py
20	5	vllm/model_executor/layers/rotary_embedding/mrope.py
62	4	vllm/model_executor/layers/rotary_embedding/xdrope.py
13	27	vllm/model_executor/models/dots_ocr.py
14	49	vllm/model_executor/models/ernie45_vl.py
10	3	vllm/model_executor/models/glm4_1v.py
9	13	vllm/model_executor/models/keye.py
11	44	vllm/model_executor/models/paddleocr_vl.py
9	3	vllm/model_executor/models/qwen2_5_vl.py
7	14	vllm/model_executor/models/qwen2_vl.py
15	41	vllm/model_executor/models/siglip2navit.py

[ff21a0fc8] Amr Mahdi 2025-12-16 [docker] Restructure Dockerfile for more efficient and cache-friendly builds (#30626)
157	115	docker/Dockerfile
-	-	docs/assets/contributing/dockerfile-stages-dependency.png

[bbd850e59] penfree 2025-12-16 [Bugfix] fix streaming final output for non harmony (#30237)
45	0	tests/entrypoints/openai/test_response_api_simple.py
32	2	vllm/entrypoints/context.py
2	1	vllm/entrypoints/openai/serving_responses.py

[511e81e7c] Shengqi Chen 2025-12-16 [BUILD] use sm_100f when compiling flashmla to fix support on sm103 (#30705)
11	5	cmake/external_projects/flashmla.cmake

[a182be430] Matthew Bonanni 2025-12-15 [UX][Attention] Add `attention_config` argument to `LLM()` (#30710)
28	44	vllm/entrypoints/llm.py

[c01d58981] Kevin Musgrave 2025-12-15 [Benchmarks] `auto_tune.sh`: Use hostname variable for server requests (#30529)
11	2	benchmarks/auto_tune/auto_tune.sh

[60dbf7d8f] Matthew Bonanni 2025-12-15 Update batch invariant to use attention config (#30704)
22	17	vllm/model_executor/layers/batch_invariant.py
2	1	vllm/v1/worker/gpu_worker.py

[a450c64a3] Michael Goin 2025-12-15 [Bugfix] Fail instead of ignoring when CompilationConfig gets invalid args (#30708)
0	8	tests/benchmarks/test_param_sweep.py
4	4	vllm/config/compilation.py

[b2191abdc] Fadi Arafeh 2025-12-15 [docs][fix]  Update Arm CPU vLLM wheel installation docs (#30594)
20	12	docs/getting_started/installation/cpu.arm.inc.md

[51e5b3e3c] Matthew Bonanni 2025-12-15 [Bugfix] Fix ViT with FlashAttention on ROCm (#30703)
4	1	vllm/attention/layer.py

[ec154c36e] Isotr0py 2025-12-16 [Platform] Refactor Platform attention backend selection to avoid breakpoint for OOT platform (#30212)
36	23	vllm/attention/selector.py
4	11	vllm/platforms/cpu.py
15	59	vllm/platforms/cuda.py
2	10	vllm/platforms/interface.py
9	13	vllm/platforms/rocm.py
3	10	vllm/platforms/tpu.py
4	11	vllm/platforms/xpu.py

[970713d4a] Harry Mellor 2025-12-15 Remove `SkipValidation` from `ModelConfig` (#30695)
14	5	vllm/config/model.py

[17fec3af0] mondaylord 2025-12-16 [Bugfix] Fix missing first token in tool calls during reasoning-to-tool transition (#30671)
28	32	vllm/entrypoints/openai/serving_chat.py

[855b101d7] yjc9696 2025-12-15 [Frontend] add tools for dsv32 developer role (#30040)
9	0	vllm/entrypoints/chat_utils.py

[d0502b492] Robert Shaw 2025-12-15 [MoE][Refactor 1/N] Separate Online Quantization (#30627)
154	89	vllm/model_executor/layers/quantization/fp8.py

[3f175f18a] Max Hu 2025-12-15 [Bugfix] Fix multimodal configuration for Qwen3VL MOE model (#30670)
5	0	vllm/model_executor/models/qwen3_vl_moe.py

[ed586e772] Cyrus Leung 2025-12-15 [Refactor] [3/N] Move tool parser tests and run on CPU (#30693)
5	15	.buildkite/test-amd.yaml
5	12	.buildkite/test-pipeline.yaml
3	1	.buildkite/test_areas/misc.yaml
1	11	.buildkite/test_areas/tool_use.yaml
0	0	tests/tool_parsers/__init__.py
0	0	tests/{tool_use => tool_parsers}/test_deepseekv31_tool_parser.py
0	0	tests/{tool_use => tool_parsers}/test_ernie45_moe_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_glm4_moe_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_jamba_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_kimi_k2_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_minimax_tool_parser.py
0	0	tests/{tool_use => tool_parsers}/test_mistral_tool_parser.py
0	0	tests/{tool_use => tool_parsers}/test_openai_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_qwen3coder_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_seed_oss_tool_parser.py
0	2	tests/{tool_use => tool_parsers}/test_xlam_tool_parser.py

[2a1776b7a] Chauncey 2025-12-15 [Refactor] [2/N] Move tool parsers into the vLLM main directory (#30675)
2	2	docs/features/tool_calling.md
1	1	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/entrypoints/openai/tool_parsers/test_gigachat3_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_hunyuan_a13b_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
1	1	tests/entrypoints/openai/tool_parsers/utils.py
3	3	tests/models/language/generation/test_mistral.py
2	2	tests/tool_use/test_deepseekv31_tool_parser.py
1	1	tests/tool_use/test_ernie45_moe_tool_parser.py
2	2	tests/tool_use/test_glm4_moe_tool_parser.py
1	1	tests/tool_use/test_jamba_tool_parser.py
1	1	tests/tool_use/test_kimi_k2_tool_parser.py
1	1	tests/tool_use/test_minimax_tool_parser.py
1	1	tests/tool_use/test_mistral_tool_parser.py
1	1	tests/tool_use/test_openai_tool_parser.py
4	4	tests/tool_use/test_qwen3coder_tool_parser.py
1	1	tests/tool_use/test_seed_oss_tool_parser.py
1	1	tests/tool_use/test_tool_choice_required.py
1	1	tests/tool_use/test_xlam_tool_parser.py
1	1	vllm/entrypoints/context.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/cli_args.py
1	1	vllm/entrypoints/openai/parser/responses_parser.py
2	2	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_engine.py
23	140	vllm/entrypoints/openai/tool_parsers/__init__.py
150	0	vllm/tool_parsers/__init__.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/abstract_tool_parser.py
1	3	vllm/{entrypoints/openai => }/tool_parsers/deepseekv31_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/deepseekv32_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/deepseekv3_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/ernie45_tool_parser.py
1	1	vllm/{entrypoints/openai => }/tool_parsers/gigachat3_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/glm4_moe_tool_parser.py
4	4	vllm/{entrypoints/openai => }/tool_parsers/granite_20b_fc_tool_parser.py
4	4	vllm/{entrypoints/openai => }/tool_parsers/granite_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/hermes_tool_parser.py
4	4	vllm/{entrypoints/openai => }/tool_parsers/hunyuan_a13b_tool_parser.py
4	4	vllm/{entrypoints/openai => }/tool_parsers/internlm2_tool_parser.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/jamba_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/kimi_k2_tool_parser.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/llama4_pythonic_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/llama_tool_parser.py
1	1	vllm/{entrypoints/openai => }/tool_parsers/longcat_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/minimax_m2_tool_parser.py
4	4	vllm/{entrypoints/openai => }/tool_parsers/minimax_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/mistral_tool_parser.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/olmo3_tool_parser.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/openai_tool_parser.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/phi4mini_tool_parser.py
2	2	vllm/{entrypoints/openai => }/tool_parsers/pythonic_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/qwen3coder_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/qwen3xml_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/seed_oss_tool_parser.py
3	3	vllm/{entrypoints/openai => }/tool_parsers/step3_tool_parser.py
0	0	vllm/{entrypoints/openai => }/tool_parsers/utils.py
1	1	vllm/{entrypoints/openai => }/tool_parsers/xlam_tool_parser.py

[185c22bf2] Nicolò Lucchesi 2025-12-15 [Misc][Hybrid allocator + kv connector] Optionally enable hybrid allocator + KV cache connector (#29805)
3	1	vllm/config/scheduler.py
60	36	vllm/config/vllm.py
1	1	vllm/engine/arg_utils.py

[e4806d973] duke 2025-12-15 [BugFix] Add embed_input_ids method to make QWenLMHeadModel a vllm model (#30674)
3	0	vllm/model_executor/models/qwen.py

[4429d934d] wang.yuqi 2025-12-15 [Model] Automatic conversion of TokenClassification model (#30666)
31	0	tests/models/language/pooling/test_token_classification.py
1	0	tests/models/registry.py
1	0	vllm/config/model.py
12	0	vllm/model_executor/models/adapters.py

[33278073d] ゆり 2025-12-15 typing: Add type hints to TurnMetrics class in context.py (#30552)
7	7	vllm/entrypoints/context.py

[1adeb3b84] 汪志鹏 2025-12-15 [New Model] BAGEL support (AR only) (#28439)
1	0	docs/models/supported_models.md
27	0	examples/offline_inference/vision_language.py
1	0	tests/models/registry.py
584	0	vllm/model_executor/models/bagel.py
32	0	vllm/model_executor/models/qwen2.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
53	0	vllm/transformers_utils/configs/bagel.py
2	0	vllm/transformers_utils/processors/__init__.py
73	0	vllm/transformers_utils/processors/bagel.py

[e3a1cd1c5] Kunshang Ji 2025-12-15 [XPU] fix Dockerfile.xpu, avoid wheel conflicts (#30662)
3	0	docker/Dockerfile.xpu

[3778673ea] Wentao Ye 2025-12-14 [Feat] Refactor for `parallel_config` in `FusedMoEModularKernel` (#30282)
2	1	tests/kernels/moe/modular_kernel_tools/common.py
14	0	tests/kernels/moe/test_flashinfer.py
0	2	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	6	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
13	8	vllm/model_executor/layers/fused_moe/modular_kernel.py
0	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	6	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[b337647aa] Seokhyun An 2025-12-15 [Bugfix] Drop empty tool_calls lists to keep assistant replies in chat template (#30648)
11	6	vllm/entrypoints/chat_utils.py

[a524d1ba0] Jee Jee Li 2025-12-15 [Bugfix] Fix  deepseek_v32 tokenizer_mode  (#30658)
1	1	vllm/entrypoints/openai/serving_engine.py
0	0	vllm/tokenizers/{deepseekv32.py => deepseek_v32.py}
1	1	vllm/tokenizers/registry.py
1	1	vllm/v1/structured_output/backend_xgrammar.py

[87b4d1557] Shanshan Shen 2025-12-15 [CustomOp][MM] Extract MMEncoderAttention as CustomOp and replace the backend of QwenVisionAttention with it. (#30125)
434	0	tests/models/multimodal/generation/test_vit_backend_functionality.py
5	68	vllm/attention/layer.py
284	0	vllm/attention/layers/mm_encoder_attention.py
3	8	vllm/attention/ops/vit_attn_wrappers.py
46	83	vllm/model_executor/models/dots_ocr.py
28	80	vllm/model_executor/models/ernie45_vl.py
46	91	vllm/model_executor/models/glm4_1v.py
27	80	vllm/model_executor/models/keye.py
1	7	vllm/model_executor/models/opencua.py
6	16	vllm/model_executor/models/ovis2_5.py
30	75	vllm/model_executor/models/paddleocr_vl.py
1	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
48	76	vllm/model_executor/models/qwen2_5_vl.py
47	96	vllm/model_executor/models/qwen2_vl.py
12	8	vllm/model_executor/models/qwen3_omni_moe_thinker.py
24	22	vllm/model_executor/models/qwen3_vl.py
1	2	vllm/model_executor/models/qwen3_vl_moe.py
45	84	vllm/model_executor/models/siglip2navit.py
8	5	vllm/model_executor/models/vision.py
36	18	vllm/platforms/cuda.py
38	7	vllm/platforms/interface.py
38	19	vllm/platforms/rocm.py
27	1	vllm/platforms/tpu.py
29	7	vllm/platforms/xpu.py

[84e23d103] Wenqi Glantz 2025-12-14 additional protection for CVE-2025-62164 (#30649)
342	0	tests/entrypoints/openai/test_sparse_tensor_validation.py
134	0	tests/multimodal/test_sparse_tensor_validation_unit.py
14	11	vllm/entrypoints/renderer.py
10	2	vllm/multimodal/audio.py
10	2	vllm/multimodal/image.py

[738648fb8] Shanshan Shen 2025-12-15 [CustomOp] Support object-level enable for CustomOp (#30547)
7	2	vllm/model_executor/custom_op.py

[917fdae5b] Boyuan Feng 2025-12-14 [Log] Skip piecewise cudagraph warn when using full cudagraph (#30657)
7	3	vllm/config/compilation.py

[e2ed23888] Robert Shaw 2025-12-14 Revert "[Fix]Load kv-cache dtype from hf_quant_config.json automatically" (#30653)
2	23	vllm/utils/torch_utils.py

[174e39ead] Or Ozeri 2025-12-15 CPU KV Offloading: Use more CUDA streams (#29013)
10	12	tests/v1/kv_offload/test_cpu_gpu.py
7	7	vllm/v1/kv_offload/cpu.py
175	86	vllm/v1/kv_offload/worker/cpu_gpu.py

[9ccbf6b69] RioS 2025-12-15 [responsesAPI]add extra body parameters (#30532)
9	0	vllm/entrypoints/openai/protocol.py

[ae2e503dd] Chendi.Xue 2025-12-14 [NIXL][BUG FIX] Fix a bug for PD with host_buffer after merging 29665 (#30420)
6	6	tests/v1/kv_connector/unit/test_nixl_connector.py
56	39	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[9e33a1a75] Tsukasa OI 2025-12-15 [Model][Quantization] Override HF defaults to GGUF ones (incl. Qwen3 MoE) (#30118)
22	0	vllm/transformers_utils/config.py

[add4b0ca4] Vensen 2025-12-14 [Bugfix][benchmarks] Fix input token calculation for rerank benchmark metrics (#30596)
3	1	vllm/benchmarks/serve.py
1	0	vllm/entrypoints/pooling/score/protocol.py
3	1	vllm/entrypoints/pooling/score/serving.py

[ae88aada3] ZiTian Zhao 2025-12-14 [Feature]Add EVS (Efficient Video Sampling) Support for Qwen3-VL (#29752)
424	12	vllm/model_executor/models/qwen3_vl.py

[5ccf0efa8] yifant-code 2025-12-14 [Bugfix]  Improve error messages in ModelConfig validation (#30213)
20	4	vllm/config/model.py

[994acec0c] ElizaWszola 2025-12-14 [Bugfix] Fix fusion for VL models (#30244)
78	0	tests/compile/distributed/test_fusions_e2e.py
52	48	vllm/compilation/fusion.py
13	7	vllm/compilation/matcher_utils.py
0	17	vllm/utils/deep_gemm.py

[48b8456ff] zifeitong 2025-12-14 [Bugfix] Revert Qwen2-VL part of change in #28271 (#30542)
12	1	vllm/model_executor/models/qwen2_vl.py

[5b64ac21f] Drew Botwinick 2025-12-14 [Bugfix] Update get_processor_data to use get_all method (#30583)
1	1	vllm/multimodal/parse.py

[a8ec48659] Bin Bao 2025-12-14 [Misc] Add a script to benchmark compilation time (#29919)
326	0	vllm/benchmarks/startup.py
2	0	vllm/entrypoints/cli/__init__.py
21	0	vllm/entrypoints/cli/benchmark/startup.py

[6ecc1e411] tjp_zju 2025-12-14 [Bugfix] fix _get_quant_method of FusedMoE for deepseekV3.2 on non-NV… (#30057)
5	0	vllm/model_executor/layers/quantization/moe_wna16.py

[0bb0bae43] Shengliang Xu 2025-12-14 Nvidia ModelOpt workaround for issue 28072 (#30164)
18	1	vllm/model_executor/layers/quantization/modelopt.py

[060893654] Johannes F 2025-12-14 fix: Update json features supported by xGrammar (#30390)
5	0	tests/v1/entrypoints/conftest.py
2	2	tests/v1/structured_output/test_utils.py
1	7	vllm/v1/structured_output/backend_xgrammar.py

[e9add129a] Matthias Gehre 2025-12-14 [Bugfix] awq_gemm: fix argument order swap (#30364)
3	3	tests/kernels/quantization/test_awq.py
4	4	vllm/_custom_ops.py

[3224ea991] Ilya Markov 2025-12-14 [torch.compile] Add encoder tag for compilation (#30489)
10	1	vllm/compilation/backends.py
1	6	vllm/compilation/piecewise_backend.py
3	3	vllm/model_executor/models/qwen2_5_vl.py

[3a20450d3] Lasha Koroshinadze 2025-12-14 Add AudioFlamingo3 model support (#30539)
1	0	docs/models/supported_models.md
73	44	examples/offline_inference/audio_language.py
1	0	tests/models/fixtures/audioflamingo3/expected_results_batched.json
1	0	tests/models/fixtures/audioflamingo3/expected_results_single.json
142	0	tests/models/multimodal/generation/test_audioflamingo3.py
125	0	tests/models/multimodal/processing/test_audioflamingo3.py
3	0	tests/models/registry.py
639	0	vllm/model_executor/models/audioflamingo3.py
4	0	vllm/model_executor/models/registry.py

[1a55cfafc] Didier Durand 2025-12-14 [Doc]: fixing typos in various files (#30540)
1	1	docs/configuration/optimization.md
1	1	docs/deployment/integrations/production-stack.md
2	2	docs/design/cuda_graphs.md
1	1	docs/design/optimization_levels.md
3	3	docs/design/paged_attention.md
1	1	docs/models/supported_models.md
1	1	docs/serving/parallelism_scaling.md
2	2	docs/usage/security.md
1	1	examples/online_serving/structured_outputs/structured_outputs.py
1	1	vllm/entrypoints/openai/serving_responses.py
2	2	vllm/model_executor/layers/fused_moe/shared_fused_moe.py
1	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py

[add1b9d3d] drslark 2025-12-14 [main][BugFix] Fixed an accuracy bug of Qwen3-next-MTP when batched inferring (#30632)
1	1	vllm/v1/attention/backends/gdn_attn.py

[dcb31196d] Cyrus Leung 2025-12-14 [Chore] Remove redundant `RequestPrompt` (#30612)
1	2	tests/entrypoints/openai/test_chat_error.py
13	13	tests/entrypoints/openai/test_serving_chat.py
3	3	tests/entrypoints/openai/test_serving_responses.py
36	19	vllm/entrypoints/openai/serving_chat.py
72	125	vllm/entrypoints/openai/serving_engine.py
10	11	vllm/entrypoints/openai/serving_responses.py
1	5	vllm/entrypoints/pooling/classify/serving.py
20	39	vllm/entrypoints/pooling/embed/serving.py
2	5	vllm/entrypoints/pooling/pooling/serving.py
18	20	vllm/entrypoints/renderer.py
3	3	vllm/entrypoints/serve/disagg/serving.py
7	6	vllm/entrypoints/serve/tokenize/serving.py

[f569c654e] Laith Sakka 2025-12-14 enable unbacked with aot_compile (#30462)
8	2	tests/compile/test_dynamic_shapes_compilation.py
0	8	vllm/compilation/decorators.py

[97f2f160f] Micah Williamson 2025-12-14 [ROCm][CI] Add "Qwen3-Next-80B-A3B-Instruct MTP Async EPLB Accuracy Test" Back Into AMD CI (#30590)
74	0	.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh
0	1	.buildkite/test-amd.yaml
0	3	vllm/distributed/eplb/rebalance_execute.py

[29f7d9771] Kayvan Mivehnejad 2025-12-13 Improve parse_raw_prompt test cases for invalid input .v2 (#30512)
7	0	tests/test_inputs.py
17	8	vllm/inputs/parse.py

[dc7fb5beb] Qier Li 2025-12-13 [Bug][KVConnector][Metrics] Remove a vacuous assertion breaking external-launcher (#30577)
0	3	vllm/distributed/kv_transfer/kv_connector/v1/metrics.py

[24429d592] Qidong Su 2025-12-13 [Doc] Add instructions for building docker image on GB300 with CUDA13 (#30414)
20	1	docs/deployment/docker.md

[6e78ed6ba] Wentao Ye 2025-12-13 [Logs] Optimize startup logs 4 (#29903)
5	6	vllm/model_executor/layers/fused_moe/fused_moe.py
3	1	vllm/model_executor/layers/fused_moe/layer.py
3	2	vllm/platforms/cuda.py
8	5	vllm/profiler/wrapper.py
1	1	vllm/v1/executor/multiproc_executor.py

[7c16f3fbc] Isotr0py 2025-12-14 [Doc] Add documents for multi-node distributed serving with MP backend (#30509)
23	1	docs/serving/parallelism_scaling.md
1	3	vllm/v1/executor/multiproc_executor.py

[ddbfbe527] lif 2025-12-14 [Docs] Clarify Expert Parallel behavior for attention and MoE layers (#30615)
2	2	docs/serving/data_parallel_deployment.md
21	1	docs/serving/expert_parallel_deployment.md

[763963aa7] Laith Sakka 2025-12-13 set assume_32bit_indexing and pass unbacked hints (#30459)
22	3	vllm/compilation/decorators.py

[39cefbdf1] Cyrus Leung 2025-12-13 [Refactor] `TokenizerRegistry` only uses lazy imports (#30609)
2	2	tests/test_inputs.py
24	23	tests/tokenizers_/test_basic.py
21	2	tests/tokenizers_/test_registry.py
3	2	vllm/entrypoints/chat_utils.py
0	6	vllm/tokenizers/__init__.py
33	14	vllm/tokenizers/deepseekv32.py
7	12	vllm/tokenizers/hf.py
2	5	vllm/tokenizers/mistral.py
1	1	vllm/tokenizers/protocol.py
100	100	vllm/tokenizers/registry.py
3	3	vllm/transformers_utils/tokenizer.py
2	2	vllm/v1/engine/async_llm.py
2	2	vllm/v1/engine/llm_engine.py
2	2	vllm/v1/structured_output/__init__.py

[ace34e378] Chen Zhang 2025-12-13 [Bugfix] Qwen3-next with  --hf-overrides \{\"num_hidden_layers\":8\}  (#30433)
7	0	vllm/model_executor/models/qwen3_next.py

[e5db3e277] Isotr0py 2025-12-13 [CI/Build] Fix broken mm processor test Mistral-3-large (#30597)
8	0	tests/models/multimodal/processing/test_tensor_schema.py

[64251f48d] Cyrus Leung 2025-12-13 [Chore] Adjust tokenizer import to avoid circular imports (#30601)
1	1	benchmarks/backend_request_func.py
1	1	tests/entrypoints/openai/test_serving_engine.py
2	1	tests/entrypoints/test_chat_utils.py
1	1	tests/models/language/generation/test_mistral.py
1	1	tests/models/multimodal/generation/test_voxtral.py
2	5	tests/models/multimodal/processing/test_common.py
1	1	tests/reasoning/test_mistral_reasoning_parser.py
1	1	tests/reasoning/utils.py
1	1	tests/tokenizers_/test_detokenize.py
2	5	tests/tool_use/test_mistral_tool_parser.py
2	1	vllm/entrypoints/chat_utils.py
2	1	vllm/entrypoints/llm.py
3	1	vllm/entrypoints/openai/serving_engine.py
2	1	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
2	1	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
4	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
2	1	vllm/entrypoints/pooling/score/serving.py
1	1	vllm/entrypoints/utils.py
2	1	vllm/model_executor/models/pixtral.py
2	1	vllm/model_executor/models/voxtral.py
1	1	vllm/reasoning/mistral_reasoning_parser.py
2	1	vllm/v1/engine/input_processor.py
2	1	vllm/v1/structured_output/backend_xgrammar.py

[1cec5b7ea] Nick Hill 2025-12-13 [Scheduer] Simplify stop checking for pooling models (#30591)
5	6	vllm/v1/core/sched/scheduler.py
2	10	vllm/v1/core/sched/utils.py

[b09806e28] Cyrus Leung 2025-12-13 [Bugfix] Dictionary MM embeddings for online chat (#30507)
105	5	tests/entrypoints/test_chat_utils.py
68	29	vllm/entrypoints/chat_utils.py
20	10	vllm/v1/engine/input_processor.py

[fdc135d76] Tsukasa OI 2025-12-13 [Misc][Quantization] Clarify the intent of GGUF `FusedMoE` weight materialization (#30310)
6	2	vllm/model_executor/layers/fused_moe/layer.py

[4fa7ce46f] Roberto L. Castro 2025-12-13 [Feature] Add SM103 (Blackwell Ultra) Support to vLLM (#30484)
1	1	tests/compile/distributed/test_fusions_e2e.py
2	2	tests/kernels/attention/test_cutlass_mla_decode.py
2	2	tests/kernels/attention/test_flashinfer_trtllm_attention.py
2	2	tests/kernels/moe/test_ocp_mx_moe.py
2	2	tests/quantization/test_blackwell_moe.py
1	1	vllm/model_executor/layers/batch_invariant.py
4	1	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
3	3	vllm/model_executor/layers/quantization/fp8.py
4	4	vllm/model_executor/layers/quantization/mxfp4.py
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	1	vllm/model_executor/models/config.py
1	1	vllm/platforms/cuda.py
15	0	vllm/platforms/interface.py
2	2	vllm/utils/deep_gemm.py
3	1	vllm/utils/flashinfer.py
1	1	vllm/v1/attention/backends/flashinfer.py
3	3	vllm/v1/attention/backends/mla/common.py
2	2	vllm/v1/attention/backends/mla/flashmla_sparse.py

[57e9bf186] Nicolò Lucchesi 2025-12-13 [CI] Whisper logprobs tests (#30504)
7	1	tests/conftest.py
123	111	tests/models/multimodal/generation/test_whisper.py
4	1	tests/models/registry.py

[2f32a68d7] Michael Goin 2025-12-12 [CI] Update several models in registry that are available online now (#30514)
2	0	.buildkite/test-pipeline.yaml
4	6	tests/models/registry.py

[f5dfbbd8e] Matthew Bonanni 2025-12-12 [Docs] Remove references to `VLLM_ATTENTION_BACKEND` (#30564)
16	6	docs/getting_started/quickstart.md

[fc0119425] Michael Goin 2025-12-12 Add IBM and Red Hat to compute resources sponsors (#30581)
2	0	README.md
2	0	docs/community/sponsors.md

[86a326152] Matthew Bonanni 2025-12-12 [Bugfix] Pass FA version in `MultiHeadAttention` (#30575)
10	0	vllm/attention/layer.py

[08f8a5627] rasmith 2025-12-12 [CI/Build][Kernel][BugFix][AMD] Fix per_token_group_quant_fp8 to use correct fp8 min/max values and update atol/rtol in test_quantfp8_group_functionality  (#30292)
2	2	tests/kernels/quantization/test_fp8_quant_group.py
5	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[b4039c08b] Kevin H. Luu 2025-12-12 [ci] Mark PrimeRL integration test as soft fail (#30578)
2	1	.buildkite/test-pipeline.yaml

[1e6b11530] Wentao Ye 2025-12-12 [Refactor] Reduce duplicate code in `per_token_group_quant` cuda kernels (#30496)
83	98	csrc/quantization/w8a8/fp8/per_token_group_quant.cu

[13618626d] danielafrimi 2025-12-12 [MoE-FP8-modelopt] Add FlashInfer alignment padding for intermediate dimensions (#29748)
48	0	vllm/model_executor/layers/quantization/modelopt.py

[6ec0d8dbe] danielafrimi 2025-12-12 [Fix]Load kv-cache dtype from hf_quant_config.json automatically (#29980)
23	2	vllm/utils/torch_utils.py

[9693dd0fe] Li, Jiang 2025-12-13 [CI/Build] Add x86 CPU wheel release pipeline (#28848)
14	0	.buildkite/release-pipeline.yaml

[1f19d8f89] Xin Yang 2025-12-12 [Perf] Set split_k to 1 for triton_kernels (#30528)
12	6	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py

[cd7740ac5] shivampr 2025-12-12 [ROCm] Enable Triton ScaledMM fallback + kernel selection fix (#26668)
1	1	.buildkite/test-pipeline.yaml
91	0	tests/kernels/quantization/test_scaled_mm_kernel_selection.py
4	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
12	28	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
15	7	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
6	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py
12	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
46	17	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
6	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py

[02a588039] Wentao Ye 2025-12-12 [CI] Fix mypy for vllm/v1/executor (#30517)
1	1	tools/pre_commit/mypy.py
1	1	vllm/v1/executor/abstract.py
7	3	vllm/v1/executor/multiproc_executor.py
3	3	vllm/v1/executor/ray_executor.py
8	5	vllm/v1/executor/uniproc_executor.py

[d2c919dcc] realliujiaxu 2025-12-13 [bugfix] fix bug when top_logprobs=0 with spec decoding (#30059)
3	1	tests/v1/sample/test_logprobs.py
1	1	tests/v1/sample/test_rejection_sampler.py
1	1	vllm/v1/sample/rejection_sampler.py

[f3237f3f6] Benjamin Bartels 2025-12-12 [Frontend] Fixes anthropic streaming message_start usage nesting (#30266)
6	3	tests/entrypoints/openai/test_messages.py
6	6	vllm/entrypoints/anthropic/serving_messages.py

[9c0ee995a] jvlunteren 2025-12-12 [Kernel] Support CUDA Graphs in 3D Triton Attention Kernel (#28306)
27	0	tests/kernels/attention/test_triton_unified_attention.py
30	39	vllm/attention/ops/triton_unified_attention.py
83	1	vllm/v1/attention/backends/triton_attn.py

[09ad3b76b] Michael Goin 2025-12-12 [Bug] Fix attention_backend arg string parsing (#30534)
7	1	vllm/engine/arg_utils.py

[dc13c99ee] Christina Norman 2025-12-12 fix(gguf): Disable bfloat16 for GGUF on blackwell device (#30408)
6	0	vllm/model_executor/layers/quantization/gguf.py

[3e34adcdf] Vladislav Nosivskoy 2025-12-12 [DeepSeek V3.2] Proper drop_thinking logic (#30490)
4	2	vllm/tokenizers/deepseekv32.py

[3e41992fe] Lucas Wilkinson 2025-12-12 [Attention] Use sparse prefill kernel for fp8 kv-cache in DeepSeek-v3.2 (#27532)
11	1	csrc/cache.h
130	1	csrc/cache_kernels.cu
7	0	csrc/torch_bindings.cpp
21	0	tests/conftest.py
1	1	tests/kernels/moe/test_batched_deepgemm.py
1	0	tests/kernels/moe/test_batched_moe.py
1	1	tests/kernels/moe/test_block_fp8.py
25	2	tests/kernels/moe/test_cutlass_moe.py
6	0	tests/kernels/moe/test_deepep_deepgemm_moe.py
6	0	tests/kernels/moe/test_deepep_moe.py
1	1	tests/kernels/moe/test_deepgemm.py
1	0	tests/kernels/moe/test_flashinfer.py
8	1	tests/kernels/moe/test_flashinfer_moe.py
1	1	tests/kernels/moe/test_gpt_oss_triton_kernels.py
6	0	tests/kernels/moe/test_modular_kernel_combinations.py
1	0	tests/kernels/moe/test_modular_oai_triton_moe.py
1	0	tests/kernels/moe/test_moe.py
1	1	tests/kernels/moe/test_nvfp4_moe.py
5	0	tests/kernels/moe/test_pplx_moe.py
215	36	tests/v1/attention/test_sparse_mla_backends.py
23	0	vllm/_custom_ops.py
4	0	vllm/envs.py
16	57	vllm/model_executor/layers/fused_moe/modular_kernel.py
18	19	vllm/model_executor/models/deepseek_v2.py
567	98	vllm/v1/attention/backends/mla/flashmla_sparse.py
12	36	vllm/v1/attention/backends/mla/indexer.py
27	0	vllm/v1/attention/backends/utils.py
6	0	vllm/v1/worker/gpu_model_runner.py
5	0	vllm/v1/worker/gpu_worker.py
245	0	vllm/v1/worker/workspace.py

[91401c7a2] 吴坎 2025-12-12 [Bugfix] Fix CMakeLists Environment Variable (#21804)
3	3	CMakeLists.txt

[f90319d5d] Jaehwang Jung 2025-12-12 [Bugfix] Schedule failure due to wrong get_image_size_with_most_features (#29692)
42	0	tests/models/multimodal/processing/test_gemma3.py
35	0	tests/models/multimodal/processing/test_qwen2_vl.py
3	2	vllm/model_executor/models/gemma3_mm.py
37	7	vllm/model_executor/models/qwen2_vl.py

[302b2c1eb] rasmith 2025-12-12 [CI/Build][AMD] Fix ref_dynamic_per_token_quant reference implementation on ROCm. (#30291)
4	9	tests/kernels/quant_utils.py

[8f8fda261] Ben Browning 2025-12-11 [Bugfix] Multiple fixes for gpt-oss Chat Completion prompting (#28729)
666	85	tests/entrypoints/openai/parser/test_harmony_utils.py
645	1	tests/entrypoints/openai/test_serving_chat.py
190	0	tests/entrypoints/openai/utils.py
204	21	vllm/entrypoints/openai/parser/harmony_utils.py
10	3	vllm/entrypoints/openai/serving_chat.py
6	1	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py

[fe1787107] Zhengxu Chen 2025-12-11 [compile] Parse compile range cache keys as Range during cache loading. (#30516)
19	1	vllm/compilation/backends.py

[783644e4a] Andreas Karatzas 2025-12-11 [ROCm][CI] Skip multi-GPU speculative decoding tests when insufficient GPUs available (#30527)
13	0	tests/v1/e2e/test_spec_decode.py

[197473c4e] Ryan Rock 2025-12-11 [CI/Build] Use spawn subprocess for ROCm (#30272)
6	0	examples/offline_inference/data_parallel.py

[947dfda9c] Nick Hill 2025-12-11 [LMCache] Relax lmcache version requirement (#30425)
1	1	requirements/kv_connectors.txt
8	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py

[9f2fc16a6] Michael Goin 2025-12-11 [Bugfix][Model] Fix Afmoe rope_parameters issue (#30505)
1	4	tests/models/registry.py
1	1	vllm/model_executor/models/afmoe.py

[6a6fc41c7] Bhanu Prakash Voutharoja 2025-12-12 gptq marlin quantization support for fused moe with lora (#30254)
1	1	csrc/moe/marlin_moe_wna16/ops.cu
36	0	vllm/model_executor/layers/fused_moe/config.py
109	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[f355ad541] Fadi Arafeh 2025-12-12 [CPU][FIX] Fix build failures on Arm CPUs with torch nightly (#30481)
14	9	cmake/utils.cmake
10	4	vllm/platforms/cpu.py

[042da7324] Lucas Wilkinson 2025-12-11 [Core] Refactor `_build_attention_metadata` (#29628)
124	126	vllm/v1/worker/gpu_model_runner.py

[b5945d49c] Andreas Karatzas 2025-12-11 [ROCm][CI] Use mi325_4 agent pool for V1 e2e tests (#30526)
3	3	.buildkite/test-amd.yaml

[ba8092668] rasmith 2025-12-11 [CI/Build][AMD] Skip test_cutlass_w4a8_moe tests on ROCm sine they require cutlass_pack_scale_fp8 (#30508)
3	1	tests/kernels/quantization/test_cutlass_w4a8_moe.py

[0ab23c2b2] jiahanc 2025-12-11 [fix] fix SM check for Flashinfer TRTLLM MOE (#30314)
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[48661d275] rasmith 2025-12-11 [CI/Build][AMD] Skip tests in test_fusions_e2e and test_dbo_dp_ep_gsm8k that require non-existing imports for ROCm  (#30417)
25	1	tests/compile/distributed/test_fusions_e2e.py
2	0	tests/v1/distributed/test_dbo.py

[d527cf0b3] Ev Lacey 2025-12-11 [FIX]Patch run-cluster.sh (fix for #28328) (#30002)
34	26	examples/online_serving/run_cluster.sh

[2cc5affc3] Concurrensee 2025-12-11 [ROCM][CI] Fix AMD Examples Test Group (#30276)
1	2	.buildkite/test-amd.yaml
8	0	examples/offline_inference/basic/embed.py
8	0	examples/offline_inference/basic/score.py

[a00d88973] Andrew Briand 2025-12-11 [EPLB] Support EPLB w/ NVFP4 (#29804)
276	0	tests/distributed/test_eplb_fused_moe_layer_dep_nvfp4.py
21	5	vllm/model_executor/layers/quantization/modelopt.py
79	0	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[61249b177] Wentao Ye 2025-12-11 [Refactor] Remove useless syncwarp (#30510)
0	5	csrc/moe/grouped_topk_kernels.cu

[c817b1415] Wentao Ye 2025-12-11 [Perf] Optimize deepgemm experts initialization, 3.9% TTFT improvement (#30494)
17	6	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py

[3efdc3fea] ioana ghiban 2025-12-11 [Docs][CPU backend] Add pre-built Arm CPU Docker images (#30491)
17	1	docs/getting_started/installation/cpu.arm.inc.md

[0efd9f867] Nicolò Lucchesi 2025-12-11 [Core] Whisper Enable Encoder Batching (#29421)
5	0	vllm/config/model.py
10	20	vllm/config/vllm.py
13	4	vllm/model_executor/models/whisper.py
53	0	vllm/v1/core/encoder_cache_manager.py
6	1	vllm/v1/core/sched/scheduler.py

[90d6cf921] Xingyu Liu 2025-12-11 [BugFix][MM]support VLLM_RANDOMIZE_DP_DUMMY_INPUTS (#30472)
23	6	vllm/v1/worker/gpu_model_runner.py

[cf3eacfe5] Harry Mellor 2025-12-11 Standardise `get_rope` to use `rope_parameters["partial_rotary_factor"]`, not `rotary_dim` (#30389)
0	1	benchmarks/kernels/benchmark_mrope.py
2	2	benchmarks/kernels/benchmark_rope.py
1	4	tests/compile/test_functionalization.py
0	2	tests/kernels/core/test_mrope.py
8	4	tests/kernels/core/test_pos_encoding.py
16	2	vllm/config/utils.py
160	166	vllm/model_executor/layers/rotary_embedding/__init__.py
0	1	vllm/model_executor/models/afmoe.py
0	1	vllm/model_executor/models/apertus.py
0	1	vllm/model_executor/models/arctic.py
0	1	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bailing_moe.py
2	5	vllm/model_executor/models/bamba.py
0	1	vllm/model_executor/models/chameleon.py
5	2	vllm/model_executor/models/chatglm.py
0	1	vllm/model_executor/models/commandr.py
7	5	vllm/model_executor/models/config.py
0	1	vllm/model_executor/models/dbrx.py
0	4	vllm/model_executor/models/deepseek_v2.py
0	1	vllm/model_executor/models/dots1.py
0	1	vllm/model_executor/models/ernie45_moe.py
0	1	vllm/model_executor/models/exaone.py
0	1	vllm/model_executor/models/exaone4.py
0	1	vllm/model_executor/models/falcon.py
2	5	vllm/model_executor/models/falcon_h1.py
0	1	vllm/model_executor/models/gemma.py
0	1	vllm/model_executor/models/gemma2.py
0	1	vllm/model_executor/models/gemma3.py
0	1	vllm/model_executor/models/gemma3n.py
0	2	vllm/model_executor/models/glm4.py
1	1	vllm/model_executor/models/glm4_1v.py
0	1	vllm/model_executor/models/glm4_moe.py
3	2	vllm/model_executor/models/gpt_j.py
0	1	vllm/model_executor/models/gpt_neox.py
0	1	vllm/model_executor/models/gpt_oss.py
0	1	vllm/model_executor/models/granite.py
0	1	vllm/model_executor/models/granitemoe.py
0	1	vllm/model_executor/models/granitemoehybrid.py
0	1	vllm/model_executor/models/grok1.py
0	2	vllm/model_executor/models/hunyuan_v1.py
0	1	vllm/model_executor/models/internlm2.py
0	1	vllm/model_executor/models/lfm2.py
0	1	vllm/model_executor/models/lfm2_moe.py
0	1	vllm/model_executor/models/llama.py
0	1	vllm/model_executor/models/llama4.py
0	1	vllm/model_executor/models/minicpm.py
0	1	vllm/model_executor/models/minicpm3.py
5	1	vllm/model_executor/models/minimax_m2.py
2	5	vllm/model_executor/models/minimax_text_01.py
0	1	vllm/model_executor/models/mixtral.py
1	1	vllm/model_executor/models/mllama4.py
0	1	vllm/model_executor/models/modernbert.py
0	1	vllm/model_executor/models/molmo.py
0	1	vllm/model_executor/models/nemotron.py
0	1	vllm/model_executor/models/nemotron_nas.py
0	1	vllm/model_executor/models/olmo.py
0	1	vllm/model_executor/models/olmo2.py
0	1	vllm/model_executor/models/olmoe.py
0	2	vllm/model_executor/models/openpangu.py
0	1	vllm/model_executor/models/orion.py
0	1	vllm/model_executor/models/ouro.py
0	1	vllm/model_executor/models/persimmon.py
4	8	vllm/model_executor/models/phi.py
0	1	vllm/model_executor/models/phimoe.py
0	1	vllm/model_executor/models/plamo2.py
0	1	vllm/model_executor/models/plamo3.py
0	1	vllm/model_executor/models/qwen.py
0	1	vllm/model_executor/models/qwen2.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
0	1	vllm/model_executor/models/qwen2_moe.py
1	1	vllm/model_executor/models/qwen2_vl.py
0	1	vllm/model_executor/models/qwen3.py
0	1	vllm/model_executor/models/qwen3_moe.py
0	1	vllm/model_executor/models/qwen3_next.py
1	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py
1	1	vllm/model_executor/models/qwen3_vl.py
0	1	vllm/model_executor/models/seed_oss.py
0	1	vllm/model_executor/models/solar.py
0	1	vllm/model_executor/models/stablelm.py
0	1	vllm/model_executor/models/starcoder2.py
0	1	vllm/model_executor/models/step3_text.py
0	1	vllm/model_executor/models/zamba2.py
13	4	vllm/transformers_utils/config.py

[92fea56fd] Zhengxu Chen 2025-12-11 [compile] Stop one-off setting enable_aot_compile and use context manager instead. (#30503)
10	8	vllm/compilation/wrapper.py

[e458270a9] Ye (Charlotte) Qi 2025-12-11 [Misc] Add mcp to requirements (#30474)
2	1	requirements/common.txt

[72aaac5b6] Andreas Karatzas 2025-12-11 [ROCm][Bugfix] Add MLACommonMetadata to allowed attention types for speculative decoding (#30430)
6	0	vllm/v1/spec_decode/eagle.py

[0e71eaa64] 汪志鹏 2025-12-12 [Feature] AWQ marlin quantization support for fused moe with lora (#30442)
36	0	vllm/model_executor/layers/fused_moe/config.py
94	1	vllm/model_executor/layers/quantization/awq_marlin.py

[8781cd6b8] Harry Mellor 2025-12-11 Add Eagle and Eagle3 support to Transformers modeling backend (#30340)
34	2	tests/v1/e2e/test_spec_decode.py
60	6	vllm/model_executor/models/transformers/base.py

[aa3c250c4] Julien Denize 2025-12-11 [IMPROVEMENT] Change MistralReasoningParser behavior (#30391)
84	61	tests/reasoning/test_mistral_reasoning_parser.py
102	3	vllm/reasoning/mistral_reasoning_parser.py

[305b168a9] Shengqi Chen 2025-12-12 [CI] refine more logic when generating and using nightly wheels & indices, add cuda130 build for aarch64, specify correct manylinux version (#30341)
18	3	.buildkite/release-pipeline.yaml
11	0	.buildkite/scripts/generate-nightly-index.py
8	4	.buildkite/scripts/upload-wheels.sh
36	3	tests/standalone_tests/python_only_compile.sh

[93db3256a] Harry Mellor 2025-12-11 Give pooling examples better names (#30488)
1	1	docs/models/supported_models.md
1	1	docs/serving/openai_compatible_server.md
0	0	examples/pooling/score/{qwen3_reranker.py => offline_reranker.py}
0	0	examples/pooling/score/{jinaai_rerank_client.py => openai_reranker.py}
1	1	vllm/model_executor/models/config.py

[17cb54024] ioana ghiban 2025-12-11 [Docs][CPU Backend] Add nightly and per revision pre-built Arm CPU wheels (#30402)
21	2	docs/getting_started/installation/cpu.arm.inc.md

[97a042f3b] Harry Mellor 2025-12-11 Make the `httpx` logger less annoying when Transformers v5 is installed (#30480)
5	0	vllm/logger.py

[3a3b06ee7] Cyrus Leung 2025-12-11 [Misc] Improve error message for `is_multimodal` (#30483)
17	3	vllm/model_executor/models/interfaces.py
2	3	vllm/model_executor/models/phi3v.py
2	1	vllm/model_executor/models/qwen3_vl.py

[f4417f844] Martin Hickey 2025-12-11 [KVConnector] Add KV events to KV Connectors (#28309)
756	0	tests/v1/kv_connector/unit/test_lmcache_connector.py
129	1	vllm/distributed/kv_events.py
15	0	vllm/distributed/kv_transfer/kv_connector/utils.py
9	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
114	3	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
6	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
4	0	vllm/v1/outputs.py
3	10	vllm/v1/worker/kv_connector_model_runner_mixin.py

[a11f4a81e] Qiu 2025-12-11 [Misc][PCP&DCP] relocate PCP feature check (#30050)
6	0	vllm/attention/backends/abstract.py
0	5	vllm/config/parallel.py
0	5	vllm/config/vllm.py
0	10	vllm/engine/arg_utils.py
42	0	vllm/v1/worker/cp_utils.py
4	14	vllm/v1/worker/gpu_model_runner.py

[853611bb1] Kenichi Maehashi 2025-12-11 Fix typo of endpoint name in CLI args docs (#30473)
1	1	vllm/entrypoints/openai/cli_args.py

[d917747c9] Cyrus Leung 2025-12-11 [Bugfix] Fix `task` still being passed in tests/benchmarks (#30476)
0	1	benchmarks/benchmark_ngram_proposer.py
0	2	tests/models/language/pooling/test_mm_classifier_conversion.py

[a5f9fb596] wang.yuqi 2025-12-11 [Deprecation] Deprecation `--convert reward`, use `--convert embed` instead. (#30463)
4	1	docs/models/pooling_models.md
7	0	vllm/config/model.py
4	2	vllm/config/pooler.py

[4515eb1a0] jeremyteboul 2025-12-11 [Fix] Update lazing loading of video loader backend (#30444)
123	1	tests/multimodal/test_video.py
8	1	vllm/multimodal/video.py

[13d63b65e] Cyrus Leung 2025-12-11 [Deprecation] Remove missed fallback for `embed_input_ids` (#30469)
1	7	vllm/model_executor/models/interfaces_base.py

[b4e8b9127] wz1qqx 2025-12-11 [Fix]fix import error from lmcache (#30376)
[6299628d3] Rei. 2025-12-11 [bugfix] fix MiniMaxM2ReasoningParser streaming output not separating reasoning_content. (#29882)
195	0	tests/reasoning/test_minimax_m2_append_reasoning_parser.py
230	0	tests/reasoning/test_minimax_m2_reasoning_parser.py
43	0	vllm/reasoning/minimax_m2_reasoning_parser.py

[fba890693] Ming Yang 2025-12-11 [perf] Use direct copy (broadcast) instead of cat for k_nope/k_pe in MLA prefill (#29710)
150	0	benchmarks/kernels/benchmark_mla_k_concat.py
30	3	vllm/v1/attention/backends/mla/common.py

[d02d1043d] Ning Xie 2025-12-11 fix: enhance human_readable_int function (#30337)
18	4	tests/engine/test_arg_utils.py
3	0	vllm/engine/arg_utils.py

[979f50efd] Cyrus Leung 2025-12-11 [Deprecation] Remove fallbacks for `embed_input_ids` and `embed_multimodal` (#30458)
2	13	vllm/model_executor/models/interfaces.py
0	9	vllm/model_executor/models/interfaces_base.py
5	34	vllm/model_executor/models/mistral_large_3_eagle.py
1	6	vllm/model_executor/models/phi3v.py
1	6	vllm/model_executor/models/qwen3_vl.py

[36c9ce255] gh-wf 2025-12-11 Ensure minimum frames for GLM 4.6V compatibility (#30285)
1	0	vllm/model_executor/models/glm4_1v.py

[1a516557e] xyDong0223 2025-12-11 [Doc] Add Baidu Kunlun XPU support (#30455)
1	0	docs/getting_started/installation/README.md

[d6464f267] Wentao Ye 2025-12-10 [Chore] Fix torch precision warning (#30428)
2	2	tests/v1/e2e/test_async_scheduling.py
6	4	vllm/envs.py
1	1	vllm/v1/worker/gpu_worker.py

[7e24e5d4d] Cyrus Leung 2025-12-11 [Deprecation] Remove deprecated task, seed and MM settings (#30397)
1	1	benchmarks/benchmark_ngram_proposer.py
1	1	examples/offline_inference/audio_language.py
1	1	examples/offline_inference/encoder_decoder_multimodal.py
1	1	examples/offline_inference/qwen2_5_omni/only_thinker.py
1	1	examples/offline_inference/qwen3_omni/only_thinker.py
1	1	examples/offline_inference/vision_language.py
3	3	examples/offline_inference/vision_language_multi_image.py
1	1	examples/pooling/plugin/prithvi_geospatial_mae_client.py
3	3	examples/pooling/pooling/vision_language_pooling.py
1	1	tests/conftest.py
0	58	tests/test_config.py
2	2	tests/utils.py
0	131	vllm/config/model.py
8	65	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/llm.py
0	5	vllm/envs.py

[5a87d8b9b] Cyrus Leung 2025-12-11 [Deprecation] Remove deprecated plugin and compilation fields for v0.13 release (#30396)
2	2	docs/design/plugin_system.md
1	62	tests/compile/test_config.py
2	2	tests/kernels/moe/test_ocp_mx_moe.py
2	2	tests/quantization/test_quark.py
1	1	tests/test_config.py
0	32	vllm/attention/backends/registry.py
12	34	vllm/attention/selector.py
1	80	vllm/config/compilation.py
1	1	vllm/config/vllm.py
0	22	vllm/engine/arg_utils.py

[d1e1fb436] Divakar Verma 2025-12-10 [Bugfix] Fix grouped_topk pytorch impl when num_experts can't be grouped properly (#29439)
9	1	vllm/model_executor/layers/fused_moe/layer.py

[b51255f36] Andreas Karatzas 2025-12-10 [ROCm] Fix broken import in platform attention backend dispatching (#30432)
15	1	vllm/platforms/rocm.py

[b4054c8ab] Sage Moore 2025-12-10 Revert "[CI] Add Async Eplb nightly CI tests (#29385)" (#30431)
0	73	.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_async_eplb.sh
0	1	.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh
0	74	.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh
1	19	.buildkite/test-pipeline.yaml
3	0	vllm/distributed/eplb/rebalance_execute.py

[25221b44b] Xu Song 2025-12-11 Add more docs for regex (#30106)
1	1	docs/features/structured_outputs.md

[8580919ac] shivampr 2025-12-10 [Bugfix] fix confusing OOM errors during v1 init (#28051)
54	0	tests/v1/engine/test_init_error_messaging.py
7	3	vllm/v1/core/kv_cache_utils.py
77	62	vllm/v1/worker/gpu_model_runner.py

[166ac3c94] Christina Norman 2025-12-10 fix(shm): Add memory barriers for cross-process shared memory visibility (#30407)
44	0	vllm/distributed/device_communicators/shm_broadcast.py

[b9e0951f9] Seiji Eicher 2025-12-10 [docs] Improve wide-EP performance + benchmarking documentation (#27933)
13	1	docs/serving/data_parallel_deployment.md
27	1	docs/serving/expert_parallel_deployment.md
2	2	tools/ep_kernels/README.md

[fcb894222] Michael Goin 2025-12-10 [Docs] Update EPLB docs (#30426)
5	4	docs/serving/expert_parallel_deployment.md

[6ccb7baeb] Nick Hill 2025-12-10 [LMCache] Fix breakage due to new LMCache version (#30216)
1	1	requirements/kv_connectors.txt
2	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py

[eea41804a] Po-Han Huang (NVIDIA) 2025-12-11 [bug] Fix "Current vLLM config is not set." warnings when FlashInfer attention is used (#30241)
4	1	vllm/utils/flashinfer.py
2	0	vllm/v1/attention/backends/flashinfer.py

[9f042ba26] Jialin Ouyang 2025-12-10 [Perf] Enable environment cache in EngineCore to enable the feature for UniProcExecutor as well (#29289)
38	0	tests/test_envs.py
2	0	vllm/distributed/parallel_state.py
20	0	vllm/envs.py
3	4	vllm/v1/engine/core.py

[e72d65b95] Cyrus Leung 2025-12-11 {Deprecation] Remove tokenizer setter (#30400)
1	12	vllm/entrypoints/llm.py
0	4	vllm/v1/engine/async_llm.py
0	4	vllm/v1/engine/input_processor.py
0	4	vllm/v1/engine/llm_engine.py

[a9e4106f2] Will Eaton 2025-12-10 [P/D] KV Load Failure Recovery/Abort Configuration (#26813)
228	0	tests/entrypoints/openai/test_chat_error.py
216	0	tests/entrypoints/openai/test_completion_error.py
89	0	tests/entrypoints/openai/test_responses_error.py
163	0	tests/v1/kv_connector/unit/test_cache_pollution_prevention.py
147	0	tests/v1/kv_connector/unit/test_error_propagation.py
454	0	tests/v1/kv_connector/unit/test_invalid_blocks_correctness.py
5	0	vllm/config/kv_transfer.py
16	1	vllm/entrypoints/openai/serving_chat.py
14	1	vllm/entrypoints/openai/serving_completion.py
61	0	vllm/entrypoints/openai/serving_engine.py
40	13	vllm/entrypoints/openai/serving_responses.py
19	0	vllm/v1/core/block_pool.py
8	0	vllm/v1/core/kv_cache_manager.py
85	31	vllm/v1/core/sched/scheduler.py
6	3	vllm/v1/engine/__init__.py
2	0	vllm/v1/request.py

[e8e8cd73e] Anker 2025-12-10 [Bugfix] Fix HunyuanOCR cross-image contamination in batch processing (#30344)
9	2	vllm/model_executor/models/hunyuan_vision.py

[253305d5b] Cyrus Leung 2025-12-11 [Chore] Delay recent deprecations (#30398)
3	3	vllm/multimodal/inputs.py
2	2	vllm/multimodal/utils.py
7	7	vllm/transformers_utils/tokenizer.py
2	2	vllm/transformers_utils/tokenizer_base.py
1	1	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/llm_engine.py
1	1	vllm/v1/engine/processor.py

[794a7875e] Matthew Bonanni 2025-12-10 [Misc] Consistent case for `vllm bench serve` results (#30403)
1	1	benchmarks/benchmark_serving_structured_output.py
1	1	docs/benchmarking/cli.md
1	1	vllm/benchmarks/serve.py

[2dcbac907] Mark McLoughlin 2025-12-10 [Docs] Generate full list of metrics in user docs (#30388)
1	11	docs/design/metrics.md
149	0	docs/mkdocs/hooks/generate_metrics.py
12	4	docs/usage/metrics.md
1	0	mkdocs.yaml

[aacf0abf8] Lucas Wilkinson 2025-12-10 [BugFix] Fix `AttributeError: 'MergedColumnParallelLinear' object has no attribute 'weight_scale'` (#30399)
1	1	vllm/model_executor/warmup/deep_gemm_warmup.py

[c756fb678] Nicolò Lucchesi 2025-12-10 [Core] Whisper enable `FULL_DECODE_ONLY` CudaGraph  (#30072)
2	0	tests/models/multimodal/generation/test_whisper.py
19	11	vllm/config/vllm.py
10	1	vllm/v1/worker/gpu_model_runner.py

[d017bceb0] Roger Young 2025-12-10 [BugFix] Fix minimax m2 model rotary_dim (#30384)
1	1	vllm/model_executor/models/minimax_m2.py

[cebda2a4a] Aditya Tewari 2025-12-10 [CPU] Support for Whisper (#30062)
5	0	.buildkite/scripts/hardware_ci/run-cpu-test-arm.sh
0	1	csrc/cpu/cpu_attn.cpp
19	2	tests/models/multimodal/generation/test_whisper.py
19	19	vllm/v1/attention/backends/cpu_attn.py
6	2	vllm/v1/worker/utils.py

[53d2420b4] Daniele 2025-12-10 [Bugfix] tpu_model_runner: set vllm config context when calling reset_dynamo_cache() (#30331)
3	2	vllm/v1/worker/tpu_worker.py

[9db78f34d] Chauncey 2025-12-10 [Bugfix] Fix the issue where DeepSeek v3.2 cannot use structured_output (#30371)
22	1	vllm/v1/structured_output/backend_xgrammar.py

[434ac76a7] Fadi Arafeh 2025-12-10 [cpu][ci] Add CPU Attention Tests for Neon Backend (#30347)
63	10	tests/kernels/attention/test_cpu_attn.py

[ed7af3178] Andreas Karatzas 2025-12-09 [ROCm][CI] Attempt to fix the failures under a subgroup of the e2e the test group (#29358)
1	1	requirements/rocm-test.txt
8	2	tests/multimodal/test_utils.py
76	10	tests/v1/e2e/test_async_scheduling.py

[180345807] Radu Salavat 2025-12-09 [CMake][Build]: Remove unused ACL CMake env variables (#30339)
0	11	cmake/cpu_extension.cmake

[d007387aa] Mingliang Li 2025-12-10 [Bugfix] Cache added_vocab to avoid per-token overhead (#30351)
4	2	vllm/tokenizers/deepseekv32.py

[3bdd42663] Wilson Wu 2025-12-10 Fix typos in comments across multiple files (#30345)
1	1	csrc/cpu/cpu_attn_impl.hpp
1	1	csrc/quantization/machete/machete_mainloop.cuh
1	1	docs/features/nixl_connector_usage.md
1	1	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/utils.py

[06462392e] haoyangli-amd 2025-12-10 [bugfix][quantization] fix quark qwen3 kv_cache quantization (#30308)
14	0	vllm/model_executor/models/qwen3_moe.py

[7d80c73d4] Micah Williamson 2025-12-09 [CI] Reduce Flakiness For test_spec_decode.py::test_suffix_decoding_acceptance (#30367)
2	2	tests/v1/e2e/test_spec_decode.py

[b75f826fc] rasmith 2025-12-09 [CI/Build][AMD] Skip quantization kernels tests that require CUTLASS or e4m3fn when not supported by platform (#30020)
14	3	tests/kernels/quantization/test_block_fp8.py
3	0	tests/kernels/quantization/test_cutlass_scaled_mm.py
3	0	tests/kernels/quantization/test_cutlass_w4a8.py

[c3487aca3] Andrew Xia 2025-12-09 [responsesAPI][6] Fix multi turn MCP tokenization (#30230)
45	7	tests/entrypoints/test_responses_utils.py
2	0	vllm/entrypoints/constants.py
5	1	vllm/entrypoints/context.py
1	0	vllm/entrypoints/openai/serving_engine.py
57	5	vllm/entrypoints/responses_utils.py

[abe93bce5] Lucas Wilkinson 2025-12-09 [Attention] Make seq_lens_cpu optional in CommonAttentionMetadata to enable true async spec-decode (#29624)
2	2	tests/v1/attention/utils.py
131	0	tests/v1/e2e/test_async_spec_decode.py
2	2	tests/v1/spec_decode/test_tree_attention.py
1	1	vllm/attention/layers/cross_attention.py
1	1	vllm/v1/attention/backends/gdn_attn.py
49	17	vllm/v1/attention/backends/utils.py
10	10	vllm/v1/spec_decode/eagle.py
2	2	vllm/v1/worker/gpu/attn_utils.py
2	2	vllm/v1/worker/gpu_model_runner.py

[2e7035dd8] ElizaWszola 2025-12-10 [Bugfix] Fix fp8 DeepGemm compilation issues (#30336)
2	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[4c2e10ea1] PatrykSaffer 2025-12-10 [Bugfix] Fix cuda graph sizes when running with speculative decoding (#30330)
7	1	vllm/config/vllm.py

[03b5f940f] dongbo910220 2025-12-10 [V1][Spec Decode] Optimize Medusa proposer to avoid GPU-CPU sync (#29723)
6	6	vllm/v1/spec_decode/medusa.py

[2e7054da0] Hashem Hashemi 2025-12-09 Improve wvsplitK tile and balance heristics. (#29937)
48	49	csrc/rocm/skinny_gemms.cu

[3c680f4a1] Charlie Fu 2025-12-09 [Rocm][torch.compile] Adding layernorm + fp8 block quant and silu + fp8 block quant for Aiter (#25693)
93	5	tests/compile/test_fusion.py
57	5	tests/compile/test_silu_mul_quant_fusion.py
170	44	vllm/_aiter_ops.py
11	0	vllm/compilation/pass_manager.py
242	0	vllm/compilation/rocm_aiter_fusion.py
37	6	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[fccd53258] Kyle Sayers 2025-12-09 [Quantization] FP8 Weight Reloading for Quantized RL Rollout (#28480)
88	0	tests/quantization/test_fp8.py
72	79	vllm/model_executor/layers/quantization/fp8.py
7	0	vllm/model_executor/layers/quantization/kv_cache.py
4	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py
7	4	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
4	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
25	0	vllm/model_executor/utils.py

[00e5cbb96] bnellnm 2025-12-09 [MoE][Refactor] Remove most arguments to FusedMoEMethodBase.apply (#29066)
5	1	vllm/model_executor/layers/fused_moe/__init__.py
0	18	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
4	22	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
18	56	vllm/model_executor/layers/fused_moe/layer.py
60	164	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
4	22	vllm/model_executor/layers/quantization/awq_marlin.py
4	22	vllm/model_executor/layers/quantization/bitsandbytes.py
67	187	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
4	22	vllm/model_executor/layers/quantization/experts_int8.py
44	60	vllm/model_executor/layers/quantization/fp8.py
4	21	vllm/model_executor/layers/quantization/gguf.py
4	22	vllm/model_executor/layers/quantization/gptq_marlin.py
6	24	vllm/model_executor/layers/quantization/ipex_quant.py
36	70	vllm/model_executor/layers/quantization/modelopt.py
4	22	vllm/model_executor/layers/quantization/moe_wna16.py
31	66	vllm/model_executor/layers/quantization/mxfp4.py
20	52	vllm/model_executor/layers/quantization/quark/quark_moe.py
3	21	vllm/model_executor/layers/quantization/rtn.py

[7618dc973] rasmith 2025-12-09 [CI/Build] Make test_mha_attn.py run on correct platform only and check for flash_attn_varlen_func in layer.py (#29145)
9	2	tests/kernels/attention/test_mha_attn.py
4	1	vllm/attention/layer.py

[f8dacc66b] dependabot[bot] 2025-12-09 Bump actions/stale from 10.1.0 to 10.1.1 (#30234)
1	1	.github/workflows/stale.yml

[7cab92fd4] dependabot[bot] 2025-12-09 Bump actions/checkout from 6.0.0 to 6.0.1 (#30233)
1	1	.github/workflows/cleanup_pr_body.yml
1	1	.github/workflows/macos-smoke-test.yml
1	1	.github/workflows/pre-commit.yml

[73a484caa] Tsukasa OI 2025-12-10 [Model][Quantization] Fix / Add GGUF support for Qwen2 MoE models (#30307)
8	0	vllm/model_executor/models/qwen2_moe.py

[b37bf51e7] Lucas Wilkinson 2025-12-09 [CI/Test] Fix FP8 per-tensor quant test reference scale shape (#30352)
1	1	tests/kernels/quant_utils.py

[95501a70e] Lucas Wilkinson 2025-12-09 [BugFix] Fix DeepSeek-R1 hang with DP and MTP (#30119)
12	3	vllm/v1/worker/gpu_model_runner.py

[e858bfe05] Benjamin Chislett 2025-12-09 [Cleanup] Refactor profiling env vars into a CLI config (#29912)
3	2	benchmarks/auto_tune/auto_tune.sh
1	2	benchmarks/benchmark_serving_structured_output.py
1	0	docs/api/README.md
10	13	docs/contributing/profiling.md
8	5	examples/offline_inference/simple_profiling.py
36	35	tests/v1/worker/test_gpu_profiler.py
5	7	vllm/benchmarks/latency.py
1	2	vllm/benchmarks/serve.py
1	2	vllm/benchmarks/throughput.py
3	0	vllm/config/__init__.py
199	0	vllm/config/profiler.py
5	0	vllm/config/vllm.py
5	1	vllm/engine/arg_utils.py
17	0	vllm/entrypoints/llm.py
7	10	vllm/entrypoints/serve/profile/api_router.py
51	67	vllm/envs.py
47	25	vllm/profiler/{gpu_profiler.py => wrapper.py}
7	15	vllm/v1/engine/async_llm.py
9	27	vllm/v1/worker/cpu_worker.py
10	8	vllm/v1/worker/gpu_worker.py
2	2	vllm/v1/worker/tpu_worker.py
9	33	vllm/v1/worker/xpu_worker.py

[d471b2aff] Woosuk Kwon 2025-12-09 [Model Runner V2] Support num NaNs in logits (#30187)
21	20	vllm/v1/worker/gpu/async_utils.py
0	0	vllm/v1/worker/gpu/metrics/__init__.py
42	0	vllm/v1/worker/gpu/metrics/logits.py
1	1	vllm/v1/worker/gpu/model_runner.py
1	3	vllm/v1/worker/gpu/sample/min_p.py
14	0	vllm/v1/worker/gpu/sample/output.py
10	2	vllm/v1/worker/gpu/sample/sampler.py

[9e6562a3f] Woosuk Kwon 2025-12-09 [Model Runner V2] Fix Triton warning on tl.where (#30355)
1	0	vllm/v1/worker/gpu/sample/penalties.py

[0b6a8a304] Ilya Markov 2025-12-09 [BugFix] Fix non detected failing tests (#30277)
6	2	.buildkite/test-pipeline.yaml
0	1	tests/compile/fullgraph/test_multimodal_compile.py
6	0	tests/compile/test_compile_ranges.py
40	33	tests/compile/test_pass_manager.py
6	2	vllm/compilation/inductor_pass.py
19	0	vllm/compilation/piecewise_backend.py

[804e3468c] Alexei-V-Ivanov-AMD 2025-12-09 Update AMD test definitions (2025-12-08) (#30298)
130	58	.buildkite/test-amd.yaml

[83319b44c] Wentao Ye 2025-12-09 [Compile] Fix torch warning `TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled` (#29897)
2	0	tests/v1/e2e/test_async_scheduling.py
9	0	vllm/envs.py
4	0	vllm/v1/worker/gpu_worker.py

[56037dfa2] Lucas Wilkinson 2025-12-09 [BugFix] Fix `assert  batch_descriptor.num_tokens == num_tokens_padded` (#30173)
2	2	tests/v1/cudagraph/test_cudagraph_dispatch.py
1	1	vllm/forward_context.py
2	2	vllm/v1/cudagraph_dispatcher.py
1	1	vllm/v1/spec_decode/eagle.py
37	12	vllm/v1/worker/dp_utils.py
22	15	vllm/v1/worker/gpu_model_runner.py

[5dcd593ba] quanliu 2025-12-09 [Feature] Batch-Invariant Support for FA2 and LoRA (#30018)
10	0	tests/v1/determinism/test_batch_invariance.py
8	2	tests/v1/determinism/utils.py
5	1	vllm/model_executor/layers/batch_invariant.py

[5c213d289] Julien Denize 2025-12-09 [BUGFIX] Mistral tool call parser v11+ (#30332)
16	0	tests/tool_use/test_mistral_tool_parser.py
16	20	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[ee14644ba] vllmellm 2025-12-09 [ROCm] Aiter Quant Kernels (#25552)
87	0	vllm/_aiter_ops.py
31	0	vllm/model_executor/layers/quantization/input_quant_fp8.py
5	2	vllm/platforms/rocm.py

[1166c31cc] Dongjie Zou 2025-12-09 [Bugfix]: Fix glm46 awq marlin moe wna16 compatibility (#30210)
45	0	vllm/model_executor/layers/fused_moe/fused_moe.py
5	4	vllm/model_executor/layers/quantization/moe_wna16.py

[03416eada] haoyangli-amd 2025-12-09 [bugfix][quantization] Fix fp8 per_tensor scale shape (#30257)
1	1	vllm/_custom_ops.py

[c72ea1072] Hubert de La Jonquiere 2025-12-09 [Structured Output][Reasoning] Improves decoding throughput for models using single-token reasoning endings. (#30056)
3	0	docs/features/reasoning_outputs.md
35	0	tests/reasoning/test_base_thinking_reasoning_parser.py
1	0	tests/reasoning/test_deepseekv3_reasoning_parser.py
1	0	tests/v1/structured_output/test_reasoning_structured_output.py
25	0	vllm/reasoning/abs_reasoning_parsers.py
6	0	vllm/reasoning/basic_parsers.py
5	0	vllm/reasoning/deepseek_v3_reasoning_parser.py
5	0	vllm/reasoning/holo2_reasoning_parser.py
5	0	vllm/reasoning/identity_reasoning_parser.py
3	1	vllm/v1/structured_output/__init__.py

[67475a6e8] Jaya Yuan 2025-12-09 [DCP][Bugfix][CI] Fix accuracy issue of DCP when using FLASH_ATTN_MLA (#30309)
4	1	tests/distributed/test_context_parallel.py
2	1	vllm/v1/attention/backends/mla/flashattn_mla.py

[9c32df610] wang.yuqi 2025-12-09 [Bugfix] Qwen 3 VL Embedding loading (#30303)
8	3	vllm/model_executor/models/adapters.py

[aeb82b193] Micah Williamson 2025-12-09 [CI] Fix Flaky test_eagle_max_len Test (#30306)
1	1	tests/v1/spec_decode/test_max_len.py

[aed846917] Lucas Wilkinson 2025-12-09 [Attention] Make `split_decodes_and_prefills(..., require_uniform=True)` support padding (#29644)
24	1	tests/v1/attention/test_attention_splitting.py
7	3	vllm/v1/attention/backends/utils.py

[e4605d225] Yongtao Huang 2025-12-09 [Misc] Fix safetensors import for safe_open (#30300)
1	1	vllm/lora/lora_model.py

[58d5b3f51] Tsukasa OI 2025-12-09 [Model][Quantization] Restore MoE + GGUF models support (incl. Qwen3 MoE) by allowing Sideload Parameters (#30116)
1	0	vllm/model_executor/layers/quantization/gguf.py
23	1	vllm/model_executor/model_loader/gguf_loader.py

[c2e1987a6] Fanli Lin 2025-12-09 [Doc] update Intel GPU MM status in Feature x Hardware matrix (#30294)
2	2	docs/features/README.md

[e13084598] Fadi Arafeh 2025-12-09 [CPU][CI] Enable fused MoE tests in Arm CI (#30132)
2	1	.buildkite/scripts/hardware_ci/run-cpu-test-arm.sh

[4b03b5021] liangel-02 2025-12-08 update torchao safetensors impl (#30155)
1	1	vllm/model_executor/model_loader/weight_utils.py

[4c6fd2588] Or Ozeri 2025-12-09 kv_transfer: Rename the shared storage connectors (#30201)
1	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh
3	3	docs/features/disagg_encoder.md
2	2	docs/features/disagg_prefill.md
1	1	examples/offline_inference/disaggregated-prefill-v1/decode_example.py
1	1	examples/offline_inference/disaggregated-prefill-v1/prefill_example.py
2	2	examples/offline_inference/kv_load_failure_recovery/README.md
3	3	examples/offline_inference/kv_load_failure_recovery/decode_example.py
10	10	examples/offline_inference/kv_load_failure_recovery/{rogue_shared_storage_connector.py => load_recovery_example_connector.py}
1	1	examples/offline_inference/kv_load_failure_recovery/prefill_example.py
3	3	examples/online_serving/disaggregated_encoder/README.md
2	2	examples/online_serving/disaggregated_encoder/disagg_1e1p1d_example.sh
2	2	examples/online_serving/disaggregated_encoder/disagg_1e1pd_example.sh
1	1	tests/distributed/test_kvlayout.py
3	3	tests/v1/core/test_scheduler.py
2	2	tests/v1/core/utils.py
4	4	tests/v1/ec_connector/integration/run_epd_correctness_test.sh
43	43	tests/v1/ec_connector/unit/{test_ec_shared_storage_connector.py => test_ec_example_connector.py}
2	2	tests/v1/engine/test_engine_core.py
4	4	tests/v1/kv_connector/unit/test_backwards_compatibility.py
3	3	tests/v1/kv_connector/unit/{test_shared_storage_connector.py => test_example_connector.py}
5	5	tests/v1/kv_connector/unit/test_kv_connector_lifecyle.py
11	11	tests/v1/kv_connector/unit/test_multi_connector.py
5	5	tests/v1/kv_connector/unit/utils.py
4	4	vllm/distributed/ec_transfer/ec_connector/{shared_storage_connector.py => example_connector.py}
3	3	vllm/distributed/ec_transfer/ec_connector/factory.py
3	3	vllm/distributed/kv_transfer/kv_connector/factory.py
5	5	vllm/distributed/kv_transfer/kv_connector/v1/{shared_storage_connector.py => example_connector.py}

[03b91f726] Michael Goin 2025-12-08 [Bugfix] Fix compressed-tensors models failing to load with transformers backend (#30287)
30	9	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[f6227c22a] czhu-cohere 2025-12-08 [Kernel]Support W4A8 Grouped GEMM on Hopper (#29691)
4	1	CMakeLists.txt
2	1	csrc/ops.h
104	0	csrc/quantization/cutlass_w4a8/get_group_starts.cuh
483	0	csrc/quantization/cutlass_w4a8/w4a8_grouped_mm_entry.cu
3	67	csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu
90	0	csrc/quantization/cutlass_w4a8/w4a8_utils.cu
11	0	csrc/quantization/cutlass_w4a8/w4a8_utils.cuh
5	3	csrc/quantization/w8a8/cutlass/moe/moe_data.cu
5	3	csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu
25	1	csrc/torch_bindings.cpp
41	5	tests/kernels/quantization/test_cutlass_w4a8.py
340	0	tests/kernels/quantization/test_cutlass_w4a8_moe.py
89	1	vllm/_custom_ops.py
4	0	vllm/model_executor/layers/fused_moe/__init__.py
29	0	vllm/model_executor/layers/fused_moe/config.py
401	0	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
339	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
4	11	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py
15	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py
49	1	vllm/model_executor/layers/quantization/utils/quant_utils.py

[ea657f207] gnovack 2025-12-08 Lora MoE Align Improvements (#29257)
0	1	CMakeLists.txt
354	71	csrc/moe/moe_align_sum_kernels.cu
0	174	csrc/moe/moe_lora_align_sum_kernels.cu
1	1	csrc/moe/moe_ops.h
2	1	csrc/moe/torch_bindings.cpp
1	1	tests/lora/test_moe_lora_align_sum.py
2	0	vllm/_custom_ops.py

[db14f61f2] Kevin H. Luu 2025-12-08 [ci] Refactor CI file structure (#29343)
24	0	.buildkite/ci_config.yaml
56	0	.buildkite/image_build/image_build.sh
57	0	.buildkite/image_build/image_build.yaml
36	0	.buildkite/image_build/image_build_cpu.sh
33	0	.buildkite/image_build/image_build_cpu_arm64.sh
34	0	.buildkite/image_build/image_build_hpu.sh
21	0	.buildkite/test_areas/attention.yaml
16	0	.buildkite/test_areas/basic_correctness.yaml
19	0	.buildkite/test_areas/benchmarks.yaml
57	0	.buildkite/test_areas/compile.yaml
22	0	.buildkite/test_areas/cuda.yaml
199	0	.buildkite/test_areas/distributed.yaml
59	0	.buildkite/test_areas/e2e_integration.yaml
26	0	.buildkite/test_areas/engine.yaml
68	0	.buildkite/test_areas/entrypoints.yaml
23	0	.buildkite/test_areas/expert_parallelism.yaml
117	0	.buildkite/test_areas/kernels.yaml
46	0	.buildkite/test_areas/lm_eval.yaml
31	0	.buildkite/test_areas/lora.yaml
163	0	.buildkite/test_areas/misc.yaml
17	0	.buildkite/test_areas/model_executor.yaml
62	0	.buildkite/test_areas/models_basic.yaml
22	0	.buildkite/test_areas/models_distributed.yaml
91	0	.buildkite/test_areas/models_language.yaml
79	0	.buildkite/test_areas/models_multimodal.yaml
34	0	.buildkite/test_areas/plugins.yaml
50	0	.buildkite/test_areas/pytorch.yaml
46	0	.buildkite/test_areas/quantization.yaml
14	0	.buildkite/test_areas/samplers.yaml
23	0	.buildkite/test_areas/tool_use.yaml
25	0	.buildkite/test_areas/weight_loading.yaml

[78c750336] Micah Williamson 2025-12-08 [ROCm][CI] Skip NVIDIA-Only Prime-RL Test in AMD CI (#29420)
5	0	.buildkite/scripts/run-prime-rl-test.sh

[e41312a2f] Christina Norman 2025-12-08 [Bugfix] Skip generation config fallback for GGUF to prevent multi-process hang (#30209)
7	0	vllm/transformers_utils/config.py

[7b35011ad] Yanan Cao 2025-12-08 Mark qwen2_5_vl as xfail (#30283)
1	0	tests/compile/fullgraph/test_multimodal_compile.py

[ae339b1a6] Zhewen Li 2025-12-08 [Bugfix] Fix DeepGEMM after #29546  (#30267)
10	6	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	0	vllm/utils/deep_gemm.py

[0ee6416f6] Wentao Ye 2025-12-08 [Perf] Optimize `group_topk` kernel, 1.9% Throughput improvement, 2.1% TPOT improvemnt (#30159)
128	47	csrc/moe/grouped_topk_kernels.cu

[d9417096d] Wentao Ye 2025-12-08 [Feature] Batch invariant: Enable `TRITON_MLA` without prefix-caching (#29125)
1	5	tests/v1/determinism/test_batch_invariance.py
4	1	tests/v1/determinism/test_online_batch_invariance.py
1	0	tests/v1/determinism/utils.py
36	0	vllm/attention/layer.py
1	1	vllm/model_executor/layers/batch_invariant.py

[9d6235ca9] Ming Yang 2025-12-08 [moe] Allow disabling DP chunking (#29936)
4	0	vllm/envs.py
1	1	vllm/model_executor/layers/fused_moe/layer.py

[f1599ca55] Victor Ziliang Peng 2025-12-08 feat(metrics): Add prefill KV compute metric excluding cached tokens (#30189)
102	1	tests/v1/metrics/test_stats.py
1	0	vllm/v1/engine/output_processor.py
20	0	vllm/v1/metrics/loggers.py
3	0	vllm/v1/metrics/stats.py

[60d17251c] Ming Yang 2025-12-08 [Disagg] Support large batch size in proxy server and update NixlConnector doc for DP (#28782)
2	0	docs/features/nixl_connector_usage.md
19	2	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py
21	2	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py

[1fb632fdb] Lain 2025-12-08 [Perf] Improve fp8 quant in mla; replace ReduceSum with ReduceScatterSum (#29795)
1	1	vllm/distributed/device_communicators/cuda_communicator.py
21	12	vllm/v1/attention/backends/mla/common.py

[6af70e11a] Charlie Fu 2025-12-08 [ROCm][CI] Fix test_max_len.py for Rocm (#29916)
4	1	tests/basic_correctness/test_basic_correctness.py
2	2	tests/utils.py
2	2	tests/v1/e2e/test_spec_decode.py
6	2	tests/v1/spec_decode/test_eagle.py
1	1	tests/v1/spec_decode/test_max_len.py

[ae0f69b16] roikoren755 2025-12-08 Add SpecDec support to `selective_state_update` (#29488)
325	0	tests/kernels/mamba/test_mamba_ssm.py
182	74	vllm/model_executor/layers/mamba/ops/mamba_ssm.py

[799804d14] Dmitry Tokarev 2025-12-08 Bump nvshmem to 3.3.24 and fix CUDA 13 installation (#30149)
6	12	tools/ep_kernels/install_python_libraries.sh

[0d402d260] Vasiliy Kuznetsov 2025-12-08 online fp8 quant with streaming weight post-processing (#29196)
66	1	vllm/model_executor/layers/quantization/fp8.py

[d1b5e7afb] Johnny Yang 2025-12-08 [TPU] Bump tpu-inference to 0.12.0 (#30221)
1	2	requirements/tpu.txt

[fcd5306f6] shaharmor98 2025-12-08 Add latent MoE support (#30203)
56	6	vllm/model_executor/models/nemotron_h.py
2	0	vllm/transformers_utils/configs/nemotron_h.py

[398a596ed] weiguihua2 2025-12-09 [MP executor] fix get device count for multi node of mp executor feature (#30042)
2	1	vllm/distributed/device_communicators/shm_broadcast.py

[67312cad1] Jee Jee Li 2025-12-09 [Misc] Split the LoRA code (#30253)
1	1	tests/lora/test_layers.py
1	1	tests/lora/test_lora_checkpoints.py
1	1	tests/lora/test_lora_huggingface.py
2	2	tests/lora/test_lora_manager.py
1	1	tests/lora/test_worker.py
246	0	vllm/lora/lora_model.py
2	235	vllm/lora/{models.py => model_manager.py}
9	0	vllm/lora/utils.py
2	2	vllm/lora/worker_manager.py

[87aee9ed2] Laith Sakka 2025-12-08 Add evaluate_guards option to DynamicShapesConfig (#27432)
132	7	tests/compile/test_dynamic_shapes_compilation.py
24	2	vllm/compilation/backends.py
2	2	vllm/compilation/decorators.py
46	13	vllm/compilation/wrapper.py
14	3	vllm/config/compilation.py
1	1	vllm/config/vllm.py

[184076c3f] Daniel Cámpora 2025-12-08 [DeepSeek v3.2] Make top-k work for any logit values. (#27568)
8	5	csrc/ops.h
536	179	csrc/sampler.cu
5	5	csrc/torch_bindings.cpp
77	18	tests/kernels/test_top_k_per_row.py
3	3	vllm/model_executor/models/deepseek_v2.py

[eb1051fb9] Ye (Charlotte) Qi 2025-12-08 [ROCm] Guard group quant RMS norm fusion patterns (#30239)
19	17	vllm/compilation/fusion.py

[80433e225] Jee Jee Li 2025-12-08 [LoRA]  Reduce the loading time of MoE LoRA (#30243)
27	7	vllm/lora/models.py

[5c2433a6f] Harry Mellor 2025-12-08 Add tip for `mypy` and `markdownlint` to the pre-commit comment (#30259)
14	0	.github/mergify.yml

[77072e93b] Simon Mo 2025-12-08 [docs] governance documents (#24801)
1	0	docs/.nav.yml
43	0	docs/governance/collaboration.md
183	0	docs/governance/committers.md
125	0	docs/governance/process.md

[2e660c243] wang.yuqi 2025-12-08 [Frontend] Binary embedding response does not return metadata by setting encoding_format to bytes_only. (#30249)
35	2	examples/pooling/embed/embedding_requests_bytes_client.py
50	0	tests/entrypoints/pooling/embed/test_online.py
56	0	tests/entrypoints/pooling/pooling/test_online.py
2	2	vllm/entrypoints/pooling/embed/api_router.py
2	2	vllm/entrypoints/pooling/embed/protocol.py
20	14	vllm/entrypoints/pooling/embed/serving.py
2	2	vllm/entrypoints/pooling/pooling/api_router.py
2	2	vllm/entrypoints/pooling/pooling/protocol.py
22	13	vllm/entrypoints/pooling/pooling/serving.py
39	4	vllm/utils/serial_utils.py

[408cf42f6] Shiming Zhang 2025-12-08 [CI] Prevents triggering of an inactive issue/PR check for forked repository. (#29654)
2	0	.github/workflows/stale.yml

[9e77ffca3] wang.yuqi 2025-12-08 [Model][7/N] Improve all pooling task | Deprecation as_reward_model. Extract hidden states prefer using new multi-vector retrieval API (#26686)
8	2	docs/models/pooling_models.md
1	8	docs/models/supported_models.md
71	0	examples/pooling/token_embed/jina_embeddings_v4.py
0	2	tests/models/test_registry.py
1	1	tests/test_config.py
7	3	vllm/config/model.py
0	4	vllm/model_executor/model_loader/utils.py
0	38	vllm/model_executor/models/adapters.py

[bcb6f5947] Dazhi Jiang 2025-12-08 [Perf] Remove sync point in vit torch sdpa attn backend (#30232)
6	6	vllm/attention/ops/vit_attn_wrappers.py
6	6	vllm/model_executor/models/ernie45_vl.py
6	6	vllm/model_executor/models/glm4_1v.py
6	6	vllm/model_executor/models/qwen2_vl.py

[cd00c443d] Zhiyu 2025-12-07 [Misc] Rename TensorRT Model Optimizer to Model Optimizer (#30091)
1	1	docs/features/quantization/README.md
3	3	docs/features/quantization/modelopt.md

[d14327123] Jiangyun Zhu 2025-12-08 [Bugfix] fix fuse_allreduce_rms when tp =1 (#30178)
7	0	vllm/compilation/collective_fusion.py

[c6df05ebb] Zhiwei 2025-12-08 [ROCm] [Fused Moe EP] Use binary expert mask for aiter fused moe kernel (#29773)
4	0	vllm/model_executor/layers/fused_moe/layer.py
1	0	vllm/model_executor/layers/quantization/quark/quark_moe.py

[d726a7b0e] Nick Hill 2025-12-07 [BugFix] Unblock use of LoRA with data parallel mode (#30220)
4	1	vllm/v1/metrics/loggers.py

[344b50d52] Zhijian Jiang 2025-12-07 Address comment to mergify.yml in #30117 (#30219)
2	2	.github/mergify.yml

[735284ed8] Andrew Xia 2025-12-07 [responsesAPI][7] Browser, Container MCP tools for non harmony models (#29989)
83	6	vllm/entrypoints/context.py

[444f0e3f3] daniel-salib 2025-12-07 [Frontend] Add MCP type support infrastructure to Responses API (#30054)
176	9	tests/entrypoints/openai/parser/test_harmony_utils.py
125	38	vllm/entrypoints/openai/parser/harmony_utils.py
8	0	vllm/entrypoints/openai/protocol.py

[af0444bf4] ElizaWszola 2025-12-07 [Performance] Fused blockwise quant RMS norm (#27883)
86	3	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
18	0	csrc/dispatch_utils.h
7	0	csrc/ops.h
124	20	csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
308	94	csrc/quantization/fused_kernels/layernorm_utils.cuh
8	0	csrc/torch_bindings.cpp
55	14	tests/compile/test_fusion.py
63	19	tests/kernels/core/test_fused_quant_layernorm.py
40	0	vllm/_custom_ops.py
171	1	vllm/compilation/fusion.py
42	2	vllm/compilation/matcher_utils.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py
6	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
17	0	vllm/utils/deep_gemm.py

[0044c4038] Lucas Wilkinson 2025-12-07 [BugFix][DeepSeek-V3.2] Fix backend selection logic for Blackwell (#30195)
2	2	vllm/platforms/cuda.py

[b952f4d3c] Isotr0py 2025-12-07 [v1] Add PrefixLM support to FlexAttention backend (#27938)
0	20	docs/models/supported_models.md
0	1	tests/models/multimodal/generation/test_common.py
1	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
9	0	vllm/attention/backends/abstract.py
5	0	vllm/attention/layer.py
5	0	vllm/attention/selector.py
14	0	vllm/config/model.py
25	0	vllm/multimodal/inputs.py
1	0	vllm/platforms/cpu.py
19	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
1	0	vllm/platforms/rocm.py
3	2	vllm/platforms/tpu.py
2	1	vllm/platforms/xpu.py
62	0	vllm/v1/attention/backends/flex_attention.py
25	1	vllm/v1/worker/gpu_model_runner.py

[541a2ef89] Wentao Ye 2025-12-07 [Perf] Deepgemm fused layout kernel for activations, 4.3% throughput improvement, 10.7% TTFT improvement. (#29546)
8	0	csrc/ops.h
185	0	csrc/quantization/w8a8/fp8/per_token_group_quant.cu
9	0	csrc/torch_bindings.cpp
30	11	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
79	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[b0f4866a7] Jee Jee Li 2025-12-07 [CI/Build]Temporary workaround for test_default_mm_loras timeout (#30202)
6	0	tests/lora/test_default_mm_loras.py

[879ddb09c] Jinzhen Lin 2025-12-07 [Kernel][MoE] optimize `moe_align_block_size` (#29642)
17	4	benchmarks/kernels/benchmark_moe_align_block_size.py
106	46	csrc/moe/moe_align_sum_kernels.cu
2	1	csrc/moe/moe_ops.h
2	1	csrc/moe/torch_bindings.cpp
15	1	tests/kernels/moe/test_moe.py
17	5	tests/kernels/moe/test_moe_align_block_size.py
2	0	vllm/_custom_ops.py
5	1	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
8	1	vllm/model_executor/layers/fused_moe/fused_moe.py
21	3	vllm/model_executor/layers/fused_moe/moe_align_block_size.py

[1b0482b9d] Yifan Qiao 2025-12-07 [Misc][Core] Remove unused `req_index` increment in scheduler (#30176)
0	1	vllm/v1/core/sched/scheduler.py

[e83b7e379] Cyrus Leung 2025-12-07 Revert "[Renderer] Separate out `RendererConfig` from `ModelConfig` (#30145)" (#30199)
6	6	docs/contributing/model/transcription.md
0	2	tests/compile/distributed/test_sequence_parallelism.py
1	5	tests/compile/test_functionalization.py
1	5	tests/compile/test_fusion.py
0	2	tests/compile/test_fusion_attn.py
2	6	tests/compile/test_pass_manager.py
1	4	tests/compile/test_qk_norm_rope_fusion.py
0	3	tests/distributed/test_kvlayout.py
18	4	tests/entrypoints/openai/test_chat_template.py
6	15	tests/entrypoints/openai/test_lora_resolvers.py
7	21	tests/entrypoints/openai/test_serving_chat.py
1	7	tests/entrypoints/openai/test_serving_engine.py
1	7	tests/entrypoints/openai/test_serving_models.py
123	71	tests/entrypoints/test_chat_utils.py
3	11	tests/lora/test_lora_manager.py
0	2	tests/lora/test_worker.py
9	13	tests/model_executor/test_model_load_with_params.py
2	3	tests/models/language/pooling/test_gritlm.py
17	5	tests/models/multimodal/processing/test_common.py
2	2	tests/models/multimodal/processing/test_glm4_1v.py
1	1	tests/models/multimodal/processing/test_h2ovl.py
1	1	tests/models/multimodal/processing/test_idefics3.py
1	1	tests/models/multimodal/processing/test_internvl.py
1	1	tests/models/multimodal/processing/test_llama4.py
3	3	tests/models/multimodal/processing/test_llava_next.py
3	3	tests/models/multimodal/processing/test_llava_onevision.py
2	2	tests/models/multimodal/processing/test_minimax_vl_01.py
1	1	tests/models/multimodal/processing/test_mllama4.py
1	1	tests/models/multimodal/processing/test_nemotron_vl.py
1	1	tests/models/multimodal/processing/test_phi3v.py
1	1	tests/models/multimodal/processing/test_phi4mm.py
1	1	tests/models/multimodal/processing/test_qwen2_vl.py
1	1	tests/models/multimodal/processing/test_smolvlm.py
16	8	tests/models/multimodal/processing/test_tensor_schema.py
2	3	tests/models/multimodal/processing/test_transformers.py
30	3	tests/models/multimodal/test_mapping.py
1	32	tests/models/registry.py
15	2	tests/models/utils.py
11	16	tests/multimodal/test_cache.py
9	15	tests/multimodal/test_processing.py
1	3	tests/multimodal/test_registry.py
51	80	tests/test_config.py
3	4	tests/test_inputs.py
0	2	tests/v1/attention/utils.py
4	16	tests/v1/core/test_kv_cache_utils.py
0	2	tests/v1/core/test_scheduler.py
0	2	tests/v1/core/utils.py
0	2	tests/v1/engine/test_engine_core.py
9	15	tests/v1/engine/test_process_multi_modal_uuids.py
0	2	tests/v1/kv_connector/unit/utils.py
0	2	tests/v1/spec_decode/test_eagle.py
0	2	tests/v1/spec_decode/test_mtp.py
0	2	tests/v1/spec_decode/test_ngram.py
3	9	tests/v1/structured_output/test_backend_guidance.py
14	21	tests/v1/structured_output/test_reasoning_structured_output.py
0	2	tests/v1/tpu/worker/test_tpu_model_runner.py
0	3	tests/v1/worker/test_gpu_model_runner.py
0	3	vllm/config/__init__.py
110	31	vllm/config/model.py
4	0	vllm/config/multimodal.py
0	109	vllm/config/renderer.py
5	0	vllm/config/speculative.py
10	15	vllm/config/vllm.py
40	59	vllm/engine/arg_utils.py
1	2	vllm/engine/protocol.py
34	45	vllm/entrypoints/chat_utils.py
6	8	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/serving_completion.py
5	6	vllm/entrypoints/openai/serving_engine.py
0	1	vllm/entrypoints/openai/serving_models.py
5	5	vllm/entrypoints/openai/speech_to_text.py
1	1	vllm/entrypoints/pooling/pooling/serving.py
3	1	vllm/entrypoints/pooling/score/serving.py
5	8	vllm/entrypoints/score_utils.py
4	4	vllm/entrypoints/utils.py
4	5	vllm/inputs/preprocess.py
10	10	vllm/model_executor/models/adapters.py
2	2	vllm/model_executor/models/deepseek_ocr.py
2	2	vllm/model_executor/models/deepseek_vl2.py
3	5	vllm/model_executor/models/gemma3n_mm.py
6	8	vllm/model_executor/models/granite_speech.py
7	7	vllm/model_executor/models/gritlm.py
4	6	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/interns1.py
6	7	vllm/model_executor/models/nano_nemotron_vl.py
1	1	vllm/model_executor/models/nemotron_vl.py
1	1	vllm/model_executor/models/pixtral.py
10	12	vllm/model_executor/models/voxtral.py
6	8	vllm/model_executor/models/whisper.py
12	10	vllm/multimodal/cache.py
5	23	vllm/multimodal/processing.py
34	30	vllm/multimodal/registry.py
12	12	vllm/tokenizers/registry.py
11	17	vllm/transformers_utils/processor.py
4	4	vllm/v1/core/encoder_cache_manager.py
1	1	vllm/v1/core/sched/scheduler.py
3	4	vllm/v1/engine/async_llm.py
3	4	vllm/v1/engine/input_processor.py
3	4	vllm/v1/engine/llm_engine.py
1	1	vllm/v1/spec_decode/eagle.py
12	6	vllm/v1/structured_output/__init__.py
3	4	vllm/v1/worker/gpu_model_runner.py
3	4	vllm/v1/worker/tpu_model_runner.py
7	12	vllm/v1/worker/utils.py

[27f4c2fd4] Cyrus Leung 2025-12-07 [Renderer] Separate out `RendererConfig` from `ModelConfig` (#30145)
6	6	docs/contributing/model/transcription.md
2	0	tests/compile/distributed/test_sequence_parallelism.py
5	1	tests/compile/test_functionalization.py
5	1	tests/compile/test_fusion.py
2	0	tests/compile/test_fusion_attn.py
6	2	tests/compile/test_pass_manager.py
4	1	tests/compile/test_qk_norm_rope_fusion.py
3	0	tests/distributed/test_kvlayout.py
4	18	tests/entrypoints/openai/test_chat_template.py
15	6	tests/entrypoints/openai/test_lora_resolvers.py
21	7	tests/entrypoints/openai/test_serving_chat.py
7	1	tests/entrypoints/openai/test_serving_engine.py
7	1	tests/entrypoints/openai/test_serving_models.py
71	123	tests/entrypoints/test_chat_utils.py
11	3	tests/lora/test_lora_manager.py
2	0	tests/lora/test_worker.py
13	9	tests/model_executor/test_model_load_with_params.py
3	2	tests/models/language/pooling/test_gritlm.py
5	17	tests/models/multimodal/processing/test_common.py
2	2	tests/models/multimodal/processing/test_glm4_1v.py
1	1	tests/models/multimodal/processing/test_h2ovl.py
1	1	tests/models/multimodal/processing/test_idefics3.py
1	1	tests/models/multimodal/processing/test_internvl.py
1	1	tests/models/multimodal/processing/test_llama4.py
3	3	tests/models/multimodal/processing/test_llava_next.py
3	3	tests/models/multimodal/processing/test_llava_onevision.py
2	2	tests/models/multimodal/processing/test_minimax_vl_01.py
1	1	tests/models/multimodal/processing/test_mllama4.py
1	1	tests/models/multimodal/processing/test_nemotron_vl.py
1	1	tests/models/multimodal/processing/test_phi3v.py
1	1	tests/models/multimodal/processing/test_phi4mm.py
1	1	tests/models/multimodal/processing/test_qwen2_vl.py
1	1	tests/models/multimodal/processing/test_smolvlm.py
8	16	tests/models/multimodal/processing/test_tensor_schema.py
3	2	tests/models/multimodal/processing/test_transformers.py
3	30	tests/models/multimodal/test_mapping.py
32	1	tests/models/registry.py
2	15	tests/models/utils.py
16	11	tests/multimodal/test_cache.py
15	9	tests/multimodal/test_processing.py
3	1	tests/multimodal/test_registry.py
80	51	tests/test_config.py
4	3	tests/test_inputs.py
2	0	tests/v1/attention/utils.py
16	4	tests/v1/core/test_kv_cache_utils.py
2	0	tests/v1/core/test_scheduler.py
2	0	tests/v1/core/utils.py
2	0	tests/v1/engine/test_engine_core.py
15	9	tests/v1/engine/test_process_multi_modal_uuids.py
2	0	tests/v1/kv_connector/unit/utils.py
2	0	tests/v1/spec_decode/test_eagle.py
2	0	tests/v1/spec_decode/test_mtp.py
2	0	tests/v1/spec_decode/test_ngram.py
9	3	tests/v1/structured_output/test_backend_guidance.py
21	14	tests/v1/structured_output/test_reasoning_structured_output.py
2	0	tests/v1/tpu/worker/test_tpu_model_runner.py
3	0	tests/v1/worker/test_gpu_model_runner.py
3	0	vllm/config/__init__.py
31	110	vllm/config/model.py
0	4	vllm/config/multimodal.py
109	0	vllm/config/renderer.py
0	5	vllm/config/speculative.py
15	10	vllm/config/vllm.py
59	40	vllm/engine/arg_utils.py
2	1	vllm/engine/protocol.py
45	34	vllm/entrypoints/chat_utils.py
8	6	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/serving_completion.py
6	5	vllm/entrypoints/openai/serving_engine.py
1	0	vllm/entrypoints/openai/serving_models.py
5	5	vllm/entrypoints/openai/speech_to_text.py
1	1	vllm/entrypoints/pooling/pooling/serving.py
1	3	vllm/entrypoints/pooling/score/serving.py
8	5	vllm/entrypoints/score_utils.py
4	4	vllm/entrypoints/utils.py
5	4	vllm/inputs/preprocess.py
10	10	vllm/model_executor/models/adapters.py
2	2	vllm/model_executor/models/deepseek_ocr.py
2	2	vllm/model_executor/models/deepseek_vl2.py
5	3	vllm/model_executor/models/gemma3n_mm.py
8	6	vllm/model_executor/models/granite_speech.py
7	7	vllm/model_executor/models/gritlm.py
6	4	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/interns1.py
7	6	vllm/model_executor/models/nano_nemotron_vl.py
1	1	vllm/model_executor/models/nemotron_vl.py
1	1	vllm/model_executor/models/pixtral.py
12	10	vllm/model_executor/models/voxtral.py
8	6	vllm/model_executor/models/whisper.py
10	12	vllm/multimodal/cache.py
23	5	vllm/multimodal/processing.py
30	34	vllm/multimodal/registry.py
12	12	vllm/tokenizers/registry.py
17	11	vllm/transformers_utils/processor.py
4	4	vllm/v1/core/encoder_cache_manager.py
1	1	vllm/v1/core/sched/scheduler.py
4	3	vllm/v1/engine/async_llm.py
4	3	vllm/v1/engine/input_processor.py
4	3	vllm/v1/engine/llm_engine.py
1	1	vllm/v1/spec_decode/eagle.py
6	12	vllm/v1/structured_output/__init__.py
4	3	vllm/v1/worker/gpu_model_runner.py
4	3	vllm/v1/worker/tpu_model_runner.py
12	7	vllm/v1/worker/utils.py

[a49d813fa] Luke 2025-12-06 Lazy loading to avoid importing all files (#29716)
52	38	vllm/transformers_utils/configs/__init__.py

[17eb25e32] Wentao Ye 2025-12-06 [Perf] Enable cuda graph for deepepHT, 5.3% throughput improvement, 4.4% TTFT improvement (#29558)
65	1	tests/compile/test_config.py
73	38	vllm/config/compilation.py
4	1	vllm/config/vllm.py
0	38	vllm/platforms/cuda.py

[dce6d229f] jeremyteboul 2025-12-06 Support multiple image/audio embeddings per requests (#29988)
8	2	docs/features/multimodal_inputs.md
178	0	tests/entrypoints/test_chat_utils.py
12	18	vllm/entrypoints/chat_utils.py

[cbedb703c] Yanan Cao 2025-12-06 [Frontend] Remove confusing -O.xx flag error (#30169)
0	19	tests/utils_/test_argparse_utils.py
0	7	vllm/utils/argparse_utils.py

[8d3da4c79] AuruTus 2025-12-07 [MISC]: change NIXL compatibility hash logging level to debug (#30182)
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[421125d03] Andrew Xia 2025-12-06 [ez] move harmony utils to parser folder (#30117)
0	0	tests/entrypoints/openai/parser/__init__.py
1	1	tests/entrypoints/{ => openai/parser}/test_harmony_utils.py
1	1	tests/entrypoints/openai/test_response_api_with_harmony.py
1	1	vllm/entrypoints/context.py
0	0	vllm/entrypoints/{ => openai/parser}/harmony_utils.py
2	2	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py
1	1	vllm/reasoning/gptoss_reasoning_parser.py

[671427efb] Cyrus Leung 2025-12-06 [Model] Move `multimodal_cpu_fields` definition to field config (#30181)
1	1	tests/distributed/test_shm_storage.py
1	1	tests/multimodal/test_cache.py
15	6	tests/v1/test_serial_utils.py
11	18	vllm/model_executor/models/glm4_1v.py
1	3	vllm/model_executor/models/hunyuan_vision.py
11	2	vllm/model_executor/models/interfaces.py
0	2	vllm/model_executor/models/opencua.py
0	2	vllm/model_executor/models/qwen2_5_vl.py
6	6	vllm/model_executor/models/qwen2_vl.py
2	4	vllm/model_executor/models/qwen3_vl.py
81	35	vllm/multimodal/inputs.py
6	2	vllm/multimodal/utils.py
6	5	vllm/v1/serial_utils.py
0	5	vllm/v1/worker/gpu_model_runner.py
0	3	vllm/v1/worker/tpu_model_runner.py

[21bb32354] Viacheslav 2025-12-06 Gigachat 3 tool parser and tests (#29905)
13	0	docs/features/tool_calling.md
176	0	tests/entrypoints/openai/tool_parsers/test_gigachat3_tool_parser.py
4	0	vllm/entrypoints/openai/tool_parsers/__init__.py
190	0	vllm/entrypoints/openai/tool_parsers/gigachat3_tool_parser.py

[17a9abec2] Chukwuma Nwaugha 2025-12-06 simplify requires_files list creation (#29656)
2	5	use_existing_torch.py

[92c35abb2] Ye (Charlotte) Qi 2025-12-06 [Misc] Fix circular import in vllm.transformers_utils.config (#30179)
2	1	vllm/transformers_utils/config.py

[43e759303] Yu Jiaqi 2025-12-06 Support tokenization_kwargs override (#29794)
13	4	tests/conftest.py
16	2	tests/models/multimodal/pooling/test_siglip.py
20	2	vllm/entrypoints/llm.py

[c46b932df] Cyrus Leung 2025-12-06 [Chore] Deprecate `SupportsMultiModal.merge_by_field_config` (#30170)
0	2	vllm/model_executor/models/aria.py
0	2	vllm/model_executor/models/aya_vision.py
0	2	vllm/model_executor/models/blip2.py
0	2	vllm/model_executor/models/chameleon.py
0	1	vllm/model_executor/models/clip.py
0	2	vllm/model_executor/models/cohere2_vision.py
0	2	vllm/model_executor/models/deepseek_ocr.py
0	2	vllm/model_executor/models/deepseek_vl2.py
0	2	vllm/model_executor/models/dots_ocr.py
0	2	vllm/model_executor/models/ernie45_vl.py
0	2	vllm/model_executor/models/fuyu.py
0	2	vllm/model_executor/models/gemma3_mm.py
0	1	vllm/model_executor/models/gemma3n_mm.py
0	2	vllm/model_executor/models/glm4_1v.py
0	2	vllm/model_executor/models/glm4v.py
0	1	vllm/model_executor/models/granite_speech.py
0	1	vllm/model_executor/models/hunyuan_vision.py
0	2	vllm/model_executor/models/hyperclovax_vision.py
0	2	vllm/model_executor/models/idefics3.py
22	3	vllm/model_executor/models/interfaces.py
0	2	vllm/model_executor/models/interns1.py
0	2	vllm/model_executor/models/internvl.py
0	2	vllm/model_executor/models/keye.py
0	2	vllm/model_executor/models/kimi_vl.py
0	2	vllm/model_executor/models/llava.py
0	2	vllm/model_executor/models/llava_next.py
0	2	vllm/model_executor/models/llava_next_video.py
0	2	vllm/model_executor/models/llava_onevision.py
0	2	vllm/model_executor/models/midashenglm.py
0	2	vllm/model_executor/models/minicpmv.py
0	2	vllm/model_executor/models/minimax_vl_01.py
0	2	vllm/model_executor/models/mistral3.py
0	2	vllm/model_executor/models/mllama4.py
0	2	vllm/model_executor/models/molmo.py
0	2	vllm/model_executor/models/nano_nemotron_vl.py
0	2	vllm/model_executor/models/nemotron_vl.py
0	1	vllm/model_executor/models/opencua.py
0	2	vllm/model_executor/models/ovis.py
0	2	vllm/model_executor/models/ovis2_5.py
0	2	vllm/model_executor/models/paddleocr_vl.py
0	2	vllm/model_executor/models/paligemma.py
0	2	vllm/model_executor/models/phi3v.py
0	2	vllm/model_executor/models/phi4mm.py
0	2	vllm/model_executor/models/pixtral.py
0	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
0	1	vllm/model_executor/models/qwen2_5_vl.py
0	2	vllm/model_executor/models/qwen2_audio.py
0	1	vllm/model_executor/models/qwen2_vl.py
0	2	vllm/model_executor/models/qwen3_omni_moe_thinker.py
0	1	vllm/model_executor/models/qwen3_vl.py
0	2	vllm/model_executor/models/qwen_vl.py
0	1	vllm/model_executor/models/siglip.py
0	2	vllm/model_executor/models/skyworkr1v.py
0	2	vllm/model_executor/models/step3_vl.py
0	2	vllm/model_executor/models/tarsier.py
0	1	vllm/model_executor/models/terratorch.py
1	1	vllm/model_executor/models/transformers/multimodal.py
0	2	vllm/model_executor/models/ultravox.py
0	2	vllm/model_executor/models/voxtral.py
0	1	vllm/model_executor/models/whisper.py
0	1	vllm/multimodal/utils.py

[647638238] redwrasse 2025-12-05 prefix caching design doc sha256 now default (#29261)
2	2	docs/design/prefix_caching.md

[d6aeaddf4] kx 2025-12-06 [bugfix] fix type[AttentionBackend] bug in kv_connector_base_v1 (#30051)
1	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py

[a238cbd89] Woosuk Kwon 2025-12-05 [Model Runner V2] Support min-p sampling (#30171)
13	0	vllm/v1/worker/gpu/sample/metadata.py
53	0	vllm/v1/worker/gpu/sample/min_p.py
4	0	vllm/v1/worker/gpu/sample/sampler.py
7	0	vllm/v1/worker/gpu/states.py

[4026ae31e] Nick Hill 2025-12-05 [Misc] Move `disable_nccl_for_dp_synchronization` init logic into `VllmConfig` (#30161)
9	0	vllm/config/vllm.py
0	6	vllm/engine/arg_utils.py

[b12f4a983] rasmith 2025-12-05 [CI/Build][AMD] Use ROCM_ATTN instead of FLASH_ATTN test for test_register_kv_caches for ROCm and update test for TRITON_ATTN (#29985)
37	11	tests/v1/kv_connector/unit/test_nixl_connector.py

[40a046cd8] Rohan Potdar 2025-12-05 [Bugfix]: Fix `TokenizerLike` interface (#30009)
32	27	vllm/benchmarks/datasets.py
16	16	vllm/benchmarks/serve.py
16	6	vllm/benchmarks/throughput.py
2	1	vllm/config/model.py
3	0	vllm/tokenizers/deepseekv32.py
5	1	vllm/tokenizers/mistral.py
3	0	vllm/tokenizers/protocol.py
1	1	vllm/tokenizers/registry.py

[e858bc4d1] Peter Salas 2025-12-05 [Model] Add support for transformer-based Ultravox v0.7 projector (#30089)
89	7	vllm/model_executor/models/ultravox.py
2	0	vllm/transformers_utils/configs/ultravox.py

[e3fbb6f15] Dongjie Zou 2025-12-05 fix#30092 Kimi-Linear model loading failure with missing indexer_rotary_emb (#30093)
1	1	vllm/model_executor/layers/mla.py

[c4d62618c] yuttian1 2025-12-06 Fix AWQ MoE marlin check issue in marlin_utils.py for AMD backend (#30102)
4	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[62079d860] rasmith 2025-12-05 [CI/Build][AMD] Skip marlin, machete, and hadacore tests since these require _C functions not defined for ROCm (#30109)
7	0	tests/kernels/quantization/test_hadacore.py
6	0	tests/kernels/quantization/test_machete_mm.py
8	0	tests/kernels/quantization/test_marlin_gemm.py

[bf4a901af] Harry Mellor 2025-12-06 Better error when world size is larger than node and `distributed_executor_backend` is not set (#30140)
13	9	vllm/config/parallel.py

[7e31c3a3f] Samuel Shen 2025-12-05 [CI]: Remove unnecessary imports from test_lmache_integration (#30157)
2	60	tests/v1/kv_connector/unit/test_lmcache_integration.py

[dc839ad03] rasmith 2025-12-05 [CI/Build][AMD][Quantization] Fix test_int8_kernel.py by updating int8_utils to use hip.libdevice.round (#30151)
2	17	vllm/model_executor/layers/quantization/utils/int8_utils.py

[02a416919] Deboleina 2025-12-05 [Tests] Tool call tests for openai/gpt-oss-20b (#26237)
1	0	requirements/rocm-test.txt
359	0	tests/entrypoints/openai/tool_parsers/test_openai_tool_parser.py

[7b5575fa7] Wentao Ye 2025-12-05 [Bug] Fix vLLM config is not set error (#29999)
2	0	vllm/model_executor/layers/fused_moe/cutlass_moe.py
6	0	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
30	27	vllm/model_executor/layers/fused_moe/modular_kernel.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
6	0	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[77e447280] Bangsheng Tang 2025-12-05 let draft model follow target model's config_format (#30152)
1	0	vllm/config/speculative.py

[962d70381] Divakar Verma 2025-12-05 [Bugfix][llama4_eagle] Fix missing 'lm_head' attribute (#29926)
5	1	tests/v1/e2e/test_spec_decode.py
11	2	vllm/model_executor/models/llama4_eagle.py

[e23ca3a0e] Nicolò Lucchesi 2025-12-05 [CI] Re-use whisper_client for all tests (#30148)
13	17	tests/entrypoints/openai/test_transcription_validation_whisper.py

[3633035a3] Russell Bryant 2025-12-05 [Misc] Rename CohereForAI references to CohereLabs (#30147)
1	1	docs/models/supported_models.md
1	1	examples/offline_inference/vision_language.py
1	1	examples/offline_inference/vision_language_multi_image.py
1	1	tests/distributed/test_pipeline_parallel.py
2	2	tests/models/multimodal/generation/test_common.py
3	3	tests/models/registry.py

[bff78310d] Nicolò Lucchesi 2025-12-05 [Enc-Dec] Fix OOT tokenizer issue (#30144)
4	3	vllm/inputs/preprocess.py

[adb315060] Tova Movshovitz 2025-12-05 [KVConnector][Feature] Support KV connector cache reset via /reset_prefix_cache (#27170)
14	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
4	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
4	2	vllm/engine/protocol.py
6	2	vllm/entrypoints/llm.py
17	4	vllm/entrypoints/openai/api_server.py
3	1	vllm/v1/core/sched/interface.py
21	1	vllm/v1/core/sched/scheduler.py
6	2	vllm/v1/engine/async_llm.py
6	2	vllm/v1/engine/core.py
18	8	vllm/v1/engine/core_client.py
6	2	vllm/v1/engine/llm_engine.py

[4e26d3b09] Ilya Markov 2025-12-05 [Compile] Conditional compilation. Introduce compile_ranges (#24252)
19	4	tests/compile/distributed/test_fusions_e2e.py
168	0	tests/compile/test_compile_ranges.py
14	0	tests/conftest.py
52	95	vllm/compilation/backends.py
60	86	vllm/compilation/collective_fusion.py
20	20	vllm/compilation/compiler_interface.py
6	5	vllm/compilation/inductor_pass.py
12	4	vllm/compilation/pass_manager.py
84	41	vllm/compilation/piecewise_backend.py
3	2	vllm/compilation/sequence_parallelism.py
37	1	vllm/config/compilation.py
33	1	vllm/config/utils.py
48	0	vllm/config/vllm.py
26	9	vllm/v1/worker/gpu_worker.py
1	1	vllm/v1/worker/utils.py

[66e674cdd] Matthew Bonanni 2025-12-05 [Attention][UX][1/N] Add AttentionConfig and change attention env vars to CLI arguments (#26315)
2	3	tests/compile/test_fusion_attn.py
30	9	tests/v1/kv_connector/unit/test_nixl_connector.py
86	86	tests/v1/worker/test_gpu_model_runner.py
10	16	vllm/attention/backends/abstract.py
2	2	vllm/attention/layer.py
9	128	vllm/attention/selector.py
6	5	vllm/attention/utils/fa_utils.py
3	0	vllm/config/__init__.py
114	0	vllm/config/attention.py
0	13	vllm/config/model.py
7	0	vllm/config/vllm.py
30	1	vllm/engine/arg_utils.py
7	4	vllm/model_executor/models/config.py
2	5	vllm/model_executor/models/vision.py
13	10	vllm/platforms/cuda.py
14	23	vllm/utils/flashinfer.py
5	4	vllm/v1/attention/backends/flash_attn.py
14	12	vllm/v1/attention/backends/flashinfer.py
14	5	vllm/v1/attention/backends/mla/common.py
3	2	vllm/v1/attention/backends/mla/flashattn_mla.py
1	1	vllm/v1/attention/backends/rocm_attn.py
2	3	vllm/v1/attention/backends/triton_attn.py

[dff0a2b39] Mark McLoughlin 2025-12-05 [NIXL] Add remote_request_id to kv_transfer_params (#29665)
6	0	tests/v1/kv_connector/unit/test_nixl_connector.py
1	0	tests/v1/kv_connector/unit/utils.py
14	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[dc264bcea] Nick Hill 2025-12-05 [BugFix] Eagerly abort cancelled final-step requests (#29987)
311	0	tests/v1/engine/test_abort_final_step.py
34	3	vllm/v1/engine/core.py
7	3	vllm/v1/worker/gpu_worker.py

[78c44fd72] Nicolò Lucchesi 2025-12-05 [NIXL] Small cleanup of unused variables (#29618)
1	1	tests/v1/kv_connector/unit/test_nixl_connector.py
7	13	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[e7296b08d] Angela Yi 2025-12-05 [bugfix] Pass globals to aot_compiled function (#29428)
3	1	vllm/compilation/decorators.py

[da7bc54ea] Andrew Xia 2025-12-05 [responsesAPI][5] ResponsesParser with tools for full MCP python loop (#29798)
1	1	examples/online_serving/openai_responses_client_with_tools.py
96	3	tests/entrypoints/openai/test_response_api_parsable_context.py
88	4	vllm/entrypoints/context.py
34	0	vllm/entrypoints/openai/parser/responses_parser.py
59	6	vllm/entrypoints/openai/serving_engine.py
5	1	vllm/entrypoints/openai/serving_responses.py
20	1	vllm/entrypoints/responses_utils.py
44	0	vllm/entrypoints/tool.py

[949a6a19d] Mark McLoughlin 2025-12-05 [NIXL] Add compatibility checking to NIXL KV connector handshake (#29503)
213	11	tests/v1/kv_connector/unit/test_nixl_connector.py
8	2	tests/v1/kv_connector/unit/utils.py
159	13	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[2c174420f] Alec S 2025-12-05 Reduce validation to a warning (#28749)
0	16	vllm/config/structured_outputs.py
4	1	vllm/reasoning/abs_reasoning_parsers.py

[0d8a7d8a2] Yi Liu 2025-12-05 [Compressed Tensors] Add XPU `wNa16` support (#29484)
1	0	.buildkite/scripts/hardware_ci/run-xpu-test.sh
4	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
97	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/xpu.py

[9843e332d] Elham 2025-12-05 [CPU][Perf] Add fast vectorized exp impl from Arm Optimized Routines (#30068)
0	13	csrc/cpu/cpu_attn_impl.hpp
50	0	csrc/cpu/cpu_attn_macros.h

[b7d85cf25] Harry Mellor 2025-12-05 [CI] Have pre-commit comment on a PR if pre-commit was not used (#30077)
32	0	.github/mergify.yml

[c2894d388] Max Hu 2025-12-05 [Feature] Add Layer-wise NVTX Support (#29990)
27	3	vllm/compilation/wrapper.py
5	0	vllm/config/observability.py
8	0	vllm/engine/arg_utils.py
286	0	vllm/utils/nvtx_pytorch_hooks.py
49	0	vllm/v1/worker/gpu_model_runner.py

[3628bcaaf] Zhiwei 2025-12-05 [ROCm][MXFP4] Infer w4a4 quant method in rocm aiter fused moe (#29775)
4	0	vllm/model_executor/layers/fused_moe/config.py
2	2	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[b73b158ab] strinczer 2025-12-05 [Bugfix] Fix parse_output_message crash on commentary with no recipient (#29972)
188	1	tests/entrypoints/test_harmony_utils.py
6	4	vllm/entrypoints/harmony_utils.py

[7ae13c66b] Ning Xie 2025-12-05 [typing] fix type (#29964)
1	1	vllm/reasoning/abs_reasoning_parsers.py
1	1	vllm/reasoning/gptoss_reasoning_parser.py

[f16356fe3] Ming Yang 2025-12-05 [bench] Support common prefix len config (for decode-only bench) (#29934)
1	0	vllm/benchmarks/datasets.py
6	0	vllm/benchmarks/serve.py

[65ee97288] Alec S 2025-12-05 [BugFix] Adding env variable to disable async grammar compilation (#29996)
74	0	tests/v1/structured_output/test_backend_guidance.py
15	2	vllm/v1/structured_output/__init__.py

[62b333344] Yanan Cao 2025-12-05 [Frontend] Remove deprecated -O.xx flag (#29991)
1	1	docs/design/debug_vllm_compile.md
12	15	tests/utils_/test_argparse_utils.py
8	13	vllm/utils/argparse_utils.py

[feecba09a] rasmith 2025-12-05 [CI/Build][AMD] Use float16 in test_reset_prefix_cache_e2e to avoid accuracy issues (#29997)
1	0	tests/v1/core/test_reset_prefix_cache_e2e.py

[6038b1b04] amitz-nv 2025-12-05 [Frontend][Model] Add 'float16' to possible mamba cache dtype values, override mamba SSM cache dtype value for NemotronH (#29978)
1	1	vllm/config/cache.py
21	0	vllm/model_executor/models/config.py
1	0	vllm/utils/torch_utils.py

[60a66ea2d] Tiger Xu / Zhonghu Xu 2025-12-05 [DOC]: Add kthena to integrations (#29931)
333	0	docs/deployment/integrations/kthena.md
1	0	docs/deployment/k8s.md

[06579f9a8] Micah Williamson 2025-12-05 [AMD][CI] Add ray[default] Dependency On ROCm To Pass v1/metrics/test_engine_logger_apis.py (#30110)
3	0	requirements/rocm-test.txt

[6e865b6a8] Chukwuma Nwaugha 2025-12-05 Refactor example prompts fixture (#29854)
25	22	tests/conftest.py

[d698bb382] Jingchun Gao 2025-12-05 [Bugfix] Correct num_q_heads on DCP for Flashinfer backends  (#29487)
2	3	vllm/v1/attention/backends/flashinfer.py

[2c22c4ca2] Charlie Fu 2025-12-04 [ROCm][CI] Increase the memory threshold for test_deep_sleep_fp8_kvcache (#30104)
7	2	tests/basic_correctness/test_cumem.py

[5867819ea] Laith Sakka 2025-12-04 Do not guard during noop elimination pass (#30095)
2	6	vllm/compilation/noop_elimination.py

[7c9b2c8f8] Charlie Fu 2025-12-04 [ROCm][CI] Add jiwer dependency for testing (#30081)
1	0	requirements/rocm-test.txt

[0098a6e3d] Qiu 2025-12-05 [PCP&DCP] move CUDAGraph check for PCP&DCP to the check func of platforms (#29952)
8	22	vllm/config/vllm.py
17	0	vllm/platforms/cuda.py
18	0	vllm/platforms/rocm.py

[befb59e5b] Hubert de La Jonquiere 2025-12-05 [Model] Add Holo2 reasoning parser (#30048)
2	0	docs/features/reasoning_outputs.md
188	0	tests/reasoning/test_holo2_reasoning_parser.py
4	0	vllm/reasoning/__init__.py
83	0	vllm/reasoning/holo2_reasoning_parser.py

[aaddc9c82] Shengqi Chen 2025-12-05 [CI] fix silent error in nightly wheel index generation script, add generation time to HTML index (#30060)
26	7	.buildkite/scripts/generate-nightly-index.py
4	1	.buildkite/scripts/upload-wheels.sh

[263c38d74] Zhewen Li 2025-12-04 [CI/Build] Update batch invariant test trigger (#30080)
2	1	.buildkite/test-pipeline.yaml

[bcf43ab1f] Zhewen Li 2025-12-04 [CI/Build][AMD] Add Llama4 Maverick FP8 to AMD CI (#28695)
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.yaml
1	0	.buildkite/lm-eval-harness/configs/models-large-rocm.txt
55	20	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
101	92	.buildkite/test-amd.yaml

[4470ee2f9] Alexander Matveev 2025-12-04 [Perf] Enable separate shared_experts stream only for CUDA (#30085)
2	1	vllm/model_executor/layers/fused_moe/layer.py

[690cc3ef2] TimWang 2025-12-05 docs: update metrics design doc to use new vllm:kv_cache_usage_perc (#30041)
1	1	docs/design/metrics.md

[1f0d18459] Laith Sakka 2025-12-04 [aot_compile]change VLLM backend to read fake args from example_value (#29104)
66	0	tests/compile/test_aot_compile.py
15	9	vllm/compilation/backends.py
0	1	vllm/compilation/decorators.py

[c8ab988b1] Lucas Wilkinson 2025-12-04 [BugFix] Fix DBO assert `assert B_block_table == B_q` (#29933)
9	3	tests/v1/attention/test_attention_splitting.py
2	2	vllm/v1/spec_decode/eagle.py
4	39	vllm/v1/worker/dp_utils.py
29	16	vllm/v1/worker/gpu_model_runner.py
39	3	vllm/v1/worker/ubatch_utils.py

[48a5fff66] Peng-YM 2025-12-05 [Bugfix] Missing tokens in `return_token_ids` when tool parsers is enabled in streaming mode (#29074)
8	3	vllm/entrypoints/openai/serving_chat.py

[1119f6e47] Mercykid-bash 2025-12-05 Abstract eplb algo (#26471)
18	14	tests/distributed/test_eplb_algo.py
4	0	vllm/config/parallel.py
1	6	vllm/distributed/eplb/__init__.py
15	5	vllm/distributed/eplb/eplb_state.py
19	0	vllm/distributed/eplb/policy/__init__.py
40	0	vllm/distributed/eplb/policy/abstract.py
267	0	vllm/distributed/eplb/policy/default.py
0	260	vllm/distributed/eplb/rebalance_algo.py

[e10c84e06] Harry Mellor 2025-12-04 Access `partial_rotary_factor` from `rope_parameters` (#29966)
2	6	tests/kernels/core/test_mrope.py
4	1	vllm/model_executor/layers/rotary_embedding/__init__.py
1	4	vllm/model_executor/models/apertus.py
0	3	vllm/model_executor/models/bailing_moe.py
1	3	vllm/model_executor/models/bamba.py
0	5	vllm/model_executor/models/config.py
1	3	vllm/model_executor/models/falcon_h1.py
2	1	vllm/model_executor/models/glm.py
1	2	vllm/model_executor/models/glm4.py
1	2	vllm/model_executor/models/glm4_moe.py
2	4	vllm/model_executor/models/gpt_neox.py
0	3	vllm/model_executor/models/llama.py
0	2	vllm/model_executor/models/nemotron.py
0	1	vllm/model_executor/models/nemotron_nas.py
0	2	vllm/model_executor/models/persimmon.py
1	4	vllm/model_executor/models/phi.py
0	1	vllm/model_executor/models/qwen3_next.py
0	4	vllm/model_executor/models/stablelm.py
9	1	vllm/transformers_utils/config.py
13	7	vllm/transformers_utils/configs/nemotron.py
5	3	vllm/transformers_utils/configs/qwen3_next.py

[ece2825a2] Kuntai Du 2025-12-05 [KVConnector] Remove v0-related kv connector components such as kv pipe and kv lookup buffer (#29705)
0	160	tests/kv_transfer/test_lookup_buffer.py
0	8	tests/kv_transfer/test_lookup_buffer.sh
0	62	tests/kv_transfer/test_module.py
0	154	tests/kv_transfer/test_send_recv.py
0	9	tests/kv_transfer/test_send_recv.sh
0	0	vllm/distributed/kv_transfer/kv_lookup_buffer/__init__.py
0	179	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
0	164	vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py
0	242	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
0	0	vllm/distributed/kv_transfer/kv_pipe/__init__.py
0	66	vllm/distributed/kv_transfer/kv_pipe/base.py
0	295	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
0	285	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py

[652ba93da] Jee Jee Li 2025-12-05 [Bugfix] Fix FP8 MoE LoRA (#29890)
9	3	vllm/model_executor/layers/quantization/fp8.py

[6dcb07f67] Tao Yun 2025-12-05 support qwen3-vl handle requests with embeddings (#30037)
2	0	docs/features/multimodal_inputs.md
5	2	vllm/model_executor/models/qwen3_vl.py

[46cbbca05] Qiu 2025-12-05 [CI][DCP][Perf] reduce DCP CI execution time (#29858)
95	93	tests/distributed/test_context_parallel.py
5	1	tests/models/registry.py

[b286a311c] Cyrus Leung 2025-12-05 [Chore] Deprecate `merge_by_field_config` arg (#30035)
2	2	tests/models/multimodal/processing/test_common.py
4	3	tests/models/multimodal/processing/test_glm4_1v.py
2	3	tests/models/multimodal/processing/test_tensor_schema.py
3	6	tests/multimodal/test_cache.py
0	91	tests/multimodal/test_inputs.py
2	2	vllm/model_executor/models/deepseek_ocr.py
1	1	vllm/model_executor/models/interfaces.py
2	2	vllm/model_executor/models/lightonocr.py
6	6	vllm/model_executor/models/nano_nemotron_vl.py
2	2	vllm/model_executor/models/opencua.py
2	2	vllm/model_executor/models/paddleocr_vl.py
3	5	vllm/model_executor/models/paligemma.py
2	2	vllm/model_executor/models/qwen2_5_vl.py
1	8	vllm/multimodal/cache.py
46	96	vllm/multimodal/inputs.py
12	47	vllm/multimodal/utils.py
0	19	vllm/v1/serial_utils.py
0	3	vllm/v1/worker/gpu_model_runner.py
0	2	vllm/v1/worker/tpu_model_runner.py

[990f80647] Shengqi Chen 2025-12-05 [Doc] clarify nightly builds in developer docs (#30019)
160	0	docs/contributing/ci/nightly_builds.md

[5b4b42c0b] Doug Smith 2025-12-04 Mark DBO test as flaky on b200 for Distributed B200 test (#29913)
19	0	tests/v1/distributed/test_dbo.py

[cc050558f] Woosuk Kwon 2025-12-04 [Model Runner V2] Implement get_num_sampled_and_rejected kernel (#30029)
49	0	vllm/v1/worker/gpu/input_batch.py
16	17	vllm/v1/worker/gpu/model_runner.py
0	12	vllm/v1/worker/gpu/spec_decode/rejection_sample.py

[5c32a06a0] Harry Mellor 2025-12-04 Use Transformers v5 RoPE standardisation and validation (#30046)
6	1	vllm/transformers_utils/config.py

[dd97e047e] Yongtao Huang 2025-12-04 Fix broken multiline assert in `LoRAModelManager.register_module` (#30032)
2	2	vllm/lora/models.py

[9998ea5b5] Harry Mellor 2025-12-04 Delete HF version of Phi 4 MM (#30049)
0	1	docs/models/supported_models.md
0	281	tests/models/multimodal/generation/test_phi4_multimodal.py
0	22	tests/models/multimodal/processing/test_common.py
0	4	tests/models/registry.py
0	1447	vllm/model_executor/models/phi4_multimodal.py
1	1	vllm/model_executor/models/registry.py

[74c4d80c6] wang.yuqi 2025-12-04 [Model][6/N] Improve all pooling task | Support chunked prefill with ALL pooling (#27145)
1	1	docs/features/README.md
2	5	tests/entrypoints/pooling/classify/test_offline.py
6	6	tests/entrypoints/pooling/classify/test_online.py
1	1	tests/entrypoints/pooling/embed/test_offline.py
7	0	tests/entrypoints/pooling/reward/test_offline.py
53	0	tests/models/language/pooling/test_all_pooling_plus_chunked_prefill.py
0	1	tests/models/language/pooling/test_extract_hidden_states.py
4	4	tests/test_config.py
14	10	vllm/config/model.py
71	22	vllm/model_executor/layers/pooler.py
2	2	vllm/model_executor/models/terratorch.py
1	1	vllm/v1/outputs.py
26	3	vllm/v1/pool/metadata.py
21	2	vllm/v1/worker/gpu_input_batch.py
15	35	vllm/v1/worker/gpu_model_runner.py

[1b7c7f515] Kevin H. Luu 2025-12-04 [release] install regex (#30008)
1	1	.buildkite/scripts/upload-wheels.sh

[6796ce8bd] Chauncey 2025-12-04 [Bugfix] Fix the issue with interleaved thinking when using streaming (#30033)
11	1	tests/reasoning/test_base_thinking_reasoning_parser.py
8	1	vllm/reasoning/basic_parsers.py

[e96a6a6dc] Andreas Karatzas 2025-12-04 [ROCm][CI][Bugfix] Fixing the `Multi-Modal Models Test (Extended) 1` group (#30013)
4	2	.buildkite/test-amd.yaml
16	0	tests/models/multimodal/generation/conftest.py
10	2	tests/models/multimodal/generation/test_common.py
13	2	tests/models/multimodal/generation/test_granite_speech.py
10	0	tests/models/multimodal/generation/test_pixtral.py
1	1	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
44	1	tests/models/multimodal/generation/vlm_utils/model_utils.py
24	0	tests/models/multimodal/pooling/conftest.py
4	0	tests/models/registry.py
13	1	vllm/v1/attention/backends/flex_attention.py

[6366c098d] Noa Neria 2025-12-04 Validating Runai Model Streamer Integration with S3 Object Storage (#29320)
1	1	docker/Dockerfile
1	1	requirements/nightly_torch_test.txt
1	1	requirements/rocm.txt
1	1	requirements/test.in
3	3	requirements/test.txt
1	1	setup.py
0	0	tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/__init__.py
39	0	tests/model_executor/model_loader/runai_streamer_loader/conftest.py
0	0	tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/test_runai_model_streamer_loader.py
52	0	tests/model_executor/model_loader/runai_streamer_loader/test_runai_model_streamer_s3.py
0	0	tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/test_runai_utils.py
0	0	tests/model_executor/model_loader/{runai_model_streamer => runai_streamer_loader}/test_weight_utils.py
1	3	vllm/transformers_utils/runai_utils.py

[842aba501] dtc 2025-12-04 [P/D] Introduce Mooncake Transfer Engine as kv_connector (#24718)
58	0	docs/features/mooncake_connector_usage.md
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
124	0	vllm/distributed/kv_transfer/kv_connector/utils.py
914	0	vllm/distributed/kv_transfer/kv_connector/v1/mooncake_connector.py
3	125	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
10	0	vllm/envs.py

[f2f4cea6c] rasmith 2025-12-04 [CI/Build][AMD] Skip test on test_hybrid_attention_mamba_tensor_shapes on ROCm, requires FLASHINFER (#29995)
4	0	tests/v1/worker/test_gpu_model_runner.py

[dfdda9674] Arpit Khandelwal 2025-12-04 [Core] Remove forced None assignment for deprecated PassConfig flags (#29994)
15	6	tests/compile/test_config.py
12	11	vllm/config/compilation.py

[ffdd18111] Xu Wenqing 2025-12-04 Add DeepSeek-V3.2 tool parser. (#29848)
4	0	vllm/entrypoints/openai/tool_parsers/__init__.py
591	0	vllm/entrypoints/openai/tool_parsers/deepseekv32_tool_parser.py

[b8a6ae415] Ye (Charlotte) Qi 2025-12-04 [ROCm] add fallback for aiter fp8 decode mla (#30005)
33	4	vllm/_aiter_ops.py

[899e2ef55] Mark McLoughlin 2025-12-04 [Core] Fix standalone runs of test_reset_prefix_cache_e2e (#29899)
3	1	tests/v1/core/test_reset_prefix_cache_e2e.py

[68eb5c8d9] Cyrus Leung 2025-12-04 [Misc] Move functions into `PoolingMetadata` (#30027)
7	43	vllm/model_executor/layers/pooler.py
2	4	vllm/model_executor/models/gritlm.py
21	0	vllm/v1/pool/metadata.py

[5430e110c] Micah Williamson 2025-12-04 [CI][AMD] Match Main CI Behavior By Skipping test_eplb_spec_decode In AMD CI (#30006)
7	0	tests/distributed/test_eplb_spec_decode.py

[3f1b03739] TJian 2025-12-04 [ROCm] [Bugfix] `compute_attn_mask_seqlen` for qwen3 omni (#29974)
4	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[9aa33a74b] Charlie Fu 2025-12-04 [Rocm][CI] Fix test_speculator_eagle3 by skipping the CompressedTensorw4a16 Model (#30001)
5	0	tests/v1/spec_decode/test_speculators_eagle3.py

[fd68e909d] CYJiang 2025-12-04 [docs] Remove _total from counter metrics names (#30028)
6	6	docs/design/metrics.md

[404fc4bfc] daniel-salib 2025-12-03 [Frontend] refactor harmony utils output message parsing (#29820)
117	99	vllm/entrypoints/harmony_utils.py

[82a64b3d8] Chauncey 2025-12-04 [Bugfix] fixed deepseekv32 tool calling error (#30025)
4	2	vllm/tokenizers/deepseek_v32_encoding.py
2	1	vllm/tokenizers/deepseekv32.py

[9ae2f6037] Cyrus Leung 2025-12-04 [Misc] Various cleanups for MM input processing (#29970)
3	5	docs/features/multimodal_inputs.md
3	9	examples/online_serving/prompt_embed_inference_with_openai_client.py
31	44	tests/entrypoints/openai/test_vision_embeds.py
4	123	tests/entrypoints/test_chat_utils.py
2	15	tests/v1/entrypoints/openai/test_completion_with_image_embeds.py
5	8	vllm/entrypoints/chat_utils.py
0	1	vllm/entrypoints/llm.py
0	6	vllm/entrypoints/openai/serving_engine.py
1	4	vllm/entrypoints/score_utils.py
2	1	vllm/model_executor/models/hunyuan_vision.py
2	2	vllm/model_executor/models/keye.py
2	2	vllm/model_executor/models/keye_vl1_5.py
2	5	vllm/multimodal/audio.py
10	0	vllm/utils/serial_utils.py

[80f8af4b2] Jianwei Mao 2025-12-04 Fix error while downloading dependencies for CPU backend (#29797)
0	1	requirements/cpu-build.txt
0	1	requirements/cpu.txt

[8aaa81b35] Kuntai Du 2025-12-04 [KVConnector] remove unused code (the model aware kv ops class) (#29709)
1	86	vllm/distributed/kv_transfer/kv_connector/utils.py

[fca3f4665] Benjamin Bartels 2025-12-04 [Frontend] Fixes anthropic /v1/messages streaming not containing input_tokens on first chunk (#29971)
11	0	tests/entrypoints/openai/test_messages.py
9	1	vllm/entrypoints/anthropic/serving_messages.py

[28097d563] gausah01 2025-12-04 [Bugfix][CPU] Fix CPU KV cache fallback memory allocation (#29604)
14	3	vllm/platforms/cpu.py

[dd38ba3a2] Jee Jee Li 2025-12-04 [Bugfix] Fix adapter_enabled IMA (#29977)
6	2	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[5f91cdda7] Li Wang 2025-12-04 [Misc] Add docker build env for Ascend NPU (#30015)
1	0	.buildkite/scripts/hardware_ci/run-npu-test.sh

[33a3d6c79] Iceber Gu 2025-12-04 fix LoRA-related examples (#29956)
4	12	examples/offline_inference/lora_with_quantization_inference.py
3	15	examples/offline_inference/multilora_inference.py

[c493b9d09] Zhewen Li 2025-12-03 [CI/Build] Add MM code path to Examples Test (#29986)
1	0	.buildkite/test-pipeline.yaml

[ad32e3e19] Xieyang Xu 2025-12-03 enable multi-node in external launcher mode (#29833)
6	2	vllm/config/parallel.py
21	18	vllm/distributed/parallel_state.py

[1109f9828] Shengqi Chen 2025-12-04 [CI] fix docker image build by specifying merge-base commit id when downloading pre-compiled wheels (#29930)
0	46	.buildkite/generate_index.py
3	0	docker/Dockerfile
14	11	setup.py
5	1	tests/standalone_tests/python_only_compile.sh
0	6	vllm/envs.py

[b5407869c] Elizabeth Thomas 2025-12-03 [Bugfix] Respect VLLM_CONFIGURE_LOGGING value (#28671)
51	0	tests/test_envs.py
4	2	vllm/envs.py
4	0	vllm/utils/system_utils.py

[2902c3482] bnellnm 2025-12-03 [Kernels] Remove BatchedTritonOrDeepGemmExperts and default fallback to Triton (#29929)
1	2	docs/design/moe_kernel_features.md
0	17	tests/kernels/moe/modular_kernel_tools/mk_objects.py
0	4	vllm/model_executor/layers/fused_moe/__init__.py
0	180	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
45	14	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[ac1886588] Wentao Ye 2025-12-03 [CI] Fix re import error (#29973)
2	1	.buildkite/scripts/generate-nightly-index.py
1	2	vllm/entrypoints/serve/instrumentator/metrics.py
2	1	vllm/tokenizers/deepseek_v32_encoding.py

[2fc5d6e0d] Yongtao Huang 2025-12-04 Fix LLMEngine.del dp_group cleanup condition (#29954)
2	4	vllm/v1/engine/llm_engine.py

[afe9eb408] elvischenv 2025-12-04 [Bugfix] Fix flashinfer ar+norm kernel not available issue (#29960)
2	1	vllm/compilation/fix_functionalization.py

[19bee6d12] Varun Sundar Rabindranath 2025-12-03 [Performance][DP/EP] Add silu_mul_per_token_group_quant_fp8_colmajor kernel (#29470)
244	0	benchmarks/kernels/benchmark_2d_silu_mul_fp8_quant.py
86	0	tests/kernels/moe/test_silu_mul_per_token_group_quant_fp8_colmajor.py
33	81	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
133	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[dd5d1ef78] avigny 2025-12-03 [Bugfix] Mistral tool parser streaming update (#19425)
1	0	requirements/common.txt
847	0	tests/tool_use/test_mistral_tool_parser.py
27	1	tests/tool_use/utils.py
397	201	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[d1f7392c5] Micah Williamson 2025-12-03 [ROCm][CI] Fix v1/logits_processors failure on ROCm (#29927)
0	5	tests/v1/logits_processors/test_custom_offline.py
2	10	tests/v1/logits_processors/test_custom_online.py
1	1	tests/v1/logits_processors/utils.py

[9ae3c55b1] Yu Jiaqi 2025-12-04 SigLIP example add chat_template (#29902)
2	1	examples/pooling/embed/openai_chat_embedding_client_for_multimodal.py
27	8	vllm/entrypoints/chat_utils.py

[9bcf92295] Lumis Chen 2025-12-04 [Core] Add xxHash as a high-performance hash option for accelerating prefix caching (#29163)
120	0	benchmarks/benchmark_hash.py
110	0	benchmarks/benchmark_prefix_block_hash.py
29	0	docs/benchmarking/cli.md
16	0	tests/v1/engine/test_engine_args.py
15	3	vllm/config/cache.py
36	0	vllm/utils/hashing.py
6	5	vllm/v1/core/kv_cache_utils.py

[5aa9b0904] rasmith 2025-12-03 [CI/Build][AMD] Skip test_shared_storage_connector_hashes in test_shared_storage_connector.py due to hipErrorLaunchFailure when calling .cpu() (#29839)
9	0	tests/v1/kv_connector/unit/test_shared_storage_connector.py

[1bb17ecb3] ioana ghiban 2025-12-03 [CPU Backend] [Doc]: Update Installation Docs for CPUs (#29868)
4	3	docs/getting_started/installation/cpu.apple.inc.md
20	6	docs/getting_started/installation/cpu.arm.inc.md
28	2	docs/getting_started/installation/cpu.md
4	3	docs/getting_started/installation/cpu.s390x.inc.md
2	0	docs/getting_started/installation/cpu.x86.inc.md

[15b1511a1] ioana ghiban 2025-12-03 [GPU Backend] [Doc]: Remove duplicate statements on missing GPU wheels. (#29962)
0	3	docs/getting_started/installation/gpu.rocm.inc.md
0	3	docs/getting_started/installation/gpu.xpu.inc.md

[b78772c43] Chauncey 2025-12-03 [Frontend] supports deepseekv32 chat template (#29837)
2	1	vllm/config/model.py
8	1	vllm/entrypoints/openai/serving_engine.py
2	0	vllm/tokenizers/__init__.py
456	0	vllm/tokenizers/deepseek_v32_encoding.py
148	0	vllm/tokenizers/deepseekv32.py

[f5d3d93c4] Amr Mahdi 2025-12-03 [docker] Build CUDA kernels in separate Docker stage for faster rebuilds (#29452)
58	8	docker/Dockerfile
-	-	docs/assets/contributing/dockerfile-stages-dependency.png
11	3	setup.py
5	0	vllm/envs.py

[78f4bb0ba] Fadi Arafeh 2025-12-03 [DOC] Add Arm to list of compute resouces providers (#29894)
1	0	README.md
1	0	docs/community/sponsors.md

[b294e28db] HDCharles 2025-12-03 [refactor] CTMoEMethods to use QuantizationArgs (#28871)
4	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
82	73	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[787b84a9f] Roger Wang 2025-12-03 [Bugfix] Follow-up fix on MediaWithBytes (#29951)
2	0	vllm/multimodal/base.py
2	1	vllm/multimodal/inputs.py
1	1	vllm/multimodal/parse.py

[42c194964] Tsukasa OI 2025-12-03 [Bugfix][Quantization] Support BF16 tensors on GGUF (#29948)
7	0	tests/models/quantization/test_gguf.py
11	1	vllm/model_executor/model_loader/weight_utils.py

[cc4e296ea] Isotr0py 2025-12-03 [CI/Build] Avoid duplicate empty inputs test for common multimodal generation tests (#29907)
7	7	tests/models/multimodal/generation/test_common.py
60	54	tests/models/multimodal/generation/vlm_utils/case_filtering.py
2	2	tests/models/multimodal/generation/vlm_utils/types.py

[a21cd9ed2] Isotr0py 2025-12-03 [Bugfix] Fix incorrect `image_grid_thw` rank for HunyuanOCR from missing `merge_by_field_config=True` (#29950)
23	0	examples/offline_inference/vision_language_multi_image.py
1	0	vllm/model_executor/models/hunyuan_vision.py

[7fe9c1a22] WeiQing Chen 2025-12-03 [CI] Add Async Eplb nightly CI tests (#29385)
73	0	.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_async_eplb.sh
1	0	.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh
74	0	.buildkite/scripts/scheduled_integration_test/qwen3_next_mtp_async_eplb.sh
19	1	.buildkite/test-pipeline.yaml
0	3	vllm/distributed/eplb/rebalance_execute.py

[3f42b05fb] Chauncey 2025-12-03 [Refactor] [1/N] to simplify the vLLM serving architecture (#28040)
1	1	tests/entrypoints/openai/test_basic.py
1	0	vllm/entrypoints/api_server.py
10	445	vllm/entrypoints/openai/api_server.py
1	2	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/sagemaker/routes.py
60	0	vllm/entrypoints/serve/__init__.py
0	0	vllm/entrypoints/serve/disagg/__init__.py
110	0	vllm/entrypoints/serve/disagg/api_router.py
90	0	vllm/entrypoints/serve/disagg/protocol.py
7	3	vllm/entrypoints/{openai/serving_tokens.py => serve/disagg/serving.py}
0	0	vllm/entrypoints/serve/elastic_ep/__init__.py
96	0	vllm/entrypoints/serve/elastic_ep/api_router.py
49	0	vllm/entrypoints/serve/elastic_ep/middleware.py
0	0	vllm/entrypoints/serve/instrumentator/__init__.py
33	0	vllm/entrypoints/serve/instrumentator/health.py
46	0	vllm/entrypoints/serve/instrumentator/metrics.py
0	0	vllm/entrypoints/serve/lora/__init__.py
16	3	vllm/entrypoints/{dynamic_lora.py => serve/lora/api_router.py}
0	0	vllm/entrypoints/serve/profile/__init__.py
49	0	vllm/entrypoints/serve/profile/api_router.py
0	0	vllm/entrypoints/serve/rlhf/__init__.py
102	0	vllm/entrypoints/serve/rlhf/api_router.py
0	0	vllm/entrypoints/serve/sleep/__init__.py
60	0	vllm/entrypoints/serve/sleep/api_router.py
0	0	vllm/entrypoints/serve/tokenize/__init__.py
118	0	vllm/entrypoints/serve/tokenize/api_router.py
0	0	vllm/entrypoints/{openai/serving_tokenization.py => serve/tokenize/serving.py}

[69520bc69] Yong Hoon Shin 2025-12-02 Add logging for cudagraph related info (#29825)
94	0	vllm/compilation/cuda_graph.py
4	0	vllm/config/observability.py
6	0	vllm/engine/arg_utils.py
7	1	vllm/v1/core/sched/scheduler.py
14	0	vllm/v1/metrics/loggers.py
3	0	vllm/v1/metrics/stats.py
4	0	vllm/v1/outputs.py
28	4	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py

[3a7751485] Andrew Xia 2025-12-02 [responsesAPI] support input output messages for non harmony models (#29549)
18	0	tests/entrypoints/openai/test_response_api_simple.py
22	0	vllm/entrypoints/context.py
18	4	vllm/entrypoints/openai/protocol.py
6	7	vllm/entrypoints/openai/serving_responses.py

[bbfb55c29] Cyrus Leung 2025-12-03 [Misc] Allow `fetch_*` utils to access local files by default (#29932)
30	8	vllm/multimodal/utils.py
1	1	vllm/multimodal/video.py

[0bec63fa3] JackieWu 2025-12-03 [BugFix] fix imgs_pos in hunyuan_vl (#29879)
1	1	vllm/transformers_utils/processors/hunyuan_vl.py

[c719c4054] elvischenv 2025-12-03 [Bugfix] Defunctionalize TRTLLM AR+Norm op for avoiding extra clone kernel before it (#29631)
12	0	vllm/compilation/fix_functionalization.py
2	2	vllm/compilation/fx_utils.py

[b08025a83] Russell Bryant 2025-12-02 [Docs] Discuss api key limitations in security guide (#29922)
110	0	docs/usage/security.md
4	0	vllm/entrypoints/cli/openai.py

[d7284a260] Arpit Khandelwal 2025-12-02 [Core] Rename PassConfig flags as per RFC #27995 (#29646)
2	2	tests/compile/distributed/test_async_tp.py
1	1	tests/compile/distributed/test_fusion_all_reduce.py
8	8	tests/compile/distributed/test_fusions_e2e.py
9	9	tests/compile/distributed/test_sequence_parallelism.py
3	1	tests/compile/fullgraph/test_full_graph.py
65	12	tests/compile/test_config.py
5	1	tests/compile/test_functionalization.py
3	1	tests/compile/test_fusion.py
1	1	tests/compile/test_fusion_attn.py
2	2	tests/compile/test_noop_elimination.py
5	2	tests/compile/test_pass_manager.py
1	1	tests/compile/test_qk_norm_rope_fusion.py
1	1	tests/compile/test_silu_mul_quant_fusion.py
20	14	tests/distributed/test_sequence_parallel.py
6	6	tests/test_config.py
7	6	vllm/compilation/pass_manager.py
102	18	vllm/config/compilation.py
29	0	vllm/config/utils.py
45	31	vllm/config/vllm.py
1	4	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py
1	1	vllm/v1/worker/utils.py

[506ed87e8] Andreas Karatzas 2025-12-02 [ROCm][CI][Bugfix] Disable Flash/MemEfficient SDP on ROCm to avoid HF Transformers accuracy issues (#29909)
1	5	docker/Dockerfile.rocm
2	2	requirements/rocm-test.txt
19	0	tests/models/multimodal/generation/conftest.py

[4dd797837] Roger Wang 2025-12-02 [Bugfix] Fix regression on pooling models from PR#29621 (#29921)
7	1	vllm/multimodal/parse.py

[5cdd66450] Lucas Wilkinson 2025-12-02 [BugFix] Fix assert in `build_for_cudagraph_capture` (#29893)
1	1	vllm/v1/worker/gpu_model_runner.py

[5f67361fd] Alexei-V-Ivanov-AMD 2025-12-02 Reverting re-direction to amd_mi355_X. (#29914)
11	11	.buildkite/test-amd.yaml

[5d91d2b29] maang-h 2025-12-03 [Doc] Add allocate_slots parameter docs (#29777)
3	0	vllm/v1/core/kv_cache_manager.py

[c014de1ec] Micah Williamson 2025-12-02 [ROCm][CI] Fix test_cudagraph_mode.py Failure For AMD CI (#29808)
14	26	tests/v1/cudagraph/test_cudagraph_mode.py

[1b1e35aaf] Julien Denize 2025-12-02 [BUGFIX] Fix regex pattern for Mistral Tool Call (#29918)
35	0	tests/models/language/generation/test_mistral.py
1	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[5e5646e20] Julien Denize 2025-12-02 [BUGFIX] llama_4_scaling wrongly passed to DeepseekAttention (#29908)
10	5	vllm/model_executor/models/deepseek_v2.py

[0a9caca9f] Chauncey 2025-12-03 [Bugfix] fix --scheduling-policy=priority & n>1 crashes engine (#29764)
11	1	tests/v1/core/test_priority_scheduler_random.py
10	14	vllm/v1/core/sched/request_queue.py
13	0	vllm/v1/request.py

[e6f114ac2] Sage Moore 2025-12-02 [Bugfix][EPLB] Prevent user-provided EPLB config from being overwritten with defaults (#29911)
9	7	tests/distributed/test_eplb_spec_decode.py
0	14	vllm/engine/arg_utils.py

[6fc5841db] Harry Mellor 2025-12-02 Fix some more Transformers nightly tests (#29872)
4	1	examples/offline_inference/vision_language.py
4	1	examples/offline_inference/vision_language_multi_image.py
4	1	tests/models/registry.py
0	9	vllm/model_executor/models/qwen2_vl.py
20	8	vllm/tokenizers/mistral.py
17	7	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
24	0	vllm/transformers_utils/configs/tarsier2.py

[3ff5b53bc] dependabot[bot] 2025-12-02 Bump actions/setup-python from 6.0.0 to 6.1.0 (#29768)
1	1	.github/workflows/cleanup_pr_body.yml
1	1	.github/workflows/pre-commit.yml

[1528e079e] jthomson04 2025-12-02 [Perf] Avoid pageable HtoD transfer in MinTokensLogitsProcessor (#29826)
6	2	vllm/v1/sample/logits_processor/builtin.py

[afb1e5b38] Divakar Verma 2025-12-02 [CI][ROCm][tests/v1/e2e] Fix multiprocessing launch for the test (#29123)
19	3	tests/v1/e2e/test_kv_sharing_fast_prefill.py

[1c593e117] Copilot 2025-12-02 Fix boolean nested params, add dict format support, and enhance plotting for vllm bench sweep (#29025)
257	0	tests/benchmarks/test_param_sweep.py
171	0	tests/benchmarks/test_plot_filters.py
76	9	vllm/benchmarks/sweep/param_sweep.py
102	7	vllm/benchmarks/sweep/plot.py
8	6	vllm/benchmarks/sweep/serve.py

[a2b053dc8] Navanit Dubey 2025-12-03 feat(model): Add BitsAndBytes quantization support for Qwen3-Omni-MoE (#29896)
23	0	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[1d93f1167] Matthew Bonanni 2025-12-02 [Attention][CUDAGraph] Remove CG padding from attention backends (#29352)
8	9	vllm/model_executor/layers/mamba/mamba_mixer.py
5	17	vllm/v1/attention/backends/gdn_attn.py
3	9	vllm/v1/attention/backends/mamba1_attn.py
3	9	vllm/v1/attention/backends/mamba2_attn.py
1	2	vllm/v1/attention/backends/short_conv_attn.py

[2d613de9a] Benjamin Bartels 2025-12-02 [CI/Build] Fixes missing runtime dependencies (#29822)
6	1	docker/Dockerfile

[c77b9929a] Alexei-V-Ivanov-AMD 2025-12-02 Update AMD-CI testing mirror (as of 2025-12-02) (#29898)
28	15	.buildkite/test-amd.yaml

[63b1da76b] Isotr0py 2025-12-03 [Chore]: Reorganize gguf utils funtions under `transformers_utils` (#29891)
1	1	tests/models/test_gguf_download.py
7	5	tests/transformers_utils/test_utils.py
3	5	vllm/config/model.py
2	1	vllm/engine/arg_utils.py
3	3	vllm/tokenizers/registry.py
7	7	vllm/transformers_utils/config.py
71	0	vllm/transformers_utils/gguf_utils.py
2	1	vllm/transformers_utils/processor.py
0	72	vllm/transformers_utils/utils.py

[52cb349fc] Andrew Xia 2025-12-02 [responsesAPI][3] ResponsesParser to set up non harmony MCP (#29413)
87	0	tests/entrypoints/openai/test_response_api_parsable_context.py
1	0	vllm/entrypoints/chat_utils.py
76	0	vllm/entrypoints/context.py
0	0	vllm/entrypoints/openai/parser/__init__.py
101	0	vllm/entrypoints/openai/parser/responses_parser.py
32	13	vllm/entrypoints/openai/serving_responses.py
30	0	vllm/entrypoints/responses_utils.py
5	0	vllm/envs.py

[0ec842217] Isotr0py 2025-12-03 [Bugfix] Fix incorrect channel order for idefics3 in edge case (#29881)
1	0	vllm/model_executor/models/idefics3.py

[2eb4fe912] wang.yuqi 2025-12-02 [examples] Resettle pooling examples. (#29365)
12	8	.buildkite/test-pipeline.yaml
2	2	.github/CODEOWNERS
1	5	docs/.nav.yml
1	1	docs/design/io_processor_plugins.md
84	70	docs/mkdocs/hooks/generate_examples.py
2	2	docs/models/pooling_models.md
2	2	docs/models/supported_models.md
7	7	docs/serving/openai_compatible_server.md
0	57	examples/offline_inference/pooling/README.md
0	97	examples/online_serving/pooling/README.md
0	0	examples/{online_serving/pooling => pooling/classify}/openai_classification_client.py
0	0	examples/{offline_inference/pooling => pooling/embed}/embed_jina_embeddings_v3.py
0	0	examples/{offline_inference/pooling => pooling/embed}/embed_matryoshka_fy.py
0	0	examples/{online_serving/pooling => pooling/embed}/embedding_requests_base64_client.py
0	0	examples/{online_serving/pooling => pooling/embed}/embedding_requests_bytes_client.py
0	0	examples/{online_serving/pooling => pooling/embed}/openai_chat_embedding_client_for_multimodal.py
0	0	examples/{online_serving/pooling => pooling/embed}/openai_embedding_client.py
0	0	examples/{online_serving => pooling/embed}/openai_embedding_long_text/README.md
0	0	examples/{online_serving => pooling/embed}/openai_embedding_long_text/client.py
0	0	examples/{online_serving => pooling/embed}/openai_embedding_long_text/service.sh
0	0	examples/{online_serving/pooling => pooling/embed}/openai_embedding_matryoshka_fy.py
0	0	examples/{online_serving/pooling/prithvi_geospatial_mae.py => pooling/plugin/prithvi_geospatial_mae_client.py}
0	0	examples/{offline_inference/pooling => pooling/plugin}/prithvi_geospatial_mae_io_processor.py
0	0	examples/{offline_inference/pooling/prithvi_geospatial_mae.py => pooling/plugin/prithvi_geospatial_mae_offline.py}
0	0	examples/{online_serving => pooling}/pooling/openai_pooling_client.py
0	0	examples/{offline_inference => pooling/pooling}/vision_language_pooling.py
0	0	examples/{online_serving/pooling => pooling/score}/cohere_rerank_client.py
0	0	examples/{offline_inference/pooling => pooling/score}/convert_model_to_seq_cls.py
0	0	examples/{online_serving/pooling => pooling/score}/jinaai_rerank_client.py
0	0	examples/{online_serving/pooling => pooling/score}/openai_cross_encoder_score.py
0	0	examples/{online_serving/pooling => pooling/score}/openai_cross_encoder_score_for_multimodal.py
0	0	examples/{offline_inference/pooling => pooling/score}/qwen3_reranker.py
0	0	examples/{offline_inference/pooling => pooling/token_classify}/ner.py
0	0	examples/{online_serving/pooling => pooling/token_classify}/ner_client.py
0	0	examples/{offline_inference/pooling => pooling/token_embed}/multi_vector_retrieval.py
0	0	examples/{online_serving/pooling => pooling/token_embed}/multi_vector_retrieval_client.py

[51c57b51d] Matthew Bonanni 2025-12-02 [Bugfix] Fix DeepSeek R1 MTP weight loading (#29545)
11	0	vllm/model_executor/models/deepseek_mtp.py

[60c3d413a] ImaGoodFella 2025-12-02 [Multimodal][Core] Optimize multimodal preprocessing cache by hashing image bytes instead of pixel values (#29621)
6	1	tests/conftest.py
6	1	tests/entrypoints/openai/test_vision.py
6	1	tests/entrypoints/pooling/embed/test_online_vision.py
28	0	vllm/multimodal/base.py
19	5	vllm/multimodal/hasher.py
14	10	vllm/multimodal/image.py
15	0	vllm/multimodal/parse.py
1	1	vllm/multimodal/processing.py

[68ffbca7e] Cyrus Leung 2025-12-02 [Chore] Use `tokenizer.encode` and `tokenizer.decode` directly (#29851)
8	6	tests/models/multimodal/processing/test_common.py
1	2	tests/models/multimodal/processing/test_llama4.py
1	1	vllm/entrypoints/openai/speech_to_text.py
2	2	vllm/entrypoints/renderer.py
1	1	vllm/entrypoints/score_utils.py
10	15	vllm/model_executor/models/nano_nemotron_vl.py
1	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
8	11	vllm/multimodal/processing.py
4	0	vllm/transformers_utils/tokenizer.py

[951445a52] Harry Mellor 2025-12-02 Remove default values from `InitVar`s so that they're not stored (#29859)
4	1	benchmarks/benchmark_ngram_proposer.py
10	5	tests/compile/test_fusion_attn.py
17	8	tests/lora/test_worker.py
13	0	tests/test_config.py
2	0	tests/v1/attention/utils.py
9	2	tests/v1/core/test_kv_cache_utils.py
7	6	tests/v1/core/test_scheduler.py
8	7	tests/v1/core/utils.py
3	1	tests/v1/cudagraph/test_cudagraph_dispatch.py
7	6	tests/v1/engine/test_engine_core.py
7	6	tests/v1/kv_connector/unit/utils.py
4	1	tests/v1/spec_decode/test_eagle.py
4	1	tests/v1/spec_decode/test_mtp.py
6	5	tests/v1/tpu/worker/test_tpu_model_runner.py
11	9	tests/v1/worker/test_gpu_model_runner.py
24	18	vllm/config/scheduler.py
3	1	vllm/config/vllm.py

[d8c6210ee] Julien Denize 2025-12-02 Add Mistral Large 3 and Ministral 3 (#29757)
3	2	docs/models/supported_models.md
14	0	tests/models/registry.py
151	7	tests/tokenizers_/test_mistral.py
4	0	vllm/config/speculative.py
1	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
4	0	vllm/model_executor/layers/mla.py
1	1	vllm/model_executor/layers/rotary_embedding/__init__.py
59	7	vllm/model_executor/models/deepseek_v2.py
63	0	vllm/model_executor/models/mistral_large_3.py
165	0	vllm/model_executor/models/mistral_large_3_eagle.py
5	0	vllm/model_executor/models/registry.py
36	0	vllm/tokenizers/mistral.py
6	0	vllm/transformers_utils/configs/eagle.py
62	12	vllm/transformers_utils/configs/mistral.py
4	0	vllm/v1/spec_decode/eagle.py

[8bbcf8b6e] Louie Tsai 2025-12-02 [vLLM Benchmark Suite] Add default parameters section and update CPU benchmark cases (#29381)
59	0	.buildkite/performance-benchmarks/README.md
43	5	.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh
0	610	.buildkite/performance-benchmarks/tests/serving-tests-cpu-snc2.json
0	1023	.buildkite/performance-benchmarks/tests/serving-tests-cpu-snc3.json
230	260	.buildkite/performance-benchmarks/tests/serving-tests-cpu.json
29	0	docs/getting_started/installation/cpu.md

[70fb77b4d] Boyuan Feng 2025-12-02 [BugFix] add max-num-batched-token to scheduler hash (#29829)
12	2	vllm/config/scheduler.py
0	4	vllm/config/vllm.py

[48d15a32a] 杰兮 2025-12-02 [CI] Fix Bad_words test for tokenizer encode/decode asymmetry (#28193)
24	3	tests/v1/sample/test_sampling_params_e2e.py

[3b221cb66] Boyuan Feng 2025-12-01 [BugFix] respect VLLM_LOGGING_LEVEL in logger (#29761)
1	0	tests/conftest.py
2	2	tests/test_config.py
2	2	tests/test_logger.py
4	1	vllm/logger.py

[0037b5746] Wushi Dong 2025-12-01 [Core] Eliminate redundant is_encoder_decoder lookups (20-40us/step) (#29800)
3	9	vllm/v1/worker/gpu_model_runner.py

[f5b0846ba] Harry Mellor 2025-12-02 Fix some Transformers nightly tests (#29802)
1	1	vllm/model_executor/models/jina_vl.py
27	26	vllm/model_executor/models/modernbert.py
1	1	vllm/model_executor/models/qwen2.py

[13ea39bc0] Zhang Xiangze 2025-12-02 [CPU]Parallelize over tokens in int4 moe (#29600)
6	6	csrc/moe/dynamic_4bit_int_moe_cpu.cpp

[4b612664f] Shengqi Chen 2025-12-02 [CI] Renovation of nightly wheel build & generation (take 2) (#29838)
1	15	.buildkite/release-pipeline.yaml
369	0	.buildkite/scripts/generate-nightly-index.py
69	52	.buildkite/scripts/upload-wheels.sh
1	2	.buildkite/test-pipeline.yaml
14	1	docs/getting_started/installation/cpu.md
33	40	docs/getting_started/installation/gpu.cuda.inc.md
1	1	docs/getting_started/installation/gpu.md
115	33	setup.py
3	4	vllm/envs.py

[653591d5e] Cyrus Leung 2025-12-02 [Chore] Move tokenizer initialization methods (#29793)
1	1	benchmarks/benchmark_prefix_caching.py
1	1	benchmarks/benchmark_serving_structured_output.py
1	1	tests/compile/test_dynamic_shapes_compilation.py
1	1	tests/entrypoints/openai/test_chat_template.py
1	1	tests/entrypoints/openai/test_lora_resolvers.py
1	1	tests/entrypoints/openai/test_return_token_ids.py
1	1	tests/entrypoints/openai/test_return_tokens_as_ids.py
1	1	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/entrypoints/openai/test_token_in_token_out.py
1	1	tests/entrypoints/openai/test_tokenization.py
1	1	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
1	1	tests/entrypoints/pooling/embed/test_online.py
1	1	tests/entrypoints/pooling/pooling/test_online.py
1	2	tests/entrypoints/test_chat_utils.py
2	5	tests/models/multimodal/processing/test_common.py
1	1	tests/models/multimodal/processing/test_tensor_schema.py
1	1	tests/models/utils.py
2	2	tests/test_inputs.py
1	2	tests/tokenizers_/test_basic.py
1	2	tests/tokenizers_/test_registry.py
1	1	tests/tool_use/test_deepseekv31_tool_parser.py
1	2	tests/tool_use/test_ernie45_moe_tool_parser.py
1	1	tests/tool_use/test_glm4_moe_tool_parser.py
1	2	tests/tool_use/test_jamba_tool_parser.py
1	1	tests/tool_use/test_kimi_k2_tool_parser.py
1	1	tests/tool_use/test_minimax_tool_parser.py
1	1	tests/tool_use/test_openai_tool_parser.py
1	2	tests/tool_use/test_qwen3coder_tool_parser.py
1	2	tests/tool_use/test_seed_oss_tool_parser.py
1	2	tests/tool_use/test_xlam_tool_parser.py
1	1	tests/transformers_utils/test_config.py
1	1	tests/utils.py
1	1	tests/v1/entrypoints/openai/test_completion.py
1	1	tests/v1/tpu/test_perf.py
1	1	vllm/benchmarks/serve.py
2	2	vllm/model_executor/models/adapters.py
1	1	vllm/model_executor/models/deepseek_ocr.py
1	1	vllm/model_executor/models/deepseek_vl2.py
4	4	vllm/model_executor/models/granite_speech.py
1	1	vllm/model_executor/models/gritlm.py
2	5	vllm/model_executor/models/nano_nemotron_vl.py
1	2	vllm/model_executor/models/pixtral.py
1	2	vllm/model_executor/models/voxtral.py
3	3	vllm/model_executor/models/whisper.py
1	2	vllm/multimodal/registry.py
10	1	vllm/tokenizers/__init__.py
37	1	vllm/tokenizers/registry.py
43	48	vllm/transformers_utils/tokenizer.py
2	3	vllm/v1/engine/async_llm.py
2	3	vllm/v1/engine/llm_engine.py
2	2	vllm/v1/structured_output/__init__.py

[e2fbfc955] Divakar Verma 2025-12-01 [CI][AMD] spec_decode:eagle skip FLASH_ATTN for deepseek on ROCm (#29827)
4	1	tests/v1/e2e/test_spec_decode.py

[a690fb5bd] Divakar Verma 2025-12-01 [CI][ROCm] Fix test_correctness_sliding_window (#29243)
9	1	tests/v1/e2e/test_correctness_sliding_window.py

[81fe3f82a] usberkeley 2025-12-02 [BugFix] Fix index error in ngram_proposer (#29779)
33	1	tests/v1/spec_decode/test_ngram.py
2	2	vllm/v1/spec_decode/ngram_proposer.py

[53bf71b0f] Zuyi Zhao 2025-12-01 [Misc] Update conftest for entrypoints/sagemaker test folder (#29799)
4	1	tests/entrypoints/sagemaker/conftest.py

[f441d36ce] Johnny Yang 2025-12-01 Add missing return in _check_vllm_model_embed_input_ids (#29834)
1	0	vllm/model_executor/models/interfaces_base.py

[22274b218] Seiji Eicher 2025-12-01 [Misc] Add ReplicaId to Ray metrics (#24267)
42	17	vllm/v1/metrics/ray_wrappers.py

[fc95521ba] Wei Wei 2025-12-01 [Misc] Throw error on unintended access to scheduler_config.max_model_len (#29771)
5	0	vllm/config/scheduler.py

[d0cd72890] Zhuohan Li 2025-12-01 [Core] Support reseting all running requests' KV while calling `reset_prefix_cache` (#28827)
98	0	examples/offline_inference/llm_engine_reset_kv.py
66	0	tests/v1/core/test_reset_prefix_cache_e2e.py
31	0	tests/v1/core/test_scheduler.py
1	1	vllm/engine/protocol.py
2	2	vllm/entrypoints/llm.py
4	2	vllm/entrypoints/openai/api_server.py
6	0	vllm/v1/core/sched/async_scheduler.py
7	1	vllm/v1/core/sched/interface.py
64	13	vllm/v1/core/sched/scheduler.py
2	2	vllm/v1/engine/async_llm.py
2	2	vllm/v1/engine/core.py
14	8	vllm/v1/engine/core_client.py
2	2	vllm/v1/engine/llm_engine.py
6	1	vllm/v1/request.py
2	0	vllm/v1/worker/gpu_input_batch.py
8	1	vllm/v1/worker/gpu_model_runner.py

[fa8804ad9] Andrew Xia 2025-12-01 [responsesAPI][4] fix responseOutputItem Kimi K2 thinking bug (#29555)
21	0	tests/entrypoints/test_responses_utils.py
6	1	vllm/entrypoints/responses_utils.py

[4b4092499] Divakar Verma 2025-12-01 [ROCm] Fallback pytorch GELU with tanh approximation to GELU() (#29244)
28	2	vllm/model_executor/layers/activation.py

[c0dfc8948] Hendrik Holtmann 2025-12-02  SM120 / NVFP4: add device guard and runtime SM dispatch to cutlass_scaled_fp4_mm (#29711)
26	13	csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu

[44822d7ff] Nick Hill 2025-12-01 [BugFix] Preserve spec decoding uniform decode when scheduling (#29759)
2	2	tests/v1/e2e/test_spec_decode.py
1	1	vllm/v1/core/sched/async_scheduler.py
22	14	vllm/v1/core/sched/scheduler.py

[342c4f147] Alexei-V-Ivanov-AMD 2025-12-01 Updated CI mirror 2025-11-25 (#29434)
80	26	.buildkite/test-amd.yaml

[1336a1ea2] Kevin H. Luu 2025-12-01 Revert #29787 and #29690 (#29815)
46	0	.buildkite/generate_index.py
15	1	.buildkite/release-pipeline.yaml
0	369	.buildkite/scripts/generate-nightly-index.py
52	69	.buildkite/scripts/upload-wheels.sh
1	14	docs/getting_started/installation/cpu.md
40	33	docs/getting_started/installation/gpu.cuda.inc.md
1	1	docs/getting_started/installation/gpu.md
26	90	setup.py
4	3	vllm/envs.py

[eaf81485e] Nengjun Ma 2025-12-02 [Ascend]: Fixed the issue where OOT Platform vllm-ascend could not enable SP in Eager mode (#28935)
7	0	vllm/config/compilation.py
8	2	vllm/config/vllm.py

[38caf7fa1] Finbarr Timbers 2025-12-01 Update FAQ on interleaving sliding windows support (#29796)
0	2	docs/contributing/model/basic.md

[cabc77cc8] shivampr 2025-12-01 [Core][Observability] Add KV cache residency metrics (#27793)
23	0	docs/design/metrics.md
224	0	tests/v1/core/test_kv_cache_metrics.py
9	1	vllm/config/observability.py
13	0	vllm/engine/arg_utils.py
18	0	vllm/v1/core/block_pool.py
22	9	vllm/v1/core/kv_cache_coordinator.py
4	0	vllm/v1/core/kv_cache_manager.py
96	0	vllm/v1/core/kv_cache_metrics.py
24	3	vllm/v1/core/sched/scheduler.py
90	0	vllm/v1/metrics/loggers.py
11	0	vllm/v1/metrics/stats.py

[ec7035c9d] Kevin H. Luu 2025-12-01 [ci] Make distributed 8 gpus test optional (#29801)
2	1	.buildkite/test-pipeline.yaml

[fc6acc88c] knlnguyen1802 2025-12-02 [Bugfix] Missing cached item in the MultiModalReceiverCache (#28525)
300	4	tests/multimodal/test_cache.py
38	1	vllm/distributed/device_communicators/shm_object_storage.py
76	1	vllm/multimodal/cache.py
2	2	vllm/multimodal/inputs.py
20	13	vllm/multimodal/processing.py

[d0985c5fe] BADAOUI Abdennacer 2025-12-01 [Hardware][AMD] Remove ROCm skip conditions for transformers backend tests (#29782)
0	4	tests/models/test_transformers.py

[092bb73b8] sangbumlikeagod 2025-12-02 [Frontend] add 'verbose_json' and 'timestamp' feature on Whisper Transcription/Translation (#24209)
1	0	docs/serving/openai_compatible_server.md
13	0	tests/entrypoints/openai/test_transcription_validation_whisper.py
4	4	vllm/entrypoints/openai/api_server.py
14	6	vllm/entrypoints/openai/protocol.py
24	4	vllm/entrypoints/openai/serving_transcription.py
163	9	vllm/entrypoints/openai/speech_to_text.py
4	0	vllm/model_executor/models/interfaces.py
1	0	vllm/model_executor/models/whisper.py

[5d43f7372] FredericOdermatt 2025-12-01 [Doc] Update description disable_any_whitespace (#29784)
4	2	vllm/config/structured_outputs.py

[37593deb0] Shengqi Chen 2025-12-01 [CI] fix url-encoding behavior in nightly metadata generation (#29787)
6	5	.buildkite/scripts/generate-nightly-index.py
20	13	setup.py

[f5516039c] Liu Jinyi 2025-12-01 [Doc] fix heading levels (#29783)
2	2	benchmarks/auto_tune/README.md

[36db0a35e] Shengqi Chen 2025-12-01 [CI] Renovation of nightly wheel build & generation (#29690)
0	46	.buildkite/generate_index.py
1	15	.buildkite/release-pipeline.yaml
368	0	.buildkite/scripts/generate-nightly-index.py
69	52	.buildkite/scripts/upload-wheels.sh
14	1	docs/getting_started/installation/cpu.md
33	40	docs/getting_started/installation/gpu.cuda.inc.md
1	1	docs/getting_started/installation/gpu.md
79	22	setup.py
3	4	vllm/envs.py

[5cfa967ef] Marcin Ostrowski 2025-12-01 [Bugfix] TypeError: 'NoneType' object is not callable (#29414)
1	1	tests/v1/core/test_prefix_caching.py

[b95db244e] Isotr0py 2025-12-01 [v1] Add real sliding window calculation to FlexAttention direct BlockMask building (#26015)
11	1	tests/v1/attention/test_attention_backends.py
27	6	vllm/v1/attention/backends/flex_attention.py

[ad9d656bf] Zhengxu Chen 2025-12-01 [multimodal][test] Reduce memory utilization for test_siglip to avoid OOM  (#29504)
7	1	tests/models/multimodal/pooling/test_siglip.py

[f37e8938d] Fanli Lin 2025-12-01 [XPU] Fix AWQ skipped layer detection in IPEX quantization (#29774)
4	1	vllm/model_executor/layers/quantization/ipex_quant.py

[f0a28bf66] Cyrus Leung 2025-12-01 [Misc] Unify tokenizer registration (#29767)
5	5	tests/entrypoints/openai/test_tokenization.py
1	1	tests/entrypoints/pooling/embed/test_online.py
1	5	tests/entrypoints/pooling/pooling/test_online.py
1	1	tests/models/registry.py
21	4	tests/tokenizers_/test_registry.py
2	7	tests/v1/entrypoints/llm/test_struct_output_generate.py
7	15	vllm/config/model.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/llm.py
8	2	vllm/tokenizers/__init__.py
2	0	vllm/tokenizers/hf.py
2	0	vllm/tokenizers/mistral.py
184	15	vllm/tokenizers/registry.py
1	126	vllm/transformers_utils/tokenizer.py

[86e178f7c] Mickaël Seznec 2025-12-01 [crashfix] Eagle + multimodal can crash on mm cache miss (#29750)
11	3	vllm/v1/core/sched/scheduler.py

[014ece97c] daniel-salib 2025-12-01 [Frontend] Add tool filtering support to ToolServer (#29224)
184	0	examples/online_serving/openai_responses_client_with_mcp_tools.py
100	0	tests/entrypoints/openai/test_response_api_mcp_tools.py
96	0	tests/entrypoints/openai/test_serving_responses.py
68	21	vllm/entrypoints/openai/serving_responses.py
29	4	vllm/entrypoints/tool_server.py

[62de4f425] wang.yuqi 2025-12-01 [Frontend] Resettle pooling entrypoints  (#29634)
1	0	.github/CODEOWNERS
1	1	docs/design/io_processor_plugins.md
5	5	docs/serving/openai_compatible_server.md
1	1	tests/entrypoints/openai/test_run_batch.py
2	1	tests/entrypoints/pooling/classify/test_online.py
1	1	tests/entrypoints/pooling/classify/test_online_vision.py
2	4	tests/entrypoints/pooling/embed/test_online.py
1	1	tests/entrypoints/pooling/embed/test_online_dimensions.py
1	1	tests/entrypoints/pooling/embed/test_online_long_text.py
1	1	tests/entrypoints/pooling/embed/test_online_vision.py
1	1	tests/entrypoints/pooling/pooling/test_online.py
2	1	tests/entrypoints/pooling/score/test_online_rerank.py
1	1	tests/entrypoints/pooling/score/test_online_score.py
4	1	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
1	1	tests/plugins_tests/test_io_processor_plugins.py
11	296	vllm/entrypoints/openai/api_server.py
1	709	vllm/entrypoints/openai/protocol.py
92	8	vllm/entrypoints/openai/run_batch.py
22	13	vllm/entrypoints/openai/serving_engine.py
12	0	vllm/entrypoints/openai/utils.py
16	0	vllm/entrypoints/pooling/__init__.py
0	0	vllm/entrypoints/pooling/classify/__init__.py
50	0	vllm/entrypoints/pooling/classify/api_router.py
181	0	vllm/entrypoints/pooling/classify/protocol.py
7	5	vllm/entrypoints/{openai/serving_classification.py => pooling/classify/serving.py}
0	0	vllm/entrypoints/pooling/embed/__init__.py
67	0	vllm/entrypoints/pooling/embed/api_router.py
208	0	vllm/entrypoints/pooling/embed/protocol.py
8	6	vllm/entrypoints/{openai/serving_embedding.py => pooling/embed/serving.py}
0	0	vllm/entrypoints/pooling/pooling/__init__.py
63	0	vllm/entrypoints/pooling/pooling/api_router.py
148	0	vllm/entrypoints/pooling/pooling/protocol.py
5	3	vllm/entrypoints/{openai/serving_pooling.py => pooling/pooling/serving.py}
0	0	vllm/entrypoints/pooling/score/__init__.py
149	0	vllm/entrypoints/pooling/score/api_router.py
145	0	vllm/entrypoints/pooling/score/protocol.py
5	3	vllm/entrypoints/{openai/serving_score.py => pooling/score/serving.py}
48	2	vllm/entrypoints/sagemaker/routes.py
1	1	vllm/plugins/io_processors/interface.py

[83805a607] Huamin Li 2025-11-30 [CI] Skip paddleocr_vl for transformer 4.57.3 (#29758)
6	0	tests/models/multimodal/generation/test_common.py

[1ab8fc819] Yifei Zhang 2025-12-01 Make PyTorch profiler gzip and CUDA time dump configurable (#29568)
2	0	docs/contributing/profiling.md
13	0	vllm/envs.py
14	11	vllm/profiler/gpu_profiler.py
3	1	vllm/v1/engine/async_llm.py
3	1	vllm/v1/worker/xpu_worker.py

[f72a817bd] Shu Wang 2025-11-30 [MoE] CuteDSL MoE with Nvfp4 DeepEP dispatch  (#27141)
7	0	vllm/envs.py
57	28	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
49	19	vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py

[ec38a7368] Woosuk Kwon 2025-11-30 [Model Runner V2] Use packed mask for prompt bin counts (#29756)
4	4	vllm/v1/worker/gpu/sample/metadata.py
21	15	vllm/v1/worker/gpu/sample/penalties.py
10	6	vllm/v1/worker/gpu/states.py

[21c262793] Xingyu Liu 2025-12-01 [Misc]Remove redundant hidden_size property in ModelConfig (#29749)
1	8	vllm/config/model.py
1	1	vllm/model_executor/models/adapters.py

[39d28108f] Omer Ullman Argov 2025-11-30 [Feat] Support non-gated activations in NVFP4 modelopt path (#29004)
17	7	tests/kernels/moe/test_flashinfer_moe.py
10	1	tests/kernels/moe/utils.py
7	1	tests/kernels/utils.py
9	3	vllm/model_executor/layers/fused_moe/layer.py
55	10	vllm/model_executor/layers/quantization/modelopt.py

[cd719de5c] Harry Mellor 2025-11-30 Fix RoPE failures in Transformers nightly (#29700)
2	17	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/qwen3_next.py

[8c363ed66] Pleaplusone 2025-11-30 [ROCm][Attention] Sliding window support for `AiterFlashAttentionBackend` (#29234)
224	49	vllm/v1/attention/backends/rocm_aiter_fa.py

[64bc09ba2] Cyrus Leung 2025-11-30 [Core] Enable `inputs_embeds_size` separate from `hidden_size` (#29741)
6	1	tests/models/multimodal/pooling/test_siglip.py
10	0	vllm/config/model.py
46	5	vllm/model_executor/models/clip.py
46	3	vllm/model_executor/models/siglip.py
4	1	vllm/v1/spec_decode/eagle.py
1	1	vllm/v1/worker/gpu/input_batch.py
2	2	vllm/v1/worker/gpu/model_runner.py
2	1	vllm/v1/worker/gpu/spec_decode/eagle.py
2	2	vllm/v1/worker/gpu_model_runner.py
4	2	vllm/v1/worker/tpu_model_runner.py

[47539cfd3] Isotr0py 2025-11-30 [Bugfix] Fix mismatched nvfp4 gemm output shape (#29742)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py

[2afcec4de] Cyrus Leung 2025-11-30 [Misc] Update `TokenizerLike` interface and move `get_cached_tokenizer` (#29730)
6	7	.buildkite/test-amd.yaml
6	7	.buildkite/test-pipeline.yaml
1	1	docs/design/huggingface_integration.md
1	1	tests/tokenizers_/{test_cached_tokenizer.py => test_hf.py}
3	3	tests/tokenizers_/test_mistral.py
11	5	tests/tokenizers_/test_registry.py
1	1	tools/pre_commit/check_pickle_imports.py
1	1	vllm/entrypoints/llm.py
2	2	vllm/entrypoints/score_utils.py
2	1	vllm/tokenizers/__init__.py
122	0	vllm/tokenizers/hf.py
41	26	vllm/tokenizers/mistral.py
19	13	vllm/tokenizers/protocol.py
43	105	vllm/transformers_utils/tokenizer.py
1	1	vllm/v1/engine/detokenizer.py

[9381b5cde] 朝 2025-11-30 [Doc]: Fix typo in fused_moe layer (#29731)
1	1	vllm/model_executor/layers/fused_moe/layer.py

[66b584028] Vensen 2025-11-30 [Bugfix][sleepmode][fp8 kv cache]: Fix FP8 KV cache + sleep(level=2) gibberish output (#28783)
32	1	tests/basic_correctness/test_cumem.py
7	0	tests/utils.py
45	1	vllm/v1/worker/gpu_model_runner.py
10	0	vllm/v1/worker/gpu_worker.py

[82c795d6f] Huamin Li 2025-11-29 Fix AttributeError about _use_fi_prefill (#29734)
1	1	vllm/v1/attention/backends/mla/common.py

[e1464c3a0] Isotr0py 2025-11-30 [Quantization] Enable compressed-tensors AWQ for Turing GPU (#29732)
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py

[a491b0911] Xin Yang 2025-11-29 [LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 (#29708)
249	0	tests/kernels/moe/test_modular_oai_triton_moe.py
26	9	vllm/lora/layers/fused_moe.py
146	0	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
18	2	vllm/model_executor/layers/quantization/mxfp4.py

[b9d0504a3] Jee Jee Li 2025-11-30 [Bugfix] Revert test_tokenization.py (#29729)
1	1	tests/entrypoints/openai/test_tokenization.py

[1656ad370] Jinzhen Lin 2025-11-29 [Kernel][Quantization] add w4a8 support for marlin kernel (#24722)
78	24	CMakeLists.txt
1	0	benchmarks/kernels/benchmark_machete.py
2	2	benchmarks/kernels/benchmark_marlin.py
2	1	csrc/moe/marlin_moe_wna16/.gitignore
227	79	csrc/moe/marlin_moe_wna16/generate_kernels.py
7	5	csrc/moe/marlin_moe_wna16/kernel.h
892	552	csrc/moe/marlin_moe_wna16/marlin_template.h
228	364	csrc/moe/marlin_moe_wna16/ops.cu
5	3	csrc/moe/torch_bindings.cpp
4	4	csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
3	3	csrc/quantization/gptq_allspark/allspark_utils.cuh
2	1	csrc/quantization/gptq_marlin/.gitignore
70	34	csrc/quantization/gptq_marlin/awq_marlin_repack.cu
87	0	csrc/quantization/gptq_marlin/dequant.h
242	91	csrc/quantization/gptq_marlin/generate_kernels.py
179	294	csrc/quantization/gptq_marlin/gptq_marlin.cu
63	32	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
6	4	csrc/quantization/gptq_marlin/kernel.h
39	0	csrc/quantization/gptq_marlin/marlin.cuh
70	4	csrc/quantization/gptq_marlin/marlin_dtypes.cuh
106	0	csrc/quantization/gptq_marlin/marlin_int4_fp8_preprocess.cu
785	416	csrc/quantization/gptq_marlin/marlin_template.h
12	5	csrc/torch_bindings.cpp
1	1	docs/design/moe_kernel_features.md
197	59	tests/kernels/moe/test_moe.py
286	111	tests/kernels/quantization/test_marlin_gemm.py
18	0	tests/kernels/utils.py
48	10	vllm/_custom_ops.py
5	0	vllm/envs.py
38	9	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
2	2	vllm/model_executor/layers/quantization/auto_round.py
2	2	vllm/model_executor/layers/quantization/awq.py
77	5	vllm/model_executor/layers/quantization/awq_marlin.py
9	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
87	18	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
11	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
19	4	vllm/model_executor/layers/quantization/fp8.py
71	3	vllm/model_executor/layers/quantization/gptq_marlin.py
1	0	vllm/model_executor/layers/quantization/hqq_marlin.py
34	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
15	3	vllm/model_executor/layers/quantization/modelopt.py
10	2	vllm/model_executor/layers/quantization/mxfp4.py
112	13	vllm/model_executor/layers/quantization/utils/marlin_utils.py
88	33	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
34	7	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
88	30	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py

[fa59fe417] Cyrus Leung 2025-11-29 [Chore] Move `detokenizer_utils` to `vllm/tokenizers` (#29727)
1	1	tests/tool_use/test_ernie45_moe_tool_parser.py
1	1	tests/tool_use/test_jamba_tool_parser.py
1	1	tests/tool_use/test_qwen3coder_tool_parser.py
1	1	tests/tool_use/test_seed_oss_tool_parser.py
1	1	tests/tool_use/test_xlam_tool_parser.py
1	1	tests/utils_/test_argparse_utils.py
0	0	vllm/{transformers_utils => tokenizers}/detokenizer_utils.py
1	1	vllm/v1/engine/detokenizer.py
1	1	vllm/v1/engine/logprobs.py

[fe3398fab] Cyrus Leung 2025-11-29 [Chore] Enable passing `tokenizer=None` into MM processor (#29724)
8	33	tests/multimodal/test_processing.py
1	1	vllm/entrypoints/openai/serving_engine.py
2	13	vllm/inputs/preprocess.py
0	6	vllm/model_executor/models/glm4_1v.py
0	3	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/qwen_vl.py
55	31	vllm/multimodal/processing.py
1	3	vllm/multimodal/registry.py

[ad7f714d6] Chukwuma Nwaugha 2025-11-29 hfrunner.classify should return list[list[float]] not list[str] (#29671)
5	2	tests/conftest.py

[f4341f45d] dublc 2025-11-29 [Doc]: fix code block rendering (#29728)
1	1	docs/design/plugin_system.md

[34a984274] Cyrus Leung 2025-11-29 [Misc] Refactor tokenizer interface (#29693)
2	2	.buildkite/test-amd.yaml
2	2	.buildkite/test-pipeline.yaml
1	1	benchmarks/backend_request_func.py
2	3	docs/features/reasoning_outputs.md
1	1	docs/features/tool_calling.md
1	1	tests/entrypoints/openai/test_serving_engine.py
2	2	tests/entrypoints/openai/tool_parsers/conftest.py
6	6	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
2	2	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
5	5	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
5	5	tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py
5	5	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
2	2	tests/entrypoints/openai/tool_parsers/utils.py
1	1	tests/entrypoints/test_chat_utils.py
1	1	tests/models/language/generation/test_mistral.py
1	1	tests/models/multimodal/generation/test_voxtral.py
2	2	tests/models/multimodal/generation/vlm_utils/core.py
2	2	tests/models/multimodal/generation/vlm_utils/types.py
1	1	tests/models/multimodal/processing/test_common.py
10	11	tests/multimodal/test_processing.py
1	1	tests/reasoning/test_mistral_reasoning_parser.py
1	1	tests/reasoning/utils.py
0	0	tests/tokenization/__init__.py
0	18	tests/tokenization/test_do_lower_case.py
0	32	tests/tokenization/test_get_eos.py
0	23	tests/tokenization/test_tokenizer.py
0	120	tests/tokenization/test_tokenizer_registry.py
4	0	tests/tokenizers_/__init__.py
59	0	tests/tokenizers_/test_basic.py
3	2	tests/{tokenization => tokenizers_}/test_cached_tokenizer.py
1	1	tests/{tokenization => tokenizers_}/test_detokenize.py
1	20	tests/{tokenization/test_mistral_tokenizer.py => tokenizers_/test_mistral.py}
36	0	tests/tokenizers_/test_registry.py
3	2	tests/tool_use/test_ernie45_moe_tool_parser.py
5	2	tests/tool_use/test_jamba_tool_parser.py
3	2	tests/tool_use/test_qwen3coder_tool_parser.py
3	2	tests/tool_use/test_seed_oss_tool_parser.py
3	2	tests/tool_use/test_xlam_tool_parser.py
28	58	tests/transformers_utils/test_config.py
0	0	tests/transformers_utils/{test_get_processor_kwargs_from_processor.py => test_processor.py}
62	0	tests/transformers_utils/test_repo_utils.py
2	2	tests/v1/engine/test_output_processor.py
1	1	tools/pre_commit/check_pickle_imports.py
1	0	tools/pre_commit/mypy.py
2	2	vllm/benchmarks/datasets.py
2	2	vllm/engine/protocol.py
6	6	vllm/entrypoints/chat_utils.py
6	9	vllm/entrypoints/llm.py
7	6	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_classification.py
11	5	vllm/entrypoints/openai/serving_completion.py
42	46	vllm/entrypoints/openai/serving_engine.py
10	10	vllm/entrypoints/openai/serving_responses.py
4	4	vllm/entrypoints/openai/serving_score.py
2	2	vllm/entrypoints/openai/serving_tokenization.py
2	2	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/ernie45_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
4	4	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/hunyuan_a13b_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/longcat_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/minimax_m2_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py
5	5	vllm/entrypoints/renderer.py
8	10	vllm/entrypoints/score_utils.py
1	1	vllm/entrypoints/utils.py
5	5	vllm/inputs/preprocess.py
2	2	vllm/logits_process.py
2	2	vllm/model_executor/models/h2ovl.py
3	3	vllm/model_executor/models/internvl.py
5	5	vllm/model_executor/models/nano_nemotron_vl.py
2	2	vllm/model_executor/models/nemotron_vl.py
2	2	vllm/model_executor/models/opencua.py
2	4	vllm/model_executor/models/pixtral.py
2	2	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/model_executor/models/skyworkr1v.py
2	2	vllm/model_executor/models/step3_vl.py
2	4	vllm/model_executor/models/voxtral.py
22	21	vllm/multimodal/processing.py
9	5	vllm/multimodal/registry.py
3	3	vllm/reasoning/abs_reasoning_parsers.py
2	2	vllm/reasoning/basic_parsers.py
2	2	vllm/reasoning/minimax_m2_reasoning_parser.py
1	1	vllm/reasoning/mistral_reasoning_parser.py
2	2	vllm/reasoning/olmo3_reasoning_parser.py
2	2	vllm/sampling_params.py
8	0	vllm/tokenizers/__init__.py
7	22	vllm/{transformers_utils => }/tokenizers/mistral.py
105	0	vllm/tokenizers/protocol.py
28	0	vllm/tokenizers/registry.py
4	3	vllm/transformers_utils/config.py
5	5	vllm/transformers_utils/detokenizer_utils.py
2	1	vllm/transformers_utils/gguf_utils.py
33	27	vllm/transformers_utils/tokenizer.py
23	140	vllm/transformers_utils/tokenizer_base.py
0	16	vllm/transformers_utils/tokenizers/__init__.py
9	7	vllm/v1/engine/async_llm.py
3	3	vllm/v1/engine/detokenizer.py
4	5	vllm/v1/engine/input_processor.py
9	7	vllm/v1/engine/llm_engine.py
3	3	vllm/v1/engine/logprobs.py
6	3	vllm/v1/engine/output_processor.py
3	3	vllm/v1/structured_output/backend_types.py
1	1	vllm/v1/structured_output/backend_xgrammar.py
5	5	vllm/v1/structured_output/utils.py

[f223ed418] Woosuk Kwon 2025-11-29 [Model Runner V2] Fuse penalties and temperature into single kernel (#29720)
3	2	vllm/v1/worker/gpu/sample/gumbel.py
42	25	vllm/v1/worker/gpu/sample/penalties.py
14	18	vllm/v1/worker/gpu/sample/sampler.py

[04a797cd0] Didier Durand 2025-11-29 [Doc]: fixing typos in various files. (#29717)
1	1	vllm/distributed/device_communicators/pynccl_allocator.py
1	1	vllm/distributed/parallel_state.py
1	1	vllm/entrypoints/openai/serving_models.py
1	1	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py
1	1	vllm/transformers_utils/repo_utils.py

[6afc0ffaf] Woosuk Kwon 2025-11-29 [Model Runner V2] Add sample/ directory and reorganize files (#29719)
11	4	vllm/v1/worker/gpu/model_runner.py
0	0	vllm/v1/worker/gpu/sample/__init__.py
100	0	vllm/v1/worker/gpu/sample/gumbel.py
167	0	vllm/v1/worker/gpu/sample/logprob.py
179	0	vllm/v1/worker/gpu/sample/metadata.py
47	1	vllm/v1/worker/gpu/{ => sample}/penalties.py
79	0	vllm/v1/worker/gpu/sample/sampler.py
0	333	vllm/v1/worker/gpu/sampler.py
2	2	vllm/v1/worker/gpu/spec_decode/eagle.py
2	230	vllm/v1/worker/gpu/states.py

[39e63dec7] Jee Jee Li 2025-11-29 [LoRA] Cleanup LoRA unused code (#29611)
2	4	examples/offline_inference/multilora_inference.py
3	3	tests/entrypoints/conftest.py
1	1	tests/entrypoints/openai/test_basic.py
8	0	tests/entrypoints/openai/test_chat.py
1	2	tests/entrypoints/openai/test_chunked_prompt.py
21	22	tests/entrypoints/openai/test_lora_adapters.py
6	6	tests/entrypoints/openai/test_models.py
3	2	tests/entrypoints/openai/test_orca_metrics.py
3	3	tests/entrypoints/openai/test_return_tokens_as_ids.py
1	1	tests/entrypoints/openai/test_tokenization.py
1	1	tests/entrypoints/openai/test_uds.py
0	1	tests/entrypoints/sagemaker/conftest.py
8	19	tests/lora/conftest.py
7	14	tests/lora/test_lora_checkpoints.py
4	7	tests/lora/test_lora_huggingface.py
6	11	tests/lora/test_lora_manager.py
17	5	tests/lora/test_olmoe_tp.py
15	13	tests/lora/test_peft_helper.py
5	5	tests/plugins/lora_resolvers/test_filesystem_resolver.py
13	21	vllm/lora/models.py
1	5	vllm/lora/worker_manager.py
0	1	vllm/model_executor/models/apertus.py
0	1	vllm/model_executor/models/bamba.py
0	1	vllm/model_executor/models/exaone.py
0	1	vllm/model_executor/models/exaone4.py
0	1	vllm/model_executor/models/falcon_h1.py
0	1	vllm/model_executor/models/granite.py
0	1	vllm/model_executor/models/granitemoe.py
0	1	vllm/model_executor/models/granitemoehybrid.py
0	1	vllm/model_executor/models/granitemoeshared.py
0	3	vllm/model_executor/models/interfaces.py
0	1	vllm/model_executor/models/jamba.py
0	1	vllm/model_executor/models/lfm2.py
0	1	vllm/model_executor/models/lfm2_moe.py
0	1	vllm/model_executor/models/llama.py
0	1	vllm/model_executor/models/minicpm.py
0	1	vllm/model_executor/models/minicpm_eagle.py
0	1	vllm/model_executor/models/minicpmv.py
0	1	vllm/model_executor/models/mixtral.py
0	1	vllm/model_executor/models/nemotron.py
0	1	vllm/model_executor/models/nemotron_h.py
0	1	vllm/model_executor/models/nemotron_nas.py
0	1	vllm/model_executor/models/phimoe.py
0	1	vllm/model_executor/models/solar.py
0	1	vllm/model_executor/models/transformers/base.py
0	1	vllm/v1/worker/lora_model_runner_mixin.py

[4a80ad0a2] Woosuk Kwon 2025-11-28 [Model Runner V2] Don't use UVA buffer for prefill_len  (#29713)
2	0	vllm/v1/worker/gpu/model_runner.py
4	1	vllm/v1/worker/gpu/states.py

[4b17ce681] Angela Yi 2025-11-28 Add gpu memory wait before test_async_tp (#28893)
3	3	.buildkite/test-pipeline.yaml
29	0	tests/conftest.py

[e23f665d8] Lucas Wilkinson 2025-11-28 [BugFix] Fix DBO failing with TypeError: 'NoneType' object is not iterable (#29698)
0	1	tests/v1/distributed/test_dbo.py
1	3	vllm/v1/attention/backends/utils.py
6	3	vllm/v1/worker/dp_utils.py

[ca1b1e729] Woosuk Kwon 2025-11-28 [Model Runner V2] Refactor prefill token preparation (#29712)
1	1	vllm/v1/worker/gpu/cudagraph_utils.py
52	34	vllm/v1/worker/gpu/input_batch.py
17	29	vllm/v1/worker/gpu/model_runner.py
8	11	vllm/v1/worker/gpu/spec_decode/eagle.py
5	3	vllm/v1/worker/gpu/states.py

[762a4a6ca] Tsukasa OI 2025-11-29 [Frontend] Perform offline path replacement to `tokenizer` (#29706)
10	0	tests/entrypoints/offline_mode/test_offline_mode.py
17	6	vllm/engine/arg_utils.py

[b2c50eda5] Cyrus Leung 2025-11-29 [Bugfix] Fix wrong mock attribute (#29704)
1	1	tests/entrypoints/openai/test_serving_chat.py

[1dcafb3de] Woosuk Kwon 2025-11-28 [Model Runner V2] Support penalties using bin counts (#29703)
15	0	vllm/v1/worker/gpu/input_batch.py
5	4	vllm/v1/worker/gpu/model_runner.py
85	0	vllm/v1/worker/gpu/penalties.py
3	0	vllm/v1/worker/gpu/sampler.py
172	10	vllm/v1/worker/gpu/states.py

[ea3370b42] Andreas Karatzas 2025-11-28 [ROCm][Bugfix] Patch for the `Multi-Modal Processor Test` group (#29702)
18	1	docker/Dockerfile.rocm
22	4	docker/Dockerfile.rocm_base
56	22	requirements/rocm-test.txt
8	1	tests/models/multimodal/processing/test_tensor_schema.py

[c625d7b1c] Mert Unsal 2025-11-28 [Bugfix] Fix O(n²) multimodal string prompt processing (#29667)
36	0	tests/multimodal/test_processing.py
29	33	vllm/multimodal/processing.py

[6173682b6] Zhengxu Chen 2025-11-28 [compile] Include `enable_sleep_mode` into caching factors. (#29696)
0	1	vllm/config/model.py

[9726e6453] Augusto Yao 2025-11-29 bugfix: correct attn output with base 2 or e (#28840)
32	10	vllm/attention/ops/common.py
9	2	vllm/v1/attention/backends/flashinfer.py
6	1	vllm/v1/attention/backends/mla/common.py

[3fd1fb0b6] Huamin Li 2025-11-28 Revert "[LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 (#28971)" (#29697)
0	250	tests/kernels/moe/test_modular_oai_triton_moe.py
9	26	vllm/lora/layers/fused_moe.py
0	146	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
2	18	vllm/model_executor/layers/quantization/mxfp4.py

[a51f4186f] Jiangyun Zhu 2025-11-29 [Bugfix] fix dots.llm1.inst (#29687)
6	5	vllm/model_executor/models/dots1.py

[7675ba30d] Cyrus Leung 2025-11-29 [Misc] Remove redundant `ClassRegistry` (#29681)
1	1	tests/models/multimodal/processing/test_common.py
1	1	tests/models/multimodal/processing/test_tensor_schema.py
7	0	vllm/model_executor/models/interfaces.py
10	14	vllm/multimodal/registry.py
6	33	vllm/utils/collection_utils.py

[7c1ed4584] Ralf Gommers 2025-11-29 [CI/Build]: make it possible to build with a free-threaded interpreter (#29241)
7	1	cmake/utils.cmake
6	1	setup.py

[1986de137] Benjamin Chislett 2025-11-28 [Perf] Optimize EAGLE prepare_inputs_padded with triton kernels (#28597)
11	19	tests/v1/spec_decode/test_eagle.py
49	58	vllm/v1/spec_decode/eagle.py
105	0	vllm/v1/spec_decode/utils.py
33	30	vllm/v1/worker/gpu_model_runner.py

[3461e7efd] Yanan Cao 2025-11-28 [Frontend] Remap -O to -cc commandline flag (#29557)
1	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh
11	11	docs/design/debug_vllm_compile.md
1	1	docs/design/torch_compile.md
3	3	tests/compile/fullgraph/test_basic_correctness.py
6	6	tests/engine/test_arg_utils.py
36	14	tests/utils_/test_argparse_utils.py
2	2	vllm/config/vllm.py
1	1	vllm/engine/arg_utils.py
11	0	vllm/utils/argparse_utils.py

[fecae12cd] Harry Mellor 2025-11-28 Remove `all_special_tokens_extended` from tokenizer code (#29686)
0	1	tests/tokenization/test_cached_tokenizer.py
40	46	tests/tokenization/test_mistral_tokenizer.py
0	4	tests/tokenization/test_tokenizer_registry.py
0	5	vllm/transformers_utils/tokenizer.py
0	5	vllm/transformers_utils/tokenizer_base.py
0	4	vllm/transformers_utils/tokenizers/mistral.py

[8d9338fae] Cyrus Leung 2025-11-29 [Chore] Rename `Processor` to `InputProcessor` (#29682)
1	1	tests/entrypoints/openai/test_lora_resolvers.py
7	7	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/entrypoints/openai/test_serving_engine.py
1	1	tests/entrypoints/openai/test_serving_models.py
2	2	tests/entrypoints/openai/test_serving_responses.py
18	17	tests/v1/engine/{test_processor_multi_modal_uuids.py => test_process_multi_modal_uuids.py}
2	2	vllm/engine/protocol.py
3	3	vllm/entrypoints/llm.py
5	5	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_models.py
17	12	vllm/v1/engine/async_llm.py
637	0	vllm/v1/engine/input_processor.py
16	12	vllm/v1/engine/llm_engine.py
12	629	vllm/v1/engine/processor.py

[d40c85400] Isotr0py 2025-11-29 [CI/Build] Rework CPU multimodal processor test (#29684)
12	2	.buildkite/test-pipeline.yaml

[433295560] Harry Mellor 2025-11-28 [Docs] Add CLI reference doc for `vllm bench sweep plot_pareto` (#29689)
9	0	docs/cli/bench/sweep/plot_pareto.md

[f946a8d74] Isotr0py 2025-11-29 [Chore]: Reorganize model repo operating functions in `transformers_utils` (#29680)
2	2	tests/transformers_utils/test_config.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/model_executor/model_loader/default_loader.py
1	1	vllm/model_executor/models/adapters.py
9	270	vllm/transformers_utils/config.py
1	1	vllm/transformers_utils/gguf_utils.py
287	0	vllm/transformers_utils/repo_utils.py
2	4	vllm/transformers_utils/tokenizer.py

[6f9d81d03] Isotr0py 2025-11-29 [V0 deprecation] Clean up legacy paged attention helper functions (#28043)
0	211	vllm/attention/ops/paged_attn.py
0	123	vllm/attention/ops/rocm_aiter_paged_attn.py

[fae694306] Didier Durand 2025-11-28 [Doc]: fixing typos in multiple files. (#29685)
1	1	vllm/compilation/sequence_parallelism.py
1	1	vllm/model_executor/models/nano_nemotron_vl.py
1	1	vllm/model_executor/models/stablelm.py
1	1	vllm/utils/gc_utils.py
1	1	vllm/v1/spec_decode/ngram_proposer.py
1	1	vllm/v1/worker/ec_connector_model_runner_mixin.py

[3bcbb30cb] 果冻虾仁 2025-11-29 add add_truncate_prompt_tokens in repr for PoolingParams (#29683)
1	0	vllm/pooling_params.py

[9e6bcda3a] Cyrus Leung 2025-11-29 [mypy] Enable type checking for more directories (#29674)
6	6	tools/pre_commit/mypy.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py
6	6	vllm/distributed/kv_transfer/kv_connector/v1/metrics.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
4	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
3	1	vllm/engine/arg_utils.py
2	2	vllm/transformers_utils/config.py
2	1	vllm/triton_utils/__init__.py
4	4	vllm/v1/metrics/loggers.py
1	1	vllm/v1/sample/logits_processor/__init__.py
3	2	vllm/v1/spec_decode/metrics.py

[9eec282cb] Harry Mellor 2025-11-28 Guard FlashInfer sampler using the same check as FlashInfer attention backend (#29415)
10	0	vllm/v1/sample/ops/topk_topp_sampler.py

[0808eb813] Cyrus Leung 2025-11-28 [Misc] Remove `yapf` directives (#29675)
0	3	vllm/distributed/ec_transfer/ec_connector/factory.py
52	40	vllm/entrypoints/openai/serving_tokens.py

[460d8bbf2] Mingyuan Ma 2025-11-28 Remove upstream fa checks (#29471)
7	50	vllm/attention/layer.py
2	8	vllm/attention/ops/vit_attn_wrappers.py
8	0	vllm/attention/utils/fa_utils.py
0	8	vllm/model_executor/models/dots_ocr.py
0	9	vllm/model_executor/models/ernie45_vl.py
1	11	vllm/model_executor/models/glm4_1v.py
0	1	vllm/model_executor/models/keye.py
0	15	vllm/model_executor/models/paddleocr_vl.py
0	18	vllm/model_executor/models/qwen2_5_vl.py
0	8	vllm/model_executor/models/qwen2_vl.py
0	6	vllm/model_executor/models/qwen3_omni_moe_thinker.py
0	12	vllm/model_executor/models/qwen3_vl.py
0	2	vllm/model_executor/models/siglip2navit.py

[e2f56c309] Li, Jiang 2025-11-28 [CPU] Update torch 2.9.1 for CPU backend (#29664)
2	2	.buildkite/scripts/hardware_ci/run-cpu-test.sh
7	6	csrc/cpu/utils.cpp
1	1	docker/Dockerfile.cpu
2	3	requirements/cpu-build.txt
4	11	requirements/cpu.txt
0	1	vllm/platforms/cpu.py

[f8151b66f] HappyAmazonian 2025-11-28 Revert "Supress verbose logs from model_hosting_container_standards (… (#29335)
1	1	requirements/common.txt
0	4	vllm/entrypoints/openai/api_server.py

[1168768a2] Cyrus Leung 2025-11-28 [Optimization] Early return for `_apply_matches` and `_iter_placeholders` (#29668)
31	2	vllm/multimodal/processing.py

[8e7a89160] Nick Hill 2025-11-28 [BugFix] Fix spec decoding max_tokens scheduling perf issue (#29542)
11	13	tests/v1/test_outputs.py
9	5	vllm/v1/core/sched/scheduler.py
8	20	vllm/v1/outputs.py

[953d9c820] Cyrus Leung 2025-11-28 [mypy] Pass type checking for `vllm/utils` and `vllm/v1/pool` (#29666)
2	1	tools/pre_commit/mypy.py
8	1	vllm/utils/async_utils.py
8	15	vllm/utils/jsontree.py
2	2	vllm/utils/mem_utils.py
3	3	vllm/utils/nccl.py
1	1	vllm/utils/network_utils.py
4	2	vllm/utils/registry.py
6	15	vllm/utils/torch_utils.py
3	3	vllm/v1/pool/metadata.py

[33b06a6f2] Cyrus Leung 2025-11-28 [Misc] Remove redundant attention var constants (#29650)
9	10	tests/kernels/attention/test_attention_selector.py
5	6	tests/kernels/attention/test_rocm_attention_selector.py
0	20	tests/kernels/utils.py
1	2	tests/models/quantization/test_fp8.py
4	5	vllm/attention/selector.py
0	3	vllm/model_executor/models/deepseek_eagle.py
0	17	vllm/utils/__init__.py

[5c2b5cb42] Wilson Wu 2025-11-28 [Docs] Add SPLADE and Ultravox models to supported models documentation (#29659)
2	0	docs/models/supported_models.md

[3cb32e5d6] 杰兮 2025-11-28 [Rocm] Set VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS default is disabled (#28985)
3	3	vllm/envs.py

[ccbdf51bd] Cyrus Leung 2025-11-28 [Doc] Reorganize benchmark docs (#29658)
5	0	docs/.nav.yml
7	0	docs/benchmarking/README.md
43	292	docs/{contributing/benchmarks.md => benchmarking/cli.md}
58	0	docs/benchmarking/dashboard.md
178	0	docs/benchmarking/sweeps.md

[5f5521bd5] Filipp Fisin 2025-11-28 Fix parameter order in GPT-OSS weight loading function for non-MXFP4 weights  (#29506)
1	1	vllm/model_executor/models/gpt_oss.py

[b2c1d294f] Julien Denize 2025-11-28 [BUGFIX] MistralTokenizer._call__ adds an invalid EOS token (#29607)
68	0	tests/tokenization/test_mistral_tokenizer.py
19	1	vllm/transformers_utils/tokenizers/mistral.py

[cc0f2a0e1] maang-h 2025-11-28 [Doc] Improve abnormal information string (#29655)
5	9	vllm/v1/engine/utils.py

[480598958] rongfu.leng 2025-11-28 [Feature][Bench] Add pareto visualization (#29477)
18	0	docs/contributing/benchmarks.md
4	0	docs/mkdocs/hooks/generate_argparse.py
3	0	vllm/benchmarks/sweep/cli.py
393	0	vllm/benchmarks/sweep/plot_pareto.py

[b34e8775a] Cyrus Leung 2025-11-28 Revert "[CPU]Update CPU PyTorch to 2.9.0 (#29589)" (#29647)
4	0	docker/Dockerfile.cpu
2	2	requirements/cpu-build.txt
4	4	requirements/cpu.txt
2	2	vllm/model_executor/models/qwen3_vl.py

[f4b76056e] wang.yuqi 2025-11-28 Improve enable chunked_prefill & prefix_caching logic. (#26623)
1	3	tests/models/language/pooling/test_auto_prefix_cache_support.py
239	1	tests/test_config.py
109	0	vllm/config/model.py
4	2	vllm/config/pooler.py
19	57	vllm/config/vllm.py
31	59	vllm/engine/arg_utils.py
2	2	vllm/model_executor/models/bert.py
32	3	vllm/model_executor/models/interfaces_base.py
2	1	vllm/model_executor/models/modernbert.py
13	2	vllm/model_executor/models/registry.py
4	3	vllm/v1/engine/core.py

[37b15e97e] EanWang211123 2025-11-28 [Multimodal][Speculative Decoding]Eagle3 mm support, enablement on qwen3vl (#29594)
4	0	tests/models/registry.py
14	0	tests/v1/e2e/test_spec_decode.py
22	1	vllm/model_executor/models/qwen3_vl.py
1	0	vllm/model_executor/models/registry.py
4	4	vllm/v1/spec_decode/eagle.py

[c7ba1f6bc] maang-h 2025-11-28 [BugFix] Fix ValueError in NewRequestData repr methods (#29392)
36	0	tests/v1/core/test_output.py
6	2	vllm/v1/core/sched/output.py

[18523b87f] Wilson Wu 2025-11-28 [Docs] Update supported models for Olmo 3 in tool calling documentation (#29411)
2	1	docs/features/tool_calling.md

[745a3bae1] Xin Yang 2025-11-27 [LoRA] Support FusedMoE LoRA Triton kernel for mxfp4 (#28971)
250	0	tests/kernels/moe/test_modular_oai_triton_moe.py
26	9	vllm/lora/layers/fused_moe.py
146	0	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
18	2	vllm/model_executor/layers/quantization/mxfp4.py

[35657bcd7] scydas 2025-11-28 [CPU]Update CPU PyTorch to 2.9.0 (#29589)
0	4	docker/Dockerfile.cpu
2	2	requirements/cpu-build.txt
4	4	requirements/cpu.txt

[be493e0b3] Lucas Wilkinson 2025-11-27 [BugFix] Fix new nightly failures (#29578)
26	0	vllm/v1/attention/backends/utils.py
11	1	vllm/v1/worker/gpu_model_runner.py

[ae0ce1be2] Woosuk Kwon 2025-11-27 [Model Runner V2][BugFix] Keep reference to GPU tensors in AsyncOutput (#29623)
7	2	vllm/v1/worker/gpu/async_utils.py

[a5345bf49] Andrii Skliar 2025-11-27 [BugFix] Fix `plan` API Mismatch when using latest FlashInfer (#29426)
2	2	docker/Dockerfile
1	1	requirements/cuda.txt
2	1	vllm/v1/attention/backends/flashinfer.py

[e5a621b72] Nicolò Lucchesi 2025-11-27 [CI] Add batched audios Whisper test (#29308)
1	196	tests/entrypoints/openai/test_transcription_validation.py
237	0	tests/entrypoints/openai/test_transcription_validation_whisper.py

[38658ec6f] Isotr0py 2025-11-28 [Bugfix][MM encoder] Fix ViT attention backend resolving for Turing GPU (#29614)
9	8	vllm/platforms/cuda.py

[a24ea5414] Cyrus Leung 2025-11-28 [Deprecation] Advance deprecation status (#29617)
1	14	vllm/config/scheduler.py
0	19	vllm/distributed/parallel_state.py
0	49	vllm/model_executor/models/utils.py
2	2	vllm/v1/core/sched/output.py

[ea228b449] Cyrus Leung 2025-11-28 [Misc] Remove unused code from `protocol.py` (#29616)
0	9	vllm/engine/protocol.py

[d45269b37] 果冻虾仁 2025-11-28 add skip_reading_prefix_cache in repr for PoolingParams (#29620)
1	0	vllm/pooling_params.py

[ee9841daa] Cyrus Leung 2025-11-28 [Bugfix] Fix doc build on main (#29619)
1	2	vllm/model_executor/models/interfaces_base.py

[0840abdd2] Injae Ryou 2025-11-28 [BugFix] Optional tokenizer argument when loading GGUF models (#29582)
8	7	vllm/config/model.py
42	0	vllm/transformers_utils/gguf_utils.py
9	1	vllm/transformers_utils/tokenizer.py

[e1f262337] Harry Mellor 2025-11-27 Update Transformers pin in CI to 4.57.3 (#29418)
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
1	1	requirements/test.txt

[fc1d8be3d] Matthew Bonanni 2025-11-27 [Attention] Update attention imports (#29540)
3	6	tests/v1/attention/test_rocm_attention_backends_selection.py
3	3	tests/v1/kv_connector/unit/test_backwards_compatibility.py
4	7	vllm/attention/backends/abstract.py
1	2	vllm/attention/layers/chunked_local_attention.py
1	2	vllm/config/model.py
2	9	vllm/config/multimodal.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/base.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
2	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
3	5	vllm/forward_context.py
2	5	vllm/model_executor/layers/attention_layer_base.py
2	5	vllm/model_executor/layers/mamba/abstract.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	3	vllm/model_executor/layers/quantization/fp8.py
1	2	vllm/model_executor/layers/quantization/modelopt.py
1	2	vllm/model_executor/layers/quantization/mxfp4.py
1	2	vllm/model_executor/layers/quantization/petit.py
1	2	vllm/model_executor/layers/quantization/ptpc_fp8.py
1	2	vllm/model_executor/layers/quantization/quark/quark.py
1	4	vllm/platforms/cpu.py
2	8	vllm/platforms/cuda.py
1	4	vllm/platforms/interface.py
1	5	vllm/platforms/rocm.py
1	4	vllm/platforms/tpu.py
1	6	vllm/platforms/xpu.py
0	2	vllm/v1/attention/backends/cpu_attn.py
0	2	vllm/v1/attention/backends/flash_attn.py
0	2	vllm/v1/attention/backends/flex_attention.py
5	2	vllm/v1/attention/backends/utils.py
2	2	vllm/v1/kv_offload/spec.py
1	2	vllm/v1/spec_decode/eagle.py
2	5	vllm/v1/worker/utils.py

[cd007a53b] Mathis Felardos 2025-11-27 [bugfix] avoid NIXL_ERR_REMOTE_DISCONNECT in nixl_connector when Prefill dies (#28120)
41	21	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[66d3d5422] Didier Durand 2025-11-27 [Doc]: fixing typos in diverse files (#29492)
2	2	vllm/benchmarks/serve.py
2	2	vllm/config/parallel.py
1	1	vllm/lora/punica_wrapper/punica_base.py
2	2	vllm/model_executor/models/adapters.py
1	1	vllm/v1/sample/tpu/sampler.py
4	2	vllm/v1/worker/dp_utils.py

[bab438ff3] Ryan Rock 2025-11-27 [CI/Build] Skip ray tests on ROCm (#29556)
5	0	tests/v1/distributed/test_async_llm_dp.py

[882851dc8] Li, Jiang 2025-11-27 [CI/Build][Bugfix] Fix auto label issues for CPU (#29610)
1	1	.github/workflows/issue_autolabel.yml

[2f5f9acd5] Jee Jee Li 2025-11-27 [LoRA] Continue optimizing MoE LoRA weight loading (#29322)
8	7	tests/lora/test_lora_checkpoints.py
4	4	tests/lora/test_lora_huggingface.py
1	1	vllm/lora/layers/base.py
8	8	vllm/lora/layers/column_parallel_linear.py
115	103	vllm/lora/layers/fused_moe.py
1	1	vllm/lora/layers/logits_processor.py
1	1	vllm/lora/layers/replicated_linear.py
2	2	vllm/lora/layers/row_parallel_linear.py
1	1	vllm/lora/layers/vocal_parallel_embedding.py
53	0	vllm/lora/lora_weights.py
27	23	vllm/lora/models.py
8	9	vllm/lora/utils.py
5	5	vllm/lora/worker_manager.py
1	0	vllm/model_executor/models/interfaces.py
1	0	vllm/model_executor/models/qwen3_vl_moe.py

[cf348c8d2] Roger Wang 2025-11-27 [Bugfix] Fix HunyuanVL XD-RoPE (#29593)
1	1	vllm/model_executor/models/hunyuan_vision.py
3	3	vllm/transformers_utils/processors/hunyuan_vl_image.py

[a5abd1d38] Li, Jiang 2025-11-27 [CI] Auto label CPU related issues (#29602)
25	0	.github/workflows/issue_autolabel.yml

[e6d4f3c25] Cyrus Leung 2025-11-27 [Bugfix] Fix pre-commit (#29601)
2	3	tests/v1/ec_connector/integration/test_epd_correctness.py
7	4	vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py

[51906c8c5] maang-h 2025-11-27 [Docs] Improve `priority` parameter documentation (#29572)
3	0	vllm/entrypoints/llm.py

[0838b52e2] Morrison Turnansky 2025-11-27 [Frontend][torch.compile] CompilationConfig Overhaul (#20283): Set up -O infrastructure (#26847)
69	0	docs/design/optimization_levels.md
2	2	tests/compile/test_config.py
41	16	tests/engine/test_arg_utils.py
7	1	tests/model_executor/test_enabled_custom_ops.py
306	1	tests/test_config.py
6	4	tests/utils_/test_argparse_utils.py
2	2	tests/v1/cudagraph/test_cudagraph_mode.py
62	13	vllm/config/compilation.py
8	0	vllm/config/model.py
205	18	vllm/config/vllm.py
7	1	vllm/engine/arg_utils.py
19	5	vllm/utils/argparse_utils.py
1	1	vllm/v1/worker/gpu/cudagraph_utils.py

[00d3310d2] Cyrus Leung 2025-11-27 [Bugfix] Update Ultravox  compatibility (#29588)
5	0	vllm/model_executor/models/ultravox.py

[da3222f37] Woosuk Kwon 2025-11-27 [Model Runner V2] Implement multi-step Eagle with CUDA graph (#29559)
5	4	vllm/v1/worker/gpu/cudagraph_utils.py
19	34	vllm/v1/worker/gpu/model_runner.py
390	32	vllm/v1/worker/gpu/spec_decode/eagle.py
112	0	vllm/v1/worker/gpu/spec_decode/eagle_cudagraph.py

[43c579259] Micah Williamson 2025-11-27 [ROCm][CI] Fix test_cpu_offloading for ROCm (#29548)
2	0	tests/v1/kv_offload/test_cpu_offloading.py

[3ecabd06e] Johnny Yang 2025-11-26 Fix tpu-inference platform path (#29554)
1	1	vllm/platforms/tpu.py

[c069086b9] Jee Jee Li 2025-11-27 [Bugfix] Fix getting device for MoE LoRA (#29475)
3	1	vllm/lora/layers/fused_moe.py
9	0	vllm/lora/layers/utils.py

[11ea5ec1f] Woosuk Kwon 2025-11-26 [Model Runner V2] Refactor CudaGraphManager (#29583)
154	95	vllm/v1/worker/gpu/cudagraph_utils.py

[ecb195237] Fadi Arafeh 2025-11-27 [cpu][fix] Fix Arm CI tests (#29552)
12	14	.buildkite/scripts/hardware_ci/run-cpu-test-arm.sh

[da8e1a1bf] TJian 2025-11-27 [DOC] Add vLLM Bangkok Meetup info (#29561)
1	0	README.md
1	0	docs/community/meetups.md

[ee80aee1c] Woosuk Kwon 2025-11-26 [Model Runner V2] Minor cleanup for build_attn_metadata (#29576)
3	5	vllm/v1/worker/gpu/attn_utils.py
2	1	vllm/v1/worker/gpu/cudagraph_utils.py
8	2	vllm/v1/worker/gpu/model_runner.py

[0aeb698b7] Woosuk Kwon 2025-11-26 [Model Runner V2] Minor code cleanup (#29570)
2	9	vllm/v1/worker/gpu/cudagraph_utils.py
9	0	vllm/v1/worker/gpu/dp_utils.py
7	9	vllm/v1/worker/gpu/model_runner.py

[9bb33c891] Louie Tsai 2025-11-26 add xpu supported model and model id for cpu (#29380)
17	9	docs/models/hardware_supported_models/cpu.md
65	0	docs/models/hardware_supported_models/xpu.md

[a67dec7cb] Jinzhen Lin 2025-11-27 [Bugfix] fix IMA issue in certain cases of the moe marlin kernel (#28619)
10	8	csrc/moe/marlin_moe_wna16/marlin_template.h
0	1	vllm/model_executor/layers/fused_moe/shared_fused_moe.py

[77740191d] Matthew Bonanni 2025-11-26 [Attention][Async] Eliminate `seq_lens_cpu` in FlashAttention metadata building with DCP > 1 (#29449)
14	13	vllm/v1/attention/backends/flash_attn.py
4	2	vllm/v1/attention/backends/utils.py

[df01eda4d] HDCharles 2025-11-26 [Bugfix] Make compressed-tensors MoEs respect ignored layers (#28878)
1	0	.buildkite/test-pipeline.yaml
48	0	tests/quantization/test_compressed_tensors.py
4	0	vllm/model_executor/layers/fused_moe/__init__.py
53	19	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
27	33	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[ba1fcd84a] Johnny Yang 2025-11-26 [TPU] add tpu_inference (#27277)
1	3	requirements/tpu.txt
0	8	vllm/distributed/device_communicators/tpu_communicator.py
3	1	vllm/platforms/tpu.py
1	1	vllm/v1/worker/tpu_worker.py

[56539cdda] Lucas Wilkinson 2025-11-26 [Core] Refactor padding logic and pad for CUDA graphs before attention metadata building  (#28579)
5	3	docs/design/cuda_graphs.md
31	12	tests/v1/cudagraph/test_cudagraph_dispatch.py
11	7	vllm/forward_context.py
1	20	vllm/v1/attention/backends/flashinfer.py
2	0	vllm/v1/attention/backends/mamba_attn.py
4	1	vllm/v1/attention/backends/utils.py
66	31	vllm/v1/cudagraph_dispatcher.py
16	1	vllm/v1/worker/dp_utils.py
230	202	vllm/v1/worker/gpu_model_runner.py
35	6	vllm/v1/worker/gpu_worker.py

[430dd4d9e] Matthew Bonanni 2025-11-26 [Attention] Remove imports from `vllm/attention/__init__.py` (#29342)
1	1	docs/contributing/model/basic.md
2	1	tests/compile/test_fusion_attn.py
2	1	tests/compile/test_qk_norm_rope_fusion.py
1	1	tests/kernels/utils.py
1	1	tests/v1/worker/test_gpu_model_runner.py
2	2	tests/v1/worker/test_utils.py
0	19	vllm/attention/__init__.py
1	1	vllm/attention/backends/abstract.py
5	2	vllm/attention/layer.py
1	1	vllm/compilation/fusion_attn.py
1	1	vllm/compilation/qk_norm_rope_fusion.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
2	1	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
1	1	vllm/model_executor/layers/mamba/linear_attn.py
1	2	vllm/model_executor/model_loader/utils.py
2	1	vllm/model_executor/models/afmoe.py
2	1	vllm/model_executor/models/apertus.py
1	1	vllm/model_executor/models/arctic.py
1	1	vllm/model_executor/models/baichuan.py
1	1	vllm/model_executor/models/bailing_moe.py
1	1	vllm/model_executor/models/bloom.py
1	1	vllm/model_executor/models/chameleon.py
1	1	vllm/model_executor/models/chatglm.py
1	2	vllm/model_executor/models/clip.py
1	1	vllm/model_executor/models/commandr.py
1	1	vllm/model_executor/models/dbrx.py
1	1	vllm/model_executor/models/deepseek_v2.py
1	1	vllm/model_executor/models/dots1.py
1	1	vllm/model_executor/models/ernie45_moe.py
1	1	vllm/model_executor/models/ernie45_vl_moe.py
1	1	vllm/model_executor/models/exaone.py
1	1	vllm/model_executor/models/exaone4.py
1	1	vllm/model_executor/models/falcon.py
1	1	vllm/model_executor/models/gemma.py
1	1	vllm/model_executor/models/gemma2.py
2	1	vllm/model_executor/models/gemma3.py
1	1	vllm/model_executor/models/gemma3n.py
2	1	vllm/model_executor/models/glm4.py
1	1	vllm/model_executor/models/glm4_moe.py
1	1	vllm/model_executor/models/gpt2.py
1	1	vllm/model_executor/models/gpt_bigcode.py
1	1	vllm/model_executor/models/gpt_j.py
1	1	vllm/model_executor/models/gpt_neox.py
2	1	vllm/model_executor/models/gpt_oss.py
1	1	vllm/model_executor/models/granite.py
1	1	vllm/model_executor/models/granitemoe.py
1	1	vllm/model_executor/models/grok1.py
2	1	vllm/model_executor/models/hunyuan_v1.py
1	1	vllm/model_executor/models/internlm2.py
1	1	vllm/model_executor/models/jais.py
1	1	vllm/model_executor/models/lfm2.py
1	1	vllm/model_executor/models/lfm2_moe.py
2	1	vllm/model_executor/models/llama.py
1	1	vllm/model_executor/models/llama4.py
1	1	vllm/model_executor/models/minicpm.py
1	1	vllm/model_executor/models/minicpm3.py
1	1	vllm/model_executor/models/minimax_m2.py
2	1	vllm/model_executor/models/minimax_text_01.py
1	1	vllm/model_executor/models/mixtral.py
1	2	vllm/model_executor/models/molmo.py
1	1	vllm/model_executor/models/mpt.py
1	1	vllm/model_executor/models/nemotron.py
1	1	vllm/model_executor/models/nemotron_nas.py
1	1	vllm/model_executor/models/olmo.py
1	1	vllm/model_executor/models/olmo2.py
1	1	vllm/model_executor/models/olmoe.py
2	1	vllm/model_executor/models/openpangu.py
1	1	vllm/model_executor/models/opt.py
1	1	vllm/model_executor/models/orion.py
2	1	vllm/model_executor/models/ouro.py
1	1	vllm/model_executor/models/persimmon.py
1	1	vllm/model_executor/models/phi.py
1	1	vllm/model_executor/models/phimoe.py
1	1	vllm/model_executor/models/qwen.py
2	1	vllm/model_executor/models/qwen2.py
1	1	vllm/model_executor/models/qwen2_moe.py
2	1	vllm/model_executor/models/qwen3.py
1	1	vllm/model_executor/models/qwen3_moe.py
2	1	vllm/model_executor/models/qwen3_next.py
2	1	vllm/model_executor/models/seed_oss.py
1	1	vllm/model_executor/models/solar.py
1	1	vllm/model_executor/models/stablelm.py
1	1	vllm/model_executor/models/starcoder2.py
1	1	vllm/model_executor/models/step3_text.py
2	1	vllm/model_executor/models/transformers/base.py
2	2	vllm/model_executor/models/whisper.py
1	1	vllm/platforms/cuda.py
1	1	vllm/v1/attention/backends/cpu_attn.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/kv_offload/cpu.py
1	1	vllm/v1/kv_offload/spec.py
1	1	vllm/v1/kv_offload/worker/cpu_gpu.py
2	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/kv_connector_model_runner_mixin.py
1	2	vllm/v1/worker/tpu_model_runner.py

[c4c0354ee] Alec 2025-11-26 [CI/Build] allow user modify pplx and deepep ref by ENV or command line (#29131)
7	1	docker/Dockerfile
56	10	tools/ep_kernels/install_python_libraries.sh

[e60312950] HDCharles 2025-11-26 [refactor] CTConfig methods to static/class methods (#28870)
32	23	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[0b0aa874e] Wentao Ye 2025-11-26 [Perf] Optimize batch invariant BMM, 18.1% Throughput improvement, 10.7% TTFT improvement (#29345)
0	3	tests/v1/determinism/test_batch_invariance.py
4	1	tests/v1/determinism/utils.py
213	12	vllm/model_executor/layers/batch_invariant.py

[70d5953f8] Huamin Li 2025-11-26 Revert "[Bugfix] Fix GPT-OSS AR+NORM fusion (#28841)" (#29483)
0	1	.buildkite/test-pipeline.yaml
0	11	tests/compile/distributed/test_fusions_e2e.py
1	1	vllm/distributed/device_communicators/symm_mem.py
6	11	vllm/model_executor/layers/fused_moe/layer.py

[3650a74ed] yxt 2025-11-26 Optimize the wording of the document and unify the terminology and th… (#29491)
23	23	docs/models/pooling_models.md

[bb706d604] Yejing Lai 2025-11-26 Fix TeleChatForCausalLM not register issue (#29473)
3	0	tests/models/registry.py
2	0	vllm/model_executor/models/registry.py

[e30859dff] Cyrus Leung 2025-11-26 [Bugfix] Fix handling of image embeds in models (#29480)
2	13	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/llava_next.py
1	1	vllm/model_executor/models/llava_onevision.py

[452a7c9f7] Roger Wang 2025-11-26 [Misc] Allow LM only loading for Pixtral (#29451)
51	22	vllm/model_executor/models/pixtral.py

[d9d342d21] Pleaplusone 2025-11-26 [Performance][MLA][ROCm] Remove redundant D2D copy in deepseek (#27457)
12	15	csrc/attention/merge_attn_states.cu
1	2	csrc/ops.h
1	2	csrc/torch_bindings.cpp
17	6	vllm/attention/ops/triton_merge_attn_states.py
18	16	vllm/v1/attention/backends/mla/common.py

[53d7f1f60] Xin Yang 2025-11-25 [Kernel] Use pre-allocated output buffer for triton kernel fused_experts (#29219)
73	11	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py

[c5ee43032] dependabot[bot] 2025-11-26 Bump actions/checkout from 4 to 6 (#29293)
1	1	.github/workflows/cleanup_pr_body.yml
1	1	.github/workflows/macos-smoke-test.yml
1	1	.github/workflows/pre-commit.yml

[8d6a89dff] Michael Goin 2025-11-25 [UX] Suppress gloo log spam (#29250)
3	1	vllm/distributed/parallel_state.py
28	26	vllm/distributed/utils.py
33	0	vllm/utils/system_utils.py

[56531b79c] George D. Torres 2025-11-25 [Misc] Add backup hash algorithm for FIPS constrained environments (#28795)
2	2	vllm/compilation/caching.py
7	7	vllm/compilation/compiler_interface.py
2	2	vllm/config/device.py
2	2	vllm/config/kv_transfer.py
2	2	vllm/config/load.py
2	2	vllm/config/lora.py
2	2	vllm/config/multimodal.py
2	2	vllm/config/observability.py
2	2	vllm/config/pooler.py
2	2	vllm/config/scheduler.py
2	2	vllm/config/speculative.py
2	2	vllm/config/structured_outputs.py
5	5	vllm/config/vllm.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	2	vllm/model_executor/models/registry.py
18	0	vllm/utils/hashing.py

[12866af74] Xieyang Xu 2025-11-25 dummy run corner case (#29433)
1	1	vllm/v1/worker/gpu_model_runner.py

[d8819c88e] Lucia Fang 2025-11-25 fix assertion for single world use case (uni) (#29429)
3	2	vllm/config/parallel.py

[de75b0bb7] Andrey Khalyavin 2025-11-26 [BugFix] Fix initialization of draft model.  (#29319)
4	0	vllm/v1/worker/gpu_model_runner.py

[7df028978] Michael Goin 2025-11-25 Change warning logs to debug for unimplemented MXFP4 Linear/Attention (#29441)
6	4	vllm/model_executor/layers/quantization/mxfp4.py

[0abc79482] Zhengxu Chen 2025-11-25 [caching] Add enable_prompt_embeds and cpu_offload_gb to compile hashes. (#29435)
1	3	vllm/config/cache.py
0	1	vllm/config/model.py

[4e57c6587] Nick Hill 2025-11-25 [Core] Support logprobs with spec decode + async scheduling  (#29223)
6	1	tests/v1/e2e/test_async_scheduling.py
0	2	vllm/v1/core/sched/scheduler.py
12	2	vllm/v1/sample/rejection_sampler.py
17	20	vllm/v1/worker/gpu_model_runner.py

[e7d776273] Ilya Markov 2025-11-25 [Compile] Refactor. Move PostGradPassManager out of Compilation config (#29340)
20	13	vllm/compilation/backends.py
1	1	vllm/compilation/piecewise_backend.py

[c32a18cbe] Eldar Kurtić 2025-11-25 Attempt to fix GPU OOM in a spec-decoding test (#29419)
1	1	examples/offline_inference/spec_decode.py

[b07555d26] Andrew Xia 2025-11-25 [responsesAPI][2] parse ResponseFunctionToolCallOutputItem (#29383)
15	0	tests/entrypoints/test_responses_utils.py
1	4	vllm/entrypoints/openai/protocol.py
9	0	vllm/entrypoints/responses_utils.py

[0353d2e16] Harry Mellor 2025-11-25 Fix RoPE related failures in Transformers nightly tests (#29333)
1	1	vllm/model_executor/models/baichuan.py
1	1	vllm/model_executor/models/gpt_j.py
1	1	vllm/model_executor/models/grok1.py
1	1	vllm/model_executor/models/llama.py
33	29	vllm/transformers_utils/config.py

[a1f267687] Harry Mellor 2025-11-25 Scheduled removal of `override_pooler_config` and `disable_log_requests` (#29402)
0	16	vllm/config/model.py
1	28	vllm/engine/arg_utils.py
0	5	vllm/entrypoints/llm.py
0	8	vllm/utils/argparse_utils.py
0	8	vllm/v1/engine/async_llm.py

[48ddb02b7] Yifan Qiao 2025-11-25 [Hybrid Allocator] Support KV cache groups with different block_size (#29143)
45	4	tests/v1/core/test_kv_cache_utils.py
93	2	tests/v1/core/test_prefix_caching.py
16	6	tests/v1/core/test_single_type_kv_cache_manager.py
5	3	vllm/engine/arg_utils.py
8	3	vllm/model_executor/models/config.py
21	1	vllm/v1/core/block_pool.py
79	19	vllm/v1/core/kv_cache_coordinator.py
5	20	vllm/v1/core/kv_cache_manager.py
136	20	vllm/v1/core/kv_cache_utils.py
1	0	vllm/v1/core/sched/scheduler.py
63	9	vllm/v1/core/single_type_kv_cache_manager.py

[e50209864] Michael Goin 2025-11-25 [Kernel] Add NVFP4 MoE CUTLASS support for SM120 (#29242)
4	1	CMakeLists.txt
224	6	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu
1	1	csrc/quantization/fp4/nvfp4_experts_quant.cu
6	4	csrc/quantization/fp4/nvfp4_quant_entry.cu
15	12	csrc/quantization/w8a8/cutlass/scaled_mm_entry.cu
1	1	docs/design/moe_kernel_features.md
9	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
4	0	vllm/model_executor/layers/quantization/modelopt.py

[dbc3d9991] Michael Goin 2025-11-25 [UX] Put CUDA attention backend selection log into one line (#29337)
2	4	vllm/platforms/cuda.py

[794029f01] Injae Ryou 2025-11-25 [Feature]: Improve GGUF loading from HuggingFace user experience like repo_id:quant_type (#29137)
240	0	tests/models/test_gguf_download.py
146	0	tests/transformers_utils/test_utils.py
12	3	vllm/config/model.py
3	3	vllm/engine/arg_utils.py
22	9	vllm/model_executor/model_loader/gguf_loader.py
46	0	vllm/model_executor/model_loader/weight_utils.py
40	11	vllm/transformers_utils/config.py
5	5	vllm/transformers_utils/processor.py
12	5	vllm/transformers_utils/tokenizer.py
53	0	vllm/transformers_utils/utils.py

[0231ce836] Eldar Kurtić 2025-11-25 Revert back to torch.equal over torch.allclose from #28819  (#29086)
7	4	vllm/v1/spec_decode/eagle.py

[516c3f784] Thomas Parnell 2025-11-25 [Bugfix] Fix logic for choosing default prefix caching setting (#29393)
2	1	tests/engine/test_arg_utils.py
5	1	vllm/engine/arg_utils.py

[51fc9e017] Harry Mellor 2025-11-25 Scheduled removal of `CompilationConfig.use_inductor` (#29323)
6	6	tests/compile/fullgraph/test_simple.py
4	5	tests/engine/test_arg_utils.py
2	2	tests/utils_/test_argparse_utils.py
1	28	vllm/config/compilation.py

[bf0c75cd4] Harry Mellor 2025-11-25 Make Transformers Nightly tests soft-fail and enable all tests (#29401)
3	2	.buildkite/test-pipeline.yaml

[c2c661af9] Roger Wang 2025-11-25 [Bugfix] Fix overallocation in MM profiling  (#29386)
8	4	vllm/v1/worker/gpu_model_runner.py

[798e87db5] Nicolò Lucchesi 2025-11-25 [Core] Generalize Encoder-Decoder `seq_lens` computation to avoid Whisper hardcoded logic   (#29268)
22	22	vllm/attention/layers/cross_attention.py
2	1	vllm/v1/attention/backends/utils.py
27	11	vllm/v1/worker/gpu_model_runner.py

[de6889946] wang.yuqi 2025-11-25 [Misc] Suppress log outputs when constructing the default vllm config. (#29291)
8	6	vllm/engine/arg_utils.py
10	1	vllm/logger.py

[7a80b0188] wang.yuqi 2025-11-25 [CI] Resettle pooling entrypoints tests.  (#29370)
0	0	tests/entrypoints/pooling/{correctness => basic}/__init__.py
0	0	tests/entrypoints/pooling/{llm => basic}/test_encode.py
0	0	tests/entrypoints/pooling/{openai => basic}/test_truncation.py
0	0	tests/entrypoints/pooling/{llm => classify}/__init__.py
0	0	tests/entrypoints/pooling/{llm/test_classify.py => classify/test_offline.py}
0	0	tests/entrypoints/pooling/{openai/test_classification.py => classify/test_online.py}
0	0	tests/entrypoints/pooling/{openai/test_vision_classification.py => classify/test_online_vision.py}
0	0	tests/entrypoints/pooling/{openai => embed}/__init__.py
0	0	tests/entrypoints/pooling/{correctness/test_mteb_embed.py => embed/test_correctness_mteb.py}
0	0	tests/entrypoints/pooling/{llm/test_embedding.py => embed/test_offline.py}
0	0	tests/entrypoints/pooling/{openai/test_embedding.py => embed/test_online.py}
0	0	tests/entrypoints/pooling/{openai/test_embedding_dimensions.py => embed/test_online_dimensions.py}
0	0	tests/entrypoints/pooling/{openai/test_embedding_long_text.py => embed/test_online_long_text.py}
0	0	tests/entrypoints/pooling/{openai/test_vision_embedding.py => embed/test_online_vision.py}
0	0	tests/entrypoints/pooling/pooling/__init__.py
0	0	tests/entrypoints/pooling/{openai/test_pooling.py => pooling/test_online.py}
0	0	tests/entrypoints/pooling/reward/__init__.py
0	0	tests/entrypoints/pooling/{llm/test_reward.py => reward/test_offline.py}
0	0	tests/entrypoints/pooling/score/__init__.py
0	0	tests/entrypoints/pooling/{correctness/test_mteb_score.py => score/test_correctness_mteb.py}
0	0	tests/entrypoints/pooling/{llm/test_score.py => score/test_offline.py}
0	0	tests/entrypoints/pooling/{openai/test_rerank.py => score/test_online_rerank.py}
0	0	tests/entrypoints/pooling/{openai/test_score.py => score/test_online_score.py}

[e1dd706cd] Ben Browning 2025-11-25 [Frontend] Respect Chat Completion parallel_tool_calls param (#26233)
2	1	docs/serving/openai_compatible_server.md
57	0	tests/tool_use/test_parallel_tool_calls.py
2	2	vllm/entrypoints/openai/protocol.py
3	0	vllm/entrypoints/openai/serving_chat.py
1	5	vllm/entrypoints/openai/serving_engine.py
37	0	vllm/entrypoints/openai/utils.py

[a685b47c5] Andrew Xia 2025-11-25 [responsesAPI] refactor construct_input_messages (#29359)
7	43	vllm/entrypoints/openai/serving_responses.py
45	1	vllm/entrypoints/responses_utils.py

[32c40b95e] Avishek Goswami 2025-11-25 [BugFix] bad_words filtering ineffective when n > 1 (#29313)
8	0	vllm/v1/engine/__init__.py
5	4	vllm/v1/engine/async_llm.py
5	2	vllm/v1/engine/llm_engine.py

[db2906108] Nick Hill 2025-11-25 [Misc] Streamline unique id generation (#29375)
8	8	vllm/entrypoints/openai/protocol.py
5	4	vllm/entrypoints/openai/serving_engine.py
3	1	vllm/utils/__init__.py

[67fc16cd8] wang.yuqi 2025-11-25 [Bugfix] If chunked_prefill is disabled, end the scheduling early. (#28911)
28	0	tests/v1/core/test_scheduler.py
2	1	tests/v1/core/utils.py
3	3	vllm/v1/core/sched/scheduler.py

[6330f9477] elvischenv 2025-11-25 [Bugfix] Fix GPT-OSS AR+NORM fusion (#28841)
1	0	.buildkite/test-pipeline.yaml
11	0	tests/compile/distributed/test_fusions_e2e.py
1	1	vllm/distributed/device_communicators/symm_mem.py
11	6	vllm/model_executor/layers/fused_moe/layer.py

[ef1f7030f] Micah Williamson 2025-11-25 [ROCm][CI] Fix test_cudagraph_mode failure in AMD CI (#29367)
7	0	tests/v1/attention/utils.py
42	20	tests/v1/cudagraph/test_cudagraph_mode.py
2	2	vllm/platforms/rocm.py

[12c007e28] Rémi Delacourt 2025-11-25 EAGLE Support DP>1 (#26086)
2	0	.buildkite/test-pipeline.yaml
77	0	tests/v1/distributed/test_eagle_dp.py
93	30	vllm/v1/spec_decode/eagle.py
4	1	vllm/v1/worker/gpu_model_runner.py

[f242cfcdd] zhrrr 2025-11-25 [Perf] use cpu all reduce to avoid sync when async_scheduling & dp > 1 (#29311)
6	0	vllm/engine/arg_utils.py

[888152bf8] Icey 2025-11-25 Allow oot custom compiler extension via CompilerInterface (#28623)
17	17	vllm/compilation/backends.py
5	7	vllm/config/compilation.py
20	0	vllm/platforms/interface.py

[fe3a4f5b3] Ryan Rock 2025-11-25 [CI/Build] Pin torchgeo dependency for AMD (#29353)
2	1	requirements/rocm-test.txt

[98caeadd5] Fadi Arafeh 2025-11-25 [fix][cpu] Use a SwigluOAI impl which supports interleaved gate-up wei (#29273)
8	21	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py

[64deead71] vllmellm 2025-11-25 [Bugfix] [ROCm] [UX]: revert Flex attention backend (#29371)
6	0	tests/v1/attention/test_rocm_attention_backends_selection.py
4	0	vllm/platforms/rocm.py

[7992324f2] Nick Hill 2025-11-24 [BugFix] Use unique ids for different transcription prompts (#29372)
2	2	vllm/entrypoints/openai/speech_to_text.py

[40a6f53f6] Inoki 2025-11-25 Display warning only when ROCm version is less than Pytorch required version (#29200)
1	1	CMakeLists.txt

[ce58fdc1c] kflu 2025-11-24 Fix PoolingParams.skip_reading_prefix_cache type (#29364)
1	1	vllm/pooling_params.py
1	1	vllm/sampling_params.py

[a21256c46] Fanli Lin 2025-11-25 Add TP CLI argument to multimodal inference examples (#29301)
15	0	examples/offline_inference/audio_language.py
15	0	examples/offline_inference/vision_language.py
35	5	examples/offline_inference/vision_language_multi_image.py

[316c8492b] Harry Mellor 2025-11-25 Scheduled removal of `guided_*` config fields (#29326)
1	1	docs/features/structured_outputs.md
4	25	tests/v1/entrypoints/llm/test_struct_output_generate.py
0	33	vllm/engine/arg_utils.py
38	165	vllm/entrypoints/openai/protocol.py
0	38	vllm/sampling_params.py

[2d9ee28ca] Lucas Wilkinson 2025-11-24 [CI/Test Fix] Fix CP tests on Blackwell (#29338)
0	1	vllm/attention/ops/common.py

[81db702ed] Jiangyun Zhu 2025-11-25 [Attention] add `_cudagraph_support` for linear attention (#28934)
3	0	vllm/v1/attention/backends/linear_attn.py

[92effb07a] Isotr0py 2025-11-25 [Model] Add HunyuanOCR support (#29327)
1	0	docs/models/supported_models.md
26	0	examples/offline_inference/vision_language.py
4	0	tests/models/registry.py
5	0	vllm/config/model.py
13	0	vllm/model_executor/layers/rotary_embedding/__init__.py
102	0	vllm/model_executor/layers/rotary_embedding/xdrope.py
10	1	vllm/model_executor/models/hunyuan_v1.py
1028	0	vllm/model_executor/models/hunyuan_vision.py
50	1	vllm/model_executor/models/interfaces.py
4	0	vllm/model_executor/models/registry.py
18	0	vllm/transformers_utils/config.py
8	0	vllm/transformers_utils/configs/__init__.py
322	0	vllm/transformers_utils/configs/hunyuan_vl.py
9	1	vllm/transformers_utils/processors/__init__.py
233	0	vllm/transformers_utils/processors/hunyuan_vl.py
477	0	vllm/transformers_utils/processors/hunyuan_vl_image.py
2	0	vllm/v1/worker/gpu_input_batch.py
103	1	vllm/v1/worker/gpu_model_runner.py

[87185c88d] Maryam Tahhan 2025-11-25 [Bugfix] Make deprecated `--task embedding` consistent with `--runner… (#29312)
20	10	vllm/config/model.py

[9cf4edae6] Mark McLoughlin 2025-11-25 [Metrics] Scheduled removal of deprecated metrics (#29330)
0	3	tests/entrypoints/openai/test_metrics.py
37	97	vllm/v1/metrics/loggers.py

[7012d8b45] 汪志鹏 2025-11-25 [Docker] Optimize Dockerfile: consolidate apt-get and reduce image size by ~200MB (#29060)
30	9	docker/Dockerfile

[22b42b540] Divakar Verma 2025-11-24 [CI][ROCm] Install arctic-inference on ROCm tests (#29344)
3	0	requirements/rocm-test.txt

[cb7214d8e] gbyu-amd 2025-11-25 [ROCm][MLA] enable fp8 MLA decode on ROCm (#28032)
10	0	vllm/_aiter_ops.py
11	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[77e10c9ca] Pleaplusone 2025-11-25 [Perf][Deepseek] optimize gather_and_maybe_dequant_cache kernel's perf for extremely long sequence (#28029)
6	5	csrc/cache.h
86	92	csrc/cache_kernels.cu
2	1	csrc/torch_bindings.cpp
9	3	tests/kernels/attention/test_cache.py
4	2	vllm/_custom_ops.py
25	3	vllm/v1/attention/backends/mla/common.py

[6f1355a1b] Michael Goin 2025-11-24 [Perf] Disable DeepGEMM MoE by default when TP=8 is used (#29346)
20	4	vllm/model_executor/layers/quantization/fp8.py

[a4ad43ad5] Harry Mellor 2025-11-25 Scheduled removal of `ParallelConfig`'s direct child EPLB fields (#29324)
0	50	vllm/config/parallel.py
0	24	vllm/engine/arg_utils.py

[a178a0b40] Nick Hill 2025-11-24 [BugFix] Fix duplicate id tool-call race condition (#29355)
9	4	vllm/entrypoints/openai/serving_chat.py
7	3	vllm/entrypoints/openai/serving_engine.py

[b8328b49f] Kunshang Ji 2025-11-25 [XPU] upgrade torch & ipex 2.9 on XPU platform (#29307)
9	4	docker/Dockerfile.xpu
3	3	requirements/xpu.txt

[5f9679a43] Hanjie Qiu 2025-11-24 [Spec Decode] Add support for EAGLE3 heads that do not use_aux_hidden_states (#27688)
24	14	vllm/model_executor/models/llama_eagle3.py
19	0	vllm/v1/spec_decode/eagle.py
3	1	vllm/v1/worker/gpu_model_runner.py

[699bca76c] Wentao Ye 2025-11-24 [UX] Raise error for attn backend of batch invariant (#29348)
7	7	vllm/model_executor/layers/batch_invariant.py

[c17610e2b] Michael Goin 2025-11-24 [Bugfix] Only use triton_kernels for MXFP4 on SM90 and SM100 (#29339)
9	6	vllm/model_executor/layers/quantization/mxfp4.py

[71df2a57e] Chen Zhang 2025-11-24 [Hybrid Allocator] Better layer padding strategy for gpt-oss eagle (#29303)
59	0	tests/v1/core/test_kv_cache_utils.py
10	1	vllm/v1/core/kv_cache_utils.py

[4dd42db56] Tyler Michael Smith 2025-11-24 Remove VLLM_SKIP_WARMUP tip (#29331)
0	3	docs/features/quantization/inc.md

[84371daf7] Nick Hill 2025-11-24 [Tests] Verify gpt_oss package is installed in harmony tests (#29336)
5	1	tests/entrypoints/openai/test_response_api_with_harmony.py

[f32c7d6f5] Woosuk Kwon 2025-11-24 [Model Runner V2] Simplify Eagle bookkeeping with num_rejected (#29347)
6	13	vllm/v1/worker/gpu/input_batch.py
22	8	vllm/v1/worker/gpu/model_runner.py
10	9	vllm/v1/worker/gpu/spec_decode/eagle.py
12	0	vllm/v1/worker/gpu/spec_decode/rejection_sample.py

[3cfa63ad9] Yan Ma 2025-11-25 [XPU]fix Kimi-VL-A3B-thinking on xpu (#29309)
14	6	vllm/model_executor/models/moonvit.py

[4d6afcadd] Benjamin Bartels 2025-11-24 [CI/Build] Moves to cuda-base runtime image while retaining minimal JIT dependencies (#29270)
14	2	docker/Dockerfile
-	-	docs/assets/contributing/dockerfile-stages-dependency.png

[97588c4d1] Woosuk Kwon 2025-11-24 [Model Runner V2] Add minor clarification comments for Eagle (#29332)
11	0	vllm/v1/worker/gpu/spec_decode/eagle.py

[839c6b7b7] Chenheli Hua 2025-11-24 [Multimodal][Qwen3 Omni] Make Qwen3 Omni work with audio-in-video inputs in V1 engine.   (#27721)
170	0	examples/offline_inference/qwen3_omni/only_thinker.py
221	0	tests/model_executor/test_qwen3_omni.py
0	25	vllm/model_executor/models/qwen2_5_omni_thinker.py
76	34	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[8f066146c] bnellnm 2025-11-24 [MoE][Refactor] Make select_experts a non-static method (#29067)
6	13	tests/kernels/moe/test_flashinfer.py
30	5	tests/test_routing_simulator.py
5	1	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
6	35	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
58	60	vllm/model_executor/layers/fused_moe/layer.py
5	27	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
2	15	vllm/model_executor/layers/quantization/awq_marlin.py
3	17	vllm/model_executor/layers/quantization/bitsandbytes.py
15	108	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	17	vllm/model_executor/layers/quantization/experts_int8.py
4	25	vllm/model_executor/layers/quantization/fp8.py
2	15	vllm/model_executor/layers/quantization/gguf.py
2	17	vllm/model_executor/layers/quantization/gptq_marlin.py
12	33	vllm/model_executor/layers/quantization/modelopt.py
2	15	vllm/model_executor/layers/quantization/moe_wna16.py
3	20	vllm/model_executor/layers/quantization/mxfp4.py
4	34	vllm/model_executor/layers/quantization/quark/quark_moe.py
2	15	vllm/model_executor/layers/quantization/rtn.py

[cec418b5d] Woosuk Kwon 2025-11-24 [Model Runner V2] Change Numba AoT to JIT (#29328)
16	55	vllm/v1/worker/gpu/input_batch.py
16	8	vllm/v1/worker/gpu/model_runner.py

[cc313cb73] Woosuk Kwon 2025-11-24 [Model Runner V2] Implement Single-step Eagle 1 (#29300)
3	0	vllm/v1/worker/gpu/input_batch.py
79	0	vllm/v1/worker/gpu/model_runner.py
3	2	vllm/v1/worker/gpu/sampler.py
18	0	vllm/v1/worker/gpu/spec_decode/__init__.py
197	0	vllm/v1/worker/gpu/spec_decode/eagle.py

[26a465584] Nicolò Lucchesi 2025-11-24 [NIXL] Use config to enable telemetry + NIXL version bump (#29305)
1	1	requirements/kv_connectors.txt
3	6	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[e924bbb4f] Varun Sundar Rabindranath 2025-11-24 [Build/CI][DP/EP] Add QWen/Qwen3-30B-A3B-FP8 + EPLB tests to Nightly H100 and B200 (#29195)
7	3	.buildkite/scripts/scheduled_integration_test/{qwen30b_a3b_fp8_block_ep.sh => qwen30b_a3b_fp8_block_ep_eplb.sh}
1	1	.buildkite/test-amd.yaml
11	2	.buildkite/test-pipeline.yaml

[656516c31] Aydin Abiar 2025-11-24 [Bugfix] properly handle nested json with llama3 tool parser (#27701)
128	0	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
73	39	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py

[e48b2e684] vllmellm 2025-11-24 [Bugfix] [ROCm] [UX] Reorganize ROCm Backend Selection Logic (#26980)
337	0	tests/v1/attention/test_rocm_attention_backends_selection.py
57	23	vllm/platforms/rocm.py

[7a228b530] Laith Sakka 2025-11-24 Add option to use unbacked, and backed size obl dynamic shapes for more sounds compilation. (#26199)
70	0	docs/design/debug_vllm_compile.md
104	1	docs/design/torch_compile.md
88	0	tests/compile/test_dynamic_shapes_compilation.py
53	10	vllm/compilation/decorators.py
23	1	vllm/compilation/wrapper.py
59	1	vllm/config/compilation.py
11	1	vllm/model_executor/models/llama.py
34	1	vllm/model_executor/models/qwen2.py

[f716a1537] Yuan Tang 2025-11-24 Update KServe guide link in documentation (#29258)
1	1	docs/deployment/integrations/kserve.md

[2601f18a8] WeiQing Chen 2025-11-24 [EPLB] Optimize EPLB for Async Rearrange Experts  (#22179)
126	1	tests/distributed/test_eplb_execute.py
38	1	tests/distributed/test_eplb_spec_decode.py
4	0	vllm/config/parallel.py
115	0	vllm/distributed/eplb/async_worker.py
374	57	vllm/distributed/eplb/eplb_state.py
119	18	vllm/distributed/eplb/rebalance_execute.py
2	0	vllm/v1/worker/gpu_model_runner.py

[4de87866a] R3hankhan 2025-11-24 [CPU][IBM Z] Fix BF16 support and vectorize math operations for s390x (#28926)
1	1	csrc/cpu/cpu_attn_impl.hpp
530	56	csrc/cpu/cpu_types_vxe.hpp

[eca7a8fb5] Didier Durand 2025-11-24 [Doc]: fix typos in various files (#29230)
1	1	benchmarks/kernels/deepgemm/README.md
1	1	vllm/config/vllm.py
1	1	vllm/forward_context.py
1	1	vllm/v1/worker/gpu_input_batch.py

[8005e606b] 杰兮 2025-11-24 [Bugfix][Rocm] Fix shared expert weight loading failure in DeepSeek-MTP (#27563)
117	42	vllm/model_executor/models/deepseek_mtp.py
7	7	vllm/model_executor/models/deepseek_v2.py

[68dfe28ea] rongfu.leng 2025-11-24 [Feature][Benchmark] add --link-vars can filter when serve_param equal bench_param (#28909)
33	1	vllm/benchmarks/sweep/serve.py

[ed40d8592] Fanli Lin 2025-11-24 [BugFix] Fix R-VL model loading error (#29299)
1	0	examples/offline_inference/vision_language_multi_image.py

[0ff70821c] Roger Wang 2025-11-23 [Core] Deprecate `xformers` (#29262)
1	34	docker/Dockerfile.nightly_torch
0	15	docs/contributing/ci/update_pytorch_version.md
1	1	docs/getting_started/quickstart.md
0	1	examples/online_serving/openai_embedding_long_text/service.sh
0	1	requirements/cuda.txt
0	3	tests/basic_correctness/test_basic_correctness.py
0	129	tests/kernels/attention/test_attention.py
1	7	tests/kernels/attention/test_attention_selector.py
0	4	tests/kernels/attention/test_mha_attn.py
11	67	tests/kernels/utils.py
0	12	tests/lora/test_minicpmv_tp.py
0	15	tests/lora/test_qwen2vl.py
0	1	vllm/attention/backends/registry.py
0	38	vllm/attention/layer.py
1	37	vllm/attention/ops/vit_attn_wrappers.py
8	1	vllm/attention/selector.py
6	0	vllm/config/multimodal.py
0	1	vllm/envs.py
4	23	vllm/model_executor/models/dots_ocr.py
5	28	vllm/model_executor/models/ernie45_vl.py
6	25	vllm/model_executor/models/glm4_1v.py
17	13	vllm/model_executor/models/keye.py
0	13	vllm/model_executor/models/paddleocr_vl.py
1	0	vllm/model_executor/models/pixtral.py
4	21	vllm/model_executor/models/qwen2_5_vl.py
4	27	vllm/model_executor/models/qwen2_vl.py
3	9	vllm/model_executor/models/qwen3_omni_moe_thinker.py
3	10	vllm/model_executor/models/qwen3_vl.py
1	6	vllm/platforms/cuda.py
0	1	vllm/utils/__init__.py
0	420	vllm/v1/attention/backends/xformers.py

[5253f4276] tongqiu 2025-11-24 [ROCm] Support for Whisper v1 with Aiter Unified Attention and Aiter Flash Attention (#28376)
14	8	vllm/v1/attention/backends/rocm_aiter_fa.py
12	2	vllm/v1/attention/backends/rocm_aiter_unified_attn.py
2	5	vllm/v1/attention/backends/rocm_attn.py

[30854783a] Zero 2025-11-24 [Model] Add OpenCUA-7B support (#29068)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
271	0	vllm/model_executor/models/opencua.py
4	0	vllm/model_executor/models/registry.py

[1073ba68b] Jee Jee Li 2025-11-24 [LoRA] Optimize 3D MoE logic (#29222)
6	1	tests/lora/test_gptoss_tp.py
2	1	vllm/lora/layers/__init__.py
2	2	vllm/lora/layers/base.py
4	2	vllm/lora/layers/base_linear.py
2	2	vllm/lora/layers/column_parallel_linear.py
286	63	vllm/lora/layers/fused_moe.py
4	2	vllm/lora/layers/logits_processor.py
5	2	vllm/lora/layers/vocal_parallel_embedding.py
75	24	vllm/lora/models.py
8	4	vllm/lora/utils.py
1	0	vllm/model_executor/models/gpt_oss.py

[c309bb524] Josh Moore 2025-11-23 [Bugfix] Update Gradio OpenAI Chatbot Webserver example to new Gradio message history format (#29249)
6	14	examples/online_serving/gradio_openai_chatbot_webserver.py

[3e1ad4065] Woosuk Kwon 2025-11-23 [Model Runner V2] Add apply_temperature option to gumbel_sample (#29276)
15	6	vllm/v1/worker/gpu/sampler.py

[62d54ba46] Woosuk Kwon 2025-11-23 [Model Runner V2] Optimize CUDA graph capture time (#29275)
4	1	vllm/v1/worker/gpu/cudagraph_utils.py
1	0	vllm/v1/worker/gpu/model_runner.py

[b004c0041] Woosuk Kwon 2025-11-23 [Model Runner V2] Support spec decoding [1/N] (#29274)
112	16	vllm/v1/worker/gpu/input_batch.py
73	13	vllm/v1/worker/gpu/model_runner.py
0	0	vllm/v1/worker/gpu/spec_decode/__init__.py
71	0	vllm/v1/worker/gpu/spec_decode/rejection_sample.py
94	0	vllm/v1/worker/gpu/states.py

[7f12c82fa] Woosuk Kwon 2025-11-23 [Model Runner V2] Change bookkeeping logic in preparation for spec decoding (#29194)
4	3	vllm/v1/worker/gpu/async_utils.py
8	6	vllm/v1/worker/gpu/attn_utils.py
4	4	vllm/v1/worker/gpu/cudagraph_utils.py
127	47	vllm/v1/worker/gpu/input_batch.py
115	76	vllm/v1/worker/gpu/model_runner.py
10	3	vllm/v1/worker/gpu/states.py

[6fb0215ee] Luke 2025-11-23 [Bugfix] Use lazy string reference for DeepseekV3Config in config registry (#28958)
2	2	vllm/transformers_utils/config.py
6	0	vllm/transformers_utils/configs/__init__.py

[55c21c883] Micah Williamson 2025-11-22 [ROCm][CI] Fix "Cannot re-initialize CUDA in forked subprocess" in test_pynccl.py  (#29119)
3	0	requirements/rocm-test.txt
5	3	tests/distributed/test_pynccl.py

[3999442f1] rasmith 2025-11-22 [CI/Build][AMD] Add check for flash_att_varlen_func to test_tree_attention.py (#29252)
8	0	tests/v1/spec_decode/test_tree_attention.py

[71362ffab] rasmith 2025-11-22 [CI/Build][AMD] Skip test_multi_shared_storage_connector_consistency  in test_multi_connector.py due to hipErrorLaunchFailure  when calling .cpu() (#29253)
8	0	tests/v1/kv_connector/unit/test_multi_connector.py

[20ee418ad] Woosuk Kwon 2025-11-22 [Model Runner V2] Minor fix for cudagraph_utils (#29256)
5	14	vllm/v1/worker/gpu/cudagraph_utils.py
1	0	vllm/v1/worker/gpu/model_runner.py

[389aa1b2e] Cyrus Leung 2025-11-23 [Doc] Update more docs with respect to V1 (#29188)
0	3	docs/configuration/conserving_memory.md
1	3	docs/configuration/optimization.md
14	20	docs/usage/reproducibility.md
70	66	docs/usage/v1_guide.md
4	1	examples/offline_inference/reproducibility.py
0	7	tests/models/language/generation/test_common.py

[3ed767ec0] Michael Act 2025-11-23 docs: fixes distributed executor backend config for multi-node vllm (#29173)
4	2	docs/serving/parallelism_scaling.md

[5f96c00c5] jiahanc 2025-11-22 [Fix] Add SM check to flashinfer MOE backend (#29144)
10	0	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[458706326] Qidong Su 2025-11-22 Patch DeepEP when building docker image with CUDA 13 (#29154)
11	0	tools/ep_kernels/install_python_libraries.sh

[472fdee97] Wentao Ye 2025-11-22 [Chore] Update batch invariant code owner (#29246)
2	0	.github/CODEOWNERS

[df78aeef0] Yizhou 2025-11-23 Refactor: Move CUDA graph dispatch logic earlier (#27382)
25	25	vllm/v1/worker/gpu_model_runner.py

[7df331c66] Nick Hill 2025-11-22 [BugFix] Fix chunked prompt logprobs + preemption (#29071)
23	4	tests/conftest.py
76	0	tests/v1/sample/test_logprobs.py
0	14	vllm/v1/worker/gpu_input_batch.py
17	3	vllm/v1/worker/gpu_model_runner.py
0	10	vllm/v1/worker/tpu_input_batch.py
11	0	vllm/v1/worker/tpu_model_runner.py

[eb5352a77] Benjamin Bartels 2025-11-22 [CI/build] Removes source compilation from runtime image (#26966)
43	27	docker/Dockerfile
-	-	docs/assets/contributing/dockerfile-stages-dependency.png
86	74	tools/ep_kernels/install_python_libraries.sh
30	14	tools/install_deepgemm.sh

[d1cf8214e] Cyrus Leung 2025-11-23 [Bugfix] Use HF config fields as fallback when loading Mistral config (#29239)
1	0	.buildkite/test-amd.yaml
1	0	.buildkite/test-pipeline.yaml
13	1	vllm/transformers_utils/config.py
10	3	vllm/transformers_utils/configs/mistral.py

[730bd3537] Fadi Arafeh 2025-11-22 [perf][cpu] Accelerate paged attention GEMMs (QK, PV) on Arm CPUs with NEON (#29193)
17	0	csrc/cpu/cpu_attn.cpp
7	1	csrc/cpu/cpu_attn_impl.hpp
386	0	csrc/cpu/cpu_attn_neon.hpp
1	2	vllm/engine/arg_utils.py
5	2	vllm/v1/attention/backends/cpu_attn.py

[f55c76c2b] Federico 2025-11-22 chore: add RTX_PRO_6000 GLM4.6-FP8 kernel tuning (#29240)
147	0	vllm/model_executor/layers/fused_moe/configs/E=20,N=1536,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Server_Edition,dtype=fp8_w8a8.json

[d84d8f442] ZiTian Zhao 2025-11-22 Fix EVS crash when using `video_embeds` inputs in Qwen2.5-VL (#29232)
16	1	vllm/model_executor/models/qwen2_5_vl.py

[ae6681837] Cyrus Leung 2025-11-22 [Misc] Fix pre-commit (#29238)
1	1	vllm/model_executor/model_loader/utils.py

[d44a63c6d] Nick Hill 2025-11-22 [BugFix] Fix returned logprobs with spec decode + prefill chunking (#29216)
9	4	tests/v1/sample/test_logprobs.py
4	1	vllm/v1/sample/sampler.py
9	10	vllm/v1/worker/gpu_model_runner.py

[066209a04] Nicolò Lucchesi 2025-11-22 [Attention] Refactor FA `block_size` limitations to hybrid models only  (#29084)
1	1	tests/v1/attention/test_mla_backends.py
3	1	tests/v1/worker/test_gpu_model_runner.py
7	3	vllm/attention/backends/abstract.py
21	6	vllm/v1/attention/backends/flash_attn.py
6	6	vllm/v1/attention/backends/flashinfer.py
4	1	vllm/v1/attention/backends/mla/cutlass_mla.py
4	1	vllm/v1/attention/backends/mla/flashattn_mla.py
4	1	vllm/v1/attention/backends/mla/flashinfer_mla.py
4	1	vllm/v1/attention/backends/mla/flashmla.py
4	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
3	3	vllm/v1/attention/backends/mla/indexer.py
3	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
4	1	vllm/v1/attention/backends/rocm_aiter_fa.py
4	1	vllm/v1/attention/backends/tree_attn.py
4	1	vllm/v1/attention/backends/triton_attn.py
4	1	vllm/v1/attention/backends/xformers.py
2	2	vllm/v1/worker/gpu_model_runner.py

[5f7209a79] Bram Wasti 2025-11-22 [tiny] Remove unsupported TRITON_MLA backend from batch invariance (#28832)
1	1	vllm/model_executor/layers/batch_invariant.py

[2d4978a57] yihong 2025-11-22 fix: clean up function never use in setup.py (#29061)
0	34	setup.py

[6965a392a] Nandan Vallamdasu 2025-11-22 Fix: Resolve circular import in model_loader/utils.py (#29189)
7	6	vllm/model_executor/model_loader/utils.py

[5a4802588] Cyrus Leung 2025-11-22 [Misc] Further clean up chunked prefill and prefix caching init (#29186)
1	1	tests/engine/test_arg_utils.py
7	12	tests/v1/core/test_scheduler.py
3	8	tests/v1/core/utils.py
2	2	vllm/config/cache.py
19	5	vllm/engine/arg_utils.py
1	1	vllm/v1/core/sched/scheduler.py

[8e22da1d7] rasmith 2025-11-22 [CI/Build Don't add FLASHINFER backend in test_cpu_offloading.py (#29229)
5	1	tests/v1/kv_offload/test_cpu_offloading.py

[a4fdf2405] rasmith 2025-11-22 [CI/Build] Skip tests that require libcudart in test_lmcache_integration.py (#29228)
15	0	tests/v1/kv_connector/unit/test_lmcache_integration.py

[e6309acdb] Jane (Yuan) Xu 2025-11-22 Simplify `from_blob` usage in `get_cuda_view_from_cpu_tensor` (#29027)
3	8	csrc/cuda_view.cu

[988ee66b0] jinghanhu 2025-11-22 Handle triton kernel import exception  (#29062)
3	2	vllm/model_executor/layers/fused_moe/config.py

[ea38474ac] Mads Kildegård 2025-11-22 [Frontend][Responses API] Multi-turn (with type: "output_text") support for non-harmony requests (#29175)
2	1	vllm/entrypoints/chat_utils.py

[742e9ff6b] Andrew Xia 2025-11-21 [responsesAPI] parse reasoning item input (#28248)
44	0	examples/online_serving/openai_responses_client.py
71	0	tests/entrypoints/openai/test_response_api_simple.py
26	1	tests/entrypoints/openai/test_response_api_with_harmony.py
58	0	tests/entrypoints/test_responses_utils.py
13	0	vllm/entrypoints/responses_utils.py

[e9056056f] Woosuk Kwon 2025-11-21 [Model Runner V2] Limit cudagraph size to max decode batch size (#29221)
9	4	vllm/v1/worker/gpu/cudagraph_utils.py

[1489902b5] Jee Jee Li 2025-11-22 [LoRA] Cleanup FusedMoEWithLoRA (#29187)
90	95	vllm/lora/layers/fused_moe.py
2	2	vllm/lora/punica_wrapper/punica_base.py
2	2	vllm/lora/punica_wrapper/punica_gpu.py

[933f67ecd] Yanan Cao 2025-11-21 [Bugfix]Fix a conditional to not check zero value (#28754)
2	1	vllm/compilation/caching.py

[fd65015a1] rasmith 2025-11-21 [CI/Build] Only use supported types and features on ROCm in MoE kernel tests (#29149)
7	2	tests/kernels/moe/test_batched_moe.py
5	0	tests/kernels/moe/test_block_fp8.py
5	0	tests/kernels/moe/test_gpt_oss_triton_kernels.py
6	0	tests/kernels/moe/test_modular_kernel_combinations.py
6	0	tests/kernels/moe/test_moe_permute_unpermute.py
6	0	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
6	0	tests/kernels/moe/test_triton_moe_ptpc_fp8.py

[77e1c035d] Yihua Cheng 2025-11-21 [chore][LMCache connector] Remove useless logs from lmcache connector (#29069)
0	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py
0	3	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py

[6f403501a] rasmith 2025-11-21 [CI/Build][AMD] Enable Entrypoints Integration Test (Pooling) to run without error on ROCm (#29212)
6	0	tests/entrypoints/pooling/correctness/test_mteb_embed.py
6	0	tests/entrypoints/pooling/correctness/test_mteb_score.py
6	0	tests/entrypoints/pooling/llm/test_embedding.py
6	0	tests/entrypoints/pooling/llm/test_encode.py
6	0	tests/entrypoints/pooling/llm/test_score.py
6	0	tests/entrypoints/pooling/openai/test_embedding.py
6	0	tests/entrypoints/pooling/openai/test_embedding_dimensions.py
6	0	tests/entrypoints/pooling/openai/test_embedding_long_text.py
6	0	tests/entrypoints/pooling/openai/test_rerank.py
6	0	tests/entrypoints/pooling/openai/test_score.py
6	0	tests/entrypoints/pooling/openai/test_truncation.py

[052950e5b] FlintyLemming 2025-11-22 Add fused MoE config for H200 E160 N192 fp8 (#29182)
147	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_H200,dtype=fp8_w8a8.json

[1ef9c9e29] qli88 2025-11-21 [CI/Build] Disable test_gptoss_tp.py in 'LoRA TP Test' group for ROCm platform (#29204)
4	1	.buildkite/test-amd.yaml

[5c8f2adf5] Jie Luo 2025-11-22 [Bugfix] Fix block size in block_table with PCP (#29094)
9	2	vllm/v1/worker/block_table.py

[ed8e6843c] Ryan Rock 2025-11-21 [CI/Build] Add terratorch for AMD (#29205)
3	0	requirements/rocm-test.txt

[d045e22df] Lukas Geiger 2025-11-22 [Model][Qwen3VL] Tune Triton w8a8 block fp8 kernel for L40s (#29217)
146	0	vllm/model_executor/layers/quantization/utils/configs/N=10240,K=5120,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=5120,K=25600,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=5120,K=8192,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=51200,K=5120,device_name=NVIDIA_L40S,dtype=fp8_w8a8,block_shape=[128,128].json

[1d34eb11e] Wentao Ye 2025-11-21 [CI] Bug: Fix triton import issue (#29202)
1	2	vllm/v1/worker/gpu/block_table.py
1	2	vllm/v1/worker/gpu/input_batch.py

[9a3101b2b] Charlie Fu 2025-11-21 [Rocm][CI] Fix DeekSeek V2-Lite Accuracy CI (#29135)
11	1	.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh
10	1	.buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep.sh

[d5dbdbfcb] Angela Yi 2025-11-21 [docs] Fix cudagraph mode config (#29170)
1	1	docs/design/debug_vllm_compile.md

[30d646623] Lucas Wilkinson 2025-11-21 [BugFix] Fix Eagle `IndexError: list index out of range` for even `num_speculative_tokens` (#29102)
8	0	tests/conftest.py
10	6	vllm/config/compilation.py
19	14	vllm/v1/spec_decode/eagle.py

[e9af6ba62] Woosuk Kwon 2025-11-21 [Model Runner V2] Optimize Gumbel Sampling Kernel (#29210)
43	50	vllm/v1/worker/gpu/sampler.py

[c6fa3895e] Mark McLoughlin 2025-11-21 [KV Connector] Fix async connector prefix cache metrics (#28585)
13	4	tests/v1/core/test_scheduler.py
8	8	vllm/v1/core/sched/scheduler.py
3	0	vllm/v1/request.py

[3137991f5] Varun Sundar Rabindranath 2025-11-21 [BugFix] EPLB + B200 + DeepGEMM : Handle column-major scales tensor (#29162)
49	0	tests/distributed/eplb_utils.py
2	40	tests/distributed/test_eplb_execute.py
285	0	tests/distributed/test_eplb_fused_moe_layer.py
41	0	vllm/model_executor/layers/fused_moe/layer.py

[57430fc95] Julien Denize 2025-11-21 Default model load/config/tokenizer to `mistral` format if relevant files exist (#28659)
18	5	docs/features/tool_calling.md
1	1	tests/models/language/generation/test_mistral.py
13	1	tests/models/multimodal/test_mapping.py
3	0	tests/models/quantization/test_bitsandbytes.py
6	0	tests/tool_use/utils.py
62	0	tests/transformers_utils/test_config.py
5	1	tests/transformers_utils/test_utils.py
11	3	tests/v1/entrypoints/llm/test_struct_output_generate.py
5	4	vllm/config/model.py
2	0	vllm/model_executor/model_loader/__init__.py
19	1	vllm/model_executor/model_loader/default_loader.py
43	3	vllm/transformers_utils/config.py
1	1	vllm/transformers_utils/configs/mistral.py
19	11	vllm/transformers_utils/tokenizer.py
22	3	vllm/v1/engine/processor.py

[c68c7b403] Lucas Wilkinson 2025-11-21 [BugFix] Fix missing symbol triggering FA2 fallback on Hopper (#29107)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[53a1ba6ec] Ning Xie 2025-11-22 [log] add weights loading time log to sharded_state loader (#28628)
8	0	vllm/model_executor/model_loader/sharded_state_loader.py

[1840c5cb1] Lucas Wilkinson 2025-11-21 [BugFix] Make sure to allocate worst case MoE workspace during profile run in the DP + EP case (#27426)
2	2	vllm/envs.py
41	0	vllm/model_executor/layers/fused_moe/modular_kernel.py

[1bed891f7] Woosuk Kwon 2025-11-21 [Chore] Fix pre-commit error after #25266 (#29190)
11	9	vllm/v1/worker/gpu/async_utils.py
8	6	vllm/v1/worker/gpu/attn_utils.py
10	2	vllm/v1/worker/gpu/cudagraph_utils.py
10	6	vllm/v1/worker/gpu/model_runner.py
1	1	vllm/v1/worker/gpu/sampler.py

[ceca06050] Cyrus Leung 2025-11-22 [Deprecation] Deprecate `seed=None` (#29185)
14	5	vllm/engine/arg_utils.py

[75648b16d] Charlie Fu 2025-11-21 [ROCm][CI] Fix config/test_config_generation.py (#29142)
2	2	docker/Dockerfile.rocm

[460d02a41] Chendi.Xue 2025-11-21 [NIXL] Fix after virtual block_size for host_buffer with heter kv_layout (#29122)
13	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
0	8	vllm/platforms/xpu.py

[b4c8fbaae] Mingyuan Ma 2025-11-21 Add TRTLLM MoE NVFP4 kernel to CompressedTensorsW4A4MoeMethod (#28892)
122	20	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
15	190	vllm/model_executor/layers/quantization/modelopt.py
221	0	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[e99e46738] rasmith 2025-11-21 [CI/Build][Kernel][AMD] Move extra dim to after load in _fwd_kv_parallel in lighting_attn.py (#29132)
7	1	vllm/model_executor/layers/lightning_attn.py

[a42ab317a] Wentao Ye 2025-11-21 [Log] Optimize startup log (#28948)
4	2	vllm/model_executor/layers/fused_moe/fused_moe.py
7	3	vllm/model_executor/layers/quantization/fp8.py
13	12	vllm/profiler/gpu_profiler.py
2	1	vllm/v1/core/kv_cache_utils.py

[b7f1f490a] Aleksandr Malyshev 2025-11-21 Upstream triton fp4 weight preshuffle (#28888)
25	0	vllm/_aiter_ops.py
52	15	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py

[30b44a159] Woosuk Kwon 2025-11-21 GPU Model Runner V2 (#25266)
3	0	.github/CODEOWNERS
5	0	vllm/envs.py
3	0	vllm/v1/attention/backends/flashinfer.py
24	0	vllm/v1/core/sched/output.py
22	6	vllm/v1/core/sched/scheduler.py
4	0	vllm/v1/worker/gpu/README.md
0	0	vllm/v1/worker/gpu/__init__.py
89	0	vllm/v1/worker/gpu/async_utils.py
187	0	vllm/v1/worker/gpu/attn_utils.py
315	0	vllm/v1/worker/gpu/block_table.py
198	0	vllm/v1/worker/gpu/cudagraph_utils.py
22	0	vllm/v1/worker/gpu/dp_utils.py
265	0	vllm/v1/worker/gpu/input_batch.py
814	0	vllm/v1/worker/gpu/model_runner.py
327	0	vllm/v1/worker/gpu/sampler.py
265	0	vllm/v1/worker/gpu/states.py
76	0	vllm/v1/worker/gpu/structured_outputs.py
20	6	vllm/v1/worker/gpu_worker.py

[1f400c58b] Wentao Ye 2025-11-21 [CI] Add batch invariant test to ci (#27842)
12	0	.buildkite/test-pipeline.yaml
2	0	tests/v1/determinism/test_batch_invariance.py
2	1	tests/v1/determinism/utils.py

[711241c13] rasmith 2025-11-21 [CI/Build] Fix illegal memory access and unsupported test in kernels/attention/test_cache.py (#29118)
9	0	tests/kernels/attention/test_cache.py

[d7219bcda] Cyrus Leung 2025-11-21 [Misc] Move dynamic seed initialization to `EngineArgs` (#29165)
7	27	vllm/config/model.py
1	6	vllm/config/speculative.py
15	1	vllm/engine/arg_utils.py
0	3	vllm/v1/worker/tpu_worker.py

[4050bae41] wangxiyuan 2025-11-21 [Doc] Update plugin doc (#28532)
96	2	docs/design/plugin_system.md
3	0	vllm/plugins/__init__.py
2	2	vllm/v1/metrics/loggers.py

[f1805db1a] skaraban3807 2025-11-21 [Perf] These changes enhance the NUMA functionality of vllm for systems with more than one NUMA nodes per socket (#25559)
45	22	csrc/cpu/utils.cpp

[434f3d3eb] Julien Denize 2025-11-21 Fix mistral config (#29172)
4	0	vllm/transformers_utils/configs/mistral.py

[2092ce8c3] sfbemerk 2025-11-21 Tool Call Parser logs should not contain user input / model output except on DEBUG (#29160)
1	1	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
7	7	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py

[fc9f821d2] who who who 2025-11-21 fix cross attention (#28346)
9	8	vllm/v1/attention/backends/triton_attn.py

[945286308] Cyrus Leung 2025-11-21 Revert "Revert #28875 (#29159)" (#29179)
0	17	docker/Dockerfile
3	4	docs/deployment/docker.md
1	4	docs/getting_started/installation/gpu.cuda.inc.md

[2b1b3dfa4] Bhagyashri 2025-11-21 Update Dockerfile to use gcc-toolset-14 and fix test case failures on power (ppc64le) (#28957)
6	4	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
19	13	docker/Dockerfile.ppc64le
2	2	requirements/common.txt

[cca2d2cdb] Russell Bryant 2025-11-21 [Core] Align whisper closer to other multimodal models (#27292)
8	5	vllm/model_executor/models/whisper.py
13	36	vllm/v1/worker/gpu_model_runner.py

[aab0102a2] Cyrus Leung 2025-11-21 [V0 deprecation] Remove more V0 references (#29088)
0	2	docs/contributing/model/basic.md
0	3	docs/design/prefix_caching.md
2	7	docs/usage/reproducibility.md
1	1	docs/usage/v1_guide.md
2	6	examples/offline_inference/reproducibility.py
4	4	examples/offline_inference/rlhf_utils.py
3	16	examples/offline_inference/save_sharded_state.py
1	5	examples/offline_inference/spec_decode.py
2	11	tests/model_executor/model_loader/test_sharded_state_loader.py
13	12	tests/tool_use/utils.py
0	1	vllm/entrypoints/llm.py
3	3	vllm/entrypoints/openai/protocol.py
0	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	2	vllm/model_executor/models/interfaces.py
0	1	vllm/model_executor/models/plamo2.py

[b34129bf8] WeiQing Chen 2025-11-21 [Misc] remove useless v1 env (#29164)
0	2	tests/v1/e2e/test_lora_with_spec_decode.py

[4d7231e77] Cyrus Leung 2025-11-21 Revert #28875 (#29159)
17	0	docker/Dockerfile
4	3	docs/deployment/docker.md
4	1	docs/getting_started/installation/gpu.cuda.inc.md

[8ac3a4148] Huamin Li 2025-11-20 [CI Failure] Fix Gemma3 RoPE configuration for sliding attention layers (#29111)
4	2	vllm/model_executor/models/gemma3.py

[7d6da483b] Canlin Guo 2025-11-21 [Minor][Clean] Remove the legacy assertion in video (#29150)
0	5	vllm/multimodal/video.py

[e4c3182c6] Chenheli Hua 2025-11-20 [Small] Capture AttributeError when checking ray dependency.  (#29024)
4	0	vllm/ray/lazy_utils.py

[b4734b955] Alex Brooks 2025-11-20 [Bugfix] Fix default MM LoRA alignment for single str prompts (#29140)
35	0	tests/lora/test_default_mm_loras.py
1	1	vllm/entrypoints/llm.py

[30b9c6774] Jialin Ouyang 2025-11-20 Revert "[Redo] #26368 (#28771)" (#29121)
1	2	tests/v1/core/test_async_scheduler.py
2	4	tests/v1/core/test_priority_scheduler_random.py
38	50	tests/v1/core/test_scheduler.py
3	4	tests/v1/kv_connector/unit/test_nixl_connector.py
1	2	tests/v1/kv_connector/unit/utils.py
1	4	tests/v1/spec_decode/test_eagle.py
9	9	tests/v1/spec_decode/test_ngram.py
8	5	vllm/utils/gc_utils.py
2	2	vllm/v1/core/sched/scheduler.py
2	2	vllm/v1/outputs.py
5	3	vllm/v1/sample/rejection_sampler.py
4	3	vllm/v1/spec_decode/eagle.py
3	3	vllm/v1/spec_decode/ngram_proposer.py
4	6	vllm/v1/spec_decode/suffix_decoding.py
13	23	vllm/v1/worker/gpu_model_runner.py
3	5	vllm/v1/worker/tpu_model_runner.py

[11857a00b] Matthew Bonanni 2025-11-20 [Attention] Add ROCM_AITER_MLA_SPARSE to attention backend registry (#29103)
3	0	vllm/attention/backends/registry.py
1	4	vllm/platforms/rocm.py

[8c25f9cfb] Boyuan Feng 2025-11-20 [BugFix] skip combo kernel on cpu (#29129)
2	0	vllm/config/compilation.py

[56e96b37e] Cyrus Leung 2025-11-21 [V0 Deprecation] Remove `best_of` (#29090)
2	2	docs/usage/v1_guide.md
0	8	tests/v1/sample/test_sampling_params_e2e.py
0	4	vllm/entrypoints/openai/protocol.py
2	8	vllm/entrypoints/openai/serving_completion.py
0	40	vllm/sampling_params.py
0	3	vllm/v1/engine/processor.py

[698024ecc] Qidong Su 2025-11-20 [Doc] update installation guide regarding aarch64+cuda pytorch build (#28875)
0	17	docker/Dockerfile
3	4	docs/deployment/docker.md
1	4	docs/getting_started/installation/gpu.cuda.inc.md

[073041499] jeremyteboul 2025-11-20 [Core] Add audio_embeds support to chat completions (#29059)
32	0	docs/features/multimodal_inputs.md
145	0	tests/entrypoints/test_chat_utils.py
147	2	vllm/entrypoints/chat_utils.py
24	0	vllm/multimodal/audio.py
12	1	vllm/multimodal/utils.py

[a982f5b5e] zhrrr 2025-11-21 [kernel][perf] support uncontiguous input for rms_norm kernel (#28103)
21	0	csrc/dispatch_utils.h
54	26	csrc/layernorm_kernels.cu
1	4	vllm/_custom_ops.py
1	3	vllm/compilation/matcher_utils.py

[0e741c12e] Cyrus Leung 2025-11-21 [Bugfix] Fix Plamo3 rope handling (#29092)
18	8	vllm/model_executor/models/plamo3.py

[56669c1f2] Wentao Ye 2025-11-20 [CI] Fix mypy for `vllm/v1/worker` (#29037)
1	1	tools/pre_commit/mypy.py
1	1	vllm/model_executor/utils.py
2	2	vllm/multimodal/utils.py
8	4	vllm/v1/worker/cpu_worker.py
82	46	vllm/v1/worker/gpu_model_runner.py
13	7	vllm/v1/worker/gpu_ubatch_wrapper.py
33	29	vllm/v1/worker/gpu_worker.py
1	1	vllm/v1/worker/kv_connector_model_runner_mixin.py
22	6	vllm/v1/worker/tpu_model_runner.py
2	3	vllm/v1/worker/tpu_worker.py
5	3	vllm/v1/worker/utils.py
2	0	vllm/v1/worker/worker_base.py
8	1	vllm/v1/worker/xpu_worker.py

[3f5f36da3] Hongxia Yang 2025-11-20 [ROCm] Fix for import when building with upstream triton for gfx1100 for gpt-oss serving (#29127)
6	6	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py

[e1eefa4c4] Wentao Ye 2025-11-20 [Bug] Fix torch warning of tf32 usage (#29112)
3	2	vllm/model_executor/layers/batch_invariant.py

[ed6ae1e36] Xiao Li 2025-11-20 [AITER] [ROCm] Fix crash when loading llama4 model with old aiter version installed, fallback to forward_native implementation (#29124)
13	6	vllm/v1/sample/ops/topk_topp_sampler.py

[9875be643] Jee Jee Li 2025-11-21 [LoRA][2/2]Remove LoRA extra vocab  (#28545)
10	0	tests/lora/conftest.py
13	176	tests/lora/test_layers.py
49	35	tests/lora/test_llama_tp.py
2	2	tests/lora/test_lora_functions.py
1	19	tests/lora/test_lora_manager.py
5	3	tests/lora/test_worker.py
0	8	tests/lora/utils.py
1	17	vllm/config/lora.py
0	5	vllm/engine/arg_utils.py
0	1	vllm/lora/layers/base.py
0	1	vllm/lora/layers/base_linear.py
0	1	vllm/lora/layers/column_parallel_linear.py
0	2	vllm/lora/layers/fused_moe.py
2	53	vllm/lora/layers/logits_processor.py
2	31	vllm/lora/layers/vocal_parallel_embedding.py
0	24	vllm/lora/lora_weights.py
10	44	vllm/lora/models.py
5	6	vllm/lora/punica_wrapper/punica_base.py
1	4	vllm/lora/punica_wrapper/punica_gpu.py
1	2	vllm/lora/punica_wrapper/punica_tpu.py
1	4	vllm/lora/punica_wrapper/punica_xpu.py
10	0	vllm/lora/utils.py
1	8	vllm/lora/worker_manager.py
5	29	vllm/model_executor/models/granite.py
5	25	vllm/model_executor/models/llama.py
8	24	vllm/model_executor/models/mixtral.py
1	1	vllm/model_executor/models/teleflm.py
0	3	vllm/v1/worker/tpu_model_runner.py

[df44df014] Wentao Ye 2025-11-20 [Feature] Shared Experts Overlap with FI deepgemm swap kernel, 2.2% throughput improvement and 3.6% TTFT improvement (#28879)
1	0	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
42	27	vllm/model_executor/layers/fused_moe/layer.py
74	5	vllm/model_executor/layers/fused_moe/modular_kernel.py
2	1	vllm/model_executor/layers/fused_moe/prepare_finalize.py

[87cbbdff6] Michael Goin 2025-11-20 Update model references for OLMo3 (#29099)
1	1	docs/models/supported_models.md
1	1	tests/models/registry.py

[986ab5db6] Michael Goin 2025-11-20 [CI Bugfix] Fix Kernels DeepGEMM Test (H100) (#29106)
4	5	.buildkite/test-pipeline.yaml

[dd39f91ed] Rob Mulla 2025-11-20 [Doc] cleanup TPU documentation and remove outdated examples (#29048)
4	2	docs/.nav.yml
0	111	docs/configuration/tpu.md
20	17	docs/features/README.md
16	13	docs/features/quantization/README.md
0	34	docs/models/hardware_supported_models/tpu.md
0	70	examples/offline_inference/profiling_tpu/README.md
0	110	examples/offline_inference/profiling_tpu/profiling.py
0	58	examples/offline_inference/tpu.py

[c7a29d2c8] rasmith 2025-11-20 [CI/Build] Remove skip global cleanup in test_struct_output_generate.py (#29022)
0	4	tests/v1/entrypoints/llm/test_struct_output_generate.py

[8237ab8a2] rasmith 2025-11-20 [CI/Build] Skip lm-format-enforcer tests in test_struct_output_generate.py for now (#29021)
26	2	tests/v1/entrypoints/llm/test_struct_output_generate.py

[3fd74189d] Driss Guessous 2025-11-20 Fixes bench (#29058)
2	1	vllm/compilation/caching.py

[5e5a7eb16] rasmith 2025-11-20 [CI/Build] Make test_attention_selector.py run tests on correct platform (#29064)
4	1	tests/kernels/attention/test_attention_selector.py

[3d84ef905] rasmith 2025-11-20 [CI/Build][AMD] Skip if flash_attn_varlen_func not available in test_aiter_flash_attn.py (#29043)
3	0	tests/kernels/attention/test_aiter_flash_attn.py

[4d01b6428] Software Developer 2025-11-20 [Bugfix] - Add Trace Headers to Beam Search Path (#29100)
1	0	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/entrypoints/openai/serving_completion.py
2	0	vllm/entrypoints/openai/serving_engine.py

[114b0e250] Kevin H. Luu 2025-11-20 [chore] Update annotate release scripts (#29077)
5	4	.buildkite/scripts/annotate-release.sh

[647464719] Or Ozeri 2025-11-20 [KVConnector][Core] Support cross-layer KV blocks (#27743)
6	2	tests/v1/kv_connector/unit/test_offloading_connector.py
93	52	tests/v1/kv_offload/test_cpu_offloading.py
4	1	tests/v1/worker/test_gpu_model_runner.py
28	1	vllm/attention/backends/abstract.py
31	2	vllm/distributed/kv_transfer/kv_connector/v1/base.py
38	5	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
10	2	vllm/v1/attention/backends/flash_attn.py
10	2	vllm/v1/attention/backends/flashinfer.py
9	0	vllm/v1/attention/backends/mla/common.py
5	1	vllm/v1/attention/backends/mla/indexer.py
5	12	vllm/v1/kv_offload/cpu.py
5	1	vllm/v1/kv_offload/spec.py
10	2	vllm/v1/kv_offload/worker/cpu_gpu.py
34	7	vllm/v1/worker/gpu_model_runner.py
165	0	vllm/v1/worker/kv_connector_model_runner_mixin.py

[e5bfcb6a8] Pan Li 2025-11-21 [BugFix][PD]: make example proxy usable with P2pNcclConnector (#26628)
155	94	benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
14	5	examples/online_serving/disaggregated_prefill.sh

[22924383e] Alexei-V-Ivanov-AMD 2025-11-20 Updating the mirror of test-amd.yaml as of 2025-11-18 (#29016)
27	14	.buildkite/test-amd.yaml

[56f45edda] rookie 2025-11-21 [Frontend] Optimize beam search loop by sorting and then splicing (#19347)
70	33	vllm/entrypoints/openai/serving_engine.py

[82b05b15e] TJian 2025-11-20 [BugFix] [FEAT] Enable fastsafetensors for ROCm platform (#28225)
1	0	requirements/rocm.txt
2	1	tests/model_executor/model_loader/fastsafetensors_loader/test_fastsafetensors_loader.py
2	1	tests/model_executor/model_loader/fastsafetensors_loader/test_weight_utils.py

[a2e9ebe9e] Fanli Lin 2025-11-20 [BugFix] Fix flash_attn import in `siglip2navit.py` (#29082)
1	1	vllm/model_executor/models/siglip2navit.py

[93c8672ce] Zhewen Li 2025-11-20 [Bugfix] Fix spec decode memory regression after #28549 (#28819)
0	5	vllm/model_executor/models/deepseek_eagle.py
0	7	vllm/model_executor/models/llama4_eagle.py
0	5	vllm/model_executor/models/llama_eagle.py
5	2	vllm/v1/spec_decode/eagle.py

[371b1d4c6] Samit 2025-11-20 [RL] Add Pause and Resume Generation for Asynchronous RL Training (#28037)
27	0	vllm/engine/protocol.py
78	0	vllm/entrypoints/openai/api_server.py
64	0	vllm/v1/engine/async_llm.py
13	0	vllm/v1/engine/output_processor.py

[c9e093116] Shinichi Hemmi 2025-11-20 [MODEL] Implement plamo3 (#28834)
1	0	docs/models/supported_models.md
1	0	tests/distributed/test_pipeline_parallel.py
4	0	tests/models/registry.py
431	0	vllm/model_executor/models/plamo3.py
1	0	vllm/model_executor/models/registry.py

[c0c2dd1e0] Or Ozeri 2025-11-20 [BugFix] kv_offloading: Fix bug in loading of partial cpu blocks (#28951)
2	2	tests/v1/kv_offload/test_cpu_gpu.py
9	11	vllm/v1/kv_offload/worker/cpu_gpu.py

[06c20c990] Pleaplusone 2025-11-20 [ROCm] Add AMD GPU support on Deepseek v3.2 and SparseMLA (#26670)
4	0	csrc/cache_kernels.cu
210	0	vllm/attention/ops/rocm_aiter_mla_sparse.py
18	4	vllm/model_executor/models/deepseek_v2.py
12	1	vllm/platforms/rocm.py
3	2	vllm/utils/deep_gemm.py
1	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
9	6	vllm/v1/attention/backends/mla/indexer.py
325	0	vllm/v1/attention/backends/mla/rocm_aiter_mla_sparse.py
1	1	vllm/v1/worker/utils.py

[6eb745d9b] Anna Shors 2025-11-20 Add truncate arg to yarn to match openai implementation of gpt-oss (#28244)
1	0	vllm/model_executor/layers/rotary_embedding/__init__.py
7	7	vllm/model_executor/layers/rotary_embedding/common.py
3	0	vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope.py
1	0	vllm/model_executor/models/gpt_oss.py

[66483a9d0] cjackal 2025-11-20 [Chore] Update `xgrammar` version from 0.1.25 to 0.1.27 (#28221)
1	1	requirements/common.txt

[edfe86720] Jinzhen Lin 2025-11-20 [Misc] don't cache `CUTLASS_REVISION` var in CMakeLists.txt (#28518)
1	1	CMakeLists.txt

[dc45efc8e] Dezhan 2025-11-20 [BugFix] Fix Llama4 Pipeline Parallelism Assert Error (#28577)
7	0	vllm/model_executor/models/llama4.py

[fb8851f25] Vensen 2025-11-20 [Bugfix][cache_kernels]: Fix OOB in cache_kernels.cu (#28760)
12	7	csrc/cache_kernels.cu
65	0	tests/kernels/test_cache_kernels.py

[a903d59ff] Boyuan Feng 2025-11-20 cleanup at::Tag::needs_fixed_stride_order (#28974)
2	5	csrc/cpu/torch_bindings.cpp
18	46	csrc/torch_bindings.cpp

[322cb0287] rasmith 2025-11-20 [CI/Build][AMD] Fix import errors in tests/kernels/attention (#29032)
13	5	tests/kernels/attention/test_cascade_flash_attn.py
14	5	tests/kernels/attention/test_flash_attn.py
10	2	tests/kernels/attention/test_flashinfer.py
2	1	tests/kernels/attention/test_flashinfer_mla_decode.py
2	1	tests/kernels/attention/test_flashinfer_trtllm_attention.py
8	1	tests/kernels/moe/test_flashinfer.py

[2c52c7fd9] Wentao Ye 2025-11-20 [Bug] Fix torch dynamo warning Dynamo detected a call to a `functools.lru_cache` (#29038)
3	2	tests/v1/determinism/conftest.py
9	26	tests/v1/determinism/test_batch_invariance.py
9	3	tests/v1/determinism/test_online_batch_invariance.py
20	0	tests/v1/determinism/utils.py
11	9	vllm/model_executor/layers/batch_invariant.py

[1e1c06789] Bradley D 2025-11-19 [ci][amd] fix EPLB execution test (#28742)
213	210	tests/distributed/test_eplb_execute.py

[7218f8399] Pleaplusone 2025-11-20 [ROCm][BugFix] Fix shared expert loading error when disable `VLLM_ROCM_USE_AITER_FUSION_SHARED_EXPERTS` (#28633)
5	2	vllm/model_executor/models/deepseek_v2.py

[20e4497be] Cyrus Leung 2025-11-20 [V0 Deprecation] Remove `num_lookahead_slots` (#29000)
0	9	vllm/config/scheduler.py
0	10	vllm/config/speculative.py
0	11	vllm/engine/arg_utils.py

[1c7bcc55b] Quentin Gallouédec 2025-11-19 [Frontend] Allow parsed tool arguments (#28820)
2	1	vllm/entrypoints/chat_utils.py

[a9705a290] Lukas Geiger 2025-11-20 [Model][QwenVL] Replace `torch.repeat_interleave` with faster `np.repeat` (#28964)
2	12	tests/models/multimodal/generation/test_qwen2_vl.py
9	6	vllm/model_executor/models/qwen2_vl.py
7	5	vllm/model_executor/models/qwen3_vl.py

[64192d562] Isotr0py 2025-11-20 [Bugfix] Revert custom attention mask for gemma3-mm (#28995)
0	5	vllm/config/model.py
1	137	vllm/model_executor/models/gemma3_mm.py
0	11	vllm/transformers_utils/config.py
0	19	vllm/v1/worker/gpu_model_runner.py

[fe25772aa] Canlin Guo 2025-11-20 [Bugfix] Handle broken frames in video loading (#29001)
-	-	tests/multimodal/assets/corrupted.mp4
37	0	tests/multimodal/test_video.py
75	43	vllm/multimodal/video.py

[0cca9b4d1] prashanth058 2025-11-19 [Bugfix] Fix precision loss in LoRA-wrapped RowParallelLinear by fusing bias into GEMM (#28972)
10	15	vllm/lora/layers/row_parallel_linear.py

[a8c536829] Shengliang Xu 2025-11-19 Consolidate Nvidia ModelOpt quant config handling for all quantization methods (#28076)
234	265	vllm/model_executor/layers/quantization/modelopt.py

[fcbcba6c7] Benjamin Chislett 2025-11-19 [Feat] Iteration-level profiling for Torch and CUDA profiler (#28987)
203	0	tests/v1/worker/test_gpu_profiler.py
16	0	vllm/envs.py
194	19	vllm/profiler/gpu_profiler.py
13	1	vllm/v1/engine/async_llm.py
9	41	vllm/v1/worker/gpu_worker.py

[3168285fc] Fadi Arafeh 2025-11-20 [cpu][ci] Add initial set of tests for Arm CPUs (#28657)
64	0	.buildkite/scripts/hardware_ci/run-cpu-test-arm.sh
10	0	docker/Dockerfile.cpu

[3fb0d9099] Qiang Zhang 2025-11-20 [AMD] Use Decoupled Kernel Block Size to Support AITER MLA block_size=1 (#27715)
6	8	vllm/attention/backends/abstract.py
7	38	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[05c2dee7e] Kuntai Du 2025-11-20 [DeepSeek + LMCache Multiprocess] handle MLA for deepseek model + LMCache Multiprocess connector (#29039)
39	8	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py

[1d642872a] liangel-02 2025-11-19 [torchao] fix safetensors for sharding (#28169)
4	5	tests/quantization/test_torchao.py
1	1	vllm/model_executor/model_loader/default_loader.py
18	5	vllm/model_executor/model_loader/weight_utils.py

[9ccef8e33] Nick Hill 2025-11-19 [Misc] Colorize logs (#29017)
51	43	tests/test_logger.py
9	0	vllm/envs.py
35	16	vllm/logger.py
2	1	vllm/logging_utils/__init__.py
50	0	vllm/logging_utils/formatter.py
5	2	vllm/utils/system_utils.py

[537cc635c] Jialin Ouyang 2025-11-19 [GC Debugger] Simply and improve GC Debugger Utils (#29029)
4	3	vllm/utils/gc_utils.py
2	3	vllm/v1/engine/core.py

[5031cd5d5] Wentao Ye 2025-11-19 [Refactor] Optimize `select_experts` (#28069)
0	5	vllm/model_executor/layers/fused_moe/fused_moe.py
4	7	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/modelopt.py
1	1	vllm/model_executor/models/longcat_flash.py
1	1	vllm/model_executor/models/openpangu.py

[3aaa94ac9] Alexander Matveev 2025-11-19 [Performance] Reduce DeepGEMM N dim restriction from 128 to 64 multiplier  (#28687)
20	0	.buildkite/test-pipeline.yaml
7	4	tests/kernels/quantization/test_block_fp8.py
9	2	vllm/utils/deep_gemm.py

[8e38e9982] JartX 2025-11-20 [Feature] EPLB on Qwen3VLMoe and CompressedTensorsWNA16MoEMethod (#28849)
24	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
58	4	vllm/model_executor/models/qwen3_vl_moe.py

[0075bfffd] Wentao Ye 2025-11-19 [CI] Fix precommit `rope_theta` issue (#29040)
1	2	vllm/model_executor/models/deepseek_v2.py

[cb0a7b4be] Max Hu 2025-11-19 [Bugfix] Move flashinfer kernel check into ```__init__``` function of ```FusedMoE``` (#29018)
4	1	vllm/model_executor/layers/fused_moe/layer.py

[8f4f77a72] Lucas Wilkinson 2025-11-19 [BugFix] Fix false assertion with spec-decode=[2,4,..] and TP>2 (#29036)
1	1	vllm/config/compilation.py

[22e44ad58] Micah Williamson 2025-11-19 [ROCm][CI] Fix Weight Loading With Multiple GPU Tests on ROCm (#28984)
2	3	.buildkite/test-amd.yaml
3	0	tests/weight_loading/models-amd.txt
3	0	tests/weight_loading/models-large-amd.txt

[88f5b19f0] Yongye Zhu 2025-11-19 [DeepSeek] Fix DeepSeek V3.2 Rope Embedding (#28968)
5	1	vllm/model_executor/layers/mla.py
12	2	vllm/model_executor/models/deepseek_v2.py

[613abb50d] Shu Wang 2025-11-19 [MoE] Nvfp4 Masked Gemm: Add flashinfer grouped_gemm_nt_masked (#25990)
1	0	.buildkite/test-pipeline.yaml
582	0	tests/kernels/moe/test_cutedsl_moe.py
6	2	vllm/envs.py
14	2	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
346	0	vllm/model_executor/layers/fused_moe/flashinfer_cutedsl_moe.py
21	9	vllm/model_executor/layers/quantization/modelopt.py
33	10	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
12	9	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
5	1	vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py
42	0	vllm/utils/flashinfer.py

[cdeec2e60] Julien Denize 2025-11-19 [BugFix] Ray with multiple nodes (#28873)
8	8	vllm/v1/worker/gpu_worker.py

[1607e664f] Wentao Ye 2025-11-19 [Bug] Fix Batch Invariant MLA test (#28967)
32	9	tests/v1/determinism/test_batch_invariance.py
1	1	vllm/model_executor/layers/batch_invariant.py

[68d723199] Ryan Rock 2025-11-19 [CI/Build] Fix test_prefix_prefill for AMD (#28905)
6	6	tests/kernels/attention/test_prefix_prefill.py

[2fd893b4c] Qiu 2025-11-20 [Feature] Prefill Context Parallel (PCP) basic support (#28718)
6	6	tests/distributed/test_context_parallel.py
6	1	tests/kernels/moe/modular_kernel_tools/common.py
2	2	tests/v1/worker/test_gpu_model_runner.py
17	0	vllm/attention/backends/abstract.py
37	3	vllm/attention/ops/common.py
31	9	vllm/config/parallel.py
26	6	vllm/config/vllm.py
61	13	vllm/distributed/parallel_state.py
22	0	vllm/engine/arg_utils.py
40	19	vllm/model_executor/layers/fused_moe/config.py
32	0	vllm/model_executor/layers/fused_moe/layer.py
7	2	vllm/model_executor/models/gpt_oss.py
3	3	vllm/v1/attention/backends/flash_attn.py
3	3	vllm/v1/attention/backends/mla/common.py
9	9	vllm/v1/attention/backends/utils.py
17	0	vllm/v1/core/kv_cache_coordinator.py
4	5	vllm/v1/core/kv_cache_manager.py
9	4	vllm/v1/core/kv_cache_utils.py
2	0	vllm/v1/core/sched/scheduler.py
15	4	vllm/v1/core/single_type_kv_cache_manager.py
1	0	vllm/v1/engine/core.py
17	6	vllm/v1/executor/multiproc_executor.py
3	2	vllm/v1/kv_cache_interface.py
22	13	vllm/v1/worker/block_table.py
2	2	vllm/v1/worker/gpu_input_batch.py
2	2	vllm/v1/worker/gpu_model_runner.py
3	0	vllm/v1/worker/gpu_worker.py

[02f5903b8] Izzy Putterman 2025-11-19 Eagle: MM Cuda Graphs with MRope (#28896)
6	8	vllm/model_executor/models/llama_eagle3.py
11	2	vllm/v1/spec_decode/eagle.py

[ac10fd3c6] Aleksandr Malyshev 2025-11-19 Upstreaming aiter triton attention backend as a new backend (#28701)
3	0	vllm/attention/backends/registry.py
3	1	vllm/platforms/rocm.py
74	0	vllm/v1/attention/backends/mla/aiter_triton_mla.py

[9d2d56125] 杰兮 2025-11-20 [Bugfix]  Fix precision corruption when shared_experts_stream=None (#28942)
7	4	vllm/model_executor/layers/fused_moe/layer.py
1	2	vllm/utils/torch_utils.py

[fe69f331f] Robert Shaw 2025-11-19 [Kernels] Improve H200 Fused MoE Config (#28992)
36	36	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json

[3319a493f] Jialin Ouyang 2025-11-19 [Core] Reuse created spec tokens lists to mitigate GC cost (#28917)
12	6	vllm/v1/worker/gpu_input_batch.py
2	1	vllm/v1/worker/gpu_model_runner.py

[61728cd1d] Copilot 2025-11-19 Re-enable FlashInfer for Llama4 on Blackwell in e2e fusion tests (#28966)
2	0	.buildkite/test-pipeline.yaml
4	8	tests/compile/distributed/test_fusions_e2e.py

[0c80efd94] Yuxuan Zhang 2025-11-20 GLM-V video segmentation solution adjustment (#28941)
90	4	vllm/model_executor/models/glm4_1v.py

[a8b70304d] Harry Mellor 2025-11-19 Update `rope_scaling` to `rope_parameters` in preparation for Transformers v5 (#28542)
3	3	.buildkite/test-pipeline.yaml
7	12	benchmarks/kernels/benchmark_mrope.py
3	3	examples/offline_inference/context_extension.py
2	2	tests/compile/test_functionalization.py
5	11	tests/kernels/core/test_mrope.py
20	19	tests/kernels/core/test_pos_encoding.py
1	1	tests/kernels/moe/test_gpt_oss_triton_kernels.py
9	7	tests/models/language/pooling/test_nomic_max_model_len.py
20	17	tests/test_config.py
29	34	vllm/config/model.py
38	38	vllm/model_executor/layers/rotary_embedding/__init__.py
1	16	vllm/model_executor/models/afmoe.py
2	20	vllm/model_executor/models/apertus.py
0	11	vllm/model_executor/models/arcee.py
1	2	vllm/model_executor/models/arctic.py
3	5	vllm/model_executor/models/baichuan.py
1	2	vllm/model_executor/models/bailing_moe.py
1	5	vllm/model_executor/models/bamba.py
4	25	vllm/model_executor/models/chameleon.py
2	1	vllm/model_executor/models/chatglm.py
1	4	vllm/model_executor/models/commandr.py
12	10	vllm/model_executor/models/config.py
5	2	vllm/model_executor/models/dbrx.py
13	30	vllm/model_executor/models/deepseek_v2.py
1	10	vllm/model_executor/models/dots1.py
5	9	vllm/model_executor/models/ernie45_moe.py
5	8	vllm/model_executor/models/ernie45_vl_moe.py
1	20	vllm/model_executor/models/exaone.py
3	16	vllm/model_executor/models/exaone4.py
1	2	vllm/model_executor/models/falcon.py
3	5	vllm/model_executor/models/falcon_h1.py
4	4	vllm/model_executor/models/gemma.py
1	4	vllm/model_executor/models/gemma2.py
12	9	vllm/model_executor/models/gemma3.py
11	9	vllm/model_executor/models/gemma3n.py
1	9	vllm/model_executor/models/glm4.py
0	1	vllm/model_executor/models/glm4_1v.py
1	10	vllm/model_executor/models/glm4_moe.py
1	2	vllm/model_executor/models/gpt_j.py
1	2	vllm/model_executor/models/gpt_neox.py
6	7	vllm/model_executor/models/gpt_oss.py
1	16	vllm/model_executor/models/granite.py
3	10	vllm/model_executor/models/granitemoe.py
1	4	vllm/model_executor/models/granitemoehybrid.py
1	5	vllm/model_executor/models/granitemoeshared.py
4	7	vllm/model_executor/models/grok1.py
2	23	vllm/model_executor/models/hunyuan_v1.py
3	9	vllm/model_executor/models/internlm2.py
1	4	vllm/model_executor/models/internlm2_ve.py
0	5	vllm/model_executor/models/kimi_linear.py
1	16	vllm/model_executor/models/lfm2.py
1	16	vllm/model_executor/models/lfm2_moe.py
2	20	vllm/model_executor/models/llama.py
1	10	vllm/model_executor/models/llama4.py
8	14	vllm/model_executor/models/longcat_flash.py
3	9	vllm/model_executor/models/minicpm.py
1	9	vllm/model_executor/models/minicpm3.py
1	4	vllm/model_executor/models/minicpm_eagle.py
3	9	vllm/model_executor/models/minimax_m2.py
3	6	vllm/model_executor/models/minimax_text_01.py
1	6	vllm/model_executor/models/mixtral.py
6	2	vllm/model_executor/models/mllama4.py
1	2	vllm/model_executor/models/molmo.py
1	16	vllm/model_executor/models/nemotron.py
1	18	vllm/model_executor/models/nemotron_nas.py
1	2	vllm/model_executor/models/olmo.py
7	6	vllm/model_executor/models/olmo2.py
1	5	vllm/model_executor/models/olmoe.py
7	19	vllm/model_executor/models/openpangu.py
3	9	vllm/model_executor/models/orion.py
1	10	vllm/model_executor/models/ouro.py
1	2	vllm/model_executor/models/persimmon.py
1	5	vllm/model_executor/models/phi.py
7	11	vllm/model_executor/models/phimoe.py
1	6	vllm/model_executor/models/plamo2.py
3	8	vllm/model_executor/models/qwen.py
5	11	vllm/model_executor/models/qwen2.py
0	1	vllm/model_executor/models/qwen2_5_vl.py
3	9	vllm/model_executor/models/qwen2_moe.py
0	1	vllm/model_executor/models/qwen2_vl.py
5	10	vllm/model_executor/models/qwen3.py
3	9	vllm/model_executor/models/qwen3_moe.py
1	2	vllm/model_executor/models/qwen3_next.py
0	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py
0	1	vllm/model_executor/models/qwen3_vl.py
5	10	vllm/model_executor/models/seed_oss.py
1	17	vllm/model_executor/models/solar.py
1	1	vllm/model_executor/models/stablelm.py
1	2	vllm/model_executor/models/starcoder2.py
6	10	vllm/model_executor/models/step3_text.py
8	2	vllm/model_executor/models/transformers/utils.py
1	3	vllm/model_executor/models/zamba2.py
75	25	vllm/transformers_utils/config.py
5	2	vllm/transformers_utils/configs/afmoe.py
14	4	vllm/transformers_utils/configs/arctic.py
11	6	vllm/transformers_utils/configs/flex_olmo.py
8	4	vllm/transformers_utils/configs/kimi_linear.py
8	4	vllm/transformers_utils/configs/lfm2_moe.py
1	1	vllm/transformers_utils/configs/midashenglm.py
2	2	vllm/transformers_utils/configs/mistral.py
31	29	vllm/transformers_utils/configs/nemotron.py
8	4	vllm/transformers_utils/configs/olmo3.py
10	7	vllm/transformers_utils/configs/qwen3_next.py
8	4	vllm/transformers_utils/configs/step3_vl.py

[d44e9df7d] Shanshan Shen 2025-11-20 [Model][Mamba] Add selector for mamba attention backend and make it pluggable for other device (#26487)
1	0	docs/contributing/model/basic.md
2	1	vllm/attention/__init__.py
100	14	vllm/attention/backends/registry.py
32	1	vllm/attention/selector.py
1	7	vllm/model_executor/layers/kda.py
5	5	vllm/model_executor/layers/mamba/abstract.py
0	14	vllm/model_executor/layers/mamba/linear_attn.py
1	9	vllm/model_executor/layers/mamba/mamba_mixer.py
0	9	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	9	vllm/model_executor/layers/mamba/short_conv.py
0	9	vllm/model_executor/models/plamo2.py
2	7	vllm/model_executor/models/qwen3_next.py

[48fc8b1e5] Lucas Wilkinson 2025-11-19 [BugFix] Fix async-scheduling + FlashAttn MLA (#28990)
9	6	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/flashattn_mla.py
1	0	vllm/v1/attention/backends/utils.py
7	3	vllm/v1/worker/gpu_model_runner.py

[1ffe934c8] vnadathur 2025-11-19 [torch.compile] caching of config fields should be opt-out by default (#26468)
166	0	tests/config/test_config_utils.py
83	22	vllm/compilation/backends.py
1	1	vllm/compilation/pass_manager.py
23	8	vllm/config/cache.py
22	18	vllm/config/compilation.py
45	47	vllm/config/model.py
35	14	vllm/config/parallel.py
118	1	vllm/config/utils.py
87	82	vllm/envs.py
2	0	vllm/logging_utils/__init__.py
20	0	vllm/logging_utils/lazy.py

[2c8b9182b] Yanan Cao 2025-11-19 [CI] Reorganize compile tests so new tests are automatically included in CI (#28625)
27	30	.buildkite/test-amd.yaml
31	31	.buildkite/test-pipeline.yaml
5	0	tests/compile/README.md
0	0	tests/compile/{piecewise => distributed}/__init__.py
3	3	tests/compile/{ => distributed}/test_async_tp.py
2	2	tests/compile/{ => distributed}/test_fusion_all_reduce.py
1	1	tests/compile/{ => distributed}/test_fusions_e2e.py
2	2	tests/compile/{ => distributed}/test_sequence_parallelism.py
0	0	tests/compile/fullgraph/__init__.py
1	1	tests/compile/{ => fullgraph}/test_basic_correctness.py
0	0	tests/compile/{piecewise => fullgraph}/test_full_cudagraph.py
1	1	tests/compile/{ => fullgraph}/test_full_graph.py
0	0	tests/compile/{ => fullgraph}/test_multimodal_compile.py
0	0	tests/compile/{piecewise => fullgraph}/test_multiple_graphs.py
0	0	tests/compile/{piecewise => fullgraph}/test_simple.py
0	0	tests/compile/{piecewise => fullgraph}/test_toy_llama.py
1	1	vllm/env_override.py

[4f5299f71] Harry Mellor 2025-11-19 Relax Transformers modeling backend MoE experts check (#28952)
3	1	docs/models/supported_models.md
8	1	vllm/model_executor/models/transformers/moe.py

[09540cd91] Didier Durand 2025-11-19 [Doc]: fix typos in various files (#29010)
1	1	docs/deployment/frameworks/skypilot.md
1	1	docs/design/prefix_caching.md
1	1	docs/features/nixl_connector_usage.md
1	1	docs/getting_started/quickstart.md
1	1	tests/v1/ec_connector/integration/README.md
1	1	vllm/multimodal/evs.py

[da2f6800e] Chen Bruce 2025-11-19 [Feat][Perf] Enable deepep-low-latency with round-robin expert placement. (#28449)
11	0	vllm/model_executor/layers/fused_moe/all2all_utils.py
28	2	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
7	2	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
135	22	vllm/model_executor/layers/fused_moe/layer.py
5	2	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
10	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
5	2	vllm/model_executor/layers/quantization/fp8.py
7	3	vllm/model_executor/layers/quantization/modelopt.py

[ba558c029] Tova Movshovitz 2025-11-19 [config] Expose `get_total_num_hidden_layers()` in ModelConfig (#28961)
10	5	vllm/config/model.py

[97cfa99d5] Harry Mellor 2025-11-19 [Docs] Take env var definition out of folded admonition (#29005)
3	5	docs/configuration/env_vars.md

[bbc6c2f1e] j20120307 2025-11-19 [CI/Build] Fix broken build on Apple M1 (#28999)
18	0	csrc/cpu/utils.hpp

[815160958] ihb2032 2025-11-19 refactor(cpu_types_scalar.hpp): Unify scalar loop implementations using unroll_loop (#28847)
87	135	csrc/cpu/cpu_types_scalar.hpp

[fdf93486d] Michael Yao 2025-11-19 [Docs] Clean up moe_kernel_features.md (#28530)
45	47	docs/design/moe_kernel_features.md

[d69062c67] gnovack 2025-11-19 add support for --fully-sharded-loras in fused_moe (#28761)
206	2	tests/lora/test_fused_moe_lora_kernel.py
8	2	tests/lora/test_olmoe_tp.py
32	4	vllm/lora/layers/fused_moe.py
22	2	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
2	0	vllm/lora/punica_wrapper/punica_base.py
4	0	vllm/lora/punica_wrapper/punica_gpu.py

[ae4821a10] Louie Tsai 2025-11-18 Add CPU support model (#28697)
26	0	docs/models/hardware_supported_models/cpu.md

[7ed27f3cb] Didier Durand 2025-11-19 [Doc]: fix typos in various files (#28945)
2	2	docs/design/moe_kernel_features.md
1	1	docs/design/plugin_system.md
1	1	docs/features/quantization/quark.md
1	1	examples/online_serving/prometheus_grafana/README.md
1	1	vllm/engine/arg_utils.py
1	1	vllm/envs.py

[a4511e38d] Michael Goin 2025-11-19 Speed up macOS smoke test (#28954)
3	4	.github/workflows/macos-smoke-test.yml

[71d0ae1c5] Roman Solomatin 2025-11-19 [Misc] Update embedding/cross encoder tests to use `mteb` v2 (#27329)
1	1	requirements/test.in
1	3	requirements/test.txt
114	65	tests/models/language/pooling_mteb_test/mteb_utils.py
21	10	tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py
3	2	tests/models/language/pooling_mteb_test/test_mxbai_rerank.py
3	2	tests/models/language/pooling_mteb_test/test_qwen3_reranker.py

[3d4e7d34b] Lukas Geiger 2025-11-19 [Model][QwenVL] Simplify cos/sin rotary embedding indexing  (#28962)
2	7	vllm/model_executor/models/glm4_1v.py
2	7	vllm/model_executor/models/qwen2_5_vl.py
2	7	vllm/model_executor/models/qwen2_vl.py
2	7	vllm/model_executor/models/qwen3_omni_moe_thinker.py
3	14	vllm/model_executor/models/qwen3_vl.py

[6a25ea5f0] Uranus 2025-11-19 [Docs] Update oneshot imports (#28188)
1	1	docs/features/quantization/fp8.md
1	1	docs/features/quantization/int4.md
1	1	docs/features/quantization/int8.md
1	1	docs/features/quantization/quantized_kvcache.md

[73ff872db] Gleb Kurchanov 2025-11-19 [Bugfix] Fix typo in Qwen3 Next model executor (#28960)
2	2	vllm/model_executor/models/qwen3_next.py

[468a8d72b] Xin Yang 2025-11-18 [Bugfix] Fix FusedMoEModularKernel for triton backend (#28913)
6	4	vllm/model_executor/layers/quantization/mxfp4.py

[4c23690f4] Matthew Bonanni 2025-11-18 [Attention] FlashAttention ViT support, make default backend (#28763)
1	1	cmake/external_projects/vllm_flash_attn.cmake
2	2	tests/kernels/attention/test_flash_attn.py
1	29	tests/kernels/attention/test_mha_attn.py
9	12	vllm/platforms/cuda.py
2	2	vllm/v1/attention/backends/flash_attn.py

[814843e02] Strahinja Stamenkovic 2025-11-19 Enable bitsandbytes quantization on AMD GPUs that use warp size 32 (#27307)
7	4	tests/models/quantization/test_bitsandbytes.py
3	0	vllm/platforms/rocm.py

[20852c8f4] Li, Jiang 2025-11-19 [CPU] Refactor CPU WNA16  (#28826)
5	6	.buildkite/scripts/hardware_ci/run-cpu-test.sh
1	0	cmake/cpu_extension.cmake
1	1	csrc/cpu/cpu_attn_impl.hpp
39	8	csrc/cpu/cpu_types_x86.hpp
402	0	csrc/cpu/cpu_wna16.cpp
3	3	csrc/cpu/dnnl_helper.cpp
245	0	csrc/cpu/micro_gemm/cpu_micro_gemm_amx.hpp
91	0	csrc/cpu/micro_gemm/cpu_micro_gemm_impl.hpp
115	0	csrc/cpu/micro_gemm/cpu_micro_gemm_vec.hpp
16	0	csrc/cpu/torch_bindings.cpp
55	0	csrc/cpu/utils.hpp
1	3	docs/getting_started/installation/cpu.md
0	1	requirements/cpu.txt
23	0	tests/quantization/test_cpu_wna16.py
25	0	vllm/_custom_ops.py
2	0	vllm/config/model.py
0	5	vllm/envs.py
0	49	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
1	1	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
5	0	vllm/model_executor/layers/quantization/__init__.py
625	0	vllm/model_executor/layers/quantization/cpu_wna16.py
1	1	vllm/model_executor/layers/quantization/ipex_quant.py

[40b6b38f2] Jialin Ouyang 2025-11-18 [Core] Switch Flat logprob control from environment variable to SamplingParams (#28914)
1	2	tests/samplers/test_logprobs.py
10	22	tests/test_logprobs.py
0	6	vllm/envs.py
4	6	vllm/logprobs.py
6	0	vllm/sampling_params.py
12	5	vllm/v1/engine/logprobs.py

[da94c7c0e] Jerry Zhang 2025-11-18 Move online quantization to `model.load_weights` (#26327)
1	1	examples/offline_inference/rlhf.py
162	0	examples/offline_inference/rlhf_online_quant.py
11	35	vllm/model_executor/model_loader/default_loader.py
128	77	vllm/model_executor/model_loader/online_quantization.py
8	0	vllm/model_executor/model_loader/utils.py
4	0	vllm/model_executor/models/utils.py

[1395461f5] tomeras91 2025-11-19 [Hybrid][torch.compile] Refactor mamba2 forward to avoid obscuring linear projections under custom op (#28587)
84	72	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	2	vllm/model_executor/models/bamba.py
1	3	vllm/model_executor/models/falcon_h1.py
1	2	vllm/model_executor/models/granitemoehybrid.py
1	2	vllm/model_executor/models/mamba2.py
1	2	vllm/model_executor/models/nemotron_h.py
1	5	vllm/model_executor/models/zamba2.py

[9912b8ccb] Varun Sundar Rabindranath 2025-11-18 [Build] Add OpenAI triton_kernels (#28788)
3	0	.gitignore
5	0	CMakeLists.txt
53	0	cmake/external_projects/triton_kernels.cmake
17	0	setup.py
2	0	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
39	1	vllm/utils/import_utils.py

[49ef847aa] Johnny 2025-11-19 [NVIDIA] Guard SM100 CUTLASS MoE macro to SM100 builds v2 (#28938)
7	7	CMakeLists.txt

[67745d189] Michael Goin 2025-11-18 Supress verbose logs from model_hosting_container_standards (#28949)
4	0	vllm/entrypoints/openai/api_server.py

[2a2d5d278] Kunshang Ji 2025-11-19 Replace `torch.cuda.Event` with `torch.Event` for better hardware compatibility (#26985)
2	2	benchmarks/kernels/benchmark_cutlass_moe_fp8.py
2	2	benchmarks/kernels/benchmark_moe.py
4	4	benchmarks/kernels/benchmark_moe_permute_unpermute.py
2	2	benchmarks/kernels/benchmark_per_token_group_quant.py
2	2	benchmarks/kernels/benchmark_silu_mul_fp8_quant.py
2	2	benchmarks/kernels/benchmark_trtllm_decode_attention.py
2	2	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
2	2	benchmarks/kernels/benchmark_w8a8_block_fp8.py
4	4	tests/kernels/attention/test_merge_attn_states.py
3	3	vllm/v1/kv_offload/worker/cpu_gpu.py
3	3	vllm/v1/worker/cpu_model_runner.py
2	2	vllm/v1/worker/gpu_input_batch.py
6	6	vllm/v1/worker/gpu_model_runner.py
4	4	vllm/v1/worker/ubatching.py
1	8	vllm/v1/worker/xpu_model_runner.py

[c3e297862] Chendi.Xue 2025-11-18 [NIXL] fix cpu PD after physical <> logical block_size PR (#28904)
7	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
1	0	tools/install_nixl_from_source_ubuntu.py
9	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[e4bb2684b] Isotr0py 2025-11-19 [Models] Replace all `nn.Conv2d` with vLLM's Conv2dLayer (#28842)
22	2	vllm/model_executor/layers/conv.py
2	1	vllm/model_executor/models/aimv2.py
2	1	vllm/model_executor/models/blip.py
14	15	vllm/model_executor/models/chameleon.py
8	5	vllm/model_executor/models/deepencoder.py
2	1	vllm/model_executor/models/dots_ocr.py
2	2	vllm/model_executor/models/glm4_1v.py
3	2	vllm/model_executor/models/glm4v.py
2	1	vllm/model_executor/models/idefics2_vision_model.py
2	1	vllm/model_executor/models/intern_vit.py
2	1	vllm/model_executor/models/interns1_vit.py
2	1	vllm/model_executor/models/keye.py
2	1	vllm/model_executor/models/midashenglm.py
2	1	vllm/model_executor/models/moonvit.py
2	1	vllm/model_executor/models/paddleocr_vl.py
3	2	vllm/model_executor/models/pixtral.py
2	1	vllm/model_executor/models/qwen_vl.py
2	1	vllm/model_executor/models/siglip.py
3	2	vllm/model_executor/models/siglip2navit.py
4	3	vllm/model_executor/models/step3_vl.py

[c64c0b78d] Kevin H. Luu 2025-11-18 [chore] Move the rest of wikimedia url to S3 (#28921)
1	1	docs/features/multimodal_inputs.md
1	1	docs/serving/openai_compatible_server.md
2	2	examples/offline_inference/vision_language_pooling.py
1	1	examples/online_serving/openai_chat_completion_client_for_multimodal.py
1	1	examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py
4	4	tests/entrypoints/openai/test_vision.py
4	4	tests/entrypoints/pooling/openai/test_vision_embedding.py
1	1	tests/models/language/pooling/test_mm_classifier_conversion.py
4	4	tests/multimodal/test_utils.py
1	1	tests/utils.py
4	4	tests/v1/entrypoints/openai/serving_responses/test_image.py

[0af3d4f0d] vllmellm 2025-11-19 [FEAT] [AITER] [ROCm] integrate aiter sampling ops (#26084)
77	0	vllm/v1/sample/ops/topk_topp_sampler.py

[da8dadf68] Nick Hill 2025-11-18 [Minor] Rename `ec_producer` field to `is_ec_producer` (#28884)
2	2	vllm/v1/engine/core.py

[f226a3f0c] Nicolò Lucchesi 2025-11-18 [CI][NIXL] Change default `block_size` for tests (#28927)
2	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh

[c2612371a] Luciano Martins 2025-11-18 [Model] Add Gemma3 GGUF multimodal support (#27772)
1	1	requirements/common.txt
115	0	tests/models/multimodal/generation/test_multimodal_gguf.py
8	1	tests/models/quantization/test_gguf.py
19	1	vllm/config/model.py
62	5	vllm/model_executor/layers/quantization/gguf.py
173	13	vllm/model_executor/model_loader/gguf_loader.py
7	3	vllm/model_executor/model_loader/weight_utils.py
116	56	vllm/model_executor/models/gemma3_mm.py
27	0	vllm/model_executor/models/siglip.py
11	0	vllm/transformers_utils/config.py
166	0	vllm/transformers_utils/gguf_utils.py
26	5	vllm/transformers_utils/processor.py
1	0	vllm/transformers_utils/utils.py
19	0	vllm/v1/worker/gpu_model_runner.py

[49a986ecd] Ido Segev 2025-11-18 [Benchmark] multi_turn: Report warmup-inclusive runtime (#28937)
4	0	benchmarks/multi_turn/README.md
49	10	benchmarks/multi_turn/benchmark_serving_multi_turn.py

[f6aa12269] Alex 2025-11-18 [CI Sprint] Quantization CI Cleanup (#24130)
2	2	tests/quantization/test_compressed_tensors.py
8	8	tests/quantization/test_cpu_offload.py
4	2	tests/quantization/test_experts_int8.py
8	5	tests/quantization/test_fp8.py
2	2	tests/quantization/test_ipex_quant.py
1	1	tests/quantization/test_lm_head.py
1	1	tests/quantization/test_modelopt.py
2	1	tests/quantization/test_ptpc_fp8.py
3	3	tests/quantization/test_register_quantization_config.py
1	1	tests/quantization/test_torchao.py

[184b12fdc] Nicolò Lucchesi 2025-11-18 [Bugfix][NIXL] Fix `block_size_ratio` when logical !=physical blocks   (#28925)
12	6	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[b9489f51e] Canlin Guo 2025-11-18 [Model][Perf] Use cos and sin cache in QwenVL (#28798)
5	0	vllm/model_executor/layers/rotary_embedding/base.py
38	50	vllm/model_executor/models/glm4_1v.py
65	58	vllm/model_executor/models/qwen2_5_vl.py
46	89	vllm/model_executor/models/qwen2_vl.py
30	10	vllm/model_executor/models/qwen3_omni_moe_thinker.py
34	10	vllm/model_executor/models/qwen3_vl.py

[285eaa428] Song Zhixin 2025-11-18 [Bugfix] Safeguard against missing backend in AttentionBackendEnum (#28846)
2	1	vllm/attention/layer.py

[439368496] Nick Hill 2025-11-18 [BugFix] Fix PP/async scheduling with pooling models (#28899)
2	1	vllm/v1/engine/core.py
4	4	vllm/v1/executor/ray_executor.py

[896e41ae0] Isotr0py 2025-11-18 [CI/Build] Replace wikipedia url with local server ones (#28908)
11	2	tests/entrypoints/openai/test_metrics.py

[5bb1da519] Kuntai Du 2025-11-18 [MISC] Remove format.sh (#28906)
0	6	format.sh

[5bdd15527] Nick Hill 2025-11-17 [CI] Fix async scheduling + spec decoding test flake (#28902)
5	3	tests/v1/e2e/test_async_scheduling.py

[0168f69e5] Ning Xie 2025-11-18 [Misc] Remove unnecessary parentheses from log statements (#28897)
5	5	vllm/model_executor/models/registry.py

[083cf326d] Didier Durand 2025-11-18 [Doc]: fix typos in various files (#28863)
1	1	docs/contributing/profiling.md
1	1	docs/design/io_processor_plugins.md
2	2	docs/design/logits_processors.md
1	1	docs/features/disagg_prefill.md
1	1	docs/features/lora.md
1	1	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[bf9e1e876] Cyrus Leung 2025-11-18 [Bugfix] Fix wrong CLI defaults for dynamic `SchedulerConfig` fields (#28872)
2	2	tests/entrypoints/openai/test_enable_force_include_usage.py
15	3	vllm/engine/arg_utils.py

[3ddcf4601] Wentao Ye 2025-11-17 [Refactor] Remove Unused Func in Batch Invariant (#28881)
0	73	vllm/model_executor/layers/batch_invariant.py

[d0a73620c] xuebwang-amd 2025-11-18 [ROCm][Quantization] add apply_vllm_mapper in quark config for models like gpt-oss (#28638)
30	5	vllm/model_executor/layers/quantization/quark/quark.py

[88ab591f0] Michael Goin 2025-11-17 Run macos smoke test workflow on main commit (#28752)
10	5	.github/workflows/macos-smoke-test.yml
3	2	requirements/cpu-build.txt

[b6e04390d] Benjamin Bartels 2025-11-18 [Bugfix] Fix Kimi-K2 tool parser concatenated tool calls parsing (#28831)
122	0	tests/tool_use/test_kimi_k2_tool_parser.py
2	1	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py

[552cac95b] Zhuohan Li 2025-11-17 [Misc] Fix wrong comment in scheduler (#28880)
4	4	vllm/v1/core/sched/scheduler.py

[61485844f] Bangsheng Tang 2025-11-17 [BugFix] Corner case that could cause out-of-sync with external launcher mode and dp >1 (#28774)
12	0	vllm/v1/worker/gpu_model_runner.py

[f77bce001] Pranav 2025-11-17 [Model] Add Afmoe architecture implementation (#28332)
1	0	docs/models/supported_models.md
4	0	tests/models/registry.py
711	0	vllm/model_executor/models/afmoe.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
84	0	vllm/transformers_utils/configs/afmoe.py

[a289cc1dd] Wentao Ye 2025-11-17 [Test] Batch Invariant: Rename and organize tests (#27421)
11	0	tests/v1/determinism/conftest.py
1	74	tests/v1/{generation => determinism}/test_batch_invariance.py
161	0	tests/v1/determinism/test_online_batch_invariance.py
1	6	tests/v1/{generation => determinism}/test_rms_norm_batch_invariant.py
74	0	tests/v1/determinism/utils.py

[95ae50b7d] Shreyas Kulkarni 2025-11-17 [Quantization] [Eagle] Add complete quantization support to the draft model in Eagle (#28435)
169	0	tests/model_executor/test_eagle_quantization.py
41	12	vllm/model_executor/models/llama_eagle.py
45	17	vllm/model_executor/models/llama_eagle3.py
27	0	vllm/model_executor/models/utils.py

[7765e5ba7] Nick Hill 2025-11-17 [BugFix] Fix PP performance and PP kv connector output regression  (#28768)
66	84	vllm/v1/engine/core.py
20	1	vllm/v1/executor/ray_executor.py
18	5	vllm/v1/worker/gpu_model_runner.py
1	14	vllm/v1/worker/gpu_worker.py

[d8874c61a] Ronald 2025-11-18 [Core] Async Scheduling X Spec Decoding Compatibility (#24799)
12	26	tests/v1/e2e/test_async_scheduling.py
17	21	vllm/config/speculative.py
17	4	vllm/config/vllm.py
11	4	vllm/v1/core/sched/async_scheduler.py
10	2	vllm/v1/core/sched/scheduler.py
5	1	vllm/v1/engine/core.py
17	0	vllm/v1/engine/processor.py
1	1	vllm/v1/sample/logits_processor/__init__.py
5	2	vllm/v1/spec_decode/eagle.py
3	0	vllm/v1/worker/gpu_input_batch.py
216	37	vllm/v1/worker/gpu_model_runner.py

[f8b19c0ff] Zhewen Li 2025-11-17 [Bugfix] Fix GPT-OSS on AMD after #28603 (#28816)
5	4	.buildkite/test-amd.yaml
2	2	vllm/model_executor/layers/quantization/mxfp4.py

[e42bd8c2e] tiehexue 2025-11-18 Cast return value to int64_t for cache size (#28814)
1	1	csrc/cpu/cpu_attn_impl.hpp

[7f064491f] Roger Wang 2025-11-17 [Bugfix][Perf] Revert applying HF processor on text-only inputs for multimodal models  (#28858)
7	28	tests/test_inputs.py
4	10	vllm/inputs/preprocess.py

[64e39d667] Lucas Wilkinson 2025-11-17 [BugFix] Temporary fix for IMA with MTP = 2 and full-cg (#28315)
64	13	vllm/config/compilation.py
16	0	vllm/v1/worker/gpu_model_runner.py

[1b82fb0ad] Kunshang Ji 2025-11-17 [XPU] work around for sp, avoid custom op import error (#28822)
1	1	vllm/compilation/pass_manager.py

[d4acf518d] Jae-Won Chung 2025-11-17 [Metrics] Fix KV cache usage percent metric multiproc (#28792)
1	0	vllm/v1/metrics/loggers.py

[ab01cd14e] wuyaoxuehun 2025-11-17 [BugFix] Fix glm4_moe_mtp load weights bug (#28805)
3	4	vllm/model_executor/models/glm4_moe_mtp.py

[577bb34ff] Li, Jiang 2025-11-17 [CPU][Bugfix] Fix _to_list in CPU model runner (#28824)
8	0	csrc/cpu/torch_bindings.cpp
0	3	vllm/v1/worker/cpu_model_runner.py

[3380ed5e1] Jee Jee Li 2025-11-17 [Doc] Add llama4 LoRA tag (#28825)
1	1	docs/models/supported_models.md

[6f3741924] Jay Caldwell 2025-11-16 [Bugfix][Model] Prevent special token leakage in KimiK2ToolParser streaming mode (#28543)
593	0	tests/tool_use/test_kimi_k2_tool_parser.py
197	5	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py

[60e089f0b] Xiake Sun 2025-11-17 [ROCm][Qwen3-32B] Fix AITER MHA accuracy issue cause by #25763 (#28670)
2	2	vllm/v1/attention/backends/rocm_aiter_fa.py

[d64429bb3] liuzhenwei 2025-11-17 [NIXL][XPU] update install script of NIXL (#28778)
2	1	docker/Dockerfile.xpu
1	0	tools/install_nixl_from_source_ubuntu.py

[561253b37] jiahanc 2025-11-16 [Performance][Fix] update nvfp4 code to support renorm routing (#28569)
11	7	vllm/model_executor/layers/quantization/modelopt.py
4	1	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[80b6080dd] Nick Hill 2025-11-16 [BugFix] Fix async scheduling + chunked prefill + preemption (#28787)
4	6	tests/v1/e2e/test_async_scheduling.py
1	3	vllm/v1/core/sched/scheduler.py
3	0	vllm/v1/utils.py

[03ee48111] amirkl94 2025-11-16 Feature: Support Relu2 in FusedMoE fp8 cutlass path (#27261)
13	5	tests/kernels/moe/test_flashinfer.py
9	2	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
20	13	vllm/model_executor/layers/quantization/modelopt.py

[5a87076d6] Lukas Geiger 2025-11-16 [Model][QwenVL] Optimize `Qwen2_5_VisionAttention` q,k preparation (#28769)
2	2	vllm/model_executor/models/dots_ocr.py
23	25	vllm/model_executor/models/qwen2_5_vl.py

[ac1daf323] Ning Xie 2025-11-17 fix comment typo (#28802)
1	1	vllm/envs.py

[63fed5550] Didier Durand 2025-11-16 [Doc]: fix typos in various files (#28811)
1	1	docs/contributing/benchmarks.md
1	1	docs/design/cuda_graphs.md
1	1	docs/features/custom_arguments.md
4	4	docs/features/custom_logitsprocs.md
1	1	docs/getting_started/installation/cpu.md
1	1	docs/getting_started/installation/cpu.s390x.inc.md
1	1	docs/getting_started/installation/cpu.x86.inc.md

[8d259fad6] Anna Shors 2025-11-16 Fix gpt oss weight loading with EP + bf16 (#28765)
1	1	vllm/model_executor/models/gpt_oss.py

[3bc117579] scottzh8 2025-11-16 [Bugfix] Fix host and port join for ipv6 in bench serve (#28679)
4	2	vllm/benchmarks/serve.py

[af02c4097] Dezhan 2025-11-16 Fixed gpt-oss _load_weights_other() parameter position bug (#28715)
1	1	vllm/model_executor/models/gpt_oss.py

[b316ac658] Lucia Fang 2025-11-16 [V1] Support MP Executor for multi node distributed inference (#23691)
437	0	tests/distributed/test_multiproc_executor.py
40	0	vllm/config/parallel.py
94	16	vllm/distributed/device_communicators/shm_broadcast.py
73	4	vllm/distributed/parallel_state.py
85	6	vllm/engine/arg_utils.py
27	4	vllm/entrypoints/cli/serve.py
12	3	vllm/v1/engine/utils.py
150	47	vllm/v1/executor/multiproc_executor.py
9	1	vllm/v1/worker/gpu_worker.py
3	1	vllm/v1/worker/worker_base.py

[a55b64635] wang.yuqi 2025-11-16 [Model] Allow users to control skip reading cache per request. (#28194)
27	2	tests/models/language/pooling/test_extract_hidden_states.py
12	0	vllm/pooling_params.py
8	0	vllm/sampling_params.py
5	6	vllm/v1/core/kv_cache_manager.py
15	0	vllm/v1/request.py

[d231876ce] ai-jz 2025-11-15 [Benchmark] Fix client seed synchronization in multi-turn benchmark (#28512)
6	2	benchmarks/multi_turn/benchmark_serving_multi_turn.py

[f849ee739] Bram Wasti 2025-11-16 Adding a benchmark for batch invariance (#28161)
380	0	benchmarks/benchmark_batch_invariance.py

[be263f764] Lucas Wilkinson 2025-11-15 [BugFix] Fix `AssertionError: DCP not support reorder_batch_threshold > 1 now.`  (#28751)
0	10	vllm/v1/worker/gpu_model_runner.py

[2bb4435cb] Didier Durand 2025-11-15 [Doc]: fix typos in various files (#28567)
1	1	docs/design/moe_kernel_features.md
1	1	docs/features/quantization/quark.md
1	1	vllm/compilation/compiler_interface.py
2	2	vllm/compilation/decorators.py
1	1	vllm/v1/worker/gpu_model_runner.py

[07cadab27] Lukas Geiger 2025-11-15 [Model][Qwen3VL] Cache positional embedding indices  (#28475)
34	23	vllm/model_executor/models/qwen3_vl.py

[637f29219] Nick Hill 2025-11-15 [CI] Fix broken pipeline (#28781)
1	1	.buildkite/test-pipeline.yaml

[e439c784f] Eldar Kurtić 2025-11-15 Add support for Eagle with separate lm-head and embed_tokens layers (#28549)
17	16	tests/v1/spec_decode/test_eagle.py
4	0	tests/v1/spec_decode/test_mtp.py
2	1	vllm/model_executor/models/deepseek_eagle.py
2	2	vllm/model_executor/models/deepseek_v2.py
65	5	vllm/model_executor/models/interfaces.py
4	2	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/llama4_eagle.py
2	1	vllm/model_executor/models/llama_eagle.py
2	1	vllm/model_executor/models/llama_eagle3.py
9	3	vllm/model_executor/models/minicpm_eagle.py
23	0	vllm/model_executor/models/utils.py
72	31	vllm/v1/spec_decode/eagle.py

[085a52533] hwhaokun 2025-11-15 [Model] Fix lmhead init bug of bailing_moe (#28777)
1	1	vllm/model_executor/models/bailing_moe.py

[89d367922] Cyrus Leung 2025-11-15 [Doc] Fix failing doc build (#28772)
2	2	docs/README.md
2	2	docs/cli/bench/latency.md
2	2	docs/cli/bench/serve.md
2	2	docs/cli/bench/sweep/plot.md
2	2	docs/cli/bench/sweep/serve.md
2	2	docs/cli/bench/sweep/serve_sla.md
2	2	docs/cli/bench/throughput.md
2	2	docs/cli/chat.md
2	2	docs/cli/complete.md
2	2	docs/cli/run-batch.md
2	2	docs/cli/serve.md
1	1	docs/configuration/serve_args.md
48	29	docs/mkdocs/hooks/generate_argparse.py
1	1	docs/usage/README.md

[cb15ee28d] tingtinggithub 2025-11-15 Allow Gemma3 to take image embeddings (#28483)
1	1	docs/models/supported_models.md
55	22	vllm/model_executor/models/gemma3_mm.py
6	5	vllm/multimodal/parse.py
7	1	vllm/v1/engine/processor.py

[f36292dbe] Angela Yi 2025-11-15 [compile] Enable sequence parallelism matching w/o custom ops enabled  (#27126)
8	6	.buildkite/test-pipeline.yaml
190	38	tests/compile/test_fusions_e2e.py
123	139	tests/compile/test_sequence_parallelism.py
14	1	tests/distributed/test_sequence_parallel.py
111	258	vllm/compilation/sequence_parallelism.py
26	2	vllm/config/vllm.py

[173b356ab] Vadim Gimpelson 2025-11-15 [PERF] Remove TRTLLM Gen attn kernel limitation `max_seq_len <=131072` (#28755)
0	15	vllm/config/vllm.py
2	4	vllm/utils/flashinfer.py

[638e4196d] Cyrus Leung 2025-11-15 [Misc] Make `SchedulerConfig.max_model_len` init-only (#28733)
0	2	tests/kernels/moe/test_batched_moe.py
0	2	tests/kernels/moe/test_block_fp8.py
0	2	tests/kernels/moe/test_block_int8.py
0	2	tests/kernels/moe/test_cutlass_moe.py
0	2	tests/kernels/moe/test_flashinfer.py
0	2	tests/kernels/moe/test_moe.py
0	2	tests/kernels/moe/test_pplx_cutlass_moe.py
0	2	tests/kernels/moe/test_pplx_moe.py
0	2	tests/kernels/moe/test_triton_moe_ptpc_fp8.py
0	2	tests/kernels/quantization/test_block_fp8.py
0	2	tests/kernels/quantization/test_block_int8.py
18	18	vllm/config/scheduler.py
0	1	vllm/config/vllm.py
1	1	vllm/platforms/cpu.py
1	1	vllm/platforms/tpu.py
1	1	vllm/platforms/xpu.py
1	1	vllm/v1/core/sched/scheduler.py

[1ec978c20] Zhewen Li 2025-11-15 [Kernel][Moe Configs] llama4 maverick fp8 moe config tp8 on mi325 (#28709)
164	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json

[74b5267d3] Jane (Yuan) Xu 2025-11-15 Use narrow over indexing in `hadacore_transform` to prep for ABI stable (#28756)
1	1	csrc/quantization/hadamard/hadacore/hadamard_transform_cuda.cu

[dd6ac1c2b] Zhuohan Li 2025-11-14 [RL] [V1] Remove unused device argument from reset_kv_cache (#28766)
1	1	vllm/engine/protocol.py
2	3	vllm/entrypoints/llm.py
3	7	vllm/entrypoints/openai/api_server.py
2	4	vllm/v1/engine/async_llm.py
1	2	vllm/v1/engine/llm_engine.py

[98b4d389e] Cyrus Leung 2025-11-15 [Redo] #26368 (#28771)
2	1	tests/v1/core/test_async_scheduler.py
4	2	tests/v1/core/test_priority_scheduler_random.py
50	38	tests/v1/core/test_scheduler.py
4	3	tests/v1/kv_connector/unit/test_nixl_connector.py
2	1	tests/v1/kv_connector/unit/utils.py
4	1	tests/v1/spec_decode/test_eagle.py
9	9	tests/v1/spec_decode/test_ngram.py
2	2	vllm/v1/core/sched/scheduler.py
2	2	vllm/v1/outputs.py
3	5	vllm/v1/sample/rejection_sampler.py
3	4	vllm/v1/spec_decode/eagle.py
3	3	vllm/v1/spec_decode/ngram_proposer.py
6	4	vllm/v1/spec_decode/suffix_decoding.py
23	13	vllm/v1/worker/gpu_model_runner.py
5	3	vllm/v1/worker/tpu_model_runner.py

[6965ef436] Varun Sundar Rabindranath 2025-11-15 [Performance][DeepGEMM] Estimate expected_m (#28694)
34	12	tests/kernels/moe/test_deepep_deepgemm_moe.py
4	0	vllm/forward_context.py
35	5	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[c9e665852] Chendi.Xue 2025-11-14 [NIXL] heterogeneous block_size support (#26759)
4	0	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
3	0	tests/v1/kv_connector/unit/test_nixl_connector.py
250	59	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[363aaeef0] Mohammad Othman 2025-11-15 Fix IntermediateTensors initialization and add type hints (#28743)
6	1	vllm/sequence.py

[ac86bff8c] Nick Hill 2025-11-14 Revert "[Core] Performance: Use list[np.ndarray] instead of list[list… (#28773)
1	2	tests/v1/core/test_async_scheduler.py
32	44	tests/v1/core/test_scheduler.py
1	2	tests/v1/kv_connector/unit/utils.py
1	4	tests/v1/spec_decode/test_eagle.py
9	9	tests/v1/spec_decode/test_ngram.py
2	2	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/outputs.py
5	3	vllm/v1/sample/rejection_sampler.py
4	3	vllm/v1/spec_decode/eagle.py
3	3	vllm/v1/spec_decode/ngram_proposer.py
4	6	vllm/v1/spec_decode/suffix_decoding.py
13	23	vllm/v1/worker/gpu_model_runner.py

[edfe49818] Michael Goin 2025-11-14 [Bugfix] Build hadacore kernels on >SM90 (#28748)
1	1	CMakeLists.txt

[f05d474c8] Lukas Geiger 2025-11-15 [Model][Qwen3VL] Use `mm_position` to compute mrope positions (#28730)
31	56	vllm/model_executor/models/qwen3_vl.py

[9fc81ec76] QiliangCui 2025-11-14 [TPU] Fix import error in tpu launch (#28758)
9	2	vllm/platforms/tpu.py

[186352b27] Jialin Ouyang 2025-11-14 [Core] Performance: Use list[np.ndarray] instead of list[list[int]] for output tokens for GC optimization (#26368)
2	1	tests/v1/core/test_async_scheduler.py
44	32	tests/v1/core/test_scheduler.py
2	1	tests/v1/kv_connector/unit/utils.py
4	1	tests/v1/spec_decode/test_eagle.py
9	9	tests/v1/spec_decode/test_ngram.py
2	2	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/outputs.py
3	5	vllm/v1/sample/rejection_sampler.py
3	4	vllm/v1/spec_decode/eagle.py
3	3	vllm/v1/spec_decode/ngram_proposer.py
6	4	vllm/v1/spec_decode/suffix_decoding.py
23	13	vllm/v1/worker/gpu_model_runner.py

[58e61e56b] Nick Hill 2025-11-14 [Test] Rework e2e async scheduling tests (#28744)
268	90	tests/v1/e2e/test_async_scheduling.py

[75f01b9d3] Gregory Shtrasberg 2025-11-14 [ROCm][CI/Build] Upgrade to ROCm 7.1 and AITER main (#28753)
5	2	docker/Dockerfile.rocm_base

[ba041d980] rasmith 2025-11-14 [Log] Save profiler results to file instead of stdout (#28144)
13	8	vllm/v1/worker/gpu_worker.py

[e0c910bb8] Thomas Parnell 2025-11-14 [Hybrid] [Kernel] Fix chunk scan kernel when BLOCK_SIZE_DSTATE > 128 (#28295)
1	1	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py

[bf3ffb61e] Benjamin Chislett 2025-11-14 [Bugfix] Fix ChunkedLocalAttention CUDA Graph setting (#28739)
16	3	vllm/attention/layers/chunked_local_attention.py

[e5c78956c] Alexander Matveev 2025-11-14 [Bugfix] Fix incorrect use of hidden_states for shared_experts due to do_naive_dispatch_combine (#28740)
4	2	vllm/model_executor/layers/fused_moe/layer.py

[2e0ad629b] Laith Sakka 2025-11-14 Avoid bytecode hook and simplify TorchCompileWrapperWithCustomDipatch (#25110)
10	1	tests/compile/piecewise/test_multiple_graphs.py
3	0	tests/compile/piecewise/test_simple.py
8	1	tests/compile/piecewise/test_toy_llama.py
115	40	tests/compile/test_wrapper.py
10	0	tests/models/multimodal/generation/test_qwen2_5_vl.py
8	0	tests/v1/e2e/test_spec_decode.py
115	119	vllm/compilation/decorators.py
141	71	vllm/compilation/wrapper.py
6	0	vllm/envs.py
6	4	vllm/v1/worker/tpu_model_runner.py

[5a84b76b8] Gregory Shtrasberg 2025-11-14 [ROCm][CI/Build] Change install location of uv (#28741)
1	4	docker/Dockerfile.rocm

[0de4f217a] Marcin Ostrowski 2025-11-14 [Bugfix] TypeError: 'NoneType' object is not callable (#27410)
1	1	tests/v1/core/test_kv_cache_utils.py

[f08eab2ac] Michael Goin 2025-11-14 [CI] Fix macos smoke test uv cache issue (#28736)
4	1	.github/workflows/macos-smoke-test.yml

[8977ffb5e] Sage Moore 2025-11-14 [ROCm][Bugfix] Fix compilation errors with fused_qknorm_rope_kernel.cu (#28682)
10	0	csrc/fused_qknorm_rope_kernel.cu

[fd4555089] Andrey Khalyavin 2025-11-14 [BugFix] Fix misprint introduced by modular_kernel refactoring. (#28728)
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py

[cec275efc] GuanH 2025-11-15 [Bugfix] resolve Qwen3-VL GPTQModel quantized model loading failure (#28663)
3	1	vllm/model_executor/models/qwen3_vl.py
3	2	vllm/model_executor/models/utils.py

[e2741f6cb] Cyrus Leung 2025-11-15 [Chore] Rename `SchedulerConfig.chunked_prefill_enabled` (#28735)
0	1	tests/v1/core/test_scheduler.py
4	6	tests/v1/e2e/test_spec_decode.py
1	1	tests/v1/engine/test_engine_core.py
8	3	vllm/config/scheduler.py
3	3	vllm/config/vllm.py
1	1	vllm/platforms/cpu.py
1	1	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/engine/core.py
2	2	vllm/v1/worker/gpu_model_runner.py

[67187554d] Harry Mellor 2025-11-14 [Docs] Enable some more markdown lint rules for the docs (#28731)
0	3	.markdownlint.yaml
0	2	docs/contributing/benchmarks.md
1	1	docs/contributing/ci/update_pytorch_version.md
2	2	docs/deployment/frameworks/chatbox.md
3	3	docs/deployment/frameworks/dify.md
4	4	docs/design/fused_moe_modular_kernel.md

[a425dc256] TJian 2025-11-14 [Bugfix] [ROCm] [AITER]: Fix aiter block quant not compatible with torch compile dynamo (#28716)
137	0	tests/rocm/aiter/test_grouped_quant.py
42	6	vllm/_aiter_ops.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[964d65dee] Fardin Hoque 2025-11-14 LLaMA4 LoRA Adapter Enablement (#28602)
34	2	vllm/model_executor/models/mllama4.py

[9261eb3dc] Chen Wang 2025-11-14 docs(lora_resolvers): clarify multi-resolver order and storage path requirement (#28153)
2	0	.markdownlint.yaml
4	1	docs/.nav.yml
220	0	docs/design/lora_resolver_plugins.md
0	16	vllm/plugins/lora_resolvers/README.md

[cdd702596] czhu-cohere 2025-11-14 [kernel] Improve FP8 PTPC on Hopper for larger shapes (#28692)
27	0	csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm90_fp8_dispatch.cuh

[085424808] Julien Denize 2025-11-14 Remove audio optional dependency for mistral-common (#28722)
1	1	docs/contributing/model/transcription.md
3	0	docs/models/supported_models.md
1	0	examples/offline_inference/audio_language.py
1	1	requirements/common.txt

[a17e36f22] Mohammad Othman 2025-11-14 Fix typo in comment: existance -> existence (#28737)
1	1	vllm/_aiter_ops.py

[8cc40f899] Matthew Bonanni 2025-11-14 [Attention] Bump FA for removed method (#28429)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[6f1e7f722] Nicolò Lucchesi 2025-11-14 [DisaggEverything] Tokens in<>out `/generate` endpoint (#24261)
49	0	examples/online_serving/token_generation_client.py
4	0	requirements/docs.txt
262	0	tests/entrypoints/openai/test_serving_tokens.py
5	0	vllm/engine/arg_utils.py
81	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/entrypoints/openai/cli_args.py
77	0	vllm/entrypoints/openai/protocol.py
4	0	vllm/entrypoints/openai/serving_engine.py
269	0	vllm/entrypoints/openai/serving_tokens.py
2	0	vllm/sampling_params.py
1	7	vllm/v1/engine/__init__.py
63	2	vllm/v1/serial_utils.py

[d54a18a47] Michael Goin 2025-11-14 [CI][CPU] Smoke test for Apple Silicon using GHA MacOS runner (#28688)
73	0	.github/workflows/macos-smoke-test.yml

[5f3cd7f7f] Harry Mellor 2025-11-14 [Docs] Update the name of `Transformers backend` -> `Transformers modeling backend` (#28725)
1	1	.github/CODEOWNERS
1	1	docs/contributing/model/README.md
2	2	docs/deployment/frameworks/hf_inference_endpoints.md
13	13	docs/models/supported_models.md
2	2	tests/models/test_transformers.py
4	4	vllm/config/model.py
1	1	vllm/lora/layers/base_linear.py
2	2	vllm/model_executor/models/adapters.py
2	2	vllm/model_executor/models/transformers/__init__.py
5	4	vllm/model_executor/models/transformers/base.py
1	1	vllm/model_executor/models/transformers/causal.py
1	1	vllm/model_executor/models/transformers/legacy.py
2	2	vllm/model_executor/models/transformers/moe.py
7	5	vllm/model_executor/models/transformers/multimodal.py
1	1	vllm/model_executor/models/transformers/pooling.py
1	1	vllm/model_executor/models/transformers/utils.py

[c934caee8] dongbo910220 2025-11-15 [Fix] improve aspect ratio in dummy image generation and add common  VLM tests for PaddleOCR-VL (#28711)
18	0	tests/models/multimodal/generation/test_common.py
1	2	vllm/model_executor/models/paddleocr_vl.py

[3f8a87406] Duncan Moss 2025-11-14 [Kernels] Enable FlashInfer FP8 Blockscale on SM90 (for TEP DSR1) (#27134)
22	1	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
97	50	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
36	12	vllm/model_executor/layers/quantization/fp8.py
24	5	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[511a6b611] Cyrus Leung 2025-11-14 [Config] Clean up SchedulerConfig initialization (#28665)
6	1	tests/models/language/generation/test_hybrid.py
2	0	tests/v1/core/test_scheduler.py
1	0	tests/v1/sample/test_logprobs.py
33	69	vllm/config/scheduler.py
135	73	vllm/engine/arg_utils.py
1	3	vllm/platforms/cpu.py
1	3	vllm/platforms/tpu.py
1	3	vllm/platforms/xpu.py
1	10	vllm/utils/__init__.py

[96b23b8e3] Nicolò Lucchesi 2025-11-14 [Bugfix][Nixl] Fix kernel physical<>logical block_size issue  (#28677)
4	2	tests/v1/worker/test_gpu_model_runner.py
57	10	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
12	5	vllm/v1/worker/block_table.py

[433c0f867] zhaozx-cn 2025-11-14 [Model] Fix bailing_moe accuracy problem (#28277)
3	2	vllm/model_executor/models/bailing_moe.py

[8d3748d3c] Fasal Shah 2025-11-14 [Doc] Fix macOS installation dependency resolution issue (#26721)
6	1	docs/getting_started/installation/cpu.apple.inc.md

[db56a5997] Lucas Wilkinson 2025-11-14 [BugFix] Fix FA3 IMA with FULL_AND_PIECEWISE and cascade attention (default) (#28702)
1	0	tests/kernels/attention/test_cascade_flash_attn.py
4	2	vllm/v1/attention/backends/flash_attn.py

[9324e1027] Yong Hoon Shin 2025-11-14 Fix KV sharing fast prefill with cudagraph enabled (#28537)
14	43	tests/v1/e2e/test_kv_sharing_fast_prefill.py
2	13	vllm/v1/attention/backends/utils.py
1	1	vllm/v1/worker/gpu_model_runner.py

[4516d44b7] Jingchun Gao 2025-11-14 [DCP] Support Decode Context Parallel (DCP) for GQA with Flashinfer (#25438)
15	2	tests/distributed/test_context_parallel.py
8	0	vllm/config/model.py
9	0	vllm/utils/flashinfer.py
294	49	vllm/v1/attention/backends/flashinfer.py
5	0	vllm/v1/executor/multiproc_executor.py

[41b92f7d3] Shanshan Shen 2025-11-14 [Model][MM] Extract conv layer as CustomOp (#28455)
236	0	vllm/model_executor/layers/conv.py
2	1	vllm/model_executor/models/clip.py
8	9	vllm/model_executor/models/glm4_1v.py
8	10	vllm/model_executor/models/qwen2_5_vl.py
8	10	vllm/model_executor/models/qwen2_vl.py
7	10	vllm/model_executor/models/qwen3_omni_moe_thinker.py
8	10	vllm/model_executor/models/qwen3_vl.py
0	16	vllm/model_executor/models/vision.py

[360bd8762] Srreyansh Sethi 2025-11-14  [Frontend] Added chat-style multimodal support to /classify. (#27516)
10	0	tests/entrypoints/pooling/openai/test_classification.py
95	0	tests/entrypoints/pooling/openai/test_vision_classification.py
3	0	vllm/entrypoints/openai/api_server.py
113	3	vllm/entrypoints/openai/protocol.py
81	19	vllm/entrypoints/openai/serving_classification.py
16	5	vllm/entrypoints/openai/serving_engine.py

[ecf8230d4] lyn610 2025-11-14 [Metrics] Log number of preempted requests (#28522)
19	4	vllm/v1/metrics/loggers.py

[8cfbe89b9] Xing Liu 2025-11-14 [Misc] fix comment in test_envs (#28529)
1	1	tests/test_envs.py

[fd75d3e8c] Boyuan Feng 2025-11-14 [Minor] avoid register new custom and just import silly_attn (#28578)
3	9	tests/compile/test_config.py

[c9a3a0214] Michael Goin 2025-11-14 Add output token counting to gsm8k eval (#28594)
23	9	tests/evals/gsm8k/gsm8k_eval.py

[bc3e43069] Nick Hill 2025-11-14 [BugFix] Fix multi-modal async scheduling race condition (#28706)
3	3	vllm/distributed/device_communicators/shm_object_storage.py
19	7	vllm/v1/serial_utils.py
21	21	vllm/v1/worker/gpu_model_runner.py

[c36bcfe6b] Jiangyun Zhu 2025-11-14 [Bugfix] fix dots.ocr pp support (#28705)
4	0	vllm/model_executor/models/dots_ocr.py

[529cea343] Yan Ma 2025-11-14 use default CCL_ZE_IPC_EXCHANGE (#28700)
0	2	vllm/v1/worker/xpu_worker.py

[93103575c] rasmith 2025-11-14 [BugFix][CI/Build][ROCM] Fix import error and apply assert in appropriate case in test_struct_output_generate (#28311)
8	3	tests/v1/entrypoints/llm/test_struct_output_generate.py

[15ae8e078] rasmith 2025-11-14 [Bugfix][CI/Test][Spec Decode] Fix illegal memory access in offline_inference/spec_decode.py (Issue  27619) (#28432)
4	2	vllm/attention/ops/triton_reshape_and_cache_flash.py

[0b2549899] haoyangli-amd 2025-11-14 [Misc] add ignore mapper for quark quantization (#28275)
9	3	vllm/model_executor/layers/quantization/quark/quark.py

[0aecd9138] Roger Wang 2025-11-13 [Misc] Update xformers to 0.33.0.post1 (#28678)
1	1	requirements/cuda.txt

[da14ae0fa] Kunshang Ji 2025-11-14 [XPU][CI]disable lm cache uts (#28696)
1	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh

[01bea115c] Cyrus Leung 2025-11-14 [Misc] Remove `warn_for_unimplemented_methods` (#28613)
0	45	vllm/utils/__init__.py
0	2	vllm/v1/worker/worker_base.py

[b39a5026e] Bradley D 2025-11-13 [ci][amd] fix basic models extra init test (#28676)
5	5	.buildkite/scripts/hardware_ci/run-amd-test.sh

[622e6106a] Michael Goin 2025-11-13 [CPU][Bugfix] Fix Apple Silicon M1 compilation failure (#28681)
28	0	csrc/cpu/cpu_attn_impl.hpp

[2aa75c752] Sage Moore 2025-11-13 [ROCm] Bump up the version of amd-smi to 6.4.3 (#28680)
1	1	requirements/rocm-build.txt

[4d5943bda] Hank_ 2025-11-14 [quantization][config] enable override existing quant_config (#28510)
9	3	tests/quantization/test_register_quantization_config.py
11	3	vllm/model_executor/layers/quantization/__init__.py

[f2b8e1c55] Alexei-V-Ivanov-AMD 2025-11-13 Mirrored test group definitions for AMD (2025-11-11) (#28573)
153	10	.buildkite/test-amd.yaml

[6e25b1cdd] Mark McLoughlin 2025-11-13 [KV Connector] Test async mode in scheduler tests (#28550)
63	37	tests/v1/core/test_scheduler.py
18	6	tests/v1/core/utils.py
84	2	tests/v1/kv_connector/unit/utils.py

[e64011f29] Wentao Ye 2025-11-13 [CI] Bug: Fix ci entrypoint pooling (#28684)
1	0	vllm/v1/engine/processor.py

[1b622deba] Simon Mo 2025-11-13 [Misc] Update CODEOWNERS for simon-mo and comaniac (#28675)
8	8	.github/CODEOWNERS

[faed7bf07] Kebe 2025-11-14 [Bugfix] [CPU] bump torch to 2.9.0 for Darwin to fix segmentation fault (#27791)
1	1	requirements/cpu.txt

[262d263f6] Yanan Cao 2025-11-13 [Bugfix] Eliminate tuple inputs to submodules in graph partitioning (#28533)
1	0	.buildkite/test-pipeline.yaml
124	0	tests/compile/test_graph_partition.py
15	2	vllm/compilation/backends.py

[968060c15] Qiu 2025-11-14 [bugfix] correct local_chunk_len for DCP in reorg_kvcache with long context (#28526)
25	4	vllm/v1/attention/backends/mla/common.py

[5d6ce2b96] elvischenv 2025-11-14 [Perf] Support stream interval for reducing host overhead (#27869)
16	2	tests/v1/engine/test_output_processor.py
6	0	vllm/config/scheduler.py
6	0	vllm/engine/arg_utils.py
2	1	vllm/v1/engine/async_llm.py
2	1	vllm/v1/engine/llm_engine.py
35	1	vllm/v1/engine/output_processor.py

[f9f3b596f] Matthew Bonanni 2025-11-13 [Attention][Bugfix] Fix FA sink support (#28660)
6	0	vllm/v1/attention/backends/flash_attn.py

[119c4927b] Yannick Schnider 2025-11-13 [Bugfix] Fix validate model input for decoder models (#27099)
63	0	tests/v1/e2e/test_context_length.py
15	0	vllm/v1/engine/processor.py

[fe1cd7704] Varun Sundar Rabindranath 2025-11-13 [Performance][B200] silu_mul_quant: pack scales in int32 (#28358)
121	43	csrc/quantization/activation_kernels.cu
13	0	tests/conftest.py
9	9	tests/kernels/moe/test_deepep_deepgemm_moe.py
1	1	tests/kernels/moe/test_deepep_moe.py
237	74	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
57	19	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
23	0	vllm/utils/deep_gemm.py

[fdfd5075a] Johnny Yang 2025-11-13 [TPU] patch TPU wheel build script to resolve metadata issue (#27279)
3	1	setup.py
28	0	tools/vllm-tpu/build.sh

[327c0a9a2] Nick Hill 2025-11-13 [BugFix] Ensure `EngineArgs.create_engine_config` is idempotent (#28515)
9	10	vllm/engine/arg_utils.py

[06c4873d9] Jane (Yuan) Xu 2025-11-13 Rewrite C++ meta funcs to Python (#28595)
0	16	csrc/quantization/gptq_marlin/awq_marlin_repack.cu
0	16	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
38	1	vllm/_custom_ops.py

[d3387750f] Roger Wang 2025-11-13 [Misc] Turn off encoder torch compile by default (#28634)
6	3	tests/compile/test_multimodal_compile.py
2	0	tests/models/multimodal/generation/test_common.py
3	2	vllm/config/compilation.py

[b230286fb] Harry Mellor 2025-11-13 Fix `get_num_experts` when config sets it explicitly to `None` (#28652)
2	1	vllm/config/model.py

[3035d1a16] Yuanping Song 2025-11-13 [BugFix] DeepSeek-OCR: apply NoRepeatNGramLogitsProcessor to greedy path (#28617)
1	1	vllm/model_executor/models/deepseek_ocr.py

[07a606aa7] Huamin Li 2025-11-13 [CI Failure] Fix backend selection for encoder-only models (#28534)
14	0	vllm/attention/backends/abstract.py
1	0	vllm/attention/layer.py
5	1	vllm/attention/layers/encoder_only_attention.py
5	0	vllm/attention/selector.py
1	0	vllm/platforms/cpu.py
10	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
1	0	vllm/platforms/rocm.py
1	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py
11	0	vllm/v1/attention/backends/cpu_attn.py
12	0	vllm/v1/attention/backends/flash_attn.py
7	0	vllm/v1/attention/backends/flex_attention.py
5	5	vllm/v1/attention/backends/mla/flashmla_sparse.py

[a7791eac9] amdfaa 2025-11-13 [CI/Build] Install uv for AMD MI300: Language Models Tests (Hybrid) %N (#28142)
27	8	docker/Dockerfile.rocm

[8da2f28f5] Pleaplusone 2025-11-13 [ROCm][BugFix]Fix `get_cu_count` in rocm_aiter_fa.py (#28618)
2	1	vllm/v1/attention/backends/rocm_aiter_fa.py

[86d15bfd8] Akash kaothalkar 2025-11-13 [Hardware][PowerPC] Fix fp16 compilation error for Power in cpu attention backend and bump oneDNN version (#28535)
2	2	cmake/cpu_extension.cmake
2	0	csrc/cpu/cpu_attn_impl.hpp

[c9fe6abe7] Fanli Lin 2025-11-13 [Bugfix] Fix FPS value type for Qwen2.5-Omni video processing (#28630)
1	1	examples/offline_inference/vision_language.py

[c47b6c85a] zofia 2025-11-13 [XPU] add sym params to IPEXConfig (#28611)
18	2	vllm/model_executor/layers/quantization/ipex_quant.py

[c428e8d80] baonudesifeizhai 2025-11-13 Fix io processor pooling  #28273 (#28484)
5	1	vllm/entrypoints/openai/serving_pooling.py

[5e973209a] Zijing Liu 2025-11-13 [BugFix] Fix type error when assign a trition kernel tensor to a torch.nn.Parameter (#28603)
2	2	vllm/model_executor/layers/quantization/mxfp4.py

[e63fd4456] Di Wu 2025-11-13 Fix: Correctly filter special tokens in benchmark_prefix_caching (#28615)
1	1	benchmarks/benchmark_prefix_caching.py

[11ac9ddd0] Yong Hoon Shin 2025-11-12 Support all interleaved layer types (#28485)
1	2	vllm/transformers_utils/config.py

[5c9ad138d] Chauncey 2025-11-13 [Frontend] supports interleaved thinking (#28531)
118	0	docs/features/interleaved_thinking.md
1	0	examples/online_serving/openai_chat_completion_client_with_tools.py
16	1	vllm/entrypoints/chat_utils.py

[fa183e927] Jiangyun Zhu 2025-11-13 [Bugfix] fix kimi-linear crash (#28445)
12	9	vllm/model_executor/layers/kda.py

[4ab34f6ef] usberkeley 2025-11-13 Add NUMA node validation for CPU thread binding (#28555)
10	0	csrc/cpu/utils.cpp

[c33b87e77] Huy Do 2025-11-12 Use official xformers-0.0.33 built for PT 2.9 (#28600)
1	2	requirements/cuda.txt

[4504e8029] tjandy98 2025-11-13 [Bugfix] Prevent crash on empty grammar string (#28210)
20	0	tests/v1/entrypoints/openai/test_chat_completion.py
6	0	vllm/v1/engine/processor.py

[ca00b1bfc] Pleaplusone 2025-11-13 [ROCm][BugFix] Remove the usage of `device_info` from aiter (#28383)
5	6	vllm/v1/attention/backends/rocm_aiter_fa.py

[d44fbbab0] Radu Salavat 2025-11-12 [build][cmake]: Bundle static ACL and torch libgomp for CPU extension builds (#28059)
58	20	cmake/cpu_extension.cmake

[7e082bc14] Lucia Fang 2025-11-12 Support DeepEP for Kimi-k2-thinking through enabling gemm selection for compressed-tensor marlin wna16 (#28574)
68	7	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
50	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[dbbe0c756] Fanli Lin 2025-11-13 [XPU] Support Triton path for LoRA operations on XPU   (#28511)
1	0	vllm/lora/ops/triton_ops/lora_expand_op.py
1	0	vllm/lora/ops/triton_ops/lora_shrink_op.py
5	1	vllm/platforms/xpu.py

[7dca0c90c] Pleaplusone 2025-11-13 [BugFix][ROCm] Fix `get_cu_count` missing variable error (#28608)
1	1	vllm/utils/platform_utils.py

[1a0b157a2] Andrew Xia 2025-11-12 [Frontend][responsesAPI][1/n] convert responses API tool input to chat completions tool format (#28231)
30	0	tests/entrypoints/test_responses_utils.py
4	1	tests/v1/entrypoints/openai/serving_responses/conftest.py
1	0	tests/v1/entrypoints/openai/serving_responses/test_function_call.py
9	20	vllm/entrypoints/openai/serving_responses.py
32	0	vllm/entrypoints/responses_utils.py

[7c38ed0f1] Andrew Xia 2025-11-12 [Frontend] split append tool output (#28333)
3	0	tests/entrypoints/openai/test_serving_responses.py
69	62	vllm/entrypoints/context.py
1	1	vllm/entrypoints/openai/serving_engine.py

[a1d3866dd] Jialin Ouyang 2025-11-12 [n-gen] DO NOT repeatedly return finished child requests (#28591)
103	0	tests/v1/engine/test_parallel_sampling.py
12	3	vllm/v1/engine/parallel_sampling.py

[97d1c9930] Harry Mellor 2025-11-13 Rename clashing method names for vLLM model protocol (#27583)
2	2	docs/contributing/model/basic.md
3	3	docs/contributing/model/multimodal.md
4	4	vllm/model_executor/models/apertus.py
4	4	vllm/model_executor/models/arcee.py
4	4	vllm/model_executor/models/arctic.py
3	3	vllm/model_executor/models/aria.py
1	1	vllm/model_executor/models/aya_vision.py
4	4	vllm/model_executor/models/baichuan.py
4	4	vllm/model_executor/models/bailing_moe.py
4	4	vllm/model_executor/models/bamba.py
7	7	vllm/model_executor/models/bert.py
3	3	vllm/model_executor/models/bert_with_rope.py
1	1	vllm/model_executor/models/blip2.py
4	4	vllm/model_executor/models/bloom.py
4	4	vllm/model_executor/models/chameleon.py
4	4	vllm/model_executor/models/chatglm.py
6	6	vllm/model_executor/models/clip.py
1	1	vllm/model_executor/models/cohere2_vision.py
4	4	vllm/model_executor/models/commandr.py
4	4	vllm/model_executor/models/dbrx.py
3	3	vllm/model_executor/models/deepseek_eagle.py
3	3	vllm/model_executor/models/deepseek_mtp.py
1	3	vllm/model_executor/models/deepseek_ocr.py
4	4	vllm/model_executor/models/deepseek_v2.py
1	1	vllm/model_executor/models/deepseek_vl2.py
4	4	vllm/model_executor/models/dots1.py
3	3	vllm/model_executor/models/dots_ocr.py
4	4	vllm/model_executor/models/ernie45_moe.py
4	6	vllm/model_executor/models/ernie45_vl.py
4	4	vllm/model_executor/models/ernie45_vl_moe.py
3	3	vllm/model_executor/models/ernie_mtp.py
4	4	vllm/model_executor/models/exaone.py
4	4	vllm/model_executor/models/exaone4.py
4	4	vllm/model_executor/models/falcon.py
4	4	vllm/model_executor/models/falcon_h1.py
1	1	vllm/model_executor/models/fuyu.py
4	4	vllm/model_executor/models/gemma.py
4	4	vllm/model_executor/models/gemma2.py
4	4	vllm/model_executor/models/gemma3.py
1	1	vllm/model_executor/models/gemma3_mm.py
6	6	vllm/model_executor/models/gemma3n.py
6	6	vllm/model_executor/models/gemma3n_mm.py
2	2	vllm/model_executor/models/glm4.py
1	3	vllm/model_executor/models/glm4_1v.py
4	4	vllm/model_executor/models/glm4_moe.py
3	3	vllm/model_executor/models/glm4_moe_mtp.py
2	2	vllm/model_executor/models/glm4v.py
6	6	vllm/model_executor/models/gpt2.py
4	4	vllm/model_executor/models/gpt_bigcode.py
4	4	vllm/model_executor/models/gpt_j.py
4	4	vllm/model_executor/models/gpt_neox.py
4	4	vllm/model_executor/models/gpt_oss.py
4	4	vllm/model_executor/models/granite.py
4	4	vllm/model_executor/models/granite_speech.py
4	4	vllm/model_executor/models/granitemoe.py
4	4	vllm/model_executor/models/granitemoehybrid.py
4	4	vllm/model_executor/models/granitemoeshared.py
4	4	vllm/model_executor/models/grok1.py
4	4	vllm/model_executor/models/hunyuan_v1.py
1	1	vllm/model_executor/models/hyperclovax_vision.py
3	3	vllm/model_executor/models/idefics3.py
19	13	vllm/model_executor/models/interfaces.py
23	20	vllm/model_executor/models/interfaces_base.py
4	4	vllm/model_executor/models/internlm2.py
4	4	vllm/model_executor/models/interns1.py
4	4	vllm/model_executor/models/internvl.py
4	4	vllm/model_executor/models/jais.py
4	4	vllm/model_executor/models/jamba.py
1	3	vllm/model_executor/models/keye.py
4	4	vllm/model_executor/models/kimi_linear.py
1	1	vllm/model_executor/models/kimi_vl.py
4	4	vllm/model_executor/models/lfm2.py
4	4	vllm/model_executor/models/lfm2_moe.py
4	4	vllm/model_executor/models/llama.py
3	3	vllm/model_executor/models/llama4_eagle.py
3	3	vllm/model_executor/models/llama_eagle.py
4	4	vllm/model_executor/models/llama_eagle3.py
1	1	vllm/model_executor/models/llava.py
4	4	vllm/model_executor/models/llava_next.py
1	1	vllm/model_executor/models/llava_next_video.py
1	1	vllm/model_executor/models/llava_onevision.py
4	4	vllm/model_executor/models/longcat_flash.py
4	4	vllm/model_executor/models/mamba.py
4	4	vllm/model_executor/models/mamba2.py
1	1	vllm/model_executor/models/midashenglm.py
1	1	vllm/model_executor/models/mimo.py
3	3	vllm/model_executor/models/mimo_mtp.py
4	4	vllm/model_executor/models/minicpm.py
4	4	vllm/model_executor/models/minicpm_eagle.py
1	1	vllm/model_executor/models/minicpmv.py
4	4	vllm/model_executor/models/minimax_m2.py
3	3	vllm/model_executor/models/minimax_text_01.py
3	3	vllm/model_executor/models/minimax_vl_01.py
1	1	vllm/model_executor/models/mistral3.py
4	4	vllm/model_executor/models/mixtral.py
1	1	vllm/model_executor/models/mllama4.py
7	7	vllm/model_executor/models/modernbert.py
2	2	vllm/model_executor/models/molmo.py
4	4	vllm/model_executor/models/mpt.py
3	3	vllm/model_executor/models/nano_nemotron_vl.py
4	4	vllm/model_executor/models/nemotron.py
4	4	vllm/model_executor/models/nemotron_h.py
4	4	vllm/model_executor/models/nemotron_nas.py
4	4	vllm/model_executor/models/nemotron_vl.py
4	4	vllm/model_executor/models/olmo.py
3	3	vllm/model_executor/models/olmo2.py
4	4	vllm/model_executor/models/olmoe.py
4	4	vllm/model_executor/models/openpangu.py
2	2	vllm/model_executor/models/openpangu_mtp.py
6	6	vllm/model_executor/models/opt.py
4	4	vllm/model_executor/models/orion.py
4	4	vllm/model_executor/models/ouro.py
1	1	vllm/model_executor/models/ovis.py
1	1	vllm/model_executor/models/ovis2_5.py
3	3	vllm/model_executor/models/paddleocr_vl.py
1	1	vllm/model_executor/models/paligemma.py
4	4	vllm/model_executor/models/persimmon.py
4	4	vllm/model_executor/models/phi.py
4	4	vllm/model_executor/models/phi3v.py
1	1	vllm/model_executor/models/phi4_multimodal.py
1	1	vllm/model_executor/models/phi4mm.py
4	4	vllm/model_executor/models/phimoe.py
1	1	vllm/model_executor/models/pixtral.py
4	4	vllm/model_executor/models/plamo2.py
2	2	vllm/model_executor/models/qwen.py
4	4	vllm/model_executor/models/qwen2.py
5	5	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
1	1	vllm/model_executor/models/qwen2_audio.py
4	4	vllm/model_executor/models/qwen2_moe.py
2	2	vllm/model_executor/models/qwen2_rm.py
1	1	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/model_executor/models/qwen3.py
4	4	vllm/model_executor/models/qwen3_moe.py
4	4	vllm/model_executor/models/qwen3_next.py
4	4	vllm/model_executor/models/qwen3_next_mtp.py
5	7	vllm/model_executor/models/qwen3_omni_moe_thinker.py
6	8	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/qwen3_vl_moe.py
1	1	vllm/model_executor/models/qwen_vl.py
2	2	vllm/model_executor/models/roberta.py
4	4	vllm/model_executor/models/seed_oss.py
5	5	vllm/model_executor/models/siglip.py
4	4	vllm/model_executor/models/skyworkr1v.py
4	4	vllm/model_executor/models/solar.py
4	4	vllm/model_executor/models/stablelm.py
4	4	vllm/model_executor/models/starcoder2.py
4	4	vllm/model_executor/models/step3_text.py
6	6	vllm/model_executor/models/step3_vl.py
3	3	vllm/model_executor/models/tarsier.py
1	1	vllm/model_executor/models/teleflm.py
1	1	vllm/model_executor/models/terratorch.py
2	2	vllm/model_executor/models/transformers/base.py
1	1	vllm/model_executor/models/transformers/multimodal.py
4	4	vllm/model_executor/models/ultravox.py
1	1	vllm/model_executor/models/utils.py
1	1	vllm/model_executor/models/voxtral.py
5	5	vllm/model_executor/models/whisper.py
4	4	vllm/model_executor/models/zamba2.py
1	1	vllm/multimodal/processing.py
3	7	vllm/v1/spec_decode/eagle.py
5	5	vllm/v1/worker/gpu_model_runner.py
10	12	vllm/v1/worker/tpu_model_runner.py
4	4	vllm/v1/worker/utils.py

[322628346] Harry Mellor 2025-11-13 [Docs] Add some details about what the MoE block needs for the Transformers backend (#28588)
23	1	docs/models/supported_models.md

[8832fff97] Nick Hill 2025-11-12 [BugFix] Fix `mm_encoder_attn_backend` arg type checking (#28599)
3	1	.buildkite/test-pipeline.yaml
3	0	vllm/config/multimodal.py

[a543e678b] Michael Goin 2025-11-12 [Bugfix] Fix SM100 gpt-oss regression due to faulty attn sink support (#28561)
21	10	vllm/utils/flashinfer.py
15	0	vllm/v1/attention/backends/flashinfer.py

[2dacd5739] wangxiyuan 2025-11-13 [platform] Move get_cu_count to utils (#27005)
19	5	tests/kernels/quantization/test_rocm_skinny_gemms.py
2	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
2	1	vllm/model_executor/layers/utils.py
0	7	vllm/platforms/interface.py
0	4	vllm/platforms/rocm.py
5	0	vllm/utils/platform_utils.py

[d75ad0481] Gregory Shtrasberg 2025-11-12 [ROCm][Bugfix] Revert removing setuptools version restriction (#28592)
1	1	requirements/rocm-build.txt
1	1	requirements/rocm.txt

[52eadcec9] Michael Goin 2025-11-12 [Docs] Update meetups.md description (#28583)
18	2	docs/community/meetups.md

[51c599f0e] Harry Mellor 2025-11-12 Skip models that cannot currently init on Transformers v5 (#28471)
4	4	.buildkite/test-pipeline.yaml
2	1	vllm/model_executor/model_loader/weight_utils.py
0	11	vllm/model_executor/models/whisper.py

[69d0e9031] Alexander Matveev 2025-11-12 [MoE][Kernel][Perf] Improve Shared Expert Stream Overlap (#28406)
8	0	vllm/envs.py
49	36	vllm/model_executor/layers/fused_moe/layer.py
24	0	vllm/utils/torch_utils.py

[4ca5cd574] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2025-11-13 [Core][AMD] Migrate fully transparent sleep mode to ROCm platform (#12695)
27	3	CMakeLists.txt
392	17	csrc/cumem_allocator.cpp
109	0	csrc/cumem_allocator_compat.h
5	1	docs/features/sleep_mode.md
3	1	setup.py
5	4	tests/basic_correctness/test_cumem.py
2	1	vllm/config/model.py
1	1	vllm/device_allocator/cumem.py
28	2	vllm/distributed/device_communicators/cuda_wrapper.py
5	0	vllm/envs.py
5	1	vllm/platforms/interface.py

[10f01d5a3] Michael Goin 2025-11-12 [Bugfix] Adjust Marlin CUDA arch selection to 8.0+PTX;9.0+PTX (#28294)
2	2	CMakeLists.txt

[3eb0c2673] QiliangCui 2025-11-12 [TPU] Support GCS path in VLLM_TORCH_PROFILER_DIR (#28487)
5	3	vllm/envs.py

[d8140b983] vllmellm 2025-11-13 [ROCM] Fix ROCm warnings, environment flag access, and GEMM kernel naming for consistency in `_aiter_ops.py` (#28464)
27	22	vllm/_aiter_ops.py
1	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py
2	2	vllm/model_executor/layers/utils.py
2	3	vllm/platforms/rocm.py

[74a9a9faa] Varun Sundar Rabindranath 2025-11-12 [Performance][B200] Fix deepgemm prologue (#27897)
7	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
29	0	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
25	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
34	40	vllm/model_executor/layers/quantization/fp8.py
53	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py
15	4	vllm/utils/deep_gemm.py

[478ee511d] Wei Wei 2025-11-12 [Misc]Fix typo in llm_engine.py (#28584)
4	4	vllm/v1/engine/llm_engine.py

[58ce8d12b] Andy Lo 2025-11-12 [BugFix] Priority scheduling and spec tokens preemption (#28558)
252	0	tests/v1/core/test_priority_scheduler_random.py
14	0	vllm/v1/core/sched/scheduler.py

[94a9ebcf3] Yihua Cheng 2025-11-12 [KV connector][WIP] KV cache proxy based on LMCache multi-process mode (#27902)
6	0	vllm/distributed/kv_transfer/kv_connector/factory.py
13	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/__init__.py
379	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/multi_process_adapter.py
867	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_mp_connector.py

[a39dd7bb0] Harry Mellor 2025-11-12 [CI] Skip "Multi-Modal Models Test (Extended) 3" test that's broken in current Transformers (#28559)
8	0	tests/models/multimodal/generation/test_common.py

[64d57c3be] Thomas Parnell 2025-11-12 [Model] [Config] Correctly identify granite-4.0-micro as non-hybrid model (#28563)
7	0	vllm/config/model.py

[a1e7fa362] PerryZhang01 2025-11-13 [EPLB][ROCm]: support EPBL for ROCm backend (#27731)
2	2	vllm/config/parallel.py
5	1	vllm/model_executor/layers/fused_moe/layer.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/__init__.py
13	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[bac904565] alberto 2025-11-12 Implement ARC KV cache eviction policy for CPU offloader (#27039)
308	0	tests/v1/kv_offload/test_cpu_manager.py
237	0	vllm/v1/kv_offload/arc_manager.py
20	5	vllm/v1/kv_offload/cpu.py

[304419576] Benjamin Chislett 2025-11-12 [Perf] Refactor cudagraph_support to enable full CUDA graphs for spec decoding with FlashInfer (#28479)
2	1	docs/design/cuda_graphs.md
1	1	vllm/attention/layers/chunked_local_attention.py
1	1	vllm/v1/attention/backends/flash_attn.py
23	15	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/gdn_attn.py
1	1	vllm/v1/attention/backends/mamba_attn.py
1	1	vllm/v1/attention/backends/mla/cutlass_mla.py
1	1	vllm/v1/attention/backends/mla/flashattn_mla.py
1	1	vllm/v1/attention/backends/mla/flashinfer_mla.py
1	1	vllm/v1/attention/backends/mla/flashmla.py
1	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
1	1	vllm/v1/attention/backends/mla/indexer.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
1	1	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/rocm_attn.py
1	1	vllm/v1/attention/backends/triton_attn.py
11	1	vllm/v1/attention/backends/utils.py
21	10	vllm/v1/worker/gpu_model_runner.py

[a742134cc] Harry Mellor 2025-11-12 Remove deprecated fields from `CompilationConfig` (#27593)
1	0	.buildkite/test-pipeline.yaml
0	10	docs/design/cuda_graphs.md
2	2	tests/compile/piecewise/test_multiple_graphs.py
0	1	tests/compile/piecewise/test_simple.py
0	1	tests/compile/piecewise/test_toy_llama.py
91	60	tests/compile/test_config.py
0	3	tests/compile/test_decorator.py
0	2	tests/models/multimodal/generation/test_qwen2_5_vl.py
22	73	vllm/config/compilation.py
3	9	vllm/config/vllm.py
1	1	vllm/v1/attention/backends/mamba1_attn.py
1	1	vllm/v1/attention/backends/mamba2_attn.py
1	1	vllm/v1/attention/backends/short_conv_attn.py

[728a9eb70] Nicolò Lucchesi 2025-11-12 [Misc] Refactor Attention kv transfer methods into decorator (#27816)
39	76	vllm/attention/layer.py
60	0	vllm/attention/utils/kv_transfer_utils.py

[bc5bd45c7] Canlin Guo 2025-11-12 [Refactor] Remove redundant TP gather/split in split_qkv in QwenVL (#28271)
0	30	vllm/model_executor/models/qwen2_5_vl.py
1	12	vllm/model_executor/models/qwen2_vl.py

[f76e85c29] Alexander Matveev 2025-11-12 [Performance][Hopper] Avoid M dim padding to 4x for most cases (due to cuda graphs paddings) (#28492)
21	14	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[54aecd9ed] Harry Mellor 2025-11-12 Fix pre-commit (and XPU) on `main` (#28556)
1	1	vllm/model_executor/layers/quantization/mxfp4.py
0	2	vllm/platforms/xpu.py

[10138c92a] wangxiyuan 2025-11-12 [V0 deprecation] Deprecate use_v1 parameter (#28112)
0	1	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
30	11	vllm/attention/selector.py
0	3	vllm/platforms/cpu.py
0	7	vllm/platforms/cuda.py
0	1	vllm/platforms/interface.py
0	7	vllm/platforms/rocm.py
0	3	vllm/platforms/tpu.py
1	2	vllm/platforms/xpu.py

[a9d18b510] Jee Jee Li 2025-11-12 [Bugfix] Fix gpt_oss packed_modules_mapping (#28536)
5	5	vllm/model_executor/models/gpt_oss.py

[edb59a947] TJian 2025-11-12 [ROCm] [Bugfix] Fix `fused_qknorm_rope_kernel` rocm compatibility (#28500)
27	27	csrc/fused_qknorm_rope_kernel.cu
0	2	csrc/torch_bindings.cpp
4	3	csrc/type_convert.cuh
2	2	tests/compile/test_qk_norm_rope_fusion.py
2	2	tests/kernels/core/test_fused_qk_norm_rope.py
2	2	vllm/config/compilation.py

[c5f10cc13] ZhengHongming888 2025-11-12 add cpu option for p/d in nixl_connector (#28356)
16	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[d14315230] ziruiliu 2025-11-12 [KVConnector] Enable get_block_ids_with_load_errors() in LMCache connector  (#27978)
15	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py

[a4730c1b4] Chaojun Zhang 2025-11-12 [XPU]Fix crash due to removed VLLM_USE_V1 attribute (#28520)
3	2	vllm/platforms/xpu.py

[d3ade61e4] wuyaoxuehun 2025-11-12 [Model] fix glm4_moe_mtp load weights with GLM-4.6 checkpoint. (#27597)
11	4	vllm/model_executor/models/glm4_moe_mtp.py

[1761dea1a] yyzxw 2025-11-12 [BugFix]: --enable-lora with model granite-4.0-micro crash (#27733)
3	0	vllm/model_executor/models/granitemoehybrid.py

[c748355e0] Huamin Li 2025-11-12 [CI] Introduce autorun_on_main feature (#27836)
3	1	.buildkite/test-pipeline.yaml

[91864b79b] Chenguang Zheng 2025-11-12 [CI/Build] Fix crash due to removed VLLM_USE_V1 attribute in EPD (#28521)
3	7	vllm/distributed/ec_transfer/ec_transfer_state.py

[ac0bb2c30] Lukas Geiger 2025-11-12 [Core] Cache `vllm_is_batch_invariant` (#28304)
2	0	vllm/model_executor/layers/batch_invariant.py

[f31419ed8] ai-jz 2025-11-11 [Benchmark] Add retry support to fix workload bias in multi-turn benchmark (#28493)
82	48	benchmarks/multi_turn/benchmark_serving_multi_turn.py

[b9ce9a301] Fanli Lin 2025-11-12 [BugFix] Add fallback path in `apply_rotary_pos_emb_flashattn` for non-cuda platforms (#28447)
7	0	vllm/model_executor/models/keye.py

[4ccffe561] Chenguang Zheng 2025-11-12 [Core] Encoder separation for Encode-Prefill-Decode Disaggregation (#25233)
-	-	docs/assets/features/disagg_encoder/disagg_encoder_flow.png
75	0	docs/features/disagg_encoder.md
119	0	examples/online_serving/disaggregated_encoder/README.md
221	0	examples/online_serving/disaggregated_encoder/disagg_1e1p1d_example.sh
186	0	examples/online_serving/disaggregated_encoder/disagg_1e1pd_example.sh
606	0	examples/online_serving/disaggregated_encoder/disagg_epd_proxy.py
1083	16	tests/v1/core/test_scheduler.py
62	13	tests/v1/core/utils.py
171	0	tests/v1/ec_connector/integration/README.md
-	-	tests/v1/ec_connector/integration/hato.jpg
476	0	tests/v1/ec_connector/integration/run_epd_correctness_test.sh
305	0	tests/v1/ec_connector/integration/test_epd_correctness.py
609	0	tests/v1/ec_connector/unit/test_ec_shared_storage_connector.py
146	0	tests/v1/engine/test_engine_core.py
3	0	vllm/config/__init__.py
110	0	vllm/config/ec_transfer.py
7	0	vllm/config/vllm.py
14	0	vllm/distributed/ec_transfer/__init__.py
0	0	vllm/distributed/ec_transfer/ec_connector/__init__.py
247	0	vllm/distributed/ec_transfer/ec_connector/base.py
88	0	vllm/distributed/ec_transfer/ec_connector/factory.py
201	0	vllm/distributed/ec_transfer/ec_connector/shared_storage_connector.py
46	0	vllm/distributed/ec_transfer/ec_transfer_state.py
7	0	vllm/engine/arg_utils.py
12	4	vllm/model_executor/warmup/kernel_warmup.py
5	0	vllm/v1/core/sched/output.py
52	4	vllm/v1/core/sched/scheduler.py
46	0	vllm/v1/outputs.py
87	0	vllm/v1/worker/ec_connector_model_runner_mixin.py
36	4	vllm/v1/worker/gpu_model_runner.py
5	0	vllm/v1/worker/gpu_worker.py

[cbb799e31] Lukas Geiger 2025-11-12 [Model][Qwen3VL] Simplify `get_mrope_input_positions` using numpy (#28302)
11	33	vllm/model_executor/models/qwen3_vl.py

[9f0247cfa] Andreas Karatzas 2025-11-11 `VLLM_USE_TRITON_FLASH_ATTN` V0 variable deprecation (#27611)
2	6	.buildkite/scripts/hardware_ci/run-amd-test.sh
0	516	tests/kernels/test_triton_flash_attention.py
0	6	tests/models/language/pooling/test_classification.py
0	7	tests/models/language/pooling/test_embedding.py
0	13	tests/models/language/pooling/test_mm_classifier_conversion.py
0	6	tests/models/language/pooling/test_reward.py
0	8	tests/models/multimodal/generation/test_common.py
0	7	tests/models/multimodal/generation/test_phi4_multimodal.py
0	7	tests/models/multimodal/generation/test_phi4mm.py
0	5	tests/quantization/test_quark.py
0	932	vllm/attention/ops/triton_flash_attention.py
0	6	vllm/envs.py
2	19	vllm/platforms/rocm.py
0	1	vllm/usage/usage_lib.py
7	48	vllm/v1/attention/backends/mla/triton_mla.py

[7f829be7d] Li, Jiang 2025-11-12 [CPU] Refactor CPU attention backend (#27954)
1	1	.buildkite/release-pipeline.yaml
2	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh
24	4	cmake/cpu_extension.cmake
0	798	csrc/cpu/attention.cpp
0	214	csrc/cpu/cache.cpp
249	0	csrc/cpu/cpu_attn.cpp
511	0	csrc/cpu/cpu_attn_amx.hpp
1977	0	csrc/cpu/cpu_attn_impl.hpp
63	0	csrc/cpu/cpu_attn_macros.h
248	0	csrc/cpu/cpu_attn_vec.hpp
171	0	csrc/cpu/cpu_attn_vec16.hpp
46	4	csrc/cpu/cpu_types_x86.hpp
1	17	csrc/cpu/dnnl_helper.cpp
0	24	csrc/cpu/dnnl_helper.h
23	0	csrc/cpu/scratchpad_manager.cpp
31	0	csrc/cpu/scratchpad_manager.h
1	1	csrc/cpu/shm.cpp
45	60	csrc/cpu/torch_bindings.cpp
4	0	docker/Dockerfile.cpu
2	0	docs/getting_started/installation/cpu.md
3	3	tests/kernels/attention/test_attention_selector.py
575	0	tests/kernels/attention/test_cpu_attn.py
0	1	tests/kernels/test_onednn.py
10	7	tests/models/language/generation/test_common.py
1	2	tests/models/language/pooling/test_embedding.py
3	1	tests/models/registry.py
82	0	vllm/_custom_ops.py
2	1	vllm/attention/backends/registry.py
0	3	vllm/engine/arg_utils.py
11	26	vllm/platforms/cpu.py
0	1	vllm/utils/__init__.py
264	717	vllm/v1/attention/backends/cpu_attn.py
1	1	vllm/v1/attention/backends/utils.py
1	13	vllm/v1/worker/cpu_model_runner.py

[e1710393c] wangxiyuan 2025-11-12 [[V0 deprecation]]Remove VLLM_USE_V1 env (#28204)
1	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh
1	2	examples/offline_inference/mlpspeculator.py
0	2	examples/offline_inference/qwen2_5_omni/README.md
1	6	examples/offline_inference/qwen2_5_omni/only_thinker.py
12	31	examples/others/lmcache/cpu_offload_lmcache.py
0	3	tests/entrypoints/openai/test_orca_metrics.py
0	13	vllm/envs.py
0	1	vllm/usage/usage_lib.py

[3f770f442] Isotr0py 2025-11-12 [Performance] Cache loaded custom logitsprocs to avoid overheads (#28462)
8	2	vllm/v1/sample/logits_processor/__init__.py

[48c879369] Yanan Cao 2025-11-11 [Frontend] Change CompilationMode to a proper Enum (#28165)
4	2	tests/compile/test_basic_correctness.py
60	0	tests/utils_/test_argparse_utils.py
3	1	vllm/compilation/wrapper.py
36	15	vllm/config/compilation.py
1	4	vllm/config/vllm.py
4	1	vllm/entrypoints/llm.py

[1788aa1ef] Ilya Markov 2025-11-12 [BugFix] Graceful handling of torch symm mem errors. (#27671)
15	7	vllm/distributed/device_communicators/symm_mem.py
2	2	vllm/envs.py

[d23539549] Adrian Abeyta 2025-11-11 Use FLASHINFER MLA backend when testing fp8_kv_scale_compile (#28491)
16	4	tests/compile/test_full_graph.py

[412e153df] Max Hu 2025-11-11 [Feature] Allow configuring FlashInfer workspace size (#28269)
6	0	vllm/envs.py
3	3	vllm/v1/attention/backends/flashinfer.py
7	9	vllm/v1/attention/backends/mla/common.py

[e5f599d4d] Michael Goin 2025-11-11 [Bugfix] Disable shared expert overlap if Marlin MoE is used (#28410)
4	0	vllm/model_executor/layers/fused_moe/layer.py
5	5	vllm/model_executor/layers/fused_moe/shared_fused_moe.py
1	0	vllm/model_executor/layers/quantization/awq_marlin.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	0	vllm/model_executor/layers/quantization/gptq_marlin.py
1	0	vllm/model_executor/layers/quantization/mxfp4.py

[28534b92b] Michael Goin 2025-11-11 Add Zurich vLLM Meetup (#28488)
1	0	README.md
1	0	docs/community/meetups.md

[d4902ba56] wangxiyuan 2025-11-12 [Misc] Cleanup Executor interface (#28441)
0	6	vllm/v1/executor/abstract.py

[df4d3a44a] Kyuyeun Kim 2025-11-11 [TPU] Rename path to tpu platform (#28452)
1	1	vllm/platforms/__init__.py

[9d1c47470] Jee Jee Li 2025-11-12 [LoRA][1/N]Remove LoRA extra vocab (#28382)
5	25	vllm/model_executor/models/apertus.py
2	8	vllm/model_executor/models/arcee.py
2	4	vllm/model_executor/models/arctic.py
2	6	vllm/model_executor/models/aria.py
2	2	vllm/model_executor/models/baichuan.py
0	2	vllm/model_executor/models/bailing_moe.py
6	24	vllm/model_executor/models/bamba.py
3	5	vllm/model_executor/models/chameleon.py
1	2	vllm/model_executor/models/chatglm.py
6	13	vllm/model_executor/models/commandr.py
2	7	vllm/model_executor/models/dbrx.py
5	22	vllm/model_executor/models/exaone.py
4	22	vllm/model_executor/models/exaone4.py
7	24	vllm/model_executor/models/falcon_h1.py
0	2	vllm/model_executor/models/gemma.py
1	2	vllm/model_executor/models/gemma2.py
1	2	vllm/model_executor/models/gemma3.py
1	2	vllm/model_executor/models/gemma3n.py
0	2	vllm/model_executor/models/glm4.py
4	16	vllm/model_executor/models/gpt_bigcode.py
4	23	vllm/model_executor/models/granitemoe.py
5	22	vllm/model_executor/models/granitemoehybrid.py
5	23	vllm/model_executor/models/granitemoeshared.py
6	20	vllm/model_executor/models/grok1.py
6	15	vllm/model_executor/models/hunyuan_v1.py
0	2	vllm/model_executor/models/internlm2.py
6	24	vllm/model_executor/models/jamba.py
2	8	vllm/model_executor/models/kimi_vl.py
5	26	vllm/model_executor/models/lfm2.py
6	26	vllm/model_executor/models/lfm2_moe.py
0	3	vllm/model_executor/models/llama_eagle3.py
1	2	vllm/model_executor/models/longcat_flash.py
6	23	vllm/model_executor/models/mamba.py
5	23	vllm/model_executor/models/mamba2.py
3	9	vllm/model_executor/models/medusa.py
0	2	vllm/model_executor/models/mimo.py
7	23	vllm/model_executor/models/minicpm.py
6	23	vllm/model_executor/models/minicpm_eagle.py
3	8	vllm/model_executor/models/minimax_text_01.py
0	1	vllm/model_executor/models/mlp_speculator.py
1	2	vllm/model_executor/models/molmo.py
7	23	vllm/model_executor/models/nemotron.py
6	24	vllm/model_executor/models/nemotron_h.py
6	25	vllm/model_executor/models/nemotron_nas.py
1	3	vllm/model_executor/models/olmo.py
0	2	vllm/model_executor/models/olmo2.py
0	2	vllm/model_executor/models/ouro.py
1	2	vllm/model_executor/models/phi.py
0	1	vllm/model_executor/models/phi3v.py
2	12	vllm/model_executor/models/phi4mm.py
7	27	vllm/model_executor/models/phimoe.py
2	9	vllm/model_executor/models/plamo2.py
0	2	vllm/model_executor/models/qwen2.py
0	2	vllm/model_executor/models/qwen2_rm.py
0	2	vllm/model_executor/models/qwen3.py
7	23	vllm/model_executor/models/qwen3_next.py
6	17	vllm/model_executor/models/qwen3_next_mtp.py
0	2	vllm/model_executor/models/qwen3_vl.py
0	2	vllm/model_executor/models/seed_oss.py
7	23	vllm/model_executor/models/solar.py
3	9	vllm/model_executor/models/starcoder2.py
3	13	vllm/model_executor/models/step3_text.py
1	2	vllm/model_executor/models/transformers/causal.py
2	4	vllm/model_executor/models/whisper.py
5	23	vllm/model_executor/models/zamba2.py

[8c32c6e4b] Jie Luo 2025-11-12 [Misc] fix typo in DCP comment (#28389)
1	1	vllm/v1/attention/backends/mla/common.py

[de120bc94] Canlin Guo 2025-11-12 [V0 deprecation] Clean up num_prefill_tokens logic for V0 (#28203)
3	15	vllm/forward_context.py

[4228be795] Jialin Ouyang 2025-11-11 [Perf] Use np.ndarray instead of list[list[int]] to reduce GC overhead (#28245)
4	3	tests/v1/engine/utils.py
6	1	vllm/v1/engine/logprobs.py
7	6	vllm/v1/outputs.py

[76e4dcf22] Lukas Geiger 2025-11-11 [Misc] Remove unused attention prefix prefill ops functions (#26971)
0	210	vllm/attention/ops/prefix_prefill.py
0	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[d5edcb867] Fanli Lin 2025-11-12 [BugFix] Fix Siglip2Attention on XPU (#28448)
9	2	vllm/model_executor/models/siglip2navit.py

[6c3c0f823] Xin Yang 2025-11-11 [Kernel] Optimize rms_norm kernel (#27931)
29	0	csrc/dispatch_utils.h
28	11	csrc/layernorm_kernels.cu
29	14	csrc/layernorm_quant_kernels.cu

[684f25458] Matthew Bonanni 2025-11-11 Prefer FlashAttention MLA as default over FlashMLA (#27363)
2	2	vllm/platforms/cuda.py

[e55342491] Zhewen Li 2025-11-11 [CI/Build] Refactor Attention backend for test_prefix_prefill from xformers to SDPA (#28424)
194	116	tests/kernels/attention/test_prefix_prefill.py

[5a1271d83] xuebwang-amd 2025-11-12 [Quantization] fix attention quantization of gpt_oss model (#27334)
80	0	tests/models/quantization/test_gpt_oss_attn_quantization.py
13	2	vllm/model_executor/layers/quantization/mxfp4.py
8	2	vllm/model_executor/models/gpt_oss.py

[05576df85] xuebwang-amd 2025-11-12 [ROCm][Quantization] extend AMD Quark to support mixed-precision quantized model (#24239)
33	1	docs/features/quantization/quark.md
69	0	tests/quantization/test_mixed_precision.py
25	7	vllm/model_executor/layers/quantization/quark/quark.py

[68c09efc3] zhrrr 2025-11-12 [Kernel][Perf] fuse QK Norm and RoPE into one cuda kernel for Qwen Model (#27165)
1	0	.buildkite/test-pipeline.yaml
1	0	CMakeLists.txt
418	0	csrc/fused_qknorm_rope_kernel.cu
6	0	csrc/ops.h
10	0	csrc/torch_bindings.cpp
44	16	csrc/type_convert.cuh
195	0	tests/compile/test_qk_norm_rope_fusion.py
141	0	tests/kernels/core/test_fused_qk_norm_rope.py
29	0	vllm/_custom_ops.py
17	0	vllm/compilation/fix_functionalization.py
4	0	vllm/compilation/fusion.py
80	1	vllm/compilation/matcher_utils.py
4	0	vllm/compilation/pass_manager.py
238	0	vllm/compilation/qk_norm_rope_fusion.py
13	0	vllm/config/compilation.py
33	12	vllm/model_executor/layers/rotary_embedding/base.py

[a7ef3eb0c] Nicolò Lucchesi 2025-11-11 [NIXL] Generalize block-first backend layouts (FlashInfer-like) (#28282)
15	2	tests/v1/kv_connector/unit/test_nixl_connector.py
37	10	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[f9a408718] Michael Goin 2025-11-11 Remove weight_scale.T special case for SM90 Block FP8 CUTLASS kernel (#28431)
29	14	benchmarks/kernels/bench_block_fp8_gemm.py
2	1	csrc/quantization/w8a8/cutlass/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	1	vllm/model_executor/layers/quantization/fp8.py
3	19	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[287bbbeb0] the-codeboy 2025-11-11 [Doc] Fix typo in serving docs (#28474)
2	2	docs/serving/openai_compatible_server.md

[3143eb23f] usberkeley 2025-11-12 [BugFix] Add test_outputs.py to CI pipeline (#28466)
1	0	.buildkite/test-amd.yaml
1	0	.buildkite/test-pipeline.yaml

[b88606805] Fanli Lin 2025-11-11 [BugFix] Fix RuntimeError in PixtralHFAttention on CPU/XPU (#28444)
1	1	vllm/model_executor/models/pixtral.py

[a90ad7d83] Mark McLoughlin 2025-11-11 Add @markmc to CODEOWNERS for Observability (#28457)
10	0	.github/CODEOWNERS

[533b018f7] jvlunteren 2025-11-11 [BugFix] Fix Failing Ruff Check (#28469)
1	1	tests/compile/test_fusions_e2e.py

[a1448b4b6] bnellnm 2025-11-11 [Kernels] Split up fused_moe/layer.py, isolate more modular kernel code (#28064)
5	4	tests/kernels/moe/modular_kernel_tools/mk_objects.py
3	1	vllm/lora/layers/fused_moe.py
3	1	vllm/model_executor/layers/fused_moe/__init__.py
160	0	vllm/model_executor/layers/fused_moe/all2all_utils.py
112	0	vllm/model_executor/layers/fused_moe/fused_moe_method_base.py
164	0	vllm/model_executor/layers/fused_moe/fused_moe_modular_method.py
32	918	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/shared_fused_moe.py
578	0	vllm/model_executor/layers/fused_moe/unquantized_fused_moe_method.py
6	23	vllm/model_executor/layers/quantization/mxfp4.py

[fa1970201] Maryam Tahhan 2025-11-11 [Docs] Fix grammar in CPU installation guide (#28461)
7	7	docs/getting_started/installation/cpu.md

[3380543b2] Ido Segev 2025-11-11 Add request timeout override for multi-turn benchmarks (#28386)
35	5	benchmarks/multi_turn/benchmark_serving_multi_turn.py

[afffd3cc8] Cyrus Leung 2025-11-11 [Model] Pass `mm_features` directly into `get_mrope_input_positions` (#28399)
13	22	vllm/model_executor/models/ernie45_vl.py
13	19	vllm/model_executor/models/glm4_1v.py
13	19	vllm/model_executor/models/glm4v.py
6	16	vllm/model_executor/models/interfaces.py
12	17	vllm/model_executor/models/keye.py
12	17	vllm/model_executor/models/keye_vl1_5.py
12	17	vllm/model_executor/models/paddleocr_vl.py
26	20	vllm/model_executor/models/qwen2_5_omni_thinker.py
17	19	vllm/model_executor/models/qwen2_5_vl.py
13	24	vllm/model_executor/models/qwen2_vl.py
32	23	vllm/model_executor/models/qwen3_omni_moe_thinker.py
12	18	vllm/model_executor/models/qwen3_vl.py
27	12	vllm/model_executor/models/transformers/multimodal.py
13	0	vllm/multimodal/inputs.py
4	29	vllm/v1/worker/gpu_model_runner.py

[7dbe6d81d] Chaojun Zhang 2025-11-11 Fix Fused MoE LoRA Triton kernel bug (#28450)
2	1	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[b30dfa03c] Matthew Bonanni 2025-11-11 [Attention] Refactor CUDA attention backend selection logic (#24794)
5	0	.buildkite/test-pipeline.yaml
16	15	tests/compile/test_fusion_attn.py
12	12	tests/compile/test_fusions_e2e.py
3	3	tests/config/test_multimodal_config.py
45	30	tests/kernels/attention/test_attention_selector.py
6	6	tests/kernels/attention/test_mha_attn.py
11	0	tests/models/test_initialization.py
27	20	tests/v1/attention/test_attention_backends.py
13	16	tests/v1/attention/test_mla_backends.py
4	6	tests/v1/attention/utils.py
13	5	tests/v1/spec_decode/test_eagle.py
4	2	tests/v1/spec_decode/test_mtp.py
4	4	tests/v1/spec_decode/test_tree_attention.py
15	10	tests/v1/worker/test_gpu_model_runner.py
139	10	vllm/attention/backends/abstract.py
168	84	vllm/attention/backends/registry.py
39	29	vllm/attention/layer.py
46	78	vllm/attention/selector.py
9	1	vllm/config/cache.py
4	4	vllm/config/model.py
11	21	vllm/config/multimodal.py
4	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
2	2	vllm/engine/arg_utils.py
3	3	vllm/envs.py
19	18	vllm/model_executor/models/dots_ocr.py
19	18	vllm/model_executor/models/ernie45_vl.py
18	17	vllm/model_executor/models/glm4_1v.py
12	12	vllm/model_executor/models/keye.py
3	3	vllm/model_executor/models/ovis2_5.py
25	22	vllm/model_executor/models/paddleocr_vl.py
24	18	vllm/model_executor/models/qwen2_5_vl.py
21	17	vllm/model_executor/models/qwen2_vl.py
8	7	vllm/model_executor/models/qwen3_omni_moe_thinker.py
13	13	vllm/model_executor/models/qwen3_vl.py
13	13	vllm/model_executor/models/siglip2navit.py
4	4	vllm/model_executor/models/vision.py
6	6	vllm/platforms/cpu.py
181	181	vllm/platforms/cuda.py
35	7	vllm/platforms/interface.py
24	25	vllm/platforms/rocm.py
7	8	vllm/platforms/tpu.py
17	17	vllm/platforms/xpu.py
12	20	vllm/v1/attention/backends/cpu_attn.py
41	30	vllm/v1/attention/backends/flash_attn.py
34	29	vllm/v1/attention/backends/flashinfer.py
12	9	vllm/v1/attention/backends/flex_attention.py
6	16	vllm/v1/attention/backends/mla/common.py
13	3	vllm/v1/attention/backends/mla/cutlass_mla.py
27	0	vllm/v1/attention/backends/mla/flashattn_mla.py
22	4	vllm/v1/attention/backends/mla/flashinfer_mla.py
34	3	vllm/v1/attention/backends/mla/flashmla.py
22	8	vllm/v1/attention/backends/mla/flashmla_sparse.py
2	4	vllm/v1/attention/backends/mla/indexer.py
10	0	vllm/v1/attention/backends/mla/triton_mla.py
3	22	vllm/v1/attention/backends/rocm_aiter_fa.py
3	7	vllm/v1/attention/backends/rocm_attn.py
3	23	vllm/v1/attention/backends/tree_attn.py
26	21	vllm/v1/attention/backends/triton_attn.py
3	23	vllm/v1/attention/backends/xformers.py
6	2	vllm/v1/spec_decode/eagle.py
2	2	vllm/v1/worker/gpu_model_runner.py

[2e78150d2] Michael Goin 2025-11-11 [CI] Add mergify rules for `nvidia` label (#28417)
17	0	.github/mergify.yml

[d381eb967] Ido Segev 2025-11-11 Multi turn benchmark progress bar for synthetic conversation generation (#28394)
15	3	benchmarks/multi_turn/bench_dataset.py
2	1	benchmarks/multi_turn/requirements.txt

[9973e6e04] Lukas Geiger 2025-11-11 [Model][Qwen3VL] Slighly speedup `fast_pos_embed_interpolate` (#28434)
2	2	vllm/model_executor/models/qwen3_vl.py

[c7991269d] Fanli Lin 2025-11-11 [BugFix] 'DeepseekV2Config' object has no attribute 'use_mla'`  (#28387)
5	1	vllm/model_executor/models/kimi_vl.py

[f0359fffa] Jiangyun Zhu 2025-11-11 [Bugfix] fix qwen3-next crash (#28202)
1	1	vllm/model_executor/models/qwen3_next.py

[798c7bebc] Sage Moore 2025-11-11 [EPLB] Refactor balance_packing to use numpy and optimize GPU-CPU transfers in EPLB (#28369)
28	12	vllm/distributed/eplb/rebalance_algo.py
9	5	vllm/distributed/eplb/rebalance_execute.py

[4fd4b743a] Roger Wang 2025-11-11 [Bugfix] Fix max image size for PaddleOCR-VL (#28442)
21	15	vllm/model_executor/models/paddleocr_vl.py

[cc079763c] David Ben-David 2025-11-11 [BugFix] Avoid calling KV connector layer APIs when metadata is unset (#28253)
4	0	vllm/attention/layer.py
8	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
6	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[a7adbc6c6] iAmir97 2025-11-11 [Doc] Sleep mode documentation  (#28357)
39	0	docs/features/sleep_mode.md

[e605e8e32] Robert Shaw 2025-11-11 [Bugfix] Fix Stream Sync for Shared Expert Overlap (#28430)
0	3	tests/evals/gsm8k/configs/Qwen1.5-MoE-W4A16-CT.yaml
15	30	vllm/model_executor/layers/fused_moe/layer.py

[bca74e32b] Zuyi Zhao 2025-11-10 [Frontend] Add sagemaker_standards dynamic lora adapter and stateful session management decorators to vLLM OpenAI API server (#27892)
1	0	requirements/common.txt
0	0	tests/entrypoints/sagemaker/__init__.py
58	0	tests/entrypoints/sagemaker/conftest.py
734	0	tests/entrypoints/sagemaker/test_sagemaker_handler_overrides.py
171	0	tests/entrypoints/sagemaker/test_sagemaker_lora_adapters.py
346	0	tests/entrypoints/sagemaker/test_sagemaker_middleware_integration.py
153	0	tests/entrypoints/sagemaker/test_sagemaker_stateful_sessions.py
57	0	vllm/entrypoints/dynamic_lora.py
17	83	vllm/entrypoints/openai/api_server.py
4	0	vllm/entrypoints/sagemaker/__init__.py
72	0	vllm/entrypoints/sagemaker/routes.py

[8d706cca9] Zhuohan Li 2025-11-10 [Misc] FlattenLogprobs -> FlatLogprobs (#28335)
6	10	tests/samplers/test_logprobs.py
20	20	tests/test_logprobs.py
4	4	vllm/envs.py
13	13	vllm/logprobs.py

[57201a6a4] Xin Yang 2025-11-10 Fix rotary embedding benchmark script (#28323)
64	90	benchmarks/kernels/benchmark_rope.py

[f2d9ad062] Michael Goin 2025-11-10 Only register rocm_aiter_ops if aiter is found (#28428)
2	1	vllm/_aiter_ops.py

[de540c035] Wentao Ye 2025-11-10 [Feature] Add env var `VLLM_MOE_USE_DEEP_GEMM` (#28422)
6	0	vllm/envs.py
9	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/fp8.py
3	0	vllm/model_executor/warmup/deep_gemm_warmup.py

[39029d519] Lucas Wilkinson 2025-11-10 [CI/Test Fix] Fix CP tests on Blackwell (#28404)
12	0	tests/distributed/test_context_parallel.py
0	1	vllm/attention/ops/common.py

[35d801f13] Wentao Ye 2025-11-10 [Feature] Refactor batch invariant fp8 DeepGEMM (#27606)
11	87	vllm/model_executor/layers/quantization/fp8.py

[0bf29fadf] Matthew Bonanni 2025-11-10 [Test] Remove old non-varlen FA2 test (#28420)
0	119	tests/kernels/attention/test_flash_attn.py

[a5a790eea] Adrian Abeyta 2025-11-10 [Bugfix] Ensure calculated KV scales are applied in attention. (#27232)
5	2	.buildkite/test-pipeline.yaml
8	2	tests/compile/test_full_graph.py
7	22	vllm/attention/layer.py
9	10	vllm/v1/worker/gpu_model_runner.py

[b30372cbd] Jialin Ouyang 2025-11-10 [Perf] Move gc.freeze logic from EngineCoreProc to EngineCore for better coverage (#27896)
2	3	vllm/benchmarks/serve.py
3	0	vllm/distributed/parallel_state.py
2	4	vllm/entrypoints/openai/api_server.py
15	0	vllm/utils/gc_utils.py
8	7	vllm/v1/engine/core.py

[d17ecc6b1] Ilya Markov 2025-11-11 [PERF] Allreduce fusion. Support torch native matching. Tuning of the thresholds (#24248)
2	2	.buildkite/test-pipeline.yaml
1129	0	benchmarks/kernels/benchmark_fused_collective.py
7	0	tests/compile/test_fusions_e2e.py
72	60	vllm/compilation/collective_fusion.py
48	2	vllm/config/compilation.py
26	19	vllm/model_executor/layers/fused_moe/layer.py

[021143561] Yong Hoon Shin 2025-11-10 [ROCm] Add missing gemm_a8w8_blockscale import (#28378)
21	20	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[30700b1cd] Robert Shaw 2025-11-10 [CI] Fix Plugin Tests Tests (#28413)
13	11	vllm/config/vllm.py

[4b94ed8f9] Andrew Xia 2025-11-10 [Frontend][2/n] remove empty content from _parse_tool_calls_from_content (#28331)
2	0	vllm/entrypoints/openai/serving_engine.py

[6dec9f610] Lucas Wilkinson 2025-11-10 [BugFix] Fix DeepGEMM over-allocating workspace (#28254)
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py

[bf6a3d0ff] Wei Wei 2025-11-10 [Misc] Add more scoping for improved trace (#28329)
61	55	vllm/v1/core/sched/scheduler.py
73	44	vllm/v1/engine/core.py
21	16	vllm/v1/engine/llm_engine.py
37	33	vllm/v1/worker/gpu_model_runner.py

[40d33264c] Sage Moore 2025-11-10 [Bugfix][EPLB] Disabled shared expert overlap when EPLB is enabled (#28377)
10	5	vllm/model_executor/layers/fused_moe/shared_fused_moe.py

[9c84ca829] Jonas M. Kübler 2025-11-10 [FA/Chore] Bump FA version for FP8 two-level accumulation  (#27889)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[6d54336ae] Rémi Delacourt 2025-11-10 [Bugfix] Fix llguidance backend, rollback when EOS was encountered (#25905)
118	0	tests/v1/structured_output/test_backend_guidance.py
8	2	vllm/v1/structured_output/backend_guidance.py

[34553b9d2] jiahanc 2025-11-10 [Performance] Support FP8 flashinfer TRTLLM MOE on Qwen3 and Qwen-3next (#27492)
21	0	vllm/model_executor/layers/fused_moe/config.py
12	14	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
20	0	vllm/model_executor/layers/fused_moe/layer.py
7	7	vllm/model_executor/layers/quantization/fp8.py
14	9	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
2	0	vllm/model_executor/models/qwen3_moe.py
2	0	vllm/model_executor/models/qwen3_next.py

[b039bfda8] Varun Sundar Rabindranath 2025-11-10 [Bugfix] Fix persistent_masked_m_silu_mul_quant tests (#28366)
10	5	csrc/quantization/activation_kernels.cu
4	1	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
2	1	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[d0e186c16] Cyrus Leung 2025-11-11 [V0 Deprecation] Remove unused `context_len` and `seq_len` from M-RoPE (#28395)
0	3	vllm/model_executor/models/ernie45_vl.py
0	3	vllm/model_executor/models/glm4_1v.py
0	3	vllm/model_executor/models/glm4v.py
0	4	vllm/model_executor/models/interfaces.py
0	3	vllm/model_executor/models/keye.py
0	3	vllm/model_executor/models/keye_vl1_5.py
0	3	vllm/model_executor/models/paddleocr_vl.py
0	3	vllm/model_executor/models/qwen2_5_omni_thinker.py
0	3	vllm/model_executor/models/qwen2_5_vl.py
0	3	vllm/model_executor/models/qwen2_vl.py
0	2	vllm/model_executor/models/qwen3_omni_moe_thinker.py
1	3	vllm/model_executor/models/qwen3_vl.py
1	3	vllm/model_executor/models/transformers/multimodal.py

[f080a8351] vllmellm 2025-11-10 [RFC][ROCm][AITER] Keep all AITER kernels in `_aiter_ops` class like `_custom_ops` and `_ipex_ops` (#24490)
1	1	docs/design/moe_kernel_features.md
6	5	tests/kernels/moe/test_moe.py
14	27	tests/model_executor/test_enabled_custom_ops.py
941	0	vllm/_aiter_ops.py
0	105	vllm/attention/ops/rocm_aiter_mla.py
4	4	vllm/envs.py
7	8	vllm/model_executor/layers/fused_moe/fused_moe.py
43	40	vllm/model_executor/layers/fused_moe/layer.py
17	312	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
21	69	vllm/model_executor/layers/layernorm.py
3	9	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
6	10	vllm/model_executor/layers/quantization/fp8.py
3	45	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
16	31	vllm/model_executor/layers/quantization/quark/quark_moe.py
7	0	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py
35	89	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
5	8	vllm/model_executor/layers/rotary_embedding/base.py
9	0	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
0	94	vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py
12	15	vllm/model_executor/models/deepseek_v2.py
18	9	vllm/platforms/rocm.py
21	34	vllm/v1/attention/backends/mla/common.py
2	7	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[40e2eeeb9] caozuoba 2025-11-11 [Kernel] Optimization of the mm_k operator. (#28280)
51	18	vllm/lora/ops/triton_ops/kernel_utils.py

[b06b9470c] zejunchen-zejun 2025-11-10 [Rocm][fused_moe][fp4] view weight to torch.float4_e2m1fn_x2 when running aiter fused moe for fp4 model (#27474)
12	0	vllm/model_executor/layers/quantization/quark/quark_moe.py

[4673e465f] TJian 2025-11-10 Add @tjtanaa to codeowner for ROCm and multi-modal (#28360)
16	6	.github/CODEOWNERS

[912744d06] Ferrebo 2025-11-10 [Fix] optimize visual token mask with caching and multi-token support (#28374)
29	5	vllm/model_executor/models/ernie45_vl.py

[15be507c8] Yu Jiaqi 2025-11-10 [bugfix] fix siglip batch text output error (#28365)
61	22	vllm/model_executor/models/siglip.py

[6f7de33be] Mark McLoughlin 2025-11-10 [Metrics] Refactor LoRA state tracking (#26801)
173	2	tests/v1/engine/test_output_processor.py
1	17	tests/v1/metrics/test_stats.py
2	0	vllm/v1/engine/async_llm.py
1	0	vllm/v1/engine/llm_engine.py
14	9	vllm/v1/engine/output_processor.py
14	14	vllm/v1/metrics/loggers.py
63	64	vllm/v1/metrics/stats.py

[a98cc35c3] Shinichi Hemmi 2025-11-10 Restore PlaMo2 unit test as `pfnet/plamo-2-1b` now supports `transformers >=4.56` (#28019)
0	2	tests/models/registry.py

[e8697faf0] Lucas Wilkinson 2025-11-10 [V0 deprecation] Remove no longer used `get_metadata_cls` (#28370)
3	241	tests/kernels/utils.py
0	9	vllm/attention/backends/abstract.py
0	4	vllm/v1/attention/backends/cpu_attn.py
0	5	vllm/v1/attention/backends/flash_attn.py
0	4	vllm/v1/attention/backends/flashinfer.py
0	5	vllm/v1/attention/backends/flex_attention.py
0	5	vllm/v1/attention/backends/mla/common.py
0	4	vllm/v1/attention/backends/mla/flashattn_mla.py
0	4	vllm/v1/attention/backends/mla/flashmla.py
0	5	vllm/v1/attention/backends/mla/flashmla_sparse.py
0	5	vllm/v1/attention/backends/mla/indexer.py
0	4	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
0	4	vllm/v1/attention/backends/pallas.py
0	5	vllm/v1/attention/backends/rocm_aiter_fa.py
1	6	vllm/v1/attention/backends/rocm_aiter_unified_attn.py
0	5	vllm/v1/attention/backends/rocm_attn.py
0	5	vllm/v1/attention/backends/tree_attn.py
0	5	vllm/v1/attention/backends/triton_attn.py
0	5	vllm/v1/attention/backends/xformers.py
5	2	vllm/v1/worker/gpu_model_runner.py

[03fa4d3fb] Xiake Sun 2025-11-10 [Hardware][AMD][Model] Add Triton MoE tuning support and optimized configs for Qwen3 omni for MI308X (#28373)
6	1	benchmarks/kernels/benchmark_moe.py
213	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI308X.json

[6b2b9fd93] Varun Sundar Rabindranath 2025-11-09 [CI] lora/test_mixtral.py : Add additional expected outputs due to flakiness (#28322)
18	11	tests/lora/test_mixtral.py

[c5f685b3a] JartX 2025-11-10 [ROCm][Platform] Add RX7900XTX device id in _ROCM_DEVICE_ID_NAME_MAP (#28279)
1	0	vllm/platforms/rocm.py

[c4768dcf4] Jiangyun Zhu 2025-11-10 [Kernel] Fix fused_gdn_gating (#28343)
5	3	vllm/model_executor/models/qwen3_next.py

[a65a934eb] Zhewen Li 2025-11-09 [CI/Build] Temporary fix to LM Eval Small Models (#28324)
1	1	.buildkite/test-pipeline.yaml
4	1	tests/evals/gsm8k/configs/Qwen1.5-MoE-W4A16-CT.yaml
3	1	tests/evals/gsm8k/test_gsm8k_correctness.py

[4a8d6bd16] usberkeley 2025-11-10 Fix cu_num_generated_tokens slicing logic in LogprobsLists.slice() method (#28214)
101	0	tests/v1/test_outputs.py
10	3	vllm/v1/outputs.py

[636efd10a] Lucas Wilkinson 2025-11-09 [Core] Separate out attention metadata building logic from prepare inputs (#26764)
184	155	vllm/v1/worker/gpu_model_runner.py

[289eb6c53] Nick Hill 2025-11-09 [Core] Simplify async KV output aggregation (#28327)
9	1	tests/v1/executor/test_executor.py
0	69	tests/v1/kv_connector/unit/test_output_aggregator.py
0	40	vllm/distributed/kv_transfer/kv_connector/utils.py
36	43	vllm/v1/executor/multiproc_executor.py

[19d91ece4] Nicolò Lucchesi 2025-11-09 [CI] Fix flaky `test_eagle_correctness` test (#28364)
2	2	tests/v1/e2e/test_spec_decode.py

[7ae5a5fb1] Jiangyun Zhu 2025-11-09 [Misc] Add some comments in qwen3-next (#28267)
2	0	vllm/model_executor/models/qwen3_next.py

[de2b78305] Yong Hoon Shin 2025-11-08 [ROCm] Add env to enable/disable aiter triton gemm (#28321)
7	0	vllm/envs.py
1	0	vllm/model_executor/layers/utils.py

[e5e9067e6] Ning Xie 2025-11-09 [Misc] fix typo and add detailed log (#28178)
1	1	examples/offline_inference/load_sharded_state.py
1	1	vllm/v1/worker/gpu_model_runner.py

[3a7d58034] yihong 2025-11-09 fix: close issue 28338 by fixed python version (#28339)
1	1	.pre-commit-config.yaml
1	1	requirements/test.txt

[05f8d6907] Kevin H. Luu 2025-11-08 [chore] Move some wikimedia images to S3 (#28351)
2	2	docs/features/multimodal_inputs.md
12	12	examples/offline_inference/spec_decode.py
12	12	examples/offline_inference/vision_language_multi_image.py
2	2	examples/online_serving/openai_chat_completion_client_for_multimodal.py

[404d7a9d1] Mohammad Miadh Angkad 2025-11-09 [Performance][gpt-oss] Revert gpt-oss max cudagraph size to 1024 (#28345)
4	6	vllm/model_executor/models/config.py

[171133f92] ElizaWszola 2025-11-08 [Bugfix] Fix test fused quant layernorm tests (#27865)
3	0	csrc/quantization/w8a8/int8/scaled_quant.cu
25	10	tests/kernels/core/test_fused_quant_layernorm.py

[32787d064] Cole Murray 2025-11-08 Remove setuptools upper bound constraint (<80) (#28337)
1	1	pyproject.toml
1	1	requirements/build.txt
1	1	requirements/common.txt
1	1	requirements/cpu-build.txt
1	1	requirements/cpu.txt
1	1	requirements/rocm-build.txt
1	1	requirements/rocm.txt
1	1	requirements/xpu.txt

[975676d17] Benjamin Chislett 2025-11-08 [Feat] Drop-in Torch CUDA Profiler (#27841)
19	27	docs/contributing/profiling.md
7	1	vllm/entrypoints/openai/api_server.py
6	0	vllm/envs.py
37	0	vllm/profiler/gpu_profiler.py
7	1	vllm/v1/worker/gpu_worker.py

[77d702a22] Ev Lacey 2025-11-08 Enhance run_cluster.sh for multi-NIC support (#28328)
24	0	examples/online_serving/run_cluster.sh

[2108a571d] zhangsicheng5 2025-11-09 [DCP] Support dcp kv_cache interleave size > 1 (#26696)
7	0	tests/distributed/test_context_parallel.py
2	0	tests/v1/worker/test_gpu_model_runner.py
1	0	vllm/attention/ops/common.py
11	0	vllm/config/parallel.py
17	0	vllm/config/vllm.py
6	0	vllm/engine/arg_utils.py
11	2	vllm/v1/attention/backends/flash_attn.py
77	75	vllm/v1/attention/backends/mla/common.py
38	0	vllm/v1/attention/backends/utils.py
16	2	vllm/v1/worker/block_table.py
2	0	vllm/v1/worker/gpu_input_batch.py
14	0	vllm/v1/worker/gpu_model_runner.py

[47604137a] Andy Lo 2025-11-08 [Bugfix] Spec decode + structured output + spec model max len edge case (#28298)
30	3	tests/v1/spec_decode/test_max_len.py
4	4	vllm/v1/core/sched/scheduler.py
2	1	vllm/v1/structured_output/__init__.py

[26990d25d] Robert Shaw 2025-11-08 [Bugfix] Update device name for H200 detection (#28349)
2	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[d9ab1ad9d] Harry Mellor 2025-11-08 `reasoning_content` -> `reasoning` (#27752)
24	21	docs/features/reasoning_outputs.md
1	1	docs/features/structured_outputs.md
11	11	examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py
4	4	examples/online_serving/openai_chat_completion_with_reasoning.py
9	11	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
4	4	examples/online_serving/streamlit_openai_chatbot_webserver.py
3	3	examples/online_serving/structured_outputs/structured_outputs.py
7	7	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
4	4	tests/entrypoints/openai/test_completion_with_function_calling.py
5	5	tests/entrypoints/openai/test_run_batch.py
10	10	tests/reasoning/test_base_thinking_reasoning_parser.py
18	18	tests/reasoning/test_deepseekr1_reasoning_parser.py
5	5	tests/reasoning/test_deepseekv3_reasoning_parser.py
9	9	tests/reasoning/test_ernie45_reasoning_parser.py
9	9	tests/reasoning/test_glm4_moe_reasoning_parser.py
24	24	tests/reasoning/test_granite_reasoning_parser.py
10	10	tests/reasoning/test_hunyuan_reasoning_parser.py
18	18	tests/reasoning/test_mistral_reasoning_parser.py
8	8	tests/reasoning/test_olmo3_reasoning_parser.py
9	9	tests/reasoning/test_qwen3_reasoning_parser.py
15	15	tests/reasoning/test_seedoss_reasoning_parser.py
13	10	tests/reasoning/utils.py
2	2	tests/tokenization/test_mistral_tokenizer.py
1	1	tests/tool_use/test_ernie45_moe_tool_parser.py
3	3	tests/v1/entrypoints/llm/test_struct_output_generate.py
4	4	vllm/entrypoints/harmony_utils.py
16	0	vllm/entrypoints/openai/protocol.py
28	38	vllm/entrypoints/openai/serving_chat.py
22	26	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py
2	2	vllm/reasoning/abs_reasoning_parsers.py
10	14	vllm/reasoning/basic_parsers.py
5	5	vllm/reasoning/deepseek_r1_reasoning_parser.py
4	4	vllm/reasoning/deepseek_v3_reasoning_parser.py
9	11	vllm/reasoning/ernie45_reasoning_parser.py
12	12	vllm/reasoning/glm4_moe_reasoning_parser.py
3	3	vllm/reasoning/gptoss_reasoning_parser.py
41	47	vllm/reasoning/granite_reasoning_parser.py
14	18	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
3	3	vllm/reasoning/identity_reasoning_parser.py
2	2	vllm/reasoning/minimax_m2_reasoning_parser.py
6	6	vllm/reasoning/olmo3_reasoning_parser.py
4	4	vllm/reasoning/qwen3_reasoning_parser.py
8	8	vllm/reasoning/step3_reasoning_parser.py
6	6	vllm/transformers_utils/chat_templates/template_minicpmv45.jinja
2	2	vllm/transformers_utils/tokenizers/mistral.py

[608bb1446] 22quinn 2025-11-07 [Attention] Remove max cudagraph size limit of 992 (#27840)
0	7	vllm/v1/attention/backends/flash_attn.py
0	7	vllm/v1/attention/backends/mla/flashattn_mla.py

[4a36681f8] Xiaozhu Meng 2025-11-07 [flashinfer][fix] do not check nvcc availability when using pre-downloaded cubins (#27990)
6	2	vllm/utils/flashinfer.py

[d15afc1fd] Abolfazl Shahbazi 2025-11-07 Refactor CPU/GPU extension targets for CMake build (#28026)
4	4	CMakeLists.txt
2	2	cmake/cpu_extension.cmake
2	2	cmake/external_projects/flashmla.cmake
34	37	cmake/utils.cmake

[934a9c3b7] Isotr0py 2025-11-08 [Model] Consolidate Deepseek-MoE implementation with DeepSeek-v2 (#28101)
4	1	tests/models/registry.py
0	517	vllm/model_executor/models/deepseek.py
0	8	vllm/model_executor/models/deepseek_ocr.py
139	13	vllm/model_executor/models/deepseek_v2.py
0	8	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/registry.py

[70af44fd1] gnovack 2025-11-07 [bugfix] support eagle with lora cudagraph specialization (#28318)
12	1	vllm/v1/worker/gpu_model_runner.py

[781f5ebf5] Aurick Qiao 2025-11-07 Bump arctic-inference requirement (#28174)
1	1	requirements/test.in
1	1	requirements/test.txt
1	1	vllm/config/speculative.py

[085252764] Michael Goin 2025-11-08 [Perf][DeepSeek] Add sigmoid+bias fusion to fused_grouped_topk from TRTLLM (#28124)
99	54	csrc/moe/grouped_topk_kernels.cu
3	3	csrc/moe/moe_ops.h
3	2	csrc/moe/torch_bindings.cpp
17	2	vllm/_custom_ops.py
27	14	vllm/model_executor/layers/fused_moe/fused_moe.py

[61d25dc44] Hamid Mukhtar 2025-11-07 Update gpu.rocm.inc.md to add support for AMD Ryzen AI MAX / AI 300 Series (gfx1151, gfx1150) (#28308)
2	1	docs/getting_started/installation/gpu.rocm.inc.md

[d0c779200] Xiaohong (Sean) Chen 2025-11-07 [Bugfix][LoRA][Spec Decode] Support LoRA with speculative decoding (#21068)
141	0	tests/v1/e2e/test_lora_with_spec_decode.py
14	0	vllm/engine/arg_utils.py
5	1	vllm/lora/punica_wrapper/punica_gpu.py
6	4	vllm/v1/worker/gpu_input_batch.py
15	3	vllm/v1/worker/gpu_model_runner.py
19	6	vllm/v1/worker/lora_model_runner_mixin.py
1	1	vllm/v1/worker/tpu_input_batch.py

[b158df281] Boyuan Feng 2025-11-07 remove resolve_op_overloads and use splitting_ops directly (#28081)
63	19	tests/compile/test_config.py
5	10	vllm/compilation/backends.py
21	36	vllm/compilation/partition_rules.py

[1aaecda07] Kunshang Ji 2025-11-08 [XPU] Enable Expert parallel for MoE models (#28263)
2	0	vllm/model_executor/layers/fused_moe/layer.py
2	0	vllm/model_executor/layers/quantization/ipex_quant.py
2	0	vllm/model_executor/layers/quantization/mxfp4.py

[811df41ee] Harry Mellor 2025-11-07 Update Flashinfer from `v0.4.1` to `v0.5.2` (#27952)
4	8	docker/Dockerfile
2	2	docker/Dockerfile.nightly_torch
1	1	requirements/cuda.txt
4	2	tests/kernels/attention/test_flashinfer_trtllm_attention.py

[67a2da890] Nick Hill 2025-11-07 [PerfFix] Avoid separate thread for MP executor shm spin (take 2) (#28319)
2	1	tests/v1/executor/test_executor.py
12	20	tests/v1/kv_connector/unit/test_output_aggregator.py
24	23	vllm/distributed/kv_transfer/kv_connector/utils.py
2	2	vllm/v1/executor/abstract.py
67	61	vllm/v1/executor/multiproc_executor.py
5	6	vllm/v1/executor/ray_executor.py
4	7	vllm/v1/executor/ray_utils.py
36	7	vllm/v1/executor/uniproc_executor.py
1	1	vllm/v1/worker/gpu_worker.py

[da786e339] Nick Hill 2025-11-07 [Core] Rework handling of async scheduling config (#28250)
17	15	tests/v1/engine/test_engine_core.py
34	9	vllm/config/scheduler.py
49	2	vllm/config/vllm.py
2	26	vllm/engine/arg_utils.py
18	0	vllm/v1/core/sched/interface.py
1	19	vllm/v1/engine/core.py

[18903216f] Benjamin Chislett 2025-11-07 [Bugfix] Fix and add tests for GptOss reasoning parser (#28000)
127	0	tests/reasoning/test_gptoss_reasoning_parser.py
24	7	vllm/reasoning/gptoss_reasoning_parser.py

[d0ceb38ae] Simon Mo 2025-11-07 [Build] Fix release pipeline failing annotation (#28272)
14	7	.buildkite/scripts/annotate-release.sh

[155ad56d7] youkaichao 2025-11-08 [doc] add guide about the provided PTX was compiled with an unsupported toolchain (#28305)
4	0	docs/usage/troubleshooting.md

[5fb4137c9] Fadi Arafeh 2025-11-07 [README] Add Arm CPUs to the list of supported targets (#28290)
1	1	README.md
1	1	docs/README.md

[68a72a5cc] Nicolò Lucchesi 2025-11-07 Revert "[PerfFix] Avoid separate thread for MP executor shm spin (#28012)" (#28289)
1	2	tests/v1/executor/test_executor.py
20	12	tests/v1/kv_connector/unit/test_output_aggregator.py
29	14	vllm/distributed/kv_transfer/kv_connector/utils.py
2	2	vllm/v1/executor/abstract.py
61	67	vllm/v1/executor/multiproc_executor.py
6	5	vllm/v1/executor/ray_executor.py
4	4	vllm/v1/executor/ray_utils.py
7	36	vllm/v1/executor/uniproc_executor.py
1	1	vllm/v1/worker/gpu_worker.py

[0f872b797] Boyuan Feng 2025-11-07 [Log] update shm wait time msg (#28255)
12	10	vllm/distributed/device_communicators/shm_broadcast.py

[4b1ff1322] Wentao Ye 2025-11-07 [Feature] Default `ignore_eos` True for `random` dataset (#28227)
8	0	vllm/benchmarks/serve.py

[e0d6b4a86] Iceber Gu 2025-11-07 [CLI] add --max-tokens to `vllm complete` (#28109)
14	6	vllm/entrypoints/cli/openai.py

[72b1c2ae2] Pavani Majety 2025-11-07 [Bugfix] Use latency MOE backend as default for Flashinfer and other misc fixes (#27439)
20	2	csrc/quantization/fp4/nvfp4_quant_kernels.cu
0	2	tests/kernels/quantization/test_nvfp4_quant.py
1	1	vllm/_custom_ops.py
3	3	vllm/envs.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
7	0	vllm/model_executor/layers/quantization/fp8.py
13	4	vllm/model_executor/layers/quantization/modelopt.py

[e0919f331] Lukas Geiger 2025-11-07 [Core][MM] Add mechanism to configure multimodal fields which should stay on CPU (#28168)
6	1	vllm/model_executor/models/interfaces.py
3	12	vllm/model_executor/models/qwen2_5_vl.py
18	20	vllm/model_executor/models/qwen2_vl.py
21	34	vllm/model_executor/models/qwen3_vl.py
13	7	vllm/multimodal/utils.py
5	0	vllm/v1/worker/gpu_model_runner.py
2	0	vllm/v1/worker/tpu_model_runner.py

[8e19d470a] Kevin H. Luu 2025-11-07 [fix] Revert "fixing mm placeholder replacement issue with gemma3" (#28285)
1	1	vllm/model_executor/models/gemma3_mm.py

[1958bda9b] Mengqing Cao 2025-11-07 [Misc][Model][Refactor] Pass the prefix into Linear layers (#28259)
8	1	vllm/model_executor/models/arctic.py
14	2	vllm/model_executor/models/baichuan.py
11	2	vllm/model_executor/models/bamba.py
6	1	vllm/model_executor/models/bloom.py
7	0	vllm/model_executor/models/chameleon.py
2	0	vllm/model_executor/models/dbrx.py
8	1	vllm/model_executor/models/deepseek.py
2	0	vllm/model_executor/models/dots1.py
6	1	vllm/model_executor/models/falcon.py
4	1	vllm/model_executor/models/falcon_h1.py
14	2	vllm/model_executor/models/gemma2.py
6	1	vllm/model_executor/models/gpt_j.py
5	0	vllm/model_executor/models/gpt_neox.py
7	1	vllm/model_executor/models/jais.py
2	0	vllm/model_executor/models/jamba.py
13	2	vllm/model_executor/models/minicpm.py
4	0	vllm/model_executor/models/minicpm3.py
6	1	vllm/model_executor/models/mpt.py
2	0	vllm/model_executor/models/olmoe.py
14	2	vllm/model_executor/models/orion.py
19	4	vllm/model_executor/models/persimmon.py
9	2	vllm/model_executor/models/phi.py
2	0	vllm/model_executor/models/phimoe.py
2	0	vllm/model_executor/models/plamo2.py
2	0	vllm/model_executor/models/qwen.py
15	1	vllm/model_executor/models/zamba2.py

[7bdb42b2f] Zhang Xiangze 2025-11-07 [CPU]Avoid repeated random sample compile (#28260)
10	9	vllm/v1/sample/ops/topk_topp_sampler.py

[315068eb4] 汪志鹏 2025-11-07 [FixBug]Aeala/ShareGPT_Vicuna_unfiltered marked as multimodal benchmark (#28265)
66	2	vllm/benchmarks/datasets.py
7	0	vllm/benchmarks/throughput.py

[ccd98b59c] Jialin Ouyang 2025-11-07 [Perf] Introduce FlattenLogprobs to store logprobs results to reduce GC overhead (#28171)
96	0	tests/samplers/test_logprobs.py
0	59	tests/samplers/test_ranks.py
222	0	tests/test_logprobs.py
6	0	vllm/envs.py
183	3	vllm/logprobs.py
27	63	vllm/v1/engine/logprobs.py

[21b82f4ea] Jee Jee Li 2025-11-07 [Kernel] LoRA triton kernels support PDL (#27402)
22	7	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
21	7	vllm/lora/ops/triton_ops/kernel_utils.py
7	1	vllm/lora/ops/triton_ops/lora_expand_op.py
7	2	vllm/lora/ops/triton_ops/lora_shrink_op.py
11	0	vllm/lora/ops/triton_ops/utils.py

[a736e5ff7] Copilot 2025-11-07 [CI] Reduce Blackwell Fusion test runtime by filtering tests and only run all tests in nightly (#28074)
26	1	.buildkite/test-pipeline.yaml
5	7	tests/compile/test_fusions_e2e.py

[9da9208b2] baonudesifeizhai 2025-11-07   [Bug] Fix missing token_ids for reasoning parser models in chat completions   #28246 (#28256)
3	0	vllm/entrypoints/openai/serving_chat.py

[11fd69dd5] smit kadvani 2025-11-06 [amd][gptoss] Perf gain because of block alignment (#28024)
4	2	vllm/model_executor/layers/quantization/mxfp4.py
9	0	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py

[c0a4b95d6] Harry Mellor 2025-11-06 Fix issues from #28242 (#28257)
3	7	vllm/config/compilation.py
1	3	vllm/model_executor/models/qwen2_5_vl.py
0	11	vllm/model_executor/models/transformers/utils.py
6	0	vllm/model_executor/models/vision.py

[a47d94f18] Alexis MacAskill 2025-11-06 Add runai model streamer e2e test for GCS (#28079)
3	0	.buildkite/test-pipeline.yaml
17	0	tests/model_executor/model_loader/runai_model_streamer/test_runai_model_streamer_loader.py

[e70fbc599] Alex Brooks 2025-11-06 [CI/Build] Loosen STT LoRA Translate Check (Flaky Test) (#28247)
1	1	tests/entrypoints/openai/test_translation_validation.py

[4bf56c79c] Lucas Kabela 2025-11-06 [Multimodal][torch.compile] Add compilation config field for turning off ViT/MM compile (#28242)
33	1	tests/compile/test_multimodal_compile.py
8	0	vllm/config/compilation.py
8	2	vllm/model_executor/models/qwen2_5_vl.py
11	0	vllm/model_executor/models/transformers/utils.py

[59b453eaa] Junhong Liu 2025-11-07 Speed up mm processor kwargs per request by spliting dynamic and static kwargs (#26483)
66	0	tests/transformers_utils/test_get_processor_kwargs_from_processor.py
89	3	vllm/transformers_utils/processor.py

[827e4237b] Eugene Khvedchenya 2025-11-07 Fix failing test for CRadio (#27738)
12	1	tests/models/multimodal/pooling/test_radio.py

[ca6f755d2] Varun Sundar Rabindranath 2025-11-06 [BugFix] Fix FusedMoELoRA + ModularKernel Integration (#28237)
3	3	vllm/lora/layers/fused_moe.py

[ca90f5030] Matthew Bonanni 2025-11-06 [Test] Add non-MoE DP test coverage (#28235)
19	10	tests/v1/distributed/test_async_llm_dp.py

[da855b42d] Fang Han 2025-11-06 [Doc]: Make extraInit containers fully configurable in helm chart (#27497)
44	5	docs/deployment/frameworks/helm.md
12	0	examples/online_serving/chart-helm/README.md
4	3	examples/online_serving/chart-helm/templates/_helpers.tpl
16	7	examples/online_serving/chart-helm/templates/deployment.yaml
11	7	examples/online_serving/chart-helm/templates/job.yaml
135	0	examples/online_serving/chart-helm/tests/deployment_test.yaml
61	0	examples/online_serving/chart-helm/tests/job_test.yaml
32	0	examples/online_serving/chart-helm/tests/pvc_test.yaml
67	3	examples/online_serving/chart-helm/values.schema.json
57	2	examples/online_serving/chart-helm/values.yaml

[449de9001] Aleksandr Malyshev 2025-11-06 [ROCm] triton fp8 kernel (#27058)
65	34	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[d4aa65c99] Vico Chu 2025-11-07 [Chore] eliminate duplicated and unconditional object serialization in anthropic messages api (#27792)
4	2	vllm/entrypoints/anthropic/serving_messages.py
3	4	vllm/entrypoints/openai/api_server.py

[7a8375f8a] Julien Denize 2025-11-06 Add llama 4 scaling support (#28145)
8	1	vllm/model_executor/layers/rotary_embedding/__init__.py
6	1	vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope.py
22	0	vllm/model_executor/models/llama.py
23	6	vllm/transformers_utils/configs/mistral.py

[5e0c1fe69] Andy Lo 2025-11-06 [Structured outputs] Upgrade llguidance to 1.3.0 (#28039)
1	1	requirements/common.txt

[4507a6dae] Russell Bryant 2025-11-06 CODEOWNERS: Add myself as reviewer on security docs (#28216)
5	0	.github/CODEOWNERS

[d1dd5f53e] Roy Wang 2025-11-07 [Frontend] Fix logging format when enable response logging (#28049)
1	2	vllm/entrypoints/openai/api_server.py

[e52e4da97] StanHatko 2025-11-06 [HARDWARE][CPU] Add Option for Disabling Binding to Specific CPU Cores (#27953)
1	1	docs/getting_started/installation/cpu.md
9	4	vllm/platforms/cpu.py
5	3	vllm/v1/worker/cpu_worker.py

[2176778cd] Milos Puzovic 2025-11-06 [Doc] Add Arm CPUs are on the list of supported targets in vLLM (#26018)
1	1	docs/README.md

[0370679ce] Eric Yue 2025-11-06 [Kernel][Model] Tune fused_moe Triton configs for MiniMax-M2 on H100 (#28200)
147	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json

[8816e375d] Harry Mellor 2025-11-06 [Docs] Switch to directory style URLs (#28058)
0	5	mkdocs.yaml

[f32229293] Michael Goin 2025-11-06 Disable nm-testing models with issues in CI (#28206)
0	12	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-W8A16-compressed-tensors.yaml
2	2	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
2	2	benchmarks/kernels/benchmark_shapes.py

[c757a15f0] xiangze-arm 2025-11-06 [CPU]Improve cpu fused moe perf (#27244)
40	12	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py

[59a50afa0] Chauncey 2025-11-06 [Frontend] OpenAI Responses API supports Tool/Function calling - non-harmony  (#26874)
83	0	examples/online_serving/openai_responses_client_with_tools.py
0	0	tests/v1/entrypoints/openai/{responses => serving_responses}/__init__.py
6	1	tests/v1/entrypoints/openai/{responses => serving_responses}/conftest.py
0	0	tests/v1/entrypoints/openai/{responses => serving_responses}/test_basic.py
198	0	tests/v1/entrypoints/openai/serving_responses/test_function_call.py
0	0	tests/v1/entrypoints/openai/{responses => serving_responses}/test_image.py
0	0	tests/v1/entrypoints/openai/{responses => serving_responses}/test_stateful.py
0	0	tests/v1/entrypoints/openai/{responses => serving_responses}/test_structured_output.py
6	6	vllm/entrypoints/openai/serving_engine.py
46	19	vllm/entrypoints/openai/serving_responses.py
20	4	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
45	0	vllm/entrypoints/responses_utils.py

[981cadb35] courage17340 2025-11-06 [Bugfix][Kernel] fix merge attn states when both prefix and suffix are empty (#28181)
26	0	csrc/attention/merge_attn_states.cu

[c3ee80a01] wangxiyuan 2025-11-06 [V0 deprecation]clean up is_v1_supported_oracle (#28116)
6	44	tests/v1/test_oracle.py
15	90	vllm/engine/arg_utils.py

[3755c1453] Aditya Tewari 2025-11-06 [CPU] Enable torch profiling (#28130)
36	0	vllm/v1/worker/cpu_worker.py

[201dc98ac] Seungduk Kim 2025-11-06 Fix hard-coded parameter name in gemma3n.py (#27946)
20	1	vllm/model_executor/models/gemma3n.py

[a404e2c0f] Julien Denize 2025-11-06 Patch Mistral Tokenizer (#28146)
21	8	tests/tokenization/test_mistral_tokenizer.py
21	14	vllm/transformers_utils/tokenizers/mistral.py

[e31946f86] Xiaozhu Meng 2025-11-05 [flashinfer] fix FI all2all with FI cutlass moe (#28166)
3	1	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py

[bde503932] gmagogsfm 2025-11-05 [CI] Add compile/test_multimodal_compile.py to CI (#28151)
1	0	.buildkite/test-pipeline.yaml
2	0	tests/compile/test_multimodal_compile.py

[d72299d47] Jacob Zhong 2025-11-06 Make the cv2 dependency optional (#27780)
2	1	vllm/benchmarks/datasets.py

[80679f108] Lukas Geiger 2025-11-06 [Core][MM] Use non-blocking CPU-GPU copy of multimodal data (#28141)
3	1	vllm/multimodal/utils.py

[43ecd0a90] Isotr0py 2025-11-06 [Chore] Clean up deepseek v2/v3 config copy (#28055)
2	1	vllm/model_executor/models/deepseek.py
2	1	vllm/model_executor/models/deepseek_v2.py
1	2	vllm/model_executor/models/kimi_vl.py
6	4	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	100	vllm/transformers_utils/configs/deepseek_v3.py
1	101	vllm/transformers_utils/configs/deepseek_vl2.py
2	10	vllm/transformers_utils/configs/eagle.py
1	1	vllm/transformers_utils/configs/kimi_vl.py

[07d614511] Chauncey 2025-11-06 [Misc] Remove the duplicate code (#28111)
0	5	vllm/entrypoints/openai/serving_responses.py

[f948ab694] Vadim Gimpelson 2025-11-06 [CI Failure] `nm-testing/Qwen2-0.5B-Instruct-FP8-SkipQKV` was removed from HF. Skip it in tests (#28170)
6	1	tests/quantization/test_fp8.py

[d71af5f50] Wentao Ye 2025-11-05 [Feature] Enable TP + EP `shared_experts` overlap with router, 3.7% E2E performance improvement (#28164)
1	1	vllm/model_executor/layers/fused_moe/layer.py
15	7	vllm/model_executor/layers/fused_moe/shared_fused_moe.py

[90189c71a] Wentao Ye 2025-11-05 [Bug] Fix env string `"0"` same to `True` (#28159)
6	4	vllm/envs.py

[d79d9f078] Wentao Ye 2025-11-05 [Bug] Fix cpu disable shared_experts `VLLM_DISABLE_SHARED_EXPERTS_STREAM` (#28157)
1	1	vllm/platforms/cpu.py

[b6a248bdd] Vadim Gimpelson 2025-11-06 [PERF] Decouple projections from GDN custom op. Attempt 2 (#28083)
1	1	vllm/config/compilation.py
103	0	vllm/model_executor/layers/layernorm.py
101	52	vllm/model_executor/models/qwen3_next.py

[176765855] Dayeol Lee 2025-11-05 [Debugging] Add annotation for easier trace analysis (#22496)
19	3	vllm/v1/worker/gpu_worker.py

[efe73e9b5] Kuntai Du 2025-11-05 [Core][Hybrid allocator + connector 2/n] Unify `remove_skipped_blocks` by `get_last_useful_token` (#25431)
112	76	vllm/v1/core/single_type_kv_cache_manager.py

[0b8e871e5] Zhewen Li 2025-11-05 [CI/Build] Fix `test_defaults_with_usage_context` in AMD CI (#27926)
4	2	tests/v1/engine/test_engine_args.py

[5ee93a595] Zhewen Li 2025-11-05 [CI/Build] Update checking logic in cutlass_group_gemm_supported  (#27948)
5	1	vllm/_custom_ops.py

[e15601789] Snehlata 2025-11-06 [Feature]: Add corrupted request metric to V1 metrics system. (#27306)
2	1	tests/v1/metrics/test_stats.py
1	1	vllm/v1/core/sched/scheduler.py
4	0	vllm/v1/engine/__init__.py
24	5	vllm/v1/metrics/loggers.py
20	2	vllm/v1/metrics/stats.py
0	4	vllm/v1/request.py

[65ac8d8dc] Richard Zou 2025-11-05 [Docs] Add guide to debugging vLLM-torch.compile integration (#28094)
-	-	docs/assets/design/debug_vllm_compile/design_diagram.png
-	-	docs/assets/design/debug_vllm_compile/dynamic_shapes.png
-	-	docs/assets/design/debug_vllm_compile/tlparse_inductor.png
239	0	docs/design/debug_vllm_compile.md

[ffb08379d] Isotr0py 2025-11-06 [Chore] Remove Nemotron-Nano-VL config copy (#28126)
1	1	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	60	vllm/transformers_utils/configs/nemotron_vl.py

[e04492449] R3hankhan 2025-11-06 [Hardware][IBM Z] Optimize s390x Dockerfile (#28023)
11	57	docker/Dockerfile.s390x
2	2	requirements/common.txt
1	1	requirements/cpu.txt

[518ec6b72] Michael Yao 2025-11-06 [Docs] Clean up README_TUNING.md (#28088)
41	41	vllm/lora/ops/triton_ops/README_TUNING.md

[802748bdd] wang.yuqi 2025-11-06 [Bugfix] Fix Qwen3-Reranker-8B load (#28117)
19	9	vllm/model_executor/models/adapters.py

[faedbb4d4] Paul Zhang 2025-11-05 [Feature] Extend batch invariant torch.compile to B200 (#27856)
0	2	tests/v1/generation/test_batch_invariance.py
24	15	vllm/model_executor/layers/batch_invariant.py
6	0	vllm/utils/flashinfer.py

[40db19444] Samuel Shen 2025-11-05 [CI]: Add LMCacheConnector Unit Tests (#27852)
1	1	.buildkite/test-amd.yaml
1	0	.buildkite/test-pipeline.yaml
271	0	tests/v1/kv_connector/unit/test_lmcache_integration.py
1	5	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py

[c765f0b44] Chen Zhang 2025-11-05 [FlashInfer] Avoid FlashInfer block_size 16 + head_size 256 on blackwell (#27994)
12	0	vllm/model_executor/models/config.py
9	0	vllm/v1/attention/backends/flashinfer.py

[002b07c4b] gmagogsfm 2025-11-05 [Bugfix] vLLM should check Inductor config for compile cache enablement status (#27637)
5	2	vllm/compilation/backends.py
21	5	vllm/compilation/compiler_interface.py

[752ddeaca] Walter Beller-Morales 2025-11-05 [Core] add support for reasoning parser plugins (#28075)
19	0	vllm/config/structured_outputs.py
11	4	vllm/engine/arg_utils.py
9	3	vllm/entrypoints/openai/api_server.py
3	3	vllm/entrypoints/openai/run_batch.py
11	5	vllm/reasoning/basic_parsers.py
9	0	vllm/v1/structured_output/__init__.py

[c18f88c6c] Jiangyun Zhu 2025-11-06 [Kernel] Fuse computation of g and beta for Gated Delta Net (#28095)
30	9	vllm/model_executor/models/qwen3_next.py

[6fd0df813] Jiaju Zhang 2025-11-06 [misc] add vLLM Beijing Meetup (#28127)
1	0	README.md
1	0	docs/community/meetups.md

[3f5a4b647] Isotr0py 2025-11-06 [Bugfix] Validate custom logits processor xargs for online serving (#27560)
13	1	docs/design/logits_processors.md
3	0	docs/features/custom_arguments.md
33	9	docs/features/custom_logitsprocs.md
17	2	examples/offline_inference/logits_processor/custom.py
8	7	examples/offline_inference/logits_processor/custom_req.py
8	7	examples/offline_inference/logits_processor/custom_req_init.py
1	0	tests/entrypoints/openai/test_lora_resolvers.py
1	0	tests/entrypoints/openai/test_serving_chat.py
29	0	tests/v1/logits_processors/test_custom_online.py
15	2	tests/v1/logits_processors/utils.py
2	2	vllm/entrypoints/openai/protocol.py
8	0	vllm/entrypoints/openai/serving_chat.py
9	0	vllm/entrypoints/openai/serving_completion.py
23	17	vllm/model_executor/models/deepseek_ocr.py
6	0	vllm/transformers_utils/configs/deepseek_vl2.py
28	0	vllm/utils/torch_utils.py
20	2	vllm/v1/sample/logits_processor/__init__.py
8	0	vllm/v1/sample/logits_processor/interface.py

[6cae1e533] Pleaplusone 2025-11-05 [ROCm][MLA] Support block-size > 1 for AITER MLA backend  (#27224)
0	7	tests/kernels/attention/test_attention_selector.py
3	10	vllm/platforms/rocm.py
31	7	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[80c927534] Alexei-V-Ivanov-AMD 2025-11-05 Enabling cooperative multi-gpu tests on multi-gpu nodes (#27986)
10	1	.buildkite/scripts/hardware_ci/run-amd-test.sh

[e50c45467] Ilya Markov 2025-11-05 [BugFix] Support EP/DP + EPLB with MTP (#25311)
3	2	.buildkite/test-pipeline.yaml
96	0	tests/distributed/test_eplb_spec_decode.py
375	184	vllm/distributed/eplb/eplb_state.py
1	1	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
7	0	vllm/model_executor/models/deepseek_eagle.py
27	1	vllm/model_executor/models/deepseek_mtp.py
57	49	vllm/model_executor/models/deepseek_v2.py
1	17	vllm/model_executor/models/ernie45_moe.py
36	29	vllm/model_executor/models/glm4_moe.py
31	3	vllm/model_executor/models/glm4_moe_mtp.py
2	18	vllm/model_executor/models/hunyuan_v1.py
13	1	vllm/model_executor/models/interfaces.py
2	18	vllm/model_executor/models/lfm2_moe.py
80	2	vllm/model_executor/models/llama4.py
3	0	vllm/model_executor/models/llama4_eagle.py
3	0	vllm/model_executor/models/minicpm.py
2	18	vllm/model_executor/models/mixtral.py
31	1	vllm/model_executor/models/mllama4.py
1	17	vllm/model_executor/models/nemotron_h.py
1	17	vllm/model_executor/models/openpangu.py
2	18	vllm/model_executor/models/qwen3_moe.py
52	59	vllm/model_executor/models/qwen3_next.py
3	1	vllm/model_executor/models/qwen3_next_mtp.py
6	4	vllm/model_executor/models/transformers/moe.py
5	0	vllm/v1/spec_decode/medusa.py
55	33	vllm/v1/worker/gpu_model_runner.py
61	35	vllm/v1/worker/gpu_worker.py

[5d16d0fa6] Chen Zhang 2025-11-05 [DCP] check return_lse for all layers in dcp (#27929)
1	4	vllm/v1/worker/gpu_model_runner.py

[0606bea2b] bigmoyan 2025-11-05 add kimi reasoning parser (#28128)
4	0	vllm/reasoning/__init__.py

[6e97eccf5] Frost Mitchell 2025-11-05 [XPU] Enable custom routing functions in IPEX for Llama4 (#28004)
1	1	vllm/model_executor/layers/fused_moe/layer.py

[6ab183813] Boyuan Feng 2025-11-05 [Graph Partition][Cache] Use inductor partition ops config (#27702)
2	3	vllm/compilation/backends.py
12	27	vllm/compilation/partition_rules.py
1	29	vllm/compilation/pass_manager.py
22	4	vllm/env_override.py

[6b7a81185] amirkl94 2025-11-05 Bugfix: Cutlass FP8 FusedMoE bad scaling factors (#27255)
18	6	tests/kernels/moe/test_flashinfer.py
8	0	vllm/model_executor/layers/fused_moe/config.py
8	6	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
6	2	vllm/model_executor/layers/quantization/modelopt.py

[b57789b62] Eric Yue 2025-11-05 Fix excessive logging noise by reducing the log level of the MinimaxM2ToolParser import success message (#27635)
1	1	vllm/entrypoints/openai/tool_parsers/minimax_m2_tool_parser.py

[377061d48] Chauncey 2025-11-05 [Misc] fix import error for DeepSeekR1ReasoningParser (#28114)
2	4	vllm/reasoning/deepseek_v3_reasoning_parser.py

[86dca07d9] Kuntai Du 2025-11-05 [Hybrid allocator + kv connector] revert connector test changes related to hybrid allocator (#28011)
0	7	tests/v1/core/test_scheduler.py
0	2	tests/v1/core/utils.py
0	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
0	2	tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh
0	1	tests/v1/kv_connector/unit/test_multi_connector.py
0	1	tests/v1/kv_connector/unit/test_nixl_connector.py
0	1	tests/v1/kv_connector/unit/test_shared_storage_connector.py
0	3	tests/v1/kv_connector/unit/utils.py

[16b37f311] Qiu 2025-11-05 [bugfix] fix wrong `dcp_local_seq_lens` calc (#27518)
1	1	vllm/v1/attention/backends/mla/common.py

[0976711f3] Chauncey 2025-11-05 [Refactor] to simplify and extract the shared logic between chat completion and responses (#27961)
35	61	vllm/entrypoints/openai/serving_chat.py
77	1	vllm/entrypoints/openai/serving_engine.py

[e261d37c9] Chauncey 2025-11-05 [Refactor] Lazy-loaded reasoning_parser (#28092)
6	1	docs/features/reasoning_outputs.md
3	5	tests/reasoning/test_deepseekv3_reasoning_parser.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/run_batch.py
3	3	vllm/entrypoints/openai/tool_parsers/__init__.py
80	31	vllm/reasoning/__init__.py
102	29	vllm/reasoning/abs_reasoning_parsers.py
0	2	vllm/reasoning/deepseek_r1_reasoning_parser.py
0	2	vllm/reasoning/deepseek_v3_reasoning_parser.py
0	2	vllm/reasoning/ernie45_reasoning_parser.py
1	2	vllm/reasoning/glm4_moe_reasoning_parser.py
1	2	vllm/reasoning/gptoss_reasoning_parser.py
1	2	vllm/reasoning/granite_reasoning_parser.py
1	2	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
1	3	vllm/reasoning/minimax_m2_reasoning_parser.py
1	2	vllm/reasoning/mistral_reasoning_parser.py
1	2	vllm/reasoning/olmo3_reasoning_parser.py
0	2	vllm/reasoning/qwen3_reasoning_parser.py
1	2	vllm/reasoning/seedoss_reasoning_parser.py
1	2	vllm/reasoning/step3_reasoning_parser.py

[b7cbc2541] Alex Brooks 2025-11-05 [Model, Core] Support Granite Speech & LoRA for STT (#24455)
1	0	docs/models/supported_models.md
35	0	tests/entrypoints/openai/test_transcription_validation.py
34	0	tests/entrypoints/openai/test_translation_validation.py
2	6	vllm/entrypoints/openai/speech_to_text.py
97	2	vllm/model_executor/models/granite_speech.py

[d43ad5a75] Lucas Wilkinson 2025-11-05 [BugFix] Fix DCP Assert (AssertionError: DCP not support reorder_batch_threshold > 1 now.) (#28100)
2	1	vllm/v1/attention/backends/mla/common.py
6	1	vllm/v1/attention/backends/mla/flashattn_mla.py
10	1	vllm/v1/attention/backends/utils.py

[0ff05e377] Isotr0py 2025-11-05 [Bugfix] Fix encoder-only model support for transformers backend (#28021)
6	6	tests/models/registry.py
1	1	tests/models/test_transformers.py
8	2	vllm/model_executor/models/transformers/base.py
1	1	vllm/model_executor/models/transformers/moe.py

[428bc7bf1] wangxiyuan 2025-11-05 [V0 deprecation] Remove VLLM_USE_V1 usage in most modules (#27955)
0	2	docs/usage/v1_guide.md
0	20	tests/conftest.py
2	5	tests/v1/engine/test_async_llm.py
0	3	tests/v1/entrypoints/llm/test_struct_output_generate.py
59	62	tests/v1/sample/test_logprobs.py
6	12	vllm/attention/layers/chunked_local_attention.py
4	10	vllm/attention/layers/cross_attention.py
4	11	vllm/attention/layers/encoder_only_attention.py
1	7	vllm/attention/selector.py
0	7	vllm/distributed/kv_transfer/kv_connector/factory.py
5	9	vllm/distributed/kv_transfer/kv_transfer_state.py
4	12	vllm/entrypoints/cli/serve.py
1	7	vllm/entrypoints/openai/api_server.py
18	39	vllm/entrypoints/openai/protocol.py
0	2	vllm/model_executor/model_loader/tensorizer.py
0	8	vllm/model_executor/models/config.py
4	6	vllm/model_executor/models/gemma3_mm.py
2	6	vllm/model_executor/models/utils.py
0	13	vllm/multimodal/profiling.py

[878fd5a16] Zhewen Li 2025-11-04 [CI/Build] Enable some fixed tests in AMD CI (#28078)
10	10	.buildkite/test-amd.yaml

[18b39828d] Kunshang Ji 2025-11-05 [XPU] Add gpt-oss model support for Intel GPU (#27786)
7	0	vllm/attention/utils/fa_utils.py
92	2	vllm/model_executor/layers/quantization/mxfp4.py
0	3	vllm/model_executor/models/gpt_oss.py
2	1	vllm/v1/attention/backends/flash_attn.py

[4ea62b77f] tou 2025-11-05 [Qwen3-Next] MOE configs for A100-SXM4-80GB TP4 TP8 (#27740)
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_A100-SXM4-80GB.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_A100-SXM4-80GB.json

[d4e547bb7] Vadim Gimpelson 2025-11-05 Revert "[PERF] Decouple projections from GDN custom op" (#28080)
1	1	vllm/config/compilation.py
0	102	vllm/model_executor/layers/layernorm.py
52	101	vllm/model_executor/models/qwen3_next.py

[2d977a7a9] Aleksandr Malyshev 2025-11-04 [ROCm] gemm_a16w16 upstreaming (#26969)
34	8	vllm/model_executor/layers/utils.py
9	1	vllm/model_executor/models/gpt_oss.py

[1fb4217a0] Chenheli Hua 2025-11-04 [Multimodal] Make MediaConnector extensible. (#27759)
7	3	vllm/entrypoints/chat_utils.py
9	0	vllm/envs.py
4	0	vllm/multimodal/utils.py
2	19	vllm/multimodal/video.py
49	0	vllm/utils/registry.py

[611c86ea3] nadavkluger 2025-11-04 Added disable rule to track files under benchmarks/lib (#28048)
3	0	.gitignore

[dc937175d] Pleaplusone 2025-11-05 [ROCm][Perf] New design on ROCm AITER MHA backend Implementation (#25763)
526	275	vllm/v1/attention/backends/rocm_aiter_fa.py
67	0	vllm/v1/attention/backends/utils.py

[2f1cc8cef] Harry Mellor 2025-11-04 Remove deprecated `--rope-scaling` and `--rope-theta` (#28006)
0	27	vllm/config/model.py
0	6	vllm/engine/arg_utils.py

[938a81692] Nick Hill 2025-11-04 [AsyncScheduling] Don't schedule past request max_tokens (#27922)
7	0	tests/v1/core/test_async_scheduler.py
0	1	tests/v1/e2e/test_spec_decode.py
7	3	vllm/v1/core/sched/scheduler.py

[c9f66da8f] Nick Hill 2025-11-04 [PerfFix] Avoid separate thread for MP executor shm spin (#28012)
2	1	tests/v1/executor/test_executor.py
12	20	tests/v1/kv_connector/unit/test_output_aggregator.py
14	29	vllm/distributed/kv_transfer/kv_connector/utils.py
2	2	vllm/v1/executor/abstract.py
67	61	vllm/v1/executor/multiproc_executor.py
5	6	vllm/v1/executor/ray_executor.py
4	4	vllm/v1/executor/ray_utils.py
36	7	vllm/v1/executor/uniproc_executor.py
1	1	vllm/v1/worker/gpu_worker.py

[05cae69f0] yt0428 2025-11-05 [model] Add support for openPangu_Ultra_MoE (#27521)
2	0	docs/models/supported_models.md
13	0	tests/models/registry.py
3	0	vllm/config/model.py
9	0	vllm/config/speculative.py
1078	0	vllm/model_executor/models/openpangu.py
265	0	vllm/model_executor/models/openpangu_mtp.py
3	0	vllm/model_executor/models/registry.py
6	1	vllm/v1/spec_decode/eagle.py

[5fd8f02ea] Vadim Gimpelson 2025-11-04 [PERF] Decouple projections from GDN custom op (#27512)
1	1	vllm/config/compilation.py
102	0	vllm/model_executor/layers/layernorm.py
101	52	vllm/model_executor/models/qwen3_next.py

[97e3dda84] lyrisz 2025-11-04 [Perf] SM100 - add swap AB optimization to CUTLASS FP8 GEMM (#27284)
4	5	csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm100_fp8.cu
229	47	csrc/quantization/w8a8/cutlass/c3x/scaled_mm_sm100_fp8_dispatch.cuh

[5a0a6dfd5] Nick Hill 2025-11-04 [BugFix] Fix incorrect preallocated sampled_token_ids tensor size (#28025)
1	1	vllm/v1/worker/gpu_model_runner.py

[938772af0] bnellnm 2025-11-04 [Kernels] Isolate modular kernel code from FusedMoEMethodBase subclasses. (#27123)
2	2	vllm/distributed/device_communicators/base_device_communicator.py
199	62	vllm/model_executor/layers/fused_moe/layer.py
6	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
0	2	vllm/model_executor/layers/quantization/awq_marlin.py
1	2	vllm/model_executor/layers/quantization/bitsandbytes.py
0	47	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
0	2	vllm/model_executor/layers/quantization/experts_int8.py
9	26	vllm/model_executor/layers/quantization/fp8.py
0	2	vllm/model_executor/layers/quantization/gguf.py
0	2	vllm/model_executor/layers/quantization/gptq_marlin.py
11	39	vllm/model_executor/layers/quantization/modelopt.py
0	2	vllm/model_executor/layers/quantization/moe_wna16.py
15	90	vllm/model_executor/layers/quantization/mxfp4.py
25	28	vllm/model_executor/layers/quantization/quark/quark_moe.py
0	2	vllm/model_executor/layers/quantization/rtn.py
3	3	vllm/model_executor/warmup/deep_gemm_warmup.py

[e4ee65867] tomeras91 2025-11-04 [Model] add optimal triton fused moe configs for NemotronH MoE (#27967)
1	0	benchmarks/kernels/benchmark_moe.py
147	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_H100_80GB_HBM3.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1856,device_name=NVIDIA_L40S.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=928,device_name=NVIDIA_H100_80GB_HBM3.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=928,device_name=NVIDIA_L40S.json

[77f8001f5] tomeras91 2025-11-04 [Model][Bugfix] fix pipeline parallelism support for NemotronH (#27968)
13	5	vllm/model_executor/models/nemotron_h.py

[300a26597] Zhuohan Li 2025-11-04 [Core] Enable StatLogger in LLMEngine (#28020)
0	5	vllm/v1/engine/llm_engine.py

[03c4c4aa9] Jerry Zhang 2025-11-04 Support using Int4PreshuffledTensor after loading (#26066)
144	2	tests/quantization/test_torchao.py
64	2	vllm/model_executor/layers/quantization/torchao.py

[2ec401bc3] yugong333 2025-11-04 Load tuned fused_moe_lora shrink and expand kernel configs separately (#27435)
446	30	benchmarks/kernels/benchmark_lora.py
11	0	tests/lora/test_fused_moe_lora_kernel.py
83	20	vllm/lora/layers/fused_moe.py
10	1	vllm/lora/ops/triton_ops/README_TUNING.md
8	1	vllm/lora/ops/triton_ops/__init__.py
296	60	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
38	5	vllm/lora/ops/triton_ops/utils.py
2	1	vllm/lora/punica_wrapper/punica_base.py
16	6	vllm/lora/punica_wrapper/punica_gpu.py

[4022a9d27] Varun Sundar Rabindranath 2025-11-04 [BugFix][Performance] Restore flashinfer autotuning for all scenarios (#27904)
2	14	tests/quantization/test_blackwell_moe.py
9	2	vllm/model_executor/layers/fused_moe/trtllm_moe.py
2	2	vllm/model_executor/layers/quantization/mxfp4.py
1	26	vllm/model_executor/warmup/kernel_warmup.py

[53f6e81df] Zhewen Li 2025-11-03 [CI/Build] Fix OpenAI API correctness on AMD CI (#28022)
4	3	.buildkite/test-amd.yaml

[43a6acfb7] CSWYF3634076 2025-11-04 [Model] fix ernie45 reasoning_parser (#27973)
2	2	vllm/reasoning/ernie45_reasoning_parser.py

[58279c60b] Mark McLoughlin 2025-11-04 [KV Connector] Make KVCacheConfig an explicit constructor argument (#27887)
275	0	tests/v1/kv_connector/unit/test_backwards_compatibility.py
1	1	tests/v1/kv_connector/unit/utils.py
33	8	vllm/distributed/kv_transfer/kv_connector/factory.py
15	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
9	3	vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py
10	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
11	3	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
10	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
8	2	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
13	3	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
13	3	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
8	3	vllm/distributed/kv_transfer/kv_transfer_state.py
3	9	vllm/v1/core/sched/scheduler.py
1	3	vllm/v1/worker/gpu_worker.py

[2f84ae1f2] Zhewen Li 2025-11-03 [CI/Build] Update LM Eval Version in AMD CI (#27944)
0	1	docker/Dockerfile.rocm
9	6	requirements/rocm-test.txt

[f32cbc9a0] xiangze-arm 2025-11-04 [CPU]Improve dynamic 4bit moe performance (#27240)
12	21	csrc/moe/dynamic_4bit_int_moe_cpu.cpp

[7e4be7410] Wentao Ye 2025-11-04 [Bug] Batch invariant: Fix flash attn MLA `RuntimeError: scheduler_metadata must have shape (metadata_size)` (#27884)
2	0	vllm/model_executor/layers/batch_invariant.py
3	3	vllm/v1/attention/backends/mla/flashattn_mla.py

[380ba6816] Mark McLoughlin 2025-11-04 [Metrics] Enable sleep state metric outside of dev mode (#27867)
23	27	vllm/v1/metrics/loggers.py

[14a125a06] liuzhenwei 2025-11-04 [NIXL][XPU] Pin NIXL version to 0.7.0 (#27849)
29	2	tools/install_nixl_from_source_ubuntu.py

[c02fccdbd] Chauncey 2025-11-04 [Refactor] Lazy import tool_parser (#27974)
6	1	docs/features/tool_calling.md
3	1	tests/tool_use/test_deepseekv31_tool_parser.py
1	1	tests/tool_use/test_ernie45_moe_tool_parser.py
3	1	tests/tool_use/test_glm4_moe_tool_parser.py
1	1	tests/tool_use/test_jamba_tool_parser.py
1	1	tests/tool_use/test_kimi_k2_tool_parser.py
1	1	tests/tool_use/test_minimax_tool_parser.py
1	1	tests/tool_use/test_openai_tool_parser.py
1	1	tests/tool_use/test_seed_oss_tool_parser.py
1	1	tests/tool_use/test_xlam_tool_parser.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/cli_args.py
139	58	vllm/entrypoints/openai/tool_parsers/__init__.py
106	36	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/ernie45_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/hunyuan_a13b_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	2	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
0	3	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/longcat_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/minimax_m2_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/olmo3_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py
0	2	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[6ddae7405] li2haipeng 2025-11-03 [LoRA] Lora shrink swizzle (#27694)
13	2	vllm/lora/ops/triton_ops/lora_shrink_op.py
1	0	vllm/lora/ops/triton_ops/utils.py

[b13a44754] vllmellm 2025-11-04 [Bugfix][ROCm] Fix ViT rotary embeddings for torch.compile compatibility on ROCm (#27748)
7	4	vllm/model_executor/layers/rotary_embedding/common.py
1	1	vllm/model_executor/models/glm4_1v.py

[7956b0c0b] QiliangCui 2025-11-03 Remove the tpu docker image nightly build. (#27997)
0	18	.buildkite/release-pipeline.yaml

[375875737] Tyler Michael Smith 2025-11-03 [Bugfix] Fix MoE Routing Simulation (#28002)
1	1	vllm/model_executor/layers/fused_moe/layer.py
10	0	vllm/model_executor/layers/fused_moe/routing_simulator.py

[ccd3e55e5] Hank_ 2025-11-04 [Bugfix][plugin] fla crash on plugin (#27322)
3	2	vllm/model_executor/layers/fla/ops/utils.py

[01baefe67] Matthew Bonanni 2025-11-03 Add TP parameter to attention tests (#27683)
1	2	.buildkite/test-pipeline.yaml
53	5	tests/v1/attention/test_attention_backends.py
29	2	tests/v1/attention/test_mla_backends.py
9	2	tests/v1/attention/test_sparse_mla_backends.py

[786030721] Ning Xie 2025-11-04 [Docs] add runai_streamer_sharded to LoadConfig (#27937)
2	0	vllm/config/load.py

[145c00a4d] Matthew Bonanni 2025-11-03 [Bugfix] change FlashMLA reorder_batch_threshold (#27777)
1	1	vllm/v1/attention/backends/mla/flashmla.py

[55011aef2] Lucas Kabela 2025-11-03 [Bugfix][Qwen][Multimodal] Move Qwen2_5_vl sdpa to custom op and reenable compile (#27764)
53	0	vllm/attention/ops/vit_attn_wrappers.py
16	28	vllm/model_executor/models/qwen2_5_vl.py

[a4398fbb5] Sophie du Couédic 2025-11-03 [Feature][Benchmarks] Support `inf` burstiness (#26941)
7	0	vllm/benchmarks/serve.py

[2c19d9677] Aurick Qiao 2025-11-03 [Spec Decode] Integrate Suffix Decoding from Arctic Inference (#25784)
40	0	docs/features/spec_decode.md
1	0	requirements/test.in
2	0	requirements/test.txt
78	7	tests/v1/e2e/test_spec_decode.py
64	2	vllm/config/speculative.py
6	0	vllm/utils/import_utils.py
101	0	vllm/v1/spec_decode/suffix_decoding.py
12	2	vllm/v1/worker/gpu_model_runner.py

[4bc400f47] Lucas Wilkinson 2025-11-04 [CI/Testing] Add basic single node dual batch overlap test (#27235)
2	0	.buildkite/test-pipeline.yaml
89	0	tests/v1/distributed/test_dbo.py

[cac4c10ef] ahao-anyscale 2025-11-03 [BUG] Make 'binary' default option for saving torch compile artifacts when using standalone_compile (#27616)
2	0	docs/design/torch_compile.md
3	1	vllm/compilation/backends.py
6	3	vllm/compilation/compiler_interface.py
22	1	vllm/config/compilation.py
10	0	vllm/envs.py

[f7d2946e9] pwschuurman 2025-11-03 [Bugfix] Skip gs:// model paths for speculator detection (#27846)
26	0	tests/transformers_utils/test_utils.py
5	5	vllm/engine/arg_utils.py
8	0	vllm/transformers_utils/utils.py

[294c805f1] gnovack 2025-11-03 Early exit for MoE LoRA kernels (#27131)
16	11	csrc/moe/moe_lora_align_sum_kernels.cu
7	8	csrc/moe/moe_ops.h
3	1	csrc/moe/torch_bindings.cpp
6	0	tests/lora/test_fused_moe_lora_kernel.py
4	0	tests/lora/test_moe_lora_align_sum.py
43	7	tests/lora/test_olmoe_tp.py
4	0	vllm/_custom_ops.py
10	1	vllm/lora/layers/fused_moe.py
20	5	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
2	0	vllm/lora/punica_wrapper/punica_base.py
8	1	vllm/lora/punica_wrapper/punica_gpu.py

[40b69e33e] zhang-prog 2025-11-03 [Model] Add PaddleOCR-VL Model Support  (#27758)
1	0	docs/models/supported_models.md
27	0	examples/offline_inference/vision_language.py
22	0	examples/offline_inference/vision_language_multi_image.py
4	0	tests/models/registry.py
10	0	vllm/model_executor/models/ernie45.py
1407	0	vllm/model_executor/models/paddleocr_vl.py
4	0	vllm/model_executor/models/registry.py

[32257297d] Jee Jee Li 2025-11-03 [CI/Build] Remove the flaky gpt-oss lora test (#27966)
0	5	tests/lora/test_gptoss_tp.py

[ba464e6ae] Misha Efimov 2025-11-03 Add ORCA endpoint load metrics support (#24905)
128	0	tests/entrypoints/openai/test_orca_metrics.py
17	2	vllm/entrypoints/openai/api_server.py
120	0	vllm/entrypoints/openai/orca_metrics.py

[7f4bdadb9] Kunshang Ji 2025-11-03 [XPU]Refine Dockerfile.xpu, avoid oneccl dependency issue (#27964)
4	4	docker/Dockerfile.xpu

[cec7c2883] Rémi Delacourt 2025-11-03 [Bugfix] Padded Eagle Specdec with Chunked Prefill (#26263)
21	7	tests/v1/e2e/test_spec_decode.py

[18961c5ea] Thomas Parnell 2025-11-03 [Hybrid] Pass kernel block size to builders (#27753)
5	1	vllm/v1/attention/backends/flash_attn.py
7	1	vllm/v1/kv_cache_interface.py
25	6	vllm/v1/worker/gpu_model_runner.py
25	19	vllm/v1/worker/utils.py

[470ad118b] Sungyoon Jeong 2025-11-03 [Frontend] Align finish_reason when tool is called with OpenAI (#25054)
18	8	vllm/entrypoints/openai/serving_chat.py

[1bf43ae35] Biswa Panda 2025-11-02 [BugFix][LoRA] use adapter_id instead of id field of lora_request (#27728)
61	2	tests/v1/core/test_prefix_caching.py
3	1	vllm/v1/core/block_pool.py

[0ce743f4e] Vensen 2025-11-03 Fix(llm): Abort orphaned requests when llm.chat() batch fails Fixes #26081 (#27420)
53	0	tests/entrypoints/llm/test_chat.py
22	14	vllm/entrypoints/llm.py

[6c317a656] Cyrus Leung 2025-11-02 [Misc] Provide Siglip2 chat template (#27939)
1	0	vllm/transformers_utils/chat_templates/registry.py

[00b31a36a] Asaf Joseph Gardin 2025-11-02 [V1] [Hybrid] Mamba1 Automatic Prefix Caching (#26377)
7	1	csrc/mamba/mamba_ssm/selective_scan.h
113	21	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
13	11	csrc/ops.h
5	1	csrc/torch_bindings.cpp
15	0	tests/kernels/mamba/test_mamba_ssm.py
14	20	tests/models/language/generation/test_hybrid.py
8	0	vllm/_custom_ops.py
6	0	vllm/config/model.py
60	31	vllm/model_executor/layers/mamba/mamba_mixer.py
23	1	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
1	1	vllm/model_executor/models/config.py
15	6	vllm/model_executor/models/jamba.py
4	5	vllm/model_executor/models/mamba.py
95	16	vllm/v1/attention/backends/mamba1_attn.py
6	34	vllm/v1/attention/backends/mamba2_attn.py
57	5	vllm/v1/attention/backends/mamba_attn.py

[73444b7b5] Julien Denize 2025-11-02 Performance fix MistralTokenizer: cache special ids and tokens (#27925)
32	34	vllm/transformers_utils/tokenizers/mistral.py

[853a8eb53] Cyrus Leung 2025-11-02 [Bugfix] Fix Qwen Omni audio inference (#27920)
2	7	vllm/model_executor/models/qwen2_5_omni_thinker.py
0	3	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[758ea2e98] Ben Browning 2025-11-01 [CI/Build] Fix flaky test_transcription_validation.py::test_basic_audio_gemma (#27924)
3	1	tests/entrypoints/openai/test_transcription_validation.py

[685c99ee7] Yue Zhang 2025-11-02 [KV offload] Offloading connector async scheduling support (#27648)
2	2	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py

[1e88fb751] Benjamin Bartels 2025-11-01 Adds anthropic /v1/messages endpoint to openai api_server (#27882)
0	0	tests/entrypoints/anthropic/__init__.py
35	35	tests/entrypoints/{anthropic => openai}/test_messages.py
17	125	tests/utils.py
0	301	vllm/entrypoints/anthropic/api_server.py
86	0	vllm/entrypoints/openai/api_server.py

[c2ed069b3] Nick Hill 2025-11-01 [BugFix] Fix mixed penalties batch with async scheduling (#27910)
8	0	vllm/v1/sample/ops/penalties.py

[af6e19f50] wenxindongwork 2025-11-01 [Core][TPU] Support TPU Data Parallalism (#27365)
6	1	vllm/entrypoints/llm.py

[99d69af9e] Cyrus Leung 2025-11-01 [Bugfix] Python 3.10 compatibility for `Self` (#27918)
2	1	vllm/config/structured_outputs.py

[d811b442d] Haco 2025-11-01 [Bugfix] DeepSeek V3.2 MTP metadata & CUDA graph issues (#26779)
8	5	vllm/v1/spec_decode/eagle.py

[30a14b034] wangxiyuan 2025-11-01 [V0 deprecation] Remove VLLM_USE_V1 usage in platform and v1 module (#27798)
84	100	vllm/platforms/cuda.py
1	8	vllm/platforms/interface.py
32	52	vllm/platforms/rocm.py
0	4	vllm/platforms/tpu.py
3	6	vllm/platforms/xpu.py
0	16	vllm/v1/engine/async_llm.py
1	10	vllm/v1/engine/llm_engine.py
4	5	vllm/v1/executor/uniproc_executor.py

[799ce45cc] Harry Mellor 2025-11-01 [Docs] Mock all imports for docs (#27873)
47	13	docs/mkdocs/hooks/generate_argparse.py
0	8	requirements/docs.txt
2	2	vllm/utils/cache.py

[2c0c7c39b] ai-jz 2025-11-01 feat(benchmarks): support HF model names in multi-turn benchmark (#27850)
0	2	benchmarks/multi_turn/benchmark_serving_multi_turn.py

[e67511884] Yihua Cheng 2025-11-01 [Add] cmdline argument parsing for KV cache offloading modules (#27621)
65	0	tests/v1/kv_connector/unit/test_config.py
12	0	vllm/config/cache.py
45	0	vllm/config/vllm.py
20	1	vllm/engine/arg_utils.py

[e2347dbf5] TJian 2025-10-31 [Bugfix] [Model] Missing MRoPE function definition from `KeyeForConditionalGeneration` (#27895)
86	0	tests/models/multimodal/generation/test_keye.py
168	17	vllm/model_executor/models/keye.py

[879a06579] Cyrus Leung 2025-11-01 [CI/Build] Bump transformers version (#27528)
1	1	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
1	1	requirements/test.txt
2	0	tests/models/multimodal/generation/test_maverick.py
6	6	tests/models/registry.py
1	1	tests/models/test_transformers.py
2	2	vllm/model_executor/models/moonvit.py
2	4	vllm/model_executor/models/qwen2_vl.py

[29de3cdee] yugong333 2025-10-31 Adding SplitK in fused_moe_lora kernel (#27818)
9	5	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[7e2729b57] Yan Ma 2025-11-01 [Multimodal][XPU]Enable vision attn backend for xpu platform (#27525)
59	25	vllm/_ipex_ops.py
18	17	vllm/attention/layer.py
1	1	vllm/attention/ops/vit_attn_wrappers.py
3	4	vllm/model_executor/models/qwen2_5_vl.py
1	4	vllm/model_executor/models/qwen2_vl.py
6	0	vllm/platforms/xpu.py

[3a5de7d2d] Jee Jee Li 2025-11-01 [Bugfix] Fix KDA output (#27905)
2	3	vllm/model_executor/layers/kda.py

[bc4486d60] Jee Jee Li 2025-11-01 [Kernel] Enable FusedMoEModularKernel  support  bias (#27754)
15	28	vllm/lora/layers/fused_moe.py
0	2	vllm/model_executor/layers/fused_moe/layer.py

[0cdbe7b74] Nick Hill 2025-10-31 [Core] Async scheduling + structured outputs compatibility (#26866)
3	0	tests/conftest.py
0	9	tests/v1/core/test_scheduler.py
12	2	tests/v1/e2e/{test_async_sched_and_preempt.py => test_async_scheduling.py}
18	1	tests/v1/engine/test_engine_core.py
3	1	tests/v1/executor/test_executor.py
0	2	tests/v1/kv_connector/unit/test_kv_connector_lifecyle.py
1	3	tests/v1/kv_connector/unit/test_nixl_connector.py
0	12	tests/v1/tpu/worker/test_tpu_model_runner.py
0	12	tests/v1/worker/test_gpu_model_runner.py
18	12	vllm/distributed/kv_transfer/kv_connector/utils.py
8	0	vllm/v1/core/sched/async_scheduler.py
7	1	vllm/v1/core/sched/interface.py
11	6	vllm/v1/core/sched/output.py
13	18	vllm/v1/core/sched/scheduler.py
57	14	vllm/v1/engine/core.py
26	10	vllm/v1/executor/abstract.py
26	17	vllm/v1/executor/multiproc_executor.py
33	4	vllm/v1/executor/ray_executor.py
20	15	vllm/v1/executor/ray_utils.py
22	15	vllm/v1/structured_output/utils.py
68	11	vllm/v1/worker/gpu_model_runner.py
12	5	vllm/v1/worker/gpu_worker.py
35	10	vllm/v1/worker/tpu_model_runner.py
6	7	vllm/v1/worker/tpu_worker.py
20	4	vllm/v1/worker/worker_base.py

[df334868c] Chen Zhang 2025-10-31 [Hybrid] A simpler algorithm to find kernel_block_size (#26476)
53	0	tests/v1/worker/test_gpu_model_runner.py
91	84	vllm/v1/worker/gpu_model_runner.py
5	1	vllm/v1/worker/utils.py

[0e0a638c3] Bram Wasti 2025-10-31 Batch invariance doc (#27839)
133	0	docs/features/batch_invariance.md

[f29aeb5a2] Matthew Bonanni 2025-10-31 Add FLASHINFER_MLA to test_mla_backends and add B200 CI run (#27663)
10	0	.buildkite/test-pipeline.yaml
182	62	tests/v1/attention/test_mla_backends.py
11	1	tests/v1/attention/utils.py
5	1	vllm/v1/attention/backends/mla/flashinfer_mla.py

[5e8862e9e] Vinay R Damodaran 2025-10-31 [Feature] Pydantic validation for scheduler.py and structured_outputs.py (#26519)
31	31	vllm/config/scheduler.py
5	2	vllm/config/structured_outputs.py
1	1	vllm/engine/arg_utils.py
2	1	vllm/entrypoints/openai/tool_parsers/minimax_m2_tool_parser.py

[9e5bd3076] Nick Hill 2025-10-31 [Cleanup] Remove no-longer-used `SpeculativeConfig.enable_chunked_prefill` (#27826)
0	10	vllm/config/speculative.py
0	6	vllm/engine/arg_utils.py
1	0	vllm/entrypoints/openai/api_server.py

[fc16f1c47] Shu Wang 2025-10-31 Flashinfer_CUTLASS_MOE fuses quantization for TP (#27223)
5	1	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
9	8	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
0	23	vllm/model_executor/layers/quantization/modelopt.py
1	0	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[bc306fe5e] ZiTian Zhao 2025-11-01 fix incorrect type annotation in KimiMLP (#27885)
1	2	vllm/model_executor/models/kimi_linear.py

[103a468bb] Chenguang Zheng 2025-11-01 [bugfix] Missing cached item in beam search (#27874)
10	18	vllm/entrypoints/openai/serving_engine.py

[70bfbd7b1] Rob Mulla 2025-10-31 Docs update tpu install instructions (#27824)
1	1	docs/configuration/tpu.md
1	1	docs/getting_started/installation/.nav.yml
1	1	docs/getting_started/installation/README.md
0	193	docs/getting_started/installation/google_tpu.md
11	0	docs/getting_started/quickstart.md

[d6517be3c] GuanLuo 2025-11-01 [Bugfix] Missing NIXL metadata for handshake initialization if instance spans multi-node (#26338)
1	1	docs/features/nixl_connector_usage.md
102	4	tests/v1/kv_connector/unit/test_nixl_connector.py
32	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
136	88	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
22	1	vllm/v1/engine/core.py
8	0	vllm/v1/executor/abstract.py
20	1	vllm/v1/worker/gpu_worker.py

[7e06c40e6] Isotr0py 2025-11-01 [Bugfix] Fix broken MRoPE for GLM-4.1V/GLM-4.5V (#27860)
147	2	vllm/model_executor/models/glm4_1v.py

[675704ac0] Madeesh Kannan 2025-10-31 [Bugfix] Allow 64-bit integer values for LoRA IDs to avoid overflow/truncation (#27876)
1	1	vllm/v1/worker/gpu_input_batch.py
1	1	vllm/v1/worker/tpu_input_batch.py

[0384aa715] Jee Jee Li 2025-10-31 [CI/Build] Add gpt-oss LoRA test (#27870)
3	1	.buildkite/test-amd.yaml
2	1	.buildkite/test-pipeline.yaml
1	1	tests/lora/conftest.py
4	0	tests/lora/test_deepseekv2_tp.py
0	52	tests/lora/test_gptoss.py
106	0	tests/lora/test_gptoss_tp.py
4	0	tests/lora/test_qwen3moe_tp.py

[3857eb872] Jiangyun Zhu 2025-10-31 [Perf] Decouple torch op from GDA to leverage torch.compile (#27871)
68	48	vllm/model_executor/layers/kda.py

[933cdea44] Huamin Li 2025-10-31 [BugFix] Don’t compute reorder threshold when there are no attention groups (#27861)
5	0	vllm/v1/worker/gpu_model_runner.py

[3933f18a5] Isotr0py 2025-10-31 [Bugfix] Avoid too small block m/n for FlexAttention kernel option (#27853)
5	0	vllm/v1/attention/backends/flex_attention.py

[e5ef4dfc1] toncao 2025-10-31 [Kimi-Linear] Correct prefixes and add compatibility to AWQ quants (#27834)
2	1	vllm/model_executor/models/kimi_linear.py

[36960501d] Akash kaothalkar 2025-10-31 [Hardware][Powerpc] Fix VLLM_CPU_OMP_THREADS_BIND="auto"  low CPU utilization for Power (#27734)
2	1	vllm/platforms/cpu.py

[b2e65cb4a] Seiji Eicher 2025-10-30 [benchmark] Make request IDs unique across clients by default (#27723)
2	1	vllm/benchmarks/serve.py

[2bf0bcc1f] Wentao Ye 2025-10-30 [CI Test] Add Scheduled Integration Test (#27765)
62	0	.buildkite/scripts/scheduled_integration_test/deepseek_v2_lite_ep_eplb.sh
61	0	.buildkite/scripts/scheduled_integration_test/qwen30b_a3b_fp8_block_ep.sh
18	0	.buildkite/test-pipeline.yaml

[697f507a8] Jakub Sochacki 2025-10-31 [CI/Build][Intel] Enable performance benchmarks for Intel Gaudi 3 (#26919)
2	1	.buildkite/performance-benchmarks/README.md
3	3	.buildkite/performance-benchmarks/performance-benchmarks-descriptions.md
13	0	.buildkite/performance-benchmarks/scripts/run-performance-benchmarks.sh
55	0	.buildkite/performance-benchmarks/tests/latency-tests-hpu.json
82	0	.buildkite/performance-benchmarks/tests/serving-tests-hpu.json
61	0	.buildkite/performance-benchmarks/tests/throughput-tests-hpu.json

[d5d2a0fe7] Matthew Bonanni 2025-10-30 [Misc] Make all tool scripts executable (#27831)
0	0	tools/check_repo.sh
0	0	tools/ep_kernels/configure_system_drivers.sh
0	0	tools/ep_kernels/elastic_ep/install_eep_libraries.sh
1	0	tools/ep_kernels/install_python_libraries.sh
0	0	tools/flashinfer-build.sh
0	0	tools/vllm-tpu/build.sh

[c9791f181] Nick Hill 2025-10-30 [BugFix] Fix broken import in initialize_ray_cluster() (#27838)
1	1	vllm/v1/executor/ray_utils.py

[e7acb2007] Paul Zhang 2025-10-30 [Feature] Batch invariant torch.compile (#27660)
0	7	vllm/config/model.py
7	1	vllm/envs.py
71	0	vllm/model_executor/layers/batch_invariant.py
4	1	vllm/model_executor/layers/quantization/fp8.py

[4b68c4a55] Jialin Ouyang 2025-10-30 [Core][Perf] Only invoke save_new_computed_blocks when computed blocks are not empty (#27799)
6	5	vllm/v1/core/kv_cache_manager.py
1	1	vllm/v1/core/single_type_kv_cache_manager.py

[a8141fa64] Wentao Ye 2025-10-30 [Refactor] Remove `VLLM_DEEPEP_LOW_LATENCY_ALLOW_NVLINK` (#27750)
1	1	vllm/distributed/device_communicators/all2all.py
0	7	vllm/envs.py

[491700252] Sumanth R Hegde 2025-10-30 [Fix] Skip `record_sleep_state` logic in `PrometheusStatsLogger` if not in dev mode (#27789)
42	1	tests/basic_correctness/test_cumem.py
3	0	vllm/v1/metrics/loggers.py

[a2981c427] cong-meta 2025-10-30 [EP/DP][API Server] Enable DP-aware routing in OpenAI API requests (#24945)
76	0	tests/entrypoints/openai/test_serving_chat.py
4	0	vllm/entrypoints/openai/serving_chat.py
4	0	vllm/entrypoints/openai/serving_completion.py
15	0	vllm/entrypoints/openai/serving_engine.py

[4574d48ba] Jialin Ouyang 2025-10-30 [Core][Bookkeeping] Update cu_num_accepted_tokens for all req_index (#27629)
9	6	vllm/v1/worker/gpu_model_runner.py

[ab98f6556] Tyler Michael Smith 2025-10-30 [Bugfix] Fix 2 precommit issues - (mamba_block_size, kv_cache_config) (#27811)
1	1	vllm/model_executor/models/config.py
9	5	vllm/v1/core/sched/scheduler.py

[2918c1b49] Roger Meier 2025-10-31 [Model] Use the same fused_moe configs for all H200 devices (#23642)
3	0	vllm/model_executor/layers/fused_moe/fused_moe.py

[100420579] Mengqing Cao 2025-10-31 [MTP] Refactor mtp predictor to avoid d2h operation (#27643)
1	1	vllm/model_executor/models/deepseek_mtp.py

[ba33e8830] Huy Do 2025-10-30 Reapply "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" (#27768)
0	7	docker/Dockerfile
2	2	requirements/cuda.txt

[33a0ea5f3] Kebe 2025-10-31 [Docs] add Shanghai Meetup - 2025/10 (#27545)
1	0	README.md
1	0	docs/community/meetups.md

[60f76baa6] Ilya Markov 2025-10-30 [Misc] Replace CUDA_VISIBLE_DEVICES in DP with torch.cuda.set_device for device selection on cuda-like devices (#27564)
8	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
10	1	vllm/v1/engine/utils.py
2	2	vllm/v1/worker/dp_utils.py
23	0	vllm/v1/worker/gpu_worker.py

[e5e076cad] Varun Sundar Rabindranath 2025-10-30 [BugFix] Stopgap - Flashinfer Autotuner + GPT-OSS + DP/TP (#27762)
13	7	vllm/model_executor/warmup/kernel_warmup.py

[eebf00cb0] Li, Jiang 2025-10-30 [Bugfix][CPU] Fix MRoPE dispatch on the CPU backend (#27800)
9	0	vllm/model_executor/layers/rotary_embedding/mrope.py

[9956aae4e] Fan Yin 2025-10-30 [Model][Ouro] Support Ouro Model (#27794)
1	0	docs/models/supported_models.md
1	0	tests/models/registry.py
518	0	vllm/model_executor/models/ouro.py
1	0	vllm/model_executor/models/registry.py

[0fe014040] Zhewen Li 2025-10-30 [KV offload] Enable CPU KV offload on CUDA alike Platforms (#27770)
0	4	tests/v1/kv_offload/test_cpu_offloading.py
2	2	vllm/v1/kv_offload/cpu.py

[4e68cc9b6] Zhiyuan Li 2025-10-30 [Model] Introduce Kimi Linear to vLLM (#27809)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
1	0	vllm/config/compilation.py
1	0	vllm/config/model.py
1	1	vllm/model_executor/layers/fla/ops/kda.py
426	0	vllm/model_executor/layers/kda.py
41	0	vllm/model_executor/layers/mamba/mamba_utils.py
4	3	vllm/model_executor/layers/mla.py
25	26	vllm/model_executor/models/config.py
663	0	vllm/model_executor/models/kimi_linear.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
144	0	vllm/transformers_utils/configs/kimi_linear.py
12	19	vllm/v1/worker/gpu_model_runner.py

[1994de99e] Huamin Li 2025-10-30 [CI Failure] Fix test_kv_cache_model_load_and_run (#27717)
12	1	tests/quantization/test_fp8.py

[4464723f2] wang.yuqi 2025-10-30 [Frontend][Doc][5/N] Improve all pooling task | Polish encode (pooling) api & Document. (#25524)
1	1	docs/design/io_processor_plugins.md
68	15	docs/models/pooling_models.md
2	2	docs/serving/openai_compatible_server.md
12	0	examples/offline_inference/pooling/README.md
1	1	examples/offline_inference/pooling/ner.py
0	0	examples/offline_inference/{ => pooling}/prithvi_geospatial_mae.py
0	0	examples/offline_inference/{ => pooling}/prithvi_geospatial_mae_io_processor.py
35	5	examples/online_serving/pooling/README.md
0	0	examples/online_serving/{ => pooling}/openai_cross_encoder_score.py
0	0	examples/online_serving/{ => pooling}/openai_cross_encoder_score_for_multimodal.py
0	0	examples/online_serving/{ => pooling}/prithvi_geospatial_mae.py
7	5	tests/entrypoints/pooling/llm/test_classify.py
7	5	tests/entrypoints/pooling/llm/test_reward.py
5	5	tests/entrypoints/pooling/llm/test_score.py
72	20	tests/entrypoints/pooling/openai/test_classification.py
51	2	tests/entrypoints/pooling/openai/test_embedding.py
46	7	tests/entrypoints/pooling/openai/test_rerank.py
8	8	tests/entrypoints/pooling/openai/test_score.py
4	4	tests/models/language/pooling/test_pooler_config_init_behaviour.py
7	7	tests/test_pooling_params.py
32	6	vllm/config/pooler.py
2	6	vllm/entrypoints/openai/api_server.py
100	8	vllm/entrypoints/openai/protocol.py
16	7	vllm/entrypoints/openai/serving_pooling.py
2	2	vllm/model_executor/layers/pooler.py
2	2	vllm/model_executor/models/config.py
19	13	vllm/pooling_params.py

[74374386e] Sairam Pillai 2025-10-30 [Bugfix] Improve GPU validation logging in Ray fallback scenarios (#25775)
5	9	vllm/config/parallel.py
44	6	vllm/v1/executor/ray_utils.py

[c01f6e525] Wentao Ye 2025-10-30 [CI] Fix mypy for `vllm/v1/core` and `vllm/v1/engine` (#27108)
13	1	tools/pre_commit/mypy.py
4	5	vllm/config/vllm.py
1	0	vllm/engine/protocol.py
9	7	vllm/v1/core/sched/scheduler.py
13	8	vllm/v1/engine/async_llm.py
1	0	vllm/v1/engine/core.py
7	7	vllm/v1/engine/core_client.py
10	3	vllm/v1/engine/detokenizer.py
9	7	vllm/v1/engine/llm_engine.py
8	2	vllm/v1/engine/output_processor.py
2	2	vllm/v1/engine/parallel_sampling.py
14	19	vllm/v1/engine/processor.py

[c7d2a554b] Huamin Li 2025-10-30 [CI Failure] fix test_default_mm_loras (#27795)
2	1	tests/lora/test_default_mm_loras.py

[af826e082] wangxiyuan 2025-10-30 [V0 deprecation] Remove VLLM_USE_V1 usage in config module (#27784)
0	5	vllm/config/lora.py
2	23	vllm/config/model.py
0	7	vllm/config/speculative.py
7	27	vllm/config/vllm.py

[e806178d2] Zhewen Li 2025-10-30 [BugFix][VL] Fix FA selection on Qwen2.5-VL (#27790)
1	1	.buildkite/test-amd.yaml
19	11	vllm/model_executor/models/qwen2_5_vl.py

[5be1bed79] Huamin Li 2025-10-30 [CI/Build]Add eval config for Qwen3-235B-A22B-Instruct-2507-FP8 (#27113)
14	0	.buildkite/lm-eval-harness/configs/Qwen3-235B-A22B-Instruct-2507-FP8.yaml
0	1	.buildkite/lm-eval-harness/configs/models-large-h100.txt
1	0	.buildkite/lm-eval-harness/configs/models-large-hopper.txt
11	3	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
13	0	.buildkite/test-pipeline.yaml

[31b55ffc6] yitingdc 2025-10-30 use stringData in secret yaml to store huggingface token (#25685)
5	2	docs/deployment/k8s.md

[ded8ada86] Bram Wasti 2025-10-30 Add more dims for batch invariant shims (#27489)
42	2	vllm/model_executor/layers/batch_invariant.py

[8bff831f0] Kuntai Du 2025-10-29 [Benchmark] Cleanup deprecated nightly benchmark and adjust the docstring for performance benchmark (#25786)
0	184	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
0	28	.buildkite/nightly-benchmarks/nightly-annotation.md
0	39	.buildkite/nightly-benchmarks/nightly-descriptions.md
0	196	.buildkite/nightly-benchmarks/nightly-pipeline.yaml
0	26	.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
0	97	.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
0	9	.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
0	78	.buildkite/nightly-benchmarks/scripts/nightly-annotate.sh
0	464	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
0	82	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
0	23	.buildkite/nightly-benchmarks/scripts/wait-for-image.sh
5	49	.buildkite/{nightly-benchmarks => performance-benchmarks}/README.md
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/performance-benchmarks-descriptions.md
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/compare-json-results.py
1	1	.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/convert-results-json-to-markdown.py
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/launch-server.sh
1	1	.buildkite/{nightly-benchmarks => performance-benchmarks}/scripts/run-performance-benchmarks.sh
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/genai-perf-tests.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/latency-tests-cpu.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/latency-tests.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/nightly-tests.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests-cpu-snc2.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests-cpu-snc3.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests-cpu.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/serving-tests.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/throughput-tests-cpu.json
0	0	.buildkite/{nightly-benchmarks => performance-benchmarks}/tests/throughput-tests.json
1	1	.github/mergify.yml
2	11	docs/contributing/benchmarks.md

[b5d70751d] Lucas Wilkinson 2025-10-30 [BugFix] Reordering extend logic fix (#27739)
18	3	tests/v1/attention/test_batch_reordering.py
5	5	vllm/v1/attention/backends/utils.py

[b8c48c5d7] Fardin Hoque 2025-10-29 kernels/moe test pruning (#27053)
18	7	tests/kernels/moe/test_batched_moe.py
0	14	tests/kernels/moe/test_block_fp8.py
1	9	tests/kernels/moe/test_block_int8.py
0	3	tests/kernels/moe/test_cutlass_moe.py
0	1	tests/kernels/moe/test_deepep_deepgemm_moe.py
0	2	tests/kernels/moe/test_deepgemm.py
0	2	tests/kernels/moe/test_flashinfer.py
1	3	tests/kernels/moe/test_flashinfer_moe.py
1	1	tests/kernels/moe/test_grouped_topk.py
8	0	tests/kernels/moe/test_modular_kernel_combinations.py
4	7	tests/kernels/moe/test_moe.py
1	3	tests/kernels/moe/test_nvfp4_moe.py
0	4	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py

[17d055f52] Benjamin Bartels 2025-10-30 [Feat] Adds runai distributed streamer (#27230)
1	1	docker/Dockerfile
9	0	docs/models/extensions/runai_model_streamer.md
1	1	requirements/nightly_torch_test.txt
1	1	requirements/rocm.txt
1	1	requirements/test.in
3	3	requirements/test.txt
1	1	setup.py
8	2	vllm/model_executor/model_loader/runai_streamer_loader.py
14	1	vllm/model_executor/model_loader/weight_utils.py

[2ce5c5d3d] Nick Hill 2025-10-29 [BugFix] Handle unscheduled requests properly when async scheduling (#27756)
3	1	tests/v1/tpu/worker/test_tpu_model_runner.py
3	3	tests/v1/worker/test_gpu_model_runner.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
24	8	vllm/v1/core/sched/output.py
20	19	vllm/v1/core/sched/scheduler.py
8	7	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/tpu_model_runner.py

[b5bae42f9] Kunshang Ji 2025-10-30 [XPU] Update latest IPEX 2.8 release (#27735)
5	2	.buildkite/scripts/hardware_ci/run-xpu-test.sh
3	1	docs/getting_started/installation/gpu.xpu.inc.md
1	1	requirements/xpu.txt
5	16	vllm/_ipex_ops.py

[d7fb10c57] Chen Zhang 2025-10-29 [Bugfix] mamba-block-size is set for vision language model (#27773)
1	9	vllm/config/cache.py
15	1	vllm/config/vllm.py

[b798e39f9] Yan Ma 2025-10-30 [XPU][bugfix] fix rope for llama4 and deepseek (#25145)
16	1	vllm/model_executor/layers/rotary_embedding/base.py
2	2	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
2	9	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
2	20	vllm/model_executor/layers/rotary_embedding/mrope.py

[48eb8eba5] Chenheli Hua 2025-10-29 [Temp fix] Disable torch.compile for Qwen2.5 VL's VisionBlock temporarily.  (#27760)
11	9	vllm/model_executor/models/qwen2_5_vl.py

[b5d90f740] Wentao Ye 2025-10-29 [Bug] Fix DBO IMA issue for DeepEPHT (#27666)
9	3	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
9	0	vllm/v1/worker/ubatching.py

[d4aa14434] Nick Hill 2025-10-29 [BugFix] Fix handling of resumed reqs in `SharedStorageConnector` (#27719)
24	26	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py

[fcb1d570b] Wentao Ye 2025-10-29 [Bug] Fix DeepEP low latency `assert self.batched_router_logits.size(-1) == full_router_logits.size(-1)` Bug (#27682)
3	3	vllm/model_executor/layers/fused_moe/layer.py

[accb8fab0] Nicolò Lucchesi 2025-10-29 [KVConnector] Add metrics to Prometheus-Grafana dashboard (#26811)
21	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
87	2	vllm/distributed/kv_transfer/kv_connector/v1/metrics.py
88	22	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
140	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
15	3	vllm/v1/metrics/loggers.py
14	0	vllm/v1/metrics/ray_wrappers.py

[5b0448104] Wentao Ye 2025-10-29 [Bug] Raise error explicitly if using incompatible backend (#27424)
15	0	vllm/platforms/cuda.py

[f7a668287] 22quinn 2025-10-29 [CI/Build] Test torchrun with 8 cards (#27548)
20	2	.buildkite/test-pipeline.yaml
74	8	examples/offline_inference/torchrun_dp_example.py

[a9fe0793f] Boyuan Feng 2025-10-29 `use_aot_compile` should respect `VLLM_DISABLE_COMPILE_CACHE` (#27698)
11	4	vllm/envs.py

[7568a282b] JartX 2025-10-29 [FIXBUG] Qwen3VL hallucinations without Contiguous on Torch.SDPA (#27744)
8	0	vllm/model_executor/models/qwen2_5_vl.py

[1da3309ac] Braulio Dumba 2025-10-29 [Core] Exposing engine sleep & wake_up state as prometheus metrics (#24176)
49	0	tests/entrypoints/openai/test_sleep.py
6	0	vllm/v1/engine/async_llm.py
6	0	vllm/v1/engine/llm_engine.py
53	1	vllm/v1/metrics/loggers.py

[5522fb274] Wentao Ye 2025-10-29 [Chore] Optimize P2PNCCLEngine `http_address` (#27488)
17	5	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py

[0f95a1c3f] Nicolò Lucchesi 2025-10-29 [CI] Fix flaky `test_two_responses_with_same_prev_id` test  (#27745)
1	1	tests/v1/entrypoints/openai/responses/conftest.py

[ded24e3e5] Xiake Sun 2025-10-29 [ROCm][Platform] Add MI308X device id in _ROCM_DEVICE_ID_NAME_MAP (#27623)
1	0	vllm/platforms/rocm.py

[d6704dd09] Roger Young 2025-10-29 Fix MiniMax-M2 rmsnorm precision and remove useless code (#27627)
1	1	vllm/model_executor/layers/mamba/linear_attn.py
0	18	vllm/model_executor/models/minimax_m2.py

[ecca3fee7] Cyrus Leung 2025-10-29 [Frontend] Add `vllm bench sweep` to CLI (#27639)
1	1	docs/cli/.nav.yml
9	0	docs/cli/bench/sweep/plot.md
9	0	docs/cli/bench/sweep/serve.md
9	0	docs/cli/bench/sweep/serve_sla.md
3	3	docs/contributing/benchmarks.md
24	10	docs/mkdocs/hooks/generate_argparse.py
1	1	setup.py
6	6	tools/profiler/visualize_layerwise_profile.py
38	0	vllm/benchmarks/sweep/cli.py
181	131	vllm/benchmarks/sweep/plot.py
11	4	vllm/benchmarks/sweep/serve.py
15	7	vllm/benchmarks/sweep/serve_sla.py
2	0	vllm/entrypoints/cli/__init__.py
1	1	vllm/entrypoints/cli/benchmark/base.py
1	1	vllm/entrypoints/cli/benchmark/latency.py
1	1	vllm/entrypoints/cli/benchmark/serve.py
21	0	vllm/entrypoints/cli/benchmark/sweep.py
1	1	vllm/entrypoints/cli/benchmark/throughput.py
6	1	vllm/profiler/layerwise_profile.py

[9a0d2f0d9] Zhewen Li 2025-10-29 [CI/Build] Skip cpu offloading test on AMD (#27690)
4	0	tests/v1/kv_offload/test_cpu_offloading.py

[ad3ec8953] Isotr0py 2025-10-29 [VLM] Add Qwen3-VL generation test (#25185)
22	0	tests/models/multimodal/generation/test_common.py
30	3	tests/models/multimodal/generation/vlm_utils/builders.py
3	0	tests/models/multimodal/generation/vlm_utils/case_filtering.py
48	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	0	tests/models/multimodal/generation/vlm_utils/runners.py
3	1	tests/models/multimodal/generation/vlm_utils/types.py
1	1	vllm/assets/video.py

[3481e4074] Kevin H. Luu 2025-10-29 [chore] Remove models weight on S3 logic (#27725)
0	10	vllm/engine/arg_utils.py
0	129	vllm/test_utils.py

[5e72216d1] Eugene Khvedchenya 2025-10-29 Feature/video support in random mm dataset (#25963)
123	0	tests/benchmarks/test_random_dataset.py
398	0	tests/benchmarks/test_random_multimodal_dataset_video.py
80	25	vllm/benchmarks/datasets.py

[1a33aacf8] Isotr0py 2025-10-29 [Misc] Raise error for missing video metadata in `MultiModalDataParser` (#27664)
5	0	vllm/multimodal/parse.py

[7ba6aa8f5] Yue Zhang 2025-10-29 [Fix] import get_kv_cache_torch_dtype error in LMCacheConnector integration (#27670)
[ab2eb27b7] Alec S 2025-10-29 [Frontend] [gpt-oss] Mcp type bug (#27689)
68	19	tests/entrypoints/openai/test_response_api_mcp_tools.py
49	1	tests/entrypoints/openai/test_serving_responses.py
13	1	tests/entrypoints/test_harmony_utils.py
108	0	tests/test_envs.py
7	3	vllm/entrypoints/harmony_utils.py
20	12	vllm/entrypoints/openai/serving_responses.py
28	8	vllm/envs.py

[3c7fefdeb] Alec S 2025-10-29 [Frontend] [gpt-oss] Tool json call parsing error retry (#27675)
37	2	vllm/entrypoints/context.py
18	1	vllm/entrypoints/harmony_utils.py
7	0	vllm/envs.py

[1891cf605] bnellnm 2025-10-29 [Bugfix] Fix modular kernel tests (#27707)
2	0	.buildkite/test-pipeline.yaml
1	0	tests/kernels/moe/modular_kernel_tools/common.py

[8df98c216] Jiangyun Zhu 2025-10-29 [perf] Enable concurrent execution of "shared_experts" and "selected_experts" in qwen3-next (#27578)
12	5	vllm/model_executor/models/qwen3_next.py

[4fb8771cc] Cyrus Leung 2025-10-29 [CI/Build] Move pre-commit only scripts to `tools/pre_commit` (#27657)
1	1	.buildkite/test-amd.yaml
1	1	.buildkite/test-pipeline.yaml
9	9	.pre-commit-config.yaml
1	1	tests/standalone_tests/pytorch_nightly_dependency.sh
1	1	tests/tools/test_config_validator.py
2	3	tools/{ => pre_commit}/check_init_lazy_imports.py
0	0	tools/{ => pre_commit}/check_spdx_header.py
0	0	tools/{ => pre_commit}/check_triton_import.py
0	0	tools/{ => pre_commit}/enforce_regex_import.py
0	0	tools/{ => pre_commit}/generate_nightly_torch_test.py
0	0	tools/{ => pre_commit}/png-lint.sh
0	0	tools/{ => pre_commit}/shellcheck.sh
0	0	tools/{ => pre_commit}/update-dockerfile-graph.sh
0	0	tools/{ => pre_commit}/validate_config.py
1	1	vllm/config/utils.py

[413ef7a3b] Dipika Sikka 2025-10-29 [Speculators] Move tests + fix integration (#27308)
80	0	tests/v1/e2e/test_spec_decode.py
0	4	tests/{speculative_decoding/speculators/test_eagle3.py => v1/spec_decode/test_speculators_eagle3.py}
17	11	vllm/engine/arg_utils.py

[8b6249507] Zhewen Li 2025-10-29 [Bugfix] Fix non-contiguous tensor error in `rocm_unquantized_gemm_impl` (#27605)
1	1	.buildkite/test-amd.yaml
3	3	vllm/model_executor/layers/utils.py

[83fd49b1f] Zhewen Li 2025-10-28 [CI/Build][Bugfix]Fix Quantized Models Test on AMD (#27712)
1	1	.buildkite/test-amd.yaml
6	0	tests/models/quantization/test_bitsandbytes.py
1	1	vllm/platforms/rocm.py

[a4a4f0f61] Shaoting 2025-10-28 [KV Connector] Update lmcache connector with latest compatibility (#27681)
20	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py

[0d8161b07] Lukas Geiger 2025-10-29 [Model] Fix Qwen3VL and Qwen3Omni after torch.compile changes (#27705)
2	4	vllm/model_executor/models/qwen2_5_vl.py
8	6	vllm/model_executor/models/qwen3_omni_moe_thinker.py
7	6	vllm/model_executor/models/qwen3_vl.py

[d2c33c397] liuzhenwei 2025-10-29 [NIXL][XPU] update name of nixl wheel (#27631)
1	1	tools/install_nixl_from_source_ubuntu.py

[f6d5f5888] Varun Sundar Rabindranath 2025-10-29 [Build] Revert triton_kernels requirements (#27659)
0	2	requirements/cuda.txt

[9007bf57e] Simon Mo 2025-10-28 Revert "Install pre-built xformers-0.0.32.post2 built with pt-2.9.0" (#27714)
7	0	docker/Dockerfile
1	1	requirements/cuda.txt

[f25754470] Huy Do 2025-10-28 Install pre-built xformers-0.0.32.post2 built with pt-2.9.0 (#27598)
0	7	docker/Dockerfile
1	1	requirements/cuda.txt

[0b51c9bd8] Jialin Ouyang 2025-10-28 [Core] Early return in SlidingWindowManager.remove_skipped_blocks (#27673)
6	0	vllm/v1/core/single_type_kv_cache_manager.py

[d3ab240f3] Wentao Ye 2025-10-28 [Bug] Fix deepep low latency use nvlink by default (#27677)
2	2	vllm/envs.py

[94666612a] Lucas Kabela 2025-10-28 [Misc][qwen2_5_vl][torch.compile] Enable `supports_torch_compile` on generic nn.Module and demonstrate speedup on Qwen Vision model (#23207)
36	0	tests/compile/test_multimodal_compile.py
125	0	vllm/attention/ops/vit_attn_wrappers.py
56	4	vllm/compilation/decorators.py
2	0	vllm/config/compilation.py
6	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
110	92	vllm/model_executor/models/qwen2_5_vl.py

[4fe589536] Nick Hill 2025-10-28 [AsyncScheduling] Make async overlap work with logprobs (#27615)
8	2	tests/conftest.py
33	4	tests/v1/e2e/test_async_sched_and_preempt.py
9	0	vllm/v1/outputs.py
15	4	vllm/v1/worker/gpu_model_runner.py

[111faf111] Or Ozeri 2025-10-28 [Core] Scheduler: Publish connector events after output (#25875)
118	26	tests/v1/kv_offload/test_cpu_offloading.py
17	17	vllm/v1/core/sched/scheduler.py

[6afc28a9b] Wentao Ye 2025-10-28 [Test] Batch Invariant: Unit test using parameterized backend (#27478)
207	203	tests/v1/generation/test_batch_invariance.py
1	1	vllm/model_executor/layers/batch_invariant.py

[141e6a050] Lucas Wilkinson 2025-10-29 [Misc] Make reorder batch also separate extends (#27367)
111	0	tests/v1/attention/test_batch_reordering.py
53	45	vllm/v1/attention/backends/utils.py

[130aa8cbc] Matvei Pashkovskii 2025-10-28 Add load pattern configuration guide to benchmarks (#26886)
-	-	docs/assets/contributing/load-pattern-examples.png
67	0	docs/contributing/benchmarks.md

[e3d818666] Zhengxu Chen 2025-10-28 [compile] Add fallback path to AOT compile when serialization fails. (#27350)
11	2	vllm/compilation/decorators.py

[f5710ef02] Cyrus Leung 2025-10-29 [Misc] Make `LayerBlockType` a `Literal` instead of `Enum` (#27658)
10	16	vllm/config/model.py
0	6	vllm/utils/__init__.py
1	2	vllm/v1/worker/tpu_model_runner.py

[a8c02fb5b] Mohammad Miadh Angkad 2025-10-28 [Bugfix][CI] Fix v1 attention backend tests and add CI coverage (#26597)
9	0	.buildkite/test-pipeline.yaml
22	14	tests/v1/attention/test_attention_backends.py
2	1	tests/v1/attention/test_mla_backends.py
34	11	vllm/v1/attention/backends/flex_attention.py

[02af36df3] Kero Liang 2025-10-28 [Bugfix] Fix allocation & free logic of SingleWriterShmRingBuffer (#27117)
65	0	tests/distributed/test_shm_buffer.py
10	4	vllm/distributed/device_communicators/shm_object_storage.py

[e88bdd60d] Zhiyuan Li 2025-10-28 [FLA] Introduce Kimi Delta Attention(KDA) to VLLM (#27654)
1	1	vllm/model_executor/layers/fla/ops/chunk.py
70	40	vllm/model_executor/layers/fla/ops/chunk_delta_h.py
11	13	vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py
18	4	vllm/model_executor/layers/fla/ops/fused_recurrent.py
1351	0	vllm/model_executor/layers/fla/ops/kda.py

[05e034f08] Samuel Shen 2025-10-28 [nit]: Fix import for the lmcache integration (#27600)
5	3	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
5	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/__init__.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils.py

[936643a86] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2025-10-28 [BugFix] Also consider RAY_EXPERIMENTAL_NOSET_* when storing compilation cache (#27294)
23	0	vllm/envs.py

[b186149e8] Junpu Fan 2025-10-28 [Bugfix][Frontend] validate arg priority in frontend LLM class before add request (#27596)
20	0	tests/entrypoints/llm/test_generate.py
6	0	vllm/entrypoints/llm.py

[2abbd351e] 22quinn 2025-10-28 [Core] Enable async scheduling for external_launcher mode (#27394)
4	3	vllm/engine/arg_utils.py
1	3	vllm/v1/engine/llm_engine.py

[446912d1c] wangln19 2025-10-28 fix: allow HuggingFace standard chat template params via **kwargs (#27622)
33	0	tests/entrypoints/test_chat_utils.py
24	1	vllm/entrypoints/chat_utils.py

[a00d6254e] Zhengxu Chen 2025-10-28 [compile] Disable dynamo guards check for AOT compilation. (#27288)
1	0	vllm/compilation/decorators.py

[05181cc57] Asaf Joseph Gardin 2025-10-28 [Hybrid] Add mamba_block_size to Engine Args (#27289)
13	3	vllm/config/cache.py
5	0	vllm/engine/arg_utils.py
7	5	vllm/model_executor/models/config.py

[259504e14] Zhengxu Chen 2025-10-28 [compile] Add enable_prompt_embeds to compile hash. (#27285)
2	1	vllm/config/model.py

[0484b6424] Wentao Ye 2025-10-28 [Bug] Fix shape issue for eplb expert weights (#27589)
2	0	vllm/model_executor/layers/fused_moe/layer.py

[f58d9b640] Cyrus Leung 2025-10-28 [Misc] Separate out `utils.counter` and move `utils.Device` to engine (#27588)
6	1	vllm/engine/protocol.py
4	3	vllm/entrypoints/llm.py
1	2	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/serving_models.py
0	44	vllm/utils/__init__.py
45	0	vllm/utils/counter.py
1	2	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/llm_engine.py

[44b5ce956] Matthew Bonanni 2025-10-28 [Bugfix] In LongRoPE, decide short vs long based on max_model_len (#27431)
1	1	tests/entrypoints/openai/test_default_mm_loras.py
11	1	vllm/config/model.py
27	9	vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope.py

[7a865f232] Nick Hill 2025-10-28 [V0 Deprecation] Remove vestigial V0 logits_processors.py file (#27601)
0	92	vllm/entrypoints/openai/logits_processors.py

[2fa90bda2] wangln19 2025-10-28 Fix a robust parsing issue in KimiK2ToolParser that causes IndexError (#27565)
3	3	tests/tool_use/test_kimi_k2_tool_parser.py
4	4	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py

[0291fbf65] Zhewen Li 2025-10-28 [CI/Build] Fix amd model executor test (#27612)
1	1	.buildkite/test-amd.yaml
6	0	tests/model_executor/model_loader/fastsafetensors_loader/test_fastsafetensors_loader.py
5	0	tests/model_executor/model_loader/fastsafetensors_loader/test_weight_utils.py

[b46e4a06f] Jialin Ouyang 2025-10-28 [Core][Bookkeeping Optimization] Update against numpy view of is_token_ids tensor (#27618)
2	1	vllm/v1/worker/gpu_input_batch.py
1	1	vllm/v1/worker/gpu_model_runner.py

[d34f5fe93] Li, Jiang 2025-10-28 [Bugfix][CPU] Fallback oneDNN linear to torch linear to fix half gemm support on legecy platforms (#27526)
1	1	docker/Dockerfile.cpu
21	9	vllm/model_executor/layers/utils.py

[bdb01a38f] Eric Yue 2025-10-28 [Hardware][AMD][Model] Triton MoE tuning configs for GLM-4.6 for MI300X (#27323)
201	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=AMD_Instinct_MI300X.json

[5b3c35a68] vllmellm 2025-10-28 [ROCm] [Doc] Update ROCm installation docs  (#27327)
74	49	docs/getting_started/installation/gpu.rocm.inc.md
1	1	docs/getting_started/installation/python_env_setup.inc.md

[61fbfe527] Chauncey 2025-10-28 [Bugfix] fixed inconsistent finish_reason handling between V0 and V1 engines (#27555)
6	7	vllm/v1/core/sched/utils.py

[255e34ca5] Kuntai Du 2025-10-27 [Stability fix] turn off HMA allocator when connector is set (#27592)
14	0	vllm/config/vllm.py

[a8d2e326e] Roger Wang 2025-10-27 [Bugfix][CI] Fix config resolving logic with remote models (#27610)
8	3	vllm/transformers_utils/config.py

[53a56e658] Andrew Xia 2025-10-27 [gpt-oss][2/N] Support input_messages in responsesRequest (#26962)
129	0	tests/entrypoints/openai/test_response_api_with_harmony.py
22	0	tests/entrypoints/openai/test_serving_responses.py
254	0	tests/entrypoints/test_harmony_utils.py
38	1	vllm/entrypoints/harmony_utils.py
7	1	vllm/entrypoints/openai/protocol.py
21	15	vllm/entrypoints/openai/serving_chat.py
10	0	vllm/entrypoints/openai/serving_responses.py

[69f064062] usberkeley 2025-10-28 Code quality improvements: version update, type annotation enhancement, and enum usage simplification (#27581)
2	2	docs/deployment/docker.md
1	1	vllm/multimodal/profiling.py
4	6	vllm/v1/core/sched/scheduler.py

[921e78f4b] Micah Williamson 2025-10-27 [ROCm] Update AITER branch for ROCm base docker (#27586)
1	1	docker/Dockerfile.rocm_base

[6ebffafbb] Cyrus Leung 2025-10-27 [Misc] Clean up more utils (#27567)
2	0	requirements/docs.txt
0	1	tools/pre_commit/check_pickle_imports.py
23	0	vllm/config/model.py
27	1	vllm/config/vllm.py
1	2	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/anthropic/api_server.py
2	1	vllm/entrypoints/api_server.py
2	2	vllm/entrypoints/openai/api_server.py
1	1	vllm/platforms/__init__.py
1	1	vllm/platforms/cuda.py
4	242	vllm/utils/__init__.py
3	23	vllm/utils/argparse_utils.py
43	0	vllm/utils/import_utils.py
114	8	vllm/utils/system_utils.py
1	2	vllm/v1/engine/coordinator.py
1	1	vllm/v1/engine/utils.py
6	2	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/executor/uniproc_executor.py
28	0	vllm/v1/serial_utils.py
1	1	vllm/v1/utils.py
2	5	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py
1	1	vllm/v1/worker/tpu_worker.py
16	18	vllm/v1/worker/worker_base.py

[3b96f85c3] Ben Browning 2025-10-27 [Chore]: Stream tokens vs characters in tool call parser tests (#26513)
12	0	tests/entrypoints/openai/tool_parsers/conftest.py
3	5	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
12	12	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
20	12	tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py
12	12	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
21	0	tests/entrypoints/openai/tool_parsers/utils.py

[23ad82055] tingtinggithub 2025-10-27 fixing mm placeholder replacement issue with gemma3 (#27538)
1	1	vllm/model_executor/models/gemma3_mm.py

[5d3be3ba4] Varun Sundar Rabindranath 2025-10-27 [Bugfix][LoRA][FusedMoE] Select MxFP4 Backend based on LoRA Enablement (#27487)
2	0	vllm/model_executor/layers/fused_moe/config.py
12	3	vllm/model_executor/layers/fused_moe/layer.py
20	3	vllm/model_executor/layers/quantization/mxfp4.py

[4f882be4a] Yu Jiaqi 2025-10-27 [Model] Siglip2 Model Support (#27566)
1	1	docs/models/supported_models.md
1	1	tests/models/multimodal/pooling/test_siglip.py
5	3	vllm/model_executor/models/siglip.py
11	1	vllm/transformers_utils/config.py

[927375422] Asaf Joseph Gardin 2025-10-27 [Hybrid] Added supports_mamba_prefix_caching Protocol (#27339)
4	0	vllm/config/model.py
15	2	vllm/model_executor/models/bamba.py
1	10	vllm/model_executor/models/config.py
15	2	vllm/model_executor/models/falcon_h1.py
15	2	vllm/model_executor/models/granitemoehybrid.py
28	0	vllm/model_executor/models/interfaces.py
8	2	vllm/model_executor/models/mamba2.py
2	0	vllm/model_executor/models/nemotron_h.py
3	0	vllm/model_executor/models/registry.py
2	2	vllm/model_executor/models/zamba2.py

[f4e815407] Jee Jee Li 2025-10-27 [Kernel] Enable moe LoRA kernel support FP16 (#27468)
17	6	tests/lora/test_fused_moe_lora_kernel.py
9	10	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[a663f6ae6] Fadi Arafeh 2025-10-27 [cpu][perf] Fix low CPU utilization with VLLM_CPU_OMP_THREADS_BIND on AArch64 (#27415)
17	4	cmake/cpu_extension.cmake
38	0	cmake/utils.cmake
28	2	vllm/platforms/cpu.py

[a4fc21895] Chauncey 2025-10-27 [Bugfix] Fixed when return_token_ids=False, the first event still contains prompt_token_ids. (#27561)
12	2	tests/entrypoints/openai/test_return_token_ids.py
1	1	vllm/entrypoints/openai/serving_completion.py

[a3e8611da] Shanshan Shen 2025-10-27 [Bugfix] Limit the default value of `max_model_len` when it is not specified by users (#27556)
5	12	vllm/config/model.py
7	0	vllm/platforms/interface.py
16	0	vllm/platforms/tpu.py

[7c2bdb83d] Cyrus Leung 2025-10-27 [Misc] Clean up utils (#27552)
3	1	docs/mkdocs/hooks/generate_argparse.py
1	3	tests/utils.py
2	91	tests/utils_/{test_utils.py => test_argparse_utils.py}
1	1	tests/utils_/test_serial_utils.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py
2	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py
2	1	vllm/entrypoints/anthropic/api_server.py
1	1	vllm/entrypoints/cli/serve.py
2	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/lora/punica_wrapper/punica_gpu.py
1	1	vllm/model_executor/layers/quantization/mxfp4.py
24	155	vllm/utils/__init__.py

[9932ed6a8] Danielle Robinson 2025-10-27 [Kernel] Adding split_K implementation for fused_moe_lora (#27291)
2	0	tests/lora/test_fused_moe_lora_kernel.py
21	9	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
1	0	vllm/lora/punica_wrapper/punica_gpu.py
11	5	vllm/model_executor/layers/fused_moe/fused_moe.py

[2d631d28c] Jee Jee Li 2025-10-27 [Doc] Slight improvement to M2 and beyond (#27554)
4	3	docs/features/reasoning_outputs.md
1	0	docs/models/supported_models.md
0	1	tests/models/registry.py
0	14	vllm/model_executor/models/minimax_m2.py

[b36838296] Cyrus Leung 2025-10-27 [Model] Deprecate `merge_by_field_config=False` (#27551)
14	4	vllm/multimodal/utils.py

[a806c14cc] gnovack 2025-10-26 [Performance][LoRA] add context varying params to 'do_not_specialize' in fused moe lora (#27445)
16	7	vllm/lora/ops/triton_ops/fused_moe_lora_op.py

[181bf5bbd] yyzxw 2025-10-27 [Docs] reemove the incorrect `enable_reasoning` parameter  (#27550)
1	1	docs/features/tool_calling.md

[cbd5e07a5] Cyrus Leung 2025-10-27 [Model] Use merge_by_field_config for MM models (Qwen series) (#27546)
15	80	vllm/model_executor/models/qwen2_5_omni_thinker.py
2	47	vllm/model_executor/models/qwen2_5_vl.py
2	23	vllm/model_executor/models/qwen2_audio.py
2	46	vllm/model_executor/models/qwen2_vl.py
9	33	vllm/model_executor/models/qwen3_omni_moe_thinker.py
2	62	vllm/model_executor/models/qwen3_vl.py
4	14	vllm/model_executor/models/qwen_vl.py

[63b22e0db] CSWYF3634076 2025-10-27 [Model][Bugfix] fix ernie45 moe 300B SharedFusedMoE output tuple (#27316)
2	0	vllm/model_executor/models/ernie45_moe.py

[5980604c4] Roger Young 2025-10-27 Fix MiniMax-M2 copyright (#27537)
2	3	vllm/model_executor/models/minimax_m2.py

[361a7463d] youkaichao 2025-10-27 fix m2 test (#27536)
3	1	tests/models/registry.py

[720af6ab7] Roger Young 2025-10-27 [Model][MiniMax-M2] Support MiniMax-M2 Model (#27535)
3	0	tests/models/registry.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
644	0	vllm/entrypoints/openai/tool_parsers/minimax_m2_tool_parser.py
585	0	vllm/model_executor/models/minimax_m2.py
1	0	vllm/model_executor/models/registry.py
2	0	vllm/reasoning/__init__.py
69	0	vllm/reasoning/minimax_m2_reasoning_parser.py

[55cba4a05] Cyrus Leung 2025-10-26 [CI/Build] Update causal-conv1d installation (#27529)
4	2	.buildkite/test-amd.yaml
4	2	.buildkite/test-pipeline.yaml

[c7abff299] Cyrus Leung 2025-10-26 Revert "[CI/Build] Use CPU for mm processing test on CI (#27522)" (#27531)
2	4	.buildkite/test-pipeline.yaml

[71b1c8b66] Yeshwanth N 2025-10-26 [Chore]:Extract math and argparse utilities to separate modules (#27188)
1	1	benchmarks/benchmark_block_pool.py
1	1	benchmarks/benchmark_long_document_qa_throughput.py
1	1	benchmarks/benchmark_ngram_proposer.py
1	1	benchmarks/benchmark_prefix_caching.py
1	1	benchmarks/benchmark_prioritization.py
1	1	benchmarks/benchmark_serving_structured_output.py
1	1	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
2	1	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
1	1	benchmarks/kernels/bench_per_token_quant_fp8.py
1	1	benchmarks/kernels/benchmark_activation.py
1	1	benchmarks/kernels/benchmark_bitblas.py
1	1	benchmarks/kernels/benchmark_cutlass_fp4_moe.py
1	1	benchmarks/kernels/benchmark_cutlass_moe_fp8.py
1	1	benchmarks/kernels/benchmark_device_communicators.py
1	1	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
1	1	benchmarks/kernels/benchmark_layernorm.py
1	1	benchmarks/kernels/benchmark_lora.py
1	1	benchmarks/kernels/benchmark_machete.py
1	1	benchmarks/kernels/benchmark_marlin.py
1	1	benchmarks/kernels/benchmark_moe.py
1	1	benchmarks/kernels/benchmark_moe_permute_unpermute.py
1	1	benchmarks/kernels/benchmark_mrope.py
1	1	benchmarks/kernels/benchmark_paged_attention.py
1	1	benchmarks/kernels/benchmark_quant.py
1	1	benchmarks/kernels/benchmark_reshape_and_cache.py
1	1	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
1	1	benchmarks/kernels/benchmark_rope.py
1	1	benchmarks/kernels/benchmark_trtllm_decode_attention.py
1	1	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
1	1	benchmarks/kernels/benchmark_w8a8_block_fp8.py
1	1	benchmarks/kernels/graph_machete_bench.py
1	1	benchmarks/overheads/benchmark_hashing.py
1	1	examples/offline_inference/audio_language.py
1	1	examples/offline_inference/basic/chat.py
1	1	examples/offline_inference/basic/classify.py
1	1	examples/offline_inference/basic/embed.py
1	1	examples/offline_inference/basic/generate.py
1	1	examples/offline_inference/basic/reward.py
1	1	examples/offline_inference/basic/score.py
1	1	examples/offline_inference/encoder_decoder_multimodal.py
1	1	examples/offline_inference/llm_engine_example.py
1	1	examples/offline_inference/load_sharded_state.py
1	1	examples/offline_inference/pooling/embed_jina_embeddings_v3.py
1	1	examples/offline_inference/pooling/embed_matryoshka_fy.py
1	1	examples/offline_inference/pooling/multi_vector_retrieval.py
1	1	examples/offline_inference/pooling/ner.py
1	1	examples/offline_inference/profiling_tpu/profiling.py
1	1	examples/offline_inference/qwen2_5_omni/only_thinker.py
1	1	examples/offline_inference/save_sharded_state.py
1	1	examples/offline_inference/spec_decode.py
1	1	examples/offline_inference/vision_language.py
1	1	examples/offline_inference/vision_language_multi_image.py
1	1	examples/offline_inference/vision_language_pooling.py
1	1	examples/online_serving/openai_chat_completion_client_for_multimodal.py
1	1	examples/others/tensorize_vllm_model.py
1	1	tests/engine/test_arg_utils.py
1	1	tests/entrypoints/openai/test_cli_args.py
1	1	tests/kernels/attention/test_deepgemm_attention.py
1	1	tests/kernels/attention/test_flashinfer_trtllm_attention.py
1	1	tests/kernels/attention/test_mla_decode_cpu.py
1	1	tests/kernels/attention/test_triton_decode_attention.py
1	1	tests/kernels/moe/test_cutlass_grouped_gemm.py
1	1	tests/kernels/moe/test_gpt_oss_triton_kernels.py
1	1	tests/kernels/moe/test_moe_align_block_size.py
1	1	tests/kernels/moe/test_pplx_cutlass_moe.py
1	1	tests/kernels/moe/test_pplx_moe.py
1	1	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
1	1	tests/kernels/moe/utils.py
1	1	tests/kernels/quant_utils.py
1	1	tests/kernels/quantization/test_cutlass_scaled_mm.py
1	1	tests/v1/attention/test_attention_backends.py
1	1	tests/v1/attention/test_mla_backends.py
1	1	tests/v1/attention/test_sparse_mla_backends.py
1	1	tests/v1/engine/test_engine_args.py
1	1	vllm/attention/layers/cross_attention.py
1	1	vllm/attention/ops/pallas_kv_cache_update.py
1	1	vllm/attention/ops/rocm_aiter_paged_attn.py
1	1	vllm/benchmarks/datasets.py
1	1	vllm/compilation/fusion_attn.py
2	1	vllm/engine/arg_utils.py
2	1	vllm/entrypoints/api_server.py
1	1	vllm/entrypoints/cli/benchmark/main.py
1	1	vllm/entrypoints/cli/collect_env.py
1	1	vllm/entrypoints/cli/main.py
1	1	vllm/entrypoints/cli/openai.py
1	1	vllm/entrypoints/cli/run_batch.py
1	1	vllm/entrypoints/cli/types.py
1	1	vllm/entrypoints/openai/cli_args.py
2	1	vllm/entrypoints/openai/run_batch.py
1	1	vllm/entrypoints/utils.py
1	1	vllm/model_executor/layers/fla/ops/layernorm_guard.py
1	1	vllm/model_executor/layers/fused_moe/config.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py
1	1	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
1	1	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	1	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
1	1	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
1	1	vllm/model_executor/layers/fused_moe/utils.py
1	1	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/model_executor/models/config.py
1	1	vllm/model_executor/models/gpt_oss.py
1	1	vllm/model_executor/models/utils.py
1	1	vllm/platforms/interface.py
48	516	vllm/utils/__init__.py
507	0	vllm/utils/argparse_utils.py
1	1	vllm/utils/deep_gemm.py
32	0	vllm/utils/math_utils.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/attention/backends/mamba2_attn.py
1	1	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/flashmla_sparse.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
1	1	vllm/v1/attention/backends/pallas.py
1	1	vllm/v1/attention/backends/utils.py
1	1	vllm/v1/core/kv_cache_utils.py
1	1	vllm/v1/core/single_type_kv_cache_manager.py
2	1	vllm/v1/engine/async_llm.py
1	1	vllm/v1/kv_cache_interface.py
1	1	vllm/v1/worker/block_table.py
1	2	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/tpu_model_runner.py
1	1	vllm/v1/worker/tpu_worker.py

[8fb7b2fab] Cyrus Leung 2025-10-26 [Doc] Fix links to GH projects (#27530)
2	2	docs/contributing/README.md

[be7b55a83] Cyrus Leung 2025-10-26 [Doc] Remove Molmo warning (#27527)
0	31	docs/models/supported_models.md

[315b860ab] Lucia Fang 2025-10-26 [bugfix]fix empty prompts for async-engine mode in benchmark throughput (#27494)
1	0	vllm/benchmarks/throughput.py

[87c41c26a] rongfu.leng 2025-10-26 [Bugfix] Fix processor initialization for model from modelscope instead of HF (#27461)
5	1	vllm/transformers_utils/processor.py
11	0	vllm/transformers_utils/utils.py

[65d2cf951] JartX 2025-10-26 [BUGFIX][ROCM] ViT FlashAttention on ROCm (no GFX9) and contiguous on qwen3vl ROCm TORCH_SDPA (#27190)
29	11	vllm/attention/layer.py
6	0	vllm/model_executor/models/qwen2_5_vl.py
6	0	vllm/model_executor/models/qwen2_vl.py
5	1	vllm/platforms/rocm.py

[d63cd9ff1] Isotr0py 2025-10-26 [CI/Build] Use CPU for mm processing test on CI (#27522)
4	2	.buildkite/test-pipeline.yaml

[66a168a19] Cyrus Leung 2025-10-26 [CI/Build] Refactor processing tests (#27470)
128	149	tests/models/multimodal/processing/test_common.py
31	66	tests/models/multimodal/processing/test_tensor_schema.py
12	8	vllm/model_executor/models/qwen3_omni_moe_thinker.py
3	7	vllm/model_executor/models/qwen3_vl.py

[a99564ac5] Matthew Bonanni 2025-10-25 [Attention] Add missing kv cache scale setup (#27490)
72	59	vllm/attention/layer.py

[4c5f63216] Cyrus Leung 2025-10-25 [Misc] Simplify max tokens in multimodal registry (#27500)
5	2	vllm/multimodal/profiling.py
6	30	vllm/multimodal/registry.py
2	2	vllm/v1/core/encoder_cache_manager.py
4	4	vllm/v1/worker/utils.py

[b85354038] Kuntai Du 2025-10-24 [Core][Hybrid allocator + kv connector 1/n] Enable hybrid allocator + KV cache connector (#25712)
7	0	tests/v1/core/test_scheduler.py
2	0	tests/v1/core/utils.py
2	0	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
2	0	tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh
1	0	tests/v1/kv_connector/unit/test_multi_connector.py
1	0	tests/v1/kv_connector/unit/test_nixl_connector.py
1	0	tests/v1/kv_connector/unit/test_shared_storage_connector.py
3	0	tests/v1/kv_connector/unit/utils.py
1	0	tests/v1/kv_offload/test_cpu_offloading.py
3	3	vllm/config/vllm.py
15	3	vllm/distributed/kv_transfer/kv_connector/factory.py
9	1	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
40	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
17	8	vllm/v1/core/sched/scheduler.py
9	2	vllm/v1/worker/gpu_worker.py

[56ed7609a] Zhuohan Li 2025-10-24 Revert "[Misc] Remove use of CUDA_VISIBLE_DEVICES for device selectio… (#27502)
4	8	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	4	vllm/v1/engine/utils.py
2	2	vllm/v1/worker/dp_utils.py
0	21	vllm/v1/worker/gpu_worker.py

[29c9cb800] Jiangyun Zhu 2025-10-25 [CI] Add tests for cudagraph (#27391)
12	0	.buildkite/test-pipeline.yaml
5	0	tests/utils.py
34	15	tests/v1/cudagraph/test_cudagraph_dispatch.py
3	3	tests/v1/cudagraph/test_cudagraph_mode.py

[83f478bb1] Yihua Cheng 2025-10-24 [KVConnector] Migrate the LMCache integration code to be vLLM native (#25542)
18	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
2	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/__init__.py
221	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/utils.py
1396	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_integration/vllm_v1_adapter.py

[269c4db0a] Varun Sundar Rabindranath 2025-10-24 [Misc][DP] Guard mxfp4 implementation selection (#27484)
7	2	vllm/model_executor/layers/quantization/mxfp4.py

[52efc34eb] Wentao Ye 2025-10-24 [Log] Optimize Startup Log (#26740)
25	10	vllm/compilation/backends.py
4	2	vllm/compilation/monitor.py
5	6	vllm/distributed/device_communicators/cuda_communicator.py
1	1	vllm/distributed/device_communicators/custom_all_reduce.py
3	1	vllm/distributed/device_communicators/pynccl.py
1	1	vllm/distributed/device_communicators/shm_broadcast.py
25	2	vllm/distributed/parallel_state.py
40	13	vllm/logger.py
4	2	vllm/model_executor/layers/fused_moe/layer.py
2	1	vllm/model_executor/model_loader/default_loader.py
1	1	vllm/model_executor/model_loader/weight_utils.py
4	2	vllm/platforms/__init__.py
3	1	vllm/platforms/cuda.py
1	1	vllm/utils/gc_utils.py
1	1	vllm/v1/core/kv_cache_utils.py
4	4	vllm/v1/engine/core.py
1	1	vllm/v1/metrics/loggers.py
4	1	vllm/v1/sample/ops/topk_topp_sampler.py
7	7	vllm/v1/worker/dp_utils.py
9	3	vllm/v1/worker/gpu_model_runner.py
6	2	vllm/v1/worker/gpu_worker.py

[d95d0f4b9] Pengchao Wang 2025-10-24 [Distributed] Basic set of configuration for large EP deployment on GB200 (#27328)
3	1	vllm/distributed/device_communicators/all2all.py
22	0	vllm/envs.py

[040242820] Lehua Ding 2025-10-25 [Perf][Async Scheduling] Remove CPU->GPU sync in dummy_run (#27455)
4	1	vllm/v1/worker/gpu_model_runner.py

[17af6aa0d] jinghanhu 2025-10-25 [Document] Add ms-swift library to rlhf.md (#27469)
1	0	docs/training/rlhf.md

[fc168c33f] Zhewen Li 2025-10-24 [CI/Build] Fix test_torch_utils in AMD CI (#27317)
1	1	.buildkite/test-amd.yaml
34	10	tests/utils_/test_torch_utils.py

[acc78aeb8] Isotr0py 2025-10-25 [Bugfix] Fix interns1-vit qk norm code path (#27480)
3	4	vllm/model_executor/models/interns1_vit.py

[0f67d4d96] Ming Yang 2025-10-24 [Attention] Add MLA prefill backend: trtllm_ragged_attention_deepseek (#26397)
6	0	vllm/envs.py
107	2	vllm/v1/attention/backends/mla/common.py

[7e1d697b5] kourosh hakhamaneshi 2025-10-24 [Bugfix] Fix MultiConnector stats reconstruction across process boundaries (#27366)
368	0	tests/v1/kv_connector/unit/test_multi_connector.py
18	0	vllm/distributed/kv_transfer/kv_connector/factory.py
35	5	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[699d62e6c] Chendi.Xue 2025-10-24 [NIXL][BUGFIX] delay done_recving queue cleanup to bottom of get_finished (#27297)
8	11	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[cd390b609] Richard Zou 2025-10-24 [compile] Turn standalone_compile back on (#27460)
4	4	vllm/envs.py

[2080b0509] Fadi Arafeh 2025-10-24 [cpu][fix] Fix onednn_mm crash on consecutive matmuls with same M,K,N and different dtype (#27472)
8	4	csrc/cpu/dnnl_helper.cpp
1	0	csrc/cpu/dnnl_helper.h

[6454afec9] Lifans 2025-10-24 [Doc] Fix minor issues in docs/design/metrics.md (#27436)
1	1	docs/design/metrics.md

[41a62564a] Chauncey 2025-10-24 Fix test named tool use (#27458)
2	0	tests/entrypoints/openai/test_completion_with_function_calling.py

[284cc9227] fhl2000 2025-10-24 [MISC] `cudagraph_capture_sizes`  related improvements (#26016)
73	0	tests/compile/test_config.py
40	38	vllm/config/compilation.py
0	15	vllm/config/scheduler.py
107	26	vllm/config/vllm.py
62	5	vllm/engine/arg_utils.py
1	1	vllm/model_executor/layers/quantization/mxfp4.py
11	13	vllm/model_executor/models/config.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/gdn_attn.py
1	1	vllm/v1/attention/backends/mamba_attn.py
1	1	vllm/v1/attention/backends/mla/flashattn_mla.py
1	1	vllm/v1/spec_decode/eagle.py
3	6	vllm/v1/worker/gpu_model_runner.py

[435be10db] ioana ghiban 2025-10-24 Fix AArch64 CPU Docker pipeline (#27331)
1	1	.buildkite/release-pipeline.yaml
4	89	docker/Dockerfile.cpu

[b7030d962] Cyrus Leung 2025-10-24 [Benchmark] Enable benchmark to run with `encoding_format="bytes"` (#27467)
9	2	vllm/benchmarks/lib/endpoint_request_func.py

[356781693] Chauncey 2025-10-24 [Refactor] move tool parsing logic from protocol.py to the tool parser (#27383)
4	3	tests/tool_use/test_tool_choice_required.py
1	72	vllm/entrypoints/openai/protocol.py
16	0	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py
105	0	vllm/entrypoints/openai/tool_parsers/utils.py

[e0ef8a292] 22quinn 2025-10-24 [BugFix] Fix torchrun DP with LLM class (#27395)
4	3	vllm/entrypoints/llm.py

[42efe609b] Isotr0py 2025-10-24 [MM][Bugfix] Replace `PatchEmbed`'s conv3d to linear layer (#27418)
13	8	vllm/model_executor/models/glm4_1v.py
14	8	vllm/model_executor/models/qwen2_5_vl.py
18	9	vllm/model_executor/models/qwen2_vl.py
18	8	vllm/model_executor/models/qwen3_omni_moe_thinker.py
18	9	vllm/model_executor/models/qwen3_vl.py
16	0	vllm/model_executor/models/vision.py

[88d3141ec] Yu Jiaqi 2025-10-24 [Docs] remove v1 column for embedding models (#27446)
7	7	docs/models/supported_models.md

[09a6a49ea] Rui Qiao 2025-10-23 [Misc] Avoid "PyTorch non-writable tensors" warning in RayPPCommunicator (#27443)
1	1	vllm/distributed/device_communicators/ray_communicator.py

[074475541] strinczer 2025-10-24 [Bugfix] Fix Pydantic union resolution for ResponseFunctionToolCall in Responses API (#26706)
330	0	tests/entrypoints/openai/test_responses_function_call_parsing.py
43	0	vllm/entrypoints/openai/protocol.py

[d4c574c39] Aaron Pham 2025-10-24 [Chore] remove structural tags logging lines (#27451)
0	1	vllm/v1/structured_output/backend_xgrammar.py

[c528b9006] usberkeley 2025-10-24 Fix EventPublisherFactory logic for disabled KV cache events (#27419)
49	0	tests/distributed/test_events.py
5	1	vllm/distributed/kv_events.py

[85fee74b3] fhl2000 2025-10-24 [Bugfix][CI] Move resolving cudagraph_mode before initializing attn_metadata_builder (#27427)
1	1	docs/design/cuda_graphs.md
3	0	tests/compile/test_fusions_e2e.py
30	18	vllm/v1/worker/gpu_model_runner.py

[8dbe0c527] hfan 2025-10-23 [Misc] Add TPU usage report when using tpu_inference. (#27423)
30	10	vllm/usage/usage_lib.py

[5cc6bddb6] Xiangyu Li 2025-10-24 [Kernel] Add GPTQv2 format support for low-bit or asymmetric quantization, by adapting gptq_gemm (#26092)
1	1	csrc/ops.h
131	92	csrc/quantization/gptq/q_gemm.cu
2	1	csrc/torch_bindings.cpp
7	1	tests/kernels/quantization/test_gptq.py
109	0	tests/quantization/test_gptq_v2.py
10	1	vllm/_custom_ops.py
29	1	vllm/model_executor/layers/quantization/gptq.py
6	1	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py

[1f9460c4c] Harry Mellor 2025-10-24 Fix pooling adapters for Transformers backend  (#27338)
11	16	vllm/config/model.py
49	45	vllm/model_executor/models/adapters.py
22	0	vllm/model_executor/models/transformers/base.py
0	7	vllm/model_executor/models/transformers/legacy.py
7	6	vllm/model_executor/models/transformers/pooling.py
8	0	vllm/model_executor/models/utils.py

[70022ffc0] xiao-llm 2025-10-23 Granite 4.0 quark quantization support (#26944)
75	0	vllm/model_executor/models/granitemoehybrid.py

[f417746ad] Akash kaothalkar 2025-10-24 [Hardware][POWERPC] Disable oneDNN path in vllm/model_executor/layers/utils.py for Powerpc (#27422)
5	2	vllm/model_executor/layers/utils.py

[0552cfb19] Yu Jiaqi 2025-10-24 [Model] Siglip Embedding Support (#27324)
7	6	docs/models/supported_models.md
49	24	examples/offline_inference/vision_language_pooling.py
78	36	examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py
137	0	tests/models/multimodal/pooling/test_siglip.py
1	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py
594	18	vllm/model_executor/models/siglip.py
3	2	vllm/transformers_utils/chat_templates/registry.py

[51dd14ac2] Kebe 2025-10-24 [Bugfix][DP] Fix creating too many DP Placement Groups (#26880)
9	0	vllm/v1/engine/utils.py

[dbfbf9f32] Matthew Bonanni 2025-10-23 [Attention] Fix FlashMLA metadata builder arguments for q_len > 1 (#27368)
5	1	vllm/v1/attention/backends/mla/flashmla.py

[ca76486a1] Jonathan Chen 2025-10-23 [Chore] Separate out `vllm.utils.platform_utils.py` (#27374)
1	1	tests/kernels/core/test_uva.py
1	1	tests/v1/logits_processors/test_correctness.py
1	1	tests/v1/sample/test_sampler.py
1	1	tests/v1/worker/test_gpu_input_batch.py
1	1	vllm/device_allocator/cumem.py
1	1	vllm/lora/lora_weights.py
1	1	vllm/lora/models.py
1	1	vllm/model_executor/model_loader/utils.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
2	2	vllm/model_executor/models/utils.py
1	1	vllm/usage/usage_lib.py
3	50	vllm/utils/__init__.py
54	0	vllm/utils/platform_utils.py
2	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/kv_offload/worker/cpu_gpu.py
1	1	vllm/v1/pool/metadata.py
1	1	vllm/v1/sample/ops/penalties.py
1	1	vllm/v1/sample/sampler.py
1	1	vllm/v1/spec_decode/eagle.py
1	1	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/tpu_model_runner.py

[a9f55dc58] Varun Sundar Rabindranath 2025-10-23 [Misc] Add triton_kernels dependency (#27370)
2	0	requirements/cuda.txt
2	117	tests/kernels/moe/test_gpt_oss_triton_kernels.py

[81d5bb765] Isotr0py 2025-10-24 [Bugfix] Fix AWQ marlin layer skipping (#27416)
9	2	vllm/model_executor/layers/quantization/awq_marlin.py

[0825197be] Gregory Shtrasberg 2025-10-23 [Bugfix][ROCm][DeepSeek] Fix for forward_hip in rope for DeepSeek (#27373)
2	5	vllm/model_executor/layers/rotary_embedding/base.py

[9ef3d5b87] Alexander Matveev 2025-10-23 [Bugfix] Fix dp_chunking enablement logic in FusedMoE layer (#27220)
32	26	vllm/model_executor/layers/fused_moe/layer.py

[295c7f026] Alexei-V-Ivanov-AMD 2025-10-23 Mirroring the test definitions (2025-10-22) (#27362)
9	2	.buildkite/test-amd.yaml

[3fa2c1218] wang.yuqi 2025-10-23 [Frontend][4/N] Improve all pooling task | Add plugin pooling task (#26973)
14	4	docs/design/io_processor_plugins.md
1	1	examples/offline_inference/prithvi_geospatial_mae.py
5	9	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
3	4	examples/online_serving/prithvi_geospatial_mae.py
1	1	tests/models/multimodal/pooling/test_prithvi_mae.py
2	2	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
1	6	tests/plugins_tests/test_io_processor_plugins.py
28	13	vllm/entrypoints/llm.py
6	1	vllm/entrypoints/openai/api_server.py
1	6	vllm/entrypoints/openai/protocol.py
11	2	vllm/entrypoints/openai/serving_pooling.py
12	0	vllm/model_executor/layers/pooler.py
2	4	vllm/model_executor/models/terratorch.py
7	0	vllm/plugins/io_processors/interface.py
5	0	vllm/pooling_params.py
3	1	vllm/tasks.py

[fe2016de2] Cyrus Leung 2025-10-23 [CI/Build] Remove unnecessary flags from test registry (#27353)
2	2	docs/models/supported_models.md
9	3	tests/distributed/test_pipeline_parallel.py
9	3	tests/distributed/test_sequence_parallel.py
3	1	tests/entrypoints/openai/test_chat_template.py
12	4	tests/entrypoints/test_chat_utils.py
3	2	tests/models/multimodal/generation/vlm_utils/core.py
3	1	tests/models/multimodal/processing/test_common.py
3	1	tests/models/multimodal/processing/test_tensor_schema.py
3	1	tests/models/multimodal/test_mapping.py
30	71	tests/models/registry.py
4	6	tests/models/test_initialization.py
3	1	tests/models/utils.py
5	27	vllm/model_executor/models/glm4_1v.py

[237cf6d32] Ilya Markov 2025-10-23 [Misc] Remove use of CUDA_VISIBLE_DEVICES for device selection (fix DP slow startup time &c) (#26709)
8	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
4	1	vllm/v1/engine/utils.py
2	2	vllm/v1/worker/dp_utils.py
21	0	vllm/v1/worker/gpu_worker.py

[faee3ccdc] Navya Srivastava 2025-10-23 [Feature] Pydantic validation for speculative.py (#27156)
20	20	vllm/config/speculative.py

[570c3e1cd] Bradley D 2025-10-23 [Bugfix] Honor --mm_encoder_attn_backend when used (#27124)
5	1	vllm/attention/layer.py
1	0	vllm/model_executor/models/dots_ocr.py
1	0	vllm/model_executor/models/ernie45_vl.py
1	0	vllm/model_executor/models/glm4_1v.py
1	0	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/siglip2navit.py

[3a4255c7c] Harry Mellor 2025-10-23 Run mypy on the lowest supported Python version instead of system Python (#27048)
2	2	.pre-commit-config.yaml

[61089465a] tomeras91 2025-10-23 [Model] Add MoE support for NemotronH (#25863)
2	0	vllm/model_executor/layers/fused_moe/config.py
3	1	vllm/model_executor/layers/fused_moe/fused_moe.py
30	5	vllm/model_executor/layers/fused_moe/layer.py
26	5	vllm/model_executor/layers/quantization/modelopt.py
3	1	vllm/model_executor/models/interfaces.py
329	27	vllm/model_executor/models/nemotron_h.py
20	0	vllm/transformers_utils/configs/nemotron_h.py

[88afa1101] Tova Movshovitz 2025-10-23 [Metrics] [KVConnector] Add connector prefix cache hit rate stats (#26245)
60	0	tests/v1/core/test_scheduler.py
5	10	vllm/v1/core/kv_cache_manager.py
28	1	vllm/v1/core/sched/scheduler.py
45	0	vllm/v1/metrics/loggers.py
14	0	vllm/v1/metrics/stats.py

[d00ce29d8] Chauncey 2025-10-23 [CI] Reorganize entrypoints tests (#27403)
0	139	tests/entrypoints/openai/test_chat.py
141	0	tests/entrypoints/openai/test_completion_with_function_calling.py

[3b7bdf983] Louie Tsai 2025-10-23 add SLA information into comparison graph for vLLM Benchmark Suite (#25525)
182	33	.buildkite/nightly-benchmarks/scripts/compare-json-results.py
3	1	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
5	0	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
13	17	.buildkite/nightly-benchmarks/tests/latency-tests-cpu.json
203	0	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc3.json
128	20	.buildkite/nightly-benchmarks/tests/serving-tests-cpu.json
13	18	.buildkite/nightly-benchmarks/tests/throughput-tests-cpu.json

[50b788a17] Zhewen Li 2025-10-23 [CI/Build] Fix AMD CI: test_cpu_gpu.py (#27388)
13	4	tests/v1/kv_offload/test_cpu_gpu.py

[fc059c706] Lucia Fang 2025-10-23 [Bugfix] Fix args settings for guided decoding args (#27375)
3	5	vllm/engine/arg_utils.py

[bfb240cc4] Cyrus Leung 2025-10-23 [CI/Build] Fix Prithvi plugin test (#27393)
1	0	tests/plugins_tests/test_io_processor_plugins.py

[e255d9299] Jonathan Chen 2025-10-23 [Chore] Remove duplicate `has_` functions in vllm.utils (#27372)
0	41	vllm/utils/__init__.py

[3729ed00b] wang.yuqi 2025-10-23 [Model] Add num_cached_tokens for PoolingRequestOutput (#27378)
24	4	tests/models/language/pooling/test_auto_prefix_cache_support.py
33	0	tests/models/language/pooling/test_extract_hidden_states.py
3	0	vllm/entrypoints/llm.py
1	0	vllm/entrypoints/openai/serving_embedding.py
1	0	vllm/entrypoints/score_utils.py
12	1	vllm/outputs.py
1	0	vllm/v1/engine/output_processor.py

[6644796bf] Giancarlo Delfin 2025-10-22 [V1][spec decode] return logprobs for spec decoding (#26060)
94	0	tests/v1/sample/test_logprobs.py
85	79	tests/v1/sample/test_rejection_sampler.py
1	1	vllm/v1/engine/logprobs.py
24	9	vllm/v1/outputs.py
121	39	vllm/v1/sample/rejection_sampler.py
20	11	vllm/v1/sample/sampler.py
8	0	vllm/v1/spec_decode/metadata.py
40	48	vllm/v1/worker/gpu_model_runner.py

[ff93cc8c8] Andrew Sansom 2025-10-23 [CORE] Support Prefix Caching with Prompt Embeds (#27219)
2	2	docs/features/README.md
136	1	tests/v1/core/test_kv_cache_utils.py
0	10	vllm/engine/arg_utils.py
30	2	vllm/v1/core/kv_cache_utils.py
4	3	vllm/v1/serial_utils.py
13	0	vllm/v1/utils.py

[243ed7d32] PiteXChen 2025-10-23 [Bugfix][Core] running queue index leakage exception (#26754)
1	0	vllm/v1/core/sched/scheduler.py

[7e0941055] fangpings 2025-10-22 [Bugfix] Fix incorrect kv cache metrics in grafana.json (#27133)
1	1	examples/online_serving/dashboards/perses/performance_statistics.yaml
4	4	examples/online_serving/dashboards/perses/query_statistics.yaml
1	1	examples/online_serving/prometheus_grafana/grafana.json

[6738e4a09] Cyrus Leung 2025-10-23 [Bugfix] Fix SLA tuner initialization (#27355)
2	0	vllm/benchmarks/sweep/serve.py
2	1	vllm/benchmarks/sweep/serve_sla.py

[2566dca2a] Isotr0py 2025-10-23 [Bugfix] Fix deepseek-ocr multi-image inference and add `merge_by_field_config=True` with tensor schema support (#27361)
48	2	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
58	55	vllm/model_executor/models/deepseek_ocr.py
5	9	vllm/transformers_utils/processors/deepseek_ocr.py

[b4fda58a2] Matthew Bonanni 2025-10-22 [MLA] Bump FlashMLA (#27354)
1	1	cmake/external_projects/flashmla.cmake

[a0003b56b] dongbo910220 2025-10-23 [Chore] Separate out system utilities from vllm.utils (#27201)
1	1	tests/compile/test_async_tp.py
1	1	tests/compile/test_fusion_all_reduce.py
1	1	tests/compile/test_sequence_parallelism.py
1	1	tests/distributed/test_eplb_execute.py
1	1	tests/distributed/test_nccl_symm_mem_allreduce.py
1	1	tests/distributed/test_pynccl.py
1	1	tests/distributed/test_shm_broadcast.py
1	1	tests/distributed/test_symm_mem_allreduce.py
1	1	tests/distributed/test_utils.py
1	1	tests/kernels/mamba/test_mamba_mixer2.py
1	1	tests/models/test_vision.py
19	0	tests/utils_/test_system_utils.py
0	13	tests/utils_/test_utils.py
1	1	tests/v1/worker/test_gpu_model_runner.py
1	1	vllm/compilation/pass_manager.py
1	1	vllm/compilation/vllm_inductor_pass.py
1	1	vllm/distributed/device_communicators/all_reduce_utils.py
2	5	vllm/entrypoints/cli/serve.py
2	6	vllm/entrypoints/openai/api_server.py
36	108	vllm/utils/__init__.py
123	0	vllm/utils/system_utils.py
2	1	vllm/v1/engine/coordinator.py
1	4	vllm/v1/engine/core.py
2	6	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/worker/worker_base.py

[5beacce2e] Daisy-Ma-coder 2025-10-22 [BugFix] bugfix for Flash Attention MLA with full cuda graph IMA following pr-25490 (#27128)
20	13	vllm/v1/attention/backends/mla/flashattn_mla.py

[8669c69af] rongfu.leng 2025-10-23 [Feature] publisher default set zmq in kv_event config (#26915)
8	1	vllm/config/kv_events.py
2	2	vllm/distributed/kv_events.py

[1651003c3] Sage 2025-10-22 [Prefix Cache] Use LoRA name for consistent KV-cache block hashing (#27211)
19	0	tests/v1/core/test_kv_cache_utils.py
6	6	vllm/v1/core/kv_cache_utils.py

[1cb8c6c5f] William Song 2025-10-23 [Doc] Fix numbering sequence in prefix caching (#27357)
10	10	docs/design/prefix_caching.md

[e05a6754a] Luciano Martins 2025-10-22 [Model] Revert PR #26715: Restore custom PaliGemma and Gemma3-MM impl… (#27309)
2	2	docs/models/hardware_supported_models/tpu.md
20	3	docs/models/supported_models.md
1	2	examples/offline_inference/vision_language.py
11	5	tests/models/language/generation/test_gemma.py
40	34	tests/models/multimodal/generation/test_common.py
10	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
4	0	tests/models/multimodal/processing/test_common.py
1	0	tests/models/multimodal/processing/test_tensor_schema.py
710	0	vllm/model_executor/models/gemma3_mm.py
412	0	vllm/model_executor/models/paligemma.py
5	8	vllm/model_executor/models/registry.py
3	0	vllm/platforms/rocm.py

[084a9dae8] Isotr0py 2025-10-23 [Bugfix] Disable FlexAttention direct block mask building for encoder-only models (#27344)
4	1	vllm/v1/attention/backends/flex_attention.py

[c9461e05a] RED 2025-10-23 Support Anthropic API /v1/messages Endpoint (#22627)
1	0	requirements/common.txt
0	0	tests/entrypoints/anthropic/__init__.py
141	0	tests/entrypoints/anthropic/test_messages.py
126	0	tests/utils.py
0	0	vllm/entrypoints/anthropic/__init__.py
300	0	vllm/entrypoints/anthropic/api_server.py
162	0	vllm/entrypoints/anthropic/protocol.py
458	0	vllm/entrypoints/anthropic/serving_messages.py
11	46	vllm/entrypoints/openai/api_server.py
63	0	vllm/entrypoints/utils.py

[4dfdb821c] Nicolò Lucchesi 2025-10-22 [P/D] Dynamic `kv_output_aggregator` collect size (#26734)
2	2	tests/v1/kv_connector/unit/test_nixl_connector.py
41	2	tests/v1/kv_connector/unit/{test_output_aggreagator.py => test_output_aggregator.py}
30	6	vllm/distributed/kv_transfer/kv_connector/utils.py
2	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
1	3	vllm/v1/engine/core.py
7	4	vllm/v1/executor/abstract.py
7	1	vllm/v1/outputs.py

[58fab50d8] Russell Bryant 2025-10-22 [Frontend] Require flag for loading text and image embeds (#27204)
25	4	docs/features/multimodal_inputs.md
5	1	docs/features/prompt_embeds.md
1	0	examples/offline_inference/prithvi_geospatial_mae.py
1	0	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
1	0	examples/online_serving/prithvi_geospatial_mae.py
16	1	tests/entrypoints/llm/test_prompt_validation.py
13	0	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
27	2	tests/entrypoints/openai/test_prompt_validation.py
29	31	tests/entrypoints/openai/{test_skip_tokenizer.py => test_vision_embeds.py}
17	4	tests/entrypoints/test_chat_utils.py
1	0	tests/entrypoints/test_renderer.py
1	2	tests/models/multimodal/generation/test_common.py
1	0	tests/models/multimodal/generation/test_qwen2_vl.py
1	0	tests/models/multimodal/pooling/test_prithvi_mae.py
6	0	tests/models/test_initialization.py
1	0	tests/models/test_terratorch.py
1	0	tests/plugins_tests/test_io_processor_plugins.py
0	15	tests/v1/entrypoints/openai/test_completion.py
1	0	tests/v1/entrypoints/openai/test_completion_with_image_embeds.py
7	2	vllm/config/model.py
8	0	vllm/config/multimodal.py
5	0	vllm/engine/arg_utils.py
20	0	vllm/entrypoints/chat_utils.py
5	2	vllm/entrypoints/renderer.py
10	0	vllm/multimodal/processing.py

[db6f28d89] Isotr0py 2025-10-22 [Bugfix] Fix HF format InternVL large variants video processing (#27330)
4	1	vllm/model_executor/models/interns1.py

[14e2f1231] Cyrus Leung 2025-10-22 [Bugfix] Make `get_mrope_input_positions` instance methods (#27342)
1	2	vllm/model_executor/models/ernie45_vl.py
1	2	vllm/model_executor/models/glm4v.py
1	2	vllm/model_executor/models/keye_vl1_5.py
1	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	2	vllm/model_executor/models/qwen2_5_vl.py
0	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py
1	2	vllm/model_executor/models/qwen3_vl.py

[7c4767f1e] Chendi.Xue 2025-10-22 [NIXL] use Host buffer to support TP_ratio > 1 for XPU (#27140)
24	5	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
10	0	vllm/platforms/xpu.py

[9771e0b43] Jee Jee Li 2025-10-22 [Bugfix] Add missing 'is_internal_router' attribute to FusedMoEWithLoRA (#27351)
4	0	vllm/lora/layers/fused_moe.py

[980de31ca] Reinforce-II 2025-10-22 [bugfix] remove unused parameters to reduce unnecessary vram usage (#26789)
2	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[1c160841e] Wentao Ye 2025-10-22 [Bug] Fix DeepSeek-V2.5-1210-FP8 issue (#27267)
11	5	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[4ca13a866] Mark McLoughlin 2025-10-22 [NIXL] Terminate handshake listener thread in shutdown (#26404)
22	7	tests/v1/kv_connector/unit/test_nixl_connector.py
30	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[675aa2ec6] Isotr0py 2025-10-22 [Model] Upstream Deepseek-OCR model (#27247)
1	0	docs/models/supported_models.md
69	20	examples/offline_inference/vision_language.py
3	0	tests/models/registry.py
673	0	vllm/model_executor/models/deepencoder.py
594	0	vllm/model_executor/models/deepseek_ocr.py
23	20	vllm/model_executor/models/deepseek_vl2.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/chat_templates/registry.py
14	0	vllm/transformers_utils/chat_templates/template_deepseek_ocr.jinja
442	0	vllm/transformers_utils/processors/deepseek_ocr.py

[3ae082c37] dongbo910220 2025-10-22 [Chore] Separate out optional dependency checks from vllm.utils (#27207)
2	1	tests/kernels/attention/test_deepgemm_attention.py
1	1	tests/kernels/moe/modular_kernel_tools/common.py
1	1	tests/kernels/moe/modular_kernel_tools/mk_objects.py
1	1	tests/kernels/moe/parallel_utils.py
1	1	tests/kernels/moe/test_block_fp8.py
1	1	tests/kernels/moe/test_deepep_deepgemm_moe.py
1	1	tests/kernels/moe/test_deepep_moe.py
1	1	tests/kernels/moe/test_gpt_oss_triton_kernels.py
1	1	tests/kernels/moe/test_modular_kernel_combinations.py
1	1	tests/kernels/quantization/test_block_fp8.py
0	1	tests/models/quantization/test_fp8.py
1	1	vllm/distributed/device_communicators/all2all.py
2	1	vllm/model_executor/layers/fused_moe/config.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
2	1	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/fp8.py
2	4	vllm/model_executor/layers/quantization/mxfp4.py
0	41	vllm/utils/__init__.py
2	1	vllm/utils/deep_gemm.py
36	0	vllm/utils/import_utils.py
1	1	vllm/v1/worker/gpu_ubatch_wrapper.py

[49c00fe30] Alexei-V-Ivanov-AMD 2025-10-22 Mirroring changes in test-pipeline.yaml into test-amd.yaml (#27242)
65	14	.buildkite/test-amd.yaml

[141d3b9fc] Mark McLoughlin 2025-10-22 [docs] Update v1 metrics design doc (#27332)
69	83	docs/design/metrics.md

[abf3db40e] Jee Jee Li 2025-10-22 [Core] Handle MoE LoRA edge cases (#27335)
7	10	vllm/lora/layers/fused_moe.py
0	1	vllm/lora/models.py

[8e4ca4d14] gnovack 2025-10-22 Bugfix - pass 'max_num_tokens_padded' into 'moe_lora_align_block_size' (#27311)
2	6	csrc/moe/moe_lora_align_sum_kernels.cu
2	1	csrc/moe/moe_ops.h
2	0	csrc/moe/torch_bindings.cpp
2	0	tests/lora/test_fused_moe_lora_kernel.py
1	1	tests/lora/test_gptoss.py
2	0	tests/lora/test_moe_lora_align_sum.py
4	0	vllm/_custom_ops.py
2	0	vllm/lora/punica_wrapper/punica_gpu.py

[1a0f4defb] Wentao Ye 2025-10-22 [Log] Add Warning for `LLM(data_parallel_size=k)` single-process DP Usage (#27282)
10	0	vllm/entrypoints/llm.py

[843af7f7f] Li, Jiang 2025-10-22 [Bugfix][CPU] Disable dual stream execution for experts on CPU (#27320)
3	0	vllm/platforms/cpu.py

[1f633b863] wang.yuqi 2025-10-22 [Frontend][3/N] Improve all pooling task | Support binary embedding response (#27066)
8	2	examples/online_serving/pooling/README.md
24	19	examples/online_serving/pooling/{embedding_embed_dtype_client.py => embedding_requests_base64_client.py}
66	0	examples/online_serving/pooling/embedding_requests_bytes_client.py
82	34	tests/entrypoints/pooling/openai/test_embedding.py
90	32	tests/entrypoints/pooling/openai/test_pooling.py
40	0	tests/utils_/test_serial_utils.py
18	1	vllm/entrypoints/openai/api_server.py
57	23	vllm/entrypoints/openai/protocol.py
69	34	vllm/entrypoints/openai/serving_embedding.py
70	54	vllm/entrypoints/openai/serving_pooling.py
0	33	vllm/entrypoints/openai/utils.py
169	0	vllm/utils/serial_utils.py

[a4c29e6e8] ExtReMLapin 2025-10-22 fixed reasoning streaming with tool_choice="required" (#24108)
10	2	tests/entrypoints/openai/test_completion_with_function_calling.py
45	20	vllm/entrypoints/openai/serving_chat.py

[8f18feb19] Harry Mellor 2025-10-22 Remove last `level` references not removed in #26355 (#27260)
2	2	tests/compile/piecewise/test_toy_llama.py
1	1	tests/compile/test_aot_compile.py
3	3	tests/compile/test_config.py
1	1	tests/compile/test_full_graph.py
3	3	tests/compile/test_fusions_e2e.py
3	3	tests/model_executor/test_enabled_custom_ops.py

[ed540d6d4] Huy Do 2025-10-22 Update release pipeline for PyTorch 2.9.0 (#27303)
12	12	.buildkite/release-pipeline.yaml
6	14	.buildkite/scripts/upload-wheels.sh
1	1	docker/Dockerfile.cpu

[f6027b285] wangxiyuan 2025-10-22 [1/N][Platform] Cleanup useless function (#26982)
5	2	tests/models/quantization/test_fp8.py
0	4	tests/quantization/test_compressed_tensors.py
1	44	vllm/platforms/cuda.py
14	27	vllm/platforms/interface.py
1	7	vllm/platforms/rocm.py
0	6	vllm/platforms/tpu.py
0	16	vllm/platforms/xpu.py

[ab3e80042] Jiangyun Zhu 2025-10-22 [torch.compile] Enable silu_mul_fp8_quant fusion without custom ops enabled (#27146)
75	43	tests/compile/test_silu_mul_quant_fusion.py
21	26	vllm/compilation/activation_quant_fusion.py
30	0	vllm/compilation/matcher_utils.py
2	1	vllm/model_executor/layers/activation.py

[ceacedc1f] Cyrus Leung 2025-10-22 [Benchmark] Add plot utility for parameter sweep (#27168)
27	11	docs/contributing/benchmarks.md
0	1157	vllm/benchmarks/serve_multi.py
0	0	vllm/benchmarks/sweep/__init__.py
91	0	vllm/benchmarks/sweep/param_sweep.py
530	0	vllm/benchmarks/sweep/plot.py
407	0	vllm/benchmarks/sweep/serve.py
483	0	vllm/benchmarks/sweep/serve_sla.py
114	0	vllm/benchmarks/sweep/server.py
132	0	vllm/benchmarks/sweep/sla_sweep.py
4	0	vllm/benchmarks/sweep/utils.py

[bfa59be8f] Nicolò Lucchesi 2025-10-22 [CI] Nixl integration tests DP-EP (#27199)
9	1	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
1	0	tests/v1/kv_connector/nixl_integration/tp_config_sweep_accuracy_test.sh

[265ecb05f] vllmellm 2025-10-22 [DOC] [ROCm] Add ROCm quickstart guide (#26505)
54	20	docs/getting_started/quickstart.md

[09a7e6f61] Lain 2025-10-21 [Deepseek v3.2] Remove extra logics in indexer (#26465)
4	0	csrc/ops.h
61	13	csrc/sampler.cu
6	0	csrc/torch_bindings.cpp
59	1	tests/kernels/test_top_k_per_row.py
11	26	vllm/model_executor/models/deepseek_v2.py

[6c2eef5a5] Tyler Michael Smith 2025-10-21 [P/D] KVConnector for decode benchmarking (#25986)
415	0	tests/v1/kv_connector/unit/test_decode_bench_connector.py
6	0	vllm/distributed/kv_transfer/kv_connector/factory.py
4	1	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
413	0	vllm/distributed/kv_transfer/kv_connector/v1/decode_bench_connector.py

[19748806f] Benjamin Chislett 2025-10-21 [Bugfix] skip cuda graph for drafter when running with eager (#26821)
4	1	vllm/v1/worker/gpu_model_runner.py

[4a8a567e1] ExtReMLapin 2025-10-22 Updated xgrammar backend to not deny supported string formats (#27253)
2	1	tests/v1/structured_output/test_utils.py
24	1	vllm/v1/structured_output/backend_xgrammar.py

[344a0017c] Alexander Matveev 2025-10-21 [Performance] Dual stream execution of "shared_experts" and "selected_experts" inside FusedMoE (#26440)
5	0	vllm/envs.py
89	15	vllm/model_executor/layers/fused_moe/layer.py
16	1	vllm/model_executor/layers/fused_moe/shared_fused_moe.py
12	6	vllm/model_executor/models/deepseek_v2.py

[becb7de40] Huy Do 2025-10-21 Update PyTorch to 2.9.0+cu129 (#24994)
7	2	.buildkite/test-pipeline.yaml
1	1	.pre-commit-config.yaml
2	2	CMakeLists.txt
12	2	docker/Dockerfile
4	0	docker/Dockerfile.cpu
-	-	docs/assets/contributing/dockerfile-stages-dependency.png
4	22	docs/contributing/ci/update_pytorch_version.md
1	1	pyproject.toml
1	1	requirements/build.txt
5	5	requirements/cuda.txt
5	5	requirements/rocm-build.txt
4	4	requirements/test.in
19	18	requirements/test.txt
1	1	tests/lora/test_chatglm3_tp.py
2	0	tests/v1/e2e/test_async_sched_and_preempt.py
0	3	tools/install_gdrcopy.sh

[250fb1b8e] Tao He 2025-10-22 [Bugfix] fixes the decoding metadata of dense mla's fp8 kvcache. (#27144)
2	1	cmake/external_projects/flashmla.cmake
6	0	vllm/attention/ops/flashmla.py
2	0	vllm/v1/attention/backends/mla/flashmla.py

[647214f3d] Nick Hill 2025-10-21 [V0 Deprecation] Remove V0 executors (#27142)
2	4	tests/basic_correctness/test_basic_correctness.py
1	1	tests/distributed/test_multi_node_assignment.py
1	3	tests/distributed/test_pipeline_parallel.py
1	1	tests/model_executor/model_loader/tensorizer_loader/conftest.py
2	1	tests/v1/engine/test_engine_core.py
4	4	tools/pre_commit/check_pickle_imports.py
2	2	vllm/__init__.py
14	8	vllm/config/parallel.py
0	6	vllm/config/scheduler.py
1	1	vllm/distributed/device_communicators/tpu_communicator.py
3	4	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/cli/serve.py
1	22	vllm/envs.py
0	0	vllm/executor/__init__.py
0	393	vllm/executor/executor_base.py
0	36	vllm/executor/msgspec_utils.py
0	10	vllm/sequence.py
1	1	vllm/transformers_utils/config.py
1	1	vllm/v1/engine/async_llm.py
2	11	vllm/v1/engine/core.py
1	1	vllm/v1/engine/core_client.py
1	1	vllm/v1/engine/llm_engine.py
1	1	vllm/v1/engine/utils.py
6	0	vllm/v1/executor/__init__.py
229	40	vllm/v1/executor/abstract.py
2	1	vllm/v1/executor/multiproc_executor.py
4	107	vllm/v1/executor/ray_distributed_executor.py
104	280	vllm/{executor/ray_distributed_executor.py => v1/executor/ray_executor.py}
28	50	vllm/{ => v1}/executor/ray_utils.py
11	29	vllm/{ => v1}/executor/uniproc_executor.py
0	22	vllm/v1/worker/worker_base.py

[ddeec11ba] David Whyte-Gray 2025-10-21 [Bugfix][P/D] Reduce num_threads used by nixl ucx backend (#27196)
11	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[86ed77022] Wentao Ye 2025-10-21 [Feature] Batch Invariant for R1 TP 8 on Blackwell (#27229)
1	1	vllm/model_executor/layers/batch_invariant.py

[aa1356ec5] Micah Williamson 2025-10-21 [ROCm] Update Triton, Torch, and AITER branches for ROCm base Dockerfile (#27206)
3	3	docker/Dockerfile.rocm_base

[ecc3c0940] Pavani Majety 2025-10-21 Add @pavanimajety to .github/codeowners for Flashinfer, ModelOpt related code (#27213)
5	4	.github/CODEOWNERS

[ba09652de] JartX 2025-10-21 [ROCM] Enable CompressedTensorsWNA16 (#27187)
4	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[bd66b8529] Harry Mellor 2025-10-21 [CI] Install pre-release version of `apache-tvm-ffi` for `flashinfer` (#27262)
6	2	docker/Dockerfile

[6c728f777] dongbo910220 2025-10-21 [Chore] Separate out NCCL utilities from vllm.utils (#27197)
1	1	vllm/distributed/device_communicators/pynccl_allocator.py
1	1	vllm/distributed/device_communicators/pynccl_wrapper.py
0	85	vllm/utils/__init__.py
64	0	vllm/utils/nccl.py

[80e945298] Daniel Cámpora 2025-10-21 [Deepseek v3.2] Optimize top_k_per_row (#26763)
1	2	csrc/ops.h
5	30	csrc/sampler.cu
1	1	csrc/torch_bindings.cpp
6	8	tests/kernels/test_top_k_per_row.py
0	8	vllm/model_executor/models/deepseek_v2.py

[c3a2c6ac5] Roger Wang 2025-10-21 [MM][Core] Decouple ViT backend from LM backend (#27061)
25	0	tests/config/test_multimodal_config.py
10	1	vllm/attention/layer.py
5	0	vllm/config/model.py
38	4	vllm/config/multimodal.py
9	0	vllm/engine/arg_utils.py
17	2	vllm/model_executor/models/dots_ocr.py
15	1	vllm/model_executor/models/ernie45_vl.py
15	1	vllm/model_executor/models/glm4_1v.py
18	1	vllm/model_executor/models/keye.py
12	0	vllm/model_executor/models/ovis2_5.py
10	1	vllm/model_executor/models/qwen2_5_vl.py
15	1	vllm/model_executor/models/qwen2_vl.py
10	1	vllm/model_executor/models/qwen3_omni_moe_thinker.py
10	2	vllm/model_executor/models/qwen3_vl.py
12	1	vllm/model_executor/models/siglip2navit.py
9	1	vllm/model_executor/models/vision.py

[72f431e70] Nicolò Lucchesi 2025-10-21 [Nixl] Minor refactor to handshake related metadata (#26410)
0	2	tests/v1/kv_connector/unit/test_nixl_connector.py
176	86	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[be4445072] Zebing Lin 2025-10-21 [Fix][Spec Decode] Fix llama4 draft loading with different quantization (#27136)
17	10	vllm/model_executor/models/llama4_eagle.py

[f381cf230] Benjamin Chislett 2025-10-21 [Bugfix] Fix broken MTP weight loading for FP8 KV Scales (#27227)
8	1	vllm/model_executor/models/deepseek_mtp.py

[5ff5d94e7] Varun Sundar Rabindranath 2025-10-21 [Bugfix] Fix gpt-oss w4a8 DP/EP on B200 (#26729)
20	0	tests/quantization/test_blackwell_moe.py
20	0	vllm/model_executor/layers/fused_moe/config.py
18	0	vllm/model_executor/layers/quantization/mxfp4.py
4	1	vllm/model_executor/layers/quantization/utils/mxfp8_utils.py
20	1	vllm/model_executor/warmup/kernel_warmup.py

[f95da13c3] Shu Wang 2025-10-21 [ModelOpt] Load w13/w2_input_scale for all experts, nvfp4 (#26135)
22	4	vllm/model_executor/layers/fused_moe/layer.py
30	5	vllm/model_executor/layers/quantization/modelopt.py
6	0	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[aef368aa0] Po-Han Huang (NVIDIA) 2025-10-21 [BugFix] GPT-OSS Attention DP + MoE TP weight loading issue (#24032)
14	9	vllm/model_executor/layers/fused_moe/config.py
16	4	vllm/model_executor/models/gpt_oss.py

[5f6cbf60d] Chen Wu 2025-10-21 [Feature][Kernel]FusedMoE LoRA (#21229)
7	1	.buildkite/test-pipeline.yaml
1	0	CMakeLists.txt
173	0	csrc/moe/moe_lora_align_sum_kernels.cu
7	0	csrc/moe/moe_ops.h
12	0	csrc/moe/torch_bindings.cpp
20	0	tests/lora/conftest.py
97	0	tests/lora/test_deepseekv2_tp.py
287	0	tests/lora/test_fused_moe_lora_kernel.py
52	0	tests/lora/test_gptoss.py
90	0	tests/lora/test_moe_lora_align_sum.py
109	0	tests/lora/test_olmoe_tp.py
111	0	tests/lora/test_qwen3moe_tp.py
22	0	vllm/_custom_ops.py
2	0	vllm/lora/layers/__init__.py
410	0	vllm/lora/layers/fused_moe.py
64	20	vllm/lora/models.py
2	0	vllm/lora/ops/triton_ops/__init__.py
350	0	vllm/lora/ops/triton_ops/fused_moe_lora_op.py
39	0	vllm/lora/punica_wrapper/punica_base.py
98	2	vllm/lora/punica_wrapper/punica_gpu.py
39	0	vllm/lora/utils.py
2	1	vllm/lora/worker_manager.py
53	18	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
7	2	vllm/model_executor/layers/fused_moe/fused_moe.py
3	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
11	0	vllm/model_executor/models/deepseek_v2.py
13	2	vllm/model_executor/models/gpt_oss.py
3	9	vllm/model_executor/models/olmoe.py

[3ada34f9c] Russell Bryant 2025-10-20 [Frontend] Enforce tokenize=False when applying chat template (#27205)
12	0	tests/entrypoints/test_chat_utils.py
19	12	vllm/entrypoints/chat_utils.py

[0eb8f2b88] Lunwen He 2025-10-20 create is_in_the_same_node on cpu (#26832)
1	0	.buildkite/test-amd.yaml
1	0	.buildkite/test-pipeline.yaml
21	7	tests/distributed/test_same_node.py
3	1	vllm/distributed/parallel_state.py

[163965d18] Fadi Arafeh 2025-10-21 [cpu] Dispatch un-quantized linear to oneDNN/ACL by default for AArch64 (#27183)
39	18	cmake/cpu_extension.cmake
1	0	requirements/cpu-build.txt
2	5	vllm/model_executor/layers/utils.py

[a03cf9bc7] Nick Hill 2025-10-20 [V0 Deprecation] Remove V0 metrics code (#27215)
0	688	vllm/engine/metrics.py
0	84	vllm/engine/metrics_types.py

[352c0c8a2] Isotr0py 2025-10-21 [Quantization] Automatically infer AWQ `modules_to_not_convert` field (#26909)
34	6	vllm/model_executor/layers/quantization/awq.py
36	9	vllm/model_executor/layers/quantization/awq_marlin.py
2	1	vllm/model_executor/layers/quantization/gptq.py
5	5	vllm/model_executor/layers/quantization/ipex_quant.py
19	8	vllm/model_executor/layers/quantization/utils/quant_utils.py
0	16	vllm/model_executor/models/minicpmv.py

[bfe0b4bd2] Andrew Xia 2025-10-20 [ez] add uv lock to gitignore (#27212)
3	0	.gitignore

[58fbbcb2f] Concurrensee 2025-10-20 [ROCm] enable some tests in entrypoints test groups on AMD (#26725)
6	2	requirements/rocm-test.txt

[87778d5f0] Heng Guo 2025-10-21 [Feature][Quantization] auto_round support for mixed bits quantization (#23812)
6	0	vllm/model_executor/layers/quantization/auto_round.py

[f9e7ad540] Nicolò Lucchesi 2025-10-20 [Bugfix][CI] Fix `Distributed Tests (4 GPUs)` async_sched+ray test (#27195)
3	0	tests/v1/distributed/test_async_llm_dp.py

[4d0f26611] shivampr 2025-10-20 [Kernel][Model] Tune fused_moe Triton configs for Qwen3-30B A3/A3B on H100 (FP8/BF16) (#26268)
82	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=bf16.json
82	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
82	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=bf16.json
82	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=8960,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json

[e93ff6c8b] Eugene Khvedchenya 2025-10-20 Nemotron Nano V2 VL + EVS Video Support (#27107)
266	49	vllm/model_executor/models/nano_nemotron_vl.py
5	33	vllm/model_executor/models/radio.py
1	1	vllm/multimodal/profiling.py
23	13	vllm/multimodal/video.py
21	9	vllm/v1/worker/gpu_model_runner.py

[1c691f4a7] ioana ghiban 2025-10-20 AArch64 CPU Docker pipeline (#26931)
30	0	.buildkite/release-pipeline.yaml
88	0	docker/Dockerfile.cpu

[9fce7bee7] Jiangyun Zhu 2025-10-20 [Kernel] Accelerate solve_tril with TMA (#26746)
32	11	vllm/model_executor/layers/fla/ops/op.py
375	290	vllm/model_executor/layers/fla/ops/solve_tril.py
5	0	vllm/model_executor/layers/fla/ops/utils.py

[b63f2143f] Andy Lo 2025-10-20 [LoRA] LoRA cuda graph specialization (#25914)
15	6	tests/lora/test_chatglm3_tp.py
8	1	tests/lora/test_llama_tp.py
8	0	vllm/config/compilation.py
7	1	vllm/forward_context.py
2	0	vllm/lora/ops/triton_ops/lora_shrink_op.py
21	13	vllm/lora/punica_wrapper/punica_gpu.py
20	4	vllm/v1/cudagraph_dispatcher.py
27	6	vllm/v1/worker/gpu_model_runner.py
14	3	vllm/v1/worker/lora_model_runner_mixin.py

[f32bf7582] Yi Zhang 2025-10-20 [Model][VLM] Support Bee-8B Model (#27012)
1	0	docs/models/supported_models.md
28	0	examples/offline_inference/vision_language.py
36	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
4	0	tests/models/registry.py
157	0	vllm/model_executor/models/bee.py
1	0	vllm/model_executor/models/registry.py

[8a81d776c] Yongtao Huang 2025-10-20 Fix typo in ValueError message: use `kv_role` instead of `kv_disagg_role` (#27166)
1	1	vllm/config/kv_transfer.py

[f6fdacd82] Sergei Skvortsov 2025-10-19 [Bugfix] Fix error with penalties when speculative decoding and structural output are enabled (#26586)
8	2	vllm/v1/worker/gpu_model_runner.py

[d31f7844f] Cyrus Leung 2025-10-19 [Misc] Move utils to avoid conflicts with stdlib, and move tests (#27169)
1	1	tests/conftest.py
1	1	tests/lora/test_add_lora.py
1	1	tests/models/multimodal/generation/test_common.py
1	1	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	1	tests/models/multimodal/processing/test_tensor_schema.py
1	1	tests/utils_/test_async_utils.py
1	1	tests/utils_/{test_collections.py => test_collection_utils.py}
1	1	tests/utils_/test_func_utils.py
25	0	tests/utils_/test_hashing.py
63	0	tests/utils_/test_mem_utils.py
104	0	tests/utils_/test_torch_utils.py
0	176	tests/utils_/test_utils.py
4	5	tools/pre_commit/check_pickle_imports.py
1	1	vllm/benchmarks/throughput.py
1	1	vllm/entrypoints/chat_utils.py
1	1	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/entrypoints/openai/serving_completion.py
2	2	vllm/entrypoints/openai/serving_embedding.py
2	2	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_pooling.py
1	1	vllm/entrypoints/openai/serving_score.py
1	1	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
1	1	vllm/entrypoints/renderer.py
1	1	vllm/executor/executor_base.py
1	1	vllm/executor/ray_distributed_executor.py
1	1	vllm/inputs/parse.py
1	1	vllm/model_executor/layers/activation.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/layers/quantization/gptq.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
1	1	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/interfaces_base.py
1	1	vllm/model_executor/models/llava_next_video.py
1	1	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/multimodal/inputs.py
1	1	vllm/multimodal/parse.py
3	3	vllm/multimodal/processing.py
1	1	vllm/multimodal/registry.py
1	1	vllm/reasoning/abs_reasoning_parsers.py
1	1	vllm/tracing.py
1	1	vllm/transformers_utils/processor.py
0	6	vllm/utils/__init__.py
0	0	vllm/utils/{asyncio.py => async_utils.py}
0	0	vllm/utils/{collections.py => collection_utils.py}
0	0	vllm/utils/{functools.py => func_utils.py}
3	3	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/core_client.py
1	1	vllm/v1/worker/gpu_input_batch.py
1	1	vllm/v1/worker/tpu_input_batch.py

[7a6c8c3fa] iAmir97 2025-10-19 [Chore] Separate out `vllm.utils.network_utils` (#27164)
1	1	examples/offline_inference/data_parallel.py
1	1	examples/offline_inference/rlhf.py
1	1	tests/distributed/test_multi_node_assignment.py
1	1	tests/distributed/test_node_count.py
1	1	tests/distributed/test_same_node.py
2	1	tests/distributed/test_shm_broadcast.py
2	4	tests/distributed/test_utils.py
1	1	tests/entrypoints/openai/test_shutdown.py
1	1	tests/kernels/moe/modular_kernel_tools/parallel_utils.py
2	1	tests/kernels/moe/parallel_utils.py
1	1	tests/model_executor/model_loader/tensorizer_loader/conftest.py
2	1	tests/models/test_vision.py
1	1	tests/utils.py
126	0	tests/utils_/test_network_utils.py
0	119	tests/utils_/test_utils.py
1	1	vllm/config/parallel.py
1	1	vllm/distributed/device_communicators/shm_broadcast.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
1	1	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
1	3	vllm/distributed/parallel_state.py
1	1	vllm/distributed/utils.py
2	1	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/cli/serve.py
1	1	vllm/entrypoints/launcher.py
1	1	vllm/entrypoints/openai/api_server.py
2	2	vllm/executor/ray_distributed_executor.py
1	1	vllm/executor/ray_utils.py
2	1	vllm/executor/uniproc_executor.py
12	322	vllm/utils/__init__.py
331	0	vllm/utils/network_utils.py
2	1	vllm/v1/engine/coordinator.py
1	1	vllm/v1/engine/core.py
2	2	vllm/v1/engine/core_client.py
2	1	vllm/v1/engine/utils.py
4	2	vllm/v1/executor/multiproc_executor.py
2	6	vllm/v1/utils.py

[221bf7257] Jianyu Huang 2025-10-19 output type conversion fix (#27159)
1	4	vllm/model_executor/layers/batch_invariant.py

[b3aba04e5] Cyrus Leung 2025-10-19 [Benchmark] Convenience script for multiple parameter combinations (#27085)
145	3	docs/contributing/benchmarks.md
1157	0	vllm/benchmarks/serve_multi.py
10	0	vllm/entrypoints/openai/api_server.py

[8a297115e] dongbo910220 2025-10-19 [Chore] Separate out hashing utilities from vllm.utils (#27151)
1	2	tests/utils_/test_utils.py
1	1	tests/v1/core/test_kv_cache_utils.py
1	1	tests/v1/core/test_prefix_caching.py
1	1	tests/v1/core/utils.py
1	1	tests/v1/kv_connector/unit/test_offloading_connector.py
1	1	tests/v1/kv_connector/unit/utils.py
1	0	tools/pre_commit/check_pickle_imports.py
1	53	vllm/utils/__init__.py
63	0	vllm/utils/hashing.py
2	1	vllm/v1/core/kv_cache_utils.py
1	1	vllm/v1/engine/core.py

[191eed0bb] 22quinn 2025-10-18 [BugFix] Fix lazy imports involving outlines_core (#27158)
2	0	vllm/v1/structured_output/backend_outlines.py
2	0	vllm/v1/structured_output/utils.py

[fb860670d] Woosuk Kwon 2025-10-18 [Minor] Remove unused env variable (#27161)
0	18	vllm/envs.py

[83e760c57] Tova Movshovitz 2025-10-19 [V1][Metrics][Plugin] Add plugin support for custom `StatLoggerBase` implementations (#22456)
5	0	.buildkite/test-pipeline.yaml
3	1	docs/design/plugin_system.md
29	0	tests/plugins/vllm_add_dummy_stat_logger/dummy_stat_logger/dummy_stat_logger.py
15	0	tests/plugins/vllm_add_dummy_stat_logger/setup.py
76	0	tests/plugins_tests/test_stats_logger_plugins.py
3	23	tests/v1/metrics/test_engine_logger_apis.py
15	6	vllm/v1/engine/async_llm.py
18	0	vllm/v1/metrics/loggers.py

[c2bba6906] Lucas Wilkinson 2025-10-18 [BugFix] Disable fp8 kv-cache by default for DeepSeek V3.2 (#27121)
2	5	vllm/model_executor/models/config.py

[e133d6d21] Boyuan Feng 2025-10-18 [BugFix] fix graph partition signature (#27139)
151	0	vllm/env_override.py

[a1946c9f6] dongbo910220 2025-10-19 [Chore] Separate out profiling utilities from vllm.utils (#27150)
9	5	docs/contributing/profiling.md
24	45	vllm/utils/__init__.py
56	0	vllm/utils/profiling.py

[9f020f4f3] Lucas Wilkinson 2025-10-18 [BugFix] Fix failing gemma-3-1b-it test: `test_lm_eval_accuracy_v1_engine[google/gemma-3-1b-it]` (#27111)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[3b4507520] Nick Hill 2025-10-18 [Minor] Add some clarifying comments to recent changes (#27130)
7	1	vllm/distributed/device_communicators/shm_broadcast.py
3	1	vllm/v1/core/sched/output.py

[168e578ef] Yongtao Huang 2025-10-19 Fix incorrect string formatting in barrier timeout exceptions (#27149)
4	2	vllm/distributed/utils.py

[6ac5e06f7] Isotr0py 2025-10-19 [Chore] Clean up pytorch helper functions in `vllm.utils` (#26908)
2	1	benchmarks/kernels/bench_per_token_quant_fp8.py
2	1	benchmarks/kernels/benchmark_activation.py
2	1	benchmarks/kernels/benchmark_layernorm.py
2	2	benchmarks/kernels/benchmark_paged_attention.py
2	1	benchmarks/kernels/benchmark_quant.py
2	2	benchmarks/kernels/benchmark_reshape_and_cache.py
2	2	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
1	1	tests/compile/piecewise/test_full_cudagraph.py
1	1	tests/compile/piecewise/test_multiple_graphs.py
1	1	tests/compile/piecewise/test_simple.py
1	1	tests/compile/piecewise/test_toy_llama.py
1	1	tests/compile/silly_attention.py
1	1	tests/compile/test_aot_compile.py
1	1	tests/compile/test_basic_correctness.py
1	1	tests/compile/test_config.py
1	1	tests/compile/test_decorator.py
1	1	tests/compile/test_full_graph.py
1	1	tests/compile/test_fusions_e2e.py
1	1	tests/conftest.py
1	1	tests/distributed/test_sequence_parallel.py
1	1	tests/distributed/test_utils.py
4	1	tests/kernels/attention/conftest.py
1	1	tests/kernels/attention/test_prefix_prefill.py
2	1	tests/kernels/core/test_uva.py
2	1	tests/kernels/moe/test_modular_kernel_combinations.py
1	1	tests/kernels/utils.py
1	1	tests/models/multimodal/pooling/test_intern_vit.py
1	1	tests/models/multimodal/pooling/test_radio.py
1	1	tests/models/multimodal/processing/test_tensor_schema.py
1	1	tests/utils.py
6	4	tests/utils_/test_utils.py
2	1	tests/v1/attention/test_attention_backends.py
2	1	tests/v1/attention/test_mla_backends.py
1	1	tests/v1/engine/test_async_llm.py
1	1	tests/v1/engine/test_engine_core.py
1	1	tests/v1/engine/test_engine_core_client.py
2	1	tests/v1/sample/test_sampler.py
1	1	tests/v1/sample/utils.py
1	1	tests/v1/shutdown/test_delete.py
1	1	tests/v1/shutdown/test_forward_error.py
1	1	tests/v1/shutdown/test_startup_error.py
2	1	tests/v1/worker/test_gpu_input_batch.py
1	1	vllm/attention/layer.py
1	1	vllm/attention/ops/rocm_aiter_mla.py
1	1	vllm/compilation/backends.py
1	1	vllm/compilation/collective_fusion.py
1	1	vllm/compilation/compiler_interface.py
1	1	vllm/compilation/cuda_graph.py
1	1	vllm/compilation/decorators.py
1	1	vllm/compilation/inductor_pass.py
1	1	vllm/config/compilation.py
2	1	vllm/config/model.py
2	1	vllm/config/parallel.py
2	1	vllm/distributed/device_communicators/all_reduce_utils.py
1	1	vllm/distributed/device_communicators/custom_all_reduce.py
2	2	vllm/distributed/device_communicators/pynccl.py
1	1	vllm/distributed/device_communicators/quick_all_reduce.py
1	1	vllm/distributed/device_communicators/ray_communicator.py
2	1	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
4	2	vllm/distributed/parallel_state.py
2	1	vllm/distributed/utils.py
1	1	vllm/env_override.py
1	1	vllm/envs.py
1	1	vllm/lora/ops/triton_ops/lora_expand_op.py
1	1	vllm/lora/ops/triton_ops/lora_shrink_op.py
1	1	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
2	1	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	1	vllm/model_executor/layers/fused_moe/utils.py
1	1	vllm/model_executor/layers/layernorm.py
1	1	vllm/model_executor/layers/mamba/linear_attn.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
4	1	vllm/model_executor/layers/mamba/mamba_utils.py
1	1	vllm/model_executor/layers/mamba/short_conv.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
1	1	vllm/model_executor/layers/quantization/fp_quant.py
1	1	vllm/model_executor/layers/quantization/gguf.py
1	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
1	1	vllm/model_executor/layers/quantization/mxfp4.py
1	1	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	1	vllm/model_executor/layers/quantization/utils/mxfp6_utils.py
1	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
1	1	vllm/model_executor/layers/rotary_embedding/common.py
1	1	vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py
1	1	vllm/model_executor/layers/utils.py
1	1	vllm/model_executor/model_loader/base_loader.py
2	1	vllm/model_executor/model_loader/bitsandbytes_loader.py
1	1	vllm/model_executor/model_loader/gguf_loader.py
1	1	vllm/model_executor/model_loader/tensorizer_loader.py
1	1	vllm/model_executor/model_loader/tpu.py
0	10	vllm/model_executor/model_loader/utils.py
2	1	vllm/model_executor/models/config.py
1	1	vllm/model_executor/models/deepseek_v2.py
1	1	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/internvl.py
1	1	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/plamo2.py
1	1	vllm/model_executor/models/qwen3_next.py
1	1	vllm/model_executor/models/transformers/moe.py
4	2	vllm/model_executor/models/utils.py
1	1	vllm/model_executor/models/whisper.py
1	1	vllm/model_executor/utils.py
1	1	vllm/platforms/__init__.py
2	1	vllm/platforms/cuda.py
1	1	vllm/platforms/rocm.py
2	1	vllm/usage/usage_lib.py
1	577	vllm/utils/__init__.py
605	0	vllm/utils/torch_utils.py
2	1	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/attention/backends/rocm_aiter_fa.py
2	1	vllm/v1/kv_cache_interface.py
2	1	vllm/v1/sample/ops/penalties.py
5	3	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/tpu_worker.py
1	1	vllm/v1/worker/ubatching.py

[5c2acb270] Lukas Geiger 2025-10-18 [Models][QwenVL] Remove unnecessary `.contiguous()` calls (#27106)
1	1	vllm/model_executor/models/qwen2_5_vl.py
1	1	vllm/model_executor/models/qwen2_vl.py

[b26b70bec] Nicolò Lucchesi 2025-10-18 [Misc] Refactor `get_kv_cache_spec` into `AttentionLayerBase` (#26587)
53	5	vllm/attention/layer.py
13	0	vllm/attention/layers/chunked_local_attention.py
9	1	vllm/attention/layers/cross_attention.py
6	0	vllm/attention/layers/encoder_only_attention.py
11	0	vllm/model_executor/layers/attention_layer_base.py
29	0	vllm/model_executor/layers/mamba/abstract.py
1	1	vllm/model_executor/models/deepseek_v2.py
9	0	vllm/utils/__init__.py
1	1	vllm/v1/spec_decode/eagle.py
19	110	vllm/v1/worker/gpu_model_runner.py

[ab4be40fc] Fadi Arafeh 2025-10-18 [fix][cpu] fix prefill attention in CPU attention backend (#27035)
9	1	vllm/engine/arg_utils.py
7	3	vllm/v1/attention/backends/cpu_attn.py

[245e4f2c0] Wentao Ye 2025-10-18 [Feature] Batch Invariant: Support DeepGEMM and Blackwell (#27127)
8	8	tests/v1/generation/test_batch_invariance.py
9	9	tests/v1/generation/test_rms_norm_batch_invariant.py
54	4	vllm/model_executor/layers/quantization/fp8.py

[1d165d6d8] iAmir97 2025-10-18 [Chore] Separate out `vllm.utils.mem_utils` (#27143)
1	1	tests/basic_correctness/test_cumem.py
1	1	tests/kernels/attention/test_attention.py
1	1	tests/models/test_initialization.py
1	1	tests/utils.py
1	2	tests/utils_/test_utils.py
2	1	tests/v1/core/test_kv_cache_utils.py
1	1	tests/v1/tpu/worker/test_tpu_model_runner.py
2	1	tests/v1/worker/test_gpu_model_runner.py
1	1	tests/v1/worker/test_worker_memory_snapshot.py
2	1	vllm/config/cache.py
2	1	vllm/engine/arg_utils.py
1	1	vllm/multimodal/cache.py
1	1	vllm/platforms/cpu.py
0	234	vllm/utils/__init__.py
13	0	vllm/utils/mem_constants.py
232	0	vllm/utils/mem_utils.py
2	1	vllm/v1/core/kv_cache_utils.py
2	2	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/gpu_worker.py

[83004020f] dongbo910220 2025-10-18  [Test] Add test for /health endpoint on engine failure (#26074)
24	0	tests/entrypoints/openai/test_basic.py

[12e21701e] Chendi.Xue 2025-10-18 [DOC][FEATURES][CPU]update cpu feature for v1 (#27135)
1	1	docs/features/README.md

[30a33b92e] Varun Sundar Rabindranath 2025-10-18 [Misc] Rev DeepEP (#27122)
1	1	tools/ep_kernels/install_python_libraries.sh
1	1	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py

[7c572544e] Hanchenli 2025-10-17 [GPT-OSS] Structure_Tag support for gpt-oss tool-call in cot (#25515)
280	0	tests/entrypoints/openai/test_gptoss_structural_tags_integration.py
46	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
172	0	tests/v1/structured_output/test_gptoss_structural_tags.py
207	0	tests/v1/structured_output/test_reasoning_structured_output.py
2	0	vllm/config/structured_outputs.py
1	0	vllm/engine/arg_utils.py
21	5	vllm/entrypoints/openai/protocol.py
15	1	vllm/entrypoints/openai/serving_responses.py
12	0	vllm/reasoning/abs_reasoning_parsers.py
75	1	vllm/reasoning/gptoss_reasoning_parser.py
32	0	vllm/sampling_params.py
14	0	vllm/v1/structured_output/__init__.py
28	24	vllm/v1/structured_output/backend_xgrammar.py
6	1	vllm/v1/structured_output/request.py

[c31232076] Huamin Li 2025-10-17 [CI/Build] tests(v1): feed Triton attention the (num_blocks, 2, …) KV cache layout in backend-correctness tests (#26663)
3	2	tests/v1/attention/test_attention_backends.py

[c981f0ea7] ZiTian Zhao 2025-10-18 [Perf]  Add H100 fused MoE config (#25398)
147	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=768,device_name=NVIDIA_H100_PCIe,dtype=fp8_w8a8,block_shape=[128,128].json

[6367bde73] Lehua Ding 2025-10-18 [BugFix][Core] Fix error when enable async-scheduling in multi-node env (#25887)
9	7	vllm/engine/arg_utils.py

[f50cc221e] Wentao Ye 2025-10-17 [Test] Make `test_failure` more stable for batch invariance (#27054)
20	5	tests/v1/generation/test_batch_invariance.py

[acedc74b1] Pradyun92 2025-10-17 [V1][Spec Decode] Fix greedy temperature detection after sampler refactor (#27077)
1	1	vllm/v1/sample/rejection_sampler.py
2	0	vllm/v1/sample/tpu/metadata.py
7	1	vllm/v1/sample/tpu/sampler.py
10	2	vllm/v1/spec_decode/eagle.py
2	2	vllm/v1/worker/tpu_input_batch.py

[d29483b58] Zhuohan Li 2025-10-17 [Minor] Remove unnecessary error message (#27115)
8	27	vllm/attention/layer.py
11	28	vllm/model_executor/layers/linear.py

[950cf9e58] Michael Goin 2025-10-17 [Bugfix] Use PIECEWISE cudagraphs on Blackwell if max_model_len > 131072 (#27114)
37	15	vllm/config/vllm.py

[3125d7995] Isotr0py 2025-10-18 [Chore] Remove unused `PolyNorm` layer (#27110)
0	155	benchmarks/kernels/benchmark_polynorm.py
0	252	csrc/layernorm_kernels.cu
0	3	csrc/ops.h
0	6	csrc/torch_bindings.cpp
1	33	tests/kernels/core/test_layernorm.py
0	12	vllm/_custom_ops.py
0	63	vllm/model_executor/layers/layernorm.py

[e33ee23ee] vllmellm 2025-10-18 [Bugfix] [AITER] [ROCm] Fix Quark MoE Quant Config and AITER Fused MoE quant type logic (#27029)
2	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	1	vllm/model_executor/layers/quantization/quark/quark_moe.py
10	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[b10c64c83] rasmith 2025-10-17 [ROCm][Bugfix][Model] Fix illegal memory access when running qwen3_moe models with  rms_norm (Qwen3-235B-A22B,  Qwen3-30B-A3B, etc.) (#26192)
16	8	csrc/layernorm_kernels.cu

[0925b28a8] Aleksandr Malyshev 2025-10-17 [ROCM] MoE fp4 CK kernel (#26545)
5	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
68	24	vllm/model_executor/layers/quantization/quark/quark_moe.py

[99722d5f0] Nicolò Lucchesi 2025-10-17 [CI] Remove forbidden slash (#27112)
1	1	.buildkite/test-pipeline.yaml

[4c91a28e3] 燃 2025-10-18 [bugfix] Qwen3-VL fix video incorrect timestamp calculations while do_sample_frames=True (#27104)
2	2	vllm/model_executor/models/qwen3_vl.py

[b038d9c40] Patrick von Platen 2025-10-17 [Data-parallel] Allow DP>1 for world_size > num_gpus on node (8) (#26367)
1	0	docs/serving/data_parallel_deployment.md
9	2	vllm/engine/arg_utils.py
4	1	vllm/envs.py
82	19	vllm/v1/engine/utils.py

[2ba60ec7f] Nicolò Lucchesi 2025-10-17 [CI] Nixl integration tests (#27010)
11	0	.buildkite/test-pipeline.yaml
13	7	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
6	1	tests/v1/kv_connector/nixl_integration/test_accuracy.py
2	1	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
40	0	tests/v1/kv_connector/nixl_integration/tp_config_sweep_accuracy_test.sh

[bd7157a07] Luka Govedič 2025-10-17 [torch.compile] Enable attention and allreduce fusion without custom ops enabled (#24604)
30	12	.buildkite/test-pipeline.yaml
2	0	csrc/layernorm_kernels.cu
2	0	csrc/layernorm_quant_kernels.cu
4	0	csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
22	3	tests/compile/backend.py
59	73	tests/compile/test_full_graph.py
61	53	tests/compile/test_functionalization.py
86	35	tests/compile/test_fusion.py
156	80	tests/compile/test_fusion_all_reduce.py
58	46	tests/compile/test_fusion_attn.py
305	0	tests/compile/test_fusions_e2e.py
3	2	tests/compile/test_pass_manager.py
60	54	tests/compile/test_sequence_parallelism.py
101	3	tests/conftest.py
1	1	tests/kernels/quant_utils.py
46	0	tests/test_logger.py
22	1	tests/utils.py
23	1	tests/utils_/test_utils.py
1	1	vllm/_custom_ops.py
70	149	vllm/compilation/collective_fusion.py
68	129	vllm/compilation/fusion.py
37	18	vllm/compilation/fusion_attn.py
12	2	vllm/compilation/fx_utils.py
208	0	vllm/compilation/matcher_utils.py
1	0	vllm/compilation/monitor.py
20	17	vllm/compilation/pass_manager.py
2	1	vllm/compilation/vllm_inductor_pass.py
39	20	vllm/model_executor/layers/layernorm.py

[be429d0cf] Yongtao Huang 2025-10-17 Fix incorrect docstring for stop_profile() method (#27101)
1	1	vllm/engine/protocol.py

[c253745eb] Reima Karhila (AMD) 2025-10-17 [Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI350 and MI355 (#25586)
164	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=AMD_Instinct_MI350_OAM,dtype=fp8_w8a8.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=AMD_Instinct_MI350_OAM,dtype=fp8_w8a8.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=AMD_Instinct_MI355_OAM,dtype=fp8_w8a8.json

[daec4d262] Jee Jee Li 2025-10-17 [Model]Improve Qwen3VLMoeForConditionalGeneration packed_modules_mapping (#27096)
13	0	vllm/model_executor/models/qwen3_vl_moe.py

[6c9fdbf72] Harry Mellor 2025-10-17 [Docs] Replace `rst` style double-backtick with `md` single-backtick (#27091)
1	1	benchmarks/multi_turn/benchmark_serving_multi_turn.py
1	1	docs/models/extensions/fastsafetensor.md
3	3	tests/models/registry.py
1	1	tests/models/utils.py
1	1	tools/check_init_lazy_imports.py
1	1	vllm/assets/base.py
1	1	vllm/benchmarks/serve.py
2	2	vllm/compilation/decorators.py
3	3	vllm/config/pooler.py
1	1	vllm/distributed/kv_events.py
1	1	vllm/entrypoints/context.py
1	1	vllm/entrypoints/llm.py
11	11	vllm/entrypoints/renderer.py
1	1	vllm/inputs/data.py
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
5	5	vllm/model_executor/layers/quantization/utils/fp8_utils.py
3	3	vllm/model_executor/models/olmo.py
3	3	vllm/model_executor/models/olmo2.py
1	1	vllm/model_executor/models/ovis.py
16	16	vllm/model_executor/models/utils.py
1	1	vllm/model_executor/parameter.py
7	7	vllm/multimodal/processing.py
2	2	vllm/multimodal/registry.py
1	1	vllm/platforms/rocm.py
1	1	vllm/platforms/xpu.py
4	4	vllm/sampling_params.py
4	4	vllm/utils/deep_gemm.py
10	10	vllm/utils/flashinfer.py
7	7	vllm/v1/core/kv_cache_utils.py
1	1	vllm/v1/worker/cpu_worker.py
2	2	vllm/v1/worker/tpu_worker.py

[483ea6461] Harry Mellor 2025-10-17 [Docs] Replace all explicit anchors with real links (#27087)
0	1	.markdownlint.yaml
1	7	docs/api/README.md
1	1	docs/configuration/README.md
0	2	docs/configuration/optimization.md
2	6	docs/contributing/benchmarks.md
1	1	docs/contributing/model/README.md
1	1	docs/contributing/model/registration.md
0	2	docs/contributing/model/tests.md
0	4	docs/deployment/docker.md
0	2	docs/deployment/frameworks/anyscale.md
0	14	docs/deployment/nginx.md
1	1	docs/design/arch_overview.md
2	6	docs/design/mm_processing.md
1	1	docs/design/multiprocessing.md
3	5	docs/features/README.md
1	1	docs/features/multimodal_inputs.md
1	1	docs/getting_started/installation/google_tpu.md
6	8	docs/getting_started/installation/gpu.cuda.inc.md
0	2	docs/getting_started/installation/gpu.md
1	1	docs/getting_started/installation/gpu.rocm.inc.md
4	8	docs/getting_started/quickstart.md
2	2	docs/models/generative_models.md
4	4	docs/models/pooling_models.md
3	13	docs/models/supported_models.md
1	1	docs/serving/distributed_troubleshooting.md
1	1	docs/serving/offline_inference.md
17	39	docs/serving/openai_compatible_server.md
0	4	docs/usage/troubleshooting.md

[e20eba753] Mengqing Cao 2025-10-17 [VLM][Refactor] Remove useless func `get_input_positions` in `MRotaryEmbedding` (#27088)
0	34	vllm/model_executor/layers/rotary_embedding/mrope.py

[bbc1b2966] cong-meta 2025-10-17 Update troubleshooting.md and remind VLLM_TRACE_FUNCTION usage (#27069)
1	1	docs/usage/troubleshooting.md

[acb1bfa60] Chauncey 2025-10-17 [CI] fix docs build failed (#27082)
1	1	docs/models/supported_models.md

[75c7ad991] zhrrr 2025-10-17 [Kernel][Performance] Fuse float cast and renormalize to topk softmax kernel  (#26717)
1	1	csrc/moe/moe_ops.h
216	87	csrc/moe/topk_softmax_kernels.cu
1	1	csrc/moe/torch_bindings.cpp
2	1	vllm/_custom_ops.py
2	5	vllm/model_executor/layers/fused_moe/fused_moe.py

[5550ff9c2] Li, Jiang 2025-10-17 [CI/Build] Update compressed tensor test path to fix CPU CI (#27068)
1	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh

[3aeb19a39] Said Taghadouini 2025-10-17 [Model] Add support for LightOnOCR (#26916)
1	0	docs/models/supported_models.md
21	0	examples/offline_inference/vision_language.py
4	0	tests/models/registry.py
195	0	vllm/model_executor/models/lightonocr.py
4	0	vllm/model_executor/models/registry.py

[8c017b349] Cyrus Leung 2025-10-17 [Model] Always use Transformers backend for PaliGemma and Gemma3-MM (#26715)
2	2	docs/models/hardware_supported_models/tpu.md
3	20	docs/models/supported_models.md
2	1	examples/offline_inference/vision_language.py
5	11	tests/models/language/generation/test_gemma.py
34	40	tests/models/multimodal/generation/test_common.py
0	10	tests/models/multimodal/generation/vlm_utils/model_utils.py
0	4	tests/models/multimodal/processing/test_common.py
0	1	tests/models/multimodal/processing/test_tensor_schema.py
0	710	vllm/model_executor/models/gemma3_mm.py
0	412	vllm/model_executor/models/paligemma.py
8	5	vllm/model_executor/models/registry.py
0	3	vllm/platforms/rocm.py

[9c2c2287a] Zhewen Li 2025-10-16 [CI/Build] Update Llama4 eval yaml (#27070)
3	2	.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8-MM.yaml
1	2	.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.yaml

[fec2b341a] Jee Jee Li 2025-10-17 [Kernel] Lazy import FlashInfer (#26977)
9	8	tests/v1/sample/test_topk_topp_sampler.py
16	30	vllm/v1/sample/ops/topk_topp_sampler.py

[87bc0c492] Jee Jee Li 2025-10-17 [Bugfix] Fix ReplicatedLinearWithLoRA  (#27065)
12	0	vllm/lora/layers/replicated_linear.py

[fe3b9372a] Nick Hill 2025-10-16 [Core] Change `execute_model_with_error_logging()` to be a ctx manager (#27060)
11	15	vllm/v1/engine/core.py

[bde9e2272] Tao He 2025-10-17 [Bugfix][Qwen] fixes the weights dtype in qwen3_next: it is actually a bfloat16 (#27030)
0	1	vllm/model_executor/models/qwen3_next.py

[08405609c] Boyuan Feng 2025-10-16 disable graph partition in custom op (#26952)
6	1	vllm/model_executor/layers/fused_moe/fused_moe.py
9	0	vllm/model_executor/utils.py

[ab81379ea] Nick Hill 2025-10-16 [Perf] Exploit out-of-band buffers in shm_broadcast (#26961)
54	16	vllm/distributed/device_communicators/shm_broadcast.py

[4ffd6e894] Harry Mellor 2025-10-17 [Docs] Reduce custom syntax used in docs (#27009)
1	1	docs/configuration/conserving_memory.md
8	8	docs/configuration/optimization.md
1	1	docs/configuration/tpu.md
5	5	docs/contributing/README.md
3	3	docs/contributing/benchmarks.md
2	2	docs/contributing/ci/failures.md
4	4	docs/contributing/ci/update_pytorch_version.md
1	1	docs/contributing/dockerfile/dockerfile.md
10	10	docs/contributing/model/basic.md
10	10	docs/contributing/model/multimodal.md
2	2	docs/contributing/model/registration.md
7	7	docs/contributing/model/tests.md
4	4	docs/contributing/model/transcription.md
1	1	docs/contributing/profiling.md
2	2	docs/deployment/docker.md
1	1	docs/deployment/frameworks/anyscale.md
2	2	docs/deployment/frameworks/retrieval_augmented_generation.md
1	1	docs/deployment/frameworks/streamlit.md
6	6	docs/design/arch_overview.md
2	2	docs/design/cuda_graphs.md
8	8	docs/design/fused_moe_modular_kernel.md
3	3	docs/design/io_processor_plugins.md
38	38	docs/design/metrics.md
1	1	docs/design/mm_processing.md
1	1	docs/design/multiprocessing.md
2	2	docs/design/torch_compile.md
12	12	docs/features/README.md
1	1	docs/features/automatic_prefix_caching.md
6	6	docs/features/disagg_prefill.md
1	1	docs/features/lora.md
11	11	docs/features/multimodal_inputs.md
4	4	docs/features/nixl_connector_usage.md
2	2	docs/features/prompt_embeds.md
1	1	docs/features/quantization/README.md
3	3	docs/features/reasoning_outputs.md
7	7	docs/features/spec_decode.md
1	1	docs/features/structured_outputs.md
19	19	docs/features/tool_calling.md
0	0	docs/getting_started/installation/{cpu/apple.inc.md => cpu.apple.inc.md}
0	0	docs/getting_started/installation/{cpu/arm.inc.md => cpu.arm.inc.md}
17	17	docs/getting_started/installation/cpu.md
0	0	docs/getting_started/installation/{cpu/s390x.inc.md => cpu.s390x.inc.md}
0	0	docs/getting_started/installation/{cpu/x86.inc.md => cpu.x86.inc.md}
1	1	docs/getting_started/installation/google_tpu.md
1	1	docs/getting_started/installation/{gpu/cuda.inc.md => gpu.cuda.inc.md}
24	24	docs/getting_started/installation/gpu.md
4	4	docs/getting_started/installation/{gpu/rocm.inc.md => gpu.rocm.inc.md}
1	1	docs/getting_started/installation/{gpu/xpu.inc.md => gpu.xpu.inc.md}
3	3	docs/getting_started/quickstart.md
9	2	docs/mkdocs/hooks/generate_examples.py
71	99	docs/mkdocs/hooks/url_schemes.py
1	1	docs/models/extensions/runai_model_streamer.md
2	2	docs/models/generative_models.md
7	7	docs/models/pooling_models.md
6	6	docs/models/supported_models.md
1	1	docs/serving/data_parallel_deployment.md
2	2	docs/serving/distributed_troubleshooting.md
3	3	docs/serving/expert_parallel_deployment.md
15	15	docs/serving/openai_compatible_server.md
2	2	docs/serving/parallelism_scaling.md
2	2	docs/usage/reproducibility.md
5	5	docs/usage/troubleshooting.md
1	1	docs/usage/usage_stats.md
9	9	docs/usage/v1_guide.md
1	1	examples/offline_inference/pooling/README.md

[965c5f491] Tomas Ruiz 2025-10-17 vllm bench serve shows num of failed requests (#26478)
10	0	vllm/benchmarks/serve.py

[4d055ef46] Lukas Geiger 2025-10-17 Remove unused imports (#26972)
0	4	vllm/model_executor/layers/mamba/linear_attn.py
0	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
0	1	vllm/model_executor/models/gemma3n_mm.py
0	1	vllm/model_executor/models/minimax_text_01.py
0	2	vllm/multimodal/utils.py
0	2	vllm/transformers_utils/tokenizer.py
0	1	vllm/utils/__init__.py
0	1	vllm/v1/worker/tpu_worker.py

[17c540a99] Boyuan Feng 2025-10-16 [torch.compile] fix simple inductor graph partition test (#27050)
2	4	tests/compile/piecewise/test_simple.py

[4d4d6bad1] Cyrus Leung 2025-10-17 [Chore] Separate out `vllm.utils.importlib` (#27022)
1	1	tests/model_executor/model_loader/tensorizer_loader/test_tensorizer.py
46	0	tests/utils_/test_import_utils.py
0	41	tests/utils_/test_utils.py
1	1	tests/v1/attention/utils.py
1	1	vllm/assets/audio.py
1	1	vllm/assets/video.py
1	1	vllm/attention/backends/registry.py
2	1	vllm/attention/selector.py
1	1	vllm/benchmarks/datasets.py
2	1	vllm/compilation/backends.py
2	1	vllm/compilation/decorators.py
2	1	vllm/config/compilation.py
2	1	vllm/config/model.py
1	1	vllm/config/speculative.py
1	1	vllm/distributed/parallel_state.py
2	1	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/speech_to_text.py
1	1	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
1	1	vllm/lora/punica_wrapper/punica_selector.py
1	1	vllm/model_executor/layers/pooler.py
2	1	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/model_executor/model_loader/weight_utils.py
1	1	vllm/multimodal/audio.py
1	1	vllm/multimodal/inputs.py
1	1	vllm/multimodal/parse.py
2	1	vllm/platforms/__init__.py
1	1	vllm/plugins/io_processors/__init__.py
1	1	vllm/reasoning/abs_reasoning_parsers.py
1	1	vllm/transformers_utils/runai_utils.py
1	1	vllm/transformers_utils/s3_utils.py
0	313	vllm/utils/__init__.py
326	0	vllm/utils/import_utils.py
1	1	vllm/v1/engine/core.py
1	1	vllm/v1/executor/abstract.py
1	1	vllm/v1/structured_output/__init__.py
1	1	vllm/v1/structured_output/backend_guidance.py
1	1	vllm/v1/structured_output/backend_lm_format_enforcer.py
1	1	vllm/v1/structured_output/backend_outlines.py
1	1	vllm/v1/structured_output/backend_xgrammar.py
1	1	vllm/v1/structured_output/utils.py
1	1	vllm/v1/worker/worker_base.py

[11ae016bd] Lucia Fang 2025-10-16 [torch.compile] Passing only necessary compilation config to inductor pass config (#27041)
9	0	tests/compile/test_async_tp.py
17	0	tests/compile/test_config.py
8	0	tests/compile/test_sequence_parallelism.py
13	2	vllm/compilation/vllm_inductor_pass.py

[41d307191] jiahanc 2025-10-16 [NVIDIA] [Perf] Update to leverage flashinfer trtllm FP4 MOE throughput kernel (#26714)
2	2	docker/Dockerfile
2	2	docker/Dockerfile.nightly_torch
1	1	requirements/cuda.txt
7	7	tests/kernels/moe/test_ocp_mx_moe.py
1	28	vllm/model_executor/layers/fused_moe/trtllm_moe.py
4	17	vllm/model_executor/layers/quantization/modelopt.py
8	39	vllm/model_executor/layers/quantization/mxfp4.py

[fb5e10d3f] Harry Mellor 2025-10-16 Refactor Transformers backend to use mixins (#26906)
1	1	.github/CODEOWNERS
6	2	tests/models/registry.py
1	1	tests/models/test_initialization.py
1	5	tests/models/test_transformers.py
14	11	vllm/config/model.py
1	1	vllm/model_executor/models/deepseek_vl2.py
26	14	vllm/model_executor/models/registry.py
0	961	vllm/model_executor/models/transformers.py
127	0	vllm/model_executor/models/transformers/__init__.py
435	0	vllm/model_executor/models/transformers/base.py
66	0	vllm/model_executor/models/transformers/causal.py
97	0	vllm/model_executor/models/transformers/legacy.py
14	37	vllm/model_executor/models/{transformers_moe.py => transformers/moe.py}
396	0	vllm/model_executor/models/transformers/multimodal.py
118	0	vllm/model_executor/models/transformers/pooling.py
207	0	vllm/model_executor/models/transformers/utils.py
0	215	vllm/model_executor/models/transformers_pooling.py

[b2f78cbad] Bram Wasti 2025-10-16 [small][batch invariance] Rename the env and internal flags to simplify usage (#26855)
4	4	csrc/core/batch_invariant.hpp
2	2	csrc/layernorm_kernels.cu
1	1	csrc/layernorm_quant_kernels.cu
1	1	tests/v1/e2e/test_async_sched_and_preempt.py
12	12	tests/v1/generation/test_batch_invariance.py
2	2	vllm/config/model.py
2	2	vllm/config/parallel.py
2	2	vllm/distributed/device_communicators/all_reduce_utils.py
2	2	vllm/distributed/device_communicators/symm_mem.py
3	3	vllm/model_executor/layers/batch_invariant.py
5	5	vllm/model_executor/layers/fused_moe/fused_moe.py
3	3	vllm/model_executor/layers/layernorm.py
3	3	vllm/model_executor/layers/quantization/fp8.py
5	5	vllm/v1/attention/backends/flash_attn.py
3	3	vllm/v1/attention/backends/flashinfer.py
2	2	vllm/v1/attention/backends/flex_attention.py
2	2	vllm/v1/attention/backends/mla/common.py
3	3	vllm/v1/attention/backends/mla/flashattn_mla.py
2	2	vllm/v1/attention/backends/mla/flashmla.py
2	2	vllm/v1/attention/backends/mla/triton_mla.py

[23583ee28] Wentao Ye 2025-10-16 [Bug] Add Assertion for `random-input-len` / `random-output-len` (#26834)
16	0	vllm/benchmarks/datasets.py

[01c977e96] Michael Goin 2025-10-16 [CI] Prune Quantization Tests and skip compilation (#27038)
1	1	tests/quantization/test_auto_round.py
29	76	tests/quantization/test_compressed_tensors.py
0	35	tests/quantization/test_cpu_offload.py
0	3	tests/quantization/test_fp8.py
3	1	tests/quantization/test_gptq_dynamic.py
3	1	tests/quantization/test_lm_head.py
9	6	tests/quantization/test_quark.py
3	2	tests/quantization/test_rtn.py
14	9	tests/quantization/test_torchao.py

[b3dda72c2] Wentao Ye 2025-10-16 [Feature] Migrate DeepGEMM API from `get_m_alignment_for_contiguous_layout` to `get_mk_alignment_for_contiguous_layout` (#26935)
5	6	tests/kernels/moe/test_block_fp8.py
6	3	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
2	2	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
10	8	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
2	13	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py
5	3	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
11	10	vllm/model_executor/warmup/deep_gemm_warmup.py
16	1	vllm/utils/deep_gemm.py

[fb0571b07] Varun Sundar Rabindranath 2025-10-16 [GPTOSS][DP/EP][Marlin] Enable GPTOSS Batched DP/EP using Marlin kernels (#25997)
92	0	csrc/moe/moe_align_sum_kernels.cu
8	0	csrc/moe/moe_ops.h
11	0	csrc/moe/torch_bindings.cpp
5	5	docs/design/moe_kernel_features.md
360	202	tests/kernels/moe/test_moe.py
94	0	tests/kernels/moe/test_moe_align_block_size.py
18	0	vllm/_custom_ops.py
25	1	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
432	103	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
5	0	vllm/model_executor/layers/fused_moe/layer.py
89	0	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
14	3	vllm/model_executor/layers/quantization/mxfp4.py

[2ed8b6b3d] Wentao Ye 2025-10-16 [Bug] Fix batch invariant test `has` to `is` (#27032)
10	32	tests/v1/generation/test_batch_invariance.py
11	42	tests/v1/generation/test_rms_norm_batch_invariant.py

[013abde6e] kimbochen 2025-10-16 Adding Warmup to Benchmark Serving (#26943)
36	1	vllm/benchmarks/serve.py

[a5464dcf9] Kyle Sayers 2025-10-16 [Compressed Tensors] Always clone output for compile robustness (#26849)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py

[ac3ed5a81] Mandy Li 2025-10-16 Support block size of 256 used by Intel HPU (#26883)
1	1	vllm/config/cache.py

[e6ba2000a] Andrew Xia 2025-10-16 [gpt-oss][1/N] EZ: refactor serving_responses for modularity (#26948)
70	54	vllm/entrypoints/openai/serving_responses.py

[aa255ff55] Harry Mellor 2025-10-16 Support `set` in the CLI generation (#27031)
5	0	tests/engine/test_arg_utils.py
29	19	vllm/engine/arg_utils.py

[7bb736d00] ZiTian Zhao 2025-10-17 Fix Qwen2.5 VL image grid docstring (#27033)
1	1	vllm/model_executor/models/qwen2_5_vl.py

[9f4e30904] Jee Jee Li 2025-10-17 [Model] Fix Qwen3VL mm mapping (#27027)
2	2	vllm/model_executor/models/qwen3_vl.py

[5afd3276d] rongfu.leng 2025-10-16 [Feature] Add process_weights_after_loading to AttentionImpl (#26870)
3	0	vllm/attention/backends/abstract.py
1	10	vllm/attention/layer.py
5	0	vllm/v1/attention/backends/flashinfer.py

[43721bc67] Tahsin Tunan 2025-10-16 [CI] Replace large models with tiny alternatives in tests (#24057)
8	8	tests/basic_correctness/test_basic_correctness.py
1	1	tests/basic_correctness/test_cpu_offload.py
2	2	tests/basic_correctness/test_cumem.py
2	2	tests/distributed/test_sequence_parallel.py
4	1	tests/entrypoints/llm/test_collective_rpc.py
1	1	tests/entrypoints/openai/test_run_batch.py
1	1	tests/entrypoints/openai/test_serving_models.py
80	24	tests/entrypoints/openai/test_shutdown.py
1	0	tests/models/registry.py
4	6	tests/samplers/test_no_bad_words.py
1	1	tests/v1/core/test_scheduler_e2e.py
4	2	tests/v1/engine/test_engine_core.py
1	1	tests/v1/entrypoints/openai/test_multi_api_servers.py
2	5	tests/v1/sample/test_sampling_params_e2e.py
1	1	tests/v1/shutdown/test_delete.py
1	1	tests/v1/shutdown/test_forward_error.py
5	3	tests/v1/shutdown/test_startup_error.py

[02d709a6f] Kay Yan 2025-10-16 [docs] standardize Hugging Face env var to `HF_TOKEN` (deprecates `HUGGING_FACE_HUB_TOKEN`) (#27020)
3	3	docs/deployment/docker.md
2	2	docs/deployment/frameworks/lws.md
3	3	docs/deployment/k8s.md

[4a510ab48] Mark McLoughlin 2025-10-16 [NIXL] Improve request_finished() debug logs (#25665)
9	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[314fa8abb] Matthew Bonanni 2025-10-16 [Attention] Tune CUTLASS MLA num_splits (#26846)
21	16	csrc/attention/mla/cutlass_sm100_mla/device/sm100_mla.hpp

[334535b6f] Cyrus Leung 2025-10-16 [Benchmark] Show E2EL by default for pooling models (#27014)
16	5	vllm/benchmarks/serve.py

[dcbb3f187] bogdanm 2025-10-16 [Bugfix] Correct LayerNorm epsilon parameter in modernbert.py (#27008)
5	2	vllm/model_executor/models/modernbert.py

[00417f4e4] Sungjae Lee 2025-10-16 [MISC] fix import violations for re and triton modules (#26654)
2	2	vllm/model_executor/layers/quantization/qutlass_utils.py

[ed344f411] Lukas Geiger 2025-10-16 Cleanup code after Python 3.10 upgrade (#26520)
2	7	benchmarks/benchmark_serving_structured_output.py
0	1	requirements/common.txt

[e51928793] CSWYF3634076 2025-10-16 [Model][Bugfix] fix ernie45 vl run failed from shared experts optimization (#26885)
22	5	vllm/model_executor/models/ernie45_vl_moe.py

[d2740fafb] Cyrus Leung 2025-10-16 [Chore] Separate out `vllm.utils.collections` (#26990)
2	1	tests/conftest.py
1	1	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	1	tests/models/multimodal/processing/test_tensor_schema.py
31	0	tests/utils_/test_collections.py
0	25	tests/utils_/test_utils.py
2	1	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_embedding.py
2	1	vllm/entrypoints/openai/serving_engine.py
2	1	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
1	1	vllm/inputs/parse.py
1	1	vllm/model_executor/layers/activation.py
1	1	vllm/model_executor/layers/quantization/gptq.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
1	1	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/llava_next_video.py
1	1	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/qwen3_vl.py
2	1	vllm/multimodal/inputs.py
7	4	vllm/multimodal/parse.py
6	3	vllm/multimodal/processing.py
1	1	vllm/multimodal/registry.py
2	1	vllm/reasoning/abs_reasoning_parsers.py
3	129	vllm/utils/__init__.py
139	0	vllm/utils/collections.py
2	1	vllm/v1/engine/async_llm.py
2	1	vllm/v1/worker/gpu_input_batch.py
2	1	vllm/v1/worker/tpu_input_batch.py

[17838e50e] Cyrus Leung 2025-10-16 [Benchmark] Use truncation by default for pooling benchmarks (#26992)
9	7	vllm/benchmarks/lib/endpoint_request_func.py

[44c855562] Zhewen Li 2025-10-16 [CI/Build] Fix AMD import failures in CI (#26841)
1	1	docker/Dockerfile.rocm
2	0	requirements/rocm-test.txt

[f7d318de2] Akash kaothalkar 2025-10-16 [Hardware][CPU][PowerPC]Disable torch.compile() in toptopk sampling (#26987)
5	1	vllm/v1/sample/ops/topk_topp_sampler.py

[76f0d05bc] Cyrus Leung 2025-10-16 [CI/Build] Update expected beam search output for Phi3V (#26978)
1	1	tests/entrypoints/openai/test_vision.py

[7d8975de8] Bram Wasti 2025-10-15 Deepseek-v3 Batch Invariant on 8xH100 (#26609)
769	73	tests/v1/generation/test_batch_invariance.py
346	0	tests/v1/generation/test_rms_norm_batch_invariant.py
3	1	vllm/compilation/caching.py
7	0	vllm/config/model.py
7	1	vllm/config/parallel.py
6	0	vllm/distributed/device_communicators/all_reduce_utils.py
5	0	vllm/distributed/device_communicators/symm_mem.py
1	1	vllm/engine/arg_utils.py
240	13	vllm/model_executor/layers/batch_invariant.py
28	4	vllm/model_executor/layers/fused_moe/fused_moe.py
10	0	vllm/model_executor/layers/layernorm.py
1	0	vllm/model_executor/layers/mla.py
65	0	vllm/model_executor/layers/quantization/fp8.py
1	0	vllm/model_executor/models/gpt_oss.py
12	1	vllm/v1/attention/backends/flash_attn.py
10	1	vllm/v1/attention/backends/mla/common.py
11	1	vllm/v1/attention/backends/mla/flashattn_mla.py
36	2	vllm/v1/attention/backends/mla/flashmla.py
6	1	vllm/v1/attention/backends/mla/triton_mla.py
0	3	vllm/v1/worker/gpu_model_runner.py
3	0	vllm/v1/worker/gpu_worker.py

[785d8b641] Vadim Gimpelson 2025-10-16 [PERF] Qwen3-next MTP speedup (change bool mask indexing to index_select / index_copy to reduce d2h) (#26437)
1	1	vllm/model_executor/layers/fla/ops/utils.py
12	12	vllm/model_executor/models/qwen3_next.py
43	23	vllm/v1/attention/backends/gdn_attn.py

[f6cdc9a02] Cyrus Leung 2025-10-16 [Chore] Rename `utils` submodules (#26920)
1	1	tests/lora/test_add_lora.py
1	1	tests/models/multimodal/generation/test_common.py
1	1	tests/utils_/test_async_utils.py
1	1	tests/utils_/test_func_utils.py
1	1	vllm/benchmarks/throughput.py
1	1	vllm/entrypoints/chat_utils.py
1	1	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_embedding.py
1	1	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_pooling.py
1	1	vllm/entrypoints/openai/serving_score.py
1	1	vllm/entrypoints/renderer.py
1	1	vllm/executor/executor_base.py
1	1	vllm/executor/ray_distributed_executor.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/interfaces_base.py
1	1	vllm/multimodal/processing.py
1	1	vllm/tracing.py
1	1	vllm/transformers_utils/processor.py
5	1	vllm/utils/{async_utils.py => asyncio.py}
0	0	vllm/utils/{func.py => functools.py}
2	2	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/core_client.py

[509cdc037] Chendi.Xue 2025-10-15 [DOC][XPU]update feature parity with Intel GPU (#26954)
17	17	docs/features/README.md

[9b6504c30] Richard Zou 2025-10-15 [BugFix] Work around graph partition x torch.compile cache issue (#26956)
1	11	tests/compile/piecewise/test_toy_llama.py
30	1	vllm/compilation/pass_manager.py

[e19b16dde] Angela Yi 2025-10-15 [bugfix] Fix SP + PP without specifying compile size (#26955)
9	0	tests/distributed/test_sequence_parallel.py
12	3	vllm/v1/worker/utils.py

[582f2c6be] ahao-anyscale 2025-10-15 [BUG] Allow runai_streamer_sharded in config check (#26958)
5	2	vllm/config/vllm.py

[f8a0acbdb] Michael Goin 2025-10-15 [CI] Enable Blackwell Llama4 MoE tests (#26731)
1	1	.buildkite/test-pipeline.yaml
35	21	tests/quantization/test_blackwell_moe.py

[131703437] kliuae 2025-10-16 [ROCm][FEAT] Fuse DeepSeek shared experts into AITER fused_moe ops (#24097)
3	3	tests/distributed/test_expert_placement.py
1	1	tests/kernels/moe/test_moe_permute_unpermute.py
7	0	vllm/envs.py
105	11	vllm/model_executor/layers/fused_moe/layer.py
93	6	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	0	vllm/model_executor/layers/quantization/fp8.py
141	67	vllm/model_executor/models/deepseek_v2.py

[0ecc553ee] InChang Jeong 2025-10-16 [Bugfix] reasoning_parser parameter handling in run_batch.py (#26225)
50	0	tests/entrypoints/openai/test_run_batch.py
17	0	vllm/entrypoints/openai/run_batch.py

[f96bc3649] felixzhu555 2025-10-15 [Qwen3-Next] Add tuned MoE config for Qwen3-Next FP8 on H100 tp2 (#26887)
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json

[938c43ea7] Alexei-V-Ivanov-AMD 2025-10-15 [ci] Adjusting AMD test composition 2025-10-14 (#26852)
6	5	.buildkite/test-amd.yaml

[0a9ef0cfc] Adrian Abeyta 2025-10-15 Move query quantization to attention layer for Flashinfer & Triton. (#26534)
3	1	tests/compile/test_fusion_attn.py
16	8	vllm/attention/backends/abstract.py
6	3	vllm/attention/layer.py
3	1	vllm/v1/attention/backends/flash_attn.py
12	10	vllm/v1/attention/backends/flashinfer.py
3	15	vllm/v1/attention/backends/triton_attn.py

[e5b438a24] Wentao Ye 2025-10-15 [Bug] Temporally Disable `VLLM_ALLREDUCE_USE_SYMM_MEM` by Default (#26925)
2	2	vllm/envs.py

[0b99f5d30] XiaobingZhang 2025-10-16 support flashinfer_fp4 moe for 5090 gpu (#26669)
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[1f491aa0c] Benji Beck 2025-10-15 Vectorize RMS norm variance using vectorize_read_with_alignment (#26234)
15	3	csrc/layernorm_kernels.cu
16	3	csrc/layernorm_quant_kernels.cu

[de92d916f] Kaixi Hou 2025-10-15 [NVIDIA] Add support for cudnn fp4 gemm via flashinfer (#26107)
11	6	vllm/envs.py
25	15	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
21	17	vllm/model_executor/layers/quantization/modelopt.py

[a1063628a] Woosuk Kwon 2025-10-15 [Chore] Clean up CODEOWNERS (#26923)
0	3	.github/CODEOWNERS

[d79637525] XiaobingZhang 2025-10-16 [ModelOpt] Remove NVFP4 MoE K%16==0 constraint (#26891)
0	12	vllm/model_executor/layers/quantization/modelopt.py

[14f845634] Sam/Samuel 2025-10-16 [Feature]: Use pydantic validation in observability.py config (#26637)
40	20	vllm/config/observability.py

[4794c2bd9] Pradeep Dasigi 2025-10-15 Olmo 3 tool parser and tests (#26143)
10	0	docs/features/tool_calling.md
243	0	tests/entrypoints/openai/tool_parsers/test_olmo3_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
368	0	vllm/entrypoints/openai/tool_parsers/olmo3_tool_parser.py

[d3cbaa08d] Harry Mellor 2025-10-15 Lower sevarity of log when model info cache misses due to exception (#26917)
1	1	vllm/model_executor/models/registry.py

[828523ad8] Cyrus Leung 2025-10-15 [Chore] Separate out `vllm.utils.async_utils` (#26913)
1	1	tests/lora/test_add_lora.py
42	0	tests/utils_/test_async_utils.py
0	36	tests/utils_/test_utils.py
1	1	vllm/benchmarks/throughput.py
2	1	vllm/entrypoints/openai/serving_completion.py
1	2	vllm/entrypoints/openai/serving_embedding.py
3	4	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_pooling.py
1	2	vllm/entrypoints/openai/serving_score.py
1	1	vllm/entrypoints/renderer.py
1	1	vllm/executor/executor_base.py
7	2	vllm/executor/ray_distributed_executor.py
0	277	vllm/utils/__init__.py
299	0	vllm/utils/async_utils.py
1	23	vllm/utils/func.py
2	1	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/core_client.py

[136a17fe6] Cyrus Leung 2025-10-15 [Chore] Separate out `vllm.utils.func` (#26904)
1	1	tests/models/multimodal/generation/test_common.py
97	0	tests/utils_/test_func_utils.py
32	0	tests/utils_/test_jsontree.py
1	121	tests/utils_/test_utils.py
2	1	vllm/entrypoints/chat_utils.py
1	1	vllm/entrypoints/openai/serving_engine.py
2	1	vllm/entrypoints/openai/serving_score.py
1	1	vllm/executor/executor_base.py
1	1	vllm/executor/ray_distributed_executor.py
2	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/interfaces_base.py
2	1	vllm/multimodal/processing.py
1	1	vllm/tracing.py
1	1	vllm/transformers_utils/processor.py
1	237	vllm/utils/__init__.py
258	0	vllm/utils/func.py
2	1	vllm/v1/engine/async_llm.py

[f57438338] Boyuan Feng 2025-10-15 [BugFix] Patch inductor memory plan logic (#26878)
8	1	docs/mkdocs/hooks/generate_argparse.py
3	3	tests/compile/piecewise/test_multiple_graphs.py
70	2	vllm/env_override.py
27	0	vllm/utils/__init__.py

[5d598680e] Max Wittig 2025-10-15 chore: remove unused marker (#26890)
0	1	pyproject.toml

[8f4b313c3] wangxiyuan 2025-10-15 [Misc] rename torch_dtype to dtype (#26695)
1	1	benchmarks/kernels/benchmark_moe.py
1	1	benchmarks/kernels/benchmark_moe_permute_unpermute.py
1	1	docs/features/quantization/auto_round.md
1	1	docs/features/quantization/fp8.md
1	1	docs/features/quantization/int4.md
1	1	docs/features/quantization/int8.md
1	1	docs/features/quantization/quantized_kvcache.md
1	1	docs/features/quantization/quark.md
1	1	docs/features/quantization/torchao.md
1	1	requirements/common.txt
4	4	tests/conftest.py
1	1	tests/models/multimodal/pooling/test_intern_vit.py
1	1	tests/models/multimodal/pooling/test_radio.py
1	1	vllm/benchmarks/throughput.py
7	7	vllm/config/model.py
2	3	vllm/entrypoints/llm.py
1	1	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/model_executor/models/chameleon.py
1	1	vllm/model_executor/models/ernie45_vl.py
1	1	vllm/model_executor/models/glm4v.py
2	2	vllm/model_executor/models/longcat_flash.py
2	2	vllm/model_executor/models/nano_nemotron_vl.py
3	3	vllm/model_executor/models/qwen3_next.py
1	1	vllm/model_executor/models/transformers.py
1	1	vllm/model_executor/models/transformers_pooling.py
2	2	vllm/platforms/cuda.py
1	1	vllm/platforms/interface.py
2	2	vllm/platforms/rocm.py
2	2	vllm/platforms/xpu.py
6	8	vllm/utils/__init__.py

[f93e34801] Cyrus Leung 2025-10-15 [Misc] Remove `isort` and `yapf` ignores (#26888)
1	2	tests/v1/engine/conftest.py
0	3	tests/v1/tpu/test_topk_topp_sampler.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
0	5	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[f54f85129] wang.yuqi 2025-10-15 [Model][2/N] Improve all pooling task | Support multi-vector retrieval (#25370)
6	0	examples/offline_inference/pooling/README.md
56	0	examples/offline_inference/pooling/multi_vector_retrieval.py
1	1	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
6	0	examples/online_serving/pooling/README.md
54	0	examples/online_serving/pooling/multi_vector_retrieval_client.py
6	2	tests/conftest.py
1	1	tests/entrypoints/pooling/llm/test_classify.py
7	0	tests/entrypoints/pooling/llm/test_embedding.py
8	4	tests/entrypoints/pooling/llm/test_encode.py
12	11	tests/entrypoints/pooling/llm/test_reward.py
18	0	tests/entrypoints/pooling/openai/test_embedding.py
18	1	tests/entrypoints/pooling/openai/test_rerank.py
45	0	tests/models/language/pooling/test_multi_vector_retrieval.py
50	8	tests/models/language/pooling/test_pooler_config_init_behaviour.py
2	2	tests/models/language/pooling/test_token_classification.py
1	1	tests/models/multimodal/pooling/test_prithvi_mae.py
1	1	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
2	3	tests/plugins_tests/test_io_processor_plugins.py
71	22	tests/test_pooling_params.py
18	19	vllm/entrypoints/llm.py
12	9	vllm/entrypoints/openai/api_server.py
2	2	vllm/entrypoints/openai/protocol.py
13	1	vllm/entrypoints/openai/serving_pooling.py
251	171	vllm/model_executor/layers/pooler.py
15	27	vllm/model_executor/models/adapters.py
10	12	vllm/model_executor/models/bert.py
5	9	vllm/model_executor/models/bert_with_rope.py
1	1	vllm/model_executor/models/clip.py
9	2	vllm/model_executor/models/gpt2.py
1	1	vllm/model_executor/models/gritlm.py
1	1	vllm/model_executor/models/internlm2.py
7	3	vllm/model_executor/models/jamba.py
9	3	vllm/model_executor/models/jina_vl.py
8	12	vllm/model_executor/models/modernbert.py
4	2	vllm/model_executor/models/qwen2_rm.py
6	20	vllm/model_executor/models/roberta.py
1	1	vllm/model_executor/models/terratorch.py
6	12	vllm/model_executor/models/transformers_pooling.py
34	27	vllm/pooling_params.py
1	1	vllm/tasks.py
7	6	vllm/v1/worker/gpu_model_runner.py

[d4d1a6024] li2haipeng 2025-10-15 [Lora]Load tuned multi-lora kernel configs from json files (#26319)
51	0	vllm/lora/ops/triton_ops/README_TUNING.md
16	7	vllm/lora/ops/triton_ops/lora_expand_op.py
16	9	vllm/lora/ops/triton_ops/lora_shrink_op.py
115	0	vllm/lora/ops/triton_ops/utils.py

[db1764e4e] wangxiyuan 2025-10-15 [Platform] allow platform to init dp group (#22243)
1	1	vllm/config/parallel.py
13	13	vllm/distributed/utils.py
0	34	vllm/platforms/cuda.py
1	1	vllm/platforms/interface.py
0	34	vllm/platforms/rocm.py

[7f83b4ee8] Jialin Ouyang 2025-10-15 [Easy] Get rid of unnecessary paraenthesis in kv_cache_manager (#26842)
1	1	vllm/v1/core/kv_cache_manager.py

[5c3bae1a6] ant-yy 2025-10-15 [Fix] Remove divisibility requirement between num_kv_heads and tp_size in bailing_moe (#26876)
1	2	vllm/model_executor/models/bailing_moe.py

[5210dc394] Xudong Ma 2025-10-15 [Misc] Update TritonLanguagePlaceholder to have attributes that are used by Flash Linear Attention ops. (#26853)
3	0	vllm/triton_utils/importing.py

[650b51f9f] youkaichao 2025-10-15 [doc] add Context Parallel Deployment doc (#26877)
47	0	docs/serving/context_parallel_deployment.md

[625669799] Cyrus Leung 2025-10-15 [Doc] ruff format remaining Python examples (#26795)
6	4	docs/features/quantization/auto_awq.md
2	2	docs/features/quantization/bitblas.md
2	2	docs/features/quantization/bnb.md
7	2	docs/features/quantization/fp8.md
7	5	docs/features/quantization/gguf.md
1	1	docs/features/quantization/gptqmodel.md
4	2	docs/features/quantization/int4.md
3	1	docs/features/quantization/int8.md
2	2	docs/features/quantization/modelopt.md
5	3	docs/features/quantization/quantized_kvcache.md
44	21	docs/features/quantization/quark.md
5	3	docs/getting_started/quickstart.md
2	2	docs/models/extensions/tensorizer.md
3	3	docs/models/generative_models.md
16	10	docs/models/pooling_models.md
2	2	docs/models/supported_models.md
3	3	docs/serving/expert_parallel_deployment.md
9	7	docs/serving/integrations/langchain.md
31	26	docs/serving/openai_compatible_server.md
11	3	examples/offline_inference/openai_batch/README.md
1	1	examples/others/tensorize_vllm_model.py

[71557a5f7] Wentao Ye 2025-10-15 [CI] Fix mypy for `vllm/executor` (#26845)
1	1	tools/pre_commit/mypy.py
4	3	vllm/executor/executor_base.py
12	6	vllm/executor/ray_distributed_executor.py
6	1	vllm/executor/ray_utils.py

[f3c378ffa] Zhewen Li 2025-10-15 [CI/Build] Add Qwen2.5-VL-7B-Instruct ChartQA Accuracy Tests in CI (#21810)
12	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-QQQ.yaml
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8-MM.yaml
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-4-Maverick-17B-128E-Instruct-FP8.yaml
2	1	.buildkite/lm-eval-harness/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
12	0	.buildkite/lm-eval-harness/configs/Qwen2.5-VL-7B-Instruct.yaml
1	0	.buildkite/lm-eval-harness/configs/models-large-h100.txt
1	0	.buildkite/lm-eval-harness/configs/models-mm-large-h100.txt
1	0	.buildkite/lm-eval-harness/configs/models-mm-small.txt
44	0	.buildkite/lm-eval-harness/run-lm-eval-chartqa-vllm-vlm-baseline.sh
0	0	.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh
50	0	.buildkite/lm-eval-harness/run-lm-eval-mmlupro-vllm-baseline.sh
9	3	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
10	0	.buildkite/test-pipeline.yaml

[f5ed68ef6] Yongye Zhu 2025-10-15 [Deepseek-V3.2][Kernel] Integrate cuda indexer k cache gather (#26456)
6	68	vllm/model_executor/models/deepseek_v2.py

[efdef57b1] Angela Yi 2025-10-15 [bugfix] Lazy import cv2 (#26869)
4	1	vllm/assets/video.py

[b8a457215] Cyrus Leung 2025-10-15 [Misc] Use helper function to generate dummy messages in OpenAI MM tests (#26875)
26	43	tests/entrypoints/openai/test_audio.py
31	66	tests/entrypoints/openai/test_video.py
34	67	tests/entrypoints/openai/test_vision.py

[302ef403a] Mengqing Cao 2025-10-15 [DSA][MLA] Tiny refactor on DeepSeek to make it reusable for different backends (#26656)
2	0	vllm/attention/layer.py
8	2	vllm/model_executor/models/deepseek_mtp.py
2	1	vllm/model_executor/models/deepseek_v2.py

[8865da157] sangho.lee 2025-10-15 [Bugfix][Multi Modal] Fix incorrect Molmo token processing (#26873)
5	2	vllm/model_executor/models/molmo.py

[f0862eae4] Boyuan Feng 2025-10-14 [Graph Partition] pass tests for decorator (#26831)
74	26	tests/compile/test_decorator.py

[8c851f6d0] Isotr0py 2025-10-15 [Bugfix] Fix qwen3-omni audio truncation issue (#26815)
16	2	vllm/model_executor/models/qwen3_omni_moe_thinker.py

[7cfa420f4] Angela Yi 2025-10-14 [BugFix] Patch inductor partitioning logic (#26735)
118	0	vllm/env_override.py

[a27b288e4] rongfu.leng 2025-10-15 [Feature] default --extra-body param to disable thinking in vllm bench serve (#26784)
13	1	vllm/benchmarks/serve.py

[e471d7ca7] zhrrr 2025-10-15 [CI/Build][Bugfix] fix qutlass cmake error when set QUTLASS_SRC_DIR (#26773)
2	2	cmake/external_projects/qutlass.cmake

[c43ca8259] Michael Yao 2025-10-15 [Docs] Move build.inc into arm.inc (#26862)
40	1	docs/getting_started/installation/cpu/arm.inc.md
0	44	docs/getting_started/installation/cpu/build.inc.md

[85a65e7f5] Tao Hui 2025-10-15 [Model] Add DeepSeek-V3.1 reasoning parser (split from PR #24972) (#25589)
3	1	docs/features/reasoning_outputs.md
76	0	tests/reasoning/test_deepseekv3_reasoning_parser.py
8	2	vllm/entrypoints/openai/serving_chat.py
4	0	vllm/reasoning/__init__.py
66	0	vllm/reasoning/deepseek_v3_reasoning_parser.py
58	0	vllm/reasoning/identity_reasoning_parser.py

[a2986b3e3] kourosh hakhamaneshi 2025-10-14 [Bugfix] Fixes prefix-repetition benchmark script (#26828)
4	3	vllm/benchmarks/datasets.py

[96b9aa5aa] Morrison Turnansky 2025-10-14 [Frontend][torch.compile] CompilationConfig Overhaul (#20283): name change  compilation level to compilation mode, deprecation compilation level (#26355)
2	2	docs/configuration/conserving_memory.md
2	2	docs/design/cuda_graphs.md
1	1	examples/offline_inference/data_parallel.py
5	5	tests/compile/piecewise/test_multiple_graphs.py
2	2	tests/compile/piecewise/test_simple.py
5	5	tests/compile/piecewise/test_toy_llama.py
2	2	tests/compile/test_aot_compile.py
2	1	tests/compile/test_async_tp.py
14	16	tests/compile/test_basic_correctness.py
10	10	tests/compile/test_config.py
5	5	tests/compile/test_decorator.py
15	14	tests/compile/test_full_graph.py
2	2	tests/compile/test_fusion.py
2	2	tests/compile/test_fusion_all_reduce.py
2	2	tests/compile/test_fusion_attn.py
3	3	tests/compile/test_noop_elimination.py
2	2	tests/compile/test_wrapper.py
2	1	tests/distributed/test_sequence_parallel.py
10	10	tests/engine/test_arg_utils.py
3	3	tests/tpu/test_custom_dispatcher.py
5	5	tests/utils_/test_utils.py
11	11	tests/v1/cudagraph/test_cudagraph_dispatch.py
17	22	tests/v1/cudagraph/test_cudagraph_mode.py
3	3	tests/v1/e2e/test_kv_sharing_fast_prefill.py
2	2	vllm/compilation/backends.py
1	1	vllm/compilation/compiler_interface.py
2	2	vllm/compilation/counter.py
5	5	vllm/compilation/decorators.py
3	3	vllm/compilation/monitor.py
4	4	vllm/compilation/wrapper.py
2	2	vllm/config/__init__.py
70	38	vllm/config/compilation.py
22	28	vllm/config/vllm.py
2	4	vllm/entrypoints/llm.py
2	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
4	4	vllm/platforms/cpu.py
6	5	vllm/platforms/tpu.py
2	2	vllm/platforms/xpu.py
5	5	vllm/utils/__init__.py
2	2	vllm/v1/cudagraph_dispatcher.py
2	2	vllm/v1/spec_decode/eagle.py
8	7	vllm/v1/worker/gpu_model_runner.py

[e66d787bc] Michael Goin 2025-10-14 Disable FlashInfer sampler by default (#26859)
6	14	vllm/v1/sample/ops/topk_topp_sampler.py

[bfad142e2] Chendi.Xue 2025-10-14 [BUGFIX][NIXL] quick fix for 'assert self.connector_worker is not None' in get_kv_connector_stats (#26851)
2	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[935466003] Zhikaiiii 2025-10-15 [Bugfix]fix Qwen3 xml tool parser (#26345)
86	2	tests/tool_use/test_qwen3coder_tool_parser.py
92	23	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py

[07ca70af8] Jialin Ouyang 2025-10-14 [Core][Easy] Use envs.__getattr__ for all Unify to environment variable access (#26810)
3	3	vllm/multimodal/cache.py
2	2	vllm/transformers_utils/utils.py
3	3	vllm/utils/gc_utils.py
2	3	vllm/v1/engine/async_llm.py

[2dcd12d35] Luka Govedič 2025-10-14 [torch.compile] Fix tests for torch==2.9 inductor partition (#26116)
21	8	tests/compile/piecewise/test_full_cudagraph.py
27	11	tests/compile/piecewise/test_multiple_graphs.py
74	43	tests/compile/piecewise/test_toy_llama.py
0	1	tests/compile/silly_attention.py
3	0	tests/compile/test_decorator.py
0	6	vllm/attention/layer.py
11	2	vllm/compilation/partition_rules.py
2	1	vllm/config/compilation.py

[579d2e545] Tyler Michael Smith 2025-10-14 [WideEP][P/D] Add usage stats for DP+EP and KV Connector (#26836)
14	1	vllm/v1/utils.py

[0512c04ae] Ye Hu 2025-10-14 [frontend][gptoss] Add per turn stats into Harmony Context (#25061)
84	9	tests/entrypoints/test_context.py
43	22	vllm/entrypoints/context.py
4	0	vllm/entrypoints/openai/protocol.py
57	31	vllm/entrypoints/openai/serving_responses.py

[7e0ef4084] Michael Goin 2025-10-14 [CI Failure] Fix torchao dep failure for Quantization Test (#26824)
2	1	.buildkite/test-amd.yaml
2	1	.buildkite/test-pipeline.yaml
2	1	tests/quantization/test_compressed_tensors.py
2	1	vllm/model_executor/layers/quantization/rtn.py

[4aed506b6] Nick Hill 2025-10-14 [Core] Streamline some structured output related code (#26737)
8	10	tests/v1/core/test_scheduler.py
1	1	tests/v1/kv_connector/unit/test_kv_connector_lifecyle.py
12	12	tests/v1/tpu/worker/test_tpu_model_runner.py
12	12	tests/v1/worker/test_gpu_model_runner.py
2	3	vllm/v1/core/sched/output.py
30	35	vllm/v1/core/sched/scheduler.py
8	10	vllm/v1/request.py
15	17	vllm/v1/structured_output/__init__.py
1	1	vllm/v1/structured_output/backend_guidance.py
25	19	vllm/v1/structured_output/request.py
2	7	vllm/v1/structured_output/utils.py
2	4	vllm/v1/worker/gpu_model_runner.py
1	5	vllm/v1/worker/tpu_model_runner.py

[a86b4c58e] Boyuan Feng 2025-10-14 remove attn output view kernel (#26680)
3	3	vllm/attention/layer.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/rocm_aiter_unified_attn.py
1	1	vllm/v1/attention/backends/rocm_attn.py
1	1	vllm/v1/attention/backends/tree_attn.py
1	1	vllm/v1/attention/backends/triton_attn.py
1	1	vllm/v1/attention/backends/xformers.py

[ff4810ba7] Nick Hill 2025-10-14 [Minor] Group async_scheduling related fields in model runner init (#26736)
9	11	vllm/v1/worker/gpu_model_runner.py

[9d6964926] Nan Qin 2025-10-14 fix: response_format for completion (#23212)
21	7	vllm/entrypoints/openai/protocol.py

[0e6581891] Dhruvil Bhatt 2025-10-14 Added MoE configs for llama 4, H200 device with tp=4/8 tuning (#26837)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=2048,device_name=NVIDIA_H200.json

[380f17527] Jialin Ouyang 2025-10-14 [Perf] Cache vllm.env.__getattr__ result to avoid recomputation (#26146)
48	1	tests/test_envs.py
26	1	vllm/envs.py
5	0	vllm/v1/engine/core.py
5	0	vllm/v1/executor/multiproc_executor.py

[b92ab3ded] HDCharles 2025-10-14 Notice for deprecation of AutoAWQ (#26820)
4	0	docs/features/quantization/auto_awq.md

[acaa2c0a4] Jialin Ouyang 2025-10-14 [Core] Reuse empty block lists whenever possible in KVCacheBlocks to mitigate GC costs (#24964)
2	2	vllm/v1/core/block_pool.py
3	2	vllm/v1/core/kv_cache_coordinator.py
36	15	vllm/v1/core/kv_cache_manager.py
1	3	vllm/v1/core/sched/scheduler.py
11	4	vllm/v1/core/single_type_kv_cache_manager.py

[82af928c4] Matthew Bonanni 2025-10-14 [Attention][Spec Decode] FlashMLA spec decode support (#26541)
156	71	tests/v1/attention/test_mla_backends.py
37	12	vllm/v1/attention/backends/mla/common.py
3	2	vllm/v1/attention/backends/mla/flashattn_mla.py
2	4	vllm/v1/attention/backends/mla/flashinfer_mla.py
16	2	vllm/v1/attention/backends/mla/flashmla.py

[87efc681d] Huamin Li 2025-10-14 llama4_vision_rope: add HIP override to accept (q, k) and avoid (positions, q, k) mismatch (#26790)
7	0	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py

[c3a722fcb] Michael Goin 2025-10-14 [CI Failure] Fix tests with missing TinyLlama-1.1B-Chat-v1.0-FP8-e2e (#26816)
1	1	tests/compile/test_async_tp.py
1	1	tests/compile/test_fusion_all_reduce.py
1	1	tests/compile/test_sequence_parallelism.py

[aba48f7db] Ze'ev Klapow 2025-10-14 [Kernel][MoE] Add MoE tunings for GLM 4.6-FP8 and GLM 4.5 Air on NVidia B200 (#26818)
147	0	vllm/model_executor/layers/fused_moe/configs/E=32,N=1408,device_name=NVIDIA_B200.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=40,N=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8.json
147	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1408,device_name=NVIDIA_B200.json

[04b5f9802] Michael Goin 2025-10-14 [CI] Raise VLLM_MAX_SIZE_MB to 500 due to failing Build wheel - CUDA 12.9 (#26722)
2	2	.buildkite/check-wheel-size.py
1	1	docker/Dockerfile

[efc8f7d81] Reza Barazesh 2025-10-14 Update coveragerc and add codecov.yml for path fixes (#26435)
16	1	.coveragerc
12	0	codecov.yml

[6d87a2838] Wentao Ye 2025-10-14 [Config] Remove Unused Environment Variable `VLLM_DISABLE_PAD_FOR_CUDAGRAPH` (#26743)
0	7	vllm/envs.py
0	1	vllm/v1/worker/gpu_model_runner.py

[e6cdbd679] wang.yuqi 2025-10-14 Revert "[issues template] Encourage the author implement their own ideas" (#26814)
0	5	.github/ISSUE_TEMPLATE/100-documentation.yml
0	5	.github/ISSUE_TEMPLATE/400-bug-report.yml
0	5	.github/ISSUE_TEMPLATE/450-ci-failure.yml
0	5	.github/ISSUE_TEMPLATE/500-feature-request.yml
0	5	.github/ISSUE_TEMPLATE/600-new-model.yml
0	5	.github/ISSUE_TEMPLATE/700-performance-discussion.yml
0	5	.github/ISSUE_TEMPLATE/750-RFC.yml

[df850c491] Chauncey 2025-10-14 [Feature][Responses API] Stream Function Call - harmony (#24317)
136	66	tests/entrypoints/openai/test_response_api_with_harmony.py
77	4	vllm/entrypoints/openai/serving_responses.py

[720394de4] Qier Li 2025-10-14 [KVConnector][Metrics] Aggregate scheduler-side KVConnectorStats (#26046)
69	0	tests/v1/kv_connector/unit/test_nixl_connector.py
4	0	vllm/v1/core/sched/scheduler.py

[88a49745a] wang.yuqi 2025-10-14 [issues template] Encourage the author implement their own ideas (#26671)
5	0	.github/ISSUE_TEMPLATE/100-documentation.yml
5	0	.github/ISSUE_TEMPLATE/400-bug-report.yml
5	0	.github/ISSUE_TEMPLATE/450-ci-failure.yml
5	0	.github/ISSUE_TEMPLATE/500-feature-request.yml
5	0	.github/ISSUE_TEMPLATE/600-new-model.yml
5	0	.github/ISSUE_TEMPLATE/700-performance-discussion.yml
5	0	.github/ISSUE_TEMPLATE/750-RFC.yml

[ca683a2a7] Boyuan Feng 2025-10-14 use combo kernel to fuse qk-norm and qk-rope (#26682)
10	0	vllm/config/compilation.py

[e9f1b8c9e] 汪志鹏 2025-10-14 Adjusted the model order of the model registration file (#26798)
6	6	vllm/model_executor/models/registry.py

[ea97940d6] Jaya Yuan 2025-10-14 [DCP] Support Decode Context Parallel (DCP) for GQA with FlashAttention (#24864)
5	1	tests/distributed/test_context_parallel.py
4	1	tests/models/registry.py
9	1	vllm/attention/ops/common.py
17	0	vllm/config/model.py
172	30	vllm/v1/attention/backends/flash_attn.py
1	0	vllm/v1/attention/backends/utils.py
1	0	vllm/v1/worker/gpu_model_runner.py

[fdd32750f] Jee Jee Li 2025-10-14 [CI/Build] Cleanup LoRA test (#26752)
0	5	tests/lora/test_chatglm3_tp.py
0	3	tests/lora/test_llama_tp.py
3	3	tests/lora/test_minicpmv_tp.py

[c715ba373] Vladislav Bronzov 2025-10-14 [Feature] Change vllm.py with pydantic validation (#26726)
11	11	vllm/config/vllm.py

[9c4cb6833] Cyrus Leung 2025-10-14 [Chore] Remove `SupportsV0Only` interface and update supported models docs (#26783)
215	238	docs/models/supported_models.md
0	6	docs/usage/v1_guide.md
0	4	tests/models/registry.py
5	7	tests/models/test_initialization.py
0	4	vllm/config/model.py
0	7	vllm/engine/arg_utils.py
0	4	vllm/model_executor/models/__init__.py
0	21	vllm/model_executor/models/interfaces.py
0	11	vllm/model_executor/models/registry.py

[780eb03d9] Chauncey 2025-10-14 [CI] Fix test_tool_id_kimi_k2 (#26787)
8	5	tests/entrypoints/openai/test_completion_with_function_calling.py

[ef9676a1f] Cyrus Leung 2025-10-14 [Doc] ruff format some Python examples (#26767)
23	21	docs/configuration/conserving_memory.md
15	9	docs/configuration/optimization.md
2	2	docs/contributing/model/basic.md
23	19	docs/contributing/model/multimodal.md
1	1	docs/contributing/model/registration.md
11	1	docs/contributing/model/transcription.md
2	2	docs/deployment/frameworks/cerebrium.md
2	2	docs/deployment/frameworks/dstack.md
1	1	docs/deployment/frameworks/haystack.md
18	18	docs/deployment/frameworks/hf_inference_endpoints.md
7	6	docs/deployment/frameworks/litellm.md
2	2	docs/deployment/frameworks/retrieval_augmented_generation.md
9	7	docs/design/cuda_graphs.md
15	12	docs/design/io_processor_plugins.md
7	5	docs/design/metrics.md
2	2	docs/design/prefix_caching.md
4	7	docs/features/lora.md
147	130	docs/features/multimodal_inputs.md
31	25	docs/features/reasoning_outputs.md
19	18	docs/features/tool_calling.md

[70b1b330e] Harry Mellor 2025-10-14 Don't allow `typos` to fix by default (#26785)
1	0	.pre-commit-config.yaml

[d1d063a58] Cyrus Leung 2025-10-14 [Chore] Use `max_transformers_version` for Qwen-VL test (#26792)
0	2	tests/models/multimodal/generation/test_common.py
2	0	tests/models/registry.py

[7e6edb146] Chendi.Xue 2025-10-14 [NIXL][HeteroTP] Enable KV transfer from HND prefill to NHD decode (#26556)
10	0	docs/features/nixl_connector_usage.md
11	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
58	2	tests/v1/kv_connector/unit/test_nixl_connector.py
2	0	tests/v1/kv_connector/unit/utils.py
3	0	vllm/config/kv_transfer.py
66	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[74704d455] Cyrus Leung 2025-10-14 [Model] Use merge_by_field_config for MM models (O-P) (#26776)
7	16	vllm/model_executor/models/phi3v.py
11	53	vllm/model_executor/models/phi4_multimodal.py
12	53	vllm/model_executor/models/phi4mm.py

[d2f816d6f] Cyrus Leung 2025-10-14 [Bugfix] Standardize merging multimodal embeddings (#26771)
3	3	vllm/model_executor/models/ernie45_vl.py
3	3	vllm/model_executor/models/glm4_1v.py
3	3	vllm/model_executor/models/hyperclovax_vision.py
3	3	vllm/model_executor/models/interns1.py
3	3	vllm/model_executor/models/internvl.py
3	3	vllm/model_executor/models/keye.py
2	2	vllm/model_executor/models/llava_onevision.py
2	2	vllm/model_executor/models/minicpmo.py
4	4	vllm/model_executor/models/minicpmv.py
3	3	vllm/model_executor/models/nano_nemotron_vl.py
2	2	vllm/model_executor/models/nemotron_vl.py
3	3	vllm/model_executor/models/ovis2_5.py
2	2	vllm/model_executor/models/phi4_multimodal.py
2	2	vllm/model_executor/models/phi4mm.py
4	4	vllm/model_executor/models/qwen2_5_omni_thinker.py
5	5	vllm/model_executor/models/qwen2_5_vl.py
3	3	vllm/model_executor/models/qwen2_vl.py
4	4	vllm/model_executor/models/qwen3_omni_moe_thinker.py
3	3	vllm/model_executor/models/qwen3_vl.py

[577d49821] wangxiyuan 2025-10-14 [Plugin] Make plugin group clear (#26757)
2	2	vllm/platforms/__init__.py
7	0	vllm/plugins/__init__.py
2	2	vllm/plugins/io_processors/__init__.py

[fd85c9f42] Max Wittig 2025-10-14 [Bugfix][FE]: Always include usage with `--enable-force-include-usage ` (#20983)
1	0	pyproject.toml
126	0	tests/entrypoints/openai/test_enable_force_include_usage.py
2	0	vllm/entrypoints/openai/api_server.py
8	0	vllm/entrypoints/openai/run_batch.py
4	11	vllm/entrypoints/openai/serving_chat.py
5	11	vllm/entrypoints/openai/serving_completion.py
0	3	vllm/entrypoints/openai/serving_engine.py
0	1	vllm/entrypoints/openai/serving_responses.py
4	0	vllm/entrypoints/openai/serving_transcription.py
4	3	vllm/entrypoints/openai/speech_to_text.py
18	1	vllm/entrypoints/utils.py

[d32c611f4] Ye (Charlotte) Qi 2025-10-14 [CI/Build] Use 127.0.0.1 instead of localhost in utils (#26750)
1	1	tests/utils.py

[01ad27faf] CSWYF3634076 2025-10-14 [Model][Bugfix]fix ernie45 load failed due to ernie45 eplb code (#26684)
22	12	vllm/model_executor/models/ernie45_moe.py

[481545b39] Ryan Li 2025-10-14 scheduler.py: Update the name of the default scheduler. (#26758)
6	6	vllm/config/scheduler.py
0	5	vllm/engine/arg_utils.py

[d3cc8427c] Alexei-V-Ivanov-AMD 2025-10-14 [ci] Adding the test-amd.yaml for test definitions for the AMD backend. (alternative PR) (#26718)
1265	0	.buildkite/test-amd.yaml

[4821ac1b4] vllmellm 2025-10-14 [CI] [ROCm] Automate CC list for ROCm related issue (#26753)
95	43	.github/workflows/issue_autolabel.yml

[4497c8f82] XiongfeiWei 2025-10-13 Fix lora tests failure in TPU CI due to the removal of LoRA bias (#26723)
1	2	vllm/v1/worker/tpu_model_runner.py

[2e36cdbe2] Michael Yao 2025-10-14 [Docs] Add a start tag to build.inc.md (#26747)
1	1	docs/getting_started/installation/cpu/arm.inc.md
2	3	docs/getting_started/installation/cpu/build.inc.md

[fe3edb4cf] Maximilien de Bayser 2025-10-14 Add support for the /rerank endpoint in vllm bench serve (#26602)
46	0	docs/contributing/benchmarks.md
129	0	vllm/benchmarks/datasets.py
43	6	vllm/benchmarks/lib/endpoint_request_func.py

[29350922c] Heng Guo 2025-10-14 [Feature][Quantization] auto_round format add support for regex (#24024)
39	5	vllm/model_executor/layers/quantization/auto_round.py

[8ae169286] Varun Sundar Rabindranath 2025-10-13 [torch.compile] Unwrap fused_marlin_moe custom op (#26739)
3	2	tests/kernels/moe/test_moe.py
0	1	vllm/model_executor/layers/fused_moe/__init__.py
0	39	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
2	1	vllm/model_executor/layers/quantization/awq_marlin.py
4	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	1	vllm/model_executor/layers/quantization/fp8.py
2	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	1	vllm/model_executor/layers/quantization/modelopt.py
5	2	vllm/model_executor/layers/quantization/mxfp4.py
2	1	vllm/model_executor/layers/quantization/quark/quark_moe.py

[8a0af6a56] youkaichao 2025-10-14 [build][torch.compile] upgrade depyf version (#26702)
1	1	requirements/common.txt

[cfded8079] Jialin Ouyang 2025-10-13 [Easy] Fix env type check errors from VLLM_DEBUG_LOG_API_SERVER_RESPONSE (#26742)
1	0	vllm/envs.py

[b59dd19b5] Angela Yi 2025-10-13 [compile] Enable sequence parallelism for full cuda graph without specifying compile sizes (#26681)
9	2	vllm/compilation/collective_fusion.py
1	1	vllm/compilation/inductor_pass.py
3	1	vllm/compilation/pass_manager.py
19	1	vllm/compilation/sequence_parallelism.py
2	0	vllm/compilation/vllm_inductor_pass.py

[3e051bda8] Michael Goin 2025-10-13 [UX] Replace VLLM_ALL2ALL_BACKEND with --all2all-backend (#26732)
2	2	docs/design/dbo.md
21	17	docs/serving/expert_parallel_deployment.md
31	2	vllm/config/parallel.py
4	4	vllm/config/vllm.py
3	0	vllm/distributed/device_communicators/base_device_communicator.py
7	8	vllm/distributed/device_communicators/cuda_communicator.py
5	7	vllm/distributed/device_communicators/xpu_communicator.py
5	0	vllm/engine/arg_utils.py
6	6	vllm/model_executor/layers/fused_moe/config.py
1	1	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
2	2	vllm/platforms/cuda.py
3	2	vllm/v1/engine/utils.py

[8317f7235] Lucia Fang 2025-10-13 [Misc][DP] support customized aggregated logger for dp (#24354)
42	13	examples/online_serving/multi_instance_data_parallel.py
51	5	tests/v1/engine/test_async_llm.py
5	3	tests/v1/metrics/test_engine_logger_apis.py
7	0	vllm/engine/arg_utils.py
1	0	vllm/entrypoints/openai/api_server.py
4	0	vllm/v1/engine/async_llm.py
2	0	vllm/v1/engine/llm_engine.py
185	64	vllm/v1/metrics/loggers.py

[d8bebb008] Maximilien de Bayser 2025-10-13 Add tests for chunked prefill and prefix cache with causal pooling models (#26526)
167	0	tests/v1/e2e/test_pooling_chunked_prefill.py

[35bc22f23] Jialin Ouyang 2025-10-13 [ResponseAPI] Further polish message serialization and unit tests (#26728)
36	0	tests/entrypoints/openai/test_protocol.py
0	31	tests/entrypoints/openai/test_response_api_with_harmony.py
1	1	vllm/entrypoints/openai/protocol.py

[fa96fb9c7] Fardin Hoque 2025-10-13 Pruning kernel Core Tests (#26727)
0	1	tests/kernels/core/test_fused_quant_layernorm.py
2	14	tests/kernels/core/test_layernorm.py
1	1	tests/kernels/core/test_permute_cols.py
2	2	tests/kernels/core/test_pos_encoding.py

[e3fdb627d] Morrison Turnansky 2025-10-13 [FrontEnd] UNREVERT CompilationConfig overhaul (#20283): deprecate use_inductor in favor of backend, simplify custom_ops  (#26502)
11	20	tests/compile/piecewise/test_toy_llama.py
30	24	tests/compile/test_basic_correctness.py
20	19	tests/model_executor/test_enabled_custom_ops.py
5	1	vllm/compilation/backends.py
63	10	vllm/config/compilation.py
14	0	vllm/config/vllm.py
10	10	vllm/model_executor/custom_op.py
0	2	vllm/platforms/cpu.py

[7200a21cd] Wentao Ye 2025-10-13 [Bug] Fix Assertion error DeepEP/csrc/kernels/intranode.cu:928: 'false and Unsupported type' (#26532)
4	0	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py

[577c72a22] Fardin Hoque 2025-10-13 [CI Perf]Prune Tests in kernel/mamba (#26538)
2	2	tests/kernels/mamba/test_causal_conv1d.py
0	1	tests/kernels/mamba/test_mamba_mixer2.py
11	11	tests/kernels/mamba/test_mamba_ssm.py
8	22	tests/kernels/mamba/test_mamba_ssm_ssd.py

[314285d4f] Wentao Ye 2025-10-13 [CI] Fix mypy for `vllm/distributed` (#26593)
1	1	tools/pre_commit/mypy.py
1	1	vllm/config/kv_transfer.py
24	12	vllm/distributed/device_communicators/all2all.py
12	7	vllm/distributed/device_communicators/custom_all_reduce.py
8	3	vllm/distributed/device_communicators/symm_mem.py
12	2	vllm/distributed/kv_transfer/kv_connector/factory.py
7	7	vllm/distributed/kv_transfer/kv_connector/utils.py
4	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
3	4	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
12	6	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
11	8	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
25	14	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
1	0	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
1	0	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py

[d2a793858] wang.yuqi 2025-10-14 [Frontend][1/N] Improve all pooling task | Support FP16 Embedding Base64 (Still uses fp32 by default). (#26414)
6	0	examples/online_serving/pooling/README.md
59	0	examples/online_serving/pooling/embedding_embed_dtype_client.py
73	1	tests/entrypoints/pooling/openai/test_embedding.py
76	1	tests/entrypoints/pooling/openai/test_pooling.py
42	3	vllm/entrypoints/openai/protocol.py
12	24	vllm/entrypoints/openai/serving_embedding.py
11	1	vllm/entrypoints/openai/serving_pooling.py
33	0	vllm/entrypoints/openai/utils.py

[89342ce4c] Alex Kogan 2025-10-13 [Quantization] [Performance] Enable Marlin GEMM kernels for the calibration-free RTN-based quantization (#26051)
188	56	vllm/model_executor/layers/quantization/rtn.py
45	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[f89f59939] Yibo Cai 2025-10-14 [CI][Release][Arm64]: Build arm64 release for gpu arch 8.9 (#26698)
2	2	.buildkite/release-pipeline.yaml

[e251e457c] Wentao Ye 2025-10-13 [Log] Optimize Startup Log (#26601)
7	6	vllm/distributed/device_communicators/cuda_communicator.py
1	2	vllm/distributed/device_communicators/pynccl.py
1	1	vllm/utils/__init__.py
1	1	vllm/v1/attention/backends/mla/cutlass_mla.py
7	5	vllm/v1/engine/core.py
0	1	vllm/v1/worker/gpu_model_runner.py

[afc47e4de] Cyrus Leung 2025-10-14 [Model] Use merge_by_field_config for MM models (M-N) (#26710)
10	4	vllm/model_executor/models/interns1.py
10	4	vllm/model_executor/models/internvl.py
27	53	vllm/model_executor/models/midashenglm.py
5	42	vllm/model_executor/models/minicpmo.py
8	56	vllm/model_executor/models/minicpmv.py
10	13	vllm/model_executor/models/mllama4.py
18	40	vllm/model_executor/models/molmo.py
27	113	vllm/model_executor/models/nano_nemotron_vl.py
5	2	vllm/model_executor/models/nemotron_vl.py
5	2	vllm/model_executor/models/skyworkr1v.py
1	1	vllm/multimodal/utils.py

[e3b90c1ba] Rahul Tuli 2025-10-13 [Bugfix][Speculative Decoding] Extend Eagle quantization config fix to llama_eagle.py (#26590)
12	0	vllm/model_executor/models/llama_eagle.py

[134f70b3e] haoyangli-amd 2025-10-14 [Bugfix][Rocm] fix qr error when different inp shape (#25892)
6	5	csrc/quickreduce/quick_reduce.h
3	6	csrc/quickreduce/quick_reduce_impl.cuh
87	0	tests/distributed/test_quick_all_reduce.py

[a1b2d658e] Sangyeon Cho 2025-10-14 [CI/Build] upgrade compressed-tensors to 0.12.2 to address LGPLv3 (#26501)
1	1	requirements/common.txt
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[5c7fe2549] Aleksei Tsvetkov 2025-10-13 [Misc] Separate prompt logging to debug (#26713)
10	6	vllm/entrypoints/logger.py

[53c9a7cee] Will Eaton 2025-10-13 [P/D] [NixlConnector] kv load recovery integration (#26171)
142	1	tests/v1/kv_connector/unit/test_nixl_connector.py
109	24	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	1	vllm/v1/core/sched/scheduler.py

[0d21b9b51] Michael Goin 2025-10-13 [UX] Speedup DeepGEMM warmup with heuristics (#25619)
20	4	vllm/envs.py
74	9	vllm/model_executor/warmup/deep_gemm_warmup.py
1	1	vllm/model_executor/warmup/kernel_warmup.py

[10214b693] Anand Roy 2025-10-13 [FEATURE]: Use pydantic validation in `multimodal.py` config (#26629)
18	7	vllm/config/multimodal.py

[4a61950f4] ihb2032 2025-10-13 [Hardware][CPU] Disable torch.compile for RISC-V to prevent APIError (#26693)
5	2	vllm/v1/sample/ops/topk_topp_sampler.py

[326379905] Bram Wasti 2025-10-13 [unrevert] Add batch invariant kernel override for FlashInfer backend [2/n] (#26373)
1	3	csrc/moe/topk_softmax_kernels.cu
37	28	tests/v1/generation/test_batch_invariance.py
14	1	vllm/model_executor/layers/batch_invariant.py
29	3	vllm/v1/attention/backends/flashinfer.py

[8e67b2557] Isotr0py 2025-10-13 [Bugfix] Fix out of bound index issue for Jina-embedding-v3 RoPE with cuda graph (#26687)
0	4	tests/models/language/pooling_mteb_test/test_jina.py
13	3	vllm/model_executor/models/config.py

[4073c82c4] Jialin Ouyang 2025-10-13 [ResponseAPI] Simplify input/output message serialization (#26620)
31	0	tests/entrypoints/openai/test_response_api_with_harmony.py
22	24	vllm/entrypoints/openai/protocol.py

[767c3ab86] wang.yuqi 2025-10-13 [Model][0/N] Improve all pooling task | clean up (#25817)
1	1	docs/models/supported_models.md
1	1	examples/online_serving/pooling/README.md
0	0	examples/online_serving/pooling/{ner.py => ner_client.py}
6	0	tests/ci_envs.py
2	0	tests/entrypoints/pooling/llm/test_classify.py
0	1	tests/entrypoints/pooling/llm/test_embedding.py
0	1	tests/entrypoints/pooling/llm/test_encode.py
0	1	tests/entrypoints/pooling/llm/test_score.py
6	20	tests/models/language/generation_ppl_test/ppl_utils.py
47	0	tests/models/language/pooling/test_head_dtype.py
9	42	tests/models/language/pooling/test_splade_sparse_pooler.py
14	41	tests/models/language/pooling_mteb_test/mteb_utils.py
30	0	tests/models/utils.py
15	0	vllm/config/model.py
17	32	vllm/model_executor/models/adapters.py
5	2	vllm/model_executor/models/gpt2.py
10	39	vllm/model_executor/models/gritlm.py
0	7	vllm/model_executor/models/utils.py
34	0	vllm/transformers_utils/config.py

[4f207c717] Harry Mellor 2025-10-13 Ignore large reformatting PRs in `git blame` (#26690)
4	0	.git-blame-ignore-revs

[782505ed8] CSWYF3634076 2025-10-13 [Model] Add reasoning_parser and tool_parser for Ernie45 thinking (#25027)
2	0	docs/features/reasoning_outputs.md
124	0	tests/reasoning/test_ernie45_reasoning_parser.py
359	0	tests/tool_use/test_ernie45_moe_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
212	0	vllm/entrypoints/openai/tool_parsers/ernie45_tool_parser.py
2	0	vllm/reasoning/__init__.py
169	0	vllm/reasoning/ernie45_reasoning_parser.py

[98f30b8cb] Jee Jee Li 2025-10-13 [Model] Fix  Skywork R1V mlp (#26673)
18	3	vllm/model_executor/models/skyworkr1v.py

[3cd36660f] yihong 2025-10-13 docs: wrong command in structured_outputs README (#26677)
1	1	examples/online_serving/structured_outputs/README.md

[46ad73955] yyzxw 2025-10-13 [FIX] Throwing an exception when the model does not support pool tasks (#25840) (#25855)
3	0	vllm/model_executor/models/adapters.py
21	1	vllm/v1/worker/gpu_model_runner.py

[41f388443] quanliu 2025-10-13 [Bugfix][Core]Fix block table out-of-range issue in priority scheduling (#26661)
3	0	vllm/v1/core/sched/scheduler.py

[60e419c1e] bnellnm 2025-10-12 [Misc] cache result of disable_inplace (#26666)
2	0	vllm/model_executor/layers/fused_moe/utils.py

[7ef605280] Michael Goin 2025-10-12 [CI/Build] Add tool to build vllm-tpu wheel (#19165)
5	0	setup.py
67	0	tools/vllm-tpu/build.sh

[4fca1a1bd] Huamin Li 2025-10-12 [easy] fix pre commit error on trunk (#26665)
2	2	vllm/model_executor/models/bert.py

[a6049be73] Lukas Geiger 2025-10-12 [Models][Qwen3VL] Speedup `fast_pos_embed_interpolate` (#26647)
11	18	vllm/model_executor/models/qwen3_vl.py

[18ed7746e] gjgjos 2025-10-13 [Feature] Add support for naver/splade-v3 (BERT-based sparse embedding model) (#26339)
122	0	tests/models/language/pooling/test_splade_sparse_pooler.py
3	0	tests/models/registry.py
214	0	vllm/model_executor/models/bert.py
1	0	vllm/model_executor/models/registry.py

[8fcaaf6a1] Harry Mellor 2025-10-12 Update `Optional[x]` -> `x | None` and `Union[x, y]` to `x | y` (#26633)
13	14	benchmarks/backend_request_func.py
2	3	benchmarks/benchmark_prefix_caching.py
1	2	benchmarks/benchmark_prioritization.py
3	4	benchmarks/benchmark_serving_structured_output.py
8	8	benchmarks/benchmark_utils.py
1	2	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
5	6	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
4	5	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
1	1	benchmarks/kernels/bench_per_token_quant_fp8.py
3	3	benchmarks/kernels/benchmark_device_communicators.py
11	10	benchmarks/kernels/benchmark_lora.py
15	16	benchmarks/kernels/benchmark_machete.py
1	2	benchmarks/kernels/benchmark_paged_attention.py
1	1	benchmarks/kernels/benchmark_per_token_group_quant.py
0	2	benchmarks/kernels/benchmark_reshape_and_cache.py
0	2	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
5	6	benchmarks/kernels/benchmark_rmsnorm.py
1	2	benchmarks/kernels/benchmark_rope.py
1	4	benchmarks/kernels/benchmark_trtllm_decode_attention.py
1	4	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
3	3	benchmarks/kernels/utils.py
11	11	benchmarks/multi_turn/bench_dataset.py
5	5	benchmarks/multi_turn/benchmark_serving_multi_turn.py
9	9	benchmarks/multi_turn/convert_sharegpt_to_openai.py
6	9	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
2	3	csrc/quantization/machete/generate.py
7	7	docs/contributing/model/transcription.md
3	3	docs/design/logits_processors.md
1	2	docs/features/custom_logitsprocs.md
6	6	examples/offline_inference/audio_language.py
2	2	examples/offline_inference/kv_load_failure_recovery/rogue_shared_storage_connector.py
1	3	examples/offline_inference/logits_processor/custom.py
3	3	examples/offline_inference/logits_processor/custom_req.py
1	3	examples/offline_inference/logits_processor/custom_req_init.py
3	4	examples/offline_inference/lora_with_quantization_inference.py
2	4	examples/offline_inference/multilora_inference.py
1	2	examples/offline_inference/prithvi_geospatial_mae.py
4	3	examples/offline_inference/rlhf_utils.py
3	3	examples/offline_inference/vision_language.py
6	6	examples/offline_inference/vision_language_multi_image.py
8	8	examples/offline_inference/vision_language_pooling.py
7	10	examples/online_serving/disaggregated_serving/disagg_proxy_demo.py
6	6	examples/online_serving/kv_events_subscriber.py
1	2	examples/online_serving/multi_instance_data_parallel.py
1	3	examples/online_serving/pooling/cohere_rerank_client.py
2	2	examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py
2	8	examples/online_serving/structured_outputs/structured_outputs.py
0	6	pyproject.toml
3	3	tests/benchmarks/test_random_dataset.py
5	4	tests/ci_envs.py
2	3	tests/compile/backend.py
3	3	tests/compile/piecewise/test_toy_llama.py
0	2	tests/compile/test_basic_correctness.py
2	4	tests/compile/test_full_graph.py
2	3	tests/compile/test_fusion_attn.py
3	4	tests/compile/test_wrapper.py
73	75	tests/conftest.py
3	3	tests/detokenizer/test_stop_strings.py
3	4	tests/distributed/conftest.py
2	3	tests/distributed/test_comm_ops.py
3	3	tests/distributed/test_context_parallel.py
10	10	tests/distributed/test_expert_parallel.py
4	4	tests/distributed/test_pipeline_parallel.py
1	7	tests/distributed/test_pp_cudagraph.py
5	5	tests/distributed/test_sequence_parallel.py
8	8	tests/engine/test_arg_utils.py
1	1	tests/entrypoints/openai/test_async_tokenization.py
1	2	tests/entrypoints/openai/test_chat.py
1	2	tests/entrypoints/openai/test_completion_with_function_calling.py
5	6	tests/entrypoints/openai/test_lora_resolvers.py
2	7	tests/entrypoints/openai/test_serving_chat.py
4	5	tests/entrypoints/openai/tool_parsers/utils.py
2	4	tests/entrypoints/pooling/openai/test_embedding_dimensions.py
2	3	tests/entrypoints/test_api_server_process_manager.py
7	7	tests/entrypoints/test_chat_utils.py
1	2	tests/entrypoints/test_renderer.py
6	7	tests/evals/gsm8k/gsm8k_eval.py
5	6	tests/kernels/attention/test_aiter_flash_attn.py
3	4	tests/kernels/attention/test_attention.py
1	2	tests/kernels/attention/test_cascade_flash_attn.py
1	2	tests/kernels/attention/test_cutlass_mla_decode.py
8	9	tests/kernels/attention/test_flash_attn.py
8	9	tests/kernels/attention/test_flashinfer.py
4	9	tests/kernels/attention/test_flashinfer_trtllm_attention.py
1	2	tests/kernels/attention/test_merge_attn_states.py
5	6	tests/kernels/attention/test_triton_unified_attention.py
15	16	tests/kernels/core/test_fused_quant_layernorm.py
2	2	tests/kernels/core/test_pos_encoding.py
1	3	tests/kernels/core/test_rotary_embedding.py
10	11	tests/kernels/mamba/test_causal_conv1d.py
17	17	tests/kernels/moe/modular_kernel_tools/common.py
2	3	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
11	12	tests/kernels/moe/modular_kernel_tools/mk_objects.py
6	5	tests/kernels/moe/modular_kernel_tools/parallel_utils.py
2	1	tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py
11	10	tests/kernels/moe/parallel_utils.py
3	4	tests/kernels/moe/test_batched_moe.py
1	2	tests/kernels/moe/test_count_expert_num_tokens.py
11	12	tests/kernels/moe/test_cutlass_moe.py
6	7	tests/kernels/moe/test_deepep_deepgemm_moe.py
18	21	tests/kernels/moe/test_deepep_moe.py
5	5	tests/kernels/moe/test_modular_kernel_combinations.py
4	4	tests/kernels/moe/test_moe.py
1	3	tests/kernels/moe/test_moe_align_block_size.py
3	5	tests/kernels/moe/test_moe_permute_unpermute.py
6	7	tests/kernels/moe/test_ocp_mx_moe.py
1	2	tests/kernels/moe/test_pplx_cutlass_moe.py
29	35	tests/kernels/moe/test_pplx_moe.py
44	45	tests/kernels/moe/utils.py
14	15	tests/kernels/quant_utils.py
10	11	tests/kernels/quantization/test_cutlass_w4a8.py
20	21	tests/kernels/quantization/test_machete_mm.py
1	2	tests/kernels/quantization/test_triton_scaled_mm.py
2	4	tests/kernels/test_onednn.py
54	56	tests/kernels/utils.py
3	4	tests/lora/test_layers.py
2	5	tests/lora/test_llama_tp.py
3	4	tests/lora/test_qwen2vl.py
1	2	tests/lora/test_resolver.py
2	2	tests/lora/test_utils.py
2	3	tests/lora/utils.py
1	1	tests/model_executor/model_loader/tensorizer_loader/conftest.py
1	2	tests/model_executor/test_enabled_custom_ops.py
1	2	tests/models/language/generation/test_common.py
1	1	tests/models/language/generation/test_hybrid.py
2	2	tests/models/language/generation_ppl_test/ppl_utils.py
1	2	tests/models/language/pooling/embed_utils.py
1	2	tests/models/language/pooling/test_embedding.py
0	2	tests/models/language/pooling/test_gritlm.py
3	4	tests/models/language/pooling_mteb_test/mteb_utils.py
2	2	tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py
3	4	tests/models/multimodal/generation/test_granite_speech.py
2	3	tests/models/multimodal/generation/test_phi4_multimodal.py
3	4	tests/models/multimodal/generation/test_phi4mm.py
2	2	tests/models/multimodal/generation/test_pixtral.py
3	3	tests/models/multimodal/generation/test_qwen2_vl.py
1	2	tests/models/multimodal/generation/test_whisper.py
7	10	tests/models/multimodal/generation/vlm_utils/builders.py
11	10	tests/models/multimodal/generation/vlm_utils/core.py
1	1	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
10	11	tests/models/multimodal/generation/vlm_utils/model_utils.py
30	30	tests/models/multimodal/generation/vlm_utils/types.py
1	1	tests/models/multimodal/pooling/test_dse_qwen2_vl.py
2	3	tests/models/multimodal/pooling/test_jinavl_reranker.py
2	3	tests/models/multimodal/processing/test_common.py
1	2	tests/models/multimodal/processing/test_h2ovl.py
1	2	tests/models/multimodal/processing/test_internvl.py
1	2	tests/models/multimodal/processing/test_nemotron_vl.py
8	8	tests/models/multimodal/processing/test_tensor_schema.py
1	2	tests/models/quantization/test_awq.py
10	10	tests/models/registry.py
4	4	tests/models/test_transformers.py
19	19	tests/models/utils.py
3	4	tests/multimodal/test_cache.py
2	2	tests/multimodal/test_processing.py
4	6	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
3	3	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/types.py
3	4	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
1	2	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
1	2	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py
1	3	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/__init__.py
1	2	tests/quantization/test_blackwell_moe.py
1	3	tests/quantization/test_compressed_tensors.py
2	3	tests/quantization/test_quark.py
8	8	tests/quantization/test_register_quantization_config.py
8	9	tests/reasoning/utils.py
3	5	tests/samplers/test_no_bad_words.py
2	2	tests/tokenization/test_detokenize.py
8	10	tests/tokenization/test_tokenizer_registry.py
3	4	tests/tool_use/mistral/utils.py
1	2	tests/tool_use/test_jamba_tool_parser.py
1	2	tests/tool_use/test_parallel_tool_calls.py
1	2	tests/tool_use/test_qwen3coder_tool_parser.py
1	2	tests/tool_use/test_seed_oss_tool_parser.py
3	4	tests/tool_use/test_tool_calls.py
1	2	tests/tool_use/test_xlam_tool_parser.py
5	5	tests/tool_use/utils.py
3	4	tests/transformers_utils/test_config_parser_registry.py
24	23	tests/utils.py
2	3	tests/v1/attention/test_attention_backends.py
2	4	tests/v1/attention/test_mla_backends.py
3	4	tests/v1/attention/utils.py
4	4	tests/v1/core/test_kv_cache_utils.py
5	5	tests/v1/core/test_prefix_caching.py
10	13	tests/v1/core/test_scheduler.py
8	9	tests/v1/core/utils.py
5	6	tests/v1/distributed/test_async_llm_dp.py
3	3	tests/v1/distributed/test_internal_lb_dp.py
3	5	tests/v1/e2e/test_min_tokens.py
2	4	tests/v1/e2e/test_spec_decode.py
4	5	tests/v1/engine/test_async_llm.py
5	7	tests/v1/engine/test_engine_core_client.py
2	2	tests/v1/engine/test_llm_engine.py
8	9	tests/v1/engine/test_output_processor.py
7	8	tests/v1/engine/utils.py
2	2	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	2	tests/v1/entrypoints/openai/test_completion.py
6	5	tests/v1/executor/test_executor.py
1	1	tests/v1/kv_connector/unit/test_kv_load_failure_recovery.py
2	3	tests/v1/kv_connector/unit/test_nixl_connector.py
3	4	tests/v1/kv_connector/unit/test_output_aggreagator.py
7	6	tests/v1/kv_connector/unit/utils.py
1	2	tests/v1/kv_offload/test_cpu_manager.py
4	4	tests/v1/logits_processors/test_correctness.py
2	2	tests/v1/logits_processors/test_custom_offline.py
3	3	tests/v1/logits_processors/test_custom_online.py
4	4	tests/v1/logits_processors/utils.py
14	14	tests/v1/sample/test_rejection_sampler.py
4	4	tests/v1/sample/utils.py
1	2	tests/v1/spec_decode/test_eagle.py
1	2	tests/v1/spec_decode/test_tree_attention.py
1	2	tests/v1/test_serial_utils.py
2	2	tests/v1/tpu/test_basic.py
2	2	tests/v1/tpu/test_perf.py
0	2	tests/v1/tracing/test_tracing.py
1	2	tests/v1/worker/test_gpu_input_batch.py
2	3	tests/v1/worker/test_worker_memory_snapshot.py
1	2	tests/vllm_test_utils/vllm_test_utils/blame.py
2	2	tests/vllm_test_utils/vllm_test_utils/monitor.py
0	2	tools/check_init_lazy_imports.py
0	2	tools/enforce_regex_import.py
2	3	tools/pre_commit/mypy.py
2	2	tools/profiler/visualize_layerwise_profile.py
2	3	vllm/_bc_linter.py
103	105	vllm/_custom_ops.py
19	20	vllm/_ipex_ops.py
1	2	vllm/assets/base.py
2	2	vllm/assets/video.py
20	20	vllm/attention/backends/abstract.py
2	3	vllm/attention/backends/registry.py
1	2	vllm/attention/backends/utils.py
27	26	vllm/attention/layer.py
6	6	vllm/attention/layers/chunked_local_attention.py
2	3	vllm/attention/layers/cross_attention.py
2	3	vllm/attention/layers/encoder_only_attention.py
9	10	vllm/attention/ops/flashmla.py
1	2	vllm/attention/ops/merge_attn_states.py
5	6	vllm/attention/ops/paged_attn.py
9	10	vllm/attention/ops/rocm_aiter_mla.py
1	2	vllm/attention/ops/rocm_aiter_paged_attn.py
1	2	vllm/attention/ops/triton_merge_attn_states.py
9	10	vllm/attention/selector.py
1	2	vllm/attention/utils/fa_utils.py
8	8	vllm/beam_search.py
33	33	vllm/benchmarks/datasets.py
2	2	vllm/benchmarks/latency.py
20	20	vllm/benchmarks/lib/endpoint_request_func.py
15	15	vllm/benchmarks/serve.py
7	7	vllm/benchmarks/throughput.py
9	8	vllm/compilation/backends.py
2	1	vllm/compilation/base_static_graph.py
9	10	vllm/compilation/collective_fusion.py
17	16	vllm/compilation/compiler_interface.py
6	5	vllm/compilation/cuda_graph.py
12	11	vllm/compilation/decorators.py
5	6	vllm/compilation/fix_functionalization.py
3	6	vllm/compilation/fx_utils.py
7	8	vllm/compilation/inductor_pass.py
2	5	vllm/compilation/noop_elimination.py
2	4	vllm/compilation/partition_rules.py
2	1	vllm/compilation/piecewise_backend.py
3	4	vllm/compilation/sequence_parallelism.py
2	2	vllm/compilation/torch25_custom_graph_pass.py
2	2	vllm/compilation/vllm_inductor_pass.py
2	2	vllm/compilation/wrapper.py
10	10	vllm/config/cache.py
10	9	vllm/config/compilation.py
2	2	vllm/config/device.py
1	2	vllm/config/kv_events.py
7	7	vllm/config/kv_transfer.py
9	13	vllm/config/load.py
4	4	vllm/config/lora.py
62	72	vllm/config/model.py
14	14	vllm/config/multimodal.py
4	4	vllm/config/observability.py
13	13	vllm/config/parallel.py
11	11	vllm/config/pooler.py
2	2	vllm/config/scheduler.py
15	15	vllm/config/speculative.py
1	2	vllm/config/speech_to_text.py
16	16	vllm/config/vllm.py
14	15	vllm/connections.py
7	6	vllm/device_allocator/cumem.py
3	3	vllm/distributed/communication_op.py
2	2	vllm/distributed/device_communicators/all2all.py
4	4	vllm/distributed/device_communicators/all_reduce_utils.py
11	12	vllm/distributed/device_communicators/base_device_communicator.py
12	12	vllm/distributed/device_communicators/cpu_communicator.py
12	13	vllm/distributed/device_communicators/cuda_communicator.py
3	3	vllm/distributed/device_communicators/cuda_wrapper.py
6	7	vllm/distributed/device_communicators/custom_all_reduce.py
3	4	vllm/distributed/device_communicators/pynccl.py
2	2	vllm/distributed/device_communicators/pynccl_allocator.py
2	2	vllm/distributed/device_communicators/pynccl_wrapper.py
1	4	vllm/distributed/device_communicators/quick_all_reduce.py
6	6	vllm/distributed/device_communicators/ray_communicator.py
15	15	vllm/distributed/device_communicators/shm_broadcast.py
8	10	vllm/distributed/device_communicators/shm_object_storage.py
5	6	vllm/distributed/device_communicators/symm_mem.py
2	3	vllm/distributed/device_communicators/tpu_communicator.py
3	4	vllm/distributed/device_communicators/xpu_communicator.py
7	8	vllm/distributed/eplb/eplb_state.py
1	2	vllm/distributed/eplb/rebalance_execute.py
15	14	vllm/distributed/kv_events.py
2	1	vllm/distributed/kv_transfer/kv_connector/factory.py
5	5	vllm/distributed/kv_transfer/kv_connector/utils.py
9	9	vllm/distributed/kv_transfer/kv_connector/v1/base.py
4	4	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
3	3	vllm/distributed/kv_transfer/kv_connector/v1/metrics.py
11	11	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
16	16	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
6	6	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
3	3	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
6	7	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
4	5	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
3	4	vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py
5	6	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
2	3	vllm/distributed/kv_transfer/kv_pipe/base.py
6	7	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
10	10	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
3	3	vllm/distributed/kv_transfer/kv_transfer_state.py
43	44	vllm/distributed/parallel_state.py
3	3	vllm/distributed/tpu_distributed_utils.py
3	3	vllm/distributed/utils.py
71	68	vllm/engine/arg_utils.py
13	13	vllm/engine/metrics.py
16	16	vllm/engine/protocol.py
3	3	vllm/entrypoints/api_server.py
104	107	vllm/entrypoints/chat_utils.py
2	2	vllm/entrypoints/cli/benchmark/main.py
2	2	vllm/entrypoints/cli/collect_env.py
0	2	vllm/entrypoints/cli/main.py
2	2	vllm/entrypoints/cli/openai.py
2	2	vllm/entrypoints/cli/run_batch.py
1	2	vllm/entrypoints/cli/serve.py
2	2	vllm/entrypoints/cli/types.py
8	8	vllm/entrypoints/context.py
5	7	vllm/entrypoints/harmony_utils.py
2	2	vllm/entrypoints/launcher.py
113	121	vllm/entrypoints/llm.py
8	9	vllm/entrypoints/logger.py
20	20	vllm/entrypoints/openai/api_server.py
16	16	vllm/entrypoints/openai/cli_args.py
4	5	vllm/entrypoints/openai/logits_processors.py
339	339	vllm/entrypoints/openai/protocol.py
2	3	vllm/entrypoints/openai/run_batch.py
22	22	vllm/entrypoints/openai/serving_chat.py
6	6	vllm/entrypoints/openai/serving_classification.py
16	16	vllm/entrypoints/openai/serving_completion.py
17	19	vllm/entrypoints/openai/serving_embedding.py
94	96	vllm/entrypoints/openai/serving_engine.py
9	10	vllm/entrypoints/openai/serving_models.py
7	7	vllm/entrypoints/openai/serving_pooling.py
36	36	vllm/entrypoints/openai/serving_responses.py
29	29	vllm/entrypoints/openai/serving_score.py
7	7	vllm/entrypoints/openai/serving_tokenization.py
4	5	vllm/entrypoints/openai/serving_transcription.py
10	15	vllm/entrypoints/openai/speech_to_text.py
6	7	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
1	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
1	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/hunyuan_a13b_tool_parser.py
1	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py
4	4	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
1	2	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
7	7	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py
2	3	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
4	4	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
6	6	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py
8	10	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py
4	4	vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py
38	43	vllm/entrypoints/renderer.py
17	17	vllm/entrypoints/score_utils.py
4	4	vllm/entrypoints/ssl.py
6	6	vllm/entrypoints/tool_server.py
6	6	vllm/entrypoints/utils.py
43	42	vllm/envs.py
19	19	vllm/executor/executor_base.py
11	10	vllm/executor/ray_distributed_executor.py
3	3	vllm/executor/ray_utils.py
6	5	vllm/executor/uniproc_executor.py
16	16	vllm/forward_context.py
26	24	vllm/inputs/data.py
10	10	vllm/inputs/parse.py
42	44	vllm/inputs/preprocess.py
2	2	vllm/logger.py
2	3	vllm/logging_utils/dump_input.py
6	6	vllm/logits_process.py
3	4	vllm/logprobs.py
8	8	vllm/lora/layers/base.py
5	8	vllm/lora/layers/base_linear.py
27	36	vllm/lora/layers/column_parallel_linear.py
7	8	vllm/lora/layers/logits_processor.py
2	3	vllm/lora/layers/replicated_linear.py
6	9	vllm/lora/layers/row_parallel_linear.py
6	7	vllm/lora/layers/vocal_parallel_embedding.py
8	8	vllm/lora/lora_weights.py
25	24	vllm/lora/models.py
1	2	vllm/lora/ops/triton_ops/lora_kernel_metadata.py
6	6	vllm/lora/peft_helper.py
24	24	vllm/lora/punica_wrapper/punica_base.py
6	6	vllm/lora/punica_wrapper/punica_cpu.py
5	5	vllm/lora/punica_wrapper/punica_gpu.py
8	8	vllm/lora/punica_wrapper/punica_tpu.py
5	5	vllm/lora/punica_wrapper/punica_xpu.py
3	3	vllm/lora/punica_wrapper/utils.py
4	5	vllm/lora/request.py
1	2	vllm/lora/resolver.py
4	4	vllm/lora/utils.py
3	3	vllm/lora/worker_manager.py
1	2	vllm/model_executor/custom_op.py
1	2	vllm/model_executor/layers/activation.py
4	6	vllm/model_executor/layers/batch_invariant.py
3	4	vllm/model_executor/layers/fla/ops/chunk.py
3	4	vllm/model_executor/layers/fla/ops/chunk_delta_h.py
3	4	vllm/model_executor/layers/fla/ops/chunk_o.py
2	3	vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py
6	7	vllm/model_executor/layers/fla/ops/cumsum.py
9	10	vllm/model_executor/layers/fla/ops/fused_recurrent.py
1	2	vllm/model_executor/layers/fla/ops/l2norm.py
6	7	vllm/model_executor/layers/fla/ops/layernorm_guard.py
1	2	vllm/model_executor/layers/fla/ops/solve_tril.py
3	2	vllm/model_executor/layers/fla/ops/utils.py
1	2	vllm/model_executor/layers/fla/ops/wy_fast.py
3	3	vllm/model_executor/layers/fused_moe/__init__.py
5	6	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
5	6	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
64	64	vllm/model_executor/layers/fused_moe/config.py
21	21	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
27	27	vllm/model_executor/layers/fused_moe/cutlass_moe.py
9	10	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
7	8	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py
14	14	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
10	10	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
10	11	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
4	5	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
6	7	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
20	22	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
36	38	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
85	84	vllm/model_executor/layers/fused_moe/fused_moe.py
10	11	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
108	108	vllm/model_executor/layers/fused_moe/layer.py
50	49	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	2	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
11	16	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
13	13	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
3	4	vllm/model_executor/layers/fused_moe/prepare_finalize.py
25	26	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
4	4	vllm/model_executor/layers/fused_moe/routing_simulator.py
2	3	vllm/model_executor/layers/fused_moe/shared_fused_moe.py
4	5	vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
5	6	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
5	6	vllm/model_executor/layers/fused_moe/trtllm_moe.py
23	24	vllm/model_executor/layers/fused_moe/utils.py
16	18	vllm/model_executor/layers/layernorm.py
1	2	vllm/model_executor/layers/lightning_attn.py
25	25	vllm/model_executor/layers/linear.py
6	8	vllm/model_executor/layers/logits_processor.py
7	7	vllm/model_executor/layers/mamba/linear_attn.py
7	7	vllm/model_executor/layers/mamba/mamba_mixer.py
10	10	vllm/model_executor/layers/mamba/mamba_mixer2.py
6	7	vllm/model_executor/layers/mamba/mamba_utils.py
15	16	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
3	3	vllm/model_executor/layers/mamba/short_conv.py
10	11	vllm/model_executor/layers/mla.py
27	27	vllm/model_executor/layers/pooler.py
3	3	vllm/model_executor/layers/quantization/auto_round.py
4	4	vllm/model_executor/layers/quantization/awq.py
15	14	vllm/model_executor/layers/quantization/awq_marlin.py
4	4	vllm/model_executor/layers/quantization/base_config.py
6	6	vllm/model_executor/layers/quantization/bitblas.py
17	16	vllm/model_executor/layers/quantization/bitsandbytes.py
13	13	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
63	63	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
6	5	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
5	5	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
5	5	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
7	8	vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/transform/module.py
2	3	vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
5	6	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
1	1	vllm/model_executor/layers/quantization/deepspeedfp.py
12	11	vllm/model_executor/layers/quantization/experts_int8.py
1	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py
19	18	vllm/model_executor/layers/quantization/fp8.py
5	5	vllm/model_executor/layers/quantization/fp_quant.py
16	15	vllm/model_executor/layers/quantization/gguf.py
6	6	vllm/model_executor/layers/quantization/gptq.py
3	3	vllm/model_executor/layers/quantization/gptq_bitblas.py
17	16	vllm/model_executor/layers/quantization/gptq_marlin.py
2	2	vllm/model_executor/layers/quantization/gptq_marlin_24.py
2	2	vllm/model_executor/layers/quantization/hqq_marlin.py
6	7	vllm/model_executor/layers/quantization/input_quant_fp8.py
18	17	vllm/model_executor/layers/quantization/ipex_quant.py
9	9	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
1	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
2	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark.py
6	7	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
3	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/conch.py
2	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py
2	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py
2	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py
2	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
2	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
5	6	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
1	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
4	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
4	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py
2	3	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
2	3	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
4	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
33	32	vllm/model_executor/layers/quantization/modelopt.py
14	13	vllm/model_executor/layers/quantization/moe_wna16.py
21	20	vllm/model_executor/layers/quantization/mxfp4.py
5	5	vllm/model_executor/layers/quantization/petit.py
2	2	vllm/model_executor/layers/quantization/ptpc_fp8.py
11	11	vllm/model_executor/layers/quantization/quark/quark.py
22	21	vllm/model_executor/layers/quantization/quark/quark_moe.py
7	6	vllm/model_executor/layers/quantization/quark/schemes/quark_ocp_mx.py
1	2	vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py
5	4	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
4	4	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
2	2	vllm/model_executor/layers/quantization/quark/utils.py
13	12	vllm/model_executor/layers/quantization/rtn.py
1	3	vllm/model_executor/layers/quantization/schema.py
2	2	vllm/model_executor/layers/quantization/torchao.py
1	1	vllm/model_executor/layers/quantization/tpu_int8.py
6	7	vllm/model_executor/layers/quantization/utils/bitblas_utils.py
0	2	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
7	8	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
20	20	vllm/model_executor/layers/quantization/utils/fp8_utils.py
7	7	vllm/model_executor/layers/quantization/utils/gptq_utils.py
4	4	vllm/model_executor/layers/quantization/utils/int8_utils.py
1	2	vllm/model_executor/layers/quantization/utils/layer_utils.py
1	2	vllm/model_executor/layers/quantization/utils/machete_utils.py
9	10	vllm/model_executor/layers/quantization/utils/marlin_utils.py
2	3	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
1	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
1	3	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
10	9	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	4	vllm/model_executor/layers/quantization/utils/ocp_mx_utils.py
4	4	vllm/model_executor/layers/quantization/utils/petit_utils.py
6	6	vllm/model_executor/layers/quantization/utils/quant_utils.py
9	9	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
9	11	vllm/model_executor/layers/resampler.py
4	4	vllm/model_executor/layers/rotary_embedding/__init__.py
8	10	vllm/model_executor/layers/rotary_embedding/base.py
2	2	vllm/model_executor/layers/rotary_embedding/common.py
6	7	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
2	3	vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py
4	5	vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope.py
1	2	vllm/model_executor/layers/rotary_embedding/linear_scaling_rope.py
4	5	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
19	20	vllm/model_executor/layers/rotary_embedding/mrope.py
1	2	vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope.py
5	6	vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope.py
6	6	vllm/model_executor/layers/utils.py
8	9	vllm/model_executor/layers/vocab_parallel_embedding.py
2	2	vllm/model_executor/model_loader/__init__.py
5	5	vllm/model_executor/model_loader/bitsandbytes_loader.py
5	5	vllm/model_executor/model_loader/default_loader.py
1	2	vllm/model_executor/model_loader/runai_streamer_loader.py
4	4	vllm/model_executor/model_loader/sharded_state_loader.py
23	25	vllm/model_executor/model_loader/tensorizer.py
1	2	vllm/model_executor/model_loader/tensorizer_loader.py
2	3	vllm/model_executor/model_loader/tpu.py
3	4	vllm/model_executor/model_loader/utils.py
18	18	vllm/model_executor/model_loader/weight_utils.py
4	4	vllm/model_executor/models/adapters.py
2	3	vllm/model_executor/models/aimv2.py
18	20	vllm/model_executor/models/apertus.py
13	15	vllm/model_executor/models/arcee.py
15	16	vllm/model_executor/models/arctic.py
14	14	vllm/model_executor/models/aria.py
10	10	vllm/model_executor/models/aya_vision.py
13	14	vllm/model_executor/models/baichuan.py
16	17	vllm/model_executor/models/bailing_moe.py
14	15	vllm/model_executor/models/bamba.py
24	25	vllm/model_executor/models/bert.py
18	19	vllm/model_executor/models/bert_with_rope.py
10	11	vllm/model_executor/models/blip.py
21	21	vllm/model_executor/models/blip2.py
12	13	vllm/model_executor/models/bloom.py
24	24	vllm/model_executor/models/chameleon.py
15	16	vllm/model_executor/models/chatglm.py
44	44	vllm/model_executor/models/clip.py
9	9	vllm/model_executor/models/cohere2_vision.py
16	17	vllm/model_executor/models/commandr.py
18	19	vllm/model_executor/models/dbrx.py
16	16	vllm/model_executor/models/deepseek.py
2	3	vllm/model_executor/models/deepseek_eagle.py
6	7	vllm/model_executor/models/deepseek_mtp.py
35	35	vllm/model_executor/models/deepseek_v2.py
15	17	vllm/model_executor/models/deepseek_vl2.py
16	16	vllm/model_executor/models/dots1.py
22	22	vllm/model_executor/models/dots_ocr.py
17	17	vllm/model_executor/models/ernie45_moe.py
34	34	vllm/model_executor/models/ernie45_vl.py
19	19	vllm/model_executor/models/ernie45_vl_moe.py
4	5	vllm/model_executor/models/ernie_mtp.py
19	19	vllm/model_executor/models/exaone.py
16	16	vllm/model_executor/models/exaone4.py
13	13	vllm/model_executor/models/falcon.py
16	17	vllm/model_executor/models/falcon_h1.py
2	4	vllm/model_executor/models/flex_olmo.py
8	8	vllm/model_executor/models/fuyu.py
17	18	vllm/model_executor/models/gemma.py
15	16	vllm/model_executor/models/gemma2.py
15	16	vllm/model_executor/models/gemma3.py
11	11	vllm/model_executor/models/gemma3_mm.py
23	24	vllm/model_executor/models/gemma3n.py
10	11	vllm/model_executor/models/glm4.py
26	26	vllm/model_executor/models/glm4_1v.py
18	18	vllm/model_executor/models/glm4_moe.py
8	9	vllm/model_executor/models/glm4_moe_mtp.py
22	22	vllm/model_executor/models/glm4v.py
14	15	vllm/model_executor/models/gpt2.py
12	13	vllm/model_executor/models/gpt_bigcode.py
12	13	vllm/model_executor/models/gpt_j.py
12	13	vllm/model_executor/models/gpt_neox.py
7	8	vllm/model_executor/models/gpt_oss.py
15	15	vllm/model_executor/models/granite.py
14	14	vllm/model_executor/models/granite_speech.py
13	13	vllm/model_executor/models/granitemoe.py
16	17	vllm/model_executor/models/granitemoehybrid.py
8	9	vllm/model_executor/models/granitemoeshared.py
6	7	vllm/model_executor/models/gritlm.py
15	16	vllm/model_executor/models/grok1.py
26	27	vllm/model_executor/models/h2ovl.py
23	23	vllm/model_executor/models/hunyuan_v1.py
25	25	vllm/model_executor/models/hyperclovax_vision.py
10	11	vllm/model_executor/models/idefics2_vision_model.py
22	24	vllm/model_executor/models/idefics3.py
67	65	vllm/model_executor/models/interfaces.py
11	15	vllm/model_executor/models/interfaces_base.py
12	13	vllm/model_executor/models/intern_vit.py
17	17	vllm/model_executor/models/internlm2.py
8	9	vllm/model_executor/models/internlm2_ve.py
19	23	vllm/model_executor/models/interns1.py
10	11	vllm/model_executor/models/interns1_vit.py
61	61	vllm/model_executor/models/internvl.py
12	13	vllm/model_executor/models/jais.py
19	20	vllm/model_executor/models/jamba.py
4	5	vllm/model_executor/models/jina_vl.py
76	100	vllm/model_executor/models/keye.py
23	19	vllm/model_executor/models/keye_vl1_5.py
10	10	vllm/model_executor/models/kimi_vl.py
18	18	vllm/model_executor/models/lfm2.py
18	18	vllm/model_executor/models/lfm2_moe.py
18	20	vllm/model_executor/models/llama.py
6	6	vllm/model_executor/models/llama4.py
5	6	vllm/model_executor/models/llama4_eagle.py
2	3	vllm/model_executor/models/llama_eagle.py
8	9	vllm/model_executor/models/llama_eagle3.py
28	28	vllm/model_executor/models/llava.py
17	15	vllm/model_executor/models/llava_next.py
11	11	vllm/model_executor/models/llava_next_video.py
23	23	vllm/model_executor/models/llava_onevision.py
13	14	vllm/model_executor/models/longcat_flash.py
8	9	vllm/model_executor/models/longcat_flash_mtp.py
9	10	vllm/model_executor/models/mamba.py
8	9	vllm/model_executor/models/mamba2.py
26	26	vllm/model_executor/models/midashenglm.py
4	5	vllm/model_executor/models/mimo.py
6	7	vllm/model_executor/models/mimo_mtp.py
19	23	vllm/model_executor/models/minicpm.py
6	6	vllm/model_executor/models/minicpm3.py
7	8	vllm/model_executor/models/minicpm_eagle.py
16	14	vllm/model_executor/models/minicpmo.py
37	37	vllm/model_executor/models/minicpmv.py
18	18	vllm/model_executor/models/minimax_text_01.py
19	19	vllm/model_executor/models/minimax_vl_01.py
16	16	vllm/model_executor/models/mistral3.py
16	17	vllm/model_executor/models/mixtral.py
17	17	vllm/model_executor/models/mllama4.py
13	14	vllm/model_executor/models/modernbert.py
4	5	vllm/model_executor/models/module_mapping.py
40	40	vllm/model_executor/models/molmo.py
7	8	vllm/model_executor/models/moonvit.py
12	13	vllm/model_executor/models/mpt.py
45	45	vllm/model_executor/models/nano_nemotron_vl.py
18	18	vllm/model_executor/models/nemotron.py
21	22	vllm/model_executor/models/nemotron_h.py
17	17	vllm/model_executor/models/nemotron_nas.py
18	19	vllm/model_executor/models/nemotron_vl.py
3	4	vllm/model_executor/models/nvlm_d.py
13	14	vllm/model_executor/models/olmo.py
7	8	vllm/model_executor/models/olmo2.py
10	11	vllm/model_executor/models/olmoe.py
16	17	vllm/model_executor/models/opt.py
14	14	vllm/model_executor/models/orion.py
11	11	vllm/model_executor/models/ovis.py
13	13	vllm/model_executor/models/ovis2_5.py
14	12	vllm/model_executor/models/paligemma.py
11	12	vllm/model_executor/models/persimmon.py
12	13	vllm/model_executor/models/phi.py
17	17	vllm/model_executor/models/phi3v.py
24	24	vllm/model_executor/models/phi4_multimodal.py
16	16	vllm/model_executor/models/phi4mm.py
38	38	vllm/model_executor/models/phi4mm_audio.py
17	20	vllm/model_executor/models/phi4mm_utils.py
16	17	vllm/model_executor/models/phimoe.py
31	31	vllm/model_executor/models/pixtral.py
9	9	vllm/model_executor/models/plamo2.py
15	15	vllm/model_executor/models/qwen.py
16	16	vllm/model_executor/models/qwen2.py
24	24	vllm/model_executor/models/qwen2_5_omni_thinker.py
32	32	vllm/model_executor/models/qwen2_5_vl.py
14	14	vllm/model_executor/models/qwen2_audio.py
19	24	vllm/model_executor/models/qwen2_moe.py
3	4	vllm/model_executor/models/qwen2_rm.py
38	38	vllm/model_executor/models/qwen2_vl.py
13	13	vllm/model_executor/models/qwen3.py
16	23	vllm/model_executor/models/qwen3_moe.py
13	14	vllm/model_executor/models/qwen3_next.py
5	6	vllm/model_executor/models/qwen3_next_mtp.py
30	30	vllm/model_executor/models/qwen3_omni_moe_thinker.py
34	36	vllm/model_executor/models/qwen3_vl.py
5	6	vllm/model_executor/models/qwen3_vl_moe.py
28	28	vllm/model_executor/models/qwen_vl.py
19	19	vllm/model_executor/models/radio.py
23	23	vllm/model_executor/models/registry.py
8	9	vllm/model_executor/models/roberta.py
1	2	vllm/model_executor/models/rvl.py
14	15	vllm/model_executor/models/seed_oss.py
17	18	vllm/model_executor/models/siglip.py
9	10	vllm/model_executor/models/siglip2navit.py
38	38	vllm/model_executor/models/skyworkr1v.py
1	4	vllm/model_executor/models/smolvlm.py
15	15	vllm/model_executor/models/solar.py
12	13	vllm/model_executor/models/stablelm.py
12	13	vllm/model_executor/models/starcoder2.py
14	14	vllm/model_executor/models/step3_text.py
22	22	vllm/model_executor/models/step3_vl.py
25	26	vllm/model_executor/models/swin.py
24	23	vllm/model_executor/models/tarsier.py
16	16	vllm/model_executor/models/terratorch.py
23	23	vllm/model_executor/models/transformers.py
4	6	vllm/model_executor/models/transformers_pooling.py
19	17	vllm/model_executor/models/ultravox.py
16	16	vllm/model_executor/models/utils.py
13	13	vllm/model_executor/models/vision.py
19	19	vllm/model_executor/models/voxtral.py
28	28	vllm/model_executor/models/whisper.py
21	21	vllm/model_executor/models/zamba2.py
12	13	vllm/model_executor/parameter.py
2	2	vllm/model_executor/utils.py
2	2	vllm/multimodal/audio.py
20	20	vllm/multimodal/cache.py
1	2	vllm/multimodal/evs.py
2	3	vllm/multimodal/hasher.py
1	2	vllm/multimodal/image.py
21	11	vllm/multimodal/inputs.py
22	24	vllm/multimodal/parse.py
69	62	vllm/multimodal/processing.py
15	15	vllm/multimodal/profiling.py
15	15	vllm/multimodal/registry.py
12	12	vllm/multimodal/utils.py
2	2	vllm/multimodal/video.py
16	16	vllm/outputs.py
6	6	vllm/platforms/__init__.py
3	3	vllm/platforms/cpu.py
6	5	vllm/platforms/cuda.py
20	22	vllm/platforms/interface.py
5	5	vllm/platforms/rocm.py
3	3	vllm/platforms/tpu.py
4	4	vllm/platforms/xpu.py
2	1	vllm/plugins/__init__.py
0	2	vllm/plugins/io_processors/__init__.py
7	7	vllm/plugins/io_processors/interface.py
1	2	vllm/plugins/lora_resolvers/filesystem_resolver.py
9	9	vllm/pooling_params.py
10	9	vllm/profiler/layerwise_profile.py
2	2	vllm/profiler/utils.py
3	4	vllm/ray/ray_env.py
8	10	vllm/reasoning/abs_reasoning_parsers.py
3	4	vllm/reasoning/basic_parsers.py
1	2	vllm/reasoning/deepseek_r1_reasoning_parser.py
2	3	vllm/reasoning/glm4_moe_reasoning_parser.py
2	3	vllm/reasoning/gptoss_reasoning_parser.py
3	4	vllm/reasoning/granite_reasoning_parser.py
2	3	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
8	8	vllm/reasoning/olmo3_reasoning_parser.py
2	3	vllm/reasoning/qwen3_reasoning_parser.py
2	3	vllm/reasoning/step3_reasoning_parser.py
51	51	vllm/sampling_params.py
6	7	vllm/scalar_type.py
10	10	vllm/sequence.py
3	4	vllm/tracing.py
6	5	vllm/transformers_utils/chat_templates/registry.py
47	46	vllm/transformers_utils/config.py
3	4	vllm/transformers_utils/config_parser_base.py
2	2	vllm/transformers_utils/configs/dotsocr.py
5	6	vllm/transformers_utils/configs/eagle.py
2	3	vllm/transformers_utils/configs/kimi_vl.py
1	2	vllm/transformers_utils/configs/lfm2_moe.py
2	3	vllm/transformers_utils/configs/medusa.py
6	7	vllm/transformers_utils/configs/midashenglm.py
1	2	vllm/transformers_utils/configs/mlp_speculator.py
4	4	vllm/transformers_utils/configs/ovis.py
3	5	vllm/transformers_utils/configs/radio.py
2	2	vllm/transformers_utils/configs/speculators/base.py
4	4	vllm/transformers_utils/configs/step3_vl.py
5	5	vllm/transformers_utils/configs/ultravox.py
2	3	vllm/transformers_utils/detokenizer_utils.py
8	9	vllm/transformers_utils/dynamic_module.py
11	11	vllm/transformers_utils/processor.py
4	4	vllm/transformers_utils/processors/ovis.py
9	9	vllm/transformers_utils/processors/ovis2_5.py
2	3	vllm/transformers_utils/runai_utils.py
3	3	vllm/transformers_utils/s3_utils.py
9	9	vllm/transformers_utils/tokenizer.py
10	12	vllm/transformers_utils/tokenizer_base.py
14	16	vllm/transformers_utils/tokenizers/mistral.py
7	7	vllm/transformers_utils/utils.py
21	21	vllm/usage/usage_lib.py
33	31	vllm/utils/__init__.py
9	15	vllm/utils/cache.py
2	3	vllm/utils/deep_gemm.py
2	3	vllm/utils/flashinfer.py
2	2	vllm/utils/gc_utils.py
27	30	vllm/utils/jsontree.py
12	10	vllm/utils/tensor_schema.py
29	29	vllm/v1/attention/backends/cpu_attn.py
24	25	vllm/v1/attention/backends/flash_attn.py
8	10	vllm/v1/attention/backends/flashinfer.py
21	22	vllm/v1/attention/backends/flex_attention.py
13	16	vllm/v1/attention/backends/gdn_attn.py
1	2	vllm/v1/attention/backends/mamba1_attn.py
7	8	vllm/v1/attention/backends/mamba2_attn.py
37	32	vllm/v1/attention/backends/mla/common.py
8	8	vllm/v1/attention/backends/mla/cutlass_mla.py
9	9	vllm/v1/attention/backends/mla/flashattn_mla.py
9	9	vllm/v1/attention/backends/mla/flashinfer_mla.py
9	9	vllm/v1/attention/backends/mla/flashmla.py
10	10	vllm/v1/attention/backends/mla/flashmla_sparse.py
4	4	vllm/v1/attention/backends/mla/indexer.py
12	12	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
6	7	vllm/v1/attention/backends/mla/triton_mla.py
8	9	vllm/v1/attention/backends/pallas.py
14	15	vllm/v1/attention/backends/rocm_aiter_fa.py
8	10	vllm/v1/attention/backends/rocm_aiter_unified_attn.py
14	14	vllm/v1/attention/backends/rocm_attn.py
4	5	vllm/v1/attention/backends/short_conv_attn.py
12	12	vllm/v1/attention/backends/tree_attn.py
15	15	vllm/v1/attention/backends/triton_attn.py
10	12	vllm/v1/attention/backends/utils.py
9	9	vllm/v1/attention/backends/xformers.py
7	7	vllm/v1/core/block_pool.py
2	3	vllm/v1/core/kv_cache_coordinator.py
7	7	vllm/v1/core/kv_cache_manager.py
10	10	vllm/v1/core/kv_cache_utils.py
0	2	vllm/v1/core/sched/async_scheduler.py
2	2	vllm/v1/core/sched/interface.py
11	6	vllm/v1/core/sched/output.py
1	3	vllm/v1/core/sched/request_queue.py
2	4	vllm/v1/core/sched/scheduler.py
1	2	vllm/v1/core/sched/utils.py
1	2	vllm/v1/cudagraph_dispatcher.py
27	27	vllm/v1/engine/__init__.py
35	35	vllm/v1/engine/async_llm.py
2	3	vllm/v1/engine/coordinator.py
24	24	vllm/v1/engine/core.py
50	52	vllm/v1/engine/core_client.py
5	6	vllm/v1/engine/detokenizer.py
21	21	vllm/v1/engine/llm_engine.py
9	10	vllm/v1/engine/logprobs.py
43	47	vllm/v1/engine/output_processor.py
1	1	vllm/v1/engine/parallel_sampling.py
17	17	vllm/v1/engine/processor.py
14	14	vllm/v1/engine/utils.py
7	6	vllm/v1/executor/abstract.py
16	15	vllm/v1/executor/multiproc_executor.py
2	3	vllm/v1/executor/ray_distributed_executor.py
6	7	vllm/v1/kv_cache_interface.py
1	2	vllm/v1/kv_offload/abstract.py
2	3	vllm/v1/kv_offload/cpu.py
2	1	vllm/v1/kv_offload/factory.py
2	3	vllm/v1/kv_offload/lru_manager.py
25	30	vllm/v1/metrics/loggers.py
1	2	vllm/v1/metrics/prometheus.py
11	12	vllm/v1/metrics/ray_wrappers.py
1	2	vllm/v1/metrics/reader.py
11	11	vllm/v1/metrics/stats.py
13	11	vllm/v1/outputs.py
2	3	vllm/v1/pool/metadata.py
22	22	vllm/v1/request.py
8	8	vllm/v1/sample/logits_processor/__init__.py
9	9	vllm/v1/sample/logits_processor/builtin.py
1	1	vllm/v1/sample/logits_processor/interface.py
8	10	vllm/v1/sample/logits_processor/state.py
7	8	vllm/v1/sample/metadata.py
13	14	vllm/v1/sample/ops/topk_topp_sampler.py
4	5	vllm/v1/sample/rejection_sampler.py
2	4	vllm/v1/sample/sampler.py
1	2	vllm/v1/sample/tpu/metadata.py
2	4	vllm/v1/sample/tpu/sampler.py
10	10	vllm/v1/serial_utils.py
5	6	vllm/v1/spec_decode/eagle.py
1	2	vllm/v1/spec_decode/metrics.py
4	3	vllm/v1/structured_output/__init__.py
4	6	vllm/v1/structured_output/backend_guidance.py
1	3	vllm/v1/structured_output/backend_lm_format_enforcer.py
0	2	vllm/v1/structured_output/backend_outlines.py
5	4	vllm/v1/structured_output/backend_types.py
0	2	vllm/v1/structured_output/backend_xgrammar.py
4	10	vllm/v1/structured_output/request.py
4	3	vllm/v1/structured_output/utils.py
10	12	vllm/v1/utils.py
1	2	vllm/v1/worker/block_table.py
2	2	vllm/v1/worker/cpu_model_runner.py
2	2	vllm/v1/worker/cpu_worker.py
5	6	vllm/v1/worker/dp_utils.py
22	22	vllm/v1/worker/gpu_input_batch.py
41	42	vllm/v1/worker/gpu_model_runner.py
5	4	vllm/v1/worker/gpu_ubatch_wrapper.py
9	9	vllm/v1/worker/gpu_worker.py
3	4	vllm/v1/worker/kv_connector_model_runner_mixin.py
5	6	vllm/v1/worker/lora_model_runner_mixin.py
9	9	vllm/v1/worker/tpu_input_batch.py
7	7	vllm/v1/worker/tpu_model_runner.py
4	3	vllm/v1/worker/tpu_worker.py
1	1	vllm/v1/worker/ubatch_utils.py
5	5	vllm/v1/worker/utils.py
6	4	vllm/v1/worker/worker_base.py

[9bb38130c] Chendi.Xue 2025-10-12 [Bugfix] Fix GPU_ID issue in test script (#26442)
12	1	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh

[b91d8db87] Jaya Yuan 2025-10-12 [Bugfix][DCP] Set default CUDAGraphMode to PIECEWISE for DCP (#26574)
9	0	vllm/config/vllm.py

[045b396d0] Isotr0py 2025-10-12 [Bugfix][CI/Build] Fix failing Mteb CI (#26638)
1	1	tests/models/language/pooling_mteb_test/mteb_utils.py
5	0	tests/models/language/pooling_mteb_test/test_jina.py
1	0	tests/models/language/pooling_mteb_test/test_st_projector.py
1	0	tests/models/utils.py
5	1	vllm/model_executor/layers/layernorm.py

[76852017e] wang.yuqi 2025-10-12 [MISC] Rename the torch profiler filename as instance_id+rank_id for merging the Profiler results of each Rank (#25867)
4	0	vllm/config/vllm.py
2	1	vllm/v1/worker/gpu_worker.py
2	1	vllm/v1/worker/xpu_worker.py

[82e64c7a2] Vadim Gimpelson 2025-10-12 [PERF] [Qwen3-next] Speed up gated RMSNorm (#26207)
388	0	tests/kernels/test_fla_layernorm_guard.py
87	33	vllm/model_executor/layers/fla/ops/layernorm_guard.py

[4ca204055] wang.yuqi 2025-10-12 Add @noooop to codeowner for pooling models (#26652)
8	0	.github/CODEOWNERS

[c5c8f5ea5] Haisheng Chen 2025-10-11 [EPLB] Support ernie4.5-moe (#22100)
132	7	vllm/model_executor/models/ernie45_moe.py

[01653a917] Angela Yi 2025-10-11 [compile] Fix inductor partition config (#26645)
1	3	vllm/config/compilation.py

[0cd103e7c] Huamin Li 2025-10-11 CP: make correct_attn_out robust to 4‑D views and fix Triton arg binding (#26509)
46	8	vllm/attention/ops/common.py

[5be7ca1b9] Cyrus Leung 2025-10-12 [Benchmark] Support Infinity API (#26641)
1	1	vllm/benchmarks/datasets.py
95	28	vllm/benchmarks/lib/endpoint_request_func.py

[f0a30a067] Jee Jee Li 2025-10-11 [Bugfix] Fix qwen-moe packed_modules_mapping (#26634)
1	1	vllm/model_executor/models/interfaces.py
13	5	vllm/model_executor/models/qwen2_moe.py
9	5	vllm/model_executor/models/qwen3_moe.py

[9d6cff3ed] JJJYmmm 2025-10-11 [Bugfix][Qwen3VL] fix deepstack in qwen3vl (#26626)
0	6	vllm/model_executor/models/qwen3_vl.py

[a25f2adee] Angela Yi 2025-10-11 [compile] Add patched_fused_scaled_matmul_reduce_scatter (#26604)
22	4	tests/compile/test_async_tp.py
2	2	vllm/compilation/collective_fusion.py
95	0	vllm/distributed/parallel_state.py

[d0bed837a] Chauncey 2025-10-11 [Refactor]Reduce duplicate code in serving_chat (#26627)
8	36	vllm/entrypoints/openai/serving_chat.py
46	1	vllm/entrypoints/openai/serving_engine.py
2	12	vllm/entrypoints/openai/serving_responses.py

[f7ee69868] muzian666 2025-10-11 [CPU] fix the issue when the node is '-' cause json decode error. (#26562)
2	0	vllm/platforms/cpu.py

[d2a71530c] Rahul Tuli 2025-10-11 Add EAGLE-3 Speculative Decoding Support for Qwen3 MoE (#26485)
33	4	vllm/model_executor/models/qwen3_moe.py

[086609de6] ihb2032 2025-10-11 fix(nix): Allow local oneDNN path to fix vLLM CPU build failure (#26401)
19	8	cmake/cpu_extension.cmake

[727144bed] dsinghvi 2025-10-11 [Refactor]: Use M-RoPE interface directly while defining model class instead of maintaining model specific M-RoPE implementation in mrope.py (#24172)
0	1015	vllm/model_executor/layers/rotary_embedding/mrope.py
149	2	vllm/model_executor/models/ernie45_vl.py
150	2	vllm/model_executor/models/glm4v.py
142	2	vllm/model_executor/models/keye_vl1_5.py
269	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
129	1	vllm/model_executor/models/qwen2_5_vl.py
113	2	vllm/model_executor/models/qwen3_vl.py
8	0	vllm/model_executor/models/utils.py
14	25	vllm/v1/worker/gpu_model_runner.py

[55392bc87] sangho.lee 2025-10-11 [Bugfix][Multi Modal] Fix incorrect Molmo image processing (#26563)
23	17	vllm/model_executor/models/molmo.py

[ddaff2938] Roger Wang 2025-10-10 [MM] Move Qwen3Omni MRoPE impl to model file (#26608)
1	360	vllm/model_executor/layers/rotary_embedding/mrope.py
329	26	vllm/model_executor/models/qwen3_omni_moe_thinker.py
37	0	vllm/model_executor/models/vision.py
1	1	vllm/v1/worker/gpu_model_runner.py

[27ed39a34] liuzhenwei 2025-10-11 [XPU] Upgrade NIXL to remove CUDA dependency (#26570)
0	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh
5	0	docker/Dockerfile.xpu
0	1	requirements/xpu.txt
1	0	tools/install_nixl_from_source_ubuntu.py
8	7	vllm/platforms/xpu.py

[8f8474fbe] Nishidha Panpaliya 2025-10-11 [CI/Build] Fix ppc64le CPU build and tests (#22443)
9	6	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
1	1	cmake/cpu_extension.cmake
45	19	docker/Dockerfile.ppc64le

[be067861c] Chauncey 2025-10-11  [Frontend] Improve the performance of `is_reasoning_end` (#25735)
2	1	vllm/reasoning/basic_parsers.py

[5bc26c438] Nick Hill 2025-10-10 [BugFix] Make penalties and bad_words work with async scheduling (#26467)
19	5	tests/v1/e2e/test_async_sched_and_preempt.py
3	1	vllm/v1/core/sched/scheduler.py
67	1	vllm/v1/worker/gpu_input_batch.py
24	7	vllm/v1/worker/gpu_model_runner.py

[eef921f45] Zhengxu Chen 2025-10-10 AOT Compilation for torch.compile (Bundled) (#24274)
1	0	.buildkite/test-pipeline.yaml
139	0	tests/compile/test_aot_compile.py
1	0	tools/pre_commit/check_pickle_imports.py
16	33	vllm/compilation/backends.py
176	0	vllm/compilation/caching.py
6	0	vllm/compilation/compiler_interface.py
105	8	vllm/compilation/decorators.py
23	0	vllm/compilation/wrapper.py
17	0	vllm/envs.py

[e317414ce] Bram Wasti 2025-10-10 Cache the environment variable check for batch invariance (#26510)
6	3	csrc/core/batch_invariant.hpp

[949cb0170] Nick Hill 2025-10-10 [BugFix] Fix async scheduling + request preemption (#26385)
96	0	tests/v1/e2e/test_async_sched_and_preempt.py
8	3	vllm/v1/worker/gpu_model_runner.py

[e94cfd51d] Vadim Gimpelson 2025-10-10 [BUG] Qwen3-next MTP. Fix attn metadata build bug (#26564)
6	7	vllm/v1/spec_decode/eagle.py

[7c12763b2] Harry Mellor 2025-10-10 Fix some typing issues found by `mypy==1.18.2` (#26596)
2	0	tests/v1/core/test_scheduler.py
3	3	vllm/model_executor/layers/linear.py
6	6	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
4	6	vllm/model_executor/layers/resampler.py
2	3	vllm/model_executor/model_loader/bitsandbytes_loader.py
2	0	vllm/transformers_utils/processors/ovis2_5.py

[3b780a4bb] Will Eaton 2025-10-10 Update CUDA architecture list in build pipeline for 12.9.1 wheels (#26592)
1	1	.buildkite/release-pipeline.yaml

[30f78af14] Harry Mellor 2025-10-10 Update `pre-commit` hook versions (#26591)
4	4	.pre-commit-config.yaml

[19a9b169b] Xiong Wang 2025-10-11 Add Qwen3-Omni moe thinker (#25550)
2	2	docs/models/supported_models.md
1	0	tests/models/multimodal/processing/test_common.py
5	0	tests/models/registry.py
374	34	vllm/model_executor/layers/rotary_embedding/mrope.py
1409	0	vllm/model_executor/models/qwen3_omni_moe_thinker.py
4	0	vllm/model_executor/models/registry.py

[96ad65b7f] Roberto L. Castro 2025-10-10 [Transform] [Quantization] Add QuTLASS support to vLLM (#24440)
2	0	.buildkite/test-pipeline.yaml
1	0	CMakeLists.txt
191	0	benchmarks/kernels/bench_mxfp4_qutlass.py
207	0	benchmarks/kernels/bench_nvfp4_qutlass.py
97	0	cmake/external_projects/qutlass.cmake
303	0	tests/kernels/quantization/test_mxfp4_qutlass.py
268	0	tests/kernels/quantization/test_nvfp4_qutlass.py
32	0	tests/quantization/fp_quant.py
139	1	vllm/_custom_ops.py
3	0	vllm/model_executor/layers/quantization/__init__.py
420	0	vllm/model_executor/layers/quantization/fp_quant.py
185	0	vllm/model_executor/layers/quantization/qutlass_utils.py

[8d2b8c0ff] Shane A 2025-10-10 [Model] Add FlexOlmo model implementation (#24923)
1	0	docs/models/supported_models.md
1	0	tests/models/registry.py
157	0	vllm/model_executor/models/flex_olmo.py
46	46	vllm/model_executor/models/olmoe.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
77	0	vllm/transformers_utils/configs/flex_olmo.py

[b2155ed31] Lukas Geiger 2025-10-10 [Model][Qwen3VL] Compute `cu_seqlens` on CPU to remove  (#26496)
7	4	vllm/model_executor/models/qwen3_vl.py

[910abdbd0] Chauncey 2025-10-11 [Bugfix] fixed top_logprobs: -1 does not appear to work as intended (#26470)
15	0	tests/entrypoints/openai/test_chat_echo.py
1	1	vllm/entrypoints/openai/serving_chat.py

[cddce79fd] baonudesifeizhai 2025-10-10 [torch.compile] Make inductor partition rules respect splitting_ops #25691 (#25845)
2	2	tests/compile/piecewise/test_multiple_graphs.py
2	2	tests/compile/piecewise/test_simple.py
2	2	tests/compile/piecewise/test_toy_llama.py
53	29	tests/compile/test_config.py
3	3	tests/compile/test_decorator.py
44	8	vllm/compilation/backends.py
12	16	vllm/compilation/compiler_interface.py
95	0	vllm/compilation/partition_rules.py
54	50	vllm/config/compilation.py

[e51928192] Mark McLoughlin 2025-10-10 [Metrics] Add test for multi-modal cache stats logging (#26588)
24	0	tests/entrypoints/llm/test_mm_cache_stats.py
1	4	vllm/v1/metrics/loggers.py
5	0	vllm/v1/metrics/stats.py

[7b03584de] Elvir Crnčević 2025-10-10 Silu v2 (#25074)
168	123	benchmarks/kernels/benchmark_silu_mul_fp8_quant.py
2	2	csrc/ops.h
295	233	csrc/quantization/activation_kernels.cu
5	5	csrc/torch_bindings.cpp
6	5	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
36	30	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[ae9d0e7da] Sage Moore 2025-10-10 [Bugfix] Make DP padding optional in coordinate_batch_across_dp (#26375)
14	1	vllm/forward_context.py
80	26	vllm/v1/worker/dp_utils.py
29	15	vllm/v1/worker/gpu_model_runner.py

[0e67102d9] Daniel Cámpora 2025-10-10 Added test_top_k_per_row to test-pipeline.yaml. (#26569)
2	1	.buildkite/test-pipeline.yaml
0	1	tests/kernels/test_top_k_per_row.py

[f4ba2061c] Jason Li 2025-10-10 [BugFix][torch.compile] Fix fused_scaled_matmul_reduce_scatter signature for PyTorch 2.8 (#26038)
2	2	.buildkite/test-pipeline.yaml
22	6	vllm/compilation/collective_fusion.py

[1e6848a65] Chauncey 2025-10-10 [CI] fix test_run_batch.py::test_completions - AssertionError (#26578)
23	17	tests/entrypoints/openai/test_run_batch.py

[67661375f] Andy Lo 2025-10-10 [BugFix] Fix noop elimination edge case (#26394)
12	3	tests/compile/test_noop_elimination.py
28	48	vllm/compilation/noop_elimination.py

[213b64452] Lucas Kabela 2025-10-10 [Bugfix] Convert untraceable GroupShape to list for AMD impl (#26535)
2	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[784c23115] Mark McLoughlin 2025-10-10 [NIXL] Ignore abort on already-finished request (#25067)
48	1	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
11	6	vllm/distributed/kv_transfer/kv_connector/v1/base.py
2	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
3	9	vllm/v1/core/sched/scheduler.py

[606b00e80] Chen Zhang 2025-10-10 [bugfix][DCP] fix block_size of hash in DCP prefix caching (#26296)
1	0	tests/v1/core/test_scheduler.py
1	0	tests/v1/core/utils.py
1	0	tests/v1/kv_connector/unit/utils.py
2	8	vllm/v1/core/sched/scheduler.py
7	2	vllm/v1/engine/core.py

[720d3cd0f] Chauncey 2025-10-10 [CI] fix ruff format (#26579)
80	86	tests/entrypoints/openai/test_serving_chat.py
4	2	vllm/entrypoints/harmony_utils.py

[ab196edef] Ashwin Phadke 2025-10-10 Remove LoRA bias support (#25807)
0	5	tests/entrypoints/openai/test_lora_adapters.py
0	5	tests/lora/test_peft_helper.py
2	13	tests/lora/test_utils.py
1	7	vllm/config/lora.py
0	3	vllm/engine/arg_utils.py
0	1	vllm/lora/layers/base.py
2	38	vllm/lora/layers/base_linear.py
1	66	vllm/lora/layers/column_parallel_linear.py
0	1	vllm/lora/layers/logits_processor.py
1	15	vllm/lora/layers/row_parallel_linear.py
0	1	vllm/lora/layers/vocal_parallel_embedding.py
1	14	vllm/lora/lora_weights.py
3	28	vllm/lora/models.py
3	3	vllm/lora/peft_helper.py
4	44	vllm/lora/punica_wrapper/punica_base.py
4	19	vllm/lora/punica_wrapper/punica_cpu.py
3	20	vllm/lora/punica_wrapper/punica_gpu.py
4	56	vllm/lora/punica_wrapper/punica_tpu.py
3	19	vllm/lora/punica_wrapper/punica_xpu.py
3	8	vllm/lora/utils.py

[3ee202ea1] Luis Tomas Bolivar 2025-10-10 [GPT-OSS] Add support for arrays  at tool message content (#25593)
132	0	tests/entrypoints/openai/test_serving_chat.py
7	0	vllm/entrypoints/harmony_utils.py

[ad430a67c] Cyrus Leung 2025-10-10 [Metrics] Log multi-modal cache stats and fix reset (#26285)
74	0	tests/entrypoints/llm/test_mm_cache_stats.py
115	65	tests/entrypoints/openai/test_metrics.py
3	4	tests/v1/core/test_kv_cache_utils.py
2	1	tests/v1/distributed/test_async_llm_dp.py
4	0	vllm/entrypoints/llm.py
4	0	vllm/entrypoints/openai/serving_engine.py
4	0	vllm/executor/executor_base.py
2	10	vllm/executor/uniproc_executor.py
39	3	vllm/inputs/preprocess.py
46	1	vllm/multimodal/cache.py
1	74	vllm/v1/core/kv_cache_utils.py
3	1	vllm/v1/engine/async_llm.py
5	2	vllm/v1/engine/core.py
3	1	vllm/v1/engine/llm_engine.py
6	2	vllm/v1/engine/processor.py
2	14	vllm/v1/executor/multiproc_executor.py
0	24	vllm/v1/executor/utils.py
82	13	vllm/v1/metrics/loggers.py
109	10	vllm/v1/metrics/stats.py
4	0	vllm/v1/worker/gpu_model_runner.py
3	0	vllm/v1/worker/gpu_worker.py
4	0	vllm/v1/worker/tpu_model_runner.py
3	0	vllm/v1/worker/tpu_worker.py
4	0	vllm/v1/worker/utils.py
66	12	vllm/v1/worker/worker_base.py

[6f0f570c4] Chen Zhang 2025-10-10 [deepseek] kernel block size for UniformTypeKVCacheSpecs (#26559)
10	2	vllm/v1/attention/backends/mla/indexer.py
9	4	vllm/v1/worker/gpu_model_runner.py

[b545a0b20] Boyuan Feng 2025-10-09 fix test_simple_inductor_graph_partition (#26522)
5	1	tests/compile/piecewise/test_simple.py
8	2	vllm/compilation/compiler_interface.py

[29255cfc3] Lucas Wilkinson 2025-10-10 [Spec-Decode] Support piecewise cudagraphs for Eagle head (#25109)
7	4	vllm/config/compilation.py
6	0	vllm/forward_context.py
2	0	vllm/model_executor/models/deepseek_mtp.py
11	0	vllm/model_executor/models/llama_eagle3.py
56	11	vllm/v1/spec_decode/eagle.py
2	1	vllm/v1/worker/gpu_model_runner.py

[da4455609] Ben Browning 2025-10-10 [Chore]: One pythonic tool parser test uses the wrong parser (#26515)
1	1	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py

[aafb99a4d] Nick Hill 2025-10-09 [Core] Small simplification in `GPUModelRunner._update_states()` (#26508)
2	7	vllm/v1/worker/gpu_model_runner.py

[757fa4a4d] Rui Qiao 2025-10-09 [DP][ray] Support different VLLM_RAY_DP_PACK_STRATEGY (#23849)
12	0	vllm/envs.py
74	33	vllm/v1/engine/utils.py

[c6187f55f] Julien Denize 2025-10-10 Refactor MistralTokenizer (#26358)
9	2	docs/features/tool_calling.md
3	1	examples/offline_inference/audio_language.py
1	1	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
1	3	requirements/test.txt
2	28	tests/entrypoints/test_chat_utils.py
1	1	tests/models/multimodal/generation/test_pixtral.py
2	6	tests/models/multimodal/generation/test_voxtral.py
2	1	tests/models/multimodal/processing/test_common.py
2	1	tests/models/multimodal/processing/test_tensor_schema.py
1	27	tests/reasoning/test_mistral_reasoning_parser.py
2077	75	tests/tokenization/test_mistral_tokenizer.py
5	13	vllm/entrypoints/chat_utils.py
2	1	vllm/model_executor/models/pixtral.py
2	6	vllm/model_executor/models/voxtral.py
231	266	vllm/transformers_utils/tokenizers/mistral.py
6	27	vllm/v1/structured_output/backend_xgrammar.py

[8983e0216] Wentao Ye 2025-10-09 [CI] Fix Pre-commit Issue Cannot determine type of "rank" and "world_size" (#26448)
5	0	vllm/distributed/device_communicators/all2all.py

[1ee35382c] Wentao Ye 2025-10-09 [Bug] Fix modular_kernel: ZeroDivisionError: integer division or modulo by zero (#26528)
7	4	vllm/model_executor/layers/fused_moe/modular_kernel.py

[6e783bc54] Benjamin Chislett 2025-10-09 [Bugfix] Fix CUDA graph selection bug in FlashInfer at high concurrency (#26499)
9	2	vllm/v1/attention/backends/flashinfer.py

[c9d33c60d] Michael Goin 2025-10-09 [UX] Add FlashInfer as default CUDA dependency (#26443)
8	69	docker/Dockerfile
2	0	requirements/cuda.txt
1	2	setup.py
9	1	vllm/utils/flashinfer.py

[2e54db4d2] Nick Hill 2025-10-09 [Core] Remove unused `prev_sampled_token_ids_invalid_indices` input batch field (#26514)
0	1	vllm/v1/worker/gpu_input_batch.py
0	3	vllm/v1/worker/gpu_model_runner.py

[44f633dba] elvischenv 2025-10-10 [Flashinfer][gpt-oss] Support FP8-qkv Flashinfer TRTLLM Sinks Attention (#25674)
76	41	tests/kernels/attention/test_flashinfer_trtllm_attention.py
0	5	vllm/utils/flashinfer.py

[a462331e3] bnellnm 2025-10-09 [Bugfix] Disable moe inplace for torch >= 2.9 (#26497)
6	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
6	2	vllm/model_executor/layers/fused_moe/fused_moe.py
2	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
8	1	vllm/model_executor/layers/fused_moe/utils.py

[4069db3f2] roikoren755 2025-10-09 [Bugfix] Enable padded FP4 quantization (#25947)
1	1	vllm/_custom_ops.py
0	2	vllm/utils/flashinfer.py

[0d37450eb] Sage Moore 2025-10-09 [BUGFIX] Add cu_tokens_across_sp to DPMetadata (#26457)
11	0	vllm/forward_context.py

[47e66c24e] bnellnm 2025-10-09 [Model] Apply shared experts overlap optimization to all models with shared experts (#26145)
2	0	vllm/model_executor/layers/fused_moe/__init__.py
23	12	vllm/model_executor/layers/{shared_fused_moe => fused_moe}/shared_fused_moe.py
2	0	vllm/model_executor/layers/quantization/fp8.py
0	5	vllm/model_executor/layers/shared_fused_moe/__init__.py
15	13	vllm/model_executor/models/aria.py
26	21	vllm/model_executor/models/bailing_moe.py
24	45	vllm/model_executor/models/deepseek_v2.py
22	18	vllm/model_executor/models/dots1.py
20	20	vllm/model_executor/models/ernie45_moe.py
34	23	vllm/model_executor/models/ernie45_vl_moe.py
25	43	vllm/model_executor/models/glm4_moe.py
20	21	vllm/model_executor/models/hunyuan_v1.py
4	5	vllm/model_executor/models/llama4.py
30	27	vllm/model_executor/models/qwen2_moe.py
24	30	vllm/model_executor/models/qwen3_next.py

[3b736e1c3] Ming Yang 2025-10-09 [Attention][DCP] Support DCP with query length > 1 (MTP) with FA3 (#25049)
1	1	cmake/external_projects/vllm_flash_attn.cmake
14	2	vllm/v1/attention/backends/mla/common.py
7	9	vllm/v1/attention/backends/mla/flashattn_mla.py
2	0	vllm/v1/attention/backends/mla/flashmla.py
2	0	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
3	0	vllm/v1/attention/backends/utils.py
2	0	vllm/v1/spec_decode/eagle.py
14	1	vllm/v1/worker/gpu_model_runner.py

[2c1c7dfb3] Lukas Geiger 2025-10-09 [Models][Qwen] Replace `pad` with `cat` for better performance (#26486)
1	1	vllm/model_executor/models/dots_ocr.py
3	2	vllm/model_executor/models/ernie45_vl.py
1	1	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/siglip2navit.py

[e246ad6f0] Harry Mellor 2025-10-09 Upgrade Pydantic to v2.12.0 and remove hack for Python 3.13 (#26481)
1	1	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
4	4	requirements/test.txt
1	11	vllm/config/utils.py

[5728da11e] Jiangyun Zhu 2025-10-09 Revert  #26113 "[Frontend] CompilationConfig overhaul (#20283): deprecate use_inductor in favor of backend, simplify custom_ops" (#26472)
20	11	tests/compile/piecewise/test_toy_llama.py
19	21	tests/model_executor/test_enabled_custom_ops.py
1	5	vllm/compilation/backends.py
11	60	vllm/config/compilation.py
0	19	vllm/config/vllm.py
10	10	vllm/model_executor/custom_op.py
2	0	vllm/platforms/cpu.py

[92be3f351] Simon Danielsson 2025-10-09 [Feature] Use pydantic validation in parallel.py config (#26417)
54	55	vllm/config/parallel.py

[d1ddf340c] Isotr0py 2025-10-09 [V0 deprecation] Remove `QKVCrossParallelLinear` implementation  (#26475)
0	8	vllm/lora/layers/qkv_x_parallel_linear.py
1	236	vllm/model_executor/layers/linear.py
1	5	vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py
0	6	vllm/model_executor/model_loader/utils.py

[ec10fd0ab] Wenzheng Bi 2025-10-09 [Bugfix] Move current_platform import to avoid python import cache. (#16601)
6	6	tests/kernels/attention/test_attention_selector.py
2	1	vllm/attention/selector.py

[0426e3c5e] Lukas Geiger 2025-10-09 [Models][Qwen3VL] Optimise `_validate_and_reshape_mm_tensor` (#26426)
1	1	vllm/model_executor/models/qwen3_vl.py

[4bdf7ac59] Cyrus Leung 2025-10-09 [Bugfix] Fix SHM cache initialization (#26427)
5	3	tests/entrypoints/openai/test_lora_resolvers.py
29	16	tests/entrypoints/openai/test_serving_chat.py
3	1	tests/entrypoints/openai/test_serving_engine.py
4	2	tests/entrypoints/openai/test_serving_models.py
9	5	tests/entrypoints/openai/test_serving_responses.py
5	3	tests/test_inputs.py
1	1	tests/v1/engine/test_processor_multi_modal_uuids.py
1	1	tests/v1/sample/test_logprobs.py
1	1	vllm/benchmarks/throughput.py
11	201	vllm/engine/protocol.py
17	42	vllm/entrypoints/llm.py
5	18	vllm/entrypoints/openai/api_server.py
3	12	vllm/entrypoints/openai/run_batch.py
3	6	vllm/entrypoints/openai/serving_chat.py
0	3	vllm/entrypoints/openai/serving_classification.py
1	4	vllm/entrypoints/openai/serving_completion.py
0	3	vllm/entrypoints/openai/serving_embedding.py
199	17	vllm/entrypoints/openai/serving_engine.py
6	6	vllm/entrypoints/openai/serving_models.py
0	6	vllm/entrypoints/openai/serving_pooling.py
1	4	vllm/entrypoints/openai/serving_responses.py
0	3	vllm/entrypoints/openai/serving_score.py
0	3	vllm/entrypoints/openai/serving_tokenization.py
0	5	vllm/entrypoints/openai/serving_transcription.py
1	4	vllm/entrypoints/openai/speech_to_text.py
4	4	vllm/inputs/data.py
7	10	vllm/inputs/preprocess.py
19	16	vllm/v1/engine/async_llm.py
20	17	vllm/v1/engine/llm_engine.py
2	0	vllm/v1/engine/processor.py

[dc7976dd9] Cyrus Leung 2025-10-09 [Misc] Upgrade more code to Python 3.10 (#26463)
1	1	tests/entrypoints/openai/test_chat.py
3	9	tests/entrypoints/test_context.py
1	2	tests/utils_/test_utils.py
2	2	tests/v1/entrypoints/openai/test_completion.py
6	9	vllm/benchmarks/serve.py
2	7	vllm/utils/__init__.py
1	3	vllm/v1/serial_utils.py

[e4791438e] Simon Danielsson 2025-10-09 [Feature] Use pydantic validation in lora.py and load.py configs (#26413)
17	9	vllm/config/load.py
22	35	vllm/config/lora.py
8	0	vllm/config/utils.py
1	1	vllm/engine/arg_utils.py

[e6e898f95] youkaichao 2025-10-09 [doc] add Volcengine as a compute sponsor (#26477)
1	0	README.md
1	0	docs/community/sponsors.md

[ddcbc2f33] Nick Hill 2025-10-09 [Misc] Misc code simplifications (#26450)
1	1	vllm/v1/core/sched/scheduler.py
1	2	vllm/v1/core/sched/utils.py
8	13	vllm/v1/sample/rejection_sampler.py
26	30	vllm/v1/sample/sampler.py
9	11	vllm/v1/worker/gpu_input_batch.py
34	33	vllm/v1/worker/gpu_model_runner.py

[a83ff278d] Jerry Zhang 2025-10-09 [torchao] Add support for ModuleFqnToConfig using regex (#26001)
17	0	tests/quantization/test_torchao.py
21	3	vllm/model_executor/layers/quantization/torchao.py

[cf4cd6c24] Rahul Tuli 2025-10-09 Add: Support for multiple hidden layers in Eagle3 (#26164)
4	0	tests/speculative_decoding/speculators/test_eagle3.py
25	13	vllm/model_executor/models/llama_eagle3.py

[b96044181] Harry Mellor 2025-10-09 Enable `RMSNorm` substitution for Transformers backend (#26353)
28	15	vllm/model_executor/models/transformers.py

[1317028aa] Luciano Martins 2025-10-09 [Model] Gemma3: Fix GGUF loading and quantization (#26189)
4	0	vllm/model_executor/model_loader/gguf_loader.py
10	0	vllm/model_executor/models/gemma3.py

[5e49c3e77] elvischenv 2025-10-09 Bump Flashinfer to v0.4.0 (#26326)
3	3	docker/Dockerfile
2	2	docker/Dockerfile.nightly_torch
1	1	setup.py
9	12	tests/kernels/attention/test_flashinfer_trtllm_attention.py
5	3	tests/kernels/quantization/nvfp4_utils.py
1	1	tests/quantization/test_blackwell_moe.py
4	1	vllm/v1/attention/backends/flashinfer.py

[0d7c3cb51] pwschuurman 2025-10-08 Update Dockerfile and install runai-model-streamer[gcs] package (#26464)
1	1	docker/Dockerfile

[1b2c440cd] Jee Jee Li 2025-10-09 [Core] Relax the LoRA  max rank (#26461)
1	1	vllm/config/lora.py
4	4	vllm/v1/worker/lora_model_runner_mixin.py

[0f29dca98] Cyrus Leung 2025-10-09 [CI/Build] Fix model nightly tests (#26466)
1	1	tests/models/language/generation/test_common.py
1	1	tests/models/language/pooling/test_token_classification.py
1	0	tests/models/multimodal/generation/test_common.py
5	3	tests/models/multimodal/pooling/test_clip.py
1	2	tests/models/registry.py

[d24cf322e] Zhiyuan Li 2025-10-09 [Hybrid]: Decouple Kernel Block Size from KV Page Size (#24486)
3	0	tests/v1/worker/test_gpu_input_batch.py
216	24	tests/v1/worker/test_gpu_model_runner.py
17	1	vllm/attention/backends/abstract.py
32	6	vllm/model_executor/models/config.py
19	7	vllm/platforms/cuda.py
6	1	vllm/v1/attention/backends/flash_attn.py
8	0	vllm/v1/attention/backends/flashinfer.py
5	0	vllm/v1/attention/backends/mla/cutlass_mla.py
5	1	vllm/v1/attention/backends/mla/flashmla.py
6	1	vllm/v1/attention/backends/rocm_aiter_fa.py
6	1	vllm/v1/attention/backends/tree_attn.py
6	1	vllm/v1/attention/backends/triton_attn.py
6	1	vllm/v1/attention/backends/xformers.py
85	4	vllm/v1/worker/block_table.py
2	0	vllm/v1/worker/gpu_input_batch.py
144	6	vllm/v1/worker/gpu_model_runner.py
2	0	vllm/v1/worker/tpu_input_batch.py
4	0	vllm/v1/worker/tpu_model_runner.py

[d17f0fbf3] Qier Li 2025-10-09 [Core][KVConnector] Propagate all tokens on resumed preemptions (#24926)
42	4	tests/v1/core/test_scheduler.py
1	0	tests/v1/worker/test_gpu_model_runner.py
4	0	vllm/v1/core/sched/output.py
13	5	vllm/v1/core/sched/scheduler.py

[43ab8cfaa] Wenlong Wang 2025-10-08 [MM][Doc] Add documentation for configurable mm profiling (#26200)
40	0	docs/configuration/conserving_memory.md

[de253d63b] Matt 2025-10-09 [Hardware][AMD] Enable FlexAttention backend on ROCm (#26439)
3	0	vllm/platforms/rocm.py

[8bd696fa5] Huy Do 2025-10-08 [Bugfix] Incorrect another MM data format in vllm bench throughput (#26462)
8	8	vllm/benchmarks/throughput.py

[bb6d8c21f] Nick Hill 2025-10-08 [Bugfix] Catch and log invalid token ids in detokenizer #2 (#26445)
2	2	vllm/v1/engine/detokenizer.py

[ebf6ef1a9] Zhuohan Li 2025-10-08 [Minor] Change warning->warning_once in preprocess (#26455)
2	2	vllm/inputs/preprocess.py

[0c52d6ef8] Jee Jee Li 2025-10-09 [Bugfix] Set the minimum python version for gpt-oss (#26392)
0	1	requirements/common.txt
1	0	requirements/test.in
38	9	requirements/test.txt

[467a4f98f] Rui Qiao 2025-10-08 [Misc] Redact ray runtime env before logging (#26302)
7	1	vllm/engine/arg_utils.py

[e614ab780] Naveenraj Kamalakannan 2025-10-08 Separate MLAAttention class from Attention (#25103)
26	0	vllm/attention/backends/abstract.py
318	22	vllm/attention/layer.py
2	0	vllm/config/compilation.py
11	21	vllm/model_executor/layers/mla.py
4	4	vllm/model_executor/model_loader/utils.py
2	2	vllm/model_executor/models/deepseek_v2.py
2	2	vllm/v1/attention/backends/utils.py
3	3	vllm/v1/spec_decode/eagle.py
72	69	vllm/v1/worker/gpu_model_runner.py
62	40	vllm/v1/worker/tpu_model_runner.py

[2a03f93de] Matthew Bonanni 2025-10-08 [Attention] Register FLASHMLA_SPARSE (#26441)
2	0	vllm/attention/backends/registry.py
1	1	vllm/v1/attention/backends/mla/flashmla_sparse.py

[da364615f] bnellnm 2025-10-08 [Kernels] Modular kernel refactor (#24812)
23	14	tests/kernels/moe/modular_kernel_tools/common.py
1	1	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
12	14	tests/kernels/moe/modular_kernel_tools/mk_objects.py
96	48	tests/kernels/moe/test_modular_kernel_combinations.py
5	10	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
4	7	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
21	36	vllm/model_executor/layers/fused_moe/cutlass_moe.py
2	4	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
3	0	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
3	0	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
4	8	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
8	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
7	10	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	4	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
2	4	vllm/model_executor/layers/fused_moe/fused_moe.py
2	4	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
37	53	vllm/model_executor/layers/fused_moe/layer.py
354	273	vllm/model_executor/layers/fused_moe/modular_kernel.py
3	0	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
3	0	vllm/model_executor/layers/fused_moe/prepare_finalize.py
1	7	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
4	8	vllm/model_executor/layers/fused_moe/trtllm_moe.py

[f08919b7d] Elaine Zhao 2025-10-08 [Bugfix] Respect min_tokens in scheduler stop check (#26317)
90	0	tests/v1/core/test_scheduler.py
5	0	vllm/v1/core/sched/utils.py

[93f2c0aa0] Lukas Geiger 2025-10-08 [Models] Improve iteration over layers (#26425)
4	1	vllm/model_executor/models/apertus.py
2	2	vllm/model_executor/models/falcon_h1.py
5	6	vllm/model_executor/models/hunyuan_v1.py
2	1	vllm/model_executor/models/lfm2_moe.py
2	2	vllm/model_executor/models/longcat_flash.py
2	2	vllm/model_executor/models/mamba.py
3	4	vllm/model_executor/models/qwen3_vl.py
3	4	vllm/model_executor/models/qwen3_vl_moe.py

[4ebc9108a] Nicolò Lucchesi 2025-10-08 [Kernel] Centralize platform kernel import in `current_platform.import_kernels` (#26286)
2	3	vllm/_custom_ops.py
1	8	vllm/platforms/interface.py
5	2	vllm/platforms/tpu.py
5	2	vllm/platforms/xpu.py

[e1ba23566] Morrison Turnansky 2025-10-08 [BugFix] Fix failing test quantization/test_compressed_tensors.py::test_compressed_tensors_fp8_block_enabled (#26436)
1	1	vllm/config/vllm.py

[b82f4307c] elvischenv 2025-10-09 [Bugfix][Flashinfer] fix VLLM_USE_TRTLLM_ATTENTION issue for models with diff hyperparameters (#25924)
27	18	vllm/v1/attention/backends/flashinfer.py
13	18	vllm/v1/attention/backends/utils.py

[76879cc16] Matthew Bonanni 2025-10-08 [Attention] Implement universal BACKEND_MAP (#25900)
2	2	tests/kernels/attention/test_attention_selector.py
1	1	tests/kernels/attention/test_rocm_attention_selector.py
2	2	tests/v1/attention/test_attention_backends.py
3	3	tests/v1/attention/test_mla_backends.py
14	38	tests/v1/attention/utils.py
5	5	tests/v1/spec_decode/test_eagle.py
2	2	tests/v1/spec_decode/test_mtp.py
2	2	tests/v1/spec_decode/test_tree_attention.py
83	2	vllm/attention/backends/registry.py
2	2	vllm/attention/layer.py
1	14	vllm/attention/selector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[b25d7b565] Vinay R Damodaran 2025-10-08 [Feature] Change cache.py with pydantic validation (#26390)
17	60	vllm/config/cache.py
8	0	vllm/engine/arg_utils.py

[e09d1753e] Harry Mellor 2025-10-08 Remove Python 3.9 support ahead of PyTorch 2.9 in v0.11.1 (#26416)
5	5	.pre-commit-config.yaml
1	1	CMakeLists.txt
15	23	benchmarks/multi_turn/benchmark_serving_multi_turn.py
1	1	docker/Dockerfile.cpu
2	2	docs/contributing/README.md
1	1	docs/getting_started/installation/cpu.md
1	1	docs/getting_started/installation/gpu.md
1	1	docs/getting_started/quickstart.md
1	1	examples/online_serving/structured_outputs/pyproject.toml
7	2	pyproject.toml
1	2	requirements/cpu.txt
1	2	requirements/cuda.txt
1	2	requirements/nightly_torch_test.txt
1	2	requirements/rocm.txt
1	2	requirements/test.in
1	1	requirements/xpu.txt
1	13	tools/validate_config.py
1	13	vllm/config/utils.py
1	6	vllm/plugins/__init__.py
1	6	vllm/v1/sample/logits_processor/__init__.py

[4ba887574] Wentao Ye 2025-10-08 [Bug] Fix Test in Batch Invariant (#26128)
4	1	tests/v1/generation/test_batch_invariance.py

[6273fe8d3] Lukas Geiger 2025-10-08 [Benchmarks] Fix imports in FP8 tuning script (#26407)
2	2	benchmarks/kernels/benchmark_w8a8_block_fp8.py

[9fb3ae4e6] Wentao Ye 2025-10-08 [Bug] Fix DeepGEMM Attention Test (#26423)
1	0	pyproject.toml
1	2	tests/kernels/attention/test_deepgemm_attention.py

[76afe4edf] Aydin Abiar 2025-10-08 [Bugfix] Fix `vllm bench ...` on CPU-only head nodes (#25283)
21	0	vllm/entrypoints/cli/main.py
10	0	vllm/platforms/__init__.py
3	0	vllm/platforms/interface.py

[c1b06fc18] Michael Goin 2025-10-08 [CI Failure] Fix pre-commit issue for install_nixl_from_source_ubuntu.py (#26424)
95	81	tools/install_nixl_from_source_ubuntu.py

[241b4cfe6] Wentao Ye 2025-10-08 [Refactor] Refactor FP8 & INT8 Quant Folder inside `w8a8` (#25293)
21	20	CMakeLists.txt
2	2	csrc/attention/attention_kernels.cuh
2	2	csrc/cache_kernels.cu
3	2	csrc/cub_helpers.h
1	1	csrc/layernorm_quant_kernels.cu
1	1	csrc/quantization/activation_kernels.cu
1	1	csrc/quantization/fused_kernels/quant_conversions.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/Epilogues.md
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/cutlass_gemm_caller.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_azp_sm90_int8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm100_fp8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm120_fp8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm90_fp8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_helper.hpp
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_kernels.hpp
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm100_fp8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm100_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm120_fp8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm120_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_fp8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_int8.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/c3x/scaled_mm_sm90_int8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/blockwise_scaled_group_mm_sm100.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/get_group_starts.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/grouped_mm_c3x.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/grouped_mm_c3x_sm100.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/grouped_mm_c3x_sm90.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/moe/moe_data.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm75_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm80_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm89_fp8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c2x_sm89_int8_dispatch.cuh
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c3x_sm100.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c3x_sm120.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_c3x_sm90.cu
0	0	csrc/quantization/{cutlass_w8a8 => w8a8/cutlass}/scaled_mm_entry.cu
1	1	csrc/quantization/{ => w8a8}/fp8/amd/quant_utils.cuh
2	2	csrc/quantization/{ => w8a8}/fp8/common.cu
0	0	csrc/quantization/{ => w8a8}/fp8/common.cuh
1	1	csrc/quantization/{ => w8a8}/fp8/nvidia/quant_utils.cuh
5	5	csrc/quantization/{ => w8a8}/fp8/per_token_group_quant.cu
12	0	csrc/quantization/w8a8/int8/per_token_group_quant.cu
4	24	csrc/quantization/{compressed_tensors/int8_quant_kernels.cu => w8a8/int8/scaled_quant.cu}
0	1	csrc/quantization/{ => w8a8}/per_token_group_quant_8bit.h
1	1	csrc/rocm/attention.cu
1	1	csrc/rocm/skinny_gemms.cu
2	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py

[9fc983c70] Chendi.Xue 2025-10-08 [NIXL][non-cuda] Add install script for nixl with non-cuda ucx (#25959)
6	0	docs/features/nixl_connector_usage.md
1	1	docs/serving/expert_parallel_deployment.md
210	0	tools/install_nixl_from_source_ubuntu.py

[2f99f2f50] Harry Mellor 2025-10-08 Tidy `vllm/config/__init__.py` to only add classes and functions (#26405)
1	1	tests/distributed/test_context_parallel.py
1	1	tests/distributed/test_expert_parallel.py
1	1	tests/distributed/test_sequence_parallel.py
1	1	tests/models/multimodal/generation/vlm_utils/core.py
1	1	tests/models/multimodal/generation/vlm_utils/types.py
1	1	tests/models/registry.py
1	1	tests/models/utils.py
1	1	tests/v1/attention/utils.py
1	1	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	1	tests/v1/metrics/test_ray_metrics.py
1	1	tests/v1/sample/test_logprobs.py
8	39	vllm/config/__init__.py
16	18	vllm/engine/arg_utils.py
5	5	vllm/entrypoints/llm.py
2	1	vllm/model_executor/layers/mamba/mamba_utils.py
2	1	vllm/platforms/tpu.py
1	1	vllm/v1/sample/ops/topk_topp_sampler.py
1	1	vllm/v1/sample/sampler.py

[338b1bf04] Lukas Geiger 2025-10-08 [Benchmarks] Add support for Qwen 3 VL MoE tuning (#26419)
12	1	benchmarks/kernels/benchmark_moe.py

[e39dc46f8] wang.yuqi 2025-10-08 [CI] Pooling models mteb test disable enforce_eager  (#26408)
0	1	tests/models/language/generation_ppl_test/ppl_utils.py
0	2	tests/models/language/pooling_mteb_test/mteb_utils.py

[10c75b543] Harry Mellor 2025-10-08 [Docs] Have mergify leave a comment with the docs preview link (#26412)
2	0	.github/mergify.yml

[f9582fd8f] Eugene Khvedchenya 2025-10-08 [Model] Allow passing custom number of max tiles to Nano 2 VL (#26403)
19	6	vllm/model_executor/models/nano_nemotron_vl.py

[f377333bd] Daniele 2025-10-08 [Misc] add `usedforsecurity=False` in md5 hash call (#26357)
1	1	vllm/model_executor/models/registry.py

[f8607863d] Wentao Ye 2025-10-08 [Feature] Enable E8M0 by Default on Hopper for DeepGEMM, 5% E2E throughput improvement (#26197)
0	7	vllm/envs.py
7	7	vllm/transformers_utils/config.py
3	15	vllm/utils/deep_gemm.py

[335b28f7d] Utkarsh Sharma 2025-10-08 [TPU] Rename tpu_commons to tpu_inference (#26279)
7	7	vllm/distributed/device_communicators/tpu_communicator.py
2	2	vllm/model_executor/model_loader/default_loader.py
1	1	vllm/platforms/__init__.py
5	5	vllm/platforms/tpu.py
1	1	vllm/v1/attention/backends/pallas.py
6	6	vllm/v1/worker/tpu_worker.py

[5e65d6b2a] Ayush Satyam 2025-10-08 fix[DP][v1]: Prevent hangs from mismatched worker configurations (#26218)
9	0	vllm/config/parallel.py
14	10	vllm/v1/engine/core.py
23	1	vllm/v1/engine/utils.py

[0d4f48fa1] Cyrus Leung 2025-10-08 [Bugfix] Incorrect MM data format in `vllm bench throughput` (#26395)
8	8	vllm/benchmarks/throughput.py

[127c8b782] Barry Kang 2025-10-08 Add gather_indexer_k_quant_cache kernel (#25931)
8	0	csrc/cache.h
120	0	csrc/cache_kernels.cu
6	0	csrc/torch_bindings.cpp
12	0	vllm/_custom_ops.py

[cd9890544] Ayush Satyam 2025-10-08 fix(v1/kv_cache): resolve async KV transfer bug in cascade attention (#23485)
9	16	vllm/v1/core/kv_cache_coordinator.py
18	28	vllm/v1/core/kv_cache_manager.py
1	1	vllm/v1/core/sched/scheduler.py
13	27	vllm/v1/core/single_type_kv_cache_manager.py

[067da2d1d] Nick Hill 2025-10-07 [Core] Simplify setting new_token_ids in CachedRequestData (#26388)
1	7	vllm/v1/core/sched/scheduler.py

[046118b93] isharif168 2025-10-08 Add SwigluOAI implementation for CPUFusedMOE (#26347)
16	2	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py

[b32260ab8] liangel-02 2025-10-07 [torchao] safetensors integration (#25969)
17	0	tests/quantization/test_torchao.py
4	0	vllm/config/load.py
15	0	vllm/model_executor/layers/quantization/torchao.py
5	0	vllm/model_executor/model_loader/default_loader.py
19	0	vllm/model_executor/model_loader/weight_utils.py

[f80e7866c] Lucas Wilkinson 2025-10-07 [Misc] Clean up cruft from previous FlashMLA sparse implementation (#26125)
2	2	tests/kernels/attention/test_attention_selector.py
6	4	tests/kernels/attention/test_flashmla.py
9	16	tests/kernels/attention/test_flashmla_sparse.py
17	66	tests/v1/attention/test_sparse_mla_backends.py
4	0	tests/v1/attention/utils.py
35	9	vllm/attention/ops/flashmla.py
4	4	vllm/platforms/cuda.py
2	2	vllm/v1/attention/backends/mla/flashmla.py
0	37	vllm/v1/attention/backends/mla/flashmla_sparse.py

[31a4b3e6c] Thomas Parnell 2025-10-08 Revert #24446 and #26168 (#26332)
2	3	tests/entrypoints/llm/test_generate.py
0	90	tests/v1/e2e/test_context_length.py
1	1	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/core/sched/utils.py
6	22	vllm/v1/worker/gpu_model_runner.py

[caf8b1c08] Benjamin Chislett 2025-10-07 [Bugfix] Fix MTP+FlashInfer crash when trtllm kernels are available but disabled (#26361)
2	0	vllm/utils/flashinfer.py

[1b86bd8e1] Michael Goin 2025-10-07 Add more libraries to rlhf.md (#26374)
13	2	docs/training/rlhf.md

[59012df99] Johnny Yang 2025-10-07 [TPU] update TPU benchmark threshold (#25713)
1	1	.buildkite/scripts/tpu/quantized_v6e_1.env

[3d1f67616] Benjamin Chislett 2025-10-07 [Spec Decode] Enable efficient speculative decoding with FlashInfer-MLA (#25984)
24	2	vllm/v1/attention/backends/mla/common.py
15	1	vllm/v1/attention/backends/mla/flashinfer_mla.py
3	2	vllm/v1/attention/backends/utils.py

[6ebaf43ee] Sergei Skvortsov 2025-10-07 [V1] Logit processors for rejection sampler (#19482)
38	21	tests/v1/logits_processors/test_custom_offline.py
171	9	tests/v1/sample/test_rejection_sampler.py
18	28	tests/v1/sample/test_sampler.py
20	0	tests/v1/sample/utils.py
1	0	tests/v1/worker/test_gpu_input_batch.py
17	0	vllm/v1/sample/logits_processor/__init__.py
3	0	vllm/v1/sample/metadata.py
17	0	vllm/v1/sample/ops/bad_words.py
104	0	vllm/v1/sample/rejection_sampler.py
59	31	vllm/v1/sample/sampler.py
17	0	vllm/v1/worker/gpu_input_batch.py
3	0	vllm/v1/worker/gpu_model_runner.py

[0c824fc46] Morrison Turnansky 2025-10-07 [Frontend] CompilationConfig overhaul (#20283): deprecate use_inductor in favor of backend, simplify custom_ops (#26113)
11	20	tests/compile/piecewise/test_toy_llama.py
21	19	tests/model_executor/test_enabled_custom_ops.py
5	1	vllm/compilation/backends.py
60	11	vllm/config/compilation.py
19	0	vllm/config/vllm.py
10	10	vllm/model_executor/custom_op.py
0	2	vllm/platforms/cpu.py

[eb577e465] Pei-Lun Liao 2025-10-07 [Bugfix] Add missing sink tensor into flash attn cascade attn implementation (#26325)
5	0	vllm/v1/attention/backends/flash_attn.py

[8f36850f7] Wentao Ye 2025-10-07 [Bug] Fix Shape Validation for Fallback while Enabling E8M0 for DeepGEMM (#26322)
1	1	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[29fd2662b] Chen Zhang 2025-10-08 [deepseek] add EP8 FusedMOE config for H200 and B200 (#26331)
147	0	vllm/model_executor/layers/fused_moe/configs/E=32,N=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
147	0	vllm/model_executor/layers/fused_moe/configs/E=32,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json

[30a3e5af6] Michael Goin 2025-10-07 [CI] Add Qwen3 MoE NVFP4 to Blackwell lm-eval (#26316)
1	1	.buildkite/test-pipeline.yaml
6	0	tests/evals/gsm8k/configs/Qwen3-30B-A3B-NVFP4.yaml
1	0	tests/evals/gsm8k/configs/models-blackwell.txt

[a38c1bfe0] fxmarty-amd 2025-10-07 [ci] Rename `test_mxfp4_moe.py` to `test_ocp_mx_moe.py` (#26364)
1	1	.buildkite/test-pipeline.yaml
1	1	tests/quantization/test_quark.py

[320feae6f] Paul Pak 2025-10-08 [Model] Lfm2Moe (#26344)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
2	7	vllm/model_executor/models/lfm2.py
797	0	vllm/model_executor/models/lfm2_moe.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
160	0	vllm/transformers_utils/configs/lfm2_moe.py

[1e4ecca1d] Cyrus Leung 2025-10-07 [V0 Deprecation] Remove `VLLM_USE_V1` from tests (#26341)
2	1	.buildkite/test-pipeline.yaml
2	7	tests/basic_correctness/test_basic_correctness.py
48	57	tests/basic_correctness/test_cumem.py
0	2	tests/compile/piecewise/test_full_cudagraph.py
0	3	tests/compile/piecewise/test_simple.py
1	7	tests/compile/test_async_tp.py
1	11	tests/compile/test_config.py
0	3	tests/compile/test_fusion_attn.py
1	4	tests/config/test_mp_reducer.py
2	4	tests/detokenizer/test_stop_strings.py
8	45	tests/distributed/test_context_parallel.py
2	7	tests/distributed/test_pipeline_parallel.py
2	44	tests/distributed/test_sequence_parallel.py
18	34	tests/entrypoints/llm/test_accuracy.py
6	15	tests/entrypoints/openai/correctness/test_lmeval.py
1	12	tests/entrypoints/openai/test_chat.py
1	14	tests/entrypoints/openai/test_lora_adapters.py
21	94	tests/entrypoints/openai/test_metrics.py
0	5	tests/entrypoints/openai/test_prompt_validation.py
9	20	tests/kernels/attention/test_attention_selector.py
6	8	tests/kernels/test_flex_attention.py
0	1	tests/models/multimodal/generation/test_maverick.py
1	5	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
0	1	tests/plugins_tests/test_scheduler_plugins.py
0	7	tests/samplers/test_no_bad_words.py
0	11	tests/tpu/lora/test_lora.py
0	1	tests/v1/attention/utils.py
3	0	tests/v1/core/test_kv_sharing.py
0	4	tests/v1/core/test_scheduler_e2e.py
2	2	tests/v1/cudagraph/test_cudagraph_mode.py
0	1	tests/v1/e2e/test_cascade_attention.py
29	34	tests/v1/e2e/test_correctness_sliding_window.py
0	2	tests/v1/e2e/test_kv_sharing_fast_prefill.py
1	5	tests/v1/e2e/test_min_tokens.py
0	1	tests/v1/e2e/test_spec_decode.py
14	44	tests/v1/engine/test_async_llm.py
0	7	tests/v1/engine/test_engine_args.py
300	314	tests/v1/engine/test_engine_core.py
125	146	tests/v1/engine/test_engine_core_client.py
8	12	tests/v1/engine/test_llm_engine.py
1	15	tests/v1/entrypoints/llm/test_struct_output_generate.py
0	3	tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh
0	2	tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh
0	14	tests/v1/metrics/test_ray_metrics.py
154	171	tests/v1/sample/test_logprobs.py
0	12	tests/v1/sample/test_sampling_params_e2e.py
0	1	tests/v1/spec_decode/test_max_len.py
46	62	tests/v1/tpu/test_basic.py
38	42	tests/v1/tpu/test_perf.py
1	1	tests/v1/tracing/test_tracing.py
7	1	vllm/v1/worker/cpu_model_runner.py

[c0a7b89d8] Cyrus Leung 2025-10-07 [Misc] Move `LRUCache` into its own file (#26342)
125	0	tests/utils_/test_cache.py
0	126	tests/utils_/test_utils.py
2	1	vllm/lora/models.py
2	1	vllm/multimodal/cache.py
0	250	vllm/utils/__init__.py
220	0	vllm/utils/cache.py

[6f59beaf0] antrec 2025-10-07 [Model] Add support for ModernBertForTokenClassification (#26340)
1	0	docs/models/supported_models.md
32	1	tests/models/language/pooling/test_token_classification.py
3	0	tests/models/registry.py
72	1	vllm/model_executor/models/modernbert.py
4	0	vllm/model_executor/models/registry.py

[41f1cf38f] fxmarty-amd 2025-10-07 [Feature][OCP MX] Support mxfp6 and mixed mxfp6-mxfp4 (#21166)
8	4	docs/features/quantization/quark.md
23	17	tests/kernels/moe/{test_mxfp4_moe.py => test_ocp_mx_moe.py}
74	19	tests/quantization/test_quark.py
56	17	vllm/model_executor/layers/fused_moe/config.py
2	2	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
69	21	vllm/model_executor/layers/fused_moe/fused_moe.py
60	13	vllm/model_executor/layers/fused_moe/utils.py
3	2	vllm/model_executor/layers/quantization/mxfp4.py
20	18	vllm/model_executor/layers/quantization/quark/quark.py
34	15	vllm/model_executor/layers/quantization/quark/quark_moe.py
2	2	vllm/model_executor/layers/quantization/quark/schemes/__init__.py
95	45	vllm/model_executor/layers/quantization/quark/schemes/{quark_w4a4_mxfp4.py => quark_ocp_mx.py}
8	2	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
142	0	vllm/model_executor/layers/quantization/utils/mxfp6_utils.py
3	3	vllm/model_executor/layers/quantization/utils/mxfp8_utils.py
54	0	vllm/model_executor/layers/quantization/utils/ocp_mx_utils.py
3	0	vllm/scalar_type.py
2	2	vllm/utils/flashinfer.py

[08d26a1b7] Isotr0py 2025-10-07 [Model] Use `merge_by_field_config` for MM models (Ovis family) (#26308)
2	6	examples/offline_inference/vision_language.py
3	5	examples/offline_inference/vision_language_multi_image.py
12	13	vllm/model_executor/models/ovis.py
62	50	vllm/model_executor/models/ovis2_5.py
1	1	vllm/transformers_utils/processors/ovis.py

[63773a620] fhl2000 2025-10-07 [Docs] add docs for cuda graph v1 (#24374)
-	-	docs/assets/design/cuda_graphs/current_design.png
-	-	docs/assets/design/cuda_graphs/executor_runtime.png
-	-	docs/assets/design/cuda_graphs/previous_design.png
-	-	docs/assets/design/cuda_graphs/wrapper_flow.png
241	0	docs/design/cuda_graphs.md
5	4	docs/design/torch_compile.md

[883b42896] Sergio Paniego Blanco 2025-10-07 Add TRL example notebook to RLHF docs (#26346)
1	0	docs/training/rlhf.md

[e1098ced9] Daniel Cámpora 2025-10-07 Add topk logits torch op for DS3.2. (#25945)
5	0	csrc/ops.h
256	1	csrc/sampler.cu
7	0	csrc/torch_bindings.cpp
143	0	tests/kernels/test_top_k_per_row.py
35	24	vllm/model_executor/models/deepseek_v2.py

[d100d78eb] Grant Holmes (Ren) 2025-10-07 Optimize KV cache distribution for asymmetric pipeline parallelism (#25164)
5	5	tests/v1/core/test_kv_cache_utils.py
1	1	vllm/config/cache.py
1	1	vllm/entrypoints/llm.py
54	28	vllm/v1/core/kv_cache_utils.py
3	3	vllm/v1/worker/gpu_worker.py

[7e4cd070b] Cyrus Leung 2025-10-07 [V0 Deprecation] Remove `VLLM_USE_V1` from docs and scripts (#26336)
0	5	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
1	2	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
1	2	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
1	1	.buildkite/scripts/tpu/run_bm.sh
2	2	benchmarks/auto_tune/auto_tune.sh
8	8	docs/design/p2p_nccl_connector.md
1	1	docs/design/torch_compile.md
2	2	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh
0	1	examples/online_serving/elastic_ep/serve_deepseek_v2.sh
1	1	examples/online_serving/openai_chat_completion_client_with_tools_required.py
0	1	examples/online_serving/ray_serve_deepseek.py

[46b077999] Snehlata 2025-10-07 [BugFix] Update KV block hash type from BlockHash to ExternalBlockHash in kv_events_subscriber - #26264 (#26265)
4	4	examples/online_serving/kv_events_subscriber.py

[de342585f] Ayush Satyam 2025-10-07 [Model] Define merge_by_field_config MM interface (R-T) (#26260)
36	33	vllm/model_executor/models/step3_vl.py
1	5	vllm/model_executor/models/tarsier.py
9	7	vllm/model_executor/models/terratorch.py

[185d8ed44] Andrew Xia 2025-10-07 [responsesAPI][bugfix] serialize harmony messages (#26185)
16	0	tests/entrypoints/openai/test_response_api_with_harmony.py
44	5	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_responses.py

[d9836d451] Cyrus Leung 2025-10-07 [Deprecation] Deprecate `LLM.set_tokenizer` (#26333)
2	1	vllm/entrypoints/llm.py

[5f7e8a916] Ayush Satyam 2025-10-07 [Model] Define merge_by_field_config MM interface (U-Z) (#26261)
18	15	vllm/model_executor/models/ultravox.py
3	2	vllm/model_executor/models/voxtral.py
8	8	vllm/model_executor/models/whisper.py
3	0	vllm/multimodal/inputs.py

[4dbdf4a29] ahao-anyscale 2025-10-06 [BUG] Fix file parsing for load_format runai_streamer_sharded (#26324)
1	1	vllm/model_executor/model_loader/sharded_state_loader.py

[c6873c4e6] Michael Goin 2025-10-06 [UX] Support nested dicts in hf_overrides (#25727)
31	0	tests/test_config.py
57	1	vllm/config/model.py

[2111b4643] Sage Moore 2025-10-06 [Core] Simplify the Dp padding/should ubatch coordination logic (#25768)
1	1	tests/v1/attention/test_attention_splitting.py
4	0	vllm/config/parallel.py
8	0	vllm/engine/arg_utils.py
0	7	vllm/envs.py
9	120	vllm/forward_context.py
177	0	vllm/v1/worker/dp_utils.py
38	123	vllm/v1/worker/gpu_model_runner.py
14	1	vllm/v1/worker/gpu_ubatch_wrapper.py
0	207	vllm/v1/worker/ubatch_splitting.py
46	3	vllm/v1/worker/ubatch_utils.py

[c50901f3b] Sage Moore 2025-10-06 [Docs][DBO] Add initial doc that describes the DBO implementation (#26024)
88	0	docs/design/dbo.md

[8229280a9] Simon Mo 2025-10-06 [Misc] Define EP kernel arch list in Dockerfile (#25635)
1	1	docker/Dockerfile

[f77df9464] Benjamin Chislett 2025-10-06 [Perf] Add decode full-graph support to FlashInfer-MLA backend (#26313)
12	1	vllm/v1/attention/backends/mla/flashinfer_mla.py

[f231e5bc2] Gregory Shtrasberg 2025-10-06 [ROCm] Split AITER unified attention into its own backend (#25507)
54	164	tests/compile/test_fusion_attn.py
1	0	vllm/attention/backends/registry.py
1	0	vllm/attention/selector.py
1	0	vllm/engine/arg_utils.py
7	6	vllm/envs.py
18	10	vllm/platforms/rocm.py
203	0	vllm/v1/attention/backends/rocm_aiter_unified_attn.py
40	121	vllm/v1/attention/backends/rocm_attn.py

[2161efe97] Benjamin Chislett 2025-10-06 [Bugfix] Allow skipping MoE in NVFP4 (fix for MTP) (#25987)
2	0	vllm/model_executor/layers/fused_moe/layer.py
4	1	vllm/model_executor/layers/quantization/modelopt.py
1	0	vllm/model_executor/models/deepseek_eagle.py
7	2	vllm/model_executor/models/deepseek_mtp.py
4	2	vllm/model_executor/models/deepseek_v2.py

[f23b4c04f] Varun Sundar Rabindranath 2025-10-06 [BugFix] Pad input buffers in _dummy_run (#26209)
10	8	vllm/v1/worker/gpu_model_runner.py

[93540958b] Varun Sundar Rabindranath 2025-10-06 [Docs] Fix broken table in moe_kernel_features doc (#26314)
0	1	docs/design/moe_kernel_features.md

[44b9af5bb] Cyrus Leung 2025-10-07 [Benchmark] Enable MM Embedding benchmarks (#26310)
104	7	docs/contributing/benchmarks.md
4	4	vllm/benchmarks/datasets.py
214	60	vllm/benchmarks/lib/endpoint_request_func.py
34	34	vllm/benchmarks/serve.py
2	2	vllm/entrypoints/chat_utils.py

[7cd95dc8a] Raushan Turganbay 2025-10-06 [Bugfix] Fix gemma3 with transformers backend (#23178)
14	0	tests/models/multimodal/generation/test_common.py
23	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
32	54	vllm/model_executor/models/transformers.py
3	5	vllm/model_executor/models/transformers_moe.py

[c02058c22] Crefeda Rodrigues 2025-10-06 Add bias handling to CPUFusedMOE kernel (#26289)
69	0	tests/kernels/moe/test_moe.py
9	2	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py

[b2ea5ba67] 7mile 2025-10-07 [Bugfix][Spec Decode] Fix wrong valid_mask for padded speculation when chunked prefill occurs (#26231)
3	7	vllm/v1/spec_decode/eagle.py

[824a3f403] Karan Goel 2025-10-06 [Misc] auto_tune: kill specific vllm process (#26304)
4	4	benchmarks/auto_tune/auto_tune.sh

[05f6846ed] Rahul Tuli 2025-10-06 Support llama3 eagle3 head with llama4 verifier (#25961)
5	0	vllm/model_executor/models/llama.py
7	1	vllm/model_executor/models/llama_eagle3.py
25	2	vllm/model_executor/models/mllama4.py
8	0	vllm/transformers_utils/configs/speculators/algos.py
38	5	vllm/v1/worker/gpu_model_runner.py

[20db99cc6] Michael Goin 2025-10-06 [CI Bugfix] Make sure TRTLLM attention is available in test_blackwell_moe (#26188)
9	1	tests/quantization/test_blackwell_moe.py

[6431be808] Yannick Schnider 2025-10-06 [Tests] conftest: Extending VllmRunner and HfRunner to accept token_ids as input (#26295)
48	31	tests/conftest.py
17	34	tests/v1/e2e/test_context_length.py

[4727a8afa] Matthew Bonanni 2025-10-06 [Attention] Remove unused reorder_batch method (#24463)
1	1	tests/v1/logits_processors/test_correctness.py
2	4	vllm/v1/attention/backends/flashinfer.py
1	10	vllm/v1/attention/backends/flex_attention.py
3	14	vllm/v1/attention/backends/tree_attn.py
0	22	vllm/v1/attention/backends/utils.py
1	13	vllm/v1/attention/backends/xformers.py

[b8f603ceb] tomeras91 2025-10-06 [Model] EVS support for nano_nemotron_vl (#26269)
207	19	vllm/model_executor/models/nano_nemotron_vl.py
6	2	vllm/model_executor/models/qwen2_5_vl.py
11	10	vllm/multimodal/evs.py

[fc679696f] Chatcharin Sangbutsarakum 2025-10-06 Fix `DotsOCR` tensor type (#26281)
6	6	vllm/model_executor/models/dots_ocr.py

[ab5e7d93f] Raushan Turganbay 2025-10-06 [Bugfix] Fix mrope in Transformers Backend (#26087)
1	2	tests/models/multimodal/generation/test_common.py
37	66	vllm/model_executor/models/transformers.py

[0340f4555] Harry Mellor 2025-10-06 Support expert parallel load balancing in Transformers backend (#26287)
13	11	vllm/model_executor/models/transformers.py
63	16	vllm/model_executor/models/transformers_moe.py

[19a00eb21] Cyrus Leung 2025-10-06 [Model] Use `merge_by_field_config` for MM models (Llava family) (#26280)
53	53	examples/offline_inference/vision_language_multi_image.py
5	15	vllm/model_executor/models/llava.py
5	19	vllm/model_executor/models/llava_next.py
13	16	vllm/model_executor/models/llava_next_video.py
7	27	vllm/model_executor/models/llava_onevision.py
5	20	vllm/model_executor/models/minimax_vl_01.py
3	8	vllm/model_executor/models/mistral3.py
4	2	vllm/model_executor/models/pixtral.py
4	13	vllm/model_executor/models/tarsier.py

[391612e78] Cyrus Leung 2025-10-06 [Frontend] Consolidate tokenizer init code (#26276)
3	5	tests/test_inputs.py
1	3	tests/v1/engine/test_processor_multi_modal_uuids.py
2	6	vllm/entrypoints/llm.py
2	6	vllm/entrypoints/openai/serving_engine.py
11	16	vllm/inputs/preprocess.py
6	14	vllm/v1/engine/async_llm.py
10	12	vllm/v1/engine/llm_engine.py
11	8	vllm/v1/engine/processor.py

[77c95f72f] abhisheksheth28 2025-10-06 [Doc] add KAITO to integrations (#25521)
5	0	docs/deployment/integrations/kaito.md
1	0	docs/deployment/k8s.md

[59f30d044] Aritra Roy Gosthipaty 2025-10-06 [Docs] Edit HF Inference Endpoints documentation (#26275)
2	2	docs/deployment/frameworks/hf_inference_endpoints.md

[43c146ca4] Roger Wang 2025-10-06 [Misc] Clean up unnecessary E501 ignore (#26274)
2	2	vllm/benchmarks/datasets.py
2	2	vllm/benchmarks/throughput.py
1	1	vllm/compilation/inductor_pass.py
1	1	vllm/model_executor/layers/quantization/__init__.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	1	vllm/model_executor/models/gemma3n_mm.py
28	28	vllm/model_executor/models/registry.py
1	1	vllm/v1/attention/backends/gdn_attn.py

[7c2ec0fe8] Yasmin Moslem 2025-10-06 [Benchmarking] Add disable_shuffle option for dataset loading (#26258)
43	13	vllm/benchmarks/datasets.py

[039b6bade] dependabot[bot] 2025-10-06 Bump actions/stale from 10.0.0 to 10.1.0 (#26272)
1	1	.github/workflows/stale.yml

[6c0463821] Harry Mellor 2025-10-06 Fix per file ruff ignores related to line length (#26262)
1	1	benchmarks/benchmark_ngram_proposer.py
2	2	benchmarks/benchmark_serving_structured_output.py
3	3	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
1	1	examples/offline_inference/vision_language_pooling.py
2	2	examples/online_serving/disaggregated_serving/disagg_proxy_demo.py
0	46	pyproject.toml
17	11	tests/compile/piecewise/test_simple.py
9	5	tests/compile/piecewise/test_toy_llama.py
1	1	tests/compile/test_functionalization.py
1	1	tests/compile/test_fusion_attn.py
2	2	tests/compile/test_sequence_parallelism.py
4	4	tests/distributed/test_pipeline_parallel.py
2	1	tests/entrypoints/conftest.py
1	1	tests/entrypoints/openai/test_audio.py
7	6	tests/entrypoints/openai/test_chat.py
2	2	tests/entrypoints/openai/test_chat_template.py
6	4	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
8	4	tests/entrypoints/openai/test_completion_with_function_calling.py
2	2	tests/entrypoints/openai/test_video.py
2	2	tests/entrypoints/openai/test_vision.py
26	23	tests/entrypoints/test_chat_utils.py
1	1	tests/kernels/attention/test_attention_selector.py
2	1	tests/kernels/moe/modular_kernel_tools/common.py
1	1	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
4	4	tests/kernels/moe/modular_kernel_tools/mk_objects.py
2	2	tests/kernels/moe/parallel_utils.py
2	2	tests/kernels/moe/test_deepep_deepgemm_moe.py
2	2	tests/kernels/moe/test_deepep_moe.py
1	1	tests/kernels/moe/test_modular_kernel_combinations.py
2	2	tests/kernels/moe/test_moe.py
2	2	tests/kernels/moe/test_mxfp4_moe.py
9	3	tests/lora/test_chatglm3_tp.py
2	2	tests/lora/test_llama_tp.py
1	1	tests/lora/test_llm_with_multi_loras.py
1	1	tests/models/language/generation/test_gemma.py
10	6	tests/models/language/generation/test_mistral.py
38	38	tests/models/multimodal/generation/test_common.py
1	1	tests/models/multimodal/generation/test_ultravox.py
2	7	tests/models/multimodal/generation/test_voxtral.py
2	2	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
2	1	tests/tool_use/test_tool_choice_required.py
7	7	tests/v1/attention/utils.py
1	1	tests/v1/entrypoints/openai/responses/test_image.py
3	2	tests/v1/kv_connector/nixl_integration/test_accuracy.py
1	1	tests/v1/kv_connector/unit/test_offloading_connector.py
2	9	tests/v1/logits_processors/test_custom_offline.py
8	4	vllm/attention/ops/pallas_kv_cache_update.py
17	5	vllm/compilation/collective_fusion.py
9	5	vllm/compilation/wrapper.py
1	1	vllm/config/vllm.py
5	3	vllm/distributed/device_communicators/all2all.py
2	1	vllm/entrypoints/openai/protocol.py
1	1	vllm/lora/layers/vocal_parallel_embedding.py
2	2	vllm/model_executor/layers/quantization/ptpc_fp8.py
1	1	vllm/model_executor/model_loader/bitsandbytes_loader.py
2	2	vllm/model_executor/models/bailing_moe.py
4	4	vllm/model_executor/models/hyperclovax_vision.py
3	2	vllm/model_executor/models/llama4_eagle.py
20	20	vllm/model_executor/models/longcat_flash_mtp.py
2	2	vllm/model_executor/models/phi4mm.py
3	2	vllm/model_executor/models/qwen3_next.py
8	5	vllm/v1/attention/backends/mla/common.py
7	4	vllm/v1/engine/utils.py
4	2	vllm/v1/utils.py
2	1	vllm/v1/worker/gpu_model_runner.py

[91ac7f764] wuhang 2025-10-06 [CI][gpt-oss] Enable python tool tests in CI (#24315)
1	0	requirements/common.txt
16	22	tests/entrypoints/openai/test_response_api_with_harmony.py
7	4	vllm/entrypoints/tool.py

[4be7d7c1c] Chen Zhang 2025-10-05 [MISC] Add heheda12345 to CODEOWNERS of vllm/config/cache.py (#26270)
1	0	.github/CODEOWNERS

[59b477645] orangeng 2025-10-05 [Doc] Edited minor typo (#26266)
1	1	docs/design/plugin_system.md

[778f55415] Thomas Parnell 2025-10-06 [V1] [Hybrid] Some additional clean-up in Mamba2 prefix caching (#26222)
96	57	vllm/model_executor/layers/mamba/mamba_mixer2.py
22	22	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
52	55	vllm/v1/attention/backends/mamba2_attn.py
1	2	vllm/v1/core/single_type_kv_cache_manager.py

[d3c84297c] Thomas Parnell 2025-10-06 [CI] Add comment about the single cudagraph capture size that is used (#26252)
4	0	tests/conftest.py

[f509a2084] Elieser Pereira 2025-10-05 [DOC] Update production-stack.md (#26177)
2	2	docs/deployment/integrations/production-stack.md

[60bc25e74] Michael Goin 2025-10-05 [CI] Add Blackwell LM Eval Small Models test to nightly (#26052)
12	2	.buildkite/test-pipeline.yaml
4	0	tests/evals/gsm8k/configs/models-blackwell.txt

[b893d661b] Harry Mellor 2025-10-05 Fix per file ruff ignores related to simplification (#26259)
0	34	pyproject.toml
1	4	tests/distributed/test_expert_placement.py
1	4	tests/kernels/attention/test_cutlass_mla_decode.py
1	4	tests/kernels/attention/test_flashmla.py
1	4	tests/kernels/attention/test_lightning_attn.py
1	4	tests/kernels/moe/test_pplx_moe.py
2	8	tests/kernels/quantization/test_cutlass_scaled_mm.py
1	4	tests/kernels/test_onednn.py
3	7	tests/kernels/utils.py
2	8	tests/multimodal/test_processing.py
1	1	tools/profiler/print_layerwise_table.py
1	4	vllm/_custom_ops.py
3	6	vllm/attention/ops/triton_reshape_and_cache_flash.py
1	4	vllm/distributed/parallel_state.py
1	4	vllm/entrypoints/chat_utils.py
1	4	vllm/entrypoints/llm.py
5	10	vllm/executor/ray_distributed_executor.py
1	4	vllm/model_executor/layers/batch_invariant.py
1	4	vllm/model_executor/layers/fla/ops/chunk_o.py
1	4	vllm/model_executor/layers/fused_moe/fused_moe.py
1	4	vllm/model_executor/layers/fused_moe/layer.py
1	4	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	4	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
1	4	vllm/model_executor/layers/layernorm.py
1	4	vllm/model_executor/layers/lightning_attn.py
3	7	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	4	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
1	4	vllm/model_executor/layers/quantization/quark/utils.py
1	4	vllm/utils/__init__.py
1	4	vllm/v1/sample/ops/bad_words.py
4	16	vllm/v1/sample/rejection_sampler.py
2	6	vllm/v1/worker/tpu_model_runner.py

[6b6e98775] Jason Li 2025-10-05 [NVIDIA] flashinfer TRTLLM attention prefill token limit (#25998)
12	5	vllm/utils/flashinfer.py

[9c3c21c51] Jiangyun Zhu 2025-10-06 [CI] fix mamba kernel test (#26250)
1	0	.buildkite/test-pipeline.yaml
11	1	tests/kernels/mamba/test_causal_conv1d.py

[512b8affa] Harry Mellor 2025-10-05 Update `ruff` pre-commit hooks version (#26255)
1	1	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
2	2	.pre-commit-config.yaml

[1c0c68202] Harry Mellor 2025-10-05 Fix per file ruff ignores related to typing (#26254)
1	17	pyproject.toml
2	2	tests/compile/test_full_graph.py
3	3	tests/entrypoints/openai/test_serving_chat.py
11	11	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
2	2	tests/v1/engine/test_llm_engine.py
6	6	vllm/attention/backends/abstract.py
3	3	vllm/attention/layer.py
2	2	vllm/attention/layers/chunked_local_attention.py
5	5	vllm/attention/ops/flashmla.py
5	5	vllm/attention/ops/paged_attn.py
7	10	vllm/engine/arg_utils.py
21	21	vllm/engine/metrics.py
17	18	vllm/engine/metrics_types.py
12	12	vllm/entrypoints/harmony_utils.py
8	8	vllm/executor/executor_base.py
2	2	vllm/executor/msgspec_utils.py
15	15	vllm/executor/ray_distributed_executor.py
8	8	vllm/executor/ray_utils.py
5	5	vllm/executor/uniproc_executor.py
2	5	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
1	1	vllm/multimodal/inputs.py
1	2	vllm/plugins/io_processors/__init__.py
48	51	vllm/utils/__init__.py
2	2	vllm/utils/deep_gemm.py
4	4	vllm/utils/flashinfer.py
25	25	vllm/v1/attention/backends/flashinfer.py
9	9	vllm/v1/core/sched/output.py
12	12	vllm/v1/core/sched/scheduler.py
5	5	vllm/v1/structured_output/__init__.py
2	2	vllm/v1/structured_output/backend_guidance.py
5	5	vllm/v1/structured_output/request.py
7	7	vllm/v1/worker/worker_base.py

[5f317530e] ihb2032 2025-10-06 fix(tests): Resolve late binding of loop variable in assert message lambda (#26249)
6	6	tests/kernels/mamba/test_mamba_ssm_ssd.py

[557b2e961] Harry Mellor 2025-10-05 Remove all cases of `fmt: on/off` (#26253)
117	109	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
47	24	tests/kernels/mamba/test_mamba_ssm_ssd.py
5	3	tests/v1/engine/test_fast_incdec_prefix_err.py
4	6	vllm/model_executor/models/qwen2_5_vl.py
44	15	vllm/model_executor/models/voxtral.py

[4e256cadc] Harry Mellor 2025-10-05 Remove all references to `yapf` as it's no longer used (#26251)
0	5	csrc/quantization/machete/generate.py
95	73	examples/others/tensorize_vllm_model.py
0	5	tests/compile/test_silu_mul_quant_fusion.py
22	20	tests/distributed/test_expert_parallel.py
3	3	tests/distributed/test_pipeline_parallel.py
10	25	tests/engine/test_arg_utils.py
179	209	tests/entrypoints/test_chat_utils.py
0	5	tests/lora/test_layers.py
0	4	tests/model_executor/model_loader/tensorizer_loader/test_tensorizer.py
191	169	tests/models/multimodal/generation/test_common.py
3	4	tests/models/multimodal/generation/vlm_utils/case_filtering.py
0	2	tests/models/multimodal/generation/vlm_utils/model_utils.py
14	12	tests/models/multimodal/generation/vlm_utils/types.py
72	71	tests/models/multimodal/processing/test_common.py
0	2	tests/models/multimodal/processing/test_idefics3.py
0	2	tests/models/multimodal/processing/test_phi3v.py
0	2	tests/models/multimodal/processing/test_phi4mm.py
0	2	tests/models/multimodal/processing/test_qwen2_vl.py
0	2	tests/models/multimodal/processing/test_smolvlm.py
24	9	tests/models/multimodal/processing/test_transformers.py
648	411	tests/models/registry.py
19	20	tests/multimodal/test_cache.py
127	99	tests/multimodal/test_processing.py
13	14	tests/multimodal/test_utils.py
52	44	tests/test_inputs.py
0	4	tests/tpu/test_moe_pallas.py
70	61	tests/utils_/test_utils.py
0	5	tests/v1/core/test_kv_cache_utils.py
0	4	tests/v1/logits_processors/test_correctness.py
0	4	tests/v1/logits_processors/test_custom_offline.py
0	4	tests/v1/logits_processors/test_custom_online.py
0	3	vllm/distributed/kv_transfer/kv_connector/factory.py
0	3	vllm/engine/arg_utils.py
1	14	vllm/entrypoints/chat_utils.py
0	5	vllm/entrypoints/llm.py
0	5	vllm/entrypoints/openai/api_server.py
0	4	vllm/entrypoints/openai/protocol.py
0	4	vllm/entrypoints/openai/run_batch.py
5	3	vllm/entrypoints/openai/serving_chat.py
0	2	vllm/entrypoints/openai/serving_classification.py
0	5	vllm/entrypoints/openai/serving_completion.py
0	5	vllm/entrypoints/openai/serving_embedding.py
0	5	vllm/entrypoints/openai/serving_engine.py
0	4	vllm/entrypoints/openai/serving_pooling.py
0	10	vllm/entrypoints/openai/serving_responses.py
0	5	vllm/entrypoints/openai/serving_score.py
0	5	vllm/entrypoints/openai/serving_tokenization.py
1	1	vllm/executor/ray_distributed_executor.py
50	36	vllm/lora/layers/base_linear.py
18	19	vllm/lora/layers/row_parallel_linear.py
0	4	vllm/lora/utils.py
0	4	vllm/model_executor/layers/fused_moe/fused_moe.py
0	4	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
0	4	vllm/model_executor/layers/linear.py
0	5	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py
0	5	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
0	4	vllm/model_executor/model_loader/bitsandbytes_loader.py
0	3	vllm/model_executor/models/aria.py
0	4	vllm/model_executor/models/gemma3_mm.py
30	21	vllm/model_executor/models/gemma3n_mm.py
0	8	vllm/model_executor/models/idefics3.py
0	5	vllm/model_executor/models/phi3v.py
0	4	vllm/model_executor/models/qwen2_5_vl.py
0	5	vllm/model_executor/models/qwen3_next.py
132	44	vllm/model_executor/models/registry.py
0	3	vllm/model_executor/models/smolvlm.py
0	4	vllm/model_executor/models/voxtral.py
0	2	vllm/transformers_utils/chat_templates/registry.py
1	2	vllm/transformers_utils/configs/arctic.py
11	7	vllm/transformers_utils/configs/nemotron_vl.py
42	36	vllm/transformers_utils/configs/ovis.py
81	37	vllm/transformers_utils/processors/deepseek_vl2.py
79	45	vllm/transformers_utils/processors/ovis.py
0	5	vllm/v1/attention/backends/flashinfer.py
0	5	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
0	4	vllm/v1/serial_utils.py
0	10	vllm/v1/worker/gpu_model_runner.py

[d6953beb9] Harry Mellor 2025-10-05 Convert formatting to use `ruff` instead of `yapf` + `isort` (#26247)
0	46	.buildkite/pyproject.toml
0	12	.pre-commit-config.yaml
1	1	benchmarks/benchmark_block_pool.py
1	1	benchmarks/benchmark_ngram_proposer.py
2	3	benchmarks/benchmark_serving_structured_output.py
0	49	benchmarks/pyproject.toml
24	19	cmake/hipify.py
13	15	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
24	18	csrc/moe/marlin_moe_wna16/generate_kernels.py
26	22	csrc/quantization/gptq_marlin/generate_kernels.py
100	66	csrc/quantization/machete/generate.py
27	36	docs/mkdocs/hooks/generate_argparse.py
18	16	docs/mkdocs/hooks/generate_examples.py
1	1	docs/mkdocs/hooks/remove_announcement.py
8	7	docs/mkdocs/hooks/url_schemes.py
0	54	examples/pyproject.toml
100	27	pyproject.toml
151	104	setup.py
60	62	tests/basic_correctness/test_basic_correctness.py
3	2	tests/basic_correctness/test_cpu_offload.py
12	11	tests/basic_correctness/test_cumem.py
12	2	tests/benchmarks/test_latency_cli.py
69	52	tests/benchmarks/test_random_dataset.py
2	3	tests/benchmarks/test_serve_cli.py
12	2	tests/benchmarks/test_throughput_cli.py
7	11	tests/compile/backend.py
47	35	tests/compile/piecewise/test_full_cudagraph.py
138	100	tests/compile/piecewise/test_multiple_graphs.py
54	49	tests/compile/piecewise/test_simple.py
176	147	tests/compile/piecewise/test_toy_llama.py
7	5	tests/compile/silly_attention.py
137	102	tests/compile/test_async_tp.py
17	11	tests/compile/test_basic_correctness.py
90	82	tests/compile/test_config.py
115	105	tests/compile/test_decorator.py
82	52	tests/compile/test_full_graph.py
26	44	tests/compile/test_functionalization.py
47	29	tests/compile/test_fusion.py
81	61	tests/compile/test_fusion_all_reduce.py
236	184	tests/compile/test_fusion_attn.py
15	15	tests/compile/test_noop_elimination.py
4	5	tests/compile/test_pass_manager.py
113	89	tests/compile/test_sequence_parallelism.py
48	36	tests/compile/test_silu_mul_quant_fusion.py
5	8	tests/compile/test_wrapper.py
14	10	tests/config/test_config_generation.py
7	7	tests/config/test_mp_reducer.py
196	179	tests/conftest.py
23	22	tests/cuda/test_cuda_context.py
8	12	tests/detokenizer/test_disable_detokenization.py
21	17	tests/detokenizer/test_min_tokens.py
21	16	tests/detokenizer/test_stop_reason.py
11	12	tests/detokenizer/test_stop_string_while_stop_model_terminates.py
58	42	tests/detokenizer/test_stop_strings.py
2	4	tests/distributed/conftest.py
4	2	tests/distributed/test_ca_buffer_sharing.py
52	46	tests/distributed/test_comm_ops.py
66	40	tests/distributed/test_context_parallel.py
16	20	tests/distributed/test_custom_all_reduce.py
1	2	tests/distributed/test_distributed_oot.py
97	81	tests/distributed/test_eplb_algo.py
97	84	tests/distributed/test_eplb_execute.py
30	23	tests/distributed/test_events.py
29	28	tests/distributed/test_expert_parallel.py
106	53	tests/distributed/test_expert_placement.py
32	26	tests/distributed/test_kvlayout.py
4	6	tests/distributed/test_multi_node_assignment.py
28	28	tests/distributed/test_nccl_symm_mem_allreduce.py
9	6	tests/distributed/test_node_count.py
63	67	tests/distributed/test_pipeline_parallel.py
2	2	tests/distributed/test_pipeline_partition.py
12	6	tests/distributed/test_pp_cudagraph.py
121	113	tests/distributed/test_pynccl.py
33	34	tests/distributed/test_quick_all_reduce.py
2	4	tests/distributed/test_same_node.py
82	53	tests/distributed/test_sequence_parallel.py
11	13	tests/distributed/test_shm_broadcast.py
20	14	tests/distributed/test_shm_buffer.py
19	19	tests/distributed/test_shm_storage.py
52	55	tests/distributed/test_symm_mem_allreduce.py
15	15	tests/distributed/test_torchrun_example.py
18	23	tests/distributed/test_torchrun_example_moe.py
32	35	tests/distributed/test_utils.py
102	84	tests/engine/test_arg_utils.py
10	11	tests/engine/test_short_mm_context.py
64	80	tests/entrypoints/conftest.py
17	12	tests/entrypoints/llm/test_accuracy.py
26	62	tests/entrypoints/llm/test_chat.py
7	5	tests/entrypoints/llm/test_collective_rpc.py
9	6	tests/entrypoints/llm/test_generate.py
2	3	tests/entrypoints/llm/test_gpu_utilization.py
2	2	tests/entrypoints/llm/test_prompt_validation.py
5	5	tests/entrypoints/offline_mode/test_offline_mode.py
3	3	tests/entrypoints/openai/conftest.py
16	12	tests/entrypoints/openai/correctness/test_lmeval.py
30	29	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py
12	16	tests/entrypoints/openai/test_async_tokenization.py
131	143	tests/entrypoints/openai/test_audio.py
31	27	tests/entrypoints/openai/test_basic.py
328	362	tests/entrypoints/openai/test_chat.py
19	26	tests/entrypoints/openai/test_chat_echo.py
2	8	tests/entrypoints/openai/test_chat_logit_bias_validation.py
45	32	tests/entrypoints/openai/test_chat_template.py
47	54	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
14	13	tests/entrypoints/openai/test_chunked_prompt.py
91	68	tests/entrypoints/openai/test_cli_args.py
13	17	tests/entrypoints/openai/test_collective_rpc.py
76	93	tests/entrypoints/openai/test_completion_with_function_calling.py
45	38	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
22	23	tests/entrypoints/openai/test_default_mm_loras.py
74	96	tests/entrypoints/openai/test_lora_adapters.py
34	32	tests/entrypoints/openai/test_lora_resolvers.py
103	82	tests/entrypoints/openai/test_metrics.py
1	2	tests/entrypoints/openai/test_models.py
4	7	tests/entrypoints/openai/test_oot_registration.py
18	14	tests/entrypoints/openai/test_openai_schema.py
7	5	tests/entrypoints/openai/test_optional_middleware.py
25	24	tests/entrypoints/openai/test_prompt_validation.py
32	26	tests/entrypoints/openai/test_response_api_mcp_tools.py
117	160	tests/entrypoints/openai/test_response_api_with_harmony.py
55	70	tests/entrypoints/openai/test_return_token_ids.py
14	16	tests/entrypoints/openai/test_return_tokens_as_ids.py
19	18	tests/entrypoints/openai/test_root_path.py
81	43	tests/entrypoints/openai/test_run_batch.py
114	123	tests/entrypoints/openai/test_serving_chat.py
6	6	tests/entrypoints/openai/test_serving_engine.py
31	28	tests/entrypoints/openai/test_serving_models.py
16	27	tests/entrypoints/openai/test_serving_responses.py
3	6	tests/entrypoints/openai/test_shutdown.py
21	23	tests/entrypoints/openai/test_skip_tokenizer.py
13	14	tests/entrypoints/openai/test_sleep.py
17	12	tests/entrypoints/openai/test_tensorizer_entrypoint.py
8	7	tests/entrypoints/openai/test_token_in_token_out.py
71	95	tests/entrypoints/openai/test_tokenization.py
64	53	tests/entrypoints/openai/test_transcription_validation.py
31	28	tests/entrypoints/openai/test_translation_validation.py
129	147	tests/entrypoints/openai/test_video.py
166	203	tests/entrypoints/openai/test_vision.py
64	61	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
108	82	tests/entrypoints/openai/tool_parsers/test_hunyuan_a13b_tool_parser.py
14	10	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
150	99	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
117	72	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
56	36	tests/entrypoints/openai/tool_parsers/utils.py
6	6	tests/entrypoints/pooling/correctness/test_mteb_embed.py
10	10	tests/entrypoints/pooling/correctness/test_mteb_score.py
19	18	tests/entrypoints/pooling/llm/test_classify.py
18	17	tests/entrypoints/pooling/llm/test_embedding.py
8	6	tests/entrypoints/pooling/llm/test_encode.py
19	18	tests/entrypoints/pooling/llm/test_reward.py
19	16	tests/entrypoints/pooling/llm/test_score.py
42	69	tests/entrypoints/pooling/openai/test_classification.py
147	104	tests/entrypoints/pooling/openai/test_embedding.py
19	17	tests/entrypoints/pooling/openai/test_embedding_dimensions.py
56	44	tests/entrypoints/pooling/openai/test_embedding_long_text.py
89	86	tests/entrypoints/pooling/openai/test_pooling.py
54	51	tests/entrypoints/pooling/openai/test_rerank.py
95	91	tests/entrypoints/pooling/openai/test_score.py
12	16	tests/entrypoints/pooling/openai/test_truncation.py
18	30	tests/entrypoints/pooling/openai/test_vision_embedding.py
33	37	tests/entrypoints/test_api_server_process_manager.py
699	1186	tests/entrypoints/test_chat_utils.py
62	32	tests/entrypoints/test_context.py
58	66	tests/entrypoints/test_renderer.py
1	2	tests/entrypoints/test_ssl_cert_refresher.py
1	1	tests/evals/gpt_oss/__init__.py
6	8	tests/evals/gpt_oss/conftest.py
29	12	tests/evals/gpt_oss/test_gpqa_correctness.py
1	1	tests/evals/gsm8k/__init__.py
11	14	tests/evals/gsm8k/conftest.py
49	52	tests/evals/gsm8k/gsm8k_eval.py
5	4	tests/evals/gsm8k/test_gsm8k_correctness.py
1	5	tests/kernels/allclose_default.py
1	2	tests/kernels/attention/conftest.py
30	33	tests/kernels/attention/test_aiter_flash_attn.py
127	62	tests/kernels/attention/test_attention.py
81	108	tests/kernels/attention/test_attention_selector.py
305	238	tests/kernels/attention/test_cache.py
25	37	tests/kernels/attention/test_cascade_flash_attn.py
69	49	tests/kernels/attention/test_cutlass_mla_decode.py
74	59	tests/kernels/attention/test_deepgemm_attention.py
70	62	tests/kernels/attention/test_flash_attn.py
125	123	tests/kernels/attention/test_flashinfer.py
18	23	tests/kernels/attention/test_flashinfer_mla_decode.py
108	90	tests/kernels/attention/test_flashinfer_trtllm_attention.py
66	50	tests/kernels/attention/test_flashmla.py
46	36	tests/kernels/attention/test_flashmla_sparse.py
41	59	tests/kernels/attention/test_lightning_attn.py
136	82	tests/kernels/attention/test_merge_attn_states.py
48	32	tests/kernels/attention/test_mha_attn.py
13	21	tests/kernels/attention/test_mla_decode_cpu.py
12	23	tests/kernels/attention/test_pack_unpack_triton.py
198	196	tests/kernels/attention/test_prefix_prefill.py
7	30	tests/kernels/attention/test_rocm_attention_selector.py
5	7	tests/kernels/attention/test_triton_decode_attention.py
31	31	tests/kernels/attention/test_triton_unified_attention.py
30	27	tests/kernels/core/test_activation.py
61	55	tests/kernels/core/test_fused_quant_layernorm.py
46	29	tests/kernels/core/test_layernorm.py
85	69	tests/kernels/core/test_mrope.py
3	3	tests/kernels/core/test_permute_cols.py
76	37	tests/kernels/core/test_pos_encoding.py
33	23	tests/kernels/core/test_rotary_embedding.py
4	14	tests/kernels/core/test_uva.py
128	132	tests/kernels/mamba/test_causal_conv1d.py
48	36	tests/kernels/mamba/test_mamba_mixer2.py
311	287	tests/kernels/mamba/test_mamba_ssm.py
154	139	tests/kernels/mamba/test_mamba_ssm_ssd.py
50	45	tests/kernels/moe/modular_kernel_tools/cli_args.py
162	152	tests/kernels/moe/modular_kernel_tools/common.py
75	52	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
100	66	tests/kernels/moe/modular_kernel_tools/mk_objects.py
12	16	tests/kernels/moe/modular_kernel_tools/parallel_utils.py
30	21	tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py
60	45	tests/kernels/moe/parallel_utils.py
11	10	tests/kernels/moe/test_batched_deepgemm.py
59	40	tests/kernels/moe/test_batched_moe.py
42	45	tests/kernels/moe/test_block_fp8.py
28	29	tests/kernels/moe/test_block_int8.py
58	53	tests/kernels/moe/test_count_expert_num_tokens.py
25	19	tests/kernels/moe/test_cutlass_grouped_gemm.py
196	169	tests/kernels/moe/test_cutlass_moe.py
183	132	tests/kernels/moe/test_deepep_deepgemm_moe.py
147	112	tests/kernels/moe/test_deepep_moe.py
35	39	tests/kernels/moe/test_deepgemm.py
32	32	tests/kernels/moe/test_flashinfer.py
46	36	tests/kernels/moe/test_flashinfer_moe.py
38	35	tests/kernels/moe/test_gpt_oss_triton_kernels.py
35	30	tests/kernels/moe/test_grouped_topk.py
73	46	tests/kernels/moe/test_modular_kernel_combinations.py
250	205	tests/kernels/moe/test_moe.py
97	108	tests/kernels/moe/test_moe_align_block_size.py
185	140	tests/kernels/moe/test_moe_permute_unpermute.py
397	279	tests/kernels/moe/test_mxfp4_moe.py
55	50	tests/kernels/moe/test_nvfp4_moe.py
115	81	tests/kernels/moe/test_pplx_cutlass_moe.py
193	139	tests/kernels/moe/test_pplx_moe.py
101	98	tests/kernels/moe/test_rocm_aiter_topk.py
18	15	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
21	24	tests/kernels/moe/test_triton_moe_ptpc_fp8.py
110	105	tests/kernels/moe/utils.py
83	73	tests/kernels/quant_utils.py
9	10	tests/kernels/quantization/nvfp4_utils.py
52	26	tests/kernels/quantization/test_allspark_gemm.py
26	24	tests/kernels/quantization/test_awq.py
52	53	tests/kernels/quantization/test_awq_triton.py
48	44	tests/kernels/quantization/test_block_fp8.py
11	10	tests/kernels/quantization/test_block_int8.py
74	103	tests/kernels/quantization/test_cutlass_2of4_sparse.py
316	271	tests/kernels/quantization/test_cutlass_scaled_mm.py
100	68	tests/kernels/quantization/test_cutlass_w4a8.py
30	30	tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py
2	3	tests/kernels/quantization/test_flashinfer_scaled_mm.py
42	37	tests/kernels/quantization/test_fp8_quant.py
49	45	tests/kernels/quantization/test_fp8_quant_group.py
30	21	tests/kernels/quantization/test_ggml.py
64	44	tests/kernels/quantization/test_gguf.py
12	15	tests/kernels/quantization/test_gptq.py
2	1	tests/kernels/quantization/test_hadacore.py
25	27	tests/kernels/quantization/test_int8_kernel.py
49	47	tests/kernels/quantization/test_int8_quant.py
148	110	tests/kernels/quantization/test_machete_mm.py
189	96	tests/kernels/quantization/test_marlin_gemm.py
35	9	tests/kernels/quantization/test_nvfp4_quant.py
50	35	tests/kernels/quantization/test_nvfp4_scaled_mm.py
5	6	tests/kernels/quantization/test_per_token_group_quant.py
30	37	tests/kernels/quantization/test_rocm_skinny_gemms.py
26	25	tests/kernels/quantization/test_silu_mul_nvfp4_quant.py
29	26	tests/kernels/quantization/test_triton_scaled_mm.py
35	29	tests/kernels/test_apply_repetition_penalties.py
64	50	tests/kernels/test_flex_attention.py
9	11	tests/kernels/test_fused_quant_activation.py
36	30	tests/kernels/test_onednn.py
27	61	tests/kernels/test_shuffle_rows.py
273	257	tests/kernels/test_triton_flash_attention.py
379	311	tests/kernels/utils.py
10	13	tests/kv_transfer/test_lookup_buffer.py
10	15	tests/kv_transfer/test_module.py
12	18	tests/kv_transfer/test_send_recv.py
68	51	tests/lora/conftest.py
21	26	tests/lora/test_add_lora.py
36	32	tests/lora/test_chatglm3_tp.py
5	8	tests/lora/test_default_mm_loras.py
434	372	tests/lora/test_layers.py
84	55	tests/lora/test_llama_tp.py
10	16	tests/lora/test_llm_with_multi_loras.py
24	18	tests/lora/test_lora_checkpoints.py
24	20	tests/lora/test_lora_functions.py
8	3	tests/lora/test_lora_huggingface.py
241	204	tests/lora/test_lora_manager.py
25	28	tests/lora/test_minicpmv_tp.py
18	11	tests/lora/test_mixtral.py
10	17	tests/lora/test_peft_helper.py
142	94	tests/lora/test_punica_ops.py
31	37	tests/lora/test_quant_model.py
62	65	tests/lora/test_qwen2vl.py
6	5	tests/lora/test_resolver.py
41	35	tests/lora/test_transformers_model.py
65	50	tests/lora/test_utils.py
21	16	tests/lora/test_worker.py
49	51	tests/lora/utils.py
12	14	tests/model_executor/model_loader/fastsafetensors_loader/test_weight_utils.py
14	13	tests/model_executor/model_loader/runai_model_streamer/test_runai_utils.py
8	7	tests/model_executor/model_loader/runai_model_streamer/test_weight_utils.py
7	14	tests/model_executor/model_loader/tensorizer_loader/conftest.py
139	105	tests/model_executor/model_loader/tensorizer_loader/test_tensorizer.py
2	5	tests/model_executor/model_loader/test_registry.py
46	36	tests/model_executor/model_loader/test_sharded_state_loader.py
56	29	tests/model_executor/test_enabled_custom_ops.py
41	29	tests/model_executor/test_model_load_with_params.py
24	15	tests/model_executor/test_weight_utils.py
41	28	tests/models/language/generation/test_common.py
6	6	tests/models/language/generation/test_gemma.py
4	2	tests/models/language/generation/test_granite.py
186	173	tests/models/language/generation/test_hybrid.py
141	150	tests/models/language/generation/test_mistral.py
37	41	tests/models/language/generation/test_phimoe.py
34	31	tests/models/language/generation_ppl_test/ppl_utils.py
15	16	tests/models/language/pooling/embed_utils.py
24	25	tests/models/language/pooling/test_auto_prefix_cache_support.py
15	12	tests/models/language/pooling/test_classification.py
20	16	tests/models/language/pooling/test_embedding.py
11	9	tests/models/language/pooling/test_gritlm.py
60	57	tests/models/language/pooling/test_mm_classifier_conversion.py
6	5	tests/models/language/pooling/test_multilabel_classification_support.py
42	41	tests/models/language/pooling/test_nomic_max_model_len.py
49	47	tests/models/language/pooling/test_pooler_config_init_behaviour.py
14	13	tests/models/language/pooling/test_reward.py
30	39	tests/models/language/pooling/test_scoring.py
3	3	tests/models/language/pooling/test_token_classification.py
29	23	tests/models/language/pooling/test_truncation_control.py
99	99	tests/models/language/pooling_mteb_test/mteb_utils.py
77	66	tests/models/language/pooling_mteb_test/test_baai.py
41	41	tests/models/language/pooling_mteb_test/test_bge_reranker_v2_gemma.py
18	10	tests/models/language/pooling_mteb_test/test_cross_encoder.py
85	65	tests/models/language/pooling_mteb_test/test_gte.py
37	32	tests/models/language/pooling_mteb_test/test_intfloat.py
47	39	tests/models/language/pooling_mteb_test/test_jina.py
28	29	tests/models/language/pooling_mteb_test/test_mxbai_rerank.py
26	23	tests/models/language/pooling_mteb_test/test_nomic.py
32	35	tests/models/language/pooling_mteb_test/test_qwen3_reranker.py
59	46	tests/models/language/pooling_mteb_test/test_snowflake_arctic_embed.py
12	9	tests/models/language/pooling_mteb_test/test_st_projector.py
46	21	tests/models/multimodal/generation/test_common.py
39	34	tests/models/multimodal/generation/test_granite_speech.py
19	19	tests/models/multimodal/generation/test_interleaved.py
167	151	tests/models/multimodal/generation/test_maverick.py
96	59	tests/models/multimodal/generation/test_phi4_multimodal.py
103	74	tests/models/multimodal/generation/test_phi4mm.py
63	48	tests/models/multimodal/generation/test_pixtral.py
50	42	tests/models/multimodal/generation/test_qwen2_5_vl.py
161	118	tests/models/multimodal/generation/test_qwen2_vl.py
77	69	tests/models/multimodal/generation/test_ultravox.py
39	31	tests/models/multimodal/generation/test_voxtral.py
14	16	tests/models/multimodal/generation/test_whisper.py
95	73	tests/models/multimodal/generation/vlm_utils/builders.py
55	39	tests/models/multimodal/generation/vlm_utils/case_filtering.py
34	32	tests/models/multimodal/generation/vlm_utils/core.py
22	14	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
151	142	tests/models/multimodal/generation/vlm_utils/model_utils.py
65	34	tests/models/multimodal/generation/vlm_utils/runners.py
18	7	tests/models/multimodal/generation/vlm_utils/types.py
17	18	tests/models/multimodal/pooling/test_clip.py
58	59	tests/models/multimodal/pooling/test_dse_qwen2_vl.py
14	12	tests/models/multimodal/pooling/test_intern_vit.py
43	35	tests/models/multimodal/pooling/test_jinavl_reranker.py
27	26	tests/models/multimodal/pooling/test_llava_next.py
25	22	tests/models/multimodal/pooling/test_phi3v.py
11	11	tests/models/multimodal/pooling/test_prithvi_mae.py
15	14	tests/models/multimodal/pooling/test_radio.py
49	43	tests/models/multimodal/processing/test_common.py
14	15	tests/models/multimodal/processing/test_glm4_1v.py
16	13	tests/models/multimodal/processing/test_h2ovl.py
9	5	tests/models/multimodal/processing/test_idefics3.py
8	8	tests/models/multimodal/processing/test_internvl.py
19	18	tests/models/multimodal/processing/test_llama4.py
29	18	tests/models/multimodal/processing/test_llava_next.py
32	24	tests/models/multimodal/processing/test_llava_onevision.py
16	10	tests/models/multimodal/processing/test_minimax_vl_01.py
13	8	tests/models/multimodal/processing/test_mllama4.py
9	10	tests/models/multimodal/processing/test_nemotron_vl.py
3	1	tests/models/multimodal/processing/test_phi3v.py
6	4	tests/models/multimodal/processing/test_phi4mm.py
5	4	tests/models/multimodal/processing/test_qwen2_vl.py
9	5	tests/models/multimodal/processing/test_smolvlm.py
59	46	tests/models/multimodal/processing/test_tensor_schema.py
6	4	tests/models/multimodal/test_mapping.py
47	38	tests/models/quantization/test_awq.py
16	10	tests/models/quantization/test_bitblas.py
143	117	tests/models/quantization/test_bitsandbytes.py
59	40	tests/models/quantization/test_fp8.py
40	32	tests/models/quantization/test_gguf.py
12	8	tests/models/quantization/test_gptq_bitblas.py
28	22	tests/models/quantization/test_gptq_marlin.py
25	16	tests/models/quantization/test_gptq_marlin_24.py
20	17	tests/models/quantization/test_modelopt.py
11	10	tests/models/quantization/test_mxfp4.py
23	20	tests/models/quantization/test_nvfp4.py
14	6	tests/models/registry.py
51	35	tests/models/test_initialization.py
21	20	tests/models/test_oot_registration.py
38	25	tests/models/test_registry.py
18	17	tests/models/test_terratorch.py
75	52	tests/models/test_transformers.py
21	17	tests/models/test_utils.py
121	93	tests/models/test_vision.py
115	89	tests/models/utils.py
18	19	tests/multimodal/test_audio.py
24	18	tests/multimodal/test_cache.py
2	4	tests/multimodal/test_hasher.py
26	19	tests/multimodal/test_image.py
12	22	tests/multimodal/test_inputs.py
14	8	tests/multimodal/test_processing.py
4	10	tests/multimodal/test_registry.py
49	35	tests/multimodal/test_utils.py
18	23	tests/multimodal/test_video.py
7	8	tests/multimodal/utils.py
2	3	tests/plugins/lora_resolvers/test_filesystem_resolver.py
56	62	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
2	4	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/types.py
8	7	tests/plugins/vllm_add_dummy_model/setup.py
1	2	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py
14	11	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
12	10	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
1	3	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py
9	7	tests/plugins/vllm_add_dummy_platform/setup.py
1	3	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py
1	2	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_custom_ops.py
12	3	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
15	17	tests/plugins_tests/test_io_processor_plugins.py
13	4	tests/plugins_tests/test_platform_plugins.py
1	4	tests/plugins_tests/test_scheduler_plugins.py
65	60	tests/quantization/reference_mxfp4.py
11	10	tests/quantization/test_auto_round.py
28	27	tests/quantization/test_blackwell_moe.py
163	121	tests/quantization/test_compressed_tensors.py
2	2	tests/quantization/test_configs.py
107	76	tests/quantization/test_cpu_offload.py
7	5	tests/quantization/test_experts_int8.py
64	44	tests/quantization/test_fp8.py
33	30	tests/quantization/test_gptq_dynamic.py
9	9	tests/quantization/test_ipex_quant.py
12	12	tests/quantization/test_lm_head.py
25	19	tests/quantization/test_modelopt.py
15	11	tests/quantization/test_ptpc_fp8.py
53	48	tests/quantization/test_quark.py
34	19	tests/quantization/test_register_quantization_config.py
6	4	tests/quantization/test_rtn.py
73	68	tests/quantization/test_torchao.py
44	60	tests/reasoning/test_base_thinking_reasoning_parser.py
9	8	tests/reasoning/test_deepseekr1_reasoning_parser.py
19	17	tests/reasoning/test_glm4_moe_reasoning_parser.py
22	26	tests/reasoning/test_granite_reasoning_parser.py
15	20	tests/reasoning/test_hunyuan_reasoning_parser.py
37	27	tests/reasoning/test_mistral_reasoning_parser.py
8	13	tests/reasoning/test_olmo3_reasoning_parser.py
7	7	tests/reasoning/test_qwen3_reasoning_parser.py
22	23	tests/reasoning/test_seedoss_reasoning_parser.py
18	15	tests/reasoning/utils.py
32	34	tests/samplers/test_beam_search.py
3	3	tests/samplers/test_ignore_eos.py
55	53	tests/samplers/test_no_bad_words.py
13	11	tests/samplers/test_ranks.py
30	22	tests/speculative_decoding/speculators/test_eagle3.py
2	1	tests/standalone_tests/lazy_imports.py
121	95	tests/test_config.py
8	8	tests/test_embedded_commit.py
59	63	tests/test_envs.py
13	14	tests/test_inputs.py
34	30	tests/test_logger.py
9	7	tests/test_outputs.py
6	5	tests/test_pooling_params.py
13	12	tests/test_regression.py
22	19	tests/test_routing_simulator.py
18	15	tests/test_scalartype.py
24	24	tests/test_seed_behavior.py
21	13	tests/test_sequence.py
3	4	tests/test_triton_utils.py
2	1	tests/test_version.py
8	5	tests/test_vllm_port.py
9	10	tests/tokenization/test_cached_tokenizer.py
80	55	tests/tokenization/test_detokenize.py
1	1	tests/tokenization/test_do_lower_case.py
3	4	tests/tokenization/test_get_eos.py
177	158	tests/tokenization/test_mistral_tokenizer.py
1	1	tests/tokenization/test_tokenizer.py
14	16	tests/tokenization/test_tokenizer_registry.py
17	13	tests/tool_use/conftest.py
6	4	tests/tool_use/mistral/conftest.py
3	3	tests/tool_use/mistral/test_mistral_tool_calls.py
6	7	tests/tool_use/mistral/utils.py
46	58	tests/tool_use/test_chat_completion_request_validations.py
22	16	tests/tool_use/test_chat_completions.py
15	10	tests/tool_use/test_deepseekv31_tool_parser.py
107	70	tests/tool_use/test_glm4_moe_tool_parser.py
138	110	tests/tool_use/test_jamba_tool_parser.py
63	47	tests/tool_use/test_kimi_k2_tool_parser.py
396	387	tests/tool_use/test_minimax_tool_parser.py
154	110	tests/tool_use/test_openai_tool_parser.py
40	33	tests/tool_use/test_parallel_tool_calls.py
355	316	tests/tool_use/test_qwen3coder_tool_parser.py
155	113	tests/tool_use/test_seed_oss_tool_parser.py
22	15	tests/tool_use/test_tool_calls.py
168	180	tests/tool_use/test_tool_choice_required.py
227	153	tests/tool_use/test_xlam_tool_parser.py
195	215	tests/tool_use/utils.py
13	10	tests/tools/test_config_validator.py
47	29	tests/tpu/lora/test_lora.py
15	14	tests/tpu/test_compilation.py
17	14	tests/tpu/test_custom_dispatcher.py
6	3	tests/tpu/test_moe_pallas.py
7	7	tests/tpu/test_quantization_accuracy.py
10	11	tests/transformers_utils/test_config_parser_registry.py
377	318	tests/utils.py
42	26	tests/utils_/test_gc_utils.py
7	13	tests/utils_/test_tensor_schema.py
158	122	tests/utils_/test_utils.py
166	154	tests/v1/attention/test_attention_backends.py
28	21	tests/v1/attention/test_attention_backends_selection.py
55	51	tests/v1/attention/test_attention_splitting.py
41	36	tests/v1/attention/test_chunked_local_attention.py
190	164	tests/v1/attention/test_mla_backends.py
130	149	tests/v1/attention/test_sparse_mla_backends.py
153	143	tests/v1/attention/utils.py
40	35	tests/v1/core/test_async_scheduler.py
3	7	tests/v1/core/test_encoder_cache_manager.py
439	349	tests/v1/core/test_kv_cache_utils.py
13	13	tests/v1/core/test_kv_sharing.py
408	333	tests/v1/core/test_prefix_caching.py
305	266	tests/v1/core/test_scheduler.py
9	7	tests/v1/core/test_scheduler_e2e.py
94	58	tests/v1/core/test_single_type_kv_cache_manager.py
49	29	tests/v1/core/utils.py
137	105	tests/v1/cudagraph/test_cudagraph_dispatch.py
37	28	tests/v1/cudagraph/test_cudagraph_mode.py
46	37	tests/v1/distributed/test_async_llm_dp.py
86	86	tests/v1/distributed/test_external_lb_dp.py
99	95	tests/v1/distributed/test_hybrid_lb_dp.py
211	195	tests/v1/distributed/test_internal_lb_dp.py
4	2	tests/v1/e2e/test_cascade_attention.py
1	1	tests/v1/e2e/test_context_length.py
29	22	tests/v1/e2e/test_correctness_sliding_window.py
12	10	tests/v1/e2e/test_kv_sharing_fast_prefill.py
116	87	tests/v1/e2e/test_min_tokens.py
108	57	tests/v1/e2e/test_spec_decode.py
19	15	tests/v1/engine/conftest.py
107	83	tests/v1/engine/test_async_llm.py
7	8	tests/v1/engine/test_engine_args.py
42	54	tests/v1/engine/test_engine_core.py
144	152	tests/v1/engine/test_engine_core_client.py
130	15	tests/v1/engine/test_fast_incdec_prefix_err.py
30	30	tests/v1/engine/test_llm_engine.py
275	220	tests/v1/engine/test_output_processor.py
49	65	tests/v1/engine/test_processor_multi_modal_uuids.py
41	28	tests/v1/engine/utils.py
50	64	tests/v1/entrypoints/conftest.py
208	177	tests/v1/entrypoints/llm/test_struct_output_generate.py
3	3	tests/v1/entrypoints/openai/responses/conftest.py
19	30	tests/v1/entrypoints/openai/responses/test_basic.py
60	60	tests/v1/entrypoints/openai/responses/test_image.py
10	8	tests/v1/entrypoints/openai/responses/test_stateful.py
5	19	tests/v1/entrypoints/openai/responses/test_structured_output.py
38	44	tests/v1/entrypoints/openai/test_chat_completion.py
192	193	tests/v1/entrypoints/openai/test_completion.py
8	14	tests/v1/entrypoints/openai/test_completion_with_image_embeds.py
25	31	tests/v1/entrypoints/openai/test_multi_api_servers.py
33	25	tests/v1/executor/test_executor.py
27	18	tests/v1/generation/test_batch_invariance.py
18	15	tests/v1/kv_connector/nixl_integration/test_accuracy.py
32	27	tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py
19	20	tests/v1/kv_connector/nixl_integration/test_edge_cases.py
79	87	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
9	6	tests/v1/kv_connector/unit/test_kv_connector_lifecyle.py
53	59	tests/v1/kv_connector/unit/test_kv_load_failure_recovery.py
63	51	tests/v1/kv_connector/unit/test_multi_connector.py
274	219	tests/v1/kv_connector/unit/test_nixl_connector.py
135	110	tests/v1/kv_connector/unit/test_offloading_connector.py
28	21	tests/v1/kv_connector/unit/test_output_aggreagator.py
46	31	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
120	80	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
112	82	tests/v1/kv_connector/unit/test_shared_storage_connector.py
82	49	tests/v1/kv_connector/unit/utils.py
33	26	tests/v1/kv_offload/test_cpu_gpu.py
47	32	tests/v1/kv_offload/test_cpu_manager.py
1	4	tests/v1/kv_offload/test_cpu_offloading.py
14	13	tests/v1/kv_offload/test_worker.py
238	163	tests/v1/logits_processors/test_correctness.py
65	41	tests/v1/logits_processors/test_custom_offline.py
23	20	tests/v1/logits_processors/test_custom_online.py
26	18	tests/v1/logits_processors/utils.py
12	11	tests/v1/metrics/test_engine_logger_apis.py
28	16	tests/v1/metrics/test_metrics_reader.py
27	21	tests/v1/metrics/test_ray_metrics.py
14	12	tests/v1/metrics/test_stats.py
114	92	tests/v1/sample/test_logprobs.py
16	13	tests/v1/sample/test_logprobs_e2e.py
126	124	tests/v1/sample/test_rejection_sampler.py
108	82	tests/v1/sample/test_sampler.py
7	9	tests/v1/sample/test_sampling_params_e2e.py
21	22	tests/v1/sample/test_topk_topp_sampler.py
22	20	tests/v1/sample/utils.py
29	19	tests/v1/shutdown/test_delete.py
28	24	tests/v1/shutdown/test_forward_error.py
9	8	tests/v1/shutdown/test_processor_error.py
27	18	tests/v1/shutdown/test_startup_error.py
202	184	tests/v1/spec_decode/test_eagle.py
9	7	tests/v1/spec_decode/test_max_len.py
57	52	tests/v1/spec_decode/test_mtp.py
47	51	tests/v1/spec_decode/test_ngram.py
31	28	tests/v1/spec_decode/test_tree_attention.py
44	109	tests/v1/structured_output/test_utils.py
0	1	tests/v1/test_oracle.py
38	37	tests/v1/test_serial_utils.py
51	39	tests/v1/tpu/test_basic.py
39	31	tests/v1/tpu/test_kv_cache_update_kernel.py
7	16	tests/v1/tpu/test_mha_attn.py
22	27	tests/v1/tpu/test_multimodal.py
9	11	tests/v1/tpu/test_pallas.py
38	30	tests/v1/tpu/test_perf.py
29	30	tests/v1/tpu/test_sampler.py
15	8	tests/v1/tpu/test_spmd_model_weight_loading.py
38	44	tests/v1/tpu/test_topk_topp_sampler.py
22	13	tests/v1/tpu/test_tpu_int8.py
8	5	tests/v1/tpu/test_tpu_qkv_linear.py
65	66	tests/v1/tpu/worker/test_tpu_model_runner.py
49	36	tests/v1/tracing/test_tracing.py
16	16	tests/v1/utils.py
77	68	tests/v1/worker/test_gpu_input_batch.py
167	150	tests/v1/worker/test_gpu_model_runner.py
24	30	tests/v1/worker/test_utils.py
44	28	tests/v1/worker/test_worker_memory_snapshot.py
3	3	tests/vllm_test_utils/setup.py
2	2	tests/vllm_test_utils/vllm_test_utils/blame.py
15	12	tests/vllm_test_utils/vllm_test_utils/monitor.py
18	14	tests/weight_loading/test_weight_loading.py
32	26	tools/check_init_lazy_imports.py
6	3	tools/check_spdx_header.py
11	10	tools/check_triton_import.py
12	14	tools/enforce_regex_import.py
37	39	tools/generate_cmake_presets.py
37	33	tools/pre_commit/check_pickle_imports.py
10	5	tools/pre_commit/mypy.py
164	133	tools/profiler/nsys_profile_tools/gputrc2graph.py
35	32	tools/profiler/print_layerwise_table.py
235	197	tools/profiler/visualize_layerwise_profile.py
63	54	tools/report_build_time_ninja.py
45	29	tools/validate_config.py
4	4	use_existing_torch.py
13	8	vllm/__init__.py
4	8	vllm/_bc_linter.py
1193	734	vllm/_custom_ops.py
106	64	vllm/_ipex_ops.py
2	4	vllm/assets/audio.py
3	3	vllm/assets/base.py
19	10	vllm/assets/image.py
6	5	vllm/assets/video.py
5	2	vllm/attention/__init__.py
7	7	vllm/attention/backends/abstract.py
1	0	vllm/attention/backends/utils.py
150	134	vllm/attention/layer.py
39	32	vllm/attention/layers/chunked_local_attention.py
61	46	vllm/attention/layers/cross_attention.py
48	33	vllm/attention/layers/encoder_only_attention.py
92	96	vllm/attention/ops/chunked_prefill_paged_decode.py
112	89	vllm/attention/ops/common.py
74	46	vllm/attention/ops/flashmla.py
14	9	vllm/attention/ops/merge_attn_states.py
12	11	vllm/attention/ops/paged_attn.py
15	13	vllm/attention/ops/pallas_kv_cache_update.py
367	271	vllm/attention/ops/prefix_prefill.py
42	40	vllm/attention/ops/rocm_aiter_mla.py
39	17	vllm/attention/ops/rocm_aiter_paged_attn.py
84	63	vllm/attention/ops/triton_decode_attention.py
80	132	vllm/attention/ops/triton_flash_attention.py
22	13	vllm/attention/ops/triton_merge_attn_states.py
35	25	vllm/attention/ops/triton_reshape_and_cache_flash.py
263	216	vllm/attention/ops/triton_unified_attention.py
44	34	vllm/attention/selector.py
32	14	vllm/attention/utils/fa_utils.py
12	12	vllm/attention/utils/kv_sharing_utils.py
5	4	vllm/beam_search.py
405	385	vllm/benchmarks/datasets.py
27	25	vllm/benchmarks/latency.py
56	79	vllm/benchmarks/lib/endpoint_request_func.py
11	12	vllm/benchmarks/lib/ready_checker.py
7	8	vllm/benchmarks/lib/utils.py
325	234	vllm/benchmarks/serve.py
266	192	vllm/benchmarks/throughput.py
234	200	vllm/collect_env.py
80	55	vllm/compilation/activation_quant_fusion.py
177	120	vllm/compilation/backends.py
383	304	vllm/compilation/collective_fusion.py
107	69	vllm/compilation/compiler_interface.py
2	1	vllm/compilation/counter.py
33	25	vllm/compilation/cuda_graph.py
89	75	vllm/compilation/decorators.py
75	63	vllm/compilation/fix_functionalization.py
221	173	vllm/compilation/fusion.py
145	100	vllm/compilation/fusion_attn.py
4	4	vllm/compilation/fx_utils.py
7	9	vllm/compilation/inductor_pass.py
8	4	vllm/compilation/monitor.py
4	5	vllm/compilation/noop_elimination.py
15	12	vllm/compilation/piecewise_backend.py
1	0	vllm/compilation/post_cleanup.py
138	123	vllm/compilation/sequence_parallelism.py
5	3	vllm/compilation/torch25_custom_graph_pass.py
31	25	vllm/compilation/vllm_inductor_pass.py
25	21	vllm/compilation/wrapper.py
43	19	vllm/config/__init__.py
22	16	vllm/config/cache.py
139	105	vllm/config/compilation.py
4	3	vllm/config/device.py
12	12	vllm/config/kv_transfer.py
5	4	vllm/config/load.py
16	12	vllm/config/lora.py
415	290	vllm/config/model.py
11	9	vllm/config/multimodal.py
20	16	vllm/config/observability.py
98	68	vllm/config/parallel.py
1	2	vllm/config/pooler.py
46	31	vllm/config/scheduler.py
186	150	vllm/config/speculative.py
14	11	vllm/config/structured_outputs.py
21	17	vllm/config/utils.py
219	158	vllm/config/vllm.py
25	21	vllm/connections.py
47	32	vllm/device_allocator/cumem.py
12	10	vllm/distributed/communication_op.py
114	85	vllm/distributed/device_communicators/all2all.py
61	41	vllm/distributed/device_communicators/all_reduce_utils.py
68	68	vllm/distributed/device_communicators/base_device_communicator.py
59	51	vllm/distributed/device_communicators/cpu_communicator.py
82	67	vllm/distributed/device_communicators/cuda_communicator.py
44	34	vllm/distributed/device_communicators/cuda_wrapper.py
72	61	vllm/distributed/device_communicators/custom_all_reduce.py
1	2	vllm/distributed/device_communicators/mnnvl_compat.py
105	60	vllm/distributed/device_communicators/pynccl.py
26	21	vllm/distributed/device_communicators/pynccl_allocator.py
239	110	vllm/distributed/device_communicators/pynccl_wrapper.py
59	44	vllm/distributed/device_communicators/quick_all_reduce.py
18	17	vllm/distributed/device_communicators/ray_communicator.py
81	61	vllm/distributed/device_communicators/shm_broadcast.py
105	84	vllm/distributed/device_communicators/shm_object_storage.py
34	31	vllm/distributed/device_communicators/symm_mem.py
16	10	vllm/distributed/device_communicators/tpu_communicator.py
33	29	vllm/distributed/device_communicators/xpu_communicator.py
2	2	vllm/distributed/eplb/__init__.py
109	82	vllm/distributed/eplb/eplb_state.py
45	40	vllm/distributed/eplb/rebalance_algo.py
22	18	vllm/distributed/eplb/rebalance_execute.py
41	37	vllm/distributed/kv_events.py
13	6	vllm/distributed/kv_transfer/__init__.py
27	16	vllm/distributed/kv_transfer/kv_connector/factory.py
60	48	vllm/distributed/kv_transfer/kv_connector/utils.py
3	1	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
46	37	vllm/distributed/kv_transfer/kv_connector/v1/base.py
30	23	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
23	19	vllm/distributed/kv_transfer/kv_connector/v1/metrics.py
62	51	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
336	233	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
82	70	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
109	79	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
178	108	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
34	28	vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool.py
83	67	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
25	20	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
32	28	vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py
57	51	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
6	6	vllm/distributed/kv_transfer/kv_pipe/base.py
65	60	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
37	33	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
14	10	vllm/distributed/kv_transfer/kv_transfer_state.py
321	282	vllm/distributed/parallel_state.py
59	49	vllm/distributed/tpu_distributed_utils.py
72	68	vllm/distributed/utils.py
677	528	vllm/engine/arg_utils.py
268	157	vllm/engine/metrics.py
2	1	vllm/engine/metrics_types.py
82	71	vllm/engine/protocol.py
20	16	vllm/entrypoints/api_server.py
76	144	vllm/entrypoints/chat_utils.py
2	3	vllm/entrypoints/cli/__init__.py
1	1	vllm/entrypoints/cli/benchmark/base.py
1	1	vllm/entrypoints/cli/benchmark/latency.py
8	7	vllm/entrypoints/cli/benchmark/main.py
1	1	vllm/entrypoints/cli/benchmark/serve.py
1	1	vllm/entrypoints/cli/benchmark/throughput.py
6	4	vllm/entrypoints/cli/collect_env.py
8	8	vllm/entrypoints/cli/main.py
60	45	vllm/entrypoints/cli/openai.py
10	9	vllm/entrypoints/cli/run_batch.py
54	48	vllm/entrypoints/cli/serve.py
2	2	vllm/entrypoints/cli/types.py
110	82	vllm/entrypoints/context.py
92	58	vllm/entrypoints/harmony_utils.py
33	22	vllm/entrypoints/launcher.py
337	250	vllm/entrypoints/llm.py
9	7	vllm/entrypoints/logger.py
678	633	vllm/entrypoints/openai/api_server.py
36	27	vllm/entrypoints/openai/cli_args.py
15	12	vllm/entrypoints/openai/logits_processors.py
499	372	vllm/entrypoints/openai/protocol.py
178	134	vllm/entrypoints/openai/run_batch.py
591	422	vllm/entrypoints/openai/serving_chat.py
23	18	vllm/entrypoints/openai/serving_classification.py
111	92	vllm/entrypoints/openai/serving_completion.py
160	125	vllm/entrypoints/openai/serving_embedding.py
190	146	vllm/entrypoints/openai/serving_engine.py
86	69	vllm/entrypoints/openai/serving_models.py
48	38	vllm/entrypoints/openai/serving_pooling.py
447	297	vllm/entrypoints/openai/serving_responses.py
117	85	vllm/entrypoints/openai/serving_score.py
41	36	vllm/entrypoints/openai/serving_tokenization.py
46	32	vllm/entrypoints/openai/serving_transcription.py
101	73	vllm/entrypoints/openai/speech_to_text.py
36	29	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
161	135	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py
161	138	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
66	49	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
101	84	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
101	82	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
189	148	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
174	124	vllm/entrypoints/openai/tool_parsers/hunyuan_a13b_tool_parser.py
93	80	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
135	119	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
166	143	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py
104	77	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
122	97	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
8	8	vllm/entrypoints/openai/tool_parsers/longcat_tool_parser.py
222	187	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py
136	113	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
14	8	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py
40	30	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
92	66	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
261	185	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py
444	330	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py
256	189	vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
84	76	vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py
5	5	vllm/entrypoints/openai/tool_parsers/utils.py
191	157	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py
53	45	vllm/entrypoints/renderer.py
36	27	vllm/entrypoints/score_utils.py
17	14	vllm/entrypoints/ssl.py
11	10	vllm/entrypoints/tool.py
46	43	vllm/entrypoints/tool_server.py
42	36	vllm/entrypoints/utils.py
2	2	vllm/env_override.py
573	696	vllm/envs.py
63	45	vllm/executor/executor_base.py
2	1	vllm/executor/msgspec_utils.py
182	133	vllm/executor/ray_distributed_executor.py
74	48	vllm/executor/ray_utils.py
31	28	vllm/executor/uniproc_executor.py
124	86	vllm/forward_context.py
20	6	vllm/inputs/__init__.py
34	25	vllm/inputs/data.py
25	16	vllm/inputs/parse.py
106	68	vllm/inputs/preprocess.py
32	21	vllm/logger.py
21	18	vllm/logging_utils/dump_input.py
1	3	vllm/logging_utils/formatter.py
5	3	vllm/logging_utils/log_time.py
15	13	vllm/logits_process.py
1	0	vllm/logprobs.py
12	7	vllm/lora/layers/__init__.py
0	1	vllm/lora/layers/base.py
7	3	vllm/lora/layers/base_linear.py
163	120	vllm/lora/layers/column_parallel_linear.py
51	44	vllm/lora/layers/logits_processor.py
1	1	vllm/lora/layers/qkv_x_parallel_linear.py
5	6	vllm/lora/layers/replicated_linear.py
5	2	vllm/lora/layers/row_parallel_linear.py
4	4	vllm/lora/layers/utils.py
36	42	vllm/lora/layers/vocal_parallel_embedding.py
45	33	vllm/lora/lora_weights.py
216	162	vllm/lora/models.py
1	2	vllm/lora/ops/ipex_ops/__init__.py
42	29	vllm/lora/ops/ipex_ops/lora_ops.py
8	4	vllm/lora/ops/torch_ops/__init__.py
77	68	vllm/lora/ops/torch_ops/lora_ops.py
91	41	vllm/lora/ops/triton_ops/kernel_utils.py
53	48	vllm/lora/ops/triton_ops/lora_expand_op.py
45	38	vllm/lora/ops/triton_ops/lora_kernel_metadata.py
41	23	vllm/lora/ops/triton_ops/lora_shrink_op.py
33	24	vllm/lora/ops/triton_ops/utils.py
1	2	vllm/lora/ops/xla_ops/__init__.py
5	8	vllm/lora/ops/xla_ops/lora_ops.py
24	23	vllm/lora/peft_helper.py
166	134	vllm/lora/punica_wrapper/punica_base.py
114	97	vllm/lora/punica_wrapper/punica_cpu.py
119	91	vllm/lora/punica_wrapper/punica_gpu.py
2	1	vllm/lora/punica_wrapper/punica_selector.py
127	107	vllm/lora/punica_wrapper/punica_tpu.py
97	78	vllm/lora/punica_wrapper/punica_xpu.py
35	21	vllm/lora/punica_wrapper/utils.py
12	8	vllm/lora/request.py
9	5	vllm/lora/resolver.py
65	48	vllm/lora/utils.py
42	31	vllm/lora/worker_manager.py
1	2	vllm/model_executor/__init__.py
15	14	vllm/model_executor/custom_op.py
67	73	vllm/model_executor/layers/activation.py
4	3	vllm/model_executor/layers/attention_layer_base.py
67	57	vllm/model_executor/layers/batch_invariant.py
69	53	vllm/model_executor/layers/fla/ops/chunk.py
117	92	vllm/model_executor/layers/fla/ops/chunk_delta_h.py
51	41	vllm/model_executor/layers/fla/ops/chunk_o.py
48	31	vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py
155	100	vllm/model_executor/layers/fla/ops/cumsum.py
46	35	vllm/model_executor/layers/fla/ops/fused_recurrent.py
16	14	vllm/model_executor/layers/fla/ops/index.py
20	16	vllm/model_executor/layers/fla/ops/l2norm.py
100	94	vllm/model_executor/layers/fla/ops/layernorm_guard.py
2	2	vllm/model_executor/layers/fla/ops/op.py
312	205	vllm/model_executor/layers/fla/ops/solve_tril.py
34	27	vllm/model_executor/layers/fla/ops/utils.py
77	32	vllm/model_executor/layers/fla/ops/wy_fast.py
29	15	vllm/model_executor/layers/fused_moe/__init__.py
47	42	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
78	35	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
89	75	vllm/model_executor/layers/fused_moe/config.py
49	44	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
287	183	vllm/model_executor/layers/fused_moe/cutlass_moe.py
93	68	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
116	101	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py
93	47	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
88	51	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
45	33	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
47	44	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
98	86	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
140	116	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
125	96	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
623	433	vllm/model_executor/layers/fused_moe/fused_moe.py
56	44	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
683	484	vllm/model_executor/layers/fused_moe/layer.py
164	103	vllm/model_executor/layers/fused_moe/modular_kernel.py
11	12	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
13	10	vllm/model_executor/layers/fused_moe/moe_pallas.py
87	58	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
1	1	vllm/model_executor/layers/fused_moe/moe_torch_iterative.py
65	47	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
14	9	vllm/model_executor/layers/fused_moe/prepare_finalize.py
174	128	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
35	26	vllm/model_executor/layers/fused_moe/routing_simulator.py
59	33	vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
54	29	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
47	70	vllm/model_executor/layers/fused_moe/trtllm_moe.py
41	39	vllm/model_executor/layers/fused_moe/utils.py
69	42	vllm/model_executor/layers/layernorm.py
170	98	vllm/model_executor/layers/lightning_attn.py
462	385	vllm/model_executor/layers/linear.py
16	14	vllm/model_executor/layers/logits_processor.py
96	92	vllm/model_executor/layers/mamba/linear_attn.py
130	90	vllm/model_executor/layers/mamba/mamba_mixer.py
173	145	vllm/model_executor/layers/mamba/mamba_mixer2.py
23	28	vllm/model_executor/layers/mamba/mamba_utils.py
235	192	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
55	51	vllm/model_executor/layers/mamba/ops/layernorm_gated.py
122	96	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
68	91	vllm/model_executor/layers/mamba/ops/ssd_bmm.py
166	159	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
251	265	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
51	51	vllm/model_executor/layers/mamba/ops/ssd_combined.py
31	29	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
44	33	vllm/model_executor/layers/mamba/short_conv.py
20	16	vllm/model_executor/layers/mla.py
83	77	vllm/model_executor/layers/pooler.py
14	8	vllm/model_executor/layers/quantization/__init__.py
100	73	vllm/model_executor/layers/quantization/auto_round.py
66	45	vllm/model_executor/layers/quantization/awq.py
206	136	vllm/model_executor/layers/quantization/awq_marlin.py
94	77	vllm/model_executor/layers/quantization/awq_triton.py
23	23	vllm/model_executor/layers/quantization/base_config.py
121	82	vllm/model_executor/layers/quantization/bitblas.py
190	158	vllm/model_executor/layers/quantization/bitsandbytes.py
345	232	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
748	555	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
18	11	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
78	53	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
8	7	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
90	74	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
64	45	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
76	56	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
83	64	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py
70	52	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int.py
72	55	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
91	64	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
58	44	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
114	91	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
88	61	vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py
49	28	vllm/model_executor/layers/quantization/compressed_tensors/transform/module.py
42	23	vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4.py
75	56	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
31	27	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
67	44	vllm/model_executor/layers/quantization/deepspeedfp.py
83	65	vllm/model_executor/layers/quantization/experts_int8.py
73	50	vllm/model_executor/layers/quantization/fbgemm_fp8.py
444	308	vllm/model_executor/layers/quantization/fp8.py
177	132	vllm/model_executor/layers/quantization/gguf.py
93	73	vllm/model_executor/layers/quantization/gptq.py
122	87	vllm/model_executor/layers/quantization/gptq_bitblas.py
209	172	vllm/model_executor/layers/quantization/gptq_marlin.py
100	76	vllm/model_executor/layers/quantization/gptq_marlin_24.py
124	86	vllm/model_executor/layers/quantization/hqq_marlin.py
10	6	vllm/model_executor/layers/quantization/inc.py
28	20	vllm/model_executor/layers/quantization/input_quant_fp8.py
162	128	vllm/model_executor/layers/quantization/ipex_quant.py
27	24	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
34	20	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
31	31	vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark.py
75	52	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
27	21	vllm/model_executor/layers/quantization/kernels/mixed_precision/conch.py
34	31	vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py
45	25	vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py
54	40	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py
68	52	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
68	40	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
24	17	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
31	22	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
51	49	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
66	52	vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py
51	47	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
14	12	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
22	20	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
35	32	vllm/model_executor/layers/quantization/kv_cache.py
522	353	vllm/model_executor/layers/quantization/modelopt.py
198	140	vllm/model_executor/layers/quantization/moe_wna16.py
389	259	vllm/model_executor/layers/quantization/mxfp4.py
48	34	vllm/model_executor/layers/quantization/petit.py
49	41	vllm/model_executor/layers/quantization/ptpc_fp8.py
163	118	vllm/model_executor/layers/quantization/quark/quark.py
221	159	vllm/model_executor/layers/quantization/quark/quark_moe.py
8	7	vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py
70	61	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
79	64	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
67	50	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
16	13	vllm/model_executor/layers/quantization/quark/utils.py
158	104	vllm/model_executor/layers/quantization/rtn.py
12	6	vllm/model_executor/layers/quantization/schema.py
41	26	vllm/model_executor/layers/quantization/torchao.py
48	33	vllm/model_executor/layers/quantization/tpu_int8.py
3	3	vllm/model_executor/layers/quantization/utils/__init__.py
41	26	vllm/model_executor/layers/quantization/utils/allspark_utils.py
92	72	vllm/model_executor/layers/quantization/utils/bitblas_utils.py
28	17	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
66	48	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
290	177	vllm/model_executor/layers/quantization/utils/fp8_utils.py
30	26	vllm/model_executor/layers/quantization/utils/gptq_utils.py
45	48	vllm/model_executor/layers/quantization/utils/int8_utils.py
9	7	vllm/model_executor/layers/quantization/utils/layer_utils.py
13	6	vllm/model_executor/layers/quantization/utils/machete_utils.py
230	188	vllm/model_executor/layers/quantization/utils/marlin_utils.py
111	109	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
73	66	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
31	33	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
68	65	vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py
78	54	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
5	4	vllm/model_executor/layers/quantization/utils/mxfp8_utils.py
22	17	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py
12	8	vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py
21	19	vllm/model_executor/layers/quantization/utils/petit_utils.py
115	80	vllm/model_executor/layers/quantization/utils/quant_utils.py
192	143	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
88	71	vllm/model_executor/layers/resampler.py
126	59	vllm/model_executor/layers/rotary_embedding/__init__.py
58	28	vllm/model_executor/layers/rotary_embedding/base.py
33	26	vllm/model_executor/layers/rotary_embedding/common.py
52	33	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
67	48	vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py
6	4	vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope.py
6	5	vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope.py
19	23	vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope.py
3	2	vllm/model_executor/layers/rotary_embedding/linear_scaling_rope.py
7	7	vllm/model_executor/layers/rotary_embedding/llama3_rope.py
16	18	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
436	282	vllm/model_executor/layers/rotary_embedding/mrope.py
24	18	vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope.py
33	20	vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope.py
15	7	vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py
27	19	vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope.py
1	2	vllm/model_executor/layers/shared_fused_moe/__init__.py
5	2	vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py
72	57	vllm/model_executor/layers/utils.py
226	154	vllm/model_executor/layers/vocab_parallel_embedding.py
28	18	vllm/model_executor/model_loader/__init__.py
15	10	vllm/model_executor/model_loader/base_loader.py
233	195	vllm/model_executor/model_loader/bitsandbytes_loader.py
67	50	vllm/model_executor/model_loader/default_loader.py
6	6	vllm/model_executor/model_loader/dummy_loader.py
49	32	vllm/model_executor/model_loader/gguf_loader.py
28	21	vllm/model_executor/model_loader/online_quantization.py
43	36	vllm/model_executor/model_loader/runai_streamer_loader.py
27	20	vllm/model_executor/model_loader/sharded_state_loader.py
209	154	vllm/model_executor/model_loader/tensorizer.py
28	19	vllm/model_executor/model_loader/tensorizer_loader.py
30	25	vllm/model_executor/model_loader/tpu.py
51	45	vllm/model_executor/model_loader/utils.py
141	139	vllm/model_executor/model_loader/weight_utils.py
22	7	vllm/model_executor/models/__init__.py
109	98	vllm/model_executor/models/adapters.py
55	52	vllm/model_executor/models/aimv2.py
136	109	vllm/model_executor/models/apertus.py
130	105	vllm/model_executor/models/arcee.py
225	152	vllm/model_executor/models/arctic.py
94	84	vllm/model_executor/models/aria.py
103	82	vllm/model_executor/models/aya_vision.py
118	107	vllm/model_executor/models/baichuan.py
100	97	vllm/model_executor/models/bailing_moe.py
109	100	vllm/model_executor/models/bamba.py
221	187	vllm/model_executor/models/bert.py
238	194	vllm/model_executor/models/bert_with_rope.py
64	53	vllm/model_executor/models/blip.py
120	116	vllm/model_executor/models/blip2.py
81	69	vllm/model_executor/models/bloom.py
255	263	vllm/model_executor/models/chameleon.py
90	69	vllm/model_executor/models/chatglm.py
132	121	vllm/model_executor/models/clip.py
92	81	vllm/model_executor/models/cohere2_vision.py
109	93	vllm/model_executor/models/commandr.py
64	66	vllm/model_executor/models/config.py
89	74	vllm/model_executor/models/dbrx.py
110	98	vllm/model_executor/models/deepseek.py
50	44	vllm/model_executor/models/deepseek_eagle.py
77	61	vllm/model_executor/models/deepseek_mtp.py
413	339	vllm/model_executor/models/deepseek_v2.py
166	137	vllm/model_executor/models/deepseek_vl2.py
126	104	vllm/model_executor/models/dots1.py
262	211	vllm/model_executor/models/dots_ocr.py
1	1	vllm/model_executor/models/ernie45.py
169	139	vllm/model_executor/models/ernie45_moe.py
402	342	vllm/model_executor/models/ernie45_vl.py
197	154	vllm/model_executor/models/ernie45_vl_moe.py
62	56	vllm/model_executor/models/ernie_mtp.py
70	58	vllm/model_executor/models/exaone.py
76	60	vllm/model_executor/models/exaone4.py
24	24	vllm/model_executor/models/fairseq2_llama.py
149	118	vllm/model_executor/models/falcon.py
95	79	vllm/model_executor/models/falcon_h1.py
49	40	vllm/model_executor/models/fuyu.py
60	56	vllm/model_executor/models/gemma.py
92	85	vllm/model_executor/models/gemma2.py
97	89	vllm/model_executor/models/gemma3.py
138	110	vllm/model_executor/models/gemma3_mm.py
200	165	vllm/model_executor/models/gemma3n.py
153	114	vllm/model_executor/models/gemma3n_mm.py
1	1	vllm/model_executor/models/glm.py
67	63	vllm/model_executor/models/glm4.py
351	288	vllm/model_executor/models/glm4_1v.py
172	141	vllm/model_executor/models/glm4_moe.py
77	62	vllm/model_executor/models/glm4_moe_mtp.py
138	118	vllm/model_executor/models/glm4v.py
82	74	vllm/model_executor/models/gpt2.py
69	61	vllm/model_executor/models/gpt_bigcode.py
56	53	vllm/model_executor/models/gpt_j.py
66	57	vllm/model_executor/models/gpt_neox.py
160	168	vllm/model_executor/models/gpt_oss.py
112	84	vllm/model_executor/models/granite.py
147	124	vllm/model_executor/models/granite_speech.py
164	125	vllm/model_executor/models/granitemoe.py
201	174	vllm/model_executor/models/granitemoehybrid.py
87	66	vllm/model_executor/models/granitemoeshared.py
50	37	vllm/model_executor/models/gritlm.py
154	128	vllm/model_executor/models/grok1.py
53	34	vllm/model_executor/models/h2ovl.py
162	138	vllm/model_executor/models/hunyuan_v1.py
172	145	vllm/model_executor/models/hyperclovax_vision.py
71	61	vllm/model_executor/models/idefics2_vision_model.py
114	106	vllm/model_executor/models/idefics3.py
117	130	vllm/model_executor/models/interfaces.py
31	30	vllm/model_executor/models/interfaces_base.py
133	120	vllm/model_executor/models/intern_vit.py
86	74	vllm/model_executor/models/internlm2.py
23	28	vllm/model_executor/models/internlm2_ve.py
165	142	vllm/model_executor/models/interns1.py
120	104	vllm/model_executor/models/interns1_vit.py
216	167	vllm/model_executor/models/internvl.py
93	74	vllm/model_executor/models/jais.py
192	169	vllm/model_executor/models/jamba.py
41	43	vllm/model_executor/models/jina_vl.py
254	214	vllm/model_executor/models/keye.py
149	114	vllm/model_executor/models/keye_vl1_5.py
133	105	vllm/model_executor/models/kimi_vl.py
74	67	vllm/model_executor/models/lfm2.py
166	135	vllm/model_executor/models/llama.py
207	171	vllm/model_executor/models/llama4.py
45	56	vllm/model_executor/models/llama4_eagle.py
30	38	vllm/model_executor/models/llama_eagle.py
65	62	vllm/model_executor/models/llama_eagle3.py
157	124	vllm/model_executor/models/llava.py
133	91	vllm/model_executor/models/llava_next.py
96	74	vllm/model_executor/models/llava_next_video.py
212	148	vllm/model_executor/models/llava_onevision.py
181	145	vllm/model_executor/models/longcat_flash.py
125	128	vllm/model_executor/models/longcat_flash_mtp.py
101	80	vllm/model_executor/models/mamba.py
93	78	vllm/model_executor/models/mamba2.py
68	53	vllm/model_executor/models/medusa.py
114	95	vllm/model_executor/models/midashenglm.py
30	27	vllm/model_executor/models/mimo.py
67	58	vllm/model_executor/models/mimo_mtp.py
164	125	vllm/model_executor/models/minicpm.py
66	58	vllm/model_executor/models/minicpm3.py
89	77	vllm/model_executor/models/minicpm_eagle.py
165	138	vllm/model_executor/models/minicpmo.py
402	356	vllm/model_executor/models/minicpmv.py
263	224	vllm/model_executor/models/minimax_text_01.py
100	86	vllm/model_executor/models/minimax_vl_01.py
142	108	vllm/model_executor/models/mistral3.py
156	124	vllm/model_executor/models/mixtral.py
165	154	vllm/model_executor/models/mllama4.py
78	58	vllm/model_executor/models/mlp_speculator.py
104	106	vllm/model_executor/models/modernbert.py
14	11	vllm/model_executor/models/module_mapping.py
220	194	vllm/model_executor/models/molmo.py
147	142	vllm/model_executor/models/moonvit.py
51	49	vllm/model_executor/models/mpt.py
224	181	vllm/model_executor/models/nano_nemotron_vl.py
123	96	vllm/model_executor/models/nemotron.py
71	58	vllm/model_executor/models/nemotron_h.py
103	74	vllm/model_executor/models/nemotron_nas.py
104	85	vllm/model_executor/models/nemotron_vl.py
45	36	vllm/model_executor/models/nvlm_d.py
68	55	vllm/model_executor/models/olmo.py
51	39	vllm/model_executor/models/olmo2.py
110	90	vllm/model_executor/models/olmoe.py
87	71	vllm/model_executor/models/opt.py
70	58	vllm/model_executor/models/orion.py
105	86	vllm/model_executor/models/ovis.py
127	107	vllm/model_executor/models/ovis2_5.py
83	66	vllm/model_executor/models/paligemma.py
91	71	vllm/model_executor/models/persimmon.py
87	74	vllm/model_executor/models/phi.py
0	1	vllm/model_executor/models/phi3.py
184	144	vllm/model_executor/models/phi3v.py
325	256	vllm/model_executor/models/phi4_multimodal.py
351	273	vllm/model_executor/models/phi4mm.py
249	207	vllm/model_executor/models/phi4mm_audio.py
284	249	vllm/model_executor/models/phi4mm_utils.py
86	70	vllm/model_executor/models/phimoe.py
219	204	vllm/model_executor/models/pixtral.py
176	168	vllm/model_executor/models/plamo2.py
75	70	vllm/model_executor/models/qwen.py
97	77	vllm/model_executor/models/qwen2.py
283	205	vllm/model_executor/models/qwen2_5_omni_thinker.py
418	308	vllm/model_executor/models/qwen2_5_vl.py
113	92	vllm/model_executor/models/qwen2_audio.py
161	129	vllm/model_executor/models/qwen2_moe.py
29	27	vllm/model_executor/models/qwen2_rm.py
480	379	vllm/model_executor/models/qwen2_vl.py
40	43	vllm/model_executor/models/qwen3.py
193	153	vllm/model_executor/models/qwen3_moe.py
319	238	vllm/model_executor/models/qwen3_next.py
85	60	vllm/model_executor/models/qwen3_next_mtp.py
499	400	vllm/model_executor/models/qwen3_vl.py
136	91	vllm/model_executor/models/qwen3_vl_moe.py
167	132	vllm/model_executor/models/qwen_vl.py
117	110	vllm/model_executor/models/radio.py
139	103	vllm/model_executor/models/registry.py
117	100	vllm/model_executor/models/roberta.py
15	16	vllm/model_executor/models/rvl.py
84	69	vllm/model_executor/models/seed_oss.py
84	76	vllm/model_executor/models/siglip.py
165	142	vllm/model_executor/models/siglip2navit.py
151	113	vllm/model_executor/models/skyworkr1v.py
8	8	vllm/model_executor/models/smolvlm.py
77	66	vllm/model_executor/models/solar.py
103	84	vllm/model_executor/models/stablelm.py
86	66	vllm/model_executor/models/starcoder2.py
209	152	vllm/model_executor/models/step3_text.py
341	273	vllm/model_executor/models/step3_vl.py
193	153	vllm/model_executor/models/swin.py
158	127	vllm/model_executor/models/tarsier.py
25	23	vllm/model_executor/models/telechat2.py
9	10	vllm/model_executor/models/teleflm.py
56	44	vllm/model_executor/models/terratorch.py
175	141	vllm/model_executor/models/transformers.py
52	36	vllm/model_executor/models/transformers_moe.py
56	41	vllm/model_executor/models/transformers_pooling.py
137	113	vllm/model_executor/models/ultravox.py
120	96	vllm/model_executor/models/utils.py
106	86	vllm/model_executor/models/vision.py
187	133	vllm/model_executor/models/voxtral.py
136	133	vllm/model_executor/models/whisper.py
208	175	vllm/model_executor/models/zamba2.py
137	114	vllm/model_executor/parameter.py
7	8	vllm/model_executor/utils.py
83	68	vllm/model_executor/warmup/deep_gemm_warmup.py
15	8	vllm/model_executor/warmup/kernel_warmup.py
11	4	vllm/multimodal/__init__.py
14	12	vllm/multimodal/audio.py
0	1	vllm/multimodal/base.py
52	40	vllm/multimodal/cache.py
61	40	vllm/multimodal/evs.py
25	20	vllm/multimodal/hasher.py
14	13	vllm/multimodal/image.py
106	78	vllm/multimodal/inputs.py
75	49	vllm/multimodal/parse.py
173	156	vllm/multimodal/processing.py
55	30	vllm/multimodal/profiling.py
36	34	vllm/multimodal/registry.py
65	52	vllm/multimodal/utils.py
40	46	vllm/multimodal/video.py
47	38	vllm/outputs.py
53	51	vllm/platforms/__init__.py
100	70	vllm/platforms/cpu.py
137	108	vllm/platforms/cuda.py
47	30	vllm/platforms/interface.py
166	120	vllm/platforms/rocm.py
63	40	vllm/platforms/tpu.py
48	26	vllm/platforms/xpu.py
7	4	vllm/plugins/__init__.py
10	7	vllm/plugins/io_processors/__init__.py
13	10	vllm/plugins/io_processors/interface.py
19	16	vllm/plugins/lora_resolvers/filesystem_resolver.py
40	33	vllm/pooling_params.py
112	101	vllm/profiler/layerwise_profile.py
21	18	vllm/profiler/utils.py
6	2	vllm/ray/lazy_utils.py
20	12	vllm/ray/ray_env.py
18	14	vllm/reasoning/abs_reasoning_parsers.py
27	23	vllm/reasoning/basic_parsers.py
6	3	vllm/reasoning/deepseek_r1_reasoning_parser.py
36	27	vllm/reasoning/glm4_moe_reasoning_parser.py
9	12	vllm/reasoning/gptoss_reasoning_parser.py
61	53	vllm/reasoning/granite_reasoning_parser.py
33	35	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
10	10	vllm/reasoning/mistral_reasoning_parser.py
26	17	vllm/reasoning/olmo3_reasoning_parser.py
7	9	vllm/reasoning/qwen3_reasoning_parser.py
3	3	vllm/reasoning/seedoss_reasoning_parser.py
16	16	vllm/reasoning/step3_reasoning_parser.py
118	84	vllm/sampling_params.py
43	39	vllm/scalar_type.py
4	2	vllm/scripts.py
9	9	vllm/sequence.py
22	22	vllm/tracing.py
6	4	vllm/transformers_utils/__init__.py
6	6	vllm/transformers_utils/chat_templates/registry.py
280	236	vllm/transformers_utils/config.py
8	7	vllm/transformers_utils/config_parser_base.py
6	3	vllm/transformers_utils/configs/__init__.py
31	28	vllm/transformers_utils/configs/chatglm.py
2	3	vllm/transformers_utils/configs/deepseek_v3.py
37	33	vllm/transformers_utils/configs/deepseek_vl2.py
8	6	vllm/transformers_utils/configs/dotsocr.py
28	19	vllm/transformers_utils/configs/eagle.py
2	3	vllm/transformers_utils/configs/falcon.py
22	16	vllm/transformers_utils/configs/jais.py
9	7	vllm/transformers_utils/configs/kimi_vl.py
19	16	vllm/transformers_utils/configs/medusa.py
8	5	vllm/transformers_utils/configs/midashenglm.py
17	25	vllm/transformers_utils/configs/mistral.py
12	10	vllm/transformers_utils/configs/mlp_speculator.py
10	10	vllm/transformers_utils/configs/moonvit.py
22	15	vllm/transformers_utils/configs/nemotron.py
9	6	vllm/transformers_utils/configs/nemotron_h.py
0	1	vllm/transformers_utils/configs/olmo3.py
1	2	vllm/transformers_utils/configs/qwen3_next.py
6	6	vllm/transformers_utils/configs/radio.py
2	4	vllm/transformers_utils/configs/speculators/algos.py
15	12	vllm/transformers_utils/configs/speculators/base.py
58	7	vllm/transformers_utils/configs/step3_vl.py
7	5	vllm/transformers_utils/configs/ultravox.py
17	17	vllm/transformers_utils/detokenizer_utils.py
26	13	vllm/transformers_utils/processor.py
1	2	vllm/transformers_utils/processors/__init__.py
1	2	vllm/transformers_utils/processors/ovis.py
90	82	vllm/transformers_utils/processors/ovis2_5.py
13	12	vllm/transformers_utils/runai_utils.py
20	18	vllm/transformers_utils/s3_utils.py
42	39	vllm/transformers_utils/tokenizer.py
16	13	vllm/transformers_utils/tokenizer_base.py
10	4	vllm/transformers_utils/tokenizers/__init__.py
105	98	vllm/transformers_utils/tokenizers/mistral.py
14	11	vllm/transformers_utils/utils.py
5	2	vllm/triton_utils/__init__.py
17	13	vllm/triton_utils/importing.py
46	30	vllm/usage/usage_lib.py
432	355	vllm/utils/__init__.py
69	46	vllm/utils/deep_gemm.py
71	61	vllm/utils/flashinfer.py
10	6	vllm/utils/gc_utils.py
14	24	vllm/utils/jsontree.py
48	39	vllm/utils/tensor_schema.py
204	132	vllm/v1/attention/backends/cpu_attn.py
139	103	vllm/v1/attention/backends/flash_attn.py
285	224	vllm/v1/attention/backends/flashinfer.py
144	113	vllm/v1/attention/backends/flex_attention.py
145	107	vllm/v1/attention/backends/gdn_attn.py
22	15	vllm/v1/attention/backends/linear_attn.py
19	13	vllm/v1/attention/backends/mamba1_attn.py
110	86	vllm/v1/attention/backends/mamba2_attn.py
23	12	vllm/v1/attention/backends/mamba_attn.py
424	354	vllm/v1/attention/backends/mla/common.py
87	75	vllm/v1/attention/backends/mla/cutlass_mla.py
98	69	vllm/v1/attention/backends/mla/flashattn_mla.py
41	29	vllm/v1/attention/backends/mla/flashinfer_mla.py
78	54	vllm/v1/attention/backends/mla/flashmla.py
144	112	vllm/v1/attention/backends/mla/flashmla_sparse.py
86	67	vllm/v1/attention/backends/mla/indexer.py
128	101	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
80	59	vllm/v1/attention/backends/mla/triton_mla.py
101	69	vllm/v1/attention/backends/pallas.py
137	101	vllm/v1/attention/backends/rocm_aiter_fa.py
70	53	vllm/v1/attention/backends/rocm_attn.py
40	28	vllm/v1/attention/backends/short_conv_attn.py
49	44	vllm/v1/attention/backends/tree_attn.py
58	39	vllm/v1/attention/backends/triton_attn.py
199	171	vllm/v1/attention/backends/utils.py
81	70	vllm/v1/attention/backends/xformers.py
58	48	vllm/v1/core/block_pool.py
47	37	vllm/v1/core/encoder_cache_manager.py
168	121	vllm/v1/core/kv_cache_coordinator.py
48	41	vllm/v1/core/kv_cache_manager.py
260	198	vllm/v1/core/kv_cache_utils.py
8	6	vllm/v1/core/sched/async_scheduler.py
2	3	vllm/v1/core/sched/interface.py
30	32	vllm/v1/core/sched/output.py
10	11	vllm/v1/core/sched/request_queue.py
246	186	vllm/v1/core/sched/scheduler.py
8	7	vllm/v1/core/sched/utils.py
145	114	vllm/v1/core/single_type_kv_cache_manager.py
42	30	vllm/v1/cudagraph_dispatcher.py
32	27	vllm/v1/engine/__init__.py
126	100	vllm/v1/engine/async_llm.py
72	52	vllm/v1/engine/coordinator.py
391	275	vllm/v1/engine/core.py
344	276	vllm/v1/engine/core_client.py
48	52	vllm/v1/engine/detokenizer.py
1	0	vllm/v1/engine/exceptions.py
79	55	vllm/v1/engine/llm_engine.py
33	20	vllm/v1/engine/logprobs.py
147	122	vllm/v1/engine/output_processor.py
20	17	vllm/v1/engine/parallel_sampling.py
97	66	vllm/v1/engine/processor.py
246	192	vllm/v1/engine/utils.py
31	27	vllm/v1/executor/abstract.py
145	107	vllm/v1/executor/multiproc_executor.py
11	7	vllm/v1/executor/ray_distributed_executor.py
2	1	vllm/v1/executor/utils.py
79	55	vllm/v1/kv_cache_interface.py
3	6	vllm/v1/kv_offload/abstract.py
5	4	vllm/v1/kv_offload/backend.py
15	14	vllm/v1/kv_offload/backends/cpu.py
21	17	vllm/v1/kv_offload/cpu.py
4	5	vllm/v1/kv_offload/factory.py
34	26	vllm/v1/kv_offload/lru_manager.py
5	4	vllm/v1/kv_offload/spec.py
43	29	vllm/v1/kv_offload/worker/cpu_gpu.py
17	15	vllm/v1/kv_offload/worker/worker.py
351	197	vllm/v1/metrics/loggers.py
15	14	vllm/v1/metrics/prometheus.py
35	31	vllm/v1/metrics/ray_wrappers.py
34	22	vllm/v1/metrics/reader.py
60	43	vllm/v1/metrics/stats.py
25	26	vllm/v1/outputs.py
26	20	vllm/v1/pool/metadata.py
29	23	vllm/v1/request.py
66	44	vllm/v1/sample/logits_processor/__init__.py
57	58	vllm/v1/sample/logits_processor/builtin.py
4	3	vllm/v1/sample/logits_processor/interface.py
17	11	vllm/v1/sample/logits_processor/state.py
0	1	vllm/v1/sample/metadata.py
1	2	vllm/v1/sample/ops/bad_words.py
1	2	vllm/v1/sample/ops/logprobs.py
12	7	vllm/v1/sample/ops/penalties.py
26	16	vllm/v1/sample/ops/topk_topp_sampler.py
50	43	vllm/v1/sample/rejection_sampler.py
32	32	vllm/v1/sample/sampler.py
23	28	vllm/v1/sample/tpu/metadata.py
10	12	vllm/v1/sample/tpu/sampler.py
74	61	vllm/v1/serial_utils.py
308	267	vllm/v1/spec_decode/eagle.py
14	12	vllm/v1/spec_decode/medusa.py
11	15	vllm/v1/spec_decode/metadata.py
47	33	vllm/v1/spec_decode/metrics.py
58	43	vllm/v1/spec_decode/ngram_proposer.py
7	5	vllm/v1/spec_decode/utils.py
54	34	vllm/v1/structured_output/__init__.py
48	32	vllm/v1/structured_output/backend_guidance.py
43	31	vllm/v1/structured_output/backend_lm_format_enforcer.py
46	42	vllm/v1/structured_output/backend_outlines.py
3	2	vllm/v1/structured_output/backend_types.py
74	42	vllm/v1/structured_output/backend_xgrammar.py
15	12	vllm/v1/structured_output/request.py
83	73	vllm/v1/structured_output/utils.py
74	81	vllm/v1/utils.py
56	40	vllm/v1/worker/block_table.py
7	12	vllm/v1/worker/cpu_model_runner.py
53	41	vllm/v1/worker/cpu_worker.py
216	184	vllm/v1/worker/gpu_input_batch.py
1163	865	vllm/v1/worker/gpu_model_runner.py
139	86	vllm/v1/worker/gpu_ubatch_wrapper.py
221	155	vllm/v1/worker/gpu_worker.py
43	31	vllm/v1/worker/kv_connector_model_runner_mixin.py
58	51	vllm/v1/worker/lora_model_runner_mixin.py
167	164	vllm/v1/worker/tpu_input_batch.py
577	454	vllm/v1/worker/tpu_model_runner.py
43	37	vllm/v1/worker/tpu_worker.py
45	30	vllm/v1/worker/ubatch_splitting.py
6	3	vllm/v1/worker/ubatch_utils.py
37	39	vllm/v1/worker/ubatching.py
23	24	vllm/v1/worker/utils.py
32	19	vllm/v1/worker/worker_base.py
0	2	vllm/v1/worker/xpu_model_runner.py
51	40	vllm/v1/worker/xpu_worker.py
1	3	vllm/version.py

[17edd8a80] Hank_ 2025-10-05 [Platform][Kernel] platform-specific kernel loading (#25823)
2	11	vllm/_custom_ops.py
17	0	vllm/platforms/interface.py
4	0	vllm/platforms/tpu.py
4	0	vllm/platforms/xpu.py

[3303cfb4a] ihb2032 2025-10-05 [Bugfix][Hardware][RISC-V] Limit supported dtypes to float32 to avoid scheduler segfault (#26228)
25	0	vllm/platforms/cpu.py

[b7e8e4e6b] Cyrus Leung 2025-10-05 [Bugfix] Always apply MM processor even when no MM items are passed (#26240)
14	15	tests/conftest.py
50	0	tests/test_inputs.py
12	4	vllm/inputs/preprocess.py
2	2	vllm/model_executor/models/phi3v.py
1	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
20	6	vllm/multimodal/processing.py

[432e1cbc2] Simon Danielsson 2025-10-05 [Bugfix]: Assertion error when using FlashInfer backend (#25933)
2	2	vllm/model_executor/layers/quantization/fp8.py

[201c971e9] Jialin Ouyang 2025-10-05 [Perf][Easy] Early stop in request_block_hasher (#26112)
4	0	vllm/v1/core/kv_cache_utils.py

[e0986ea07] Maximilien de Bayser 2025-10-05 Add documentation for granite 4 tool calling (#26175)
5	1	docs/features/tool_calling.md

[a964e5e6c] Cyrus Leung 2025-10-05 [Bugfix] Allow `--skip-tokenizer-init` with `echo and return_token_ids` (#26238)
1	1	tests/entrypoints/openai/test_token_in_token_out.py
2	1	vllm/entrypoints/openai/serving_completion.py

[78c1d5bfd] 22quinn 2025-10-04 [Easy] Add str repr for IterationStats (#26232)
21	0	tests/v1/metrics/test_stats.py
5	0	vllm/v1/metrics/stats.py

[59a85c366] Cyrus Leung 2025-10-05 [Model] Use `merge_by_field_config` for MM models (H-L) (#26230)
1	1	examples/offline_inference/vision_language_multi_image.py
9	23	vllm/model_executor/models/idefics3.py
7	52	vllm/model_executor/models/keye.py
5	51	vllm/model_executor/models/keye_vl1_5.py
1	32	vllm/model_executor/models/kimi_vl.py
6	2	vllm/utils/tensor_schema.py

[119f00630] Cyrus Leung 2025-10-05 [Renderer] Clean up renderer code (#26216)
1	1	tests/entrypoints/openai/test_token_in_token_out.py
13	13	tests/test_inputs.py
1	2	vllm/entrypoints/openai/serving_completion.py
72	64	vllm/entrypoints/renderer.py
7	54	vllm/inputs/parse.py

[a42d2df75] Isotr0py 2025-10-04 [Frontend] Cache chat template kwargs resolution (#26227)
17	7	vllm/entrypoints/chat_utils.py
3	0	vllm/entrypoints/openai/api_server.py
9	11	vllm/entrypoints/openai/serving_chat.py
16	0	vllm/entrypoints/openai/serving_embedding.py
16	0	vllm/entrypoints/openai/serving_engine.py
10	0	vllm/entrypoints/openai/serving_pooling.py
10	0	vllm/entrypoints/openai/serving_tokenization.py

[5c057e068] Li, Jiang 2025-10-04 [CPU] Refine batch reorder of CPU attention backend (#26096)
42	87	vllm/v1/attention/backends/cpu_attn.py
2	41	vllm/v1/worker/cpu_model_runner.py

[ed3aeb25a] Thomas Parnell 2025-10-04 [V1] [Hybrid] Remove code to override default CUDA graph configuration (#26226)
0	15	vllm/model_executor/models/config.py

[86ee94912] yuafng 2025-10-04 Fix tensor device and dtype placement in Qwen2VL model (#26219)
1	1	vllm/model_executor/models/qwen2_vl.py

[4570535ec] Cyrus Leung 2025-10-04 [Model] CLIP Embedding Support (#26010)
1	0	docs/models/supported_models.md
31	3	examples/offline_inference/vision_language_pooling.py
64	8	examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py
138	0	tests/models/multimodal/pooling/test_clip.py
8	2	tests/models/registry.py
5	1	vllm/attention/layer.py
1	1	vllm/model_executor/models/bert.py
596	61	vllm/model_executor/models/clip.py
1	0	vllm/model_executor/models/registry.py
4	2	vllm/model_executor/models/vision.py
1	0	vllm/transformers_utils/chat_templates/registry.py

[2a6dc67eb] Nicolò Lucchesi 2025-10-04 [Bugfix] Fix `_reqs_to_process` leak on abort (#26012)
66	0	tests/v1/kv_connector/unit/test_nixl_connector.py
16	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[f05fea1f5] Yannick Schnider 2025-10-04 [Core] Enable decode of context length equal to max model length (#26168)
3	2	tests/entrypoints/llm/test_generate.py
27	11	tests/v1/e2e/test_context_length.py
1	1	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/core/sched/utils.py

[d0df145c2] Luca Soldaini 2025-10-04 Add Olmo 3 reasoning parser (#26054)
157	0	tests/reasoning/test_olmo3_reasoning_parser.py
2	0	vllm/reasoning/__init__.py
294	0	vllm/reasoning/olmo3_reasoning_parser.py

[1838cd486] Cyrus Leung 2025-10-04 Revert "Add batch invariant kernel override for FlashInfer backend [2/n]" (#26220)
23	40	tests/v1/generation/test_batch_invariance.py
1	12	vllm/model_executor/layers/batch_invariant.py
5	32	vllm/v1/attention/backends/flashinfer.py

[7d6b03381] Huamin Li 2025-10-04 [CI Failure] fix_test_auto_prefix_cache_support (#26053)
2	2	tests/v1/core/test_scheduler.py
12	5	vllm/config/vllm.py

[7c2e91c4e] Cyrus Leung 2025-10-04 [Misc] Remove unused `executor.apply_model` (#26215)
1	12	vllm/executor/executor_base.py

[736fbf4c8] Cyrus Leung 2025-10-04 [Misc] Require `merge_by_field_config` argument (#26214)
9	33	vllm/multimodal/utils.py
1	12	vllm/v1/worker/gpu_input_batch.py

[44ea85137] Cyrus Leung 2025-10-04 [Model] Support nested structures for TensorSchema (#26212)
39	29	tests/utils_/test_tensor_schema.py
2	2	vllm/model_executor/models/glm4_1v.py
177	210	vllm/model_executor/models/hyperclovax_vision.py
3	3	vllm/model_executor/models/phi3v.py
53	48	vllm/utils/tensor_schema.py

[d3d649efe] Harry Mellor 2025-10-04 Support expert parallel in Transformers backend (#26162)
2	1	docs/models/supported_models.md
30	20	vllm/model_executor/models/transformers_moe.py

[ea507c3a9] Stan Wozniak 2025-10-04 [V1] [Hybrid] Mamba2 Automatic Prefix Caching (#25752)
413	1	tests/models/language/generation/test_hybrid.py
2	1	vllm/config/cache.py
6	1	vllm/engine/arg_utils.py
105	8	vllm/model_executor/layers/mamba/mamba_mixer2.py
174	68	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
17	1	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
25	19	vllm/model_executor/layers/mamba/ops/ssd_combined.py
0	4	vllm/model_executor/models/bamba.py
59	10	vllm/model_executor/models/config.py
0	3	vllm/model_executor/models/falcon_h1.py
0	5	vllm/model_executor/models/granitemoehybrid.py
0	3	vllm/model_executor/models/mamba2.py
0	3	vllm/model_executor/models/nemotron_h.py
0	3	vllm/model_executor/models/zamba2.py
90	1	vllm/v1/attention/backends/mamba2_attn.py
22	4	vllm/v1/core/single_type_kv_cache_manager.py
2	4	vllm/v1/kv_cache_interface.py
2	8	vllm/v1/worker/gpu_model_runner.py

[9705fba7b] Fadi Arafeh 2025-10-04 [cpu][perf] Accelerate unquantized-linear for AArch64 through oneDNN/ACL and weight prepack (#25948)
2	1	cmake/cpu_extension.cmake
69	11	csrc/cpu/dnnl_helper.cpp
1	1	csrc/cpu/dnnl_helper.h
22	1	csrc/cpu/dnnl_kernels.cpp
5	0	csrc/cpu/torch_bindings.cpp
5	0	setup.py
4	0	vllm/_custom_ops.py
3	2	vllm/model_executor/layers/utils.py

[2f7dbc9b4] Bram Wasti 2025-10-03 Add batch invariant kernel override for FlashInfer backend [2/n] (#25769)
40	23	tests/v1/generation/test_batch_invariance.py
12	1	vllm/model_executor/layers/batch_invariant.py
32	5	vllm/v1/attention/backends/flashinfer.py

[ea25a76c0] Ben Browning 2025-10-03 [BugFix] Use async Mistral Tokenizer in Chat Completions (#26134)
69	0	tests/entrypoints/openai/test_serving_engine.py
4	2	vllm/entrypoints/openai/serving_engine.py

[67bc0c003] Roger Wang 2025-10-03 [Bugfix] Fix qwen3 vl dummy data generation with overrides (#26193)
43	20	vllm/model_executor/models/qwen3_vl.py

[5a05f2660] Eugene Khvedchenya 2025-10-04 Fix issue of using only the part of video frame [Nemotron Nano] (#26186)
1	1	vllm/model_executor/models/nano_nemotron_vl.py

[7ef40bb98] Varun Sundar Rabindranath 2025-10-03 [GPTOSS][DP/EP][Marlin] Enable GPTOSS DP/EP using Marlin kernels (#25488)
4	2	docs/design/moe_kernel_features.md
1	1	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
1	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
164	26	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
48	42	vllm/model_executor/layers/fused_moe/modular_kernel.py
34	27	vllm/model_executor/layers/quantization/mxfp4.py
10	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[767cbb011] Wentao Ye 2025-10-03 [CI] Fix Pre-commit Mypy Error (#26181)
3	3	vllm/transformers_utils/config.py

[7cfa4b24b] Angela Yi 2025-10-03 [BugFix] Fix de-functionalization pass for rotary_embedding (#23953)
1	0	.buildkite/test-pipeline.yaml
228	70	tests/compile/test_functionalization.py
37	17	vllm/compilation/fix_functionalization.py

[b71fcd490] Sergei Skvortsov 2025-10-03 [Misc] Add penalties sampling parameters to serve tool (#25974)
24	0	vllm/benchmarks/serve.py

[75003f34e] Sahithi Chigurupati 2025-10-03 [CI] Push multiarch manifests as nightly builds (#25764)
11	5	.buildkite/release-pipeline.yaml
26	3	.buildkite/scripts/cleanup-nightly-builds.sh

[78b8015a4] Bowen Bao 2025-10-03 [Bugfix] Relax tokenizer regex for mixtral to include 'tokenizer.model' (#25964)
11	5	vllm/transformers_utils/tokenizers/mistral.py

[831b12415] Andrew Xia 2025-10-03 [responsesAPI] add better error messaging for long prompts (#25724)
62	1	tests/entrypoints/openai/test_serving_responses.py
22	0	vllm/entrypoints/openai/serving_responses.py

[c1ffcb55d] Wentao Ye 2025-10-03 [Refactor] Optimize FP8 MOE Backend Choice and Log (#26044)
71	46	vllm/model_executor/layers/quantization/fp8.py

[0879736aa] Corey Lowman 2025-10-03 [Perf] Remove hardcoded num_warps=1 (#26183)
2	5	vllm/v1/sample/rejection_sampler.py

[a26917332] Pavani Majety 2025-10-03 [Quantization/NVFP4] Speed up TRTLLM NVFP4 MOE weight loading and fix K/V scale loading for MLA Attn (#25968)
4	4	vllm/model_executor/layers/quantization/kv_cache.py
66	50	vllm/model_executor/layers/quantization/modelopt.py
7	1	vllm/model_executor/model_loader/weight_utils.py

[cd9e5b834] Nikhil G 2025-10-03 Fix V1 engine serialization error with Ray distributed executor (#26148)
6	0	vllm/executor/ray_utils.py

[300a59c4c] Matthew Bonanni 2025-10-03 Avoid division by zero in cache DS MLA kernel (#26174)
2	1	csrc/cache_kernels.cu

[d76541a6c] Harry Mellor 2025-10-03 Stop mergify from keeping stale PRs alive (#26169)
26	4	.github/mergify.yml

[dd96465fd] Chendi.Xue 2025-10-03 [BugFix][QWEN-VL]fix wrong apply_rotary_emb_torch selection introduced by #24642 (#26123)
8	3	vllm/model_executor/layers/rotary_embedding/common.py
2	1	vllm/model_executor/models/qwen2_vl.py

[4f8f47e87] Jun Jiang 2025-10-03 Fix undefined symbol: cutlass_moe_mm_sm100 (#26098)
1	1	CMakeLists.txt
2	2	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
20	0	vllm/utils/__init__.py

[d78fda7cd] Cyrus Leung 2025-10-03 [Renderer] Move Processor out of LLMEngine (#26165)
57	13	vllm/entrypoints/llm.py
10	31	vllm/entrypoints/openai/serving_engine.py
25	1	vllm/inputs/parse.py
15	7	vllm/v1/engine/llm_engine.py

[73a99cc2a] Aleksandr Samarin 2025-10-03 [Model] Fixed stream generator for gpt-oss + spec-decoding (#26027)
3	1	vllm/entrypoints/openai/serving_chat.py

[adae0c1f4] Xiang Si 2025-10-03 [CI/Build] do not enforce precompilation on tpu ci tests (#25992)
1	4	tests/v1/entrypoints/llm/test_struct_output_generate.py

[cbf922199] whx 2025-10-03 [Model] Supplement to PR 24862: Pass param prefix to LLMHead (#25805)
6	2	vllm/model_executor/models/deepseek_mtp.py
6	2	vllm/model_executor/models/glm4_moe_mtp.py
1	0	vllm/model_executor/models/gpt_neox.py
1	0	vllm/model_executor/models/longcat_flash_mtp.py
2	1	vllm/model_executor/models/medusa.py
14	4	vllm/model_executor/models/mlp_speculator.py
2	1	vllm/model_executor/models/qwen3_vl_moe.py
3	2	vllm/model_executor/models/whisper.py

[5f42fc53b] Paul Pak 2025-10-03 [backends][short_conv] CUDA graph piecewise edits (#24215)
1	1	vllm/model_executor/layers/mamba/short_conv.py
20	20	vllm/v1/attention/backends/short_conv_attn.py

[8ee846c27] Yannick Schnider 2025-10-03 [Bugfix] Re-enable prefill of max model length (#24446)
91	0	tests/v1/e2e/test_context_length.py
22	8	vllm/v1/worker/gpu_model_runner.py

[812b7f54a] Yang Liu 2025-10-03 [Renderer] Move Processor out of AsyncLLM  (#24138)
3	0	tests/entrypoints/openai/test_lora_resolvers.py
34	64	tests/entrypoints/openai/test_serving_chat.py
6	1	vllm/engine/protocol.py
16	2	vllm/entrypoints/openai/serving_chat.py
24	20	vllm/entrypoints/openai/serving_completion.py
93	10	vllm/entrypoints/openai/serving_engine.py
41	30	vllm/v1/engine/async_llm.py

[5f2cacdb1] Sage Moore 2025-10-03 Quick fix for IMA with the Prefix Prefill kernel during graph capture (#25983)
8	0	vllm/v1/attention/backends/rocm_attn.py

[aa5053e3f] Egor 2025-10-03 [Doc] Fixed shape description for fused_batched_moe.py (#25668)
1	1	vllm/model_executor/layers/fused_moe/fused_batched_moe.py

[79aa24467] Wenlong Wang 2025-10-03 [Multi Modal] Configurable MM Profiling (#25631)
10	2	docs/contributing/model/multimodal.md
19	3	tests/models/multimodal/processing/test_common.py
5	5	tests/models/multimodal/processing/test_mllama4.py
16	1	tests/models/multimodal/processing/test_tensor_schema.py
3	1	vllm/config/model.py
83	12	vllm/config/multimodal.py
1	1	vllm/engine/arg_utils.py
6	1	vllm/model_executor/models/aria.py
6	1	vllm/model_executor/models/aya_vision.py
6	1	vllm/model_executor/models/blip2.py
6	1	vllm/model_executor/models/chameleon.py
6	1	vllm/model_executor/models/cohere2_vision.py
6	1	vllm/model_executor/models/deepseek_vl2.py
6	1	vllm/model_executor/models/dots_ocr.py
9	2	vllm/model_executor/models/ernie45_vl.py
6	1	vllm/model_executor/models/fuyu.py
6	1	vllm/model_executor/models/gemma3_mm.py
10	2	vllm/model_executor/models/gemma3n_mm.py
34	2	vllm/model_executor/models/glm4_1v.py
6	1	vllm/model_executor/models/glm4v.py
5	0	vllm/model_executor/models/granite_speech.py
8	0	vllm/model_executor/models/hyperclovax_vision.py
6	1	vllm/model_executor/models/idefics3.py
9	2	vllm/model_executor/models/interns1.py
12	3	vllm/model_executor/models/internvl.py
7	0	vllm/model_executor/models/keye.py
6	1	vllm/model_executor/models/kimi_vl.py
6	1	vllm/model_executor/models/llava.py
5	0	vllm/model_executor/models/llava_next_video.py
8	1	vllm/model_executor/models/llava_onevision.py
6	1	vllm/model_executor/models/midashenglm.py
8	2	vllm/model_executor/models/minicpmo.py
9	2	vllm/model_executor/models/minicpmv.py
6	1	vllm/model_executor/models/mistral3.py
6	1	vllm/model_executor/models/mllama4.py
6	1	vllm/model_executor/models/molmo.py
12	3	vllm/model_executor/models/nano_nemotron_vl.py
6	1	vllm/model_executor/models/nvlm_d.py
6	1	vllm/model_executor/models/ovis.py
9	1	vllm/model_executor/models/ovis2_5.py
6	1	vllm/model_executor/models/paligemma.py
6	1	vllm/model_executor/models/phi3v.py
9	2	vllm/model_executor/models/phi4_multimodal.py
9	2	vllm/model_executor/models/phi4mm.py
8	2	vllm/model_executor/models/pixtral.py
12	3	vllm/model_executor/models/qwen2_5_omni_thinker.py
7	1	vllm/model_executor/models/qwen2_audio.py
8	1	vllm/model_executor/models/qwen2_vl.py
9	1	vllm/model_executor/models/qwen3_vl.py
6	1	vllm/model_executor/models/qwen_vl.py
7	1	vllm/model_executor/models/rvl.py
6	1	vllm/model_executor/models/skyworkr1v.py
6	1	vllm/model_executor/models/step3_vl.py
11	0	vllm/model_executor/models/terratorch.py
6	1	vllm/model_executor/models/transformers.py
7	1	vllm/model_executor/models/ultravox.py
9	2	vllm/model_executor/models/voxtral.py
7	1	vllm/model_executor/models/whisper.py
75	5	vllm/multimodal/profiling.py
44	7	vllm/multimodal/registry.py

[2ed3f20db] kyt 2025-10-03 [openai] Fix missing tool usage check (system message) (#24768)
16	0	tests/entrypoints/openai/test_response_api_with_harmony.py
3	1	vllm/entrypoints/openai/serving_chat.py

[48f309029] Nicolò Lucchesi 2025-10-03 [NIXL][Misc] Expose metrics from NIXL for logging to CLI (#25388)
1	1	requirements/kv_connectors.txt
54	11	tests/v1/kv_connector/unit/test_nixl_connector.py
69	13	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
3	3	vllm/v1/metrics/loggers.py

[0e93ac0b3] Thomas Parnell 2025-10-03 [CI] Fix distributed hybrid tests in CI (#26155)
2	2	tests/models/language/generation/test_hybrid.py

[5446ad1d2] Yannick Schnider 2025-10-03 [test utils] correct wrong typing (#26159)
1	1	tests/conftest.py

[f9a8084e4] Cyrus Leung 2025-10-03 [Model] Use `merge_by_field_config` for MM models (InternVL family) (#26153)
1	1	docs/models/supported_models.md
1	1	examples/offline_inference/vision_language.py
1	1	examples/offline_inference/vision_language_multi_image.py
19	53	vllm/model_executor/models/interns1.py
27	54	vllm/model_executor/models/internvl.py
16	21	vllm/model_executor/models/nano_nemotron_vl.py
5	21	vllm/model_executor/models/nemotron_vl.py
1	1	vllm/model_executor/models/nvlm_d.py
13	29	vllm/model_executor/models/skyworkr1v.py

[3e70e3d4d] HUIJONG JEONG 2025-10-03 add(v1): RequestStatesStats to RequestOutput (#24947)
13	0	tests/entrypoints/llm/test_generate.py
2	1	vllm/outputs.py
9	10	vllm/v1/engine/output_processor.py

[eb0fa4386] Jiangyun Zhu 2025-10-03 [Perf] Optimize `reshape_and_cache` CUDA Kernel (#25955)
174	0	benchmarks/kernels/benchmark_reshape_and_cache.py
51	45	csrc/cache_kernels.cu

[0ad9951c4] Cyrus Leung 2025-10-03 [Input] Remove unused `prompt` field (#26097)
2	1	tests/models/multimodal/processing/test_transformers.py
8	3	vllm/engine/protocol.py
0	8	vllm/inputs/data.py
17	20	vllm/inputs/preprocess.py
1	2	vllm/model_executor/models/llava.py
0	1	vllm/model_executor/models/paligemma.py
5	5	vllm/model_executor/models/phi3v.py
8	16	vllm/model_executor/models/qwen2_5_omni_thinker.py
0	1	vllm/model_executor/models/terratorch.py
0	1	vllm/model_executor/models/transformers.py
0	6	vllm/multimodal/inputs.py
7	21	vllm/multimodal/processing.py
9	5	vllm/v1/engine/async_llm.py
8	5	vllm/v1/engine/llm_engine.py
2	6	vllm/v1/engine/processor.py

[8c9117181] Varun Sundar Rabindranath 2025-10-03 [Misc] Remove typing.List (#26150)
3	6	vllm/model_executor/layers/fused_moe/fused_moe.py

[c4b48d3c0] ahao-anyscale 2025-10-02 [BUG] Reorder model config creation (#26124)
4	1	vllm/engine/arg_utils.py

[10d765482] Harry Mellor 2025-10-03 `FusedMoE` support for the Transformers backend (#22650)
3	2	docs/models/supported_models.md
4	0	tests/models/registry.py
9	0	tests/models/test_transformers.py
16	7	tests/models/utils.py
23	16	vllm/config/model.py
31	0	vllm/model_executor/layers/fused_moe/layer.py
6	2	vllm/model_executor/models/registry.py
107	54	vllm/model_executor/models/transformers.py
268	0	vllm/model_executor/models/transformers_moe.py
18	10	vllm/model_executor/models/transformers_pooling.py

[39b643dc1] Cyrus Leung 2025-10-03 [Model] Use `merge_by_field_config` for MM models (G) (#26117)
14	21	vllm/model_executor/models/gemma3_mm.py
31	38	vllm/model_executor/models/gemma3n_mm.py
2	36	vllm/model_executor/models/glm4_1v.py
6	9	vllm/model_executor/models/glm4v.py
3	4	vllm/model_executor/models/granite_speech.py

[711f48564] Zhewen Li 2025-10-02 [Bugfix] Fix import `gemm_afp4wfp4` failure on AMD (#26068)
1	1	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py

[9c5ee91b2] TJian 2025-10-02 [ROCm] [VL] [Bugfix] Fix vit flash attn dispatcher logic for ROCm (#26104)
49	23	vllm/attention/layer.py
19	22	vllm/model_executor/models/dots_ocr.py
23	26	vllm/model_executor/models/ernie45_vl.py
17	14	vllm/model_executor/models/glm4_1v.py
17	17	vllm/model_executor/models/qwen2_5_vl.py
18	22	vllm/model_executor/models/qwen2_vl.py
3	1	vllm/model_executor/models/qwen3_vl.py
8	14	vllm/model_executor/models/siglip2navit.py
0	2	vllm/platforms/rocm.py

[27edd2aeb] Tyler Michael Smith 2025-10-03 [Build/CI] Revert back to Ubuntu 20.04, install python 3.12 with uv (#26103)
21	31	docker/Dockerfile
-	-	docs/assets/contributing/dockerfile-stages-dependency.png

[e5017cd6d] Andrew Xia 2025-10-02 [gpt-oss] disable tool server initialization if no tool in request (#25790)
129	0	tests/entrypoints/openai/test_serving_responses.py
19	12	vllm/entrypoints/openai/serving_responses.py

[6a7796e87] Benjamin Chislett 2025-10-03 [Bug]: Limit num_reqs in dummy_run when max_num_seqs is small (#26144)
2	2	vllm/v1/worker/gpu_model_runner.py

[47b933954] Matthew Bonanni 2025-10-02 [DeepSeek] Improve performance of DS MLA cache kernel (#26132)
62	68	csrc/cache_kernels.cu

[5d5146eee] Michael Goin 2025-10-02 [CI/Build] Conditionally register cutlass_fp4_group_mm to fix building on Hopper (#26138)
6	0	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu
1	1	csrc/torch_bindings.cpp

[2aaa42384] Matthew Bonanni 2025-10-02 [Attention] Move Backend enum into registry (#25893)
1	1	tests/compile/test_full_graph.py
2	2	tests/compile/test_fusion_attn.py
2	1	tests/kernels/attention/test_mha_attn.py
1	1	tests/kernels/utils.py
2	2	tests/v1/attention/test_attention_backends.py
2	2	tests/v1/attention/test_mla_backends.py
2	1	tests/v1/attention/utils.py
2	2	tests/v1/spec_decode/test_eagle.py
2	2	tests/v1/spec_decode/test_mtp.py
2	1	tests/v1/spec_decode/test_tree_attention.py
27	0	vllm/attention/backends/registry.py
2	1	vllm/attention/layer.py
2	1	vllm/attention/selector.py
2	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
3	2	vllm/envs.py
1	1	vllm/model_executor/models/dots_ocr.py
2	1	vllm/model_executor/models/ernie45_vl.py
1	1	vllm/model_executor/models/glm4_1v.py
1	1	vllm/model_executor/models/keye.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
1	1	vllm/model_executor/models/qwen2_vl.py
1	1	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/siglip2navit.py
2	1	vllm/model_executor/models/vision.py
0	1	vllm/platforms/__init__.py
5	2	vllm/platforms/cpu.py
7	2	vllm/platforms/cuda.py
5	26	vllm/platforms/interface.py
7	2	vllm/platforms/rocm.py
5	2	vllm/platforms/tpu.py
5	2	vllm/platforms/xpu.py

[ad2d78801] Ekagra Ranjan 2025-10-02 [Bug][Benchmark] Fix duplicate req in oversampling (#26140)
11	5	vllm/benchmarks/datasets.py

[36ce76c63] Wentao Ye 2025-10-02 [Log] Optimize DeepGEMM Missing Log (#26106)
2	2	vllm/utils/deep_gemm.py

[f1fc2107a] Michael Goin 2025-10-02 [Bugfix] Disable cascade attention with FlashInfer (#26130)
3	2	vllm/v1/attention/backends/flashinfer.py

[13cdc0217] Matthew Bonanni 2025-10-02 Fix MTP with deepep_low_latency (#25904)
16	2	vllm/model_executor/layers/fused_moe/layer.py

[502640c3f] ElizaWszola 2025-10-02 [Perf] Fix and reapply move apply w8a8 block fp8 linear to class (#25696)
2	2	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
2	2	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
3	2	tests/kernels/quantization/test_block_fp8.py
21	9	tests/kernels/quantization/test_fp8_quant_group.py
0	30	tests/model_executor/test_enabled_custom_ops.py
35	0	tests/quantization/test_compressed_tensors.py
17	0	vllm/config/vllm.py
8	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
26	11	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
28	12	vllm/model_executor/layers/quantization/fp8.py
16	8	vllm/model_executor/layers/quantization/input_quant_fp8.py
240	117	vllm/model_executor/layers/quantization/utils/fp8_utils.py
14	7	vllm/utils/deep_gemm.py

[3d5f1c864] Chen Zhang 2025-10-02 [Mamba][KVCacheManager] Simplify kv cache manage logic for mamba + MTP (#25119)
4	25	vllm/v1/core/single_type_kv_cache_manager.py

[1cab2f9ca] Ekagra Ranjan 2025-10-02 EAGLE 3: Fix preamble so that measured speedup over Eagle 1 becomes 32% instead of 5% on MTBench (#25916)
39	33	vllm/benchmarks/datasets.py

[1e50f1be7] Chen Zhang 2025-10-02 [Deepseek v3.2] Support indexer prefill chunking (#25999)
22	0	tests/v1/attention/test_sparse_mla_backends.py
37	38	vllm/model_executor/models/deepseek_v2.py
90	41	vllm/v1/attention/backends/mla/indexer.py

[ad87ba927] Chenheli Hua 2025-10-02 [Small] Prevent bypassing media domain restriction via HTTP redirects (#26035)
3	0	docs/features/multimodal_inputs.md
3	0	docs/usage/security.md
18	6	vllm/connections.py
6	0	vllm/envs.py
10	2	vllm/multimodal/utils.py

[decf7f794] Lucas Wilkinson 2025-10-02 [BugFix] Fix FI accuracy issue when used for MLA prefill (#26063)
9	2	vllm/v1/attention/backends/mla/common.py

[d00d65299] Cyrus Leung 2025-10-03 [CI/Build] Replace `vllm.entrypoints.openai.api_server` entrypoint with `vllm serve` command (#25967)
2	6	.buildkite/nightly-benchmarks/scripts/launch-server.sh
1	2	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
1	1	.buildkite/scripts/run-benchmarks.sh
2	6	benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh
4	12	benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
1	1	docker/Dockerfile
1	1	docker/Dockerfile.cpu
1	1	docker/Dockerfile.ppc64le
1	1	docker/Dockerfile.s390x
1	1	docker/Dockerfile.xpu
2	4	docs/contributing/benchmarks.md
1	2	docs/contributing/profiling.md
1	2	docs/deployment/frameworks/autogen.md
1	1	docs/deployment/frameworks/open-webui.md
6	6	docs/deployment/frameworks/skypilot.md
5	0	docs/design/arch_overview.md
1	2	docs/features/sleep_mode.md
1	2	docs/features/spec_decode.md
1	2	docs/getting_started/installation/gpu/xpu.inc.md
1	1	examples/online_serving/sagemaker-entrypoint.sh
37	7	tests/utils_/test_utils.py
29	5	vllm/utils/__init__.py

[3b279a84b] Michael Goin 2025-10-02 [CI] Add Blackwell DeepSeek FP8 FlashInfer MoE tests (#26040)
29	11	tests/quantization/test_blackwell_moe.py

[5e4a8223c] vllmellm 2025-10-02 [Qwen][ROCm] Flash Attention Rotary Embeddings (#24642)
23	0	vllm/model_executor/layers/rotary_embedding/common.py
5	5	vllm/model_executor/models/qwen2_vl.py

[e51de388a] leo-pony 2025-10-02 [Platform][CI] Added OOT platform interface e2e test that running on Ascend NPU (#25470)
191	0	.buildkite/scripts/hardware_ci/run-npu-test.sh

[cc253b73d] Cyrus Leung 2025-10-02 [Model] Use `merge_by_field_config` for MM models (D-F) (#26076)
25	33	vllm/model_executor/models/deepseek_vl2.py
24	51	vllm/model_executor/models/dots_ocr.py
24	52	vllm/model_executor/models/ernie45_vl.py
26	41	vllm/model_executor/models/fuyu.py

[7d6fb905d] Cyrus Leung 2025-10-02 [Model] Use `merge_by_field_config` for MM models (A-C) (#26073)
13	6	vllm/model_executor/models/aria.py
4	3	vllm/model_executor/models/aya_vision.py
4	9	vllm/model_executor/models/blip2.py
4	3	vllm/model_executor/models/chameleon.py
4	3	vllm/model_executor/models/cohere2_vision.py

[418d111f8] Lucas Wilkinson 2025-10-02 [FA/Chore] Bump vllm-flash-attention (#25537)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[be8921fbb] Thomas Parnell 2025-10-02 Change size of single CUDA graph for CI to 4 (#26089)
1	1	tests/conftest.py

[d4e7a1152] Huy Do 2025-10-02 Update base image to 22.04 (jammy) (#26065)
2	2	docker/Dockerfile
1	1	docker/Dockerfile.nightly_torch
-	-	docs/assets/contributing/dockerfile-stages-dependency.png

[be22bb6f3] pwschuurman 2025-10-01 Run:ai model streamer add GCS package support (#24909)
7	0	docs/models/extensions/runai_model_streamer.md
1	1	requirements/nightly_torch_test.txt
2	2	requirements/rocm.txt
1	1	requirements/test.in
20	1	requirements/test.txt
23	4	tests/model_executor/model_loader/runai_model_streamer/test_runai_utils.py

[169313b9f] Nick Hill 2025-10-01 [Misc] Make handling of SamplingParams clearer in n>1 case (#26032)
11	5	vllm/v1/engine/async_llm.py

[0b018d8ba] Gregory Shtrasberg 2025-10-01 [ROCm][Bugfix] Add missing parameter to ROCm backend (#26029)
1	0	vllm/v1/attention/backends/rocm_attn.py

[c31246800] Jerry Zhang 2025-10-01 Support RL online quantization with torchao (#23014)
121	5	tests/quantization/test_torchao.py
64	8	vllm/model_executor/layers/quantization/torchao.py
29	2	vllm/model_executor/model_loader/default_loader.py
217	0	vllm/model_executor/model_loader/online_quantization.py
8	1	vllm/model_executor/model_loader/utils.py
26	0	vllm/model_executor/model_loader/weight_utils.py

[4134312b3] Lucas Wilkinson 2025-10-01 [BugFix] ChunkedLocalAttention is currently not CG compatible (#26034)
5	3	vllm/attention/layers/chunked_local_attention.py

[da554f932] Wentao Ye 2025-10-01 [Bug] Fix Negative Cuda Memory Usage (#25683)
4	2	vllm/v1/worker/gpu_model_runner.py

[aac622e0c] Hosang 2025-10-01 [ROCm][Build] Add support for AMD Ryzen AI MAX / AI 300 Series (#25908)
1	1	CMakeLists.txt
2	1	csrc/rocm/attention.cu
2	2	docker/Dockerfile.rocm_base

[1726e93ef] Lucas Wilkinson 2025-10-01 [BugFix][DP/EP] Fix CUTLASS MLA hang under load (#26026)
32	32	csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_fmha_mla_tma_warpspecialized.hpp

[ee04c0cd0] Michael Goin 2025-10-01 [CI] Tweaks to GPT-OSS Eval (Blackwell) for stability (#26030)
1	1	.buildkite/test-pipeline.yaml
2	3	tests/evals/gpt_oss/test_gpqa_correctness.py

[c36f0aa30] Huamin Li 2025-10-01 Fix test_mamba_ssm_ssd.py due to missing _query_start_loc_to_chunk_indices_offsets (#25995)
48	63	tests/kernels/mamba/test_mamba_ssm_ssd.py
70	0	vllm/v1/attention/backends/mamba2_attn.py

[5234dc745] Johnny 2025-10-01 [NVIDIA] Blackwell Family (#24673)
53	10	CMakeLists.txt
4	4	cmake/utils.cmake
5	5	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh
1	1	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh
3	2	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[3b7c20a6b] Kenichi Maehashi 2025-10-01 [Bugfix] Apply same sampling parameters for both `n=1` and `n>1` (#26005)
1	1	vllm/v1/engine/async_llm.py

[f9e714813] Nathan Scott 2025-10-01 [Benchmark] Finish documented v0.11.0 deprecation of --endpoint-type (#26007)
0	24	vllm/benchmarks/serve.py

[2518230d3] billishyahao 2025-10-01 [MISC] Fix misleading batch_size_capture_list when cuda_graph_sizes < 4 (#25829)
6	3	vllm/config/vllm.py

[a332b8457] Harry Mellor 2025-10-01 [CI] Only capture a single CUDA graph size in CI by default (#25951)
3	0	tests/conftest.py

[1405f0c7b] Cyrus Leung 2025-10-01 [Misc] Factor out common `_apply_feature_select_strategy` (#26003)
3	16	vllm/model_executor/models/llava.py
3	2	vllm/model_executor/models/llava_next.py
6	17	vllm/model_executor/models/tarsier.py
28	4	vllm/model_executor/models/vision.py

[84d57342b] Wenlong Wang 2025-10-01 [BugFix][MM] Fix Nonetype error when video is cache in qwen2.5-omni-thinker (#26004)
9	3	vllm/model_executor/models/qwen2_5_omni_thinker.py

[57b46d769] nadathurv 2025-10-01 [Doc] updating torch.compile doc link (#25989)
1	1	docs/design/torch_compile.md

[f48b6a03b] Lucia Fang 2025-09-30 [Misc]allow disable pynccl (#25421)
4	0	vllm/distributed/device_communicators/cuda_communicator.py
2	1	vllm/distributed/device_communicators/pynccl.py
6	0	vllm/envs.py

[2a69ab489] Harry Mellor 2025-10-01 Update to Transformers `v4.56.2` (#24638)
2	2	requirements/nightly_torch_test.txt
2	2	requirements/test.in
2	2	requirements/test.txt
3	1	tests/models/multimodal/generation/test_common.py
11	27	vllm/model_executor/models/transformers.py

[8d7da92fd] Lucas Wilkinson 2025-10-01 [BugFix] Fix default kv-cache-dtype default for DeepseekV3.2 (#25988)
14	14	vllm/model_executor/models/config.py

[e952eee69] Zhewen Li 2025-09-30 [Bugfix] Fix `__syncwarp` on ROCM (#25996)
5	1	csrc/cache_kernels.cu

[66bca9b8b] Roger Wang 2025-09-30 [MM] Add text-only mode for Qwen3-VL (#26000)
25	14	vllm/model_executor/models/qwen3_vl.py
20	12	vllm/model_executor/models/qwen3_vl_moe.py

[99028fda4] Param 2025-09-30 Fix INT8 quantization error on Blackwell GPUs (SM100+) (#25935)
4	1	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_helper.hpp
5	1	docs/features/quantization/int8.md

[124494888] Wentao Ye 2025-09-30 [Log] Optimize Log for FP8MOE (#25709)
4	4	vllm/model_executor/layers/quantization/fp8.py

[a73f6491c] Salvatore Cena 2025-10-01 Update launch_bounds_utils.h for correct compile on Multiple Cuda Arch - PTXAS out of range Warning (#25843)
29	3	csrc/launch_bounds_utils.h

[001e50c92] Lucia Fang 2025-09-30 [Model] MTP fallback to eager for DeepSeek v32 (#25982)
10	1	tests/v1/spec_decode/test_eagle.py
7	1	tests/v1/spec_decode/test_mtp.py
7	1	vllm/config/speculative.py
1	1	vllm/v1/attention/backends/mla/indexer.py
7	1	vllm/v1/spec_decode/eagle.py

[96ebcaa3a] Lucas Wilkinson 2025-09-30 [Misc] Make EP kernels install script support uv (#25785)
7	5	tools/ep_kernels/install_python_libraries.sh

[5db1870bb] Andrew Xia 2025-09-30 [gpt-oss] use vLLM instead of openai types for streaming (#25186)
8	0	tests/entrypoints/openai/test_response_api_with_harmony.py
38	12	vllm/entrypoints/openai/protocol.py
13	10	vllm/entrypoints/openai/serving_responses.py

[2ce26b9b5] Harry Mellor 2025-09-30 [Docs] Remove API Reference from search index (#25949)
1	1	docs/api/vllm/.meta.yml

[a388252ac] Harry Mellor 2025-09-30 Add explicit pooling classes for the Transformers backend (#25322)
2	1	tests/models/registry.py
41	58	tests/models/test_transformers.py
27	6	vllm/config/model.py
13	0	vllm/config/utils.py
3	2	vllm/model_executor/models/registry.py
10	71	vllm/model_executor/models/transformers.py
200	0	vllm/model_executor/models/transformers_pooling.py

[9a9f48dff] David Ben-David 2025-10-01 [V1] [P/D] Add Support for KV Load Failure Recovery (#19330)
30	0	examples/offline_inference/kv_load_failure_recovery/README.md
85	0	examples/offline_inference/kv_load_failure_recovery/decode_example.py
58	0	examples/offline_inference/kv_load_failure_recovery/prefill_example.py
145	0	examples/offline_inference/kv_load_failure_recovery/rogue_shared_storage_connector.py
33	0	examples/offline_inference/kv_load_failure_recovery/run.sh
341	0	tests/v1/kv_connector/unit/test_kv_load_failure_recovery.py
2	2	tests/v1/kv_connector/unit/test_offloading_connector.py
51	23	tests/v1/kv_connector/unit/test_output_aggreagator.py
3	3	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
7	7	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
31	18	tests/v1/kv_connector/unit/utils.py
1	0	tests/v1/worker/test_gpu_model_runner.py
5	1	vllm/distributed/kv_transfer/kv_connector/utils.py
29	2	vllm/distributed/kv_transfer/kv_connector/v1/base.py
6	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
2	5	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
1	1	vllm/v1/core/block_pool.py
2	0	vllm/v1/core/sched/output.py
174	14	vllm/v1/core/sched/scheduler.py
3	0	vllm/v1/core/single_type_kv_cache_manager.py
5	2	vllm/v1/outputs.py
17	0	vllm/v1/worker/gpu_model_runner.py
1	2	vllm/v1/worker/gpu_worker.py
3	2	vllm/v1/worker/kv_connector_model_runner_mixin.py

[67f3fb084] Jee Jee Li 2025-10-01 [Bench] Add DeepSeekV32 to MoE benchmark (#25962)
2	1	benchmarks/kernels/benchmark_moe.py

[43b752c32] cjackal 2025-10-01 [Llama4] [multimodal] Fix misplaced dtype cast of `cos_sin_cache` in `Llama4VisionRotaryEmbedding` (#25889)
3	1	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py

[cfd302db9] Or Ozeri 2025-09-30 OffloadingConnector: Fix GPU block tracking bug (#25856)
3	2	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py

[fb610ae68] bnellnm 2025-09-30 [Docs] Add moe kernel features doc  (#25297)
2	24	docs/design/fused_moe_modular_kernel.md
119	0	docs/design/moe_kernel_features.md

[2f652e6cd] Cyrus Leung 2025-10-01 [Doc] Improve MM Pooling model documentation (#25966)
1	1	docs/features/multimodal_inputs.md
25	3	docs/models/supported_models.md
41	17	docs/serving/openai_compatible_server.md
77	8	examples/offline_inference/vision_language_pooling.py
128	67	examples/online_serving/pooling/openai_chat_embedding_client_for_multimodal.py
0	0	examples/{template_vlm2vec.jinja => template_vlm2vec_phi3v.jinja}
15	0	examples/template_vlm2vec_qwen2vl.jinja
1	1	tests/entrypoints/pooling/openai/test_vision_embedding.py
2	1	tests/entrypoints/test_chat_utils.py

[e6a226efb] Wentao Ye 2025-09-30 [Bug] Fix AttributeError: 'QKVParallelLinear' object has no attribute 'orig_dtype' (#25958)
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py

[a2e6fa7e0] youkaichao 2025-10-01 [bugfix][deepseek] fix flashmla kernel selection (#25956)
1	1	vllm/attention/ops/flashmla.py

[9f1c4ecaf] Cyrus Leung 2025-10-01 [Bugfix] Token type and position embeddings fail to be applied to `inputs_embeds` (#25922)
10	7	vllm/model_executor/models/bert.py
4	2	vllm/model_executor/models/roberta.py

[ef283548f] Pavani Majety 2025-09-30 [Bugfix] Fix accuracy issue of TRTLLM FP8 MOE and improve logging (#25895)
23	16	vllm/model_executor/layers/quantization/fp8.py
6	1	vllm/utils/deep_gemm.py

[f4db5e6de] Anion 2025-09-30 [Bugfix][Model] Fix inference for Hunyuan dense models (#25354)
59	47	vllm/model_executor/models/hunyuan_v1.py

[099aaee53] Sergio Paniego Blanco 2025-09-30 Add Hugging Face Inference Endpoints guide to Deployment docs (#25886)
-	-	docs/assets/deployment/hf-inference-endpoints-catalog.png
-	-	docs/assets/deployment/hf-inference-endpoints-choose-infra.png
-	-	docs/assets/deployment/hf-inference-endpoints-click-deploy-button.png
-	-	docs/assets/deployment/hf-inference-endpoints-configure-container.png
-	-	docs/assets/deployment/hf-inference-endpoints-create-endpoint.png
-	-	docs/assets/deployment/hf-inference-endpoints-locate-deploy-button.png
-	-	docs/assets/deployment/hf-inference-endpoints-new-endpoint.png
-	-	docs/assets/deployment/hf-inference-endpoints-select-hardware.png
-	-	docs/assets/deployment/hf-inference-endpoints-select-model.png
170	0	docs/deployment/frameworks/hf_inference_endpoints.md

[35fe398c7] Asaf Joseph Gardin 2025-09-30 [Kernel][Moe Configs] Add more tuned triton configs for ExpertsInt8 and FP8 (#25858)
146	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=1792,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
63	135	vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
46	46	vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H200,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
52	52	vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H200,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
41	41	vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json

[bb6d43047] ihb2032 2025-09-30 [Fix] Improve CPU backend compatibility for RISC-V (#25816)
5	4	vllm/engine/arg_utils.py

[bc546f76a] Reza Barazesh 2025-09-30 [CI] Move applicable tests to CPU (#24080)
49	16	.buildkite/test-pipeline.yaml
0	1	.github/mergify.yml
9	8	docker/Dockerfile.cpu
1	0	pyproject.toml
3	0	tests/models/test_utils.py
2	0	tests/models/test_vision.py
2	0	tests/multimodal/test_cache.py
2	0	tests/multimodal/test_hasher.py
2	0	tests/multimodal/test_image.py
3	0	tests/multimodal/test_inputs.py
2	0	tests/multimodal/test_processing.py
2	0	tests/multimodal/test_registry.py
2	0	tests/multimodal/test_video.py
2	0	tests/test_inputs.py
4	0	tests/test_outputs.py
0	0	tests/{mistral_tool_use => tool_use/mistral}/__init__.py
2	2	tests/{mistral_tool_use => tool_use/mistral}/conftest.py
0	0	tests/{mistral_tool_use => tool_use/mistral}/test_mistral_tool_calls.py
0	0	tests/{mistral_tool_use => tool_use/mistral}/utils.py
2	0	tests/tool_use/test_glm4_moe_tool_parser.py
2	0	tests/tool_use/test_jamba_tool_parser.py
2	0	tests/tool_use/test_kimi_k2_tool_parser.py
2	0	tests/tool_use/test_minimax_tool_parser.py
2	0	tests/tool_use/test_qwen3coder_tool_parser.py
2	0	tests/tool_use/test_seed_oss_tool_parser.py
2	0	tests/tool_use/test_tool_choice_required.py
2	0	tests/tool_use/test_xlam_tool_parser.py
2	0	tests/v1/core/test_async_scheduler.py
3	0	tests/v1/core/test_encoder_cache_manager.py
2	0	tests/v1/core/test_kv_cache_utils.py
3	1	tests/v1/core/test_prefix_caching.py
2	0	tests/v1/core/test_scheduler.py
3	0	tests/v1/core/test_single_type_kv_cache_manager.py
4	0	tests/v1/kv_connector/unit/test_output_aggreagator.py
4	0	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
4	0	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
2	0	tests/v1/metrics/test_metrics_reader.py
2	0	tests/v1/structured_output/test_utils.py
2	0	tests/v1/test_serial_utils.py

[80608ba5a] Nicolò Lucchesi 2025-09-30 [NIXL] Add support for MLA caches with different latent dim (#25902)
7	6	tests/v1/kv_connector/unit/test_nixl_connector.py
59	36	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[e184c9c51] Lehua Ding 2025-09-30 [perf] Use CPU tensor to reduce GPU->CPU sync (#25884)
1	1	vllm/v1/worker/gpu_model_runner.py

[d7e34b421] Cyrus Leung 2025-09-30 [Model] Move `vision_feature_select_strategy` into `resolve_visual_encoder_outputs` (#25938)
6	5	tests/models/test_vision.py
4	23	vllm/model_executor/models/aya_vision.py
21	10	vllm/model_executor/models/clip.py
4	21	vllm/model_executor/models/llava.py
6	19	vllm/model_executor/models/llava_next.py
3	14	vllm/model_executor/models/llava_next_video.py
6	19	vllm/model_executor/models/llava_onevision.py
4	20	vllm/model_executor/models/minimax_vl_01.py
15	7	vllm/model_executor/models/pixtral.py
22	12	vllm/model_executor/models/siglip.py
4	19	vllm/model_executor/models/tarsier.py
60	10	vllm/model_executor/models/vision.py

[ef6e0e713] CSWYF3634076 2025-09-30 [Bugfix][Model]fix ernie45 moe gate&bias dtype to float32 (#25936)
3	2	vllm/model_executor/models/ernie45_moe.py
10	5	vllm/model_executor/models/ernie45_vl_moe.py

[1ad3aca68] Sergio Paniego Blanco 2025-09-30 Updated TRL integration docs (#25684)
47	5	docs/training/trl.md
0	1	mkdocs.yaml

[8d0afa9b4] a120092009 2025-09-30 [Doc] Add Cambricon MLU support (#25942)
1	0	docs/getting_started/installation/README.md

[fa7e254a7] Yongye Zhu 2025-09-30 [New Model] DeepSeek-V3.2 (Rebased to Main) (#25896)
76	11	cmake/external_projects/flashmla.cmake
8	0	csrc/cache.h
252	6	csrc/cache_kernels.cu
11	0	csrc/quantization/fp8/nvidia/quant_utils.cuh
7	0	csrc/torch_bindings.cpp
4	0	setup.py
0	1	tests/compile/test_fusion_attn.py
113	0	tests/kernels/attention/test_cache.py
279	0	tests/kernels/attention/test_deepgemm_attention.py
10	12	tests/kernels/attention/test_flashmla.py
119	0	tests/kernels/attention/test_flashmla_sparse.py
245	0	tests/kernels/attention/test_pack_unpack_triton.py
1	0	tests/models/registry.py
6	3	tests/models/test_initialization.py
1	1	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
58	11	tests/v1/attention/test_mla_backends.py
426	0	tests/v1/attention/test_sparse_mla_backends.py
0	1	tests/v1/attention/utils.py
47	9	tests/v1/core/test_kv_cache_utils.py
2	5	tests/v1/core/test_prefix_caching.py
0	6	tests/v1/core/test_single_type_kv_cache_manager.py
1	2	tests/v1/engine/test_engine_core_client.py
0	1	tests/v1/worker/test_gpu_model_runner.py
9	0	vllm/_custom_ops.py
1	0	vllm/attention/backends/abstract.py
4	1	vllm/attention/layer.py
205	0	vllm/attention/ops/common.py
117	48	vllm/attention/ops/flashmla.py
1	0	vllm/attention/ops/paged_attn.py
4	1	vllm/attention/selector.py
13	7	vllm/config/cache.py
1	0	vllm/config/compilation.py
2	2	vllm/config/model.py
1	1	vllm/config/speculative.py
18	0	vllm/model_executor/layers/layernorm.py
16	0	vllm/model_executor/layers/mla.py
27	2	vllm/model_executor/models/config.py
13	1	vllm/model_executor/models/deepseek_mtp.py
445	4	vllm/model_executor/models/deepseek_v2.py
3	0	vllm/model_executor/models/longcat_flash.py
1	0	vllm/model_executor/models/registry.py
4	1	vllm/platforms/cpu.py
14	1	vllm/platforms/cuda.py
1	1	vllm/platforms/interface.py
4	1	vllm/platforms/rocm.py
4	1	vllm/platforms/tpu.py
4	1	vllm/platforms/xpu.py
2	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
101	0	vllm/transformers_utils/configs/deepseek_v3.py
7	0	vllm/utils/__init__.py
120	4	vllm/utils/deep_gemm.py
1	0	vllm/v1/attention/backends/cpu_attn.py
1	0	vllm/v1/attention/backends/flash_attn.py
1	0	vllm/v1/attention/backends/flashinfer.py
1	0	vllm/v1/attention/backends/flex_attention.py
139	31	vllm/v1/attention/backends/mla/common.py
1	0	vllm/v1/attention/backends/mla/flashmla.py
544	0	vllm/v1/attention/backends/mla/flashmla_sparse.py
293	0	vllm/v1/attention/backends/mla/indexer.py
1	0	vllm/v1/attention/backends/pallas.py
1	0	vllm/v1/attention/backends/rocm_aiter_fa.py
1	0	vllm/v1/attention/backends/tree_attn.py
1	0	vllm/v1/attention/backends/triton_attn.py
1	0	vllm/v1/attention/backends/xformers.py
5	4	vllm/v1/core/kv_cache_utils.py
2	1	vllm/v1/core/single_type_kv_cache_manager.py
41	10	vllm/v1/kv_cache_interface.py
45	11	vllm/v1/spec_decode/eagle.py
29	16	vllm/v1/worker/gpu_model_runner.py
0	2	vllm/v1/worker/tpu_model_runner.py

[e23cacda3] Simon Danielsson 2025-09-30 [Bugfix]: Clean up chunked prefill logging when using whisper (#25075)
51	1	tests/v1/core/test_scheduler.py
19	2	vllm/config/scheduler.py
4	5	vllm/config/vllm.py
1	0	vllm/engine/arg_utils.py

[2e1b8bc2b] Zhou Jiahao 2025-09-30 [Model][Bugfix] Fix MiDashengLM audio encoder mask by removing incorrect `logical_not` (#25925)
1	2	vllm/model_executor/models/midashenglm.py

[e47433b3c] acisseJZhong 2025-09-29 [BugFix] Pass config_format via try_get_generation_config (#25912)
2	0	vllm/config/model.py
2	0	vllm/transformers_utils/config.py

[23194d83e] Lucas Wilkinson 2025-09-30 [BugFix] Fix DP/EP hang  (#25906)
15	3	vllm/v1/worker/gpu_model_runner.py

[61aedb5ff] Harry Mellor 2025-09-30 Move`VllmConfig` from `config/__init__.py` to `config/vllm.py` (#25271)
1	2	vllm/attention/layer.py
2	1	vllm/attention/layers/chunked_local_attention.py
79	826	vllm/config/__init__.py
36	6	vllm/config/utils.py
789	0	vllm/config/vllm.py
1	2	vllm/model_executor/layers/mamba/linear_attn.py
2	3	vllm/model_executor/layers/quantization/auto_round.py
2	3	vllm/model_executor/layers/quantization/bitblas.py
2	3	vllm/model_executor/layers/quantization/bitsandbytes.py
2	3	vllm/model_executor/layers/quantization/deepspeedfp.py
6	2	vllm/model_executor/layers/quantization/gptq.py
2	3	vllm/model_executor/layers/quantization/gptq_bitblas.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	3	vllm/model_executor/layers/quantization/gptq_marlin_24.py
2	3	vllm/model_executor/layers/quantization/ipex_quant.py
1	2	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
2	3	vllm/model_executor/layers/quantization/tpu_int8.py
13	5	vllm/model_executor/layers/quantization/utils/gptq_utils.py
1	2	vllm/model_executor/models/aimv2.py
2	1	vllm/model_executor/models/aria.py
1	2	vllm/model_executor/models/bailing_moe.py
1	2	vllm/model_executor/models/granite.py
1	2	vllm/model_executor/models/granitemoe.py
1	2	vllm/model_executor/models/granitemoeshared.py
1	2	vllm/model_executor/models/hunyuan_v1.py
1	2	vllm/model_executor/models/interfaces.py
1	2	vllm/model_executor/models/llama4_eagle.py
1	2	vllm/model_executor/models/mamba.py
1	2	vllm/model_executor/models/mamba2.py
1	2	vllm/model_executor/models/minimax_text_01.py
1	2	vllm/model_executor/models/ovis.py
1	2	vllm/model_executor/models/ovis2_5.py
1	2	vllm/model_executor/models/phimoe.py
1	1	vllm/model_executor/models/siglip2navit.py
1	2	vllm/model_executor/models/step3_text.py
1	2	vllm/model_executor/models/whisper.py

[d3bd17112] Zhuohan Li 2025-09-29 [Benchmark] Support benchmark throughput for external launcher DP (#25913)
27	6	vllm/benchmarks/throughput.py

[89e4050af] Wentao Ye 2025-09-29 [Bug] Fix Weight Loading for Block FP8 Cutlass SM90 (#25909)
4	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[78a47f87c] Andrew Sansom 2025-09-29 Test Prompt Embeds/LoRA compatibility and Enable LoRA Support for OPT Models  (#25717)
1	1	docs/features/README.md
1	1	docs/models/supported_models.md
8	0	tests/entrypoints/conftest.py
28	6	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
2	3	vllm/model_executor/models/opt.py

[6a113d9ae] Aaron Pham 2025-09-29 [V0 Deprecation] Remove `vllm.worker` and update according imports (#25901)
1	1	tests/model_executor/model_loader/tensorizer_loader/conftest.py
0	1	tools/pre_commit/check_pickle_imports.py
5	5	vllm/executor/executor_base.py
1	1	vllm/executor/ray_utils.py
5	5	vllm/executor/uniproc_executor.py
1	11	vllm/platforms/cuda.py
1	11	vllm/platforms/rocm.py
2	2	vllm/v1/executor/multiproc_executor.py
260	11	vllm/v1/worker/worker_base.py
0	0	vllm/worker/__init__.py
0	279	vllm/worker/worker_base.py

[2e4fe48c3] Nicolò Lucchesi 2025-09-29 [NIXL] Increase default KV block eviction timeout on P (#25897)
1	1	docs/features/nixl_connector_usage.md
2	2	vllm/envs.py

[8eb0a1d90] Zhuohan Li 2025-09-29 [Doc] Polish example for torchrun dp (#25899)
10	6	examples/offline_inference/torchrun_dp_example.py

[fea3e476a] Thomas Parnell 2025-09-29 [Kernel] Chunk-aligned mamba2 (#24683)
4	4	vllm/model_executor/layers/mamba/mamba_mixer2.py
17	25	vllm/model_executor/layers/mamba/ops/ssd_bmm.py
79	157	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
19	29	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
21	35	vllm/model_executor/layers/mamba/ops/ssd_combined.py
26	71	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
4	4	vllm/model_executor/models/plamo2.py
80	109	vllm/v1/attention/backends/mamba2_attn.py

[61a343161] Gregory Shtrasberg 2025-09-29 [Bugfix][ROCm] Fixing trying to import non-existent symbols from libnccl.so (#25605)
23	4	vllm/distributed/device_communicators/pynccl_wrapper.py

[9bedac962] Naman Lalit 2025-09-29 [Doc] Add documentation for vLLM continuous benchmarking and profiling (#25819)
24	0	docs/contributing/benchmarks.md
16	0	docs/contributing/profiling.md

[c42ff4f4f] Adrian Abeyta 2025-09-29 [BugFix][torch.compile] KV scale calculation issues with FP8 quantization (#25513)
15	0	tests/compile/test_full_graph.py
40	3	vllm/attention/layer.py
9	0	vllm/v1/worker/gpu_model_runner.py

[d5ab28511] Lee Nau 2025-09-29 [Bugfix] Use correct key "ignore" for config.json non-quantized layers (#25706)
6	2	vllm/model_executor/layers/quantization/modelopt.py

[e61eb5e09] Jee Jee Li 2025-09-30 [Model] Remove MotifForCausalLM (#25866)
0	2	docs/models/supported_models.md
0	3	tests/models/registry.py
0	4	tests/models/test_initialization.py
0	345	vllm/model_executor/models/motif.py
1	1	vllm/model_executor/models/registry.py

[0899ba5b4] Isotr0py 2025-09-30 [CI/Build] Include Transformers backend test in nightly transformers test (#25885)
1	0	.buildkite/test-pipeline.yaml

[145ac7331] Rahul Tuli 2025-09-29 [Bugfix][Speculative Decoding] Fix Eagle3 quantization config issue (#25883)
3	0	tests/speculative_decoding/speculators/test_eagle3.py
6	1	vllm/model_executor/models/llama.py
13	1	vllm/model_executor/models/llama_eagle3.py

[d0d138bc5] Chenxi Yang 2025-09-29 [Nixl][P/D] Add cuda2cpu support (HD->DH transfer) (#24690)
28	3	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
32	5	tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh
2	2	vllm/config/kv_transfer.py
7	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
24	0	vllm/platforms/cuda.py
3	4	vllm/v1/worker/gpu_model_runner.py

[43227236e] Jiangyun Zhu 2025-09-29 [torch.compile] serialize cudagraph_mode as its enum name instead of value (#25868)
8	4	vllm/config/compilation.py

[8616300ae] Zhou Jiahao 2025-09-29 [Model][Bugfix] Fix issues in MiDashengLM implementation for quantized models (#25854)
122	71	vllm/model_executor/models/midashenglm.py

[edbaadd91] Yingjun Mou 2025-09-29 [Bugfix] Fix requirements paths in install instructions (#25827)
6	6	docs/getting_started/installation/cpu/s390x.inc.md

[9360d34fa] youkaichao 2025-09-29 update to latest deepgemm for dsv3.2 (#25871)
1	1	docker/Dockerfile
1	1	tools/install_deepgemm.sh

[1b67b0465] Cyrus Leung 2025-09-29 [Misc] Remove more `get_input_embeddings_v0` (#25857)
1	2	vllm/model_executor/models/gemma3n_mm.py
1	45	vllm/model_executor/models/keye.py
1	36	vllm/model_executor/models/phi4_multimodal.py
4	0	vllm/model_executor/models/utils.py

[bd51f78e3] Isotr0py 2025-09-29 [V0 Deprecation][Models] Remove all V0 condition for mm embeddings merge (#25331)
0	11	vllm/model_executor/models/aya_vision.py
0	11	vllm/model_executor/models/blip2.py
0	12	vllm/model_executor/models/chameleon.py
0	11	vllm/model_executor/models/cohere2_vision.py
0	11	vllm/model_executor/models/deepseek_vl2.py
0	11	vllm/model_executor/models/fuyu.py
0	19	vllm/model_executor/models/gemma3_mm.py
1	49	vllm/model_executor/models/glm4_1v.py
1	16	vllm/model_executor/models/glm4v.py
0	11	vllm/model_executor/models/granite_speech.py
1	14	vllm/model_executor/models/hyperclovax_vision.py
0	11	vllm/model_executor/models/idefics3.py
1	17	vllm/model_executor/models/interns1.py
1	17	vllm/model_executor/models/internvl.py
0	16	vllm/model_executor/models/kimi_vl.py
0	11	vllm/model_executor/models/llava.py
0	11	vllm/model_executor/models/llava_next.py
0	11	vllm/model_executor/models/llava_next_video.py
1	45	vllm/model_executor/models/llava_onevision.py
1	14	vllm/model_executor/models/minicpmv.py
0	11	vllm/model_executor/models/mistral3.py
0	11	vllm/model_executor/models/mllama4.py
0	11	vllm/model_executor/models/molmo.py
1	17	vllm/model_executor/models/nano_nemotron_vl.py
0	11	vllm/model_executor/models/nemotron_vl.py
0	11	vllm/model_executor/models/ovis.py
0	12	vllm/model_executor/models/ovis2_5.py
0	11	vllm/model_executor/models/paligemma.py
0	11	vllm/model_executor/models/phi3v.py
0	16	vllm/model_executor/models/phi4_multimodal.py
1	47	vllm/model_executor/models/phi4mm.py
0	11	vllm/model_executor/models/pixtral.py
1	30	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	57	vllm/model_executor/models/qwen2_5_vl.py
0	11	vllm/model_executor/models/qwen2_audio.py
1	49	vllm/model_executor/models/qwen2_vl.py
1	92	vllm/model_executor/models/qwen3_vl.py
0	12	vllm/model_executor/models/qwen_vl.py
0	11	vllm/model_executor/models/skyworkr1v.py
0	13	vllm/model_executor/models/transformers.py
0	12	vllm/model_executor/models/ultravox.py
0	13	vllm/model_executor/models/voxtral.py

[65ecb4f13] Roger Wang 2025-09-28 [Bugfix] Fallback ViT attn backend to SDPA for blackwell (#25851)
1	9	vllm/model_executor/models/qwen3_vl.py
6	0	vllm/platforms/cuda.py

[143844fa4] Kunshang Ji 2025-09-29 [XPU]Fix xpu spec decoding UTs, avoid using cuda graph (#25847)
1	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh
2	0	tests/utils.py
4	3	vllm/v1/spec_decode/eagle.py

[219cfbe7f] Thomas Parnell 2025-09-29 Add Phi4FlashForCausalLM to _PREVIOUSLY_SUPPORTED_MODELS (#25832)
1	0	vllm/model_executor/models/registry.py

[9b44a7d92] Robert Shaw 2025-09-29 [P/D] NIXL Updates (#25844)
5	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
6	1	vllm/v1/core/sched/scheduler.py

[a3ae45a38] Juechen Liu 2025-09-28 [Misc] fix tests failure by using current_platform (#25825)
1	1	vllm/attention/ops/triton_reshape_and_cache_flash.py

[0307428d6] Michael Goin 2025-09-28 Remove redundant cudagraph dispatcher warning (#25841)
0	5	vllm/v1/cudagraph_dispatcher.py

[471997adf] JJJYmmm 2025-09-29 [Bugfix] fix Qwen3VLMoe load when pp > 1 (#25838)
2	2	vllm/model_executor/models/qwen3_vl_moe.py

[b1ded114b] Yuxuan Zhang 2025-09-28 Update GLM-4.5 Doc transformers version (#25830)
4	2	docs/features/tool_calling.md
1	1	docs/models/supported_models.md
1	1	tests/models/registry.py
1	1	vllm/model_executor/models/glm4_moe.py

[f4e4088c9] weiliang 2025-09-28 Fix random dataset mismatched token length with config. (#24937)
118	27	vllm/benchmarks/datasets.py

[0efd540db] Isotr0py 2025-09-28 [VLM] Update Qwen3-VL max_num_video_tokens calculation for configurable video profiling (#25557)
9	4	vllm/model_executor/models/qwen2_vl.py
65	5	vllm/model_executor/models/qwen3_vl.py

[614475401] Roger Wang 2025-09-27 [Bugfix] Fix Qwen3-VL regression from #24982 (#25814)
4	4	vllm/model_executor/models/qwen3_moe.py

[69311446b] Roger Wang 2025-09-27 [MM] Optimize memory profiling for scattered multimodal embeddings (#25810)
17	0	vllm/v1/worker/gpu_model_runner.py

[da63274d9] Nicolò Lucchesi 2025-09-27 [Bugfix][NIXL] Fix Async Scheduler timeout issue (#25808)
28	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[c216119d6] Jialin Ouyang 2025-09-27 [Core] GC Debug callback (#24829)
8	0	docs/contributing/profiling.md
69	0	tests/utils_/test_gc_utils.py
8	0	vllm/envs.py
128	0	vllm/utils/gc_utils.py
4	0	vllm/v1/engine/core.py

[5546acb46] Clayton Coleman 2025-09-27 [Bug]: Set LD_LIBRARY_PATH to include the 'standard' CUDA location (#25766)
6	0	docker/Dockerfile

[c0ec81836] Jiangyun Zhu 2025-09-28 [torch.compile]: Add VLLM_DEBUG_DUMP_PATH environment variable (#25651)
4	6	vllm/compilation/monitor.py
1	4	vllm/compilation/vllm_inductor_pass.py
5	6	vllm/compilation/wrapper.py
26	0	vllm/config/__init__.py
2	1	vllm/config/compilation.py
6	0	vllm/envs.py

[b65e56bab] Patrick C. Toulme 2025-09-27 [Core] Refactor self.model() to call a helper for subclassing. (#25084)
33	1	vllm/v1/worker/gpu_model_runner.py

[49996cd59] Peter Pan 2025-09-27 [env] default nixl side port conflicts with kv-event zmq port (#25056)
2	2	vllm/envs.py

[ecb37e276] yyzxw 2025-09-27 [docs] transcriptions API audio upload (#25446)
80	1	docs/serving/openai_compatible_server.md

[a5354b3ed] Tyler Michael Smith 2025-09-27 [Bugfix][WideEP] Apply TP Attn + EP MoE fix to other models (#24982)
18	0	vllm/config/parallel.py
82	40	vllm/distributed/device_communicators/all2all.py
20	8	vllm/distributed/device_communicators/base_device_communicator.py
12	10	vllm/distributed/device_communicators/cuda_communicator.py
11	5	vllm/distributed/device_communicators/xpu_communicator.py
12	5	vllm/distributed/parallel_state.py
66	31	vllm/forward_context.py
64	55	vllm/model_executor/layers/fused_moe/layer.py
7	9	vllm/model_executor/models/aria.py
3	55	vllm/model_executor/models/deepseek_v2.py
5	11	vllm/model_executor/models/ernie_mtp.py
9	7	vllm/model_executor/models/glm4.py
28	14	vllm/model_executor/models/gpt_oss.py
28	10	vllm/model_executor/models/granitemoe.py
13	12	vllm/model_executor/models/llama.py
29	15	vllm/model_executor/models/llama4.py
2	2	vllm/model_executor/models/llama4_eagle.py
5	3	vllm/model_executor/models/llama_eagle.py
11	16	vllm/model_executor/models/llama_eagle3.py
33	27	vllm/model_executor/models/qwen3_moe.py
35	35	vllm/model_executor/models/qwen3_next.py
1	5	vllm/model_executor/models/qwen3_next_mtp.py
47	1	vllm/model_executor/models/utils.py

[f9df8b4ad] Tyler Michael Smith 2025-09-27 [Bugfix] Fix triton import precommit failure (#25803)
2	2	vllm/model_executor/layers/batch_invariant.py

[ec152c874] Harry Mellor 2025-09-27 Fix GPTQ model loading in Transformers backend (#25770)
7	3	tests/models/test_transformers.py
17	5	vllm/model_executor/models/transformers.py
5	2	vllm/model_executor/models/utils.py

[7977e5027] Russell Bryant 2025-09-27 Add filtering for chat template kwargs (#25794)
85	0	tests/entrypoints/test_chat_utils.py
52	2	vllm/entrypoints/chat_utils.py
1	0	vllm/entrypoints/openai/api_server.py
7	3	vllm/entrypoints/openai/cli_args.py
13	1	vllm/entrypoints/openai/serving_chat.py

[3f5d902d2] Russell Bryant 2025-09-27 Validate API tokens in constant time (#25781)
24	4	vllm/entrypoints/openai/api_server.py

[27d7638b9] Cyrus Leung 2025-09-27 [Bugfix] Merge MM embeddings by index instead of token IDs (#16229)
5	28	docs/contributing/model/multimodal.md
6	1	vllm/config/model.py
6	19	vllm/model_executor/models/aria.py
6	21	vllm/model_executor/models/aya_vision.py
12	0	vllm/model_executor/models/bert.py
6	0	vllm/model_executor/models/bert_with_rope.py
6	16	vllm/model_executor/models/blip2.py
7	17	vllm/model_executor/models/chameleon.py
3	0	vllm/model_executor/models/chatglm.py
6	21	vllm/model_executor/models/cohere2_vision.py
6	0	vllm/model_executor/models/deepseek_eagle.py
6	0	vllm/model_executor/models/deepseek_mtp.py
7	18	vllm/model_executor/models/deepseek_vl2.py
12	32	vllm/model_executor/models/dots_ocr.py
18	13	vllm/model_executor/models/ernie45_vl.py
6	0	vllm/model_executor/models/ernie_mtp.py
6	20	vllm/model_executor/models/fuyu.py
6	20	vllm/model_executor/models/gemma3_mm.py
13	10	vllm/model_executor/models/gemma3n_mm.py
0	17	vllm/model_executor/models/glm4_1v.py
6	0	vllm/model_executor/models/glm4_moe_mtp.py
10	25	vllm/model_executor/models/glm4v.py
21	13	vllm/model_executor/models/granite_speech.py
3	0	vllm/model_executor/models/hunyuan_v1.py
10	25	vllm/model_executor/models/hyperclovax_vision.py
7	24	vllm/model_executor/models/idefics3.py
75	8	vllm/model_executor/models/interfaces.py
23	1	vllm/model_executor/models/interfaces_base.py
27	20	vllm/model_executor/models/interns1.py
27	19	vllm/model_executor/models/internvl.py
0	18	vllm/model_executor/models/keye.py
3	26	vllm/model_executor/models/kimi_vl.py
3	0	vllm/model_executor/models/lfm2.py
8	23	vllm/model_executor/models/llama4_eagle.py
6	0	vllm/model_executor/models/llama_eagle.py
4	13	vllm/model_executor/models/llama_eagle3.py
6	20	vllm/model_executor/models/llava.py
18	13	vllm/model_executor/models/llava_next.py
6	17	vllm/model_executor/models/llava_next_video.py
0	13	vllm/model_executor/models/llava_onevision.py
6	19	vllm/model_executor/models/midashenglm.py
6	0	vllm/model_executor/models/mimo_mtp.py
6	21	vllm/model_executor/models/minicpmv.py
2	8	vllm/model_executor/models/minimax_text_01.py
6	19	vllm/model_executor/models/minimax_vl_01.py
6	20	vllm/model_executor/models/mistral3.py
7	23	vllm/model_executor/models/mllama4.py
9	0	vllm/model_executor/models/modernbert.py
7	25	vllm/model_executor/models/molmo.py
13	30	vllm/model_executor/models/nano_nemotron_vl.py
22	15	vllm/model_executor/models/nemotron_vl.py
6	0	vllm/model_executor/models/olmo2.py
5	16	vllm/model_executor/models/ovis.py
5	13	vllm/model_executor/models/ovis2_5.py
6	17	vllm/model_executor/models/paligemma.py
32	12	vllm/model_executor/models/phi3v.py
3	15	vllm/model_executor/models/phi4_multimodal.py
0	14	vllm/model_executor/models/phi4mm.py
6	20	vllm/model_executor/models/pixtral.py
15	13	vllm/model_executor/models/qwen2_5_omni_thinker.py
0	13	vllm/model_executor/models/qwen2_5_vl.py
6	17	vllm/model_executor/models/qwen2_audio.py
0	13	vllm/model_executor/models/qwen2_vl.py
59	30	vllm/model_executor/models/qwen3_vl.py
7	18	vllm/model_executor/models/qwen_vl.py
3	0	vllm/model_executor/models/roberta.py
22	14	vllm/model_executor/models/skyworkr1v.py
3	0	vllm/model_executor/models/solar.py
3	0	vllm/model_executor/models/step3_text.py
26	26	vllm/model_executor/models/step3_vl.py
6	19	vllm/model_executor/models/tarsier.py
3	0	vllm/model_executor/models/terratorch.py
50	14	vllm/model_executor/models/transformers.py
20	16	vllm/model_executor/models/ultravox.py
37	57	vllm/model_executor/models/utils.py
9	20	vllm/model_executor/models/voxtral.py
5	5	vllm/model_executor/models/whisper.py
27	19	vllm/v1/spec_decode/eagle.py
39	13	vllm/v1/worker/gpu_model_runner.py
66	13	vllm/v1/worker/tpu_model_runner.py

[176173989] Xiaohan Zou 2025-09-27 [Bugfix] Add missing `image_size` for phi4_multimodal (#25796)
1	0	vllm/model_executor/models/phi4_multimodal.py

[23b8ee672] Roger Wang 2025-09-27 [Misc] Update openai client example file for multimodal (#25795)
36	27	examples/online_serving/openai_chat_completion_client_for_multimodal.py

[393915206] 22quinn 2025-09-27 [Misc] Fix codeowners override for v1 sample and attention (#25037)
5	5	.github/CODEOWNERS

[cd87bfbf3] Cyrus Leung 2025-09-27 [CI/Build] Reorganize root-level V1 tests (#25767)
1	2	.buildkite/scripts/hardware_ci/run-xpu-test.sh
8	15	.buildkite/test-pipeline.yaml
0	0	tests/v1/{ => core}/test_kv_sharing.py
0	0	tests/v1/distributed/__init__.py
0	0	tests/v1/{ => distributed}/test_async_llm_dp.py
0	0	tests/v1/{ => distributed}/test_external_lb_dp.py
1	1	tests/v1/{ => distributed}/test_hybrid_lb_dp.py
1	1	tests/v1/{ => distributed}/test_internal_lb_dp.py
1	1	tests/v1/entrypoints/openai/test_multi_api_servers.py
0	0	tests/v1/{ => metrics}/test_metrics_reader.py
0	61	tests/v1/{test_utils.py => utils.py}
63	0	tests/v1/worker/test_utils.py

[b3613e3ac] 22quinn 2025-09-26 [CI/Build] Add timing to Model Executor Test (#25799)
2	2	.buildkite/test-pipeline.yaml

[d346ec695] Cyrus Leung 2025-09-27 [CI/Build] Consolidate model loader tests and requirements (#25765)
4	15	.buildkite/test-pipeline.yaml
1	1	.github/mergify.yml
1	1	docker/Dockerfile
1	2	requirements/nightly_torch_test.txt
1	4	requirements/rocm.txt
1	2	requirements/test.in
6	4	requirements/test.txt
1	4	setup.py
0	52	tests/model_executor/conftest.py
0	0	tests/{ => model_executor/model_loader}/fastsafetensors_loader/__init__.py
0	0	tests/{ => model_executor/model_loader}/fastsafetensors_loader/test_fastsafetensors_loader.py
0	0	tests/{ => model_executor/model_loader}/fastsafetensors_loader/test_weight_utils.py
0	0	tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/__init__.py
0	0	tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/test_runai_model_streamer_loader.py
0	0	tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/test_runai_utils.py
0	0	tests/{runai_model_streamer_test => model_executor/model_loader/runai_model_streamer}/test_weight_utils.py
0	0	tests/{ => model_executor/model_loader}/tensorizer_loader/__init__.py
0	0	tests/{ => model_executor/model_loader}/tensorizer_loader/conftest.py
1	1	tests/{ => model_executor/model_loader}/tensorizer_loader/test_tensorizer.py
31	4	vllm/model_executor/model_loader/weight_utils.py

[c242c9803] Wentao Ye 2025-09-26 [Bugfix] Allow Only SDPA Backend for ViT on B200 for Qwen3-VL (#25788)
37	36	vllm/model_executor/models/qwen2_5_vl.py
38	15	vllm/model_executor/models/qwen3_vl.py

[f1d53d150] WeiQing Chen 2025-09-27 [Multimodal][Speculative Decoding]Eagle Eagle3 mm support, enablement on qwen2.5vl (#22872)
3	0	tests/models/registry.py
7	2	tests/v1/e2e/test_spec_decode.py
80	0	vllm/benchmarks/datasets.py
19	8	vllm/model_executor/models/llama_eagle3.py
9	2	vllm/model_executor/models/qwen2_5_vl.py
1	0	vllm/model_executor/models/registry.py
79	29	vllm/v1/spec_decode/eagle.py
12	4	vllm/v1/worker/gpu_model_runner.py

[92da847cf] Michael Goin 2025-09-26 Add flashinfer-build.sh and register precompiled cu128 wheel in Dockerfile (#25782)
20	10	docker/Dockerfile
63	0	tools/flashinfer-build.sh

[3958b96bf] Russell Bryant 2025-09-26 Add option to restrict media domains (#25783)
4	0	docs/features/multimodal_inputs.md
6	0	docs/usage/security.md
1	0	tests/entrypoints/openai/test_lora_resolvers.py
1	0	tests/entrypoints/openai/test_serving_chat.py
32	1	tests/multimodal/test_utils.py
3	0	vllm/config/model.py
2	0	vllm/config/speculative.py
5	0	vllm/engine/arg_utils.py
6	0	vllm/entrypoints/chat_utils.py
4	0	vllm/entrypoints/llm.py
16	0	vllm/multimodal/utils.py

[8bf8f4582] Zhuohan Li 2025-09-26 [Core] Don't count preempted tokens in prefix cache hit rate (#25787)
16	8	vllm/v1/core/kv_cache_manager.py
34	32	vllm/v1/core/sched/scheduler.py
7	1	vllm/v1/metrics/stats.py
3	0	vllm/v1/request.py

[6f5c0931c] Jonas M. Kübler 2025-09-27 [Spec decode] automatically disable mm for text-only draft models (#25667)
69	67	tests/v1/e2e/test_spec_decode.py
14	0	vllm/v1/spec_decode/eagle.py

[4e33a7ea8] Naman Lalit 2025-09-26 [Bugfix] Optimize CpuGpuBuffer initialization (#25447)
1	1	vllm/v1/utils.py

[dc48ba0c7] Bram Wasti 2025-09-26 Kernel-override Determinism [1/n] (#25603)
16	0	csrc/core/batch_invariant.hpp
6	2	csrc/layernorm_kernels.cu
4	1	csrc/layernorm_quant_kernels.cu
3	1	csrc/moe/topk_softmax_kernels.cu
290	0	tests/v1/generation/test_batch_invariance.py
561	0	vllm/model_executor/layers/batch_invariant.py
7	0	vllm/v1/attention/backends/flex_attention.py
3	0	vllm/v1/worker/gpu_model_runner.py

[4778b4266] Sage Moore 2025-09-26 Reduce the Cuda Graph memory footprint when running with DBO (#25779)
20	28	vllm/v1/worker/gpu_model_runner.py
12	0	vllm/v1/worker/gpu_ubatch_wrapper.py

[c70ac4b8f] qizixi 2025-09-26 [spec decode] Consolidate speculative decode method name for MTP (#25232)
3	2	examples/offline_inference/spec_decode.py
65	0	tests/v1/e2e/test_spec_decode.py
195	0	tests/v1/spec_decode/test_mtp.py
19	31	vllm/config/speculative.py
1	1	vllm/engine/arg_utils.py
4	6	vllm/v1/spec_decode/eagle.py

[cf8920285] Michael Goin 2025-09-26 [CI] Fix FlashInfer AOT in release docker image (#25730)
1	1	.buildkite/release-pipeline.yaml
3	0	docker/Dockerfile

[f075693da] fhl2000 2025-09-27 [V1] address post issues related to #20059 (part 1) (#23046)
1	85	tests/compile/piecewise/test_full_cudagraph.py
65	2	tests/compile/test_config.py
86	1	tests/v1/attention/utils.py
26	35	tests/v1/cudagraph/test_cudagraph_dispatch.py
9	75	tests/v1/cudagraph/test_cudagraph_mode.py
3	3	vllm/compilation/backends.py
2	2	vllm/compilation/decorators.py
0	0	vllm/compilation/{cuda_piecewise_backend.py => piecewise_backend.py}
25	12	vllm/config/__init__.py
76	37	vllm/config/compilation.py
1	2	vllm/forward_context.py
21	20	vllm/v1/cudagraph_dispatcher.py
31	16	vllm/v1/worker/gpu_model_runner.py

[f708bd490] Michael Goin 2025-09-26 [CI] Add E2E Blackwell Quantized MoE Test (#25723)
18	1	.buildkite/test-pipeline.yaml
132	0	tests/quantization/test_blackwell_moe.py
3	1	tests/utils.py
2	0	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py

[0002b7f0d] Michael Goin 2025-09-26 [Docs] Add Toronto Meetup (#25773)
1	0	README.md
1	0	docs/community/meetups.md

[11aafd988] Frank Wang 2025-09-26 [Bugfix] Improve GLM4 MoE Reasoning Parser's is_reasoning_end Condition (#25355)
203	0	tests/reasoning/test_glm4_moe_reasoning_parser.py
16	3	vllm/reasoning/glm4_moe_reasoning_parser.py

[b761df963] Clouddude 2025-09-26 [Doc]: improve CPU(x86) build-wheel-from-source section (#25617)
75	2	docs/getting_started/installation/cpu/x86.inc.md

[33f6aaf97] 阿丹(adan) 2025-09-27 Eagle3 that supports the Minicpm3 model (#24243)
1	1	vllm/config/speculative.py
43	8	vllm/model_executor/models/minicpm.py

[56aafa8c0] Jiangyun Zhu 2025-09-27 [Misc] fix unique_filepath (#25732)
21	8	tests/utils_/test_utils.py
21	0	vllm/utils/__init__.py

[8d52f2b3a] Seiji Eicher 2025-09-26 [ray][metrics] Replace ':' with '_' for OpenTelemetry compatibility in Ray (#25439)
38	1	tests/v1/metrics/test_ray_metrics.py
19	0	vllm/v1/metrics/ray_wrappers.py

[984d18498] Lucas Wilkinson 2025-09-26 [BugFix] Fix using `dbo_decode_token_threshold` always (and ignoring `dbo_prefill_token_threshold`) (#25622)
7	2	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/ubatch_splitting.py

[d4d989986] Isotr0py 2025-09-26 [Quantization] Add field to skip unquantized modules for GPTQ config (#25455)
1	0	vllm/config/__init__.py
6	0	vllm/model_executor/layers/quantization/base_config.py
46	6	vllm/model_executor/layers/quantization/gptq.py
55	10	vllm/model_executor/layers/quantization/gptq_marlin.py
51	1	vllm/model_executor/layers/quantization/utils/gptq_utils.py
2	10	vllm/model_executor/models/keye.py
1	35	vllm/model_executor/models/minicpmo.py
1	12	vllm/model_executor/models/ovis.py
1	12	vllm/model_executor/models/qwen2_5_vl.py
1	12	vllm/model_executor/models/qwen2_vl.py
6	22	vllm/model_executor/models/qwen3_moe.py
5	19	vllm/model_executor/models/qwen3_next.py
1	11	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/qwen3_vl_moe.py
31	1	vllm/transformers_utils/config.py
10	1	vllm/transformers_utils/utils.py

[db1e42f62] Cyrus Leung 2025-09-26 [CI/Build] Fix some V1 tests not being run (#25569)
4	2	.buildkite/test-pipeline.yaml
4	93	tests/v1/test_kv_sharing.py

[bc9d7b559] Cyrus Leung 2025-09-26 [CI/Build] Split up Distributed Tests (#25572)
27	16	.buildkite/test-pipeline.yaml
1	2	tests/{ => model_executor/model_loader}/test_sharded_state_loader.py

[fe6b19c31] wang.yuqi 2025-09-26 [Bugfix] Properly abort pooling request. (#25734)
33	0	tests/v1/engine/test_output_processor.py
8	1	vllm/v1/engine/output_processor.py

[2827b3f4a] Chauncey 2025-09-26 [CI] Fix test_shared_storage_connector_hashes (#25748)
22	0	tests/v1/kv_connector/unit/test_nixl_connector.py

[2b6b1d780] Chih-Chieh Yang 2025-09-26 [Model] Mamba2 varlen refactor  (#21467)
71	54	tests/kernels/mamba/test_mamba_ssm_ssd.py
10	13	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	1	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
69	89	vllm/model_executor/layers/mamba/ops/ssd_bmm.py
174	221	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
202	228	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
73	83	vllm/model_executor/layers/mamba/ops/ssd_combined.py
110	158	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
9	13	vllm/model_executor/models/plamo2.py
4	5	vllm/v1/attention/backends/mamba2_attn.py

[633f943e3] Cyrus Leung 2025-09-26 [Doc] Update Batch-level DP docs (#25757)
6	5	docs/configuration/optimization.md

[b03b1b97f] Xu Wenqing 2025-09-26 Support LongCat-Flash-Chat tool call (#24083)
9	0	docs/features/tool_calling.md
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
39	0	vllm/entrypoints/openai/tool_parsers/longcat_tool_parser.py

[dfb9af201] Sage Moore 2025-09-26 [Bugfix] Fix Shared Expert/Zero expert code in FusedMoE.process_chunk (#25698)
3	1	vllm/model_executor/layers/fused_moe/layer.py

[19f76ee68] yyzxw 2025-09-26 [misc] refactor speculative config (#25657)
6	8	vllm/config/speculative.py

[dd70437a4] Icey 2025-09-26 Remove cuda hard-code in compute_causal_conv1d_metadata (#25555)
3	2	vllm/v1/attention/backends/utils.py

[99b3a504c] Tao He 2025-09-26 [Qwen3-Next][GDN] fixes cuda graph capturing bug in GDN metadata and a stride bug in causal_conv_1d. (#25743)
8	3	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
26	35	vllm/v1/attention/backends/gdn_attn.py
16	7	vllm/v1/worker/gpu_model_runner.py

[6e30010d2] Iceber Gu 2025-09-26 fix: print outputt offline_inference/base/chat.py example (#25744)
1	0	examples/offline_inference/basic/chat.py

[52621c8f5] xaguilar-amd 2025-09-26 [Harware][AMD][Model] Triton MoE tuning configs for GLM-4.5 for MI300X (#25703)
164	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json

[d48f4d6da] Andrew Sansom 2025-09-26 perf: Avoid copying inputs_embeds tensors to GPU unless prompt_embeds is enabled (#25739)
15	11	vllm/v1/worker/gpu_model_runner.py

[e84e0735c] Andrew Sansom 2025-09-26 fix: revert cast to cpu in `MsgpackEncoder._encode_tensor` to avoid hidden performance regressions (#25738)
5	0	vllm/inputs/preprocess.py
1	1	vllm/v1/serial_utils.py

[3edf87d25] yitingdc 2025-09-26 [CI/Build] fix doc build warning: Failed to get 'name: description' pair (#25733)
2	2	vllm/v1/spec_decode/ngram_proposer.py

[392edee34] Eugene Khvedchenya 2025-09-26 EVS Support (Video tokens pruning) (#22980)
132	0	tests/models/multimodal/generation/test_qwen2_5_vl.py
16	11	vllm/config/model.py
9	0	vllm/config/multimodal.py
5	0	vllm/engine/arg_utils.py
55	0	vllm/model_executor/models/interfaces.py
226	12	vllm/model_executor/models/qwen2_5_vl.py
273	0	vllm/multimodal/evs.py
67	16	vllm/v1/worker/gpu_model_runner.py

[983056e45] Nick Hill 2025-09-25 [Misc] Remove unnecessary memoryviews in shm_broadcast.py (#25721)
4	4	vllm/distributed/device_communicators/shm_broadcast.py

[13dd93c66] Russell Bryant 2025-09-25 [Core] Force PIECEWISE CUDAGraph mode for encoder-decoder (#25701)
4	2	vllm/config/__init__.py

[53a30845b] Aleksandr Malyshev 2025-09-25 Llamas 3.1 405B fp4 changes upstreaming from 355_wip (#25135)
16	0	vllm/envs.py
20	8	vllm/model_executor/layers/linear.py
1	0	vllm/model_executor/layers/quantization/quark/quark.py
156	29	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
22	1	vllm/model_executor/layers/rotary_embedding/base.py
86	0	vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py

[8b77328ff] Nick Hill 2025-09-25 [Misc] Don't log shm dequeue delay warning on worker side (#25720)
30	30	vllm/distributed/device_communicators/shm_broadcast.py
1	1	vllm/v1/executor/multiproc_executor.py

[9fe4c2bdb] Wentao Ye 2025-09-25 [Refactor] Remove DeepGEMM OP Register (#25710)
0	78	vllm/model_executor/layers/quantization/deepgemm.py
5	12	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[081b5594a] Shu Wang 2025-09-25 Fix routing_bias dtype  (#25711)
4	1	vllm/model_executor/layers/quantization/modelopt.py

[57329a8c0] tomeras91 2025-09-26 [Model] rename NemotronH_Nano_VL -> NemotronH_Nano_VL_V2 (#25708)
1	1	tests/models/registry.py
4	4	vllm/model_executor/models/nano_nemotron_vl.py
1	1	vllm/model_executor/models/registry.py

[8c435c9bc] Zhuohan Li 2025-09-25 [Core] Enable command line logging for LLMEngine (#25610)
31	8	vllm/v1/engine/llm_engine.py
0	1	vllm/v1/metrics/loggers.py

[e71b8e210] Ekagra Ranjan 2025-09-25 [Spec Decode] Add Batch Parallel Ngram. Upto 8x lower overhead. (#24986)
104	3	benchmarks/benchmark_ngram_proposer.py
114	28	tests/v1/spec_decode/test_ngram.py
1	1	vllm/v1/sample/rejection_sampler.py
157	38	vllm/v1/spec_decode/ngram_proposer.py
5	37	vllm/v1/worker/gpu_model_runner.py

[89fa54e6f] Cyrus Leung 2025-09-26 [Optimization] Use a cheaper cache key in `get_model_architecture` (#25682)
9	2	vllm/model_executor/model_loader/utils.py

[3d54bdcb7] Cyrus Leung 2025-09-26 [Optimization] Streamline `InputPreprocessor` (#25702)
12	278	vllm/inputs/preprocess.py

[6b0fcbbf4] Cyrus Leung 2025-09-26 [Misc] Simplify `test_argsort_mm_positions` (#25690)
21	26	tests/multimodal/test_utils.py

[0fa673af4] Jee Jee Li 2025-09-26 [V0 deprecation] Clean up LoRA  (#25686)
1	8	vllm/lora/punica_wrapper/punica_gpu.py

[3468f17eb] Matthew Bonanni 2025-09-25 [V0 deprecation] Remove _VLLM_V1 suffixes from attention backend names (#25489)
1	1	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	1	tests/compile/piecewise/test_full_cudagraph.py
1	1	tests/compile/test_fusion_attn.py
1	1	tests/entrypoints/openai/test_serving_chat.py
18	16	tests/kernels/attention/test_attention_selector.py
5	9	tests/kernels/attention/test_rocm_attention_selector.py
4	4	tests/kernels/utils.py
1	1	tests/models/test_initialization.py
4	4	tests/utils.py
7	8	tests/v1/attention/test_attention_backends.py
2	2	tests/v1/attention/test_mla_backends.py
6	6	tests/v1/attention/utils.py
1	1	tests/v1/cudagraph/test_cudagraph_mode.py
5	2	tests/v1/e2e/test_cascade_attention.py
3	4	tests/v1/e2e/test_spec_decode.py
10	12	tests/v1/spec_decode/test_eagle.py
3	4	tests/v1/spec_decode/test_max_len.py
1	1	tests/v1/spec_decode/test_tree_attention.py
0	23	tests/v1/test_oracle.py
6	15	vllm/attention/layer.py
8	0	vllm/attention/selector.py
4	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
4	8	vllm/engine/arg_utils.py
1	1	vllm/model_executor/warmup/kernel_warmup.py
10	11	vllm/platforms/cuda.py
2	10	vllm/platforms/interface.py
2	3	vllm/platforms/rocm.py
1	2	vllm/platforms/tpu.py
6	6	vllm/platforms/xpu.py
1	1	vllm/v1/attention/backends/cpu_attn.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/flashmla.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
1	1	vllm/v1/attention/backends/mla/triton_mla.py
1	1	vllm/v1/attention/backends/pallas.py
1	1	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/rocm_attn.py
1	1	vllm/v1/attention/backends/tree_attn.py
1	1	vllm/v1/attention/backends/triton_attn.py
1	1	vllm/v1/attention/backends/xformers.py

[71b25b0d4] Isotr0py 2025-09-26 [V0 deprecation] Clean up V0 fallback in compilation config (#25675)
20	70	vllm/config/__init__.py
2	3	vllm/config/compilation.py

[0ea80c87d] Cyrus Leung 2025-09-26 [Model] Define `merge_by_field_config` MM interface (#25676)
18	5	tests/models/multimodal/processing/test_tensor_schema.py
2	3	vllm/config/model.py
6	0	vllm/model_executor/models/interfaces.py
11	2	vllm/v1/worker/gpu_model_runner.py
7	2	vllm/v1/worker/tpu_model_runner.py

[b8d9e4a32] Tao Hui 2025-09-26 [Model] Add optional parameter to reasoning parser constructor (#25554)
1	1	vllm/reasoning/abs_reasoning_parsers.py
2	2	vllm/reasoning/basic_parsers.py
2	2	vllm/reasoning/glm4_moe_reasoning_parser.py
2	2	vllm/reasoning/gptoss_reasoning_parser.py
2	2	vllm/reasoning/granite_reasoning_parser.py
2	2	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
2	2	vllm/reasoning/mistral_reasoning_parser.py
2	2	vllm/reasoning/step3_reasoning_parser.py

[13cc7f537] Lucas Wilkinson 2025-09-25 [BugFix] Fix DBO hang (#25625)
2	1	vllm/v1/worker/gpu_ubatch_wrapper.py

[916bd9204] Michael Goin 2025-09-25 Revert "[Bug] Dynamo Unsupported due to `BasevLLMParameter.torch_function` calling disabled super()" (#25681)
0	10	vllm/model_executor/parameter.py

[e04a1b6b2] AlonKejzman 2025-09-25 [BUGFIX] Fix crash in Eagle Speculative Decoding models when exceedin… (#24662)
16	2	vllm/v1/worker/gpu_model_runner.py

[2e5df88c9] Tyler Michael Smith 2025-09-25 [Logging] Remove TORCH_NCCL_AVOID_RECORD_STREAMS to squash a warning (#25532)
0	8	vllm/v1/worker/gpu_worker.py

[0754ac4c4] Nicolò Lucchesi 2025-09-25 [Misc] Remove cruft file in repo (#25678)
0	61	tests/test_test.py

[03858e6d1] Isotr0py 2025-09-25 [Bugfix] Fix InternS1 video processing after Transformers v4.56 (#25644)
2	1	.buildkite/test-pipeline.yaml
1	0	tests/models/multimodal/processing/test_common.py
10	1	vllm/model_executor/models/interns1.py
55	1	vllm/transformers_utils/processor.py

[532a6cfcc] Russell Bryant 2025-09-25 [ux] Switch a warning to debug about a pytorch fallback (#23750)
3	3	vllm/v1/sample/ops/topk_topp_sampler.py

[eb32335e3] Li, Jiang 2025-09-25 [CPU] update torch 2.8 and fix missing fields in TorchSDPAMetadata (#25652)
2	5	.buildkite/scripts/hardware_ci/run-cpu-test.sh
0	3	docker/Dockerfile.cpu
1	3	requirements/cpu-build.txt
2	2	requirements/cpu.txt
13	1	vllm/v1/attention/backends/cpu_attn.py
41	0	vllm/v1/sample/ops/topk_topp_sampler.py
0	39	vllm/v1/worker/cpu_worker.py

[69a8c8e99] Jonas M. Kübler 2025-09-25 [torch.compile] Make Query Quantization Fusable (#24914)
8	0	vllm/attention/backends/abstract.py
22	1	vllm/attention/layer.py
2	7	vllm/v1/attention/backends/flash_attn.py

[6c340da4d] youkaichao 2025-09-25 [misc] log info messages by default for hanging / busy / idle (#25627)
6	4	vllm/distributed/device_communicators/shm_broadcast.py

[2f1711760] Cyrus Leung 2025-09-25 [mypy] Fix wrong type annotations related to tuple (#25660)
4	4	benchmarks/kernels/benchmark_lora.py
3	0	tests/engine/test_arg_utils.py
1	1	tests/kernels/core/test_pos_encoding.py
2	2	tests/kernels/test_onednn.py
6	6	tests/models/multimodal/generation/vlm_utils/types.py
5	3	tests/v1/sample/test_sampler.py
1	1	tests/v1/spec_decode/test_eagle.py
1	1	vllm/distributed/device_communicators/ray_communicator.py
2	2	vllm/logits_process.py

[1e9a77e03] chenlang 2025-09-25 [Hardware][RISC-V] Add riscv64 support for vLLM with scalar (#22112)
8	1	cmake/cpu_extension.cmake
2	1	csrc/cpu/cpu_types.hpp
513	0	csrc/cpu/cpu_types_scalar.hpp
106	0	csrc/cpu/float_convert.hpp
3	0	vllm/platforms/interface.py

[d2af67441] Kunshang Ji 2025-09-25 [XPU][Triton]add xpu config in triton_reshape_and_cache_flash (#25643)
1	1	vllm/attention/ops/triton_reshape_and_cache_flash.py

[0bcc3a160] Cyrus Leung 2025-09-25 [CI/Build] Fix flaky entrypoints test (#25663)
41	23	tests/entrypoints/openai/test_completion_with_prompt_embeds.py

[70fbdb26e] Harry Mellor 2025-09-25 Add backward compatibility for `guided_...` API (#25615)
11	0	docs/features/structured_outputs.md
122	2	vllm/entrypoints/openai/protocol.py

[7f570f1ca] wang.yuqi 2025-09-25 [V0 deprecation] Remove unreachable model_config.supported_tasks (#25642)
0	3	tests/test_config.py
0	61	vllm/config/model.py
5	0	vllm/engine/protocol.py
1	5	vllm/entrypoints/openai/api_server.py
1	7	vllm/entrypoints/openai/run_batch.py

[eaeca3cd7] yyzxw 2025-09-25 [Bugfix] Parse SpeculativeConfig Error (#25142)
12	6	vllm/engine/arg_utils.py

[12c1287d6] Cyrus Leung 2025-09-25 [mypy] Further improve MM type annotations (#25654)
5	2	vllm/model_executor/models/transformers.py
15	8	vllm/multimodal/inputs.py
9	10	vllm/multimodal/processing.py
3	3	vllm/multimodal/profiling.py
24	20	vllm/multimodal/utils.py
34	5	vllm/utils/jsontree.py

[17b4c6685] Isotr0py 2025-09-25 [Bugfix] Fix Qwen3-VL max_num_video_tokens calculation for video profiling (#25648)
1	1	vllm/model_executor/models/qwen2_vl.py
12	0	vllm/model_executor/models/qwen3_vl.py

[3c2b2ccec] Agata Dobrzyniewicz 2025-09-25 [Bugfix] Add triton.language.tensor placeholder (#25649)
2	0	tests/test_triton_utils.py
1	0	vllm/triton_utils/importing.py

[7be9ffcd9] Roger Wang 2025-09-25 [Misc] Fix Qwen3-VL `video_grid_thw` typing (#25646)
1	1	vllm/model_executor/models/qwen3_vl.py

[393de22d2] Fadi Arafeh 2025-09-25 [fix] Update torch version in cpu-build.txt for AArch64/ppc64le and Darwin (#25579)
2	1	requirements/cpu-build.txt

[1260180c6] Tyler Michael Smith 2025-09-25 Revert "[Performance] Move apply_w8a8_block_fp8_linear to an op class… (#25607)
2	2	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
2	2	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
2	3	tests/kernels/quantization/test_block_fp8.py
7	19	tests/kernels/quantization/test_fp8_quant_group.py
30	0	tests/model_executor/test_enabled_custom_ops.py
0	35	tests/quantization/test_compressed_tensors.py
0	17	vllm/config/__init__.py
0	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
11	26	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
5	5	vllm/model_executor/layers/quantization/deepgemm.py
12	28	vllm/model_executor/layers/quantization/fp8.py
8	16	vllm/model_executor/layers/quantization/input_quant_fp8.py
122	177	vllm/model_executor/layers/quantization/utils/fp8_utils.py
4	8	vllm/utils/deep_gemm.py

[af4ee63e0] Nicole LiHui 🥜 2025-09-25 typo: remove duplicate `is` (#25641)
2	2	vllm/v1/engine/processor.py

[bc092ea87] Jacob Kahn 2025-09-25 Map CwmForCausalLM to llama and LlamaForCausalLM (#25611)
3	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py

[755ed7b05] Cyrus Leung 2025-09-25 [Misc] Simplify PoolerOutput and move to `v1/outputs` (#25629)
2	2	vllm/executor/executor_base.py
8	21	vllm/model_executor/layers/pooler.py
3	3	vllm/model_executor/models/gritlm.py
0	48	vllm/sequence.py
6	1	vllm/v1/outputs.py
15	7	vllm/v1/worker/gpu_model_runner.py

[a676e668e] courage17340 2025-09-25 [Bugfix] fix apply_temperature to avoid nan in probs (#24734)
6	1	vllm/v1/sample/sampler.py
2	2	vllm/v1/worker/gpu_input_batch.py

[c85be1f6d] Nicole LiHui 🥜 2025-09-25 optimize: eliminate duplicate split_enc_dec_inputs calls (#25573)
4	5	vllm/v1/engine/processor.py

[845adb3ec] XuruiYang 2025-09-25 [Model] Add LongCat-Flash  (#23991)
8	2	csrc/moe/moe_align_sum_kernels.cu
1	0	docs/models/supported_models.md
2	2	tests/kernels/moe/test_flashinfer.py
6	0	tests/models/registry.py
9	3	tests/models/utils.py
1	1	tests/test_routing_simulator.py
5	1	vllm/config/model.py
19	2	vllm/config/speculative.py
89	0	vllm/model_executor/layers/fused_moe/fused_moe.py
72	15	vllm/model_executor/layers/fused_moe/layer.py
0	1	vllm/model_executor/layers/mla.py
1	1	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
5	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/experts_int8.py
29	9	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/gguf.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	2	vllm/model_executor/layers/quantization/modelopt.py
1	1	vllm/model_executor/layers/quantization/moe_wna16.py
3	3	vllm/model_executor/layers/quantization/mxfp4.py
2	2	vllm/model_executor/layers/quantization/quark/quark_moe.py
1	1	vllm/model_executor/layers/quantization/rtn.py
5	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
712	0	vllm/model_executor/models/longcat_flash.py
352	0	vllm/model_executor/models/longcat_flash_mtp.py
2	0	vllm/model_executor/models/registry.py
13	5	vllm/model_executor/models/utils.py
7	4	vllm/v1/spec_decode/eagle.py
3	1	vllm/v1/worker/gpu_model_runner.py
3	1	vllm/v1/worker/utils.py

[90b139cff] Saman A. Pour 2025-09-24 Enable Fbgemm NVFP4 on Dense models (#25609)
61	4	benchmarks/kernels/bench_nvfp4_gemm.py
4	1	vllm/envs.py
24	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py

[4492e3a55] Wentao Ye 2025-09-24 [Bug] Dynamo Unsupported due to `BasevLLMParameter.torch_function` calling disabled super() (#25613)
10	0	vllm/model_executor/parameter.py

[05c19485a] Wei Wei 2025-09-24 [Kernel] Support DCP for Triton backend  (#25132)
5	0	tests/kernels/attention/test_triton_decode_attention.py
17	2	vllm/attention/ops/triton_decode_attention.py
1	1	vllm/model_executor/models/deepseek_v2.py
7	5	vllm/v1/attention/backends/mla/triton_mla.py

[52d0cb845] Jee Jee Li 2025-09-25 [Model] Improve DotsOCRForCausalLM (#25466)
143	94	vllm/model_executor/models/dots_ocr.py

[5c1e496a7] Shiyan Deng 2025-09-24 [MISC] replace c10::optional with std::optional (#25602)
2	2	csrc/rocm/ops.h
2	2	csrc/rocm/skinny_gemms.cu

[e7f27ea64] Harry Mellor 2025-09-25 Improve `--help` for enhanced user experience (#24903)
1	1	docs/mkdocs/hooks/generate_argparse.py
2	2	vllm/engine/arg_utils.py
5	8	vllm/entrypoints/cli/benchmark/main.py
1	1	vllm/entrypoints/cli/main.py
4	6	vllm/entrypoints/cli/run_batch.py
13	7	vllm/entrypoints/cli/serve.py
9	104	vllm/entrypoints/utils.py
79	8	vllm/utils/__init__.py

[1f2914125] Wentao Ye 2025-09-24 [Refactor] Use DeepGEMM Col Major TMA Aligned Tensor (#25517)
6	2	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
4	3	tests/kernels/quantization/test_block_fp8.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
6	4	vllm/model_executor/layers/quantization/fp8.py
1	65	vllm/model_executor/layers/quantization/utils/fp8_utils.py
14	1	vllm/utils/deep_gemm.py

[6160ba415] Duncan Moss 2025-09-24 feat: BF16 FlashInfer Fused Cutlass MOE for Hopper and Blackwell Expert Parallel (#25503)
6	0	vllm/envs.py
60	5	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
2	1	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
51	0	vllm/model_executor/layers/fused_moe/layer.py
2	0	vllm/model_executor/layers/fused_moe/modular_kernel.py

[fea800606] Tyler Michael Smith 2025-09-24 [Logging] Improve log for when DeepEP HT disables CUDA Graphs (#25531)
6	5	vllm/platforms/cuda.py

[e6750d0b1] Woosuk Kwon 2025-09-24 [V0 Deprecation] Remove unused classes in attention (#25541)
1	5	vllm/attention/__init__.py
3	142	vllm/attention/backends/abstract.py
1	544	vllm/attention/backends/utils.py
0	15	vllm/v1/attention/backends/cpu_attn.py
0	5	vllm/v1/attention/backends/pallas.py
6	5	vllm/v1/spec_decode/eagle.py

[8c853050e] Harry Mellor 2025-09-24 [Docs] Enable `fail_on_warning` for the docs build in CI (#25580)
1	0	.readthedocs.yaml
4	4	docs/features/nixl_connector_usage.md
3	2	docs/mkdocs/hooks/generate_argparse.py
1	1	docs/models/generative_models.md
1	1	docs/models/supported_models.md
1	1	docs/usage/README.md
2	2	examples/online_serving/dashboards/grafana/README.md
2	2	examples/online_serving/dashboards/perses/README.md
19	17	vllm/attention/ops/common.py
2	2	vllm/inputs/data.py
6	6	vllm/model_executor/layers/fused_moe/layer.py
2	2	vllm/model_executor/model_loader/weight_utils.py
12	8	vllm/model_executor/models/qwen3_vl.py
2	3	vllm/model_executor/models/zamba2.py
5	5	vllm/reasoning/granite_reasoning_parser.py
17	30	vllm/transformers_utils/configs/radio.py
1	1	vllm/transformers_utils/dynamic_module.py
0	0	vllm/v1/kv_offload/__init__.py
0	0	vllm/v1/kv_offload/backends/__init__.py
0	0	vllm/v1/kv_offload/worker/__init__.py

[f84a472a0] Sage Moore 2025-09-24 Suppress benign cuBLAS warning when capturing cudagraphs with DBO (#25596)
2	0	vllm/v1/worker/gpu_ubatch_wrapper.py

[54e42b72d] Shu Wang 2025-09-24 Support mnnvl all2allv from Flashinfer (#21003)
3	2	tests/kernels/moe/modular_kernel_tools/mk_objects.py
110	15	vllm/distributed/device_communicators/all2all.py
5	0	vllm/distributed/device_communicators/cuda_communicator.py
28	0	vllm/distributed/device_communicators/mnnvl_compat.py
5	2	vllm/envs.py
3	4	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
220	13	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
4	2	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
2	2	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
30	0	vllm/utils/flashinfer.py

[2dda3e35d] rongfu.leng 2025-09-25 [Bugfix] add cache model when from object storage get model (#24764)
6	0	vllm/envs.py
8	8	vllm/transformers_utils/runai_utils.py

[d83f3f7cb] Michael Goin 2025-09-24 Fixes and updates to bench_per_token_quant_fp8 (#25591)
26	24	benchmarks/kernels/bench_per_token_quant_fp8.py

[302eb941f] Gregory Shtrasberg 2025-09-24 [ROCm][Build][Bugfix] Fix ROCm base docker whls installation order (#25415)
12	12	docker/Dockerfile.rocm_base

[487745ff4] Gregory Shtrasberg 2025-09-24 [ROCm][Bugfix] Only enable +rms_norm based on aiter if not explicitly disabled (#25275)
2	1	vllm/platforms/rocm.py

[9313be501] Cyrus Leung 2025-09-24 [Misc] Improve type annotations for jsontree (#25577)
12	11	vllm/model_executor/models/aya_vision.py
4	6	vllm/model_executor/models/llava.py
4	6	vllm/model_executor/models/minimax_vl_01.py
4	11	vllm/model_executor/models/tarsier.py
64	5	vllm/utils/jsontree.py

[8938774c7] Harry Mellor 2025-09-24 Move `DeviceConfig`, `ObservabilityConfig`, `SpeechToTextConfig` to their own files (#25564)
3	183	vllm/config/__init__.py
74	0	vllm/config/device.py
99	0	vllm/config/observability.py
39	0	vllm/config/speech_to_text.py

[e18b714b2] Tao Hui 2025-09-24 [Bugfix] Fix DeepSeekV31ToolParser to correctly parse multiple tools in non-streaming output (#25405)
54	0	tests/tool_use/test_deepseekv31_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py

[b1068903f] Peter Pan 2025-09-24 [docs] fix nixl kv_connector_extra_config.backends key (#25565)
1	1	docs/features/disagg_prefill.md
1	1	docs/serving/expert_parallel_deployment.md

[164299500] Russell Bryant 2025-09-24 [Benchmark] Fix regression in structured output benchmark (#25500)
2	1	benchmarks/benchmark_serving_structured_output.py

[58c360d9b] Jonas M. Kübler 2025-09-24 [Bug] fix import and unit test (#25558)
1	1	tests/v1/attention/test_attention_splitting.py

[42488dae6] Roger Wang 2025-09-24 [Bugfix] Fix dummy video number of frames calculation (#25553)
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/tpu_model_runner.py

[b67dece2d] youkaichao 2025-09-24 [misc] update the warning message (#25566)
4	2	vllm/distributed/device_communicators/shm_broadcast.py

[2338daffd] Lucas Wilkinson 2025-09-24 [BugFix] Potential Fix for FA3 full-cudagraph IMA  (#25490)
11	11	vllm/v1/attention/backends/flash_attn.py

[2e19a848d] Woosuk Kwon 2025-09-24 [V0 Deprecation] Remove max_seq_len_to_capture (#25543)
0	1	tests/tpu/lora/test_lora.py
2	2	vllm/attention/backends/utils.py
0	18	vllm/config/model.py
0	2	vllm/config/speculative.py
0	4	vllm/engine/arg_utils.py
0	7	vllm/entrypoints/llm.py
0	14	vllm/model_executor/models/config.py

[77a7fce1b] Jackmin801 2025-09-24 [CI/Build] add nightly prime-rl integration tests (#25207)
59	0	.buildkite/scripts/run-prime-rl-test.sh
12	0	.buildkite/test-pipeline.yaml

[6488f3481] Cyrus Leung 2025-09-24 [Misc]] Move processing context to multimodal directory (#25548)
2	2	tests/models/multimodal/processing/test_common.py
2	2	tests/models/multimodal/processing/test_tensor_schema.py
9	4	tests/models/utils.py
2	2	tests/multimodal/test_processing.py
0	3	vllm/inputs/__init__.py
0	206	vllm/inputs/registry.py
3	3	vllm/model_executor/models/hyperclovax_vision.py
4	3	vllm/model_executor/models/llava.py
4	3	vllm/model_executor/models/mistral3.py
4	3	vllm/model_executor/models/mllama4.py
3	3	vllm/model_executor/models/tarsier.py
226	5	vllm/multimodal/processing.py
3	3	vllm/multimodal/registry.py

[27ec3c78f] Isotr0py 2025-09-24 [CI/Build] Fix v1 OOT registration test (#25547)
2	2	tests/conftest.py
0	2	tests/models/test_oot_registration.py
18	17	vllm/model_executor/models/registry.py

[1cbcfb94d] Li, Jiang 2025-09-24 [Bugfix][CPU] Skip unsupported custom op register on CPU (#25534)
8	6	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[fed8a9b10] Cyrus Leung 2025-09-24 [Misc] Retry HF processing if "Already borrowed" error occurs (#25535)
20	0	vllm/inputs/registry.py

[190c45a6a] Chengji Yao 2025-09-23 [TPU][Bugfix] fix the missing apply_model in tpu worker (#25526)
1	5	tests/v1/tpu/test_tpu_int8.py
7	1	vllm/v1/worker/tpu_worker.py

[5caaeb714] Ben Browning 2025-09-23 [Bugfix] [Frontend] Cleanup gpt-oss non-streaming chat tool calls (#25514)
1	0	tests/entrypoints/openai/test_serving_chat.py
74	2	tests/tool_use/test_openai_tool_parser.py
5	8	vllm/entrypoints/openai/serving_chat.py
22	2	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py

[d747c2ef1] Corey Lowman 2025-09-23 [Perf] Fix jit compiles at runtime of fla gated delta rule (#25432)
2	2	vllm/model_executor/layers/fla/ops/fused_recurrent.py

[c30b405b8] Benjamin Chislett 2025-09-23 [Spec Decode] Enable FlashInfer Spec Decoding (#25196)
108	1	tests/v1/attention/test_attention_splitting.py
15	0	vllm/utils/flashinfer.py
44	11	vllm/v1/attention/backends/flashinfer.py
3	3	vllm/v1/attention/backends/gdn_attn.py
1	2	vllm/v1/attention/backends/linear_attn.py
1	1	vllm/v1/attention/backends/mamba_attn.py
2	2	vllm/v1/attention/backends/mla/common.py
2	2	vllm/v1/attention/backends/mla/flashattn_mla.py
2	2	vllm/v1/attention/backends/short_conv_attn.py
66	6	vllm/v1/attention/backends/utils.py
2	2	vllm/v1/attention/backends/xformers.py
4	17	vllm/v1/spec_decode/eagle.py

[77d906995] Yong Hoon Shin 2025-09-23 [KV sharing] Re-land Gemma3n model changes from #22628 (#24357)
344	58	vllm/model_executor/models/gemma3n.py

[359d29300] Nikhil Gupta 2025-09-24 [fix]: add Arm 4bit fused moe support (#23809)
2	1	cmake/cpu_extension.cmake
10	0	csrc/cpu/torch_bindings.cpp
156	0	csrc/moe/dynamic_4bit_int_moe_cpu.cpp
6	0	csrc/ops.h
9	6	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
0	2	vllm/model_executor/layers/fused_moe/layer.py
305	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[9df8da548] Lucas Wilkinson 2025-09-23 [BugFix] Fix MLA assert with CUTLASS MLA (#25478)
46	18	vllm/v1/attention/backends/mla/common.py

[bf68fd76a] Wentao Ye 2025-09-23 [Compile] Fix AMD Compile Error (#25518)
6	1	csrc/quantization/activation_kernels.cu
6	0	csrc/rocm/attention.cu

[de94289a9] Kyle Sayers 2025-09-24 [Core] Support weight_loader_v2 for `UnquantizedLinearMethod` (#23036)
37	6	vllm/compilation/decorators.py
11	5	vllm/model_executor/layers/linear.py
22	1	vllm/model_executor/parameter.py

[198360923] Benjamin Chislett 2025-09-23 [Bugfix] Use a separate FlashInfer workspace buffer for trtllm-gen (#25520)
12	2	vllm/v1/attention/backends/flashinfer.py

[d06b5a95c] baxingpiaochong 2025-09-24 [V1][Metrics] Add per-request TPOT histogram (#24015)
15	0	vllm/v1/metrics/loggers.py
9	1	vllm/v1/metrics/stats.py

[be0bb568c] 0xNullPath 2025-09-24 [Model] Support SeedOss Reason Parser (#24263)
392	0	tests/reasoning/test_base_thinking_reasoning_parser.py
237	0	tests/reasoning/test_seedoss_reasoning_parser.py
4	0	vllm/reasoning/__init__.py
4	4	vllm/reasoning/abs_reasoning_parsers.py
156	0	vllm/reasoning/basic_parsers.py
27	133	vllm/reasoning/deepseek_r1_reasoning_parser.py
14	5	vllm/reasoning/mistral_reasoning_parser.py
25	104	vllm/reasoning/qwen3_reasoning_parser.py
28	0	vllm/reasoning/seedoss_reasoning_parser.py

[c8bde9336] ahao-anyscale 2025-09-23 [BUG] Allows for RunAI Streamer and Torch.compile cache to be used together (#24922)
107	0	tests/test_config.py
3	2	vllm/config/model.py
9	2	vllm/transformers_utils/runai_utils.py

[88d7bdbd2] Wentao Ye 2025-09-23 [Bug] Fix AttributeError: 'FusedMoE' object has no attribute 'w13_weight_scale'. Did you mean: 'w13_weight_scale_inv' (#25519)
4	2	vllm/model_executor/warmup/deep_gemm_warmup.py

[0d235b874] Chenxi Yang 2025-09-23 Add CUTLASS FP8 MOE benchmark scripts and kernel config (#25302)
406	0	benchmarks/kernels/benchmark_cutlass_moe_fp8.py
123	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=NVIDIA_H100,dtype=fp8_w8a8.json

[7ad5e50ad] Doug Smith 2025-09-23 Improve output when failing json.loads() on structured output test (#25483)
18	13	tests/v1/entrypoints/llm/test_struct_output_generate.py

[dc464a3d3] Lucas Wilkinson 2025-09-23 [BugFix] AssertionError: Do not capture num_reqs > max_num_reqs for uniform batch (#25505)
16	16	vllm/v1/worker/gpu_model_runner.py

[1210e4d95] Alexander Matveev 2025-09-23 [Bugfix] [B200] cutlass_mla - ensure kv_split == 1 for batch size > 1 (#25509)
2	2	csrc/attention/mla/cutlass_sm100_mla/device/sm100_mla.hpp

[e0b24ea03] Lucas Wilkinson 2025-09-23 [Perf] Increase default max splits for FA3 full cudagraphs (#25495)
2	2	vllm/envs.py

[bde2a1a8a] Juan Villamizar 2025-09-23 [ROCm] Small functional changes for gptoss (#25201)
6	3	vllm/model_executor/layers/quantization/mxfp4.py
14	3	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
6	0	vllm/platforms/rocm.py

[5e25b1223] Thomas Parnell 2025-09-24 [Kernel] [Mamba] Remove BLOCK_H=1 from list of tuneable configurations for `_chunk_cumsum_fwd_kernel` (#25197)
0	1	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py

[c85d75cf0] Corey Lowman 2025-09-23 Add `VLLM_NVTX_SCOPES_FOR_PROFILING=1` to enable `nvtx.annotate` scopes (#25501)
5	0	vllm/envs.py
17	3	vllm/v1/utils.py

[abad204be] kourosh hakhamaneshi 2025-09-23 [BugFix] Fix OOM in vLLM replicas by ensuring consistent NCCL memory accounting (#25359)
3	0	.buildkite/test-pipeline.yaml
174	0	tests/v1/worker/test_worker_memory_snapshot.py
14	7	vllm/v1/worker/gpu_worker.py

[7361ab379] Michael Goin 2025-09-23 Remove redundant mutates_args and dispatch_key for direct_register_custom_op (#25512)
0	3	vllm/attention/layer.py
0	1	vllm/compilation/collective_fusion.py
0	1	vllm/distributed/device_communicators/pynccl.py
0	7	vllm/distributed/parallel_state.py
0	2	vllm/lora/ops/triton_ops/lora_expand_op.py
0	2	vllm/lora/ops/triton_ops/lora_shrink_op.py
0	1	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
0	1	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
0	1	vllm/model_executor/layers/fused_moe/fused_moe.py
0	2	vllm/model_executor/layers/fused_moe/layer.py
0	7	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
0	4	vllm/model_executor/layers/layernorm.py
0	2	vllm/model_executor/layers/mamba/linear_attn.py
0	2	vllm/model_executor/layers/mamba/mamba_mixer.py
0	2	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	2	vllm/model_executor/layers/mamba/short_conv.py
0	3	vllm/model_executor/layers/quantization/deepgemm.py
0	3	vllm/model_executor/layers/quantization/gguf.py
0	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
0	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py
0	2	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
0	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
0	1	vllm/model_executor/layers/rotary_embedding/common.py
0	2	vllm/model_executor/layers/utils.py
0	3	vllm/model_executor/models/deepseek_v2.py
0	2	vllm/model_executor/models/plamo2.py
0	1	vllm/model_executor/models/qwen3_next.py
9	2	vllm/utils/__init__.py

[95bc60e4c] Andrew Xia 2025-09-23 [gpt-oss][bugfix] remove logic to require resp_ in ResponseAPI (#25428)
1	0	tests/entrypoints/openai/test_response_api_with_harmony.py
0	15	vllm/entrypoints/openai/serving_responses.py

[4f2954f72] Michael Goin 2025-09-23 Fix triton_reshape_and_cache_flash.py triton import (#25522)
1	2	vllm/attention/ops/triton_reshape_and_cache_flash.py

[eca7be907] rouchenzi 2025-09-23 Add VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE & VLLM_ENABLE_INDUCTOR_COORDINA… (#25493)
3	2	vllm/compilation/compiler_interface.py
15	0	vllm/envs.py

[969b4da3a] Thomas Parnell 2025-09-24 [V0 Deprecation] Remove placeholder attn (#25510)
10	27	tests/kernels/attention/test_attention_selector.py
0	314	vllm/attention/backends/placeholder_attn.py
0	3	vllm/attention/layer.py
0	9	vllm/attention/selector.py
0	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[4f8c4b890] Jialin Ouyang 2025-09-23 [Core] Use KVCacheBlock as much as possible instead of dict[block_id, KVCacheBlock] (#24830)
127	51	tests/v1/core/test_prefix_caching.py
8	10	tests/v1/core/test_single_type_kv_cache_manager.py
112	26	vllm/v1/core/block_pool.py

[ae002924e] Isotr0py 2025-09-24 [CI/Build] Fix and re-enable v1 PP test on CI (#25496)
0	3	tests/distributed/test_pipeline_parallel.py
0	7	vllm/model_executor/models/granite.py
0	7	vllm/model_executor/models/granitemoe.py
0	7	vllm/model_executor/models/granitemoeshared.py

[690f948e4] Gregory Shtrasberg 2025-09-23 [Bugfix] Fix for the import error from #24588 (#25481)
1	1	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py

[08275ec0a] Chauncey 2025-09-24 [Build] Update Xgrammar to 0.1.25 (#25467)
1	1	requirements/common.txt
6	2	vllm/v1/structured_output/backend_xgrammar.py

[c828d1bf9] Alec S 2025-09-23 [Bugfix] gpt-oss container tool output bug (#25485)
2	1	vllm/entrypoints/harmony_utils.py

[8b8a8afc8] Wentao Ye 2025-09-23 [CI] Fix Pre-commit Issue (#25497)
7	2	vllm/v1/worker/gpu_model_runner.py

[8bdd8b5c5] Ilya Markov 2025-09-23 Enable symmetric memory all reduce by default only enabling for TP (#25070)
2	0	.buildkite/test-pipeline.yaml
47	12	tests/distributed/test_symm_mem_allreduce.py
5	2	vllm/distributed/device_communicators/cuda_communicator.py
2	2	vllm/envs.py

[a8ffc4f0f] Michael Goin 2025-09-23 [Bugfix] Lower gpt-oss max cudagraph size to 992 to be compatible with FA3 (#25508)
5	5	vllm/model_executor/models/config.py

[d5944d514] jiahanc 2025-09-23 [Speculators][Speculative Decoding] Fix gpt-oss eagle3 accuracy issue (#25406)
4	0	tests/v1/spec_decode/test_eagle.py
1	0	vllm/config/model.py
5	0	vllm/model_executor/models/llama_eagle.py
5	0	vllm/model_executor/models/llama_eagle3.py
56	14	vllm/v1/spec_decode/eagle.py
8	3	vllm/v1/worker/gpu_model_runner.py

[24fab45d9] Michael Goin 2025-09-23 [Perf] Change default CUDAGraphMode from PIECEWISE to FULL_AND_PIECEWISE (#25444)
8	1	vllm/config/__init__.py
3	4	vllm/config/compilation.py
21	2	vllm/v1/worker/gpu_model_runner.py

[63400259d] ElizaWszola 2025-09-23 [Performance] Move apply_w8a8_block_fp8_linear to an op class (#24666)
2	2	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
2	2	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
3	2	tests/kernels/quantization/test_block_fp8.py
19	7	tests/kernels/quantization/test_fp8_quant_group.py
0	30	tests/model_executor/test_enabled_custom_ops.py
35	0	tests/quantization/test_compressed_tensors.py
17	0	vllm/config/__init__.py
8	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
26	11	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
5	5	vllm/model_executor/layers/quantization/deepgemm.py
28	12	vllm/model_executor/layers/quantization/fp8.py
16	8	vllm/model_executor/layers/quantization/input_quant_fp8.py
176	122	vllm/model_executor/layers/quantization/utils/fp8_utils.py
8	4	vllm/utils/deep_gemm.py

[8c1c81a3d] Amir Samani 2025-09-23 [core] add nccl symmetric memory for all reduce (#24532)
1	0	.buildkite/test-pipeline.yaml
24	2	benchmarks/kernels/benchmark_device_communicators.py
94	0	tests/distributed/test_nccl_symm_mem_allreduce.py
6	0	vllm/compilation/cuda_graph.py
26	1	vllm/distributed/device_communicators/all_reduce_utils.py
15	0	vllm/distributed/device_communicators/cuda_communicator.py
52	1	vllm/distributed/device_communicators/pynccl.py
186	0	vllm/distributed/device_communicators/pynccl_allocator.py
36	2	vllm/distributed/device_communicators/pynccl_wrapper.py
11	0	vllm/envs.py
32	0	vllm/utils/__init__.py
6	0	vllm/v1/worker/gpu_ubatch_wrapper.py

[a3a782801] Hashem Hashemi 2025-09-23 [ROCm] Add skinny gemm bias support for dtypes fp16,bf16,fp8 (#24988)
6	3	csrc/rocm/ops.h
139	42	csrc/rocm/skinny_gemms.cu
3	2	csrc/rocm/torch_bindings.cpp
62	18	tests/kernels/quantization/test_rocm_skinny_gemms.py
15	8	vllm/_custom_ops.py
5	3	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
3	3	vllm/model_executor/layers/utils.py

[5abb11790] Jee Jee Li 2025-09-24 [Core] Ensure LoRA linear respect the base_layer's tp_size and tp_rank (#25487)
3	2	vllm/lora/layers/base_linear.py
14	26	vllm/lora/layers/column_parallel_linear.py
0	1	vllm/lora/layers/replicated_linear.py
5	10	vllm/lora/layers/row_parallel_linear.py
2	2	vllm/lora/lora_weights.py

[867ecdd1c] Ekagra Ranjan 2025-09-23 [Spec Decode][CI] Add e2e test for `examples/spec_decode.py` and prevent breaking Acceptance Length (#24531)
2	0	.buildkite/test-pipeline.yaml
38	4	examples/offline_inference/spec_decode.py

[24e822274] Weida Hong 2025-09-24 [Misc] Reduce initialization time of auto_tune (#23682)
6	1	benchmarks/auto_tune/auto_tune.sh

[100b630a6] Burkhard Ringlein 2025-09-23 [V1][Kernel] Add triton implementation for `reshape_and_cache_flash` (#24503)
67	11	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
21	6	tests/kernels/attention/test_cache.py
176	0	vllm/attention/ops/triton_reshape_and_cache_flash.py
12	3	vllm/v1/attention/backends/triton_attn.py

[527821d19] Ming Yang 2025-09-23 Use macro guard CUDA functions for back compatibility in grouped_topk_kernel.cu (#25346)
14	6	csrc/moe/grouped_topk_kernels.cu

[846197f50] Wentao Ye 2025-09-23 [Log] Optimize kv cache memory log from Bytes to GiB (#25204)
6	4	vllm/v1/worker/gpu_worker.py

[2357480b1] rivos-shreeasish 2025-09-23 [BugFix] Fix UB in per_token_group_quant.cu (#24913)
3	3	csrc/quantization/fp8/per_token_group_quant.cu

[f11e3c516] bnellnm 2025-09-23 [Kernels] Support blocked fp8 quantization for compressed tensors MoE (#25219)
96	13	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
8	14	vllm/model_executor/layers/quantization/fp8.py
6	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
2	2	vllm/model_executor/warmup/deep_gemm_warmup.py

[875d6def9] Harry Mellor 2025-09-23 Add backward compatibility for `GuidedDecodingParams` (#25422)
23	1	tests/v1/entrypoints/llm/test_struct_output_generate.py
36	0	vllm/sampling_params.py

[cc1dc7ed6] Lucas Wilkinson 2025-09-23 [Core/DBO][2/N] Dual-Batch Overlap add DeepEP High Throughput support and Prefill support (#24845)
82	1	tests/v1/attention/test_attention_splitting.py
2	4	tests/v1/spec_decode/test_eagle.py
7	5	vllm/config/__init__.py
10	4	vllm/config/parallel.py
19	9	vllm/distributed/device_communicators/all2all.py
6	0	vllm/distributed/device_communicators/base_device_communicator.py
6	0	vllm/engine/arg_utils.py
11	0	vllm/envs.py
51	25	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
4	6	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
95	36	vllm/model_executor/layers/fused_moe/modular_kernel.py
41	3	vllm/v1/attention/backends/utils.py
3	6	vllm/v1/spec_decode/eagle.py
76	97	vllm/v1/worker/gpu_model_runner.py
81	6	vllm/v1/worker/gpu_ubatch_wrapper.py
57	21	vllm/v1/worker/ubatch_splitting.py
8	0	vllm/v1/worker/ubatch_utils.py
24	11	vllm/v1/worker/ubatching.py
21	4	vllm/v1/worker/utils.py

[a903669e1] Thomas Parnell 2025-09-23 [V1] Remove V0 code paths for Hybrid models (#25400)
19	36	tests/models/language/generation/test_hybrid.py
6	7	tests/models/registry.py
1	4	vllm/model_executor/layers/mamba/abstract.py
40	67	vllm/model_executor/layers/mamba/linear_attn.py
0	177	vllm/model_executor/layers/mamba/mamba2_metadata.py
50	99	vllm/model_executor/layers/mamba/mamba_mixer.py
56	116	vllm/model_executor/layers/mamba/mamba_mixer2.py
2	18	vllm/model_executor/layers/mamba/mamba_utils.py
0	3	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
3	18	vllm/model_executor/layers/mamba/short_conv.py
3	69	vllm/model_executor/models/bamba.py
0	137	vllm/model_executor/models/constant_size_cache.py
1	64	vllm/model_executor/models/falcon_h1.py
6	73	vllm/model_executor/models/granitemoehybrid.py
8	45	vllm/model_executor/models/jamba.py
0	7	vllm/model_executor/models/lfm2.py
6	39	vllm/model_executor/models/mamba.py
5	58	vllm/model_executor/models/mamba2.py
0	83	vllm/model_executor/models/mamba_cache.py
0	36	vllm/model_executor/models/minimax_cache.py
0	39	vllm/model_executor/models/minimax_text_01.py
3	69	vllm/model_executor/models/nemotron_h.py
0	731	vllm/model_executor/models/phi4flash.py
59	181	vllm/model_executor/models/plamo2.py
8	28	vllm/model_executor/models/qwen3_next.py
0	1	vllm/model_executor/models/registry.py
0	93	vllm/model_executor/models/zamba2.py
7	1	vllm/v1/attention/backends/gdn_attn.py
12	3	vllm/v1/attention/backends/mamba2_attn.py
13	1	vllm/v1/attention/backends/short_conv_attn.py
51	0	vllm/v1/attention/backends/utils.py

[2c58742df] Michael Goin 2025-09-23 [UX] Change kv-cache-memory log level to debug (#25479)
1	1	vllm/v1/worker/gpu_worker.py

[4c966e440] Fanli Lin 2025-09-23 [XPU] Fix MOE DP accuracy issue on XPU (#25465)
10	1	examples/offline_inference/data_parallel.py
19	0	vllm/distributed/device_communicators/xpu_communicator.py

[da5e7e432] Peter Pan 2025-09-23 [Docs] NixlConnector quickstart guide (#24249)
1	1	docs/features/disagg_prefill.md
159	0	docs/features/nixl_connector_usage.md
8	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh

[f05a4f0e3] Chauncey 2025-09-23 [P/D] Support NIXL connector to disconnect during a clean shutdown (#24423)
52	0	tests/v1/kv_connector/unit/test_nixl_connector.py
28	7	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[61d1b3556] Joel 2025-09-23 [BugFix] Register expert_map as named buffer for wake_up and sleep (#25458)
7	2	vllm/model_executor/layers/fused_moe/layer.py

[b6a136b58] Isotr0py 2025-09-23 [CI/Build] Fix disabled v1 attention backend selection test (#25471)
1	2	tests/kernels/attention/test_attention_selector.py

[0d9fe260d] vllmellm 2025-09-23 [docs] Benchmark Serving Incorrect Arg (#25474)
2	2	docs/contributing/benchmarks.md

[273690a50] Jee Jee Li 2025-09-23 [Core] Optimize LoRA weight loading (#25403)
14	12	tests/lora/test_layers.py
6	6	tests/lora/test_lora_manager.py
4	4	tests/lora/utils.py
5	5	vllm/lora/layers/base_linear.py
33	34	vllm/lora/layers/column_parallel_linear.py
4	4	vllm/lora/layers/logits_processor.py
2	2	vllm/lora/layers/row_parallel_linear.py
6	4	vllm/lora/layers/vocal_parallel_embedding.py
2	2	vllm/lora/lora_weights.py
7	10	vllm/lora/models.py

[231c2c63e] Isotr0py 2025-09-23 [Bugfix] Fix idefics3 `tie_word_embeddings` (#25454)
1	1	vllm/model_executor/models/idefics3.py

[4322c553a] Andreas Hartel 2025-09-23 [Test]: Hermes tool parser stream output error in Qwen3 case (#25203)
203	6	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py

[babad6e5d] Cyrus Leung 2025-09-23 [Misc] Move DP for ViT code inside model executor dir (#25459)
423	1	tests/models/test_vision.py
1	425	tests/multimodal/test_utils.py
1	2	vllm/model_executor/models/glm4_1v.py
2	1	vllm/model_executor/models/idefics2_vision_model.py
2	1	vllm/model_executor/models/intern_vit.py
1	1	vllm/model_executor/models/kimi_vl.py
1	1	vllm/model_executor/models/mllama4.py
1	2	vllm/model_executor/models/qwen2_5_vl.py
1	2	vllm/model_executor/models/qwen2_vl.py
1	5	vllm/model_executor/models/qwen3_vl.py
1	1	vllm/model_executor/models/step3_vl.py
280	1	vllm/model_executor/models/vision.py
6	287	vllm/multimodal/utils.py

[9383cd6f1] Zhikaiiii 2025-09-23 [Frontend] Add a new xml-based tool parser for qwen3-coder (#25028)
9	0	docs/features/tool_calling.md
90	22	tests/tool_use/test_qwen3coder_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
1137	0	vllm/entrypoints/openai/tool_parsers/qwen3xml_tool_parser.py

[ba8d2165b] Ming Yang 2025-09-23 Handle triton kernel import exception (#25319)
8	3	vllm/model_executor/layers/fused_moe/config.py

[c98be0a23] Cyrus Leung 2025-09-23 [Model] Enable DP for ViT in Qwen2-VL (#25445)
59	19	vllm/model_executor/models/qwen2_vl.py

[5774b0a1d] Chendi.Xue 2025-09-22 [NIXL][OOT platform] support nixl_connector with oot platform and other nixl_backend (#25121)
6	0	docs/features/disagg_prefill.md
1	1	docs/serving/expert_parallel_deployment.md
51	1	tests/v1/kv_connector/unit/test_nixl_connector.py
26	7	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
15	0	vllm/platforms/interface.py

[e8db44f88] Varun Sundar Rabindranath 2025-09-23 [DP/EP][GPTOSS] Use triton matmul-ogs kernels for GPTOSS DP/EP (#24588)
21	1	vllm/model_executor/layers/fused_moe/config.py
18	0	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
140	34	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
60	22	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
34	17	vllm/model_executor/layers/quantization/mxfp4.py

[fafbe11af] Michael Yao 2025-09-23 [Docs] Fix griffe warnings in vllm/lora/ops (#25369)
3	3	vllm/lora/ops/triton_ops/lora_kernel_metadata.py
0	1	vllm/lora/ops/xla_ops/lora_ops.py

[78237e43b] Michael Goin 2025-09-22 [Bugfix] Remove contiguous output req for context parallel MLA (#25414)
0	1	vllm/attention/ops/common.py

[eea178398] Lucia Fang 2025-09-22 [benchmarks]allow skip ready check for bench serve (#25420)
17	12	vllm/benchmarks/serve.py

[f225ea7dd] Kunshang Ji 2025-09-23 [XPU] Fix `compile_size` is `None` case. (#25433)
2	0	vllm/platforms/xpu.py

[fc97733da] JJJYmmm 2025-09-23 [feat] Support MRoPE +  YaRN (#25384)
17	5	vllm/model_executor/layers/rotary_embedding/__init__.py
31	0	vllm/model_executor/layers/rotary_embedding/mrope.py

[4741239db] Wentao Ye 2025-09-22 [Bug] Fix Long Context OOM Issue (#25290)
1	1	vllm/v1/attention/backends/mla/common.py

[c625f9043] Isotr0py 2025-09-23 [V0 deprecation] Remove `_set_default_args_v0` function (#25409)
11	72	vllm/engine/arg_utils.py

[6fa78d8f2] Isotr0py 2025-09-23 [V0 deprecation] Remove platform v1 controling interface (#25410)
0	5	tests/v1/test_async_llm_dp.py
0	27	vllm/engine/arg_utils.py
0	17	vllm/platforms/cpu.py
0	4	vllm/platforms/cuda.py
0	14	vllm/platforms/interface.py
0	5	vllm/platforms/rocm.py
0	5	vllm/platforms/tpu.py
0	4	vllm/platforms/xpu.py

[9949aa2ef] Wentao Ye 2025-09-22 [Perf] Apply torch.compile for `per_block_cast_to_fp8` (#24611)
2	2	vllm/utils/deep_gemm.py

[0b7bed9c3] Alexander Matveev 2025-09-22 [Performance] Remove input pads in cutlass_mla and optimize v_proj output handling (#25184)
39	6	vllm/v1/attention/backends/mla/common.py
16	14	vllm/v1/attention/backends/mla/cutlass_mla.py

[ac0048c0a] Matthew Bonanni 2025-09-22 [BugFix] [DP/EP] Fix slow execution when BS <= DP (#25407)
4	3	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py

[090197034] Nicolò Lucchesi 2025-09-23 [Bugfix] Fix missing `clear_connector_metadata` (#25397)
59	0	tests/v1/kv_connector/unit/test_kv_connector_lifecyle.py
1	0	vllm/v1/worker/kv_connector_model_runner_mixin.py

[f31ff8746] Russell Bryant 2025-09-22 [Core] Drop overly aggressive whisper assertion (#25408)
0	4	vllm/v1/core/sched/scheduler.py

[d588cd240] Luka Govedič 2025-09-22 [Bugfix] fix custom op test (#25429)
12	10	tests/model_executor/test_enabled_custom_ops.py

[45d7d852d] Alec S 2025-09-22 [Frontend] Responses API MCP tools for built in tools and to pass through headers (#24628)
106	0	tests/entrypoints/openai/test_response_api_mcp_tools.py
8	1	tests/entrypoints/openai/test_response_api_with_harmony.py
216	0	tests/test_envs.py
30	7	vllm/entrypoints/context.py
3	1	vllm/entrypoints/harmony_utils.py
19	6	vllm/entrypoints/openai/serving_responses.py
20	9	vllm/entrypoints/tool_server.py
61	5	vllm/envs.py

[8bed17910] Johnny Yang 2025-09-22 [TPU] update torch_xla dependency for PyPI compatibility (#25278)
1	11	requirements/tpu.txt

[f552d5e57] Cyrus Leung 2025-09-23 [CI/Build] Skip Qwen3-VL initialization tests until models are actually released (#25394)
5	3	tests/models/registry.py

[8db293928] Or Ozeri 2025-09-22 [KV offload][5/N] Add `CPUOffloadingSpec` (#24251)
6	0	docs/features/disagg_prefill.md
0	0	tests/v1/kv_offload/{test_cpu.py => test_cpu_manager.py}
62	0	tests/v1/kv_offload/test_cpu_offloading.py
75	0	vllm/v1/kv_offload/cpu.py
3	0	vllm/v1/kv_offload/factory.py

[d5e0fca26] Luka Govedič 2025-09-22 [torch.compile] Cleanup compilation tests and custom passes, add debug utils, fix DCE bug (#23091), fix test (#24376), and prep for custom op matching (#24604) (#24542)
27	1	tests/compile/backend.py
2	0	tests/compile/test_async_tp.py
9	1	tests/compile/test_config.py
6	4	tests/compile/test_functionalization.py
10	7	tests/compile/test_fusion.py
5	1	tests/compile/test_fusion_all_reduce.py
13	7	tests/compile/test_fusion_attn.py
14	7	tests/compile/test_sequence_parallelism.py
12	1	tests/compile/test_silu_mul_quant_fusion.py
7	11	vllm/compilation/activation_quant_fusion.py
14	15	vllm/compilation/collective_fusion.py
3	6	vllm/compilation/fix_functionalization.py
31	248	vllm/compilation/fusion.py
7	15	vllm/compilation/fusion_attn.py
0	109	vllm/compilation/multi_output_match.py
1	4	vllm/compilation/noop_elimination.py
43	6	vllm/compilation/pass_manager.py
20	0	vllm/compilation/post_cleanup.py
6	12	vllm/compilation/sequence_parallelism.py
108	2	vllm/compilation/vllm_inductor_pass.py
3	4	vllm/config/__init__.py
46	2	vllm/config/compilation.py
6	0	vllm/envs.py
14	1	vllm/utils/__init__.py

[8d0ee5a56] Simon Mo 2025-09-22 [misc] Remove RFC review hours reference (#25416)
0	4	.github/ISSUE_TEMPLATE/750-RFC.yml

[922979bfc] Lucia Fang 2025-09-22 [DP] support torchrun external launcher with Data Parallelism (#24899)
10	2	.buildkite/test-pipeline.yaml
81	0	examples/offline_inference/torchrun_dp_example.py
81	0	tests/distributed/test_torchrun_example_moe.py
12	1	vllm/config/parallel.py
3	1	vllm/distributed/parallel_state.py
15	3	vllm/v1/engine/llm_engine.py

[239ef0c1a] Michael Goin 2025-09-22 [CI Failure] Fix fp8 kv cache on <SM90 (#25396)
6	2	vllm/platforms/cuda.py

[1d7f95b85] ElizaWszola 2025-09-22 [Compiler] Disable Inductor standalone compile by default (#25391)
3	2	vllm/envs.py

[cfbee3d0e] Daisy-Ma-coder 2025-09-22 [CLI env var] Add VLLM_FLASH_ATTN_MAX_NUM_SPLITS_FOR_CUDA_GRAPH in env variables (#25274)
9	2	tests/compile/piecewise/test_full_cudagraph.py
9	2	tests/v1/cudagraph/test_cudagraph_mode.py
8	0	vllm/envs.py
3	4	vllm/v1/attention/backends/flash_attn.py
3	5	vllm/v1/attention/backends/mla/flashattn_mla.py

[06a41334c] Bowen Wang 2025-09-22 [EPLB] Reduce EPLB Inference Overhead (#24573)
73	0	vllm/model_executor/layers/fused_moe/fused_moe.py
19	50	vllm/model_executor/layers/fused_moe/layer.py

[175811e3b] Burkhard Ringlein 2025-09-22 [V1][Attention] Split triton_attn in triton-only and rocm specific backends  (#24648)
1	0	vllm/engine/arg_utils.py
1	0	vllm/platforms/interface.py
10	0	vllm/platforms/rocm.py
426	0	vllm/v1/attention/backends/rocm_attn.py
45	124	vllm/v1/attention/backends/triton_attn.py

[c10101a3e] Csrayz 2025-09-22 [Bugfix] Fix several issues with p2p xPyD in GET type (#23993)
4	1	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
17	9	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py

[ac243886b] Sara-KS 2025-09-22 [Kernel] MI-300X triton moe configs (#23445)
200	0	vllm/model_executor/layers/fused_moe/configs/E=62,N=128,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=62,N=256,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=62,N=512,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=72,N=192,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=72,N=384,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=72,N=768,device_name=AMD_Instinct_MI300X.json

[3d2c56b7a] Harry Mellor 2025-09-22 Make `mypy` behave like a proper pre-commit hook (#25313)
1	0	.github/CODEOWNERS
14	20	.pre-commit-config.yaml
0	21	pyproject.toml
0	35	tools/mypy.sh
140	0	tools/pre_commit/mypy.py
2	2	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/renderer.py
5	4	vllm/utils/__init__.py
3	4	vllm/utils/tensor_schema.py

[64c824cd7] Harry Mellor 2025-09-22 Make pickle import check fast (#25379)
2	3	.pre-commit-config.yaml
16	63	tools/{ => pre_commit}/check_pickle_imports.py

[417a164af] Cyrus Leung 2025-09-22 [Misc] Remove unused encoder-decoder error strings (#25374)
0	5	vllm/attention/backends/utils.py
0	58	vllm/utils/__init__.py

[b6f01bd9a] Yizhou 2025-09-22 refactor: abstract graph mode support into platform interface (#25161)
1	1	vllm/config/__init__.py
4	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py
7	6	vllm/platforms/xpu.py

[4cf71cc88] Nicolò Lucchesi 2025-09-22 [TPU] Deprecate `xm.mark_step` in favor of ``torch_xla.sync`  (#25254)
2	1	tests/tpu/test_moe_pallas.py
6	5	tests/v1/tpu/test_topk_topp_sampler.py
2	2	vllm/lora/punica_wrapper/punica_tpu.py
4	5	vllm/model_executor/model_loader/default_loader.py
17	16	vllm/v1/worker/tpu_model_runner.py

[a66d13138] Nicolò Lucchesi 2025-09-22 [TPU][Bugfix][CI] Fix broken tests/build dependency (#25255)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[21467f9a1] Eldar Kurtić 2025-09-22 Enable Eagle3 speculative decoding for GPT-OSS model (#25246)
1	1	vllm/config/speculative.py
17	2	vllm/model_executor/models/gpt_oss.py
23	9	vllm/v1/spec_decode/eagle.py

[f92d95263] Cyrus Leung 2025-09-22 [V0 Deprecation] Remove `MultiModalPlaceholderMap` (#25366)
0	2	tests/kernels/utils.py
0	10	vllm/attention/backends/abstract.py
1	22	vllm/attention/backends/placeholder_attn.py
0	18	vllm/attention/backends/utils.py
0	2	vllm/multimodal/__init__.py
1	73	vllm/multimodal/base.py
0	1	vllm/v1/attention/backends/cpu_attn.py

[6d0b827cb] Cyrus Leung 2025-09-22 [V0 Deprecation] Remove V0-only methods in multi-modal registry (#25362)
0	1	tests/models/multimodal/generation/test_qwen2_vl.py
1	31	vllm/multimodal/registry.py

[0eecb3166] WeiQing Chen 2025-09-22 [Bugfix] Fix hermes tool parser handling of non-string argument types (#22002)
131	0	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
35	7	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py

[793be8d05] WeiQing Chen 2025-09-22 [Docs] GSM8K Accuracy Evaluation doc update (#25360)
1	1	tests/evals/gsm8k/README.md

[7b57a433d] Roger Wang 2025-09-21 [Model] Support Dots OCR (#24645)
1	0	docs/models/supported_models.md
18	0	examples/offline_inference/vision_language.py
2	0	tests/models/registry.py
824	0	vllm/model_executor/models/dots_ocr.py
1	0	vllm/model_executor/models/registry.py
2	0	vllm/transformers_utils/configs/__init__.py
69	0	vllm/transformers_utils/configs/dotsocr.py

[5aeb92545] Deboleina 2025-09-21 Multimodal - audio tests (#25285)
140	0	tests/multimodal/test_audio.py

[04d375232] Yang Liu 2025-09-21 [Bugfix][V0 Deprecation][CI] use async mock and await for async method (#25325)
20	9	tests/entrypoints/openai/test_lora_resolvers.py

[bc6e542d9] Woosuk Kwon 2025-09-21 Remove V0 attention backends (#25351)
0	1	examples/offline_inference/qwen_1m.py
3	2	tests/compile/test_fusion_attn.py
3	3	tests/kernels/attention/test_attention.py
1	0	tests/kernels/attention/test_attention_selector.py
3	3	tests/kernels/attention/test_prefix_prefill.py
1	0	tests/kernels/attention/test_rocm_attention_selector.py
56	10	tests/kernels/utils.py
2	3	tests/models/test_initialization.py
0	931	vllm/attention/backends/differential_flash_attn.py
0	1495	vllm/attention/backends/dual_chunk_flash_attn.py
0	929	vllm/attention/backends/flash_attn.py
0	227	vllm/attention/backends/flashmla.py
0	0	vllm/attention/backends/mla/__init__.py
0	1305	vllm/attention/backends/mla/common.py
0	407	vllm/attention/backends/rocm_aiter_mla.py
0	953	vllm/attention/backends/rocm_flash_attn.py
0	111	vllm/attention/backends/triton_mla.py
6	8	vllm/attention/backends/utils.py
0	805	vllm/attention/backends/xformers.py
1	6	vllm/config/model.py
1	1	vllm/distributed/kv_transfer/kv_connector/utils.py
2	13	vllm/engine/arg_utils.py
0	1	vllm/envs.py
12	7	vllm/model_executor/layers/mamba/mamba2_metadata.py
2	1	vllm/model_executor/models/deepseek_v2.py
26	113	vllm/platforms/cuda.py
23	38	vllm/platforms/rocm.py
0	2	vllm/utils/__init__.py

[af7dfb0d1] Isotr0py 2025-09-22 [Perf] Further optimization for Qwen3-VL `fast_pos_embed_interpolate` (#25347)
32	18	vllm/model_executor/models/qwen3_vl.py

[1c3ffdbec] Woosuk Kwon 2025-09-21 [V0 Deprecation] Remove V0 sampling metadata (#25345)
3	5	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
3	5	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py
0	2	vllm/model_executor/__init__.py
0	2	vllm/model_executor/layers/logits_processor.py
1	4	vllm/model_executor/models/apertus.py
3	4	vllm/model_executor/models/arcee.py
1	4	vllm/model_executor/models/arctic.py
2	5	vllm/model_executor/models/aria.py
1	4	vllm/model_executor/models/aya_vision.py
1	4	vllm/model_executor/models/baichuan.py
1	4	vllm/model_executor/models/bailing_moe.py
1	4	vllm/model_executor/models/bamba.py
1	4	vllm/model_executor/models/blip2.py
1	4	vllm/model_executor/models/bloom.py
1	4	vllm/model_executor/models/chameleon.py
1	4	vllm/model_executor/models/chatglm.py
1	4	vllm/model_executor/models/cohere2_vision.py
2	4	vllm/model_executor/models/commandr.py
1	4	vllm/model_executor/models/dbrx.py
1	4	vllm/model_executor/models/deepseek.py
1	4	vllm/model_executor/models/deepseek_eagle.py
2	7	vllm/model_executor/models/deepseek_mtp.py
1	4	vllm/model_executor/models/deepseek_v2.py
1	4	vllm/model_executor/models/deepseek_vl2.py
1	4	vllm/model_executor/models/dots1.py
1	4	vllm/model_executor/models/ernie45_moe.py
1	4	vllm/model_executor/models/ernie45_vl.py
1	4	vllm/model_executor/models/ernie45_vl_moe.py
2	6	vllm/model_executor/models/ernie_mtp.py
1	4	vllm/model_executor/models/exaone.py
1	4	vllm/model_executor/models/exaone4.py
1	4	vllm/model_executor/models/falcon.py
1	4	vllm/model_executor/models/falcon_h1.py
1	3	vllm/model_executor/models/fuyu.py
1	4	vllm/model_executor/models/gemma.py
1	4	vllm/model_executor/models/gemma2.py
1	4	vllm/model_executor/models/gemma3.py
1	4	vllm/model_executor/models/gemma3_mm.py
1	4	vllm/model_executor/models/gemma3n.py
1	4	vllm/model_executor/models/gemma3n_mm.py
1	4	vllm/model_executor/models/glm4.py
1	4	vllm/model_executor/models/glm4_1v.py
1	4	vllm/model_executor/models/glm4_moe.py
2	7	vllm/model_executor/models/glm4_moe_mtp.py
1	4	vllm/model_executor/models/gpt2.py
1	4	vllm/model_executor/models/gpt_bigcode.py
1	3	vllm/model_executor/models/gpt_j.py
1	4	vllm/model_executor/models/gpt_neox.py
2	5	vllm/model_executor/models/gpt_oss.py
3	6	vllm/model_executor/models/granite.py
1	6	vllm/model_executor/models/granite_speech.py
3	6	vllm/model_executor/models/granitemoe.py
1	4	vllm/model_executor/models/granitemoehybrid.py
3	6	vllm/model_executor/models/granitemoeshared.py
1	4	vllm/model_executor/models/grok1.py
1	4	vllm/model_executor/models/hunyuan_v1.py
1	4	vllm/model_executor/models/hyperclovax_vision.py
2	5	vllm/model_executor/models/idefics3.py
0	3	vllm/model_executor/models/interfaces_base.py
1	4	vllm/model_executor/models/internlm2.py
1	4	vllm/model_executor/models/interns1.py
1	4	vllm/model_executor/models/internvl.py
1	4	vllm/model_executor/models/jais.py
1	4	vllm/model_executor/models/jamba.py
1	4	vllm/model_executor/models/keye.py
1	4	vllm/model_executor/models/kimi_vl.py
2	5	vllm/model_executor/models/lfm2.py
1	4	vllm/model_executor/models/llama.py
1	4	vllm/model_executor/models/llama_eagle3.py
1	4	vllm/model_executor/models/llava.py
1	4	vllm/model_executor/models/llava_next.py
1	4	vllm/model_executor/models/llava_next_video.py
1	4	vllm/model_executor/models/llava_onevision.py
2	5	vllm/model_executor/models/mamba.py
2	5	vllm/model_executor/models/mamba2.py
1	2	vllm/model_executor/models/medusa.py
1	3	vllm/model_executor/models/midashenglm.py
1	4	vllm/model_executor/models/mimo.py
2	6	vllm/model_executor/models/mimo_mtp.py
1	4	vllm/model_executor/models/minicpm.py
1	4	vllm/model_executor/models/minicpm_eagle.py
1	3	vllm/model_executor/models/minicpmv.py
2	5	vllm/model_executor/models/minimax_text_01.py
1	4	vllm/model_executor/models/minimax_vl_01.py
1	4	vllm/model_executor/models/mistral3.py
1	4	vllm/model_executor/models/mixtral.py
1	4	vllm/model_executor/models/mllama4.py
2	5	vllm/model_executor/models/molmo.py
1	4	vllm/model_executor/models/mpt.py
1	4	vllm/model_executor/models/nano_nemotron_vl.py
1	4	vllm/model_executor/models/nemotron.py
1	4	vllm/model_executor/models/nemotron_h.py
1	4	vllm/model_executor/models/nemotron_nas.py
1	4	vllm/model_executor/models/nemotron_vl.py
1	4	vllm/model_executor/models/olmo.py
1	4	vllm/model_executor/models/olmo2.py
2	5	vllm/model_executor/models/olmoe.py
1	4	vllm/model_executor/models/opt.py
1	4	vllm/model_executor/models/orion.py
1	3	vllm/model_executor/models/ovis.py
1	3	vllm/model_executor/models/ovis2_5.py
1	4	vllm/model_executor/models/paligemma.py
1	4	vllm/model_executor/models/persimmon.py
1	3	vllm/model_executor/models/phi.py
1	4	vllm/model_executor/models/phi3v.py
1	4	vllm/model_executor/models/phi4_multimodal.py
0	3	vllm/model_executor/models/phi4flash.py
1	4	vllm/model_executor/models/phi4mm.py
2	5	vllm/model_executor/models/phimoe.py
1	4	vllm/model_executor/models/pixtral.py
1	4	vllm/model_executor/models/plamo2.py
1	4	vllm/model_executor/models/qwen.py
1	4	vllm/model_executor/models/qwen2.py
1	4	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	4	vllm/model_executor/models/qwen2_5_vl.py
1	4	vllm/model_executor/models/qwen2_audio.py
1	4	vllm/model_executor/models/qwen2_moe.py
1	4	vllm/model_executor/models/qwen2_vl.py
1	4	vllm/model_executor/models/qwen3.py
1	4	vllm/model_executor/models/qwen3_moe.py
1	4	vllm/model_executor/models/qwen3_next.py
1	4	vllm/model_executor/models/qwen3_next_mtp.py
1	4	vllm/model_executor/models/qwen3_vl.py
1	4	vllm/model_executor/models/seed_oss.py
1	4	vllm/model_executor/models/skyworkr1v.py
2	5	vllm/model_executor/models/solar.py
1	4	vllm/model_executor/models/stablelm.py
1	4	vllm/model_executor/models/starcoder2.py
2	5	vllm/model_executor/models/step3_text.py
1	4	vllm/model_executor/models/step3_vl.py
1	4	vllm/model_executor/models/tarsier.py
1	4	vllm/model_executor/models/transformers.py
2	5	vllm/model_executor/models/ultravox.py
1	4	vllm/model_executor/models/voxtral.py
2	5	vllm/model_executor/models/whisper.py
1	4	vllm/model_executor/models/zamba2.py
0	7	vllm/model_executor/sampling_metadata.py
3	6	vllm/v1/spec_decode/eagle.py
1	1	vllm/v1/spec_decode/medusa.py
4	5	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/tpu_model_runner.py

[c438b2951] Rahul Tuli 2025-09-21 feat: Enable engine-level arguments with speculators models (#25250)
34	20	tests/speculative_decoding/speculators/test_eagle3.py
1	11	vllm/config/model.py
12	23	vllm/engine/arg_utils.py
38	8	vllm/transformers_utils/config.py
36	16	vllm/transformers_utils/configs/speculators/base.py

[0ff8ebb2d] Woosuk Kwon 2025-09-21 [V0 Deprecation] Remove async_output_proc, preemption mode, delay factor (#25334)
5	41	tests/detokenizer/test_stop_strings.py
0	10	tests/v1/engine/test_processor_multi_modal_uuids.py
0	18	tests/v1/test_oracle.py
0	4	vllm/config/__init__.py
6	42	vllm/config/model.py
1	14	vllm/config/scheduler.py
0	34	vllm/engine/arg_utils.py
0	4	vllm/entrypoints/llm.py
0	4	vllm/executor/uniproc_executor.py
0	4	vllm/platforms/cpu.py
0	10	vllm/platforms/cuda.py
0	7	vllm/platforms/interface.py
0	10	vllm/platforms/rocm.py
0	4	vllm/platforms/tpu.py
0	4	vllm/platforms/xpu.py

[26e673fe9] Woosuk Kwon 2025-09-21 [V0 Deprecation] Remove V0 Sequence class & Sampler (#25332)
1	1	tests/conftest.py
1	1	tests/models/multimodal/generation/test_granite_speech.py
1	1	tests/models/multimodal/generation/test_phi4mm.py
1	1	tests/models/multimodal/generation/test_pixtral.py
1	1	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	1	tests/models/multimodal/generation/vlm_utils/types.py
1	1	tests/models/utils.py
1	139	tests/tokenization/test_detokenize.py
1	1	tests/tool_use/test_jamba_tool_parser.py
1	1	tests/tool_use/test_qwen3coder_tool_parser.py
1	1	tests/tool_use/test_seed_oss_tool_parser.py
1	1	tests/tool_use/test_xlam_tool_parser.py
1	1	tests/v1/engine/test_output_processor.py
1	1	vllm/executor/executor_base.py
1	1	vllm/executor/ray_distributed_executor.py
1	12	vllm/inputs/__init__.py
1	66	vllm/inputs/registry.py
1	3	vllm/model_executor/__init__.py
0	91	vllm/model_executor/layers/logits_processor.py
0	1198	vllm/model_executor/layers/sampler.py
4	56	vllm/model_executor/models/medusa.py
40	40	vllm/model_executor/models/mlp_speculator.py
1	5	vllm/model_executor/models/phi4flash.py
2	592	vllm/model_executor/sampling_metadata.py
4	1318	vllm/sequence.py
0	162	vllm/transformers_utils/detokenizer.py
1	1	vllm/worker/worker_base.py

[65a5910ce] Cyrus Leung 2025-09-21 [Optimization] Cache chat template result when processor fails to be loaded (#25341)
49	22	vllm/entrypoints/chat_utils.py

[9aea7373f] Simon Danielsson 2025-09-21 [Bugfix] Typos in error message for missing model config file (#25339)
4	4	vllm/transformers_utils/config.py

[30d08911f] Roger Wang 2025-09-21 [MM][Perf] Minor Optimization on Qwen3-VL `fast_pos_embed_interpolate` (#25337)
60	75	vllm/model_executor/models/qwen3_vl.py

[cf56cf78b] Isotr0py 2025-09-21 [V1] Add sliding window support to Flex Attention backend (#24089)
157	51	tests/v1/attention/test_attention_backends.py
72	18	vllm/v1/attention/backends/flex_attention.py

[7ed82d197] Woosuk Kwon 2025-09-20 [V0 Deprecation] Remove V0 MP executor (#25329)
0	244	vllm/executor/mp_distributed_executor.py
0	279	vllm/executor/multiproc_worker_utils.py
33	7	vllm/v1/executor/multiproc_executor.py

[12dbd834c] Woosuk Kwon 2025-09-20 [V0 Deprecation] Remove from_seq_group methods (#25330)
1	121	vllm/multimodal/base.py
1	194	vllm/outputs.py

[035fd2bd2] Wenlong Wang 2025-09-20 [Multi Modal][Performance] Fused Q,K's apply_rope in more models (#25005)
9	6	vllm/model_executor/models/ernie45_vl.py
10	7	vllm/model_executor/models/glm4_1v.py
10	6	vllm/model_executor/models/qwen2_vl.py

[1cd885bd5] Woosuk Kwon 2025-09-20 [V0 Deprecation] Remove V0 model runner base & simplify worker base (#25328)
5	10	vllm/attention/backends/abstract.py
2	6	vllm/attention/backends/utils.py
0	307	vllm/worker/model_runner_base.py
4	383	vllm/worker/worker_base.py

[62b38dc83] Huamin Li 2025-09-20 [Doc] improve test-pipeline.yaml documentation (#25305)
18	14	.buildkite/test-pipeline.yaml

[c99db8c8d] Woosuk Kwon 2025-09-20 [V0 Deprecation] Remove V0 core (#25321)
0	3	.buildkite/test-pipeline.yaml
0	2	.github/CODEOWNERS
0	2	pyproject.toml
4	8	vllm/attention/backends/differential_flash_attn.py
3	7	vllm/attention/backends/dual_chunk_flash_attn.py
4	8	vllm/attention/backends/flash_attn.py
4	9	vllm/attention/backends/mla/common.py
3	8	vllm/attention/backends/placeholder_attn.py
2	5	vllm/attention/backends/rocm_aiter_mla.py
2	7	vllm/attention/backends/utils.py
0	0	vllm/core/__init__.py
0	0	vllm/core/block/__init__.py
0	399	vllm/core/block/block_table.py
0	371	vllm/core/block/common.py
0	439	vllm/core/block/cpu_gpu_block_allocator.py
0	319	vllm/core/block/interfaces.py
0	466	vllm/core/block/naive_block.py
0	1135	vllm/core/block/prefix_caching_block.py
0	28	vllm/core/block/utils.py
0	523	vllm/core/block_manager.py
0	157	vllm/core/evictor.py
0	139	vllm/core/interfaces.py
0	103	vllm/core/placeholder_block_space_manager.py
0	2028	vllm/core/scheduler.py
1	7	vllm/engine/protocol.py
1	5	vllm/v1/engine/async_llm.py
0	145	vllm/worker/cache_engine.py
0	2031	vllm/worker/model_runner.py
0	666	vllm/worker/worker.py

[72dd1595b] Woosuk Kwon 2025-09-20 [CI] Skip tests failing on main (#25326)
1	0	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
4	1	tests/models/quantization/test_fp8.py
1	0	tests/models/test_oot_registration.py
4	1	tests/quantization/test_compressed_tensors.py

[572ddf83c] Woosuk Kwon 2025-09-20 [Chore] Remove unused sampler in models (#25324)
0	3	tests/lora/conftest.py
0	10	vllm/model_executor/models/ernie_mtp.py
0	10	vllm/model_executor/models/plamo2.py
0	10	vllm/model_executor/models/step3_text.py
0	16	vllm/model_executor/models/step3_vl.py

[86647d1cd] Woosuk Kwon 2025-09-20 [V0 Deprecation] Remove V0 Output Processor (#25320)
0	39	tests/build_cython.py
0	0	vllm/engine/output_processor/__init__.py
0	59	vllm/engine/output_processor/interfaces.py
0	145	vllm/engine/output_processor/single_step.py
0	139	vllm/engine/output_processor/stop_checker.py
40	2	vllm/v1/engine/detokenizer.py

[52c2a8d4a] Woosuk Kwon 2025-09-20 [V0 Deprecation] Remove LLMEngine (#25033)
0	4	.buildkite/scripts/hardware_ci/run-amd-test.sh
3	5	.buildkite/test-pipeline.yaml
0	1	.github/CODEOWNERS
0	510	examples/offline_inference/profiling.py
1	15	tests/basic_correctness/test_basic_correctness.py
3	2	tests/basic_correctness/test_cumem.py
1	2	tests/compile/test_fusion_attn.py
0	20	tests/conftest.py
0	6	tests/entrypoints/llm/test_generate.py
0	8	tests/entrypoints/llm/test_prompt_validation.py
1	1	tests/entrypoints/openai/test_metrics.py
30	61	tests/kernels/attention/test_attention_selector.py
1	1	tests/lora/test_lora_functions.py
2	4	tests/models/language/generation/test_common.py
1	121	tests/models/language/generation/test_hybrid.py
1	2	tests/models/language/pooling/test_reward.py
0	12	tests/models/quantization/test_fp8.py
4	9	tests/models/test_initialization.py
1	0	tests/models/test_oot_registration.py
0	9	tests/plugins_tests/test_platform_plugins.py
4	33	tests/plugins_tests/test_scheduler_plugins.py
0	7	tests/samplers/test_beam_search.py
0	7	tests/samplers/test_ignore_eos.py
0	6	tests/samplers/test_ranks.py
0	55	tests/tokenization/test_detokenize.py
0	8	vllm/engine/arg_utils.py
2	1831	vllm/engine/llm_engine.py
2	9	vllm/entrypoints/llm.py
9	15	vllm/model_executor/model_loader/tensorizer.py

[367a480bd] Michael Yao 2025-09-21 [Docs] Fix warnings in vllm/profiler and vllm/transformers_utils (#25220)
1	0	mkdocs.yaml
2	2	vllm/profiler/layerwise_profile.py
1	2	vllm/transformers_utils/configs/jais.py

[bef180f00] Cyrus Leung 2025-09-21 [V0 Deprecation] Enable the remaining multimodal tests in V1 (#25307)
56	20	tests/conftest.py
79	79	tests/models/multimodal/generation/test_common.py
2	47	tests/models/multimodal/generation/test_pixtral.py
3	6	tests/models/multimodal/generation/test_qwen2_vl.py
11	15	tests/models/multimodal/pooling/test_prithvi_mae.py
20	18	tests/models/quantization/test_awq.py
13	15	tests/models/quantization/test_bitsandbytes.py
11	14	tests/models/test_terratorch.py

[d88918e4c] lirong 2025-09-20 [Core] Enable sharded state loader for V1 engine and enhance test coverage (#25308)
12	8	tests/test_sharded_state_loader.py
0	6	vllm/engine/arg_utils.py

[3c713a971] Isotr0py 2025-09-20 [Model] Cleanup InternViT's data parallel implementation  (#25306)
37	121	vllm/model_executor/models/intern_vit.py

[bf8b26cad] Manoel Marques 2025-09-20  Generate _ModelInfo properties file when loading to improve loading speed (#23558)
2	0	vllm/logging_utils/__init__.py
32	0	vllm/logging_utils/log_time.py
44	0	vllm/model_executor/model_loader/weight_utils.py
89	3	vllm/model_executor/models/registry.py

[032d661d2] Wenlong Wang 2025-09-20 [Docs] Fix warnings in mkdocs build (continued)  (#25042)
1	1	vllm/multimodal/__init__.py
3	3	vllm/utils/__init__.py
2	2	vllm/v1/core/kv_cache_utils.py
5	5	vllm/v1/sample/rejection_sampler.py
1	1	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/utils.py
10	2	vllm/worker/model_runner.py

[e08a3a3fd] Michael Goin 2025-09-20 [CI Failure] Disable FlashInfer RoPE to unblock CI (#25299)
7	7	vllm/model_executor/layers/rotary_embedding/base.py

[3d9a1d2de] Cyrus Leung 2025-09-20 [V1] Support `LLM.apply_model` (#18465)
1	11	tests/conftest.py
22	15	tests/kernels/moe/test_mxfp4_moe.py
23	23	tests/models/multimodal/generation/test_qwen2_vl.py
1	1	tests/models/quantization/test_awq.py
8	10	tests/quantization/test_compressed_tensors.py
4	4	tests/quantization/test_fp8.py
38	33	tests/quantization/test_gptq_dynamic.py
2	2	tests/quantization/test_lm_head.py
3	7	tests/quantization/test_modelopt.py
28	19	tests/quantization/test_ptpc_fp8.py
11	15	tests/quantization/test_quark.py
10	7	tests/quantization/test_register_quantization_config.py
6	1	vllm/engine/llm_engine.py
7	2	vllm/entrypoints/llm.py
16	17	vllm/executor/executor_base.py
6	1	vllm/v1/engine/llm_engine.py
8	1	vllm/worker/worker_base.py

[be874c020] Roger Wang 2025-09-20 [Bugfix] Fix Qwen3-VL-MoE weight loading for EP (#25300)
7	5	vllm/model_executor/models/qwen3_vl_moe.py

[9607d5eb4] Chen Zhang 2025-09-19 [Hybrid Allocator] Support full attention with different hidden size  (#25101)
103	15	tests/v1/core/test_kv_cache_utils.py
108	38	vllm/v1/core/kv_cache_utils.py
6	10	vllm/v1/engine/core.py
70	0	vllm/v1/kv_cache_interface.py
36	29	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/utils.py

[c60e6137f] Cyrus Leung 2025-09-20 [Optimization] Avoid repeated model architecture conversion for pooling models (#25261)
21	1	vllm/config/model.py
16	1	vllm/model_executor/model_loader/utils.py

[f91480b2d] Chauncey 2025-09-20 [Bugfix] fix tool call arguments is empty (#25223)
60	0	tests/entrypoints/openai/test_completion_with_function_calling.py
5	3	vllm/entrypoints/chat_utils.py

[6c5f82e5a] Chendi.Xue 2025-09-19 [BUG FIX][NON-CUDA]quick fix to avoid call cudagraph_unsafe in attention (#25298)
6	2	vllm/attention/layer.py

[b7f186bbb] Nick Hill 2025-09-19 [BugFix] Exclude self when checking for port collision (#25286)
3	1	vllm/utils/__init__.py

[364290961] JartX 2025-09-20 [BUGFIX] GPTQ quantization compatibility for Qwen3 Next MOE models (AutoGPTQ and AutoRound-GPTQ) (#25268)
5	3	vllm/model_executor/models/qwen3_next.py

[c308501cb] Harry Mellor 2025-09-20 Improve weight loading for encoder models in Transformers backend (#25289)
26	2	vllm/model_executor/models/transformers.py

[535d80056] Nick Hill 2025-09-19 [Misc] Support more collective_rpc return types (#25294)
202	1	tests/v1/engine/test_engine_core_client.py
44	16	vllm/v1/serial_utils.py

[a25ade5d4] Nick Hill 2025-09-19 [BugFix] Ensure appropriate guards in destructors (#25284)
1	1	vllm/compilation/collective_fusion.py
5	4	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
0	3	vllm/executor/executor_base.py
2	1	vllm/v1/worker/gpu_worker.py

[8945b001d] Boyuan Feng 2025-09-19 [torch.compile] CUDAGraph Inductor partition integration (#24281)
60	11	tests/compile/piecewise/test_simple.py
1	0	tests/compile/silly_attention.py
58	1	tests/compile/test_full_graph.py
15	1	tests/compile/test_fusion_attn.py
2	0	vllm/attention/layer.py
8	2	vllm/compilation/backends.py
55	2	vllm/compilation/decorators.py
73	11	vllm/config/compilation.py
8	4	vllm/v1/cudagraph_dispatcher.py

[b8a287a0a] Andrew Sansom 2025-09-19 [docs] Prompt Embedding feature support (#25288)
18	16	docs/features/README.md
0	3	docs/features/prompt_embeds.md

[c7e713616] Andrew Sansom 2025-09-19 test: Remove vestigial skip for prompt embeds tests after landing v1 Prompt Embeds support (#25291)
0	3	tests/entrypoints/openai/test_completion_with_prompt_embeds.py

[a36c67581] Maximilien de Bayser 2025-09-19 Don't skip special tokens with hermes-style tool calling (#25281)
9	0	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py

[3da17c2cc] Lucas Kabela 2025-09-19 [Bugfix] Remove VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE #2969 (#25090)
2	14	tests/compile/test_basic_correctness.py
1	3	tests/compile/test_full_graph.py
4	6	vllm/compilation/wrapper.py
0	5	vllm/envs.py
1	3	vllm/v1/worker/gpu_model_runner.py
3	5	vllm/worker/model_runner.py

[14c143278] Nick Hill 2025-09-19 [BugFix] Fix async scheduling CPU tensor race take 2 (#25279)
32	22	vllm/v1/worker/gpu_model_runner.py

[ee7a66dd9] Lucia Fang 2025-09-19 allow disable flashinfer prefill (#25276)
3	0	vllm/envs.py
2	1	vllm/v1/attention/backends/mla/common.py

[431535b52] Zhiyu 2025-09-19 Enable modelopt gemma3 nvfp4/fp8, make workflow more robust (#22771)
2	1	tests/kernels/moe/test_modular_kernel_combinations.py
5	2	vllm/compilation/backends.py
3	0	vllm/config/model.py
2	2	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
37	16	vllm/model_executor/layers/quantization/modelopt.py
16	0	vllm/model_executor/models/gemma3.py
17	1	vllm/model_executor/models/siglip.py

[711e91294] Wentao Ye 2025-09-19 [Compile] Fix Compile Warning for Ignoring `MIN_BLOCK_PER_SM` (#25193)
38	0	csrc/launch_bounds_utils.h
4	2	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
6	4	csrc/quantization/fp4/nvfp4_experts_quant.cu
5	3	csrc/quantization/fp4/nvfp4_quant_kernels.cu

[e69e0b8b5] Alec S 2025-09-19 [Frontend] Responses API messages out, just harmony for now (#24985)
15	0	tests/entrypoints/openai/test_response_api_with_harmony.py
16	1	vllm/entrypoints/openai/protocol.py
13	0	vllm/entrypoints/openai/serving_responses.py

[ddc904839] David-Wen 2025-09-20 Fix: Correct FusedMoE layer reference in auto_round quantization (#24818)
2	2	vllm/model_executor/layers/quantization/auto_round.py

[b1a63d1b3] nvjullin 2025-09-20 [BugFix] Make FlashInferMetadataBuilder non-blocking (#25040)
3	2	vllm/v1/attention/backends/flashinfer.py

[48ecb4438] Michael Goin 2025-09-19 [Perf] Use FlashInfer RoPE for RotaryEmbedding.forward_cuda when available (#21126)
28	10	vllm/model_executor/layers/rotary_embedding/base.py
46	0	vllm/model_executor/layers/rotary_embedding/common.py
1	3	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
1	1	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
2	0	vllm/model_executor/layers/rotary_embedding/mrope.py

[e57fc1597] Harry Mellor 2025-09-19 Specify platform in `pip-compile` `pre-commit` hook so it runs on MacOS (#25273)
1	1	.pre-commit-config.yaml
1	1	requirements/test.txt

[4bdf40021] bnellnm 2025-09-19 [Bugfix] Fix chunked a2_scales in modular kernels (#25264)
1	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
2	1	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
3	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py
2	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
3	1	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	1	vllm/model_executor/layers/fused_moe/fused_moe.py
1	0	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
5	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
2	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/trtllm_moe.py

[7852b82b9] Varun Sundar Rabindranath 2025-09-19 [Bugfix] GPT OSS Attritbute error on H100 (#25228)
2	2	vllm/model_executor/layers/fused_moe/config.py
2	2	vllm/model_executor/layers/quantization/mxfp4.py

[a2a5f79e0] qizixi 2025-09-19 Optimize triton unified attention performance for sliding window attention (#24390)
1	1	tests/kernels/attention/test_triton_unified_attention.py
24	2	vllm/attention/ops/triton_unified_attention.py

[c59a0eca4] Or Ozeri 2025-09-19 [KV offload][4/N] Offloading KV connector (#22595)
505	0	tests/v1/kv_connector/unit/test_offloading_connector.py
2	1	tests/v1/kv_connector/unit/utils.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
485	0	vllm/distributed/kv_transfer/kv_connector/v1/offloading_connector.py
53	0	vllm/v1/kv_offload/factory.py
61	0	vllm/v1/kv_offload/spec.py

[b716ab93a] Lucia Fang 2025-09-19 [bugfix] fix structured outputs key missing issue from #24929 (#25195)
3	1	vllm/v1/core/sched/scheduler.py
6	5	vllm/v1/structured_output/utils.py

[138f0d1e7] samzong 2025-09-20 [Docs] add __init__.py to vllm/model_executor/layers/quantization/compressed_tensors/transform (#24974)
0	0	vllm/model_executor/layers/quantization/compressed_tensors/transform/__init__.py
0	0	vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/__init__.py
1	1	vllm/model_executor/models/blip2.py
1	1	vllm/model_executor/models/llava.py
3	2	vllm/model_executor/models/llava_next.py

[2506ce518] Jialin Ouyang 2025-09-19 [Core][Prefix Hash] Fix prefix hash metrics sliding window maintainance (#24990)
39	7	tests/v1/core/test_kv_cache_utils.py
11	2	vllm/v1/core/kv_cache_utils.py

[47fd08aaf] Chauncey 2025-09-20 [CI/Build] fix test function_calling (#25072)
1	0	tests/entrypoints/openai/test_response_api_with_harmony.py

[12aed7e45] Harry Mellor 2025-09-19 Encoder model support for the Transformers backend (#25174)
25	12	docs/models/supported_models.md
35	1	tests/models/test_transformers.py
4	4	vllm/attention/backends/abstract.py
47	7	vllm/model_executor/models/transformers.py

[d90e212a3] LJH-LBJ 2025-09-20 Remove Redundant Assignment in Qwen3_VisionPatchMerger (#25224)
1	3	vllm/model_executor/models/qwen3_vl.py

[282198645] Jee Jee Li 2025-09-20 [Core] Modify the initialization parameters of the lora manager (#25249)
28	8	tests/lora/test_lora_manager.py
1	1	tests/lora/utils.py
0	0	vllm/lora/{lora.py => lora_weights.py}
1	1	vllm/lora/models.py
12	11	vllm/lora/worker_manager.py
2	3	vllm/v1/worker/cpu_model_runner.py
1	4	vllm/v1/worker/gpu_model_runner.py
3	12	vllm/v1/worker/lora_model_runner_mixin.py
1	3	vllm/v1/worker/tpu_model_runner.py
2	9	vllm/worker/model_runner.py

[6c117cff7] Cyrus Leung 2025-09-20 [Frontend] Pass API server count to each process (#23717)
1	1	benchmarks/kernels/benchmark_w8a8_block_fp8.py
1	8	examples/others/tensorize_vllm_model.py
1	1	tests/entrypoints/test_api_server_process_manager.py
48	4	tests/v1/test_external_lb_dp.py
49	5	tests/v1/test_hybrid_lb_dp.py
49	8	tests/v1/test_internal_lb_dp.py
25	0	vllm/config/parallel.py
8	1	vllm/engine/arg_utils.py
6	12	vllm/entrypoints/cli/serve.py
29	9	vllm/entrypoints/openai/api_server.py
2	1	vllm/multimodal/cache.py
2	1	vllm/v1/engine/core_client.py

[7ac67ea52] Or Ozeri 2025-09-19 [KV offload][3/N] Add worker-side CPU support (#21448)
177	0	tests/v1/kv_offload/test_cpu_gpu.py
171	0	vllm/v1/kv_offload/worker/cpu_gpu.py

[ce75e1537] samzong 2025-09-20 refactor(benchmarks): add type annotations to wait_for_endpoint parameters (#25218)
13	2	vllm/benchmarks/lib/endpoint_request_func.py
3	2	vllm/benchmarks/lib/ready_checker.py

[aed16879a] Harry Mellor 2025-09-19 Move `ModelConfig` from `config/__init__.py` to `config/model.py` (#25252)
2	1	tests/conftest.py
1	1	tests/distributed/test_pipeline_parallel.py
2	3	tests/models/test_initialization.py
4	5	tests/v1/sample/test_logprobs.py
14	2094	vllm/config/__init__.py
2006	0	vllm/config/model.py
2	6	vllm/config/scheduler.py
99	1	vllm/config/utils.py
6	9	vllm/engine/arg_utils.py
3	4	vllm/model_executor/model_loader/utils.py
11	11	vllm/model_executor/models/registry.py
5	8	vllm/v1/sample/ops/topk_topp_sampler.py
5	6	vllm/v1/sample/sampler.py

[cf278ff3b] Harry Mellor 2025-09-19 Update CODEOWNERS (#25269)
10	3	.github/CODEOWNERS

[838d7116b] Icey 2025-09-19 [Qwen] Remove cuda hard-code in qwen3 next (#25243)
1	1	vllm/model_executor/models/qwen3_next.py

[5089fd749] Cyrus Leung 2025-09-19 [V0 Deprecation] Remove V0 logic from `get_input_embeddings` interface (#25242)
17	28	vllm/model_executor/models/hyperclovax_vision.py
0	24	vllm/model_executor/models/interfaces.py
4	15	vllm/model_executor/models/ultravox.py
1	17	vllm/model_executor/models/utils.py

[a3d087ade] Nicolò Lucchesi 2025-09-19 [P/D][Nixl] Introduce `KVTransferMetrics` and aggregation strategy (#22188)
210	1	tests/v1/kv_connector/unit/test_nixl_connector.py
18	3	vllm/distributed/kv_transfer/kv_connector/utils.py
21	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
100	0	vllm/distributed/kv_transfer/kv_connector/v1/metrics.py
65	3	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
65	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
17	10	vllm/v1/core/sched/scheduler.py
7	1	vllm/v1/metrics/loggers.py
2	1	vllm/v1/metrics/stats.py
10	1	vllm/v1/outputs.py
10	1	vllm/v1/worker/kv_connector_model_runner_mixin.py

[058525b99] Harry Mellor 2025-09-19 Move `PoolerConfig` from `config/__init__.py` to `config/pooler.py` (#25181)
2	2	docs/models/pooling_models.md
2	2	docs/models/supported_models.md
1	1	examples/online_serving/openai_embedding_long_text/README.md
2	2	examples/online_serving/openai_embedding_long_text/client.py
1	1	examples/online_serving/openai_embedding_long_text/service.sh
1	1	tests/entrypoints/pooling/openai/test_embedding_long_text.py
1	1	tests/models/language/pooling/test_embedding.py
2	1	tests/models/language/pooling/test_mm_classifier_conversion.py
29	33	tests/models/language/pooling/{test_override_pooler_config.py => test_pooler_config_init_behaviour.py}
6	12	tests/test_config.py
36	119	vllm/config/__init__.py
97	0	vllm/config/pooler.py
6	1	vllm/engine/arg_utils.py
7	3	vllm/entrypoints/llm.py

[1dfea5f4a] Roger Wang 2025-09-19 [Bugfix][Perf] Misc fixes for Qwen3 VL (#25238)
10	13	vllm/model_executor/models/qwen3_vl.py
2	0	vllm/model_executor/models/qwen3_vl_moe.py

[cea91a32f] Isotr0py 2025-09-19 [Kernel][Performance] Add Triton kernel for Qwen3-VL interleaved MRoPE (#25055)
66	32	tests/kernels/core/test_mrope.py
22	14	vllm/model_executor/layers/rotary_embedding/mrope.py

[a684c0124] Yan Ma 2025-09-19 [bugfix] fix MHA for models like OpenGVLab/InternVL3_5-38B (#25146)
5	3	vllm/attention/layer.py

[f2718d294] Isotr0py 2025-09-19 [Misc] Cleanup test conftest for deprecated encoder-decoder models (#25231)
0	138	tests/conftest.py

[825fdb11a] Li, Jiang 2025-09-19 [Bugfix][CPU] Add placeholder to avoid import errors when using fused_moe ops on platforms without triton (#25137)
9	0	vllm/model_executor/layers/fused_moe/__init__.py

[8c1d4acbf] Li, Jiang 2025-09-19 [CPU] Disable oneDNN linear on non-x86 platforms (#25166)
3	2	vllm/model_executor/layers/utils.py

[486c5599e] Russell Bryant 2025-09-19 [Build] Update Xgrammar to 0.1.24 to get a CVE fix (#25188)
1	1	requirements/common.txt

[a6149aa58] Chendi.Xue 2025-09-19 [OOT] Support sync_model_loading for OOT (#25126)
3	3	vllm/model_executor/parameter.py
3	14	vllm/model_executor/utils.py
23	0	vllm/platforms/interface.py
4	0	vllm/platforms/tpu.py

[6c8a3c099] Michael Yao 2025-09-19 [Docs] Fix griffe warnings in vllm/multimodal (#25216)
3	3	vllm/multimodal/inputs.py
3	1	vllm/multimodal/utils.py

[31a8a2a7b] Roger Wang 2025-09-18 [Misc] Clean up MM profiling warnings (#25222)
0	29	vllm/multimodal/profiling.py

[1a0a04dae] Chen Ding 2025-09-19 [Perf] Optimize memory peak during EAGLE model loading. (#24585)
8	7	vllm/model_executor/models/deepseek_eagle.py
10	12	vllm/model_executor/models/llama4_eagle.py
8	7	vllm/model_executor/models/llama_eagle.py

[6d8246aaf] Andrew Xia 2025-09-18 [gpt-oss] Add ResponseReasoningPartAddedEvent, ResponseReasoningPartDoneEvent for streaming (#24938)
55	1	tests/entrypoints/openai/test_response_api_with_harmony.py
68	20	vllm/entrypoints/openai/protocol.py
20	12	vllm/entrypoints/openai/serving_responses.py

[9d1c50a5a] Or Ozeri 2025-09-19 [KV offload][2/N] Introduce LRU-based CPU offloading management (#20075)
175	0	tests/v1/kv_offload/test_cpu.py
96	0	vllm/v1/kv_offload/backend.py
61	0	vllm/v1/kv_offload/backends/cpu.py
132	0	vllm/v1/kv_offload/lru_manager.py

[9a4600e4d] Andrew Sansom 2025-09-18 [CORE] Prompt Embeddings Support for v1 Engine (#24278)
0	10	tests/basic_correctness/test_basic_correctness.py
0	1	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
0	6	tests/models/language/generation/test_common.py
18	6	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/openai/protocol.py
27	0	vllm/utils/__init__.py
18	6	vllm/v1/core/sched/output.py
2	1	vllm/v1/engine/__init__.py
21	10	vllm/v1/engine/detokenizer.py
20	5	vllm/v1/engine/output_processor.py
29	9	vllm/v1/engine/processor.py
12	3	vllm/v1/request.py
1	1	vllm/v1/sample/logits_processor/__init__.py
4	2	vllm/v1/sample/logits_processor/builtin.py
1	1	vllm/v1/sample/logits_processor/interface.py
1	1	vllm/v1/serial_utils.py
48	7	vllm/v1/worker/gpu_input_batch.py
96	3	vllm/v1/worker/gpu_model_runner.py
4	2	vllm/v1/worker/tpu_input_batch.py
1	0	vllm/v1/worker/tpu_model_runner.py

[9fac6aa30] Lucas Wilkinson 2025-09-18 [BugFix] Fix DeepGEMM warmup, no m.weight_scale_inv (#25206)
1	1	vllm/model_executor/warmup/deep_gemm_warmup.py

[a53ad626d] Or Ozeri 2025-09-18 [KV offload][1b/N] rename offloading to kv_offload (#25191)
1	1	.buildkite/test-pipeline.yaml
2	2	tests/v1/{offloading => kv_offload}/test_worker.py
0	0	vllm/v1/{offloading => kv_offload}/abstract.py
1	1	vllm/v1/{offloading => kv_offload}/mediums.py
1	1	vllm/v1/{offloading => kv_offload}/worker/worker.py

[1c3dad22f] Woosuk Kwon 2025-09-18 [V0 Deprecation] Remove unused async_timeout.py (#25190)
0	173	vllm/engine/async_timeout.py

[d2a30a2d9] Wentao Ye 2025-09-18 [Bug] Fix torch Compilation Cache Hit Error (#25093)
0	12	vllm/config/compilation.py
10	7	vllm/platforms/cuda.py

[75fb112d8] Wentao Ye 2025-09-18 [Bug] Fix `returned_lse` not Defined issue (#25106)
3	4	vllm/v1/attention/backends/mla/cutlass_mla.py

[38db529f6] Aziz 2025-09-18 [feat]: Create interface for model-specific M-RoPE (#24194)
7	4	vllm/model_executor/models/__init__.py
68	0	vllm/model_executor/models/interfaces.py
115	3	vllm/model_executor/models/qwen2_vl.py
23	10	vllm/v1/worker/gpu_model_runner.py
29	13	vllm/worker/model_runner.py

[064cac7bb] Nikhil Gupta 2025-09-18 [fix]: remove data type hardcoding from gptoss model implementation (#23807)
1	3	vllm/model_executor/models/gpt_oss.py

[e19bce40a] Woosuk Kwon 2025-09-18 [V0 Deprecation] Remove AsyncLLMEngine (#25025)
19	35	tests/entrypoints/openai/test_chat.py
0	830	tests/entrypoints/openai/test_completion.py
3	0	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
3	2	tests/entrypoints/openai/test_lora_adapters.py
1	1	tests/entrypoints/openai/test_metrics.py
24	2	tests/entrypoints/openai/test_return_tokens_as_ids.py
0	8	tests/entrypoints/openai/test_skip_tokenizer.py
0	18	tests/v1/test_oracle.py
2	1028	vllm/engine/async_llm_engine.py
0	2	vllm/entrypoints/launcher.py
26	43	vllm/entrypoints/openai/api_server.py

[505805b64] Or Ozeri 2025-09-18 [KV offload][1/N] Introduce an offloading component (#19848)
1	0	.buildkite/test-pipeline.yaml
152	0	tests/v1/offloading/test_worker.py
165	0	vllm/v1/offloading/abstract.py
39	0	vllm/v1/offloading/mediums.py
142	0	vllm/v1/offloading/worker/worker.py

[bbdc0f236] Rohan Potdar 2025-09-18 [ROCm][AITER][Bugfix] Switch AITER to use PIECEWISE_AND_FULL compilation (#25104)
1	1	vllm/v1/attention/backends/rocm_aiter_fa.py

[dc3405936] Gregory Shtrasberg 2025-09-18 [ROCm][CI/Build] Use ROCm7.0 as the base (#25178)
4	1	docker/Dockerfile.rocm
12	49	docker/Dockerfile.rocm_base

[c4cb0af98] qizixi 2025-09-18 [spec decode] Fix MTP inference path for MiMo-7B model (#25136)
5	1	examples/offline_inference/spec_decode.py
1	1	vllm/config/speculative.py
14	4	vllm/model_executor/models/mimo_mtp.py

[1c3b1634a] Harry Mellor 2025-09-18 [Misc] Add codeowner for Transformers backend (#25180)
4	0	.github/CODEOWNERS

[2ea50e977] Shu Wang 2025-09-18 Enable Allgather/ReduceScatter backend for NaiveAllToAll (#23964)
39	0	vllm/distributed/device_communicators/all2all.py
4	0	vllm/distributed/device_communicators/cuda_communicator.py
12	5	vllm/envs.py

[b419937c7] Hyogeun Oh (오효근) 2025-09-19 [Docs] Fix warnings in mkdocs build (continued) (#25163)
1	1	vllm/distributed/device_communicators/shm_object_storage.py
2	6	vllm/entrypoints/openai/serving_engine.py

[5f696c33b] wang.yuqi 2025-09-18 [New Model] Support BertForTokenClassification / Named Entity Recognition (NER) task (#24872)
11	0	docs/models/supported_models.md
7	1	examples/offline_inference/pooling/README.md
54	0	examples/offline_inference/pooling/ner.py
6	0	examples/online_serving/pooling/README.md
71	0	examples/online_serving/pooling/ner.py
39	0	tests/models/language/pooling/test_token_classification.py
1	0	tests/models/registry.py
4	0	vllm/entrypoints/llm.py
52	0	vllm/model_executor/models/bert.py
1	0	vllm/model_executor/models/registry.py
11	1	vllm/v1/attention/backends/flex_attention.py

[67244c86f] dongbo910220 2025-09-18 feat(api): Return 503 on /health when engine is dead (#24897)
6	2	vllm/entrypoints/openai/api_server.py

[072d7e53e] Vadim Gimpelson 2025-09-18 [PERF] Add `conv1d` metadata to GDN attn (#25105)
5	3	vllm/model_executor/layers/mamba/mamba2_metadata.py
9	1	vllm/model_executor/models/qwen3_next.py
6	0	vllm/v1/attention/backends/gdn_attn.py
2	2	vllm/v1/attention/backends/mamba2_attn.py
2	2	vllm/v1/attention/backends/short_conv_attn.py

[01a583fea] jvlunteren 2025-09-18 [Kernel] Decouple Tile Size from Block Size in Triton Unified Attention Kernel (#21197)
0	3	tests/kernels/attention/test_triton_unified_attention.py
70	52	vllm/attention/ops/triton_unified_attention.py

[bc19d7598] Nicolò Lucchesi 2025-09-18 [Misc] Add kv-connector label (#25156)
17	0	.github/mergify.yml

[fbd6523ac] Michael Goin 2025-09-18 Refactor dense FP8 tensor/channel/block utils and add CT FP8 block (#21404)
7	7	vllm/model_executor/layers/linear.py
35	33	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
96	95	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
84	183	vllm/model_executor/layers/quantization/fp8.py
220	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[470484a4f] Shanshan Shen 2025-09-18 [Structured Output][Refactor] Move `apply_grammar_bitmask()` method from `ModelRunner` to structured output utils (#21999)
80	0	vllm/v1/structured_output/utils.py
4	71	vllm/v1/worker/gpu_model_runner.py

[21da73343] Roger Wang 2025-09-18 [Misc] Clean up flags in `vllm bench serve` (#25138)
0	3	docs/contributing/benchmarks.md
1	1	tests/benchmarks/test_serve_cli.py
4	4	vllm/benchmarks/datasets.py
34	15	vllm/benchmarks/serve.py

[66072b36d] Asaf Joseph Gardin 2025-09-18 [Bugfix][Mamba] - Fix Conv State Kernel FP32 Support (#24883)
6	3	tests/models/language/generation/test_hybrid.py
8	2	vllm/model_executor/layers/mamba/ops/causal_conv1d.py

[3ed1ec4af] Harry Mellor 2025-09-18 Fix `validate-config` pre-commit check (#25157)
1	3	.pre-commit-config.yaml
16	7	tools/validate_config.py
2	0	vllm/config/__init__.py

[5a33ae9a3] Harry Mellor 2025-09-18 Fix forward reference warning in documentation (#25150)
13	13	vllm/engine/async_timeout.py

[c9ff9e6f0] William Song 2025-09-18 [Docs] add the parallel sampling usage in LLMEngine and AsyncLLM (#24222)
7	1	vllm/sampling_params.py

[eaffe4486] Kay Yan 2025-09-18 [Docs] Fix pooling-params doc references in openai_compatible_server.md (#24939)
0	1	docs/api/README.md
12	8	docs/serving/openai_compatible_server.md
14	6	vllm/pooling_params.py

[8ed039d52] Harry Mellor 2025-09-18 Move `StructuredOutputsConfig` from `config/__init__.py` to `config/structured_outputs.py` (#25153)
1	60	vllm/config/__init__.py
64	0	vllm/config/structured_outputs.py

[37970105f] Jee Jee Li 2025-09-18 [Model] Improve Pooling Model (#25149)
6	6	vllm/model_executor/layers/pooler.py
1	0	vllm/v1/worker/gpu_model_runner.py

[cc935fdd7] Chauncey 2025-09-18 [Frontend] Support setting logprobs to -1 (#25031)
23	0	tests/entrypoints/openai/test_chat_echo.py
5	3	vllm/entrypoints/openai/protocol.py

[abdfcd4f3] Elvir Crnčević 2025-09-18 silu-v1: Fix EPS not being used during max-reduction (#25069)
1	2	csrc/quantization/activation_kernels.cu

[4f02b77de] ihb2032 2025-09-18 Fix: Add explicit #include <omp.h> for OpenMP compatibility on certain toolchains  (#24951)
4	0	csrc/cpu/cpu_types.hpp

[29283e897] Aaron Pham 2025-09-18 [Chore] Cleanup guided namespace, move to structured outputs config (#22772)
0	6	.buildkite/scripts/hardware_ci/run-amd-test.sh
1	2	.buildkite/test-pipeline.yaml
1	1	.github/mergify.yml
8	8	benchmarks/benchmark_serving_structured_output.py
1	1	docs/api/README.md
5	5	docs/features/reasoning_outputs.md
18	18	docs/features/structured_outputs.md
5	6	docs/features/tool_calling.md
2	2	docs/serving/openai_compatible_server.md
29	25	examples/offline_inference/structured_outputs.py
1	1	examples/online_serving/openai_chat_completion_client_with_tools_required.py
5	3	examples/online_serving/structured_outputs/structured_outputs.py
1	1	tests/entrypoints/conftest.py
0	82	tests/entrypoints/llm/test_lazy_outlines.py
59	64	tests/entrypoints/openai/test_chat.py
39	40	tests/entrypoints/openai/test_completion.py
2	2	tests/entrypoints/openai/test_completion_with_function_calling.py
5	3	tests/entrypoints/openai/test_openai_schema.py
1	1	tests/entrypoints/openai/test_prompt_validation.py
0	4	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/entrypoints/openai/test_transcription_validation.py
1	1	tests/entrypoints/openai/test_translation_validation.py
0	84	tests/test_sampling_params.py
6	5	tests/tool_use/test_tool_choice_required.py
3	3	tests/v1/core/test_scheduler.py
2	2	tests/v1/engine/test_llm_engine.py
1	1	tests/v1/entrypoints/conftest.py
72	63	tests/v1/entrypoints/llm/test_struct_output_generate.py
11	3	tests/v1/entrypoints/openai/test_chat_completion.py
11	3	tests/v1/entrypoints/openai/test_completion.py
18	17	vllm/config/__init__.py
55	40	vllm/engine/arg_utils.py
1	6	vllm/engine/async_llm_engine.py
6	12	vllm/engine/llm_engine.py
1	6	vllm/engine/protocol.py
23	4	vllm/entrypoints/llm.py
5	5	vllm/entrypoints/openai/api_server.py
72	136	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py
3	3	vllm/model_executor/models/config.py
18	44	vllm/sampling_params.py
1	4	vllm/transformers_utils/tokenizers/mistral.py
0	3	vllm/v1/engine/async_llm.py
28	29	vllm/v1/engine/processor.py
1	1	vllm/v1/request.py
7	6	vllm/v1/structured_output/__init__.py
2	2	vllm/v1/structured_output/backend_guidance.py
11	11	vllm/v1/structured_output/backend_lm_format_enforcer.py
16	16	vllm/v1/structured_output/backend_outlines.py
19	19	vllm/v1/structured_output/backend_xgrammar.py
1	1	vllm/v1/structured_output/request.py

[05b044e69] Punitvara 2025-09-18 [Doc] Fix cross-reference warnings (#25058)
2	1	vllm/benchmarks/datasets.py
4	4	vllm/distributed/device_communicators/shm_object_storage.py
7	5	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
1	1	vllm/model_executor/models/mistral3.py
1	1	vllm/multimodal/profiling.py
3	2	vllm/v1/core/kv_cache_manager.py

[aa3f105c5] Gerard Finol 2025-09-18 Add 'path' option to ImagePrompt data_format (#25081)
1	1	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/types.py

[ef7eefe17] Tao He 2025-09-18 [Qwen] Add fp8 checkpoint support for qwen3-next. (#25079)
17	18	vllm/model_executor/models/qwen3_next.py
5	3	vllm/model_executor/models/qwen3_next_mtp.py

[350c94deb] rongfu.leng 2025-09-18 [Bugfix] when use s3 model cannot use default load_format (#24435)
12	0	vllm/config/__init__.py
0	1	vllm/engine/arg_utils.py

[f4cd80f94] Harry Mellor 2025-09-18 Retrieve `sliding_window` from text config in Gemma3 MM (#25085)
2	1	vllm/model_executor/models/gemma3_mm.py
0	3	vllm/model_executor/models/gemma3n_mm.py

[349e0e346] Harry Mellor 2025-09-18 [Docs] Fix API Reference (#25140)
1	1	mkdocs.yaml

[81b16a2bc] Lumina 2025-09-18 [Kernel] Better inf handling for grouped topk cu (#24886)
24	20	csrc/moe/grouped_topk_kernels.cu

[e111d5b0a] Simon Mo 2025-09-17 [CLI] Use streaming in CLI chat and completion commands (#23769)
45	26	vllm/entrypoints/cli/openai.py

[a904ea78e] Simon Mo 2025-09-17 [benchmark] add peak throughput metrics and plot (#23867)
5	0	vllm/benchmarks/lib/endpoint_request_func.py
129	69	vllm/benchmarks/serve.py

[b7433ca1a] Benjamin Chislett 2025-09-18 [Spec Decode] Efficient padded speculation (#24539)
174	5	tests/v1/spec_decode/test_eagle.py
5	0	vllm/config/speculative.py
223	35	vllm/v1/spec_decode/eagle.py
4	1	vllm/v1/worker/gpu_input_batch.py
101	63	vllm/v1/worker/gpu_model_runner.py

[5c65a72bb] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove more V0 tests (#25117)
0	6	.buildkite/test-pipeline.yaml
0	2	.github/CODEOWNERS
0	0	tests/async_engine/__init__.py
0	54	tests/async_engine/api_server_async_engine.py
0	12	tests/async_engine/conftest.py
0	139	tests/async_engine/test_api_server.py
0	71	tests/async_engine/test_request_tracker.py
0	189	tests/basic_correctness/test_preemption.py
0	11	tests/detokenizer/conftest.py
0	83	tests/detokenizer/test_stop_checker.py
0	10	tests/entrypoints/openai/correctness/test_lmeval.py
0	182	tests/samplers/test_logprobs.py
0	0	tests/worker/__init__.py
0	11	tests/worker/conftest.py
0	113	tests/worker/test_model_input.py
0	462	tests/worker/test_model_runner.py
0	68	tests/worker/test_profile.py
0	87	tests/worker/test_swap.py

[9d8a2d86d] YiwenC 2025-09-17 [EPLB] Add EPLB support for hunyuan_v1 (#23078)
2	2	vllm/model_executor/layers/fused_moe/layer.py
121	14	vllm/model_executor/models/hunyuan_v1.py

[3bc18127f] Chaojun Zhang 2025-09-18 [XPU] Whisper model support on XPU Platform (#25123)
2	2	vllm/attention/layer.py
1	1	vllm/v1/worker/utils.py

[bec060fd9] Andrew Sansom 2025-09-17 Mark prompt logprobs as incompatible with prompt embeds at API level (#25077)
17	0	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
7	4	vllm/engine/llm_engine.py
5	0	vllm/entrypoints/openai/serving_completion.py

[52bc9d5b3] YiwenC 2025-09-17 [Model] enable data parallel for InternVL vision encoder (#23909)
1	0	docs/configuration/optimization.md
75	32	vllm/model_executor/models/intern_vit.py
4	1	vllm/model_executor/models/internvl.py

[dc2979c58] bnellnm 2025-09-18 [Kernels] Overlap shared experts with combine instead of dispatch (#24254)
45	5	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
47	8	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
76	19	vllm/model_executor/layers/fused_moe/modular_kernel.py
35	4	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py

[027d37df3] toncao 2025-09-18 [Bugfix][Qwen3-Next] add prefixes to shared_expert in qwen3-next and mlp in qwen2moe to successfully load ignored params in quantized models (#24960)
25	23	vllm/model_executor/models/qwen2_moe.py
1	0	vllm/model_executor/models/qwen3_next.py

[b98219670] Lukas Geiger 2025-09-18 [Core][MM] Cleanup `MultiModalCache` (#25006)
6	19	vllm/multimodal/cache.py

[32baf1d03] Harry Mellor 2025-09-18 [Docs] Clean up the contributing README (#25099)
75	66	docs/contributing/README.md
1	1	docs/getting_started/installation/python_env_setup.inc.md
1	0	mkdocs.yaml

[3127274d0] Roger Wang 2025-09-17 [MM Encoder] Apply DP ViT for Qwen3-VL model series (#24955)
75	19	vllm/model_executor/models/qwen3_vl.py
2	0	vllm/model_executor/models/qwen3_vl_moe.py

[4ac510f48] bnellnm 2025-09-17 [Kernels] Enable DeepGEMM by default (#24462)
2	2	vllm/envs.py

[7fb2a5be2] Woosuk Kwon 2025-09-17 [V0 Deprecation] Skip PP test (#25128)
28	86	tests/distributed/test_pipeline_parallel.py

[6c036615d] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove misc V0 tests (#25118)
0	98	tests/model_executor/test_logits_processor.py
0	92	tests/test_cache_block_hashing.py

[2fc24e94f] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove V0 Tracing & Metrics tests (#25115)
1	3	.buildkite/test-pipeline.yaml
0	268	tests/metrics/test_metrics.py
0	0	tests/tracing/__init__.py
0	237	tests/tracing/test_tracing.py
0	0	tests/{metrics => v1/tracing}/__init__.py

[2c3c1bd07] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove V0 Engine tests (#25114)
0	12	tests/engine/conftest.py
0	37	tests/engine/test_computed_prefix_blocks.py
0	111	tests/engine/test_executor.py
0	179	tests/engine/test_multiproc_workers.py
0	58	tests/engine/test_options.py
1	0	tests/engine/test_short_mm_context.py
0	225	tests/engine/test_stop_checker.py

[5963b98b4] bnellnm 2025-09-17 [Kernel] Delegate construction of FusedMoEQuantConfig to FusedMoEMethodBase subclasses (#22537)
36	22	benchmarks/kernels/benchmark_cutlass_fp4_moe.py
27	16	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
35	38	benchmarks/kernels/benchmark_moe.py
62	48	tests/kernels/moe/modular_kernel_tools/common.py
3	2	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
79	84	tests/kernels/moe/modular_kernel_tools/mk_objects.py
11	9	tests/kernels/moe/test_batched_deepgemm.py
2	2	tests/kernels/moe/test_batched_moe.py
32	49	tests/kernels/moe/test_block_fp8.py
22	25	tests/kernels/moe/test_block_int8.py
42	11	tests/kernels/moe/test_cutlass_moe.py
52	35	tests/kernels/moe/test_deepep_deepgemm_moe.py
33	37	tests/kernels/moe/test_deepep_moe.py
16	18	tests/kernels/moe/test_deepgemm.py
20	12	tests/kernels/moe/test_flashinfer.py
24	44	tests/kernels/moe/test_flashinfer_moe.py
17	13	tests/kernels/moe/test_gpt_oss_triton_kernels.py
21	19	tests/kernels/moe/test_modular_kernel_combinations.py
19	15	tests/kernels/moe/test_moe.py
12	7	tests/kernels/moe/test_nvfp4_moe.py
13	8	tests/kernels/moe/test_pplx_cutlass_moe.py
31	31	tests/kernels/moe/test_pplx_moe.py
9	6	tests/kernels/moe/test_triton_moe_ptpc_fp8.py
112	37	tests/kernels/moe/utils.py
21	14	tests/kernels/quantization/test_int8_kernel.py
4	4	vllm/model_executor/layers/fused_moe/__init__.py
15	25	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
17	41	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
482	200	vllm/model_executor/layers/fused_moe/config.py
62	103	vllm/model_executor/layers/fused_moe/cutlass_moe.py
28	37	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
4	9	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
20	25	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
10	40	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
1	6	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
185	0	vllm/model_executor/layers/fused_moe/flashinfer_trtllm_moe.py
34	85	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
194	552	vllm/model_executor/layers/fused_moe/fused_moe.py
24	71	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
92	45	vllm/model_executor/layers/fused_moe/layer.py
71	78	vllm/model_executor/layers/fused_moe/modular_kernel.py
3	7	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
1	3	vllm/model_executor/layers/fused_moe/prepare_finalize.py
26	26	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
9	39	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
9	16	vllm/model_executor/layers/fused_moe/trtllm_moe.py
4	0	vllm/model_executor/layers/fused_moe/utils.py
7	1	vllm/model_executor/layers/quantization/awq_marlin.py
7	1	vllm/model_executor/layers/quantization/bitsandbytes.py
215	180	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
12	4	vllm/model_executor/layers/quantization/experts_int8.py
80	79	vllm/model_executor/layers/quantization/fp8.py
6	1	vllm/model_executor/layers/quantization/gguf.py
7	1	vllm/model_executor/layers/quantization/gptq_marlin.py
5	0	vllm/model_executor/layers/quantization/ipex_quant.py
130	107	vllm/model_executor/layers/quantization/modelopt.py
22	11	vllm/model_executor/layers/quantization/moe_wna16.py
27	10	vllm/model_executor/layers/quantization/mxfp4.py
27	19	vllm/model_executor/layers/quantization/quark/quark_moe.py
22	12	vllm/model_executor/layers/quantization/rtn.py
6	14	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
14	24	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
1	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
5	5	vllm/model_executor/models/bert_with_rope.py
14	8	vllm/model_executor/models/deepseek.py
13	8	vllm/model_executor/models/minicpm.py
1	1	vllm/model_executor/models/qwen3_moe.py
8	3	vllm/model_executor/warmup/deep_gemm_warmup.py

[e6585ddb4] elvischenv 2025-09-18 [Bugfix] Fix accuracy issue for silu_mul + nvfp4 quant fusion kernel (#24833)
1	1	.buildkite/test-pipeline.yaml
27	93	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
7	6	tests/compile/test_silu_mul_quant_fusion.py
75	0	tests/kernels/quantization/test_silu_mul_nvfp4_quant.py
0	126	tests/kernels/quantization/test_silu_nvfp4_quant_fusion.py

[2a4d6412e] Karan Goel 2025-09-17 Add a batched auto tune script (#25076)
67	0	benchmarks/auto_tune/README.md
128	0	benchmarks/auto_tune/batch_auto_tune.sh

[e67a79db0] elvischenv 2025-09-18 [Bugfix] Refactor Flashinfer TRTLLM attention kernel selection logic (#24600)
5	2	vllm/envs.py
48	22	vllm/utils/flashinfer.py
12	5	vllm/v1/attention/backends/flashinfer.py

[9f882d879] Michael Goin 2025-09-17 Disable failing GPT-OSS Eval (Blackwell) for now (#25107)
1	1	.buildkite/test-pipeline.yaml

[1a456c7c9] Douglas Lehr 2025-09-17 Aiter mha fp8 fix (#24991)
2	2	vllm/attention/ops/rocm_aiter_paged_attn.py
2	2	vllm/v1/attention/backends/rocm_aiter_fa.py

[fedb75fa2] Alexander Matveev 2025-09-17 [Bugfix][B200] Fix `cutlass_mla` hang (#24966)
8	0	csrc/attention/mla/cutlass_sm100_mla/device/sm100_mla.hpp

[bff2e5f1d] Andrew Xia 2025-09-17 [gpt-oss][2] fix types for streaming (#24556)
5	4	vllm/entrypoints/openai/api_server.py
33	4	vllm/entrypoints/openai/protocol.py
66	88	vllm/entrypoints/openai/serving_responses.py

[3c068c637] czhu-cohere 2025-09-17 [Kernel] Faster pre-processing time for W4A8 (#23972)
71	1	csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu

[f20c3b095] ahao-anyscale 2025-09-17 [BUG] Exclude .pth files when pulling remote files  (#25092)
8	4	vllm/config/__init__.py

[883131544] Mohammad Miadh Angkad 2025-09-18 [Bugfix] Update import path for bc_linter_include (#24766)
1	1	vllm/v1/core/sched/output.py

[ee5fd4915] Yihua Cheng 2025-09-17 [Misc] Update owners for KV connector and V1 offloading (#25041)
7	4	.github/CODEOWNERS

[7ae988754] afeldman-nm 2025-09-17 [V1] Logits processor docs (#22919)
559	0	docs/design/logits_processors.md
46	0	docs/features/custom_arguments.md
445	0	docs/features/custom_logitsprocs.md
4	6	examples/offline_inference/logits_processor/custom.py
4	3	tests/v1/logits_processors/utils.py
3	3	vllm/v1/sample/logits_processor/interface.py
4	4	vllm/v1/sample/logits_processor/state.py

[e3db5ebb6] Michael Goin 2025-09-17 [CI Bugfix] Fix failing test_model_load_with_params tests due to tokenizer refactor (#25086)
5	6	tests/model_executor/test_model_load_with_params.py

[9d442b7c4] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove V0 tests in test_sequence.py (#25088)
1	96	tests/test_sequence.py

[eb68c2dcd] Woosuk Kwon 2025-09-17 [CI] Revert back prepare_prompts and check_answers (#25087)
1	2	tests/models/test_transformers.py
47	0	tests/utils.py
1	2	tests/v1/e2e/test_correctness_sliding_window.py

[8b32464ac] Michael Goin 2025-09-17 Change log level from info to debug for IOProcessor (#24999)
1	1	vllm/plugins/io_processors/__init__.py

[99cc41ad5] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove unused output processor util (#25023)
0	28	vllm/engine/output_processor/util.py

[d6a518fdd] Simon Mo 2025-09-17 Remove unused find_cuda_init helper script (#25044)
0	36	find_cuda_init.py

[4aa8c7b04] Simon Mo 2025-09-17 cleanup: remove adapter commons  (#25045)
0	1	pyproject.toml
0	0	vllm/adapter_commons/__init__.py
0	16	vllm/adapter_commons/layers.py
0	106	vllm/adapter_commons/models.py
0	26	vllm/adapter_commons/request.py
0	93	vllm/adapter_commons/utils.py
0	39	vllm/adapter_commons/worker_manager.py
8	3	vllm/lora/layers/utils.py
52	25	vllm/lora/models.py
2	4	vllm/lora/request.py
27	17	vllm/lora/worker_manager.py

[4b946d693] Woosuk Kwon 2025-09-17 [V0 Deprecation] Remove V0 Core tests (#25082)
0	11	.buildkite/test-pipeline.yaml
0	0	tests/core/__init__.py
0	0	tests/core/block/__init__.py
0	15	tests/core/block/conftest.py
0	0	tests/core/block/e2e/__init__.py
0	71	tests/core/block/e2e/conftest.py
0	479	tests/core/block/e2e/test_correctness.py
0	185	tests/core/block/e2e/test_correctness_sliding_window.py
0	341	tests/core/block/test_block_manager.py
0	577	tests/core/block/test_block_table.py
0	45	tests/core/block/test_common.py
0	96	tests/core/block/test_cpu_gpu_block_allocator.py
0	148	tests/core/block/test_naive_block.py
0	1035	tests/core/block/test_prefix_caching_block.py
0	12	tests/core/conftest.py
0	858	tests/core/test_chunked_prefill_scheduler.py
0	67	tests/core/test_num_computed_tokens_update.py
0	1338	tests/core/test_scheduler.py
0	36	tests/core/test_serialization.py
0	392	tests/core/utils.py

[087c6ffc9] Michael Goin 2025-09-17 [CI Bugfix] Fix failing test_invalid_env (#25078)
1	1	tests/kernels/attention/test_attention_selector.py

[4a2d33e37] samzong 2025-09-17 [Docs] vllm/benchmarks/datasets.py fix docstring param format. (#24970)
5	5	vllm/benchmarks/datasets.py

[8f3616f42] Matthew Bonanni 2025-09-17 Remove old cutlass mla (#23961)
0	2	CMakeLists.txt
0	38	csrc/attention/mla/cutlass_mla_entry.cu
0	225	csrc/attention/mla/cutlass_mla_kernels.cu
0	7	csrc/torch_bindings.cpp
0	9	vllm/_custom_ops.py
10	64	vllm/v1/attention/backends/mla/cutlass_mla.py

[47f670b03] samzong 2025-09-17 [Docs] improve code formatting and comments for eliminate griffe build warning. (#25010)
1	1	vllm/benchmarks/serve.py
5	4	vllm/distributed/eplb/eplb_state.py
14	9	vllm/distributed/eplb/rebalance_algo.py

[dd6a910aa] Tao He 2025-09-17 [Bugfix][Qwen3-Next] fixes the varlen issue in qwen3-next's MTP implementation. (#24957)
116	16	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
3	7	vllm/model_executor/models/qwen3_next.py
20	11	vllm/v1/attention/backends/gdn_attn.py

[1b962e245] dolpm 2025-09-17 [fix] lora benchmarks pass no_lora_flag_cpu (#23774)
7	1	benchmarks/kernels/benchmark_lora.py

[bfe938016] Aidyn-A 2025-09-17 Apply fixes for CUDA 13 (#24599)
10	0	CMakeLists.txt
17	0	csrc/cub_helpers.h
4	9	csrc/layernorm_kernels.cu
4	9	csrc/layernorm_quant_kernels.cu
3	13	csrc/moe/topk_softmax_kernels.cu
2	9	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
2	7	csrc/quantization/fp8/common.cu
5	9	csrc/quantization/fused_kernels/layernorm_utils.cuh

[9fccd04e3] Li, Jiang 2025-09-17 [Bugfix] Fix Stream usage in CPU model runner and OneDNN kernel check (#25046)
1	1	csrc/cpu/dnnl_kernels.cpp
5	0	vllm/platforms/cpu.py
8	0	vllm/v1/worker/cpu_model_runner.py

[252ada555] danielafrimi 2025-09-17 Add RADIO Vision Encoder Support to vLLM (#24595)
86	0	tests/models/multimodal/pooling/test_radio.py
60	58	vllm/model_executor/models/nano_nemotron_vl.py
576	0	vllm/model_executor/models/radio.py
2	0	vllm/transformers_utils/configs/__init__.py
104	0	vllm/transformers_utils/configs/radio.py

[e120533d7] Cyrus Leung 2025-09-17 [Misc] Avoid use of deprecated `AutoModelForVision2Seq` (#25065)
7	7	tests/models/multimodal/generation/test_common.py

[2b8569703] Shijun Yin 2025-09-17 [BugFix] enable DOTALL to match multi-line tool_call parameters in extract_tool_call_required_streaming (#24668)
1	1	vllm/entrypoints/openai/serving_chat.py

[544fe76b9] Chauncey 2025-09-17 [Frontend] Support returning all prompt logprobs (#24956)
22	0	tests/entrypoints/openai/test_chat_echo.py
16	8	vllm/entrypoints/openai/protocol.py

[bb58dc8c2] Xinyu Chen 2025-09-17 [DP] Create placement groups by ray_device_key (#25026)
11	8	vllm/v1/engine/utils.py

[0fb2551c2] Michael Yao 2025-09-17 [Docs] Fix griffe warning in base_static_graph.py (#25018)
8	3	vllm/compilation/base_static_graph.py

[6c47f6bfa] Zhuohan Li 2025-09-17 [Core] Remove tokenizer group in vLLM (#24078)
1	7	tests/detokenizer/test_stop_checker.py
2	5	tests/engine/test_stop_checker.py
0	22	tests/entrypoints/conftest.py
2	7	tests/entrypoints/openai/test_chat.py
4	19	tests/entrypoints/openai/test_completion.py
1	4	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
1	9	tests/entrypoints/openai/test_lora_adapters.py
0	2	tests/entrypoints/openai/test_models.py
10	18	tests/entrypoints/openai/test_tokenization.py
2	0	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
10	46	tests/entrypoints/test_chat_utils.py
4	24	tests/lora/test_llama_tp.py
0	135	tests/lora/test_lora_allowed_token_ids.py
2	27	tests/lora/test_quant_model.py
0	72	tests/lora/test_tokenizer_group.py
3	8	tests/test_cache_block_hashing.py
6	10	tests/tokenization/test_detokenize.py
0	27	tests/tokenization/test_tokenizer_group.py
4	0	tests/tokenization/test_tokenizer_registry.py
2	6	tests/v1/engine/conftest.py
5	5	tests/v1/engine/test_output_processor.py
2	4	tests/v1/engine/utils.py
1	1	tests/v1/entrypoints/llm/test_struct_output_generate.py
80	93	vllm/benchmarks/datasets.py
4	11	vllm/engine/async_llm_engine.py
14	43	vllm/engine/llm_engine.py
2	4	vllm/engine/output_processor/interfaces.py
1	4	vllm/engine/output_processor/stop_checker.py
3	7	vllm/engine/protocol.py
6	14	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	4	vllm/entrypoints/openai/serving_classification.py
1	2	vllm/entrypoints/openai/serving_completion.py
3	4	vllm/entrypoints/openai/serving_embedding.py
1	2	vllm/entrypoints/openai/serving_pooling.py
1	1	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/entrypoints/openai/serving_score.py
2	2	vllm/entrypoints/openai/serving_tokenization.py
18	66	vllm/inputs/preprocess.py
8	15	vllm/transformers_utils/detokenizer.py
16	17	vllm/transformers_utils/tokenizer.py
5	0	vllm/transformers_utils/tokenizer_base.py
0	132	vllm/transformers_utils/tokenizer_group.py
4	0	vllm/transformers_utils/tokenizers/mistral.py
5	10	vllm/v1/engine/async_llm.py
4	6	vllm/v1/engine/llm_engine.py
2	6	vllm/v1/engine/output_processor.py
29	26	vllm/v1/engine/processor.py
2	5	vllm/v1/structured_output/__init__.py

[c15309a73] whx 2025-09-17 [Model] Apply SharedFusedMoE to glm4_moe. (#24849)
55	30	vllm/model_executor/models/glm4_moe.py

[4a9375fe9] whx 2025-09-17 [Model] Pass param prefix to LLMHead (#24862)
1	0	vllm/model_executor/models/arctic.py
1	0	vllm/model_executor/models/aria.py
4	2	vllm/model_executor/models/baichuan.py
1	0	vllm/model_executor/models/bamba.py
3	1	vllm/model_executor/models/bloom.py
1	0	vllm/model_executor/models/chameleon.py
1	0	vllm/model_executor/models/dbrx.py
6	3	vllm/model_executor/models/deepseek.py
2	1	vllm/model_executor/models/deepseek_eagle.py
6	3	vllm/model_executor/models/deepseek_v2.py
3	1	vllm/model_executor/models/dots1.py
3	1	vllm/model_executor/models/ernie45_moe.py
3	1	vllm/model_executor/models/ernie45_vl_moe.py
2	1	vllm/model_executor/models/ernie_mtp.py
1	0	vllm/model_executor/models/exaone.py
1	0	vllm/model_executor/models/exaone4.py
1	0	vllm/model_executor/models/falcon.py
1	0	vllm/model_executor/models/falcon_h1.py
3	1	vllm/model_executor/models/glm4_moe.py
2	1	vllm/model_executor/models/gpt_bigcode.py
1	0	vllm/model_executor/models/gpt_j.py
1	0	vllm/model_executor/models/gpt_oss.py
1	0	vllm/model_executor/models/granite.py
1	0	vllm/model_executor/models/granitemoe.py
2	1	vllm/model_executor/models/hunyuan_v1.py
1	0	vllm/model_executor/models/idefics3.py
3	1	vllm/model_executor/models/jais.py
1	0	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/kimi_vl.py
1	1	vllm/model_executor/models/llama_eagle3.py
1	0	vllm/model_executor/models/mamba.py
1	0	vllm/model_executor/models/mamba2.py
3	0	vllm/model_executor/models/medusa.py
2	1	vllm/model_executor/models/mimo_mtp.py
1	0	vllm/model_executor/models/minicpm.py
1	0	vllm/model_executor/models/minicpm_eagle.py
1	0	vllm/model_executor/models/minimax_text_01.py
1	0	vllm/model_executor/models/mixtral.py
1	0	vllm/model_executor/models/molmo.py
1	0	vllm/model_executor/models/nemotron.py
1	0	vllm/model_executor/models/nemotron_h.py
1	0	vllm/model_executor/models/olmo.py
2	1	vllm/model_executor/models/olmoe.py
3	1	vllm/model_executor/models/opt.py
2	1	vllm/model_executor/models/orion.py
2	1	vllm/model_executor/models/persimmon.py
2	1	vllm/model_executor/models/phi.py
1	0	vllm/model_executor/models/phi4flash.py
1	0	vllm/model_executor/models/phi4mm.py
1	0	vllm/model_executor/models/phimoe.py
2	1	vllm/model_executor/models/qwen.py
2	1	vllm/model_executor/models/qwen2_moe.py
2	1	vllm/model_executor/models/qwen3_moe.py
1	1	vllm/model_executor/models/qwen3_next.py
2	1	vllm/model_executor/models/qwen3_next_mtp.py
1	0	vllm/model_executor/models/solar.py
3	1	vllm/model_executor/models/step3_text.py
1	0	vllm/model_executor/models/zamba2.py

[03191cd8f] Lukas Geiger 2025-09-17 [Core][MultiModalHasher] Hash images without converting image mode (#24969)
6	3	vllm/multimodal/hasher.py

[b77bf34e5] rouchenzi 2025-09-17 [EPLB] Support EPLB for Mixtral Model (#22842)
137	23	vllm/model_executor/models/mixtral.py

[dd39baf71] Kunshang Ji 2025-09-17 [XPU] Fix xpu model runner call torch.cuda APIs (#25011)
5	1	vllm/v1/worker/xpu_model_runner.py

[43a62c51b] Daniel Serebrenik 2025-09-17 Add more documentation and improve usability of lognormal dist (benchmark_serving_multi_turn) (#23255)
101	0	benchmarks/multi_turn/README.md
100	5	benchmarks/multi_turn/bench_dataset.py
2	3	benchmarks/multi_turn/generate_multi_turn.json

[ca2d1925e] haoyangli-amd 2025-09-17 [Rocm] [quantization] Fix quark ptpc moe and add test case (#24649)
25	0	tests/quantization/test_quark.py
171	52	vllm/model_executor/layers/quantization/quark/quark_moe.py

[0f7acdd73] Roger Wang 2025-09-16 [Model] Support Qwen3-VL Model Series (#24727)
2	0	docs/models/supported_models.md
78	0	examples/offline_inference/vision_language.py
34	1	tests/models/multimodal/processing/test_common.py
6	0	tests/models/registry.py
2	0	vllm/model_executor/layers/rotary_embedding/__init__.py
133	11	vllm/model_executor/layers/rotary_embedding/mrope.py
1	1	vllm/model_executor/models/qwen2.py
1	1	vllm/model_executor/models/qwen2_vl.py
1	1	vllm/model_executor/models/qwen3_moe.py
1478	0	vllm/model_executor/models/qwen3_vl.py
344	0	vllm/model_executor/models/qwen3_vl_moe.py
3	1	vllm/model_executor/models/registry.py
1	1	vllm/multimodal/video.py

[5801e4977] Woosuk Kwon 2025-09-16 [V0 Deprecation] Remove MQLLMEngine (#25019)
0	2	.buildkite/test-pipeline.yaml
2	2	tests/entrypoints/openai/test_lora_resolvers.py
7	7	tests/entrypoints/openai/test_serving_chat.py
0	0	tests/mq_llm_engine/__init__.py
0	12	tests/mq_llm_engine/conftest.py
0	69	tests/mq_llm_engine/test_abort.py
0	376	tests/mq_llm_engine/test_error_handling.py
0	59	tests/mq_llm_engine/test_load.py
0	81	tests/mq_llm_engine/utils.py
0	145	vllm/engine/multiprocessing/__init__.py
0	643	vllm/engine/multiprocessing/client.py
0	470	vllm/engine/multiprocessing/engine.py
0	2	vllm/entrypoints/launcher.py
2	100	vllm/entrypoints/openai/api_server.py
1	1	vllm/platforms/rocm.py

[58d4c705a] Russell Bryant 2025-09-16 [Core] Get num_encoder_tokens from scheduler config (#24989)
2	3	vllm/v1/core/sched/scheduler.py
2	3	vllm/v1/kv_cache_interface.py
2	2	vllm/v1/worker/gpu_model_runner.py

[ea3de5ef0] Prashant Gupta 2025-09-16 [misc] fix typo in value error (#24995)
1	1	vllm/entrypoints/renderer.py

[67532a1a6] Michael Goin 2025-09-16 [UX] Remove "quantization is not fully optimized yet" log (#25012)
0	21	vllm/config/__init__.py

[5672ba90b] yyzxw 2025-09-17 [Docs] fix invalid doc link (#25017)
1	1	docs/contributing/model/README.md

[dd83a157f] Michael Goin 2025-09-16 [UX] Enforce valid choices for envs like VLLM_ATTENTION_BACKEND, etc (#24761)
77	24	vllm/envs.py

[5a411ef6c] Isotr0py 2025-09-17 [Benchmarks] Add MMVU video dataset support and clean up deprecated datasets (#24719)
0	1288	benchmarks/benchmark_dataset.py
1	0	docs/contributing/benchmarks.md
64	2	vllm/benchmarks/datasets.py

[eeb135eb8] Nick Hill 2025-09-16 [Core] Use `CpuGpuBuffer` for block table tensors (#24795)
1	1	tests/v1/tpu/worker/test_tpu_model_runner.py
4	1	tests/v1/worker/test_gpu_input_batch.py
1	1	tests/v1/worker/test_gpu_model_runner.py
32	43	vllm/v1/worker/block_table.py
4	4	vllm/v1/worker/cpu_model_runner.py
11	13	vllm/v1/worker/gpu_model_runner.py

[3059b9cc6] elvischenv 2025-09-17 [Doc] Add --force-overwrite option to generate_cmake_presets.py (#24375)
10	0	docs/contributing/incremental_build.md
21	8	tools/generate_cmake_presets.py

[64ad55187] Benjamin Bartels 2025-09-17 Removes source compilation of nixl dependency (#24874)
16	4	docker/Dockerfile
2	2	docs/serving/expert_parallel_deployment.md
2	1	requirements/kv_connectors.txt
57	0	tools/install_gdrcopy.sh
0	109	tools/install_nixl.sh

[cef32104b] Tahsin Tunan 2025-09-17 [FP8] Extend per-token-group quantization support to QuantFP8 (#24342)
216	47	benchmarks/kernels/bench_per_token_quant_fp8.py
150	0	tests/kernels/quantization/test_fp8_quant_group.py
3	1	vllm/model_executor/layers/fused_moe/fused_moe.py
66	13	vllm/model_executor/layers/quantization/input_quant_fp8.py
9	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[493b10f8b] Michael Goin 2025-09-16 [CI] GPT-OSS GPQA eval test for Blackwell (#24920)
14	0	.buildkite/test-pipeline.yaml
2	0	tests/evals/gpt_oss/__init__.py
18	0	tests/evals/gpt_oss/conftest.py
102	0	tests/evals/gpt_oss/test_gpqa_correctness.py

[d119fc861] Matthew Bonanni 2025-09-16 [CI][Bugfix] Fix failing Blackwell test (#24993)
2	5	vllm/model_executor/layers/fused_moe/modular_kernel.py

[dbebb7f81] Michael Goin 2025-09-16 [Perf] Reuse workspace for FP8+FP4 Marlin MoE (#20500)
4	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	1	vllm/model_executor/layers/quantization/fp8.py
2	1	vllm/model_executor/layers/quantization/modelopt.py

[3053a22b3] Aleksandr Malyshev 2025-09-16 fp8 kv cache support fix for torch.compile (#22758)
3	1	vllm/model_executor/layers/quantization/kv_cache.py
1	1	vllm/v1/attention/backends/triton_attn.py

[02d4b8545] Andrew Sansom 2025-09-16 Use kwargs for long lists of `EngineCoreRequest` arguments in tests and fix extra kwargs (#24987)
8	10	tests/detokenizer/test_min_tokens.py
8	8	tests/tokenization/test_detokenize.py

[86daa875f] Andrew Xia 2025-09-16 [gpt-oss][1][bugfix] fix streaming final output (#24466)
2	0	tests/entrypoints/openai/test_response_api_with_harmony.py
79	4	tests/entrypoints/test_context.py
10	1	vllm/entrypoints/context.py

[dcf2f3ec0] Concurrensee 2025-09-16 [ROCm] Add dependencies for ROCm (#24900)
1	0	requirements/rocm-build.txt
1	0	requirements/rocm-test.txt
2	1	requirements/rocm.txt

[218454b9b] Chen Zhang 2025-09-16 [MISC] Add code owners of vllm/v1 to vllm/v1/core (#24928)
2	2	.github/CODEOWNERS

[f4d6eb95c] Andrew Xia 2025-09-16 [gpt-oss][1b] streaming add item id, content id (#24788)
23	0	tests/entrypoints/openai/test_response_api_with_harmony.py
8	2	vllm/entrypoints/openai/serving_responses.py

[cd1f885bc] Sugar 2025-09-17 Directly get max encoder len from VLLM config in V1 (#24866)
7	4	vllm/attention/layers/cross_attention.py

[d593cf28f] Isotr0py 2025-09-17 [Misc] Add removed encoder-decoder models to previously supported models list (#24961)
11	1	vllm/model_executor/models/registry.py

[faa7a5daa] lianyibo 2025-09-17 [Bugfix] Fix unable to run encoder model when disable_hybrid_kv_cache_manager is true (#24571)
4	0	vllm/v1/core/kv_cache_utils.py

[567939953] Sage Moore 2025-09-16 [Core/DBO][1/N] Add Dual-Batch Overlap mechanism to VLLM (#23693)
8	0	examples/offline_inference/data_parallel.py
5	5	tests/v1/attention/test_attention_splitting.py
6	2	tests/v1/spec_decode/test_eagle.py
8	0	vllm/config/__init__.py
8	0	vllm/config/parallel.py
0	5	vllm/distributed/device_communicators/all2all.py
10	0	vllm/engine/arg_utils.py
101	20	vllm/forward_context.py
14	12	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
29	16	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
43	15	vllm/model_executor/layers/fused_moe/layer.py
48	15	vllm/model_executor/layers/fused_moe/modular_kernel.py
18	25	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
9	9	vllm/v1/attention/backends/utils.py
8	4	vllm/v1/spec_decode/eagle.py
7	2	vllm/v1/worker/cpu_model_runner.py
236	39	vllm/v1/worker/gpu_model_runner.py
303	0	vllm/v1/worker/gpu_ubatch_wrapper.py
155	0	vllm/v1/worker/ubatch_splitting.py
19	0	vllm/v1/worker/ubatch_utils.py
211	0	vllm/v1/worker/ubatching.py
9	1	vllm/v1/worker/utils.py

[08369289a] Lukas Geiger 2025-09-16 [Core][MultiModalHasher] Don't convert memoryviews to bytes during hashing (#24925)
18	29	vllm/multimodal/hasher.py

[73cfb3c5e] Chih-Chieh Yang 2025-09-16 [Model] Clean up and simplify Mamba2 Metadata Usage in both V0 and V1 (#24331)
24	40	vllm/model_executor/layers/mamba/mamba2_metadata.py
10	19	vllm/model_executor/layers/mamba/mamba_mixer2.py
11	18	vllm/model_executor/models/plamo2.py

[4e5affeaa] Ming Yang 2025-09-16 [CI] Add Decode Context Parallelism (DCP) test to CI (#24487)
14	3	.buildkite/test-pipeline.yaml
7	4	tests/distributed/test_context_parallel.py

[e4f0b4cd9] TeeKen Lau 2025-09-16 (doc): set cmake c++ compatible standard when building on MacOS CPU. (#23483)
18	0	docs/getting_started/installation/cpu/apple.inc.md

[de3e53a75] liangwen12year 2025-09-16 feat: Add Grafana and Perces monitoring dashboards for vLLM (#23498)
87	0	examples/online_serving/dashboards/README.md
59	0	examples/online_serving/dashboards/grafana/README.md
1405	0	examples/online_serving/dashboards/grafana/performance_statistics.json
760	0	examples/online_serving/dashboards/grafana/query_statistics.json
48	0	examples/online_serving/dashboards/perses/README.md
764	0	examples/online_serving/dashboards/perses/performance_statistics.yaml
392	0	examples/online_serving/dashboards/perses/query_statistics.yaml

[85e0df139] Ye (Charlotte) Qi 2025-09-16 [Docs] move benchmarks README to contributing guides (#24820)
13	867	benchmarks/README.md
787	7	docs/contributing/benchmarks.md

[0faf3cc3e] Harry Mellor 2025-09-16 Move `SpeculativeConfig` from `config/__init__.py` to `config/speculative.py` (#24904)
2	523	vllm/config/__init__.py
554	0	vllm/config/speculative.py

[7ea5c73ad] Chen Bruce 2025-09-16 [Feat][EPLB] A novel static EPLB placement strategy for MoE models. (#23745)
194	0	tests/distributed/test_expert_placement.py
10	0	vllm/config/parallel.py
7	0	vllm/engine/arg_utils.py
54	12	vllm/model_executor/layers/fused_moe/layer.py

[27fcfe7bc] tomeras91 2025-09-16 [Mamba] Support TP>1 with quantization for mamba2 mixer in case `n_groups % tp_size == 0` (#24593)
119	84	vllm/model_executor/layers/mamba/mamba_mixer2.py

[68dbde5db] Cheng Kuan Yong Jason 2025-09-16 [Bugfix] remove duplicate tokens streamed in required tool choice streaming (#23312)
0	3	vllm/entrypoints/openai/serving_chat.py

[04ad0dc27] Jee Jee Li 2025-09-16 [benchmark] Add triton version in the moe tuned config (#24769)
1	1	benchmarks/kernels/benchmark_moe.py
4	1	vllm/model_executor/layers/fused_moe/fused_moe.py
1	1	vllm/triton_utils/importing.py

[238c4c170] Saman A. Pour 2025-09-15 [QWEN NEXT] Fused MoE kernels Optimization configs (#24924)
147	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_GB200,dtype=fp8_w8a8.json

[8c5461026] vllmellm 2025-09-16 [Bug] [Spec Dec]: Fix kv_cache dtype mismatch for Eagle3 drafter on FP8 target (#24505)
9	2	vllm/model_executor/models/llama_eagle3.py

[17871983a] cascade 2025-09-15 [Bugfix] Fix sequence parallelism bug when enable pipeline parallelism (#24021)
2	1	tests/distributed/test_sequence_parallel.py
46	5	vllm/distributed/parallel_state.py
15	3	vllm/v1/worker/cpu_worker.py
33	30	vllm/v1/worker/gpu_model_runner.py
13	2	vllm/v1/worker/gpu_worker.py
26	1	vllm/v1/worker/utils.py

[759ef49b1] Woosuk Kwon 2025-09-15 Remove V0 Encoder-Decoder Support (#24907)
0	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh
0	9	.buildkite/test-pipeline.yaml
0	1	docs/contributing/model/multimodal.md
0	8	docs/models/supported_models.md
1	1	docs/usage/v1_guide.md
0	311	examples/offline_inference/dolphin.py
0	195	examples/offline_inference/encoder_decoder.py
1	113	examples/offline_inference/encoder_decoder_multimodal.py
0	62	examples/offline_inference/vision_language.py
0	21	examples/offline_inference/vision_language_multi_image.py
1	154	tests/core/block/test_block_manager.py
0	105	tests/core/test_scheduler_encoder_decoder.py
0	3	tests/distributed/test_pipeline_parallel.py
0	0	tests/encoder_decoder/__init__.py
0	131	tests/encoder_decoder/test_e2e_correctness.py
0	56	tests/entrypoints/openai/test_encoder_decoder.py
0	199	tests/entrypoints/test_chat_utils.py
0	1105	tests/kernels/attention/test_encoder_decoder_attn.py
0	222	tests/models/language/generation/test_bart.py
0	123	tests/models/language/generation/test_mbart.py
0	147	tests/models/multimodal/generation/test_florence2.py
0	768	tests/models/multimodal/generation/test_mllama.py
0	5	tests/models/multimodal/processing/test_common.py
0	72	tests/models/multimodal/processing/test_mllama.py
0	1	tests/models/multimodal/processing/test_tensor_schema.py
1	15	tests/models/registry.py
0	4	tests/models/test_initialization.py
0	1	tests/models/test_registry.py
1	2	tests/test_config.py
0	28	tests/utils_/test_utils.py
0	21	tests/v1/test_oracle.py
0	648	tests/worker/test_encoder_decoder_model_runner.py
2	9	vllm/config/__init__.py
1	1	vllm/engine/llm_engine.py
0	1319	vllm/model_executor/models/bart.py
0	381	vllm/model_executor/models/donut.py
0	1097	vllm/model_executor/models/florence2.py
0	1697	vllm/model_executor/models/mllama.py
2	9	vllm/model_executor/models/registry.py
1	1	vllm/multimodal/profiling.py
0	1	vllm/test_utils.py
0	1	vllm/transformers_utils/chat_templates/registry.py
0	5	vllm/transformers_utils/config.py
1	1	vllm/v1/engine/processor.py
0	553	vllm/worker/enc_dec_model_runner.py
0	49	vllm/worker/utils.py
1	5	vllm/worker/worker.py

[5206ab20b] Kunshang Ji 2025-09-16 [XPU] Fix circular import error.  (#24927)
5	1	vllm/platforms/xpu.py

[0af3ce135] Lu Fang 2025-09-15 Upgrade flashinfer to 0.3.1 (#24470)
2	2	docker/Dockerfile.nightly_torch

[e1279ef00] Richard Zou 2025-09-15 [Docs] Update instructions for how to using existing torch binary (#24892)
13	1	docs/getting_started/installation/gpu/cuda.inc.md

[2942970d4] Mark McLoughlin 2025-09-16 [Metrics] Hide deprecated metrics with gpu_ prefix (#24245)
15	6	tests/entrypoints/openai/test_metrics.py
48	40	vllm/v1/metrics/loggers.py

[3c96e7b8a] Wentao Ye 2025-09-15 [CI] Small Accuracy Eval Test for Deepseek Model (#24259)
6	0	tests/evals/gsm8k/configs/DeepSeek-V2-Lite-Instruct-FP8.yaml
1	0	tests/evals/gsm8k/configs/models-small.txt

[b42566f44] Wentao Ye 2025-09-15 [Bug] Fix `is_flashmla_supported` Check Error (#24774)
2	13	vllm/attention/backends/flashmla.py
2	13	vllm/v1/attention/backends/mla/flashmla.py

[d96e11167] Reza Barazesh 2025-09-15 Add pytest-cov and .coveragerc (#24778)
32	0	.coveragerc
1	0	requirements/test.in
8	1	requirements/test.txt

[2891603ef] Gregory Shtrasberg 2025-09-15 [ROCm][Bugfix] Fix the case where there's bias (#24895)
31	0	tests/kernels/quantization/test_rocm_skinny_gemms.py
1	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[de2cc3d86] Wentao Ye 2025-09-15 [Deprecation] Remove DeepGEMM Old Symbol Wrapper (#24902)
3	25	vllm/utils/deep_gemm.py

[e95084308] Michael Goin 2025-09-15 Updated CODEOWNERS for flashinfer, mla, fused_moe (#24906)
7	2	.github/CODEOWNERS

[7f6f2c118] Sergio Paniego Blanco 2025-09-16 `HuggingFace` -> `Hugging Face` in `Integration with Hugging Face` docs (#24889)
11	11	docs/design/huggingface_integration.md

[5bcc153d7] Jiangyun Zhu 2025-09-16 [Compile] Fix noop_elimination pass and add tests for noop_elimination (#24880)
1	0	.buildkite/test-pipeline.yaml
5	1	tests/compile/backend.py
106	0	tests/compile/test_noop_elimination.py
18	22	vllm/compilation/noop_elimination.py

[45bfa49cb] Mickaël Seznec 2025-09-15 [Tests] fix initialization of kv hash in tests (#24273)
10	7	tests/v1/core/test_kv_cache_utils.py
10	5	tests/v1/core/test_prefix_caching.py

[fd2f10546] Simon Mo 2025-09-15 [ci] fix wheel names for arm wheels (#24898)
4	12	.buildkite/release-pipeline.yaml
22	7	.buildkite/scripts/annotate-release.sh
2	0	docker/Dockerfile
2	4	setup.py
6	0	vllm/envs.py

[e757a629e] Wentao Ye 2025-09-15 [Bug] Fix Cutlass Scaled MM Compilation Error (#24887)
23	19	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh
15	11	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh
15	11	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh

[aae725af7] Alexander Matveev 2025-09-15 [Performance] Remove redundant clone() calls in cutlass_mla (#24891)
8	8	vllm/v1/attention/backends/mla/cutlass_mla.py

[73df49ef3] Andrew Xia 2025-09-15 [gpt-oss][1a] create_responses stream outputs BaseModel type, api server is SSE still (#24759)
21	6	vllm/entrypoints/openai/api_server.py
69	65	vllm/entrypoints/openai/serving_responses.py

[25aba2b6a] Andrew Xia 2025-09-15 [gpt-oss] Add IncompleteDetails to ResponsesRepsonse (#24561)
14	0	tests/entrypoints/openai/test_response_api_with_harmony.py
15	10	vllm/entrypoints/context.py
3	1	vllm/entrypoints/harmony_utils.py
13	4	vllm/entrypoints/openai/protocol.py
15	3	vllm/entrypoints/openai/serving_responses.py
4	4	vllm/v1/core/sched/utils.py
3	3	vllm/v1/engine/output_processor.py

[94b03f88d] Benjamin Bartels 2025-09-15 Bump Flashinfer to 0.3.1 (#24868)
1	1	docker/Dockerfile
1	1	setup.py

[49bfc538e] Sage Moore 2025-09-15 Update num_tokens_across_dp to use nccl instead of gloo (#24105)
8	0	vllm/envs.py
17	4	vllm/forward_context.py

[a0b26701c] Kyle Sayers 2025-09-15 [Transform] Deterministic Hadacore Transforms (#24106)
11	0	CMakeLists.txt
2	0	csrc/ops.h
817	0	csrc/quantization/hadamard/hadacore/hadamard_transform_cuda.cu
3	0	csrc/torch_bindings.cpp
25	0	tests/kernels/quantization/test_hadacore.py
24	0	vllm/_custom_ops.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
18	7	vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py
47	29	vllm/model_executor/layers/quantization/compressed_tensors/transform/module.py
31	6	vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4.py

[c4afdb69c] Harry Mellor 2025-09-15 Move `MultiModalConfig` from `config/__init__.py` to `config/multimodal.py` (#24659)
1	1	tests/entrypoints/openai/test_lora_resolvers.py
1	1	tests/entrypoints/openai/test_serving_chat.py
12	9	tests/multimodal/test_cache.py
2	2	tests/test_config.py
1	1	tests/v1/engine/test_processor_multi_modal_uuids.py
56	221	vllm/config/__init__.py
120	0	vllm/config/multimodal.py
18	0	vllm/config/utils.py
8	6	vllm/engine/arg_utils.py
6	3	vllm/entrypoints/chat_utils.py
4	3	vllm/model_executor/models/transformers.py

[b834b4cbf] Rafael Marcelino Koike 2025-09-15 [USAGE] Improve error handling for weight initialization in Unquantized… (#20321)
21	4	vllm/attention/layer.py
22	4	vllm/model_executor/layers/linear.py

[740f0647b] Harry Mellor 2025-09-15 Reinstate existing torch script (#24729)
18	2	use_existing_torch.py

[01413e0cf] xiao-llm 2025-09-15 Fp8 paged attention update (#22222)
247	78	csrc/rocm/attention.cu
2	1	csrc/rocm/ops.h
2	1	csrc/rocm/torch_bindings.cpp
2	1	vllm/_custom_ops.py
6	0	vllm/envs.py

[0e219cd50] Isotr0py 2025-09-15 [Bugfix] Fix GLM4.1V multimodal processor with compatability for Transformers v4.56 (#24822)
9	1	examples/offline_inference/vision_language.py
6	3	tests/models/multimodal/processing/test_common.py
14	3	tests/models/multimodal/processing/test_glm4_1v.py
11	4	vllm/assets/video.py
50	38	vllm/model_executor/models/glm4_1v.py
28	21	vllm/multimodal/video.py

[72c99f2a7] ant-yy 2025-09-15 [Model]: support Ling2.0 (#24627)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
166	50	vllm/model_executor/models/bailing_moe.py
1	0	vllm/model_executor/models/registry.py

[bf214ca22] wang.yuqi 2025-09-15 [Misc] Fix examples openai_pooling_client.py  (#24853)
2	2	docs/models/pooling_models.md
1	1	docs/models/supported_models.md
5	5	docs/serving/openai_compatible_server.md
33	0	examples/offline_inference/pooling/README.md
0	0	examples/offline_inference/{ => pooling}/convert_model_to_seq_cls.py
0	0	examples/offline_inference/{ => pooling}/embed_jina_embeddings_v3.py
0	0	examples/offline_inference/{ => pooling}/embed_matryoshka_fy.py
0	0	examples/offline_inference/{ => pooling}/qwen3_reranker.py
1	1	examples/online_serving/openai_embedding_long_text/service.sh
43	0	examples/online_serving/pooling/README.md
0	0	examples/online_serving/{ => pooling}/cohere_rerank_client.py
0	0	examples/online_serving/{ => pooling}/jinaai_rerank_client.py
6	0	examples/online_serving/{ => pooling}/openai_chat_embedding_client_for_multimodal.py
5	0	examples/online_serving/{ => pooling}/openai_classification_client.py
5	0	examples/online_serving/{ => pooling}/openai_embedding_client.py
0	0	examples/online_serving/{ => pooling}/openai_embedding_matryoshka_fy.py
4	2	examples/online_serving/{ => pooling}/openai_pooling_client.py

[2e41f5abc] Nicolò Lucchesi 2025-09-15 [XPU] Set consistent default KV cache layout (#24745)
9	6	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
4	6	vllm/platforms/xpu.py
10	4	vllm/v1/attention/backends/utils.py

[bc0f6059a] Ning Xie 2025-09-15 [UT] enhance free kv cache block queue popleft_n (#24220)
4	0	tests/v1/core/test_kv_cache_utils.py

[8de261b04] Chao Lei 2025-09-15 [P/D]`kv_output_aggregator` support P TP > D TP (#23917)
11	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
7	0	vllm/executor/executor_base.py
3	0	vllm/v1/engine/core.py
0	3	vllm/v1/executor/multiproc_executor.py
0	2	vllm/v1/executor/ray_distributed_executor.py

[a0d8b9738] Nicolò Lucchesi 2025-09-15 [Misc] Own KVConnectors installation (#24867)
4	1	.github/CODEOWNERS

[59e17dd4a] Ning Xie 2025-09-15 [Misc] rename interval to max_recent_requests (#24229)
2	2	vllm/v1/core/kv_cache_utils.py

[4979eb79d] Didier Durand 2025-09-15 [Doc]: fix typos in various files (#24821)
1	1	.buildkite/nightly-benchmarks/nightly-descriptions.md
1	1	vllm/model_executor/layers/quantization/moe_wna16.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
1	1	vllm/model_executor/layers/sampler.py
1	1	vllm/model_executor/models/glm4_1v.py
1	1	vllm/model_executor/models/interns1.py
1	1	vllm/model_executor/models/ultravox.py

[a8c0f5997] bingchen-mi 2025-09-15 [Bugfix] MiDashengLM model contact error under concurrent testing (#24738)
17	11	vllm/model_executor/models/midashenglm.py

[f4a948f33] Ce Gao 2025-09-15 [Frontend] Skip `stop` in reasoning content (#14550)
228	0	tests/engine/test_stop_checker.py
15	2	vllm/engine/llm_engine.py
13	2	vllm/engine/output_processor/stop_checker.py

[3f3313981] Ning Xie 2025-09-15 [kv cache] update num_free_blocks in the end (#24228)
12	0	tests/v1/core/test_kv_cache_utils.py
2	1	vllm/v1/core/kv_cache_utils.py

[78818dd1b] Michael Yao 2025-09-15 [Docs] Have a try to improve frameworks/streamlit.md (#24841)
18	20	docs/deployment/frameworks/streamlit.md

[8e5cdcda4] Chen Zhang 2025-09-14 [Hybrid Allocator] Support Pipeline Parallel (#23974)
1	3	tests/distributed/test_pipeline_parallel.py
5	5	tests/models/test_initialization.py
289	81	tests/v1/core/test_kv_cache_utils.py
5	5	tests/v1/tpu/worker/test_tpu_model_runner.py
7	7	tests/v1/worker/test_gpu_model_runner.py
145	104	vllm/v1/core/kv_cache_utils.py
5	15	vllm/v1/engine/core.py

[90f3f7d73] wuhang 2025-09-15 [Spec Decoding]Support Spec Decoding Metrics in DP Mode (#24049)
6	10	vllm/v1/metrics/loggers.py
48	28	vllm/v1/spec_decode/metrics.py

[6dc8da5dc] Robert Shaw 2025-09-14 [Chore] Remove ipex_ops warning (#24835)
1	1	vllm/_ipex_ops.py

[79cbcab87] FengjinChen 2025-09-15 Force use C++17 globally to avoid compilation error (#24823)
4	0	CMakeLists.txt
0	1	cmake/utils.cmake

[ff6803593] Ye (Charlotte) Qi 2025-09-14 [Benchmarks] Throw usage error when using dataset-name random and dataset-path together (#24819)
22	0	vllm/benchmarks/datasets.py

[1177dd53e] co63oc 2025-09-15 fix type of sampling rate for encode_base64 (#24826)
1	1	vllm/multimodal/audio.py
1	1	vllm/multimodal/utils.py

[fc2dbcda8] Wentao Ye 2025-09-14 [Perf] Fix DeepGEMM Contiguous Layout Issue, 5.5% Throughput Improvement (#24783)
4	4	vllm/model_executor/layers/quantization/fp8.py

[fec347dee] Hyogeun Oh (오효근) 2025-09-14 [Misc] Improve `s3_utils` type hints with `BaseClient` (#24825)
6	3	vllm/transformers_utils/s3_utils.py

[cc3173ae9] Wenlong Wang 2025-09-14 [Multi Modal][Performance] Fused Q,K's apply_rope into one (#24511)
10	6	vllm/model_executor/models/qwen2_5_vl.py

[3e903b6cb] Woosuk Kwon 2025-09-13 [Chore] Minor simplification for non-PP path (#24810)
39	25	vllm/v1/worker/gpu_model_runner.py

[973c9d01d] Victor Ziliang Peng 2025-09-13 [Minor] Simplify duplicative device check for cuda (#24793)
1	2	vllm/platforms/cuda.py

[15b8fef45] TaoYu Chen 2025-09-13 Remove redundant assignment in xfer_buffers, This is a little fix (#24732)
0	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[cfa3234a5] Wenlong Wang 2025-09-13 [CI][Spec Decode] Adjust threshold for flaky ngram spec decoding test again (#24771)
2	2	tests/v1/e2e/test_spec_decode.py

[41ae4a1ea] Didier Durand 2025-09-13 [Doc]: fix typos in various files (#24798)
1	1	examples/offline_inference/tpu.py
1	1	vllm/core/block/naive_block.py
1	1	vllm/core/evictor.py
2	2	vllm/engine/metrics.py
2	2	vllm/model_executor/layers/linear.py
1	1	vllm/model_executor/layers/vocab_parallel_embedding.py
1	1	vllm/model_executor/models/ernie45_vl.py
2	2	vllm/model_executor/models/gemma3n_mm.py
1	1	vllm/model_executor/models/nemotron_vl.py
1	1	vllm/model_executor/models/phi4mm.py

[4dad72f0d] Russell Bryant 2025-09-13 [Misc] Correct an outdated comment. (#24765)
4	4	vllm/v1/engine/processor.py

[59d7ffc17] Michael Goin 2025-09-13 [CI Failure] Fix test_flashinfer_cutlass_mxfp4_mxfp8_fused_moe (#24750)
1	0	csrc/attention/mla/sm100_cutlass_mla_kernel.cu
2	2	tests/kernels/moe/test_mxfp4_moe.py

[1da0f1441] Lukas Geiger 2025-09-13 [Core][Multimodal] Cache `supports_kw` (#24773)
1	0	vllm/utils/__init__.py

[98229db24] Elvir Crnčević 2025-09-13 [Kernels][DP/EP] Optimize Silu Kernel for R1 (#24054)
636	38	benchmarks/kernels/benchmark_silu_mul_fp8_quant.py
6	0	csrc/ops.h
465	0	csrc/quantization/activation_kernels.cu
7	0	csrc/torch_bindings.cpp
71	33	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
87	60	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[dbeee3844] elvischenv 2025-09-13 [Perf] Use NVIDIA hardware-accelerated instruction for float to fp8_e4m3 quantization (#24757)
6	2	csrc/quantization/fp8/common.cuh
16	3	csrc/quantization/fp8/nvidia/quant_utils.cuh

[30498f2a6] Rakesh Asapanna 2025-09-13 [Doc]: Remove 404 hyperlinks (#24785)
3	3	docs/examples/README.md

[abc7989ad] Harry Mellor 2025-09-13 [Docs] Remove Neuron install doc as backend no longer exists (#24396)
1	1	README.md
1	1	docs/README.md
0	3	docs/features/README.md
13	13	docs/features/quantization/README.md
0	2	docs/getting_started/installation/.nav.yml
0	1	docs/getting_started/installation/README.md
0	147	docs/getting_started/installation/aws_neuron.md

[9a8966bcc] Hyogeun Oh (오효근) 2025-09-13 [Docs] Fix warnings in mkdocs build (continued) (#24791)
4	4	vllm/distributed/eplb/eplb_state.py
3	3	vllm/distributed/eplb/rebalance_algo.py
3	2	vllm/distributed/kv_transfer/kv_connector/v1/base.py
3	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
4	3	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
3	2	vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool.py
4	3	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	2	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
1	1	vllm/model_executor/models/interfaces.py
3	9	vllm/model_executor/models/keye.py
6	5	vllm/model_executor/models/keye_vl1_5.py
3	1	vllm/model_executor/models/llava.py
3	2	vllm/model_executor/models/llava_next.py
3	1	vllm/model_executor/models/mistral3.py
4	5	vllm/model_executor/models/mllama4.py
14	4	vllm/model_executor/models/moonvit.py
2	2	vllm/model_executor/models/phi4_multimodal.py
3	9	vllm/model_executor/models/qwen2_vl.py
8	19	vllm/model_executor/models/siglip2navit.py
5	4	vllm/model_executor/models/ultravox.py
2	2	vllm/model_executor/models/zamba2.py
7	6	vllm/transformers_utils/config.py
4	4	vllm/transformers_utils/configs/jais.py
0	4	vllm/transformers_utils/configs/ultravox.py
4	5	vllm/transformers_utils/processors/deepseek_vl2.py
1	3	vllm/transformers_utils/runai_utils.py
3	3	vllm/transformers_utils/s3_utils.py

[5febdc875] Woosuk Kwon 2025-09-13 [Chore] Remove unused batched RoPE op & kernel (#24789)
0	6	csrc/ops.h
0	122	csrc/pos_encoding_kernels.cu
0	10	csrc/torch_bindings.cpp
1	146	tests/kernels/core/test_pos_encoding.py
6	16	tests/kernels/core/test_rotary_embedding.py
0	10	vllm/_custom_ops.py
0	11	vllm/_ipex_ops.py
9	27	vllm/model_executor/layers/rotary_embedding/base.py

[99bfef841] Jee Jee Li 2025-09-13 [Bugfix] Fix GPUModelRunner has no attribute lora_manager (#24762)
10	10	vllm/v1/worker/lora_model_runner_mixin.py

[89e08d6d1] Shane A 2025-09-12 [Model] Add Olmo3 model implementation (#24534)
1	0	docs/models/supported_models.md
1	0	tests/models/registry.py
28	14	vllm/model_executor/models/olmo2.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
80	0	vllm/transformers_utils/configs/olmo3.py

[7f2ea7074] Chenheli Hua 2025-09-12 [Frontend][Multimodal] Allow skipping media data when UUIDs are provided.  (#23950)
59	0	docs/features/multimodal_inputs.md
71	10	examples/offline_inference/vision_language.py
65	0	tests/entrypoints/openai/test_vision.py
546	2	tests/entrypoints/test_chat_utils.py
155	73	vllm/entrypoints/chat_utils.py
40	0	vllm/entrypoints/llm.py
3	2	vllm/multimodal/inputs.py
19	4	vllm/multimodal/parse.py
12	5	vllm/multimodal/processing.py

[4fdd6f5cb] Nick Hill 2025-09-12 [Core] Support async scheduling with uniproc executor  (#24219)
19	5	tests/basic_correctness/test_basic_correctness.py
4	0	tests/v1/engine/test_engine_core.py
2	5	vllm/engine/arg_utils.py
46	28	vllm/executor/uniproc_executor.py
5	4	vllm/v1/engine/core.py
2	2	vllm/v1/engine/core_client.py
14	3	vllm/v1/executor/abstract.py
7	5	vllm/v1/executor/multiproc_executor.py
4	3	vllm/v1/executor/ray_distributed_executor.py

[8226dd56b] Tao He 2025-09-13 [Qwen3Next] Fixes the cuda graph capture conditions under large batch sizes (#24660) (#24667)
2	1	vllm/v1/attention/backends/gdn_attn.py

[5fe643fc2] Matthew Bonanni 2025-09-12 Add FLASHINFER_MLA to backend selector test (#24753)
41	19	tests/kernels/attention/test_attention_selector.py
2	0	tests/v1/attention/utils.py

[7ba32aa60] Matthew Bonanni 2025-09-12 [Attention][FlashInfer] Enable FP8 FlashInfer (TRTLLM) MLA decode (#24705)
1	0	tests/v1/attention/test_attention_backends.py
2	0	tests/v1/tpu/test_pallas.py
1	0	vllm/attention/backends/abstract.py
2	0	vllm/envs.py
2	0	vllm/model_executor/layers/quantization/kv_cache.py
4	1	vllm/platforms/cuda.py
0	2	vllm/v1/attention/backends/mla/common.py
11	7	vllm/v1/attention/backends/mla/flashinfer_mla.py

[c89ed8de4] Alexandre Marques 2025-09-12 Invert pattern order to make sure that out_proj layers are identified (#24781)
3	3	vllm/model_executor/models/voxtral.py

[3beadc2f2] Wentao Ye 2025-09-12 [Compilation Bug] Fix Inductor Graph Output with Shape Issue (#24772)
6	3	vllm/model_executor/models/qwen3_moe.py

[bc636f21a] Clayton Coleman 2025-09-12 [Benchmark] Allow arbitrary headers to be passed to benchmarked endpoints (#23937)
7	0	vllm/benchmarks/lib/endpoint_request_func.py
28	1	vllm/benchmarks/serve.py

[017354c0e] Zhewen Li 2025-09-12 [CI] Trigger BC Linter when labels are added/removed (#24767)
2	0	.github/workflows/bc-lint.yml

[010acc6e1] Cyrus Leung 2025-09-13 [Bugfix] Fix incompatibility between #20452 and #24548 (#24754)
2	4	vllm/v1/executor/utils.py

[c8c42597a] afeldman-nm 2025-09-12 [CI] Speed up model unit tests in CI (#24253)
66	12	.buildkite/test-pipeline.yaml
1	0	pyproject.toml
10	4	tests/models/language/generation/test_common.py
4	1	tests/models/language/pooling/test_classification.py
5	2	tests/models/language/pooling/test_embedding.py
37	2	tests/models/test_initialization.py

[9d2a44606] Michael Goin 2025-09-12 [UX] Remove AsyncLLM torch profiler disabled log (#24609)
0	3	vllm/v1/engine/async_llm.py

[f17c07588] Samit 2025-09-13 [Model] Switch to Fused RMSNorm in GLM-4.1V model (#24733)
3	2	vllm/model_executor/models/glm4_1v.py

[b0d1213ac] Lukas Geiger 2025-09-12 [Models] Prevent CUDA sync in Qwen2.5-VL (#24741)
4	1	vllm/model_executor/models/qwen2_5_vl.py

[57f94e88e] Lukas Geiger 2025-09-12 [Models] Optimise and simplify `_validate_and_reshape_mm_tensor` (#24742)
1	1	vllm/model_executor/models/ernie45_vl.py
1	1	vllm/model_executor/models/glm4_1v.py
2	2	vllm/model_executor/models/keye.py
2	2	vllm/model_executor/models/keye_vl1_5.py
1	1	vllm/model_executor/models/midashenglm.py
2	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
1	1	vllm/model_executor/models/qwen2_audio.py
1	1	vllm/model_executor/models/qwen2_vl.py

[684b6870e] Kebe 2025-09-13 [Bugfix][Frontend] Fix `--enable-log-outputs` does not match the documentation (#24626)
5	2	vllm/entrypoints/openai/cli_args.py

[a5b84f1cb] dongluw 2025-09-12 [Core] Shared memory based object store for Multimodal data caching and IPC (#20452)
2	0	.buildkite/test-pipeline.yaml
26	5	docs/configuration/optimization.md
172	0	tests/distributed/test_shm_buffer.py
327	0	tests/distributed/test_shm_storage.py
4	4	tests/multimodal/test_cache.py
1	0	tests/tensorizer_loader/conftest.py
1	0	tools/check_pickle_imports.py
20	0	vllm/config/__init__.py
635	0	vllm/distributed/device_communicators/shm_object_storage.py
15	2	vllm/engine/arg_utils.py
7	0	vllm/envs.py
10	0	vllm/executor/uniproc_executor.py
211	9	vllm/multimodal/cache.py
6	0	vllm/utils/__init__.py
2	2	vllm/v1/engine/core.py
23	5	vllm/v1/executor/multiproc_executor.py
25	0	vllm/v1/executor/utils.py

[9f04d9d55] Elvir Crnčević 2025-09-12 [Qwen3-Next] MoE configs for H100 TP=1,2 and TP2/EP (#24739)
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=512,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_H100_80GB_HBM3.json

[4d7c1d531] Yan Ma 2025-09-12 [Bugfix] Fix MRoPE dispatch on XPU (#24724)
9	0	vllm/model_executor/layers/rotary_embedding/mrope.py

[41f17bf29] Hyogeun Oh (오효근) 2025-09-12 [Docs] Fix warnings in mkdocs build (continued) (#24740)
5	5	vllm/model_executor/layers/quantization/torchao.py
1	1	vllm/model_executor/layers/quantization/utils/int8_utils.py
2	2	vllm/model_executor/layers/rotary_embedding/mrope.py
45	44	vllm/model_executor/model_loader/tensorizer.py
4	12	vllm/model_executor/models/aria.py
40	64	vllm/model_executor/models/bart.py
0	1	vllm/model_executor/models/blip2.py
6	12	vllm/model_executor/models/donut.py
14	24	vllm/model_executor/models/florence2.py
4	11	vllm/model_executor/models/glm4_1v.py

[bcb06d7ba] Didier Durand 2025-09-12 [Doc]: fix typos in various files (#24726)
1	1	benchmarks/kernels/benchmark_w8a8_block_fp8.py
1	1	csrc/cpu/cpu_types_vxe.hpp
1	1	csrc/cpu/sgl-kernels/moe.cpp
1	1	docs/design/multiprocessing.md
1	1	vllm/attention/backends/flash_attn.py
1	1	vllm/benchmarks/datasets.py
1	1	vllm/entrypoints/openai/protocol.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	1	vllm/model_executor/models/minicpmv.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/tpu_input_batch.py

[0377802c2] Flora Feng 2025-09-12 [Multimodal] Remove legacy multimodal fields in favor of MultiModalFeatureSpec  (#24548)
11	1	tests/v1/core/test_encoder_cache_manager.py
1	3	tests/v1/tpu/worker/test_tpu_model_runner.py
1	3	tests/v1/worker/test_gpu_input_batch.py
1	3	tests/v1/worker/test_gpu_model_runner.py
22	19	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
5	5	vllm/v1/core/encoder_cache_manager.py
16	20	vllm/v1/core/kv_cache_utils.py
5	13	vllm/v1/core/sched/output.py
11	11	vllm/v1/core/sched/scheduler.py
2	7	vllm/v1/request.py
4	6	vllm/v1/worker/gpu_input_batch.py
15	13	vllm/v1/worker/gpu_model_runner.py
8	12	vllm/v1/worker/tpu_model_runner.py

[72fc8aa41] Wenlong Wang 2025-09-12 [Multi Modal] Add FA3 in VIT (#24347)
2	2	tests/entrypoints/openai/test_vision.py
35	14	tests/kernels/attention/test_mha_attn.py
67	8	vllm/attention/layer.py
20	3	vllm/model_executor/models/ernie45_vl.py
19	3	vllm/model_executor/models/glm4_1v.py
15	2	vllm/model_executor/models/keye.py
21	3	vllm/model_executor/models/qwen2_5_vl.py
21	3	vllm/model_executor/models/qwen2_vl.py
14	2	vllm/model_executor/models/siglip2navit.py
5	5	vllm/model_executor/models/vision.py
17	11	vllm/platforms/cuda.py
2	1	vllm/platforms/interface.py
9	9	vllm/platforms/rocm.py

[fdb09c77d] youkaichao 2025-09-12 [sleep mode] save memory for on-the-fly quantization (#24731)
31	6	vllm/device_allocator/cumem.py

[7a1c4025f] Ignacio Sica 2025-09-12 [Kernel] [CPU] refactor `cpu_attn.py:_run_sdpa_forward` for better memory access (#24701)
4	4	vllm/v1/attention/backends/cpu_attn.py

[60a095192] Jee Jee Li 2025-09-12 [Bugfix] Fix BNB name match (#24735)
6	5	vllm/model_executor/model_loader/bitsandbytes_loader.py

[64d90c3e4] Chen Zhang 2025-09-12 [Misc][gpt-oss] Add gpt-oss label to PRs that mention harmony or related to builtin tool call (#24717)
7	0	.github/mergify.yml

[59d5d2c73] Li, Jiang 2025-09-12 [CI/Build] Skip prompt embeddings tests on V1-only CPU backend (#24721)
6	0	tests/models/language/generation/test_common.py

[d21a36f5f] wang.yuqi 2025-09-12 [CI] Add ci_envs for convenient local testing (#24630)
45	0	tests/ci_envs.py
14	5	tests/models/language/generation_ppl_test/ppl_utils.py
30	9	tests/models/language/pooling_mteb_test/mteb_utils.py
9	4	vllm/config/__init__.py

[561a0baee] Chen Zhang 2025-09-12 [CI] Fix flaky test  v1/worker/test_gpu_model_runner.py::test_kv_cache_stride_order          (#24640)
23	24	tests/v1/worker/test_gpu_model_runner.py

[f592b3174] Nick Hill 2025-09-11 [BugFix] Fix Qwen3-Next PP (#24709)
7	3	vllm/model_executor/models/qwen3_next.py

[7920de0a2] Li, Jiang 2025-09-12 [Bugfix] Fix MRoPE dispatch on CPU (#24712)
9	0	vllm/model_executor/layers/rotary_embedding/mrope.py

[ddcec289c] Andrew Sansom 2025-09-11 Fix implementation divergence for BLOOM models between vLLM and HuggingFace when using prompt embeds (#24686)
2	4	tests/models/language/generation/test_common.py
2	1	vllm/model_executor/models/bloom.py

[e090b7b45] Maximilien de Bayser 2025-09-12 Enable conversion of multimodal models to pooling tasks (#24451)
114	0	tests/models/language/pooling/test_mm_classifier_conversion.py
90	52	vllm/entrypoints/llm.py
14	4	vllm/model_executor/model_loader/utils.py
43	2	vllm/model_executor/models/adapters.py
5	1	vllm/model_executor/models/gemma3_mm.py

[6a50eaa0d] Gregory Shtrasberg 2025-09-11 [DOCs] Update ROCm installation docs section (#24691)
15	25	docs/getting_started/installation/gpu/rocm.inc.md

[12a8414d8] Jee Jee Li 2025-09-12 [Qwen3-Next] MoE configs for H20 TP=1,2,4,8 (#24707)
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H20-3e.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H20-3e.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_H20-3e.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_H20-3e.json

[880c741bb] Tao He 2025-09-12 [Bugfix] fixes the causal_conv1d_update kernel update non-speculative decoding cases (#24680)
8	5	vllm/model_executor/layers/mamba/ops/causal_conv1d.py

[40b6c9122] RichardoMu 2025-09-12 [V1] feat:add engine v1 tracing (#20372)
1	1	.buildkite/test-pipeline.yaml
137	0	tests/v1/tracing/test_tracing.py
0	6	vllm/engine/arg_utils.py
5	0	vllm/tracing.py
1	1	vllm/v1/core/sched/scheduler.py
5	1	vllm/v1/engine/__init__.py
8	1	vllm/v1/engine/async_llm.py
7	0	vllm/v1/engine/llm_engine.py
79	7	vllm/v1/engine/output_processor.py
1	2	vllm/v1/engine/processor.py
4	0	vllm/v1/metrics/stats.py
5	1	vllm/v1/request.py

[2e6bc4682] Lucas Wilkinson 2025-09-11 [Startup] Make DeepGEMM warmup scale with max-num-batched-tokens (#24693)
13	9	vllm/model_executor/warmup/deep_gemm_warmup.py

[fcba05c43] Wentao Ye 2025-09-11 [Bug] Fix Layer `weight_block_size` Assertion Issue (#24674)
3	3	vllm/model_executor/layers/quantization/fp8.py

[7a30fa870] Zazzle516 2025-09-12 [Doc] Clarify cudagraph capture size logic and default behavior in scheduler (#18698)
29	19	vllm/config/__init__.py

[f82f7a899] Chen Zhang 2025-09-11 [Qwen3-Next] MOE configs for H100 TP4 (#24699)
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H100_80GB_HBM3.json

[c3aea10dc] Michael Goin 2025-09-11 [Perf] Use upstream CUTLASS for SM90 Block FP8 kernel (#23280)
44	13	benchmarks/kernels/bench_block_fp8_gemm.py
0	123	csrc/cutlass_extensions/gemm/collective/collective_builder.hpp
0	183	csrc/cutlass_extensions/gemm/collective/fp8_accumulation.hpp
0	729	csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp
0	39	csrc/cutlass_extensions/gemm/dispatch_policy.hpp
1	1	csrc/cutlass_extensions/vllm_collective_builder.cuh
0	3	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh
0	3	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh
90	112	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh
1	27	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_helper.hpp
50	2	tests/kernels/quantization/test_block_fp8.py
11	1	vllm/model_executor/layers/quantization/fp8.py
25	25	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[d4fd2768e] Matthew Bonanni 2025-09-11 [Bugfix][Attention] Fix FlashInfer MLA block size logic (#24692)
11	2	vllm/platforms/cuda.py

[7a70a7189] Vadim Gimpelson 2025-09-12 [Qwen3-Next] Add B200 MoE configs for Qwen3-next (#24698)
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_B200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_B200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_B200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_B200.json

[7d4651997] Zhewen Li 2025-09-11 [CI/Build] Add bc-linter to vLLM CI (#21234)
24	0	.github/.bc-linter.yml
27	0	.github/workflows/bc-lint.yml
6	0	vllm/__init__.py
59	0	vllm/_bc_linter.py
5	0	vllm/v1/core/sched/output.py

[569bf1c9c] Woosuk Kwon 2025-09-11 [Qwen3-Next] MoE configs for H200 TP=1,2,4 (#24695)
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=128,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=512,device_name=NVIDIA_H200.json

[1ec20355f] Wentao Ye 2025-09-11 [Bugfix] Set `VLLM_ALLREDUCE_USE_SYMM_MEM` default to False (#24696)
1	1	vllm/envs.py

[e42af78b1] Xiaozhu Meng 2025-09-11 [flashinfer] [kernel] support for fp8 kv cache for trtllm prefill attention (#24197)
6	0	vllm/envs.py
6	5	vllm/utils/flashinfer.py
109	9	vllm/v1/attention/backends/flashinfer.py

[074854b24] Duncan Moss 2025-09-11 [Kernel][B200] `mxfp4` fused cutlass moe (#23696)
319	0	tests/kernels/moe/test_mxfp4_moe.py
12	1	vllm/envs.py
10	3	vllm/model_executor/layers/fused_moe/layer.py
283	58	vllm/model_executor/layers/quantization/mxfp4.py
2	2	vllm/model_executor/warmup/kernel_warmup.py

[79ac59f32] Andrew Xia 2025-09-11 Update Spec Decode metrics to include drafted and accepted token throughput (#24127)
16	3	vllm/v1/spec_decode/metrics.py

[b971f9150] Nick Hill 2025-09-11 [BugFix] Fix tokenize asyncio task leak (#24677)
30	28	vllm/entrypoints/renderer.py

[c733bd5e8] Woosuk Kwon 2025-09-11 [Qwen3-Next] Add MoE Config for H200 (#24688)
146	0	vllm/model_executor/layers/fused_moe/configs/E=512,N=64,device_name=NVIDIA_H200.json

[a892b259b] Wentao Ye 2025-09-11 [Doc] Remove Useless Comments (#24687)
1	2	vllm/model_executor/layers/quantization/fp8.py

[127ded0a9] Peter Salas 2025-09-11 [Ultravox] Use wrapped_model_config to instantiate inner model (#24679)
2	2	vllm/model_executor/models/ultravox.py
3	4	vllm/transformers_utils/configs/ultravox.py

[bb2b5126d] Isotr0py 2025-09-12 [VLM] Migrate remain DP-supported ViT models to use `disable_tp` (#24363)
20	65	vllm/model_executor/models/idefics2_vision_model.py
13	19	vllm/model_executor/models/mllama4.py
21	22	vllm/model_executor/models/qwen2_5_vl.py

[361ae27f8] Harry Mellor 2025-09-11 [Docs] Fix formatting of transcription doc (#24676)
163	159	docs/contributing/model/transcription.md

[e26fef839] co63oc 2025-09-12 fix some typos (#24616)
1	1	tests/compile/test_basic_correctness.py
1	1	tests/kernels/mamba/test_mamba_ssm_ssd.py
1	1	vllm/model_executor/layers/fla/ops/cumsum.py
1	1	vllm/v1/attention/backends/mla/common.py
2	2	vllm/v1/worker/block_table.py

[c1eda615b] Harry Mellor 2025-09-11 Fix model name included in responses (#24663)
1	54	tests/entrypoints/openai/test_chat.py
41	1	tests/entrypoints/openai/test_serving_chat.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_classification.py
1	1	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_embedding.py
0	11	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_pooling.py
1	1	vllm/entrypoints/openai/serving_responses.py
2	2	vllm/entrypoints/openai/serving_score.py

[4aa23892d] Konrad Zawora 2025-09-11 [Bugfix] Fix platform-specific routing in CustomOp implementations (#24444)
4	1	vllm/model_executor/layers/activation.py
8	1	vllm/model_executor/layers/fused_moe/layer.py
10	1	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
10	1	vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py
9	1	vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope.py
8	1	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
0	23	vllm/model_executor/layers/rotary_embedding/mrope.py
4	1	vllm/model_executor/layers/vocab_parallel_embedding.py

[1fdd5c42d] Ilya Markov 2025-09-11 [Kernels] Enable Torch Symmetric Memory All-Reduce By Default (#24111)
486	0	benchmarks/kernels/benchmark_device_communicators.py
40	13	csrc/custom_all_reduce.cuh
2	2	vllm/distributed/device_communicators/all_reduce_utils.py
8	5	vllm/distributed/device_communicators/cuda_communicator.py
3	2	vllm/distributed/device_communicators/custom_all_reduce.py
31	6	vllm/distributed/device_communicators/symm_mem.py
2	2	vllm/envs.py

[bcbe2a4d9] Isotr0py 2025-09-12 [VLM] Optimize GLM4.5-V-style video processing to only decode necessary frames (#24161)
47	0	tests/models/multimodal/processing/test_glm4_1v.py
26	0	tests/multimodal/test_utils.py
8	8	vllm/assets/video.py
60	40	vllm/model_executor/models/glm4_1v.py
92	7	vllm/multimodal/video.py

[51d41265a] Harry Mellor 2025-09-11 [Docs] Fix typos in EP deployment doc (#24669)
2	2	docs/serving/expert_parallel_deployment.md

[4984a291d] Wentao Ye 2025-09-11 [Doc] Fix Markdown Pre-commit Error (#24670)
5	6	docs/contributing/model/transcription.md

[404c85ca7] Nicolò Lucchesi 2025-09-11 [Docs] Add transcription support to model (#24664)
1	0	docs/.nav.yml
1	0	docs/contributing/model/README.md
273	0	docs/contributing/model/transcription.md

[817beef7f] Jee Jee Li 2025-09-11 [Bugifx] Fix qwen-next packed_modules_mapping (#24656)
1	1	vllm/model_executor/models/qwen3_next.py

[4f6593b05] Mengqing Cao 2025-09-11 [HybridKVCache][Platform] Add support_hybrid_kv_cache for platform (#24646)
1	2	vllm/config/__init__.py
4	0	vllm/platforms/cpu.py
4	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py

[94e6b2d55] Boyuan Feng 2025-09-11 Allow users to specify kv cache memory size (#21489)
9	0	vllm/config/cache.py
11	1	vllm/engine/arg_utils.py
2	1	vllm/engine/llm_engine.py
10	0	vllm/entrypoints/llm.py
4	1	vllm/utils/__init__.py
2	1	vllm/v1/utils.py
3	2	vllm/v1/worker/gpu_model_runner.py
77	7	vllm/v1/worker/gpu_worker.py
4	2	vllm/worker/model_runner.py
114	32	vllm/worker/worker.py

[fd1ce98cd] wang.yuqi 2025-09-11 [CI] Split mteb test from Language Models Test (#24634)
10	0	.buildkite/test-pipeline.yaml
3	4	tests/entrypoints/pooling/correctness/test_mteb_embed.py
3	9	tests/entrypoints/pooling/correctness/test_mteb_score.py
0	0	tests/models/language/pooling_mteb_test/__init__.py
0	0	tests/models/language/{pooling => pooling_mteb_test}/mteb_utils.py
6	4	tests/models/language/{pooling => pooling_mteb_test}/test_baai.py
3	3	tests/models/language/{pooling => pooling_mteb_test}/test_bge_reranker_v2_gemma.py
3	2	tests/models/language/{pooling => pooling_mteb_test}/test_cross_encoder.py
6	4	tests/models/language/{pooling => pooling_mteb_test}/test_gte.py
4	2	tests/models/language/{pooling => pooling_mteb_test}/test_intfloat.py
5	4	tests/models/language/{pooling => pooling_mteb_test}/test_jina.py
1	1	tests/models/language/{pooling => pooling_mteb_test}/test_mxbai_rerank.py
4	2	tests/models/language/{pooling => pooling_mteb_test}/test_nomic.py
1	1	tests/models/language/{pooling => pooling_mteb_test}/test_qwen3_reranker.py
4	2	tests/models/language/{pooling => pooling_mteb_test}/test_snowflake_arctic_embed.py
3	2	tests/models/language/{pooling => pooling_mteb_test}/test_st_projector.py

[d11ec124a] Jee Jee Li 2025-09-11 [Bench] Add qwen-next in benchmark_moe.py (#24661)
5	1	benchmarks/kernels/benchmark_moe.py

[f51071588] youkaichao 2025-09-11 [build] add torch to tool.uv no-build-isolation-package (#24303)
4	4	docs/getting_started/installation/gpu/cuda.inc.md
3	0	pyproject.toml
2	18	use_existing_torch.py

[f94619747] Tao He 2025-09-11 [Docs] Fixes a typo in the qwen3next model name. (#24654)
1	1	docs/models/supported_models.md

[0cd72a7b7] Fanli Lin 2025-09-11 [XPU] add missing dependency tblib for XPU CI (#24639)
1	0	.buildkite/scripts/hardware_ci/run-xpu-test.sh

[5f5271f1e] Harry Mellor 2025-09-11 Move `LoRAConfig` from `config/__init__.py` to `config/lora.py` (#24644)
2	1	tests/core/test_scheduler.py
1	1	tests/lora/test_layers.py
2	2	tests/lora/test_lora_allowed_token_ids.py
1	1	tests/lora/test_lora_manager.py
1	1	tests/lora/test_peft_helper.py
2	1	tests/lora/test_worker.py
1	110	vllm/config/__init__.py
132	0	vllm/config/lora.py
2	1	vllm/core/scheduler.py
3	2	vllm/engine/async_llm_engine.py
3	3	vllm/engine/llm_engine.py
1	1	vllm/lora/layers/base.py
1	1	vllm/lora/layers/base_linear.py
1	1	vllm/lora/layers/column_parallel_linear.py
1	1	vllm/lora/layers/logits_processor.py
1	1	vllm/lora/layers/replicated_linear.py
1	1	vllm/lora/layers/row_parallel_linear.py
1	1	vllm/lora/layers/vocal_parallel_embedding.py
1	1	vllm/lora/models.py
1	1	vllm/lora/peft_helper.py
1	1	vllm/lora/utils.py
1	1	vllm/lora/worker_manager.py
2	1	vllm/model_executor/models/bart.py
2	1	vllm/transformers_utils/tokenizer_group.py
2	1	vllm/v1/worker/lora_model_runner_mixin.py

[d6249d069] Harry Mellor 2025-09-11 Fix typing for `safetensors_load_strategy` (#24641)
1	1	vllm/config/load.py
1	2	vllm/engine/arg_utils.py
1	1	vllm/model_executor/model_loader/weight_utils.py

[25bb9e8c6] wang.yuqi 2025-09-11 [CI Failure] fix models/language/pooling/test_auto_prefix_cache_support.py (#24636)
4	0	vllm/config/__init__.py

[a1213fae5] Nicolò Lucchesi 2025-09-11 [Misc] Add @NickLucche to codeowners (#24647)
12	4	.github/CODEOWNERS

[a8b0361c9] wang.yuqi 2025-09-11 [CI] Split pooling from entrypoints Test (#24632)
14	1	.buildkite/test-pipeline.yaml
0	0	tests/entrypoints/pooling/__init__.py
0	0	tests/entrypoints/pooling/correctness/__init__.py
0	0	tests/entrypoints/{openai => pooling}/correctness/test_mteb_embed.py
0	0	tests/entrypoints/{openai => pooling}/correctness/test_mteb_score.py
0	0	tests/entrypoints/pooling/llm/__init__.py
1	2	tests/entrypoints/{ => pooling}/llm/test_classify.py
0	0	tests/entrypoints/{ => pooling}/llm/test_embedding.py
0	0	tests/entrypoints/{ => pooling}/llm/test_encode.py
1	2	tests/entrypoints/{ => pooling}/llm/test_reward.py
1	2	tests/entrypoints/{ => pooling}/llm/test_score.py
0	0	tests/entrypoints/pooling/openai/__init__.py
1	2	tests/entrypoints/{ => pooling}/openai/test_classification.py
4	5	tests/entrypoints/{ => pooling}/openai/test_embedding.py
5	6	tests/entrypoints/{ => pooling}/openai/test_embedding_dimensions.py
1	2	tests/entrypoints/{ => pooling}/openai/test_embedding_long_text.py
1	2	tests/entrypoints/{ => pooling}/openai/test_pooling.py
1	2	tests/entrypoints/{ => pooling}/openai/test_rerank.py
1	2	tests/entrypoints/{ => pooling}/openai/test_score.py
0	0	tests/entrypoints/{ => pooling}/openai/test_truncation.py
1	2	tests/entrypoints/{ => pooling}/openai/test_vision_embedding.py

[ed5ae4aac] Kyuyeun Kim 2025-09-11 [Bugfix] Fix _synced_weight_loader (#24565)
2	1	vllm/model_executor/utils.py

[0fc36463e] Xingyu Liu 2025-09-11 [CI]Add transformers_utils to Async Engine, Inputs, Utils, Worker Test (#24615)
4	2	.buildkite/test-pipeline.yaml

[d14c4ebf0] Michael Yao 2025-09-11 [Docs] Use 1-2-3 list for deploy steps in deployment/frameworks/ (#24633)
7	9	docs/deployment/frameworks/autogen.md
14	10	docs/deployment/frameworks/chatbox.md
28	22	docs/deployment/frameworks/dify.md
6	6	docs/deployment/frameworks/haystack.md
11	11	docs/deployment/frameworks/litellm.md
32	32	docs/deployment/frameworks/retrieval_augmented_generation.md

[ba6011027] Russell Bryant 2025-09-11 [Docs] Update V1 doc to reflect whisper support (#24606)
1	1	docs/models/supported_models.md
4	3	docs/usage/v1_guide.md

[85df8afda] Michael Yao 2025-09-11 [Docs] Revise frameworks/anything-llm.md (#24489)
34	22	docs/deployment/frameworks/anything-llm.md

[6aeb1dab4] Cyrus Leung 2025-09-11 [Bugfix] Fix incorrect import of CacheConfig (#24631)
1	2	vllm/attention/layers/cross_attention.py

[e93f4cc9e] Tao He 2025-09-11 Add the support for the qwen3 next model (a hybrid attention model). (#24526)
1	0	.yapfignore
1	0	docs/models/supported_models.md
1	0	pyproject.toml
5	1	tests/models/registry.py
46	10	vllm/config/__init__.py
1	0	vllm/config/compilation.py
3	2	vllm/model_executor/layers/fla/ops/chunk_delta_h.py
5	4	vllm/model_executor/layers/fla/ops/chunk_o.py
6	4	vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py
2	2	vllm/model_executor/layers/fla/ops/fused_recurrent.py
1	1	vllm/model_executor/layers/fla/ops/l2norm.py
0	5	vllm/model_executor/layers/fla/ops/op.py
37	0	vllm/model_executor/layers/mamba/mamba_utils.py
41	7	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
2	1	vllm/model_executor/models/config.py
1294	0	vllm/model_executor/models/qwen3_next.py
285	0	vllm/model_executor/models/qwen3_next_mtp.py
2	0	vllm/model_executor/models/registry.py
1	1	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
275	0	vllm/transformers_utils/configs/qwen3_next.py
319	0	vllm/v1/attention/backends/gdn_attn.py
40	4	vllm/v1/core/single_type_kv_cache_manager.py
1	0	vllm/v1/kv_cache_interface.py
8	2	vllm/v1/spec_decode/eagle.py
13	7	vllm/v1/worker/block_table.py
17	0	vllm/v1/worker/gpu_input_batch.py
65	9	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/worker/worker.py

[2048c4e37] Jerry Zhang 2025-09-10 [torchao] Support quantization configs using module swap (#21982)
4	0	.buildkite/test-pipeline.yaml
20	0	tests/quantization/test_torchao.py
9	7	vllm/model_executor/layers/quantization/torchao.py

[d13360183] Chenxi Yang 2025-09-10 Remove redundant all gather + split (#23441)
0	13	vllm/model_executor/models/glm4_1v.py

[9bd831f50] TaehyunKim 2025-09-11 [Model] New model support for Motif-1-Tiny (#23414)
155	0	benchmarks/kernels/benchmark_polynorm.py
251	0	csrc/layernorm_kernels.cu
4	1	csrc/ops.h
6	0	csrc/torch_bindings.cpp
1	0	docs/models/supported_models.md
32	1	tests/kernels/core/test_layernorm.py
3	0	tests/models/registry.py
3	2	tests/models/test_initialization.py
8	0	vllm/_custom_ops.py
3	0	vllm/attention/backends/differential_flash_attn.py
59	0	vllm/model_executor/layers/layernorm.py
345	0	vllm/model_executor/models/motif.py
1	0	vllm/model_executor/models/registry.py

[e2b1f863a] Didier Durand 2025-09-11 [Doc]: fixing doc typos (#24635)
1	1	vllm/config/__init__.py
1	1	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	1	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
1	1	vllm/model_executor/models/arcee.py
1	1	vllm/model_executor/models/llava_onevision.py
1	1	vllm/model_executor/models/phi4_multimodal.py
1	1	vllm/model_executor/models/phi4mm_audio.py
2	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	1	vllm/v1/attention/backends/mla/common.py

[41329a0ff] shengshiqi-google 2025-09-11 [Core] feat: Add --safetensors-load-strategy flag for faster safetensors loading from Lustre (#24469)
9	0	vllm/config/load.py
5	0	vllm/engine/arg_utils.py
1	0	vllm/model_executor/model_loader/default_loader.py
16	6	vllm/model_executor/model_loader/weight_utils.py

[ee0bc5e1b] Tomas Ruiz 2025-09-11 Enable --profile in 'vllm bench throughput' (#24575)
34	4	vllm/benchmarks/throughput.py

[3d1393f6f] Saman A. Pour 2025-09-10 Kimi K2 Fused MoE kernels Optimization configs (#24597)
146	0	vllm/model_executor/layers/fused_moe/configs/E=384,N=128,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=384,N=128,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=384,N=128,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=384,N=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=384,N=256,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json

[8a894084d] Guy Stone 2025-09-11 [Engine][Chore] use local variable and remove output var assignment (#24554)
6	7	vllm/engine/protocol.py

[e2d8c27f6] Nick Hill 2025-09-10 [BugFix] Fix pipeline parallel (#24621)
4	0	vllm/executor/uniproc_executor.py
0	1	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/kv_connector_model_runner_mixin.py

[29799ddac] Li, Jiang 2025-09-11 [Bugfix] Add missing VIT backend dispatch on CPU (#24623)
2	1	vllm/attention/layer.py

[f17a6aa4e] Peter Salas 2025-09-10 [Ultravox] Fix Gemma instantiation, support quantization via --hf-overrides (#24131)
8	4	vllm/config/__init__.py
1	1	vllm/model_executor/models/ultravox.py
42	33	vllm/transformers_utils/configs/ultravox.py

[6c8deacd7] Wenlong Wang 2025-09-10 [Bug] [Spec Decode] Fix model_initialization test and mismatch in aux_hidden_layers (#24613)
16	7	tests/models/registry.py
4	1	tests/models/test_initialization.py
12	3	tests/models/utils.py

[55b823ba0] Chauncey 2025-09-11 Add @chaunceyjiang to codeowner for reasoning Reasoning and Tool parser (#24406)
2	2	.github/CODEOWNERS

[8c5a74724] youkaichao 2025-09-11 [distributed] update known issues (#24624)
1	0	docs/usage/troubleshooting.md

[5931b7e5d] Alexandre Marques 2025-09-10 [Models][Quantization] Add quantization configuration update in Voxtral model (#24122)
15	4	vllm/model_executor/models/llama.py
73	0	vllm/model_executor/models/voxtral.py

[cc99baf14] Jonathan Berkhahn 2025-09-10 [Misc] Make timeout passable in init_distributed_environment (#24522)
9	8	vllm/distributed/parallel_state.py

[dcb28a332] Hanjie Qiu 2025-09-10 [Kernel] Flashinfer MLA (trtllm-gen) decode kernel integration (#21078)
2	1	.buildkite/test-pipeline.yaml
0	0	tests/kernels/{ => attention}/test_cutlass_mla_decode.py
123	0	tests/kernels/attention/test_flashinfer_mla_decode.py
1	0	vllm/engine/arg_utils.py
15	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
3	0	vllm/v1/attention/backends/mla/common.py
110	0	vllm/v1/attention/backends/mla/flashinfer_mla.py

[fba785658] Michael Goin 2025-09-10 [Perf] Warmup FlashInfer attention during startup (#23439)
27	1	vllm/model_executor/warmup/kernel_warmup.py
0	16	vllm/v1/attention/backends/flashinfer.py
28	3	vllm/v1/worker/gpu_model_runner.py

[b5e383cd8] Chen Zhang 2025-09-10 [gpt-oss] raise error for flashinfer backend without trtllm (#24482)
10	2	vllm/v1/attention/backends/flashinfer.py

[9a161307f] Gregory Shtrasberg 2025-09-10 [torch.compile][ROCm][V1] Enable attention output FP8 fusion for V1 attention backends (#19767)
87	42	tests/compile/test_fusion_attn.py
16	2	vllm/attention/ops/chunked_prefill_paged_decode.py
13	1	vllm/attention/ops/prefix_prefill.py
81	58	vllm/attention/ops/triton_unified_attention.py
2	1	vllm/compilation/backends.py
22	9	vllm/compilation/fusion_attn.py
19	19	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
9	3	vllm/v1/attention/backends/triton_attn.py

[37e8182bf] Russell Bryant 2025-09-10 [v1] Add Whisper model support (encoder-decoder) (#21088)
3	3	.buildkite/test-pipeline.yaml
2	0	examples/offline_inference/encoder_decoder.py
3	0	examples/offline_inference/encoder_decoder_multimodal.py
1	0	tests/encoder_decoder/test_e2e_correctness.py
1	0	tests/entrypoints/openai/test_encoder_decoder.py
2	0	tests/models/language/generation/test_bart.py
1	2	tests/models/multimodal/generation/test_whisper.py
1	0	tests/models/multimodal/processing/test_tensor_schema.py
6	0	tests/models/test_initialization.py
0	1	tests/v1/test_oracle.py
160	0	vllm/attention/layers/cross_attention.py
28	9	vllm/config/__init__.py
0	1	vllm/model_executor/models/voxtral.py
51	23	vllm/model_executor/models/whisper.py
1	0	vllm/transformers_utils/configs/mistral.py
2	2	vllm/v1/attention/backends/cpu_attn.py
1	2	vllm/v1/attention/backends/flash_attn.py
1	3	vllm/v1/attention/backends/flashinfer.py
2	1	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/attention/backends/linear_attn.py
3	6	vllm/v1/attention/backends/mamba_attn.py
2	3	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/short_conv_attn.py
2	1	vllm/v1/attention/backends/tree_attn.py
2	2	vllm/v1/attention/backends/triton_attn.py
6	0	vllm/v1/attention/backends/utils.py
2	1	vllm/v1/attention/backends/xformers.py
21	11	vllm/v1/core/sched/scheduler.py
0	5	vllm/v1/engine/processor.py
111	13	vllm/v1/worker/gpu_model_runner.py
12	1	vllm/v1/worker/utils.py

[4db442640] Nick Hill 2025-09-10 [CI] Fail subprocess tests with root-cause error (#23795)
1	0	requirements/test.in
3	1	requirements/test.txt
26	0	tests/async_engine/test_api_server.py
10	0	tests/conftest.py
92	28	tests/utils.py
6	4	vllm/executor/ray_distributed_executor.py

[a0933c3bd] Thien Tran 2025-09-11 [Bugfix] Enable FP8 KV cache for FlashInfer and Triton backend on non-sm100 GPUs (#24577)
4	0	vllm/platforms/cuda.py
5	1	vllm/v1/attention/backends/flashinfer.py

[09e68bce3] rongfu.leng 2025-09-11 [Misc] update log level debug to warning when process port is used by (#24226)
1	1	vllm/entrypoints/launcher.py

[9fb74c27a] Xingyu Liu 2025-09-10 [Core] Support configuration parsing plugin (#24277)
0	0	tests/transformers_utils/__init__.py
37	0	tests/transformers_utils/test_config_parser_registry.py
1	4	vllm/config/__init__.py
3	4	vllm/engine/arg_utils.py
176	99	vllm/transformers_utils/config.py
20	0	vllm/transformers_utils/config_parser_base.py

[403294963] Ming Yang 2025-09-10 [Bugfix] Fix DeepEP config for DP4TP4 (#23619)
4	4	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py

[08abfa78e] tomeras91 2025-09-10 [Bugfix] fix modelopt exclude_modules name mapping (#24178)
3	0	vllm/model_executor/layers/mamba/mamba_mixer2.py
20	4	vllm/model_executor/layers/quantization/modelopt.py
36	34	vllm/model_executor/models/nemotron_h.py

[2bef2d140] Shiyan Deng 2025-09-10 [Logging] allow config logging stream (#24336)
5	0	vllm/envs.py
2	1	vllm/logger.py

[36cacd095] Robin 2025-09-10 [Doc] Add documentation for GLM-4.5 series models: tool-calling and reasoning parser (#24589)
1	0	docs/features/reasoning_outputs.md
9	0	docs/features/tool_calling.md

[bb3eb80d9] Jee Jee Li 2025-09-10 [Core] Split LoRA layers (#24574)
5	5	tests/lora/test_layers.py
0	355	vllm/lora/fully_sharded_layers.py
0	1192	vllm/lora/layers.py
34	0	vllm/lora/layers/__init__.py
69	0	vllm/lora/layers/base.py
184	0	vllm/lora/layers/base_linear.py
622	0	vllm/lora/layers/column_parallel_linear.py
247	0	vllm/lora/layers/logits_processor.py
8	0	vllm/lora/layers/qkv_x_parallel_linear.py
61	0	vllm/lora/layers/replicated_linear.py
201	0	vllm/lora/layers/row_parallel_linear.py
60	0	vllm/lora/layers/utils.py
172	0	vllm/lora/layers/vocal_parallel_embedding.py
5	5	vllm/lora/utils.py

[fcc0a3130] pwschuurman 2025-09-10 [CI] Fix tensorizer test assertion (#24545)
4	3	tests/tensorizer_loader/test_tensorizer.py

[736569da8] zzhxxx 2025-09-10 [Platform] Custom ops support for LMhead and LogitsProcessor (#23564)
3	2	vllm/model_executor/layers/logits_processor.py
1	0	vllm/model_executor/layers/vocab_parallel_embedding.py

[2eb9986a2] Kay Yan 2025-09-10 [BugFix] `python collect_env.py` and `vllm collect-env` compatibility with uv venv (#24066)
11	1	vllm/collect_env.py

[ccee371e8] Hyogeun Oh (오효근) 2025-09-10 [Docs] Fix warnings in `mkdocs build` (continued) (#24092)
1	1	vllm/model_executor/layers/fused_moe/layer.py
2	3	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
5	3	vllm/model_executor/layers/fused_moe/routing_simulator.py
5	5	vllm/model_executor/layers/quantization/bitblas.py
3	3	vllm/model_executor/layers/quantization/gptq_bitblas.py
4	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
144	123	vllm/model_executor/models/phi4mm_audio.py
164	173	vllm/model_executor/models/phi4mm_utils.py
2	14	vllm/model_executor/models/qwen2_5_vl.py
7	13	vllm/model_executor/models/zamba2.py

[c0bd6a684] RoadToNowhereX 2025-09-10 Fix Auto_Round Quatization Loading on SM75 and Lower GPUs (#24217)
2	1	vllm/model_executor/layers/quantization/auto_round.py

[3144d9021] co63oc 2025-09-10 fix some typos (#24167)
1	1	tests/v1/e2e/test_spec_decode.py
1	1	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	1	vllm/v1/sample/logits_processor/__init__.py

[2f5e5c18d] Daniele 2025-09-10 [CI/Build] bump timm dependency (#24189)
1	1	docker/Dockerfile

[bd98842c8] wang.yuqi 2025-09-10 [CI] Add PPL test for generation models (#24485)
10	0	.buildkite/test-pipeline.yaml
0	0	tests/models/language/generation_ppl_test/__init__.py
131	0	tests/models/language/generation_ppl_test/ppl_utils.py
18	0	tests/models/language/generation_ppl_test/test_gemma.py
14	0	tests/models/language/generation_ppl_test/test_gpt.py
21	0	tests/models/language/generation_ppl_test/test_qwen.py
1	1	tests/models/language/pooling/embed_utils.py
7	4	tests/models/language/pooling/mteb_utils.py
9	2	tests/models/utils.py

[d6069887c] Lifans 2025-09-10 [rocm] enable torchao quantization for rocm (#24400)
1	1	vllm/platforms/rocm.py

[492196ed0] Ye (Charlotte) Qi 2025-09-10 [CI/Build] split true unit tests to Entrypoints Unit Tests (#24418)
16	5	.buildkite/test-pipeline.yaml
1	1	tests/entrypoints/test_api_server_process_manager.py

[f4f1a8df2] Nick Hill 2025-09-10 [BugFix] Ensure integrity of reused CPU tensors during async scheduling (#24527)
24	8	vllm/v1/worker/gpu_model_runner.py

[0b9a612fa] lacora 2025-09-10 [BugFix][easy] Fix flaky test test_gpt_oss_multi_turn_chat (#24549)
1	1	tests/entrypoints/openai/test_serving_chat.py

[4c04eef70] Wenlong Wang 2025-09-10 [BugFix][Multi Modal] Fix TensorSchema shape mismatch in Molmo (#24559)
8	6	vllm/model_executor/models/molmo.py

[f36355abf] Harry Mellor 2025-09-10 Move `LoadConfig` from `config/__init__.py` to `config/load.py` (#24566)
3	3	tests/lora/test_worker.py
2	1	tests/model_executor/model_loader/test_registry.py
1	1	tests/runai_model_streamer_test/test_runai_model_streamer_loader.py
3	2	tests/test_config.py
2	1	tests/v1/spec_decode/test_eagle.py
1	88	vllm/config/__init__.py
104	0	vllm/config/load.py
3	2	vllm/model_executor/model_loader/__init__.py
2	1	vllm/model_executor/model_loader/base_loader.py
2	1	vllm/model_executor/model_loader/bitsandbytes_loader.py
2	1	vllm/model_executor/model_loader/default_loader.py
2	1	vllm/model_executor/model_loader/dummy_loader.py
2	1	vllm/model_executor/model_loader/gguf_loader.py
2	1	vllm/model_executor/model_loader/runai_streamer_loader.py
2	1	vllm/model_executor/model_loader/sharded_state_loader.py
2	1	vllm/model_executor/model_loader/tensorizer_loader.py
2	1	vllm/model_executor/model_loader/weight_utils.py

[9e3c3a7df] Yash Pratap Singh 2025-09-10 [LoRA]: Add LoRA support to Mistral's Voxtral models (#24517)
1	1	docs/models/supported_models.md
18	3	vllm/model_executor/models/voxtral.py

[6cbd41909] baonudesifeizhai 2025-09-10 Feature/vit attention unification# 23880 (#23978)
17	5	tests/kernels/attention/test_mha_attn.py
19	6	vllm/attention/layer.py
3	0	vllm/model_executor/models/idefics2_vision_model.py
6	5	vllm/model_executor/models/intern_vit.py
7	10	vllm/model_executor/models/interns1_vit.py
9	15	vllm/model_executor/models/mllama.py
7	16	vllm/model_executor/models/step3_vl.py
1	1	vllm/model_executor/models/vision.py
1	0	vllm/platforms/interface.py

[72d30108a] danielafrimi 2025-09-10 Support for NemotronH Nano VLM (#23644)
3	0	tests/models/registry.py
1	1	vllm/config/__init__.py
1395	0	vllm/model_executor/models/nano_nemotron_vl.py
1	0	vllm/model_executor/models/registry.py

[8b83b9373] Tyler Michael Smith 2025-09-10 [Docs] Document the extra memory footprint overhead when using EPLB (#24537)
7	0	docs/serving/expert_parallel_deployment.md

[9dbefd88e] Harry Mellor 2025-09-10 [Docs] Improve organisation of API Reference nav (#24569)
1	1	docs/.nav.yml

[7c195d43d] vllmellm 2025-09-10 [ROCm][Bugfix] Fix Aiter RMSNorm  (#23412)
22	17	tests/model_executor/test_enabled_custom_ops.py
71	16	vllm/model_executor/layers/layernorm.py
15	3	vllm/platforms/rocm.py

[0ae43dbf8] Lucas Wilkinson 2025-09-10 [Attention] add DCP support for FLASH_ATTN_MLA backend (#24453)
16	2	vllm/v1/attention/backends/mla/flashattn_mla.py
3	0	vllm/v1/worker/gpu_model_runner.py

[267c80d31] li-jinpeng 2025-09-10 [Model] Limit CPU threads for image transformations in InternVL to reduce cpu contention. (#24519)
16	1	vllm/model_executor/models/internvl.py

[77f62613f] Flora Feng 2025-09-10 Consolidate rendering parameters into RenderConfig dataclass (#24543)
45	25	tests/entrypoints/test_renderer.py
8	2	vllm/entrypoints/openai/serving_classification.py
17	8	vllm/entrypoints/openai/serving_completion.py
15	9	vllm/entrypoints/openai/serving_embedding.py
15	1	vllm/entrypoints/openai/serving_engine.py
9	4	vllm/entrypoints/openai/serving_pooling.py
7	4	vllm/entrypoints/openai/serving_tokenization.py
51	55	vllm/entrypoints/renderer.py

[feaf202e9] Remy 2025-09-10 [Bugfix] Guard `_may_reorder_batch` for encoder-only models on CPU (#24319) (#24348)
8	2	tests/models/language/pooling/test_embedding.py
2	1	vllm/config/__init__.py
16	4	vllm/v1/worker/cpu_model_runner.py

[91130ae37] Simon Mo 2025-09-09 [docs] promo pytorch conf and ray summit (#24562)
3	0	README.md

[e40827280] Harry Mellor 2025-09-10 [Docs] Enable relative links in examples to function when rendered in the docs (#24041)
36	7	docs/mkdocs/hooks/generate_examples.py

[4377b1ae3] pwschuurman 2025-09-09 [Bugfix] Update Run:AI Model Streamer Loading Integration (#23845)
4	2	setup.py
39	0	tests/runai_model_streamer_test/test_runai_utils.py
33	31	vllm/config/__init__.py
5	4	vllm/engine/arg_utils.py
8	14	vllm/model_executor/model_loader/runai_streamer_loader.py
99	0	vllm/transformers_utils/runai_utils.py
0	72	vllm/transformers_utils/s3_utils.py

[009d689b0] Chenheli Hua 2025-09-09 [Core] Simplify and unify mm uuid handling & auto-generated mm hash overrides processing.  (#24271)
6	6	tests/v1/engine/test_processor_multi_modal_uuids.py
37	51	vllm/inputs/preprocess.py
5	4	vllm/model_executor/models/deepseek_vl2.py
4	4	vllm/model_executor/models/h2ovl.py
4	3	vllm/model_executor/models/llava.py
3	3	vllm/model_executor/models/mllama.py
4	3	vllm/model_executor/models/paligemma.py
3	3	vllm/model_executor/models/pixtral.py
3	3	vllm/model_executor/models/terratorch.py
5	5	vllm/model_executor/models/transformers.py
4	3	vllm/model_executor/models/voxtral.py
22	28	vllm/multimodal/processing.py
10	11	vllm/v1/engine/processor.py

[0efdb5c3b] Wei 2025-09-09 [gpt-oss] Cache permute indices for faster MXFP4 MoE layer loading (#24154)
86	17	tests/kernels/moe/test_mxfp4_moe.py
60	18	vllm/model_executor/layers/quantization/mxfp4.py

[53b42f410] Wenlong Wang 2025-09-09 [BugFix][Spec Decode] Fix out-of-range index triggered by eagle3; re-enable test for LlamaForCausalLMEagle3 (#24392)
4	5	tests/models/registry.py
24	31	tests/v1/e2e/test_spec_decode.py
7	2	vllm/config/__init__.py
16	1	vllm/model_executor/models/llama.py
4	0	vllm/model_executor/models/llama_eagle3.py
1	2	vllm/model_executor/models/registry.py
2	0	vllm/transformers_utils/configs/eagle.py

[309d7aa40] Chauncey 2025-09-10 [P/D] MultiConnector supports shutdown (#24425)
12	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[b4a01aaf9] Yihua Cheng 2025-09-09 [KV Connector] More async support for `get_num_new_matched_tokens` (#23620)
6	3	vllm/distributed/kv_transfer/kv_connector/v1/base.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
5	1	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
8	0	vllm/v1/core/sched/scheduler.py

[83dd28aae] Nick Hill 2025-09-09 [CI] Adjust threshold for flaky ngram spec decoding test (#24528)
2	2	tests/v1/e2e/test_spec_decode.py

[f88e84016] Nick Hill 2025-09-09 [BugFix] Fix async core engine client finalizer (#24540)
6	4	vllm/v1/engine/core_client.py

[3c2156b3a] Ignacio Sica 2025-09-10 [Hardware][Apple-CPU] Enable native bfloat16 on Apple Silicon (M2 and later) (#24129)
6	6	vllm/platforms/cpu.py

[7e7db0431] Nick Hill 2025-09-09 [CI] Retry flaky fp8 cutlass mla tests (#24536)
7	1	tests/kernels/test_cutlass_mla_decode.py

[41f160b97] Chen Zhang 2025-09-09 Add @heheda12345 to CODEOWNERS of KVCacheManager related code (#24546)
3	0	.github/CODEOWNERS

[dc625ea6b] Yong Hoon Shin 2025-09-09 [Perf] Convert np array to torch tensor to index into block table for attn chunking (#24474)
8	1	vllm/v1/attention/backends/utils.py

[b23fb7862] bnellnm 2025-09-09 [Bugfix] Fix for 24530. Fix naive all2all shared expert overlap. (#24538)
9	6	vllm/model_executor/layers/fused_moe/layer.py

[561f38dc3] Tyler Michael Smith 2025-09-09 [Bugfix] Improve EPLB config validation error message (#24524)
4	2	vllm/config/parallel.py

[73e688cb7] Charlie Fu 2025-09-09 [ROCm][Feature] Enable Pipeline Parallelism with Ray Compiled Graph on ROCm (#24275)
1	0	docker/Dockerfile.rocm
1	1	requirements/rocm.txt
14	2	vllm/utils/__init__.py

[fb1a8f932] Ekagra Ranjan 2025-09-09 [Benchmark] Add option to skip oversampling in benchmark (#24457)
58	13	vllm/benchmarks/datasets.py

[0dc9cbb52] Ekagra Ranjan 2025-09-09 [Benchmark] Update bench doc with mtbench, blazedit, spec bench (#24450)
67	0	benchmarks/README.md

[b5fb3005a] Jiangyun Zhu 2025-09-10 [Log] Use a relative path in debug-level logs to distinguish files with identical names (#23846)
1	1	vllm/logger.py
63	2	vllm/logging_utils/formatter.py

[15de5ff9e] Wentao Ye 2025-09-09 [Feature] Disallow FlashMLA on Blackwell (#24521)
11	0	vllm/attention/backends/flashmla.py
11	0	vllm/v1/attention/backends/mla/flashmla.py

[b8a93076d] Jiangyun Zhu 2025-09-10 [CI] execute all piecewise compilation tests together (#24502)
1	5	.buildkite/test-pipeline.yaml
3	25	tests/compile/piecewise/test_multiple_graphs.py
8	35	tests/compile/piecewise/test_simple.py
2	25	tests/compile/piecewise/test_toy_llama.py
63	0	tests/compile/silly_attention.py
4	27	tests/compile/test_decorator.py

[c3f9773b2] Chenyaaang 2025-09-09 [TPU] Fix tpu structured decoding in mixed batches (#24458)
14	20	vllm/v1/worker/tpu_model_runner.py

[3707cb250] Nicolò Lucchesi 2025-09-09 [Docs] Gemma3n `transcriptions` endpoint support (#24512)
1	0	docs/models/supported_models.md

[920ed46b0] Kazuhiro Serizawa 2025-09-10 [Misc] bump outlines_core to fix the version conflicts with outlines >= 1.2.0 (#24368)
1	1	requirements/common.txt

[15cb047e2] Flora Feng 2025-09-09 Extend renderer with embedding support and integrate completion endpoint (#24405)
9	5	tests/entrypoints/openai/test_prompt_validation.py
133	0	tests/entrypoints/test_renderer.py
1	1	tests/v1/entrypoints/openai/test_completion.py
13	2	vllm/entrypoints/openai/protocol.py
28	27	vllm/entrypoints/openai/serving_completion.py
2	13	vllm/entrypoints/openai/serving_embedding.py
2	215	vllm/entrypoints/openai/serving_engine.py
220	47	vllm/entrypoints/renderer.py
3	0	vllm/inputs/data.py

[9ad0688e4] Jee Jee Li 2025-09-10 [Bugfix] Fix  hidden_size for multimodal classification model (#24501)
3	2	vllm/model_executor/models/adapters.py
7	0	vllm/model_executor/models/utils.py

[b9a1c4c8a] Gregory Shtrasberg 2025-09-09 [ROCm][CI/Build] Sync ROCm dockerfiles with the ROCm fork (#24279)
3	1	docker/Dockerfile.rocm
30	32	docker/Dockerfile.rocm_base

[1aa427fdc] youkaichao 2025-09-10 [Kernels] Add Flash Linear Attention Kernels (#24518)
1	1	tools/mypy.sh
8	0	vllm/model_executor/layers/fla/__init__.py
17	0	vllm/model_executor/layers/fla/ops/__init__.py
225	0	vllm/model_executor/layers/fla/ops/chunk.py
289	0	vllm/model_executor/layers/fla/ops/chunk_delta_h.py
176	0	vllm/model_executor/layers/fla/ops/chunk_o.py
138	0	vllm/model_executor/layers/fla/ops/chunk_scaled_dot_kkt.py
226	0	vllm/model_executor/layers/fla/ops/cumsum.py
366	0	vllm/model_executor/layers/fla/ops/fused_recurrent.py
39	0	vllm/model_executor/layers/fla/ops/index.py
143	0	vllm/model_executor/layers/fla/ops/l2norm.py
337	0	vllm/model_executor/layers/fla/ops/layernorm_guard.py
44	0	vllm/model_executor/layers/fla/ops/op.py
365	0	vllm/model_executor/layers/fla/ops/solve_tril.py
180	0	vllm/model_executor/layers/fla/ops/utils.py
114	0	vllm/model_executor/layers/fla/ops/wy_fast.py
3	1	vllm/triton_utils/__init__.py

[1c63a16b6] Micah Williamson 2025-09-09 [Core] Run garbage collector after CUDA graph capture to fix throughput regression (#24128)
1	0	vllm/v1/worker/gpu_model_runner.py

[922d3b401] d.transposed 2025-09-09 [Bugfix] Handle the edge case in detokenizer where processed tokens contain both `stop` str and `eos` token (#23938)
103	0	tests/detokenizer/test_stop_string_while_stop_model_terminates.py
3	6	vllm/v1/engine/detokenizer.py

[19332c047] wang.yuqi 2025-09-09 [Model] Systematic support for fp32 head, pooling models part (#23810)
31	6	tests/models/language/pooling/mteb_utils.py
1	0	tests/models/language/pooling/test_bge_reranker_v2_gemma.py
52	1	vllm/config/__init__.py
23	15	vllm/model_executor/layers/pooler.py
4	6	vllm/model_executor/models/adapters.py
3	1	vllm/model_executor/models/bert.py
8	8	vllm/model_executor/models/bert_with_rope.py
6	4	vllm/model_executor/models/gpt2.py
11	8	vllm/model_executor/models/internlm2.py
1	1	vllm/model_executor/models/jamba.py
8	5	vllm/model_executor/models/jina_vl.py
3	1	vllm/model_executor/models/modernbert.py
4	0	vllm/model_executor/models/qwen2_rm.py
11	5	vllm/model_executor/models/roberta.py

[a55cf41a0] Wentao Ye 2025-09-09 [Compilation][WideEP] Enable Piecewise CUDAGraph for DeepEPHT (#24123)
14	1	vllm/config/compilation.py
7	9	vllm/platforms/cuda.py

[6fb278816] Ye (Charlotte) Qi 2025-09-09 [CI/Build][Doc] Fully deprecate old bench scripts for serving / throughput / latency (#24411)
2	2	benchmarks/README.md
11	185	benchmarks/benchmark_latency.py
11	1318	benchmarks/benchmark_serving.py
11	735	benchmarks/benchmark_throughput.py

[3d2a2de8f] Weixiao Huang 2025-09-09 [RL] fast weight update with zmq + ipc handles (#24295)
77	18	examples/offline_inference/rlhf_colocate.py
75	15	examples/offline_inference/rlhf_utils.py

[1116590b1] Chen Zhang 2025-09-09 [gpt-oss] Validate gpt-oss python tool during initialization (#23856)
2	0	vllm/entrypoints/openai/api_server.py
24	0	vllm/entrypoints/tool.py
4	1	vllm/entrypoints/tool_server.py

[ccb97338a] Roger Wang 2025-09-09 [Misc] Add Codex settings to gitignore (#24493)
4	0	.gitignore

[45c9cb583] Ye (Charlotte) Qi 2025-09-09 [Misc] Add claude settings to gitignore (#24492)
6	2	.gitignore

[e283976f3] WeiQing Chen 2025-09-09 [Performance][MM] Building the inverse permutation in O(n) time in Qwen2_5_VisionTransformer (#24443)
11	1	vllm/model_executor/models/qwen2_5_vl.py

[46876dff3] Didier Durand 2025-09-09 [Doc]: fixing typos to improve docs (#24480)
1	1	docs/features/tool_calling.md
1	1	docs/getting_started/installation/gpu/rocm.inc.md
2	2	examples/tool_chat_template_phi4_mini.jinja
1	1	tests/engine/test_executor.py
2	2	tests/entrypoints/offline_mode/test_offline_mode.py
1	1	tests/kernels/utils.py
2	2	tests/models/language/generation/test_hybrid.py
1	1	tests/tpu/test_quantization_accuracy.py
1	1	vllm/distributed/parallel_state.py

[1823a00d6] Ming Yang 2025-09-08 [Misc] Support bench serve long context (#24373)
31	0	tests/benchmarks/test_serve_cli.py
136	84	vllm/benchmarks/lib/endpoint_request_func.py

[ed16d0f26] Mickaël Seznec 2025-09-09 [Doc] mention fpdb for multiprocess breakpoints (#24452)
28	0	docs/usage/troubleshooting.md

[0cdd21364] 22quinn 2025-09-08 [Misc] Improve Worker process title and logging prefix (#22205)
3	7	vllm/utils/__init__.py
3	3	vllm/v1/engine/core.py
1	1	vllm/v1/engine/utils.py
30	12	vllm/v1/executor/multiproc_executor.py

[948dd3443] Cyrus Leung 2025-09-09 [Bugfix] Fix Apertus HF repo name (#24447)
1	0	docs/models/supported_models.md
1	1	tests/models/language/generation/test_common.py
1	1	tests/models/registry.py

[b2f774577] cong-meta 2025-09-08 Add data_parallel_size to VllmConfig string representation (#24298)
1	0	vllm/config/__init__.py

[82dfb12e5] Zebing Lin 2025-09-09 [Core] Use sha256 bytes instead of BlockHash to reduce GC overhead (#23673)
5	3	examples/online_serving/kv_events_subscriber.py
9	11	tests/utils_/test_utils.py
27	38	tests/v1/core/test_kv_cache_utils.py
122	103	tests/v1/core/test_prefix_caching.py
8	8	tests/v1/core/test_single_type_kv_cache_manager.py
3	2	tests/v1/core/utils.py
7	6	tests/v1/engine/test_engine_args.py
3	2	tests/v1/kv_connector/unit/utils.py
6	11	vllm/config/cache.py
4	3	vllm/distributed/kv_events.py
6	14	vllm/engine/arg_utils.py
6	0	vllm/envs.py
10	17	vllm/utils/__init__.py
18	9	vllm/v1/core/block_pool.py
64	56	vllm/v1/core/kv_cache_utils.py

[bba1042c6] elvischenv 2025-09-09 [Flashinfer] Support Flashinfer TRTLLM FP8-qkv BF16/FP16-out Attention Kernel (#23647)
1	0	benchmarks/kernels/benchmark_trtllm_decode_attention.py
1	0	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
12	0	tests/kernels/attention/test_flashinfer_trtllm_attention.py
4	2	vllm/compilation/fusion_attn.py
4	9	vllm/v1/attention/backends/flashinfer.py

[b6fbc1563] CSWYF3634076 2025-09-09 [BugFix][Model] Fix Ernie4.5-VL hanging on long inputs (#24074)
10	4	vllm/model_executor/models/ernie45_vl.py
8	3	vllm/model_executor/models/ernie45_vl_moe.py

[3e0d4a347] Harry Mellor 2025-09-09 Move `KVTransferConfig` from `config/__init__.py` to `config/kv_transfer.py` (#24434)
1	102	vllm/config/__init__.py
111	0	vllm/config/kv_transfer.py
2	1	vllm/distributed/kv_transfer/kv_connector/factory.py
2	1	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
1	1	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
1	1	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
1	1	vllm/entrypoints/llm.py

[562663a04] dependabot[bot] 2025-09-09 Bump actions/github-script from 7.0.1 to 8.0.0 (#24413)
1	1	.github/workflows/add_label_automerge.yml
1	1	.github/workflows/issue_autolabel.yml
1	1	.github/workflows/reminder_comment.yml

[ed1623a88] dependabot[bot] 2025-09-09 Bump actions/stale from 9.1.0 to 10.0.0 (#24412)
1	1	.github/workflows/stale.yml

[13b89bd82] cjackal 2025-09-09 [doc] update `vllm serve` cli args documentation (#24329)
4	5	vllm/entrypoints/openai/cli_args.py

[22a007053] dependabot[bot] 2025-09-09 Bump actions/setup-python from 5.4.0 to 6.0.0 (#24414)
1	1	.github/workflows/cleanup_pr_body.yml
1	1	.github/workflows/pre-commit.yml

[170129eb2] zhiweiz 2025-09-08 [gpt-oss] Harmony changes with container tool support (#23386)
79	8	vllm/entrypoints/context.py
44	6	vllm/entrypoints/harmony_utils.py
26	7	vllm/entrypoints/openai/serving_responses.py
10	6	vllm/entrypoints/tool_server.py
11	0	vllm/envs.py

[955c62491] Tyler Michael Smith 2025-09-08 [Bugfix][Wide EP] Fix redundant work when using DeepEP, TP Attn, and EP MoE (#24134)
17	4	vllm/model_executor/layers/fused_moe/layer.py
1	6	vllm/model_executor/models/deepseek_eagle.py
9	18	vllm/model_executor/models/deepseek_mtp.py
105	31	vllm/model_executor/models/deepseek_v2.py

[4f87abdcc] Zhiyu 2025-09-08 Update reviewers for modelopt related files (#24468)
14	0	.github/mergify.yml

[6910b56da] Sahithi Chigurupati 2025-09-08 [CI] Add nightly multiarch manifests to dockerhub (#24102)
22	0	.buildkite/release-pipeline.yaml
97	0	.buildkite/scripts/cleanup-nightly-builds.sh

[e10fef088] R3hankhan 2025-09-09 [Hardware][IBM Z] Fix Outlines Core issue for s390x (#24034)
44	2	docker/Dockerfile.s390x
1	2	requirements/common.txt

[e680723eb] Chauncey 2025-09-09 [Bugfix] Disable the statslogger if the api_server_count is greater than 1 (#22227)
1	0	vllm/v1/engine/async_llm.py
7	1	vllm/v1/metrics/loggers.py

[620db1fc5] Matthew Bonanni 2025-09-08 [Attention] FlashAttention MLA cudagraph support (#23958)
11	1	tests/compile/piecewise/test_full_cudagraph.py
0	5	tests/v1/attention/test_mla_backends.py
10	0	tests/v1/cudagraph/test_cudagraph_mode.py
14	8	vllm/v1/attention/backends/mla/common.py
71	6	vllm/v1/attention/backends/mla/flashattn_mla.py
6	5	vllm/v1/attention/backends/mla/flashmla.py
6	4	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[41183c1fe] Ekagra Ranjan 2025-09-08 [Spec Decode] Fix offline spec_decode.py (#24257)
4	0	vllm/benchmarks/datasets.py

[43d9ad03b] Yang Kaiyong 2025-09-09 [Model loader]: support multi-thread model weight loading (#23928)
41	12	vllm/model_executor/model_loader/default_loader.py
64	0	vllm/model_executor/model_loader/weight_utils.py

[7be141b2c] Jiangyun Zhu 2025-09-09 [CI] Enable encoder model compilation test (#24442)
17	14	tests/compile/test_basic_correctness.py

[8d7f39b48] Jee Jee Li 2025-09-09 [Model] Remove quantized mixtral (#24437)
0	1	tests/models/registry.py
0	16	vllm/model_executor/model_loader/utils.py
0	454	vllm/model_executor/models/mixtral_quant.py
0	1	vllm/model_executor/models/registry.py

[cd0863692] Ekagra Ranjan 2025-09-08 [Spec Decode][Benchmark] Add Blitzedit dataset (#23605)
113	0	vllm/benchmarks/datasets.py

[3feeeb9fe] Ekagra Ranjan 2025-09-08 [Spec Decode][Benchmark] Add Spec Bench Dataset for benchmarking (#23563)
79	1	vllm/benchmarks/datasets.py

[6f4a82f8b] Jee Jee Li 2025-09-09 [Model] Enable BNB support for qwen2_5_omni_thinker (#24420)
29	2	vllm/model_executor/models/qwen2_5_omni_thinker.py

[c44797a4d] rongfu.leng 2025-09-09 [Docs]add eplb_config param use docs (#24213)
27	8	docs/serving/expert_parallel_deployment.md

[55be93baf] Didier Durand 2025-09-08 [Doc]: fix 2 hyperlinks leading to Ray site after they changed Ray's doc structure (#24438)
2	2	docs/serving/parallelism_scaling.md

[717fc00e9] Harry Mellor 2025-09-08 [Docs] Move feature compatibility tables to README (#24431)
1	4	docs/.nav.yml
5	3	docs/features/{compatibility_matrix.md => README.md}

[01dfb5e98] Chenheli Hua 2025-09-08 [Frontend] User-provided uuids for medias in chat. (RFC #22044) (#23449)
33	9	docs/features/multimodal_inputs.md
129	0	tests/entrypoints/openai/test_vision.py
740	28	tests/entrypoints/test_chat_utils.py
138	43	vllm/entrypoints/chat_utils.py
4	1	vllm/entrypoints/llm.py
5	1	vllm/entrypoints/openai/serving_engine.py
34	2	vllm/inputs/preprocess.py
4	3	vllm/model_executor/models/terratorch.py

[03dd652c1] Harry Mellor 2025-09-08 Move `KVEventsConfig` from `config/__init__.py` to `config/kv_events.py` (#24433)
1	1	tests/distributed/conftest.py
1	42	vllm/config/__init__.py
50	0	vllm/config/kv_events.py
1	1	vllm/distributed/kv_events.py

[9cd76b71a] Christian Pinto 2025-09-08 [Misc] Terratorch related fixes (#24337)
2	2	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
2	2	examples/online_serving/prithvi_geospatial_mae.py
1	1	requirements/test.in
1	1	requirements/test.txt
1	1	tests/entrypoints/openai/test_skip_tokenizer.py
2	2	tests/models/registry.py
1	1	tests/models/test_terratorch.py
2	4	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/__init__.py
2	18	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
1	2	tests/plugins/prithvi_io_processor_plugin/setup.py
3	3	tests/plugins_tests/test_io_processor_plugins.py

[e04131418] tomeras91 2025-09-08 [Bugfix] Fix mamba2 prefill chunking (#23279)
225	8	tests/kernels/mamba/test_mamba_ssm_ssd.py
3	0	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
6	3	vllm/model_executor/layers/mamba/ops/ssd_combined.py
62	20	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
52	3	vllm/v1/attention/backends/mamba2_attn.py

[5e537f45b] Li Wang 2025-09-08 [Bugfix] Fix get_quant_config when using modelscope (#24421)
4	34	vllm/model_executor/model_loader/default_loader.py
44	2	vllm/model_executor/model_loader/weight_utils.py

[c2a8b08fc] Michael Yao 2025-09-08 [Doc] Fix issues in integrations/llamastack.md (#24428)
7	7	docs/deployment/integrations/llamastack.md

[f4962a6d5] Didier Durand 2025-09-08 [Doc]: fix typos in Python comments (#24417)
1	1	examples/offline_inference/chat_with_tools.py
1	1	vllm/attention/backends/mla/common.py
1	1	vllm/config/__init__.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/engine/multiprocessing/client.py
1	1	vllm/entrypoints/openai/cli_args.py
1	1	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
1	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py
1	1	vllm/v1/worker/block_table.py

[2f0b833a0] Michael Yao 2025-09-08 [Docs] Fix a tip indentation and typo (#24419)
1	1	docs/contributing/profiling.md

[425b04b8f] Chauncey 2025-09-08 [gpt-oss][Responses API] Fix the function call id format (#24409)
1	1	vllm/entrypoints/harmony_utils.py

[60f0843ef] Chatcharin Sangbutsarakum 2025-09-08 [Model] Remove unnecessary CUDA sync of Qwen2VL image and video preprocess (#24334)
8	4	vllm/model_executor/models/qwen2_vl.py

[8a4660260] Chatcharin Sangbutsarakum 2025-09-08 [Model] Remove unnecessary CUDA sync of GLM-4.1V image and video preprocess (#24332)
8	4	vllm/model_executor/models/glm4_1v.py

[61aa4b290] Chauncey 2025-09-08 [P/D] Add a shutdown method to the Connector API (#22699)
4	3	vllm/distributed/kv_transfer/__init__.py
8	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
7	0	vllm/distributed/kv_transfer/kv_transfer_state.py
1	1	vllm/executor/executor_base.py
2	0	vllm/v1/core/sched/scheduler.py
9	7	vllm/v1/executor/multiproc_executor.py
3	0	vllm/v1/worker/gpu_worker.py
7	1	vllm/v1/worker/kv_connector_model_runner_mixin.py
3	0	vllm/v1/worker/tpu_worker.py
8	0	vllm/worker/worker_base.py

[8c892b183] Al-Ekram Elahee Hridoy 2025-09-07 [Doc] Fix UTF-8 encoding issues in documentation generation on Windows (#24361)
2	1	docs/mkdocs/hooks/generate_argparse.py
4	2	docs/mkdocs/hooks/generate_examples.py

[3bca396f7] Chenheli Hua 2025-09-07 [CI/Build] Fix local image inputs in test_pixtral.py (#24401)
11	19	tests/models/multimodal/generation/test_pixtral.py

[3a3e91bdf] 22quinn 2025-09-07 [CI/Build] Disable flaky test_structured_output tests (#24404)
4	4	tests/v1/entrypoints/llm/test_struct_output_generate.py

[b3d7e3c84] Xingyu Liu 2025-09-07 [Sampler] Support returning all prompt logprobs (#23868)
10	2	tests/v1/sample/test_logprobs.py
7	4	vllm/sampling_params.py
18	10	vllm/v1/engine/processor.py
3	2	vllm/v1/worker/gpu_input_batch.py

[67841317d] Yan Ma 2025-09-08 [xpu] upgrade ipex/python3.12 for xpu (#23830)
22	11	docker/Dockerfile.xpu
9	10	docs/getting_started/installation/gpu/xpu.inc.md
3	4	requirements/xpu.txt

[86173ad59] Ming Yang 2025-09-07 [Kernel] Support decode context parallelism on Blackwell with CUTLASS MLA (#24385)
12	5	csrc/attention/mla/sm100_cutlass_mla_kernel.cu
4	4	csrc/torch_bindings.cpp
22	10	tests/kernels/test_cutlass_mla_decode.py
3	3	vllm/_custom_ops.py
22	10	vllm/v1/attention/backends/mla/cutlass_mla.py

[795b6951c] Lucia Fang 2025-09-07 Add @luccafong to codeowner for spec decode (#24397)
4	1	.github/CODEOWNERS

[2e5d21378] Woosuk Kwon 2025-09-07 Skip MM Encoder for non-first PP ranks (#24387)
1	4	vllm/v1/worker/gpu_model_runner.py

[0661cb9df] Flora Feng 2025-09-07 Add renderer-based prompt processing for embedding and classification endpoints (#24356)
4	10	tests/entrypoints/openai/test_truncation.py
17	0	tests/entrypoints/test_renderer.py
5	8	vllm/entrypoints/openai/serving_classification.py
19	26	vllm/entrypoints/openai/serving_embedding.py
7	10	vllm/entrypoints/openai/serving_engine.py
7	2	vllm/entrypoints/renderer.py

[105d3d62e] Woosuk Kwon 2025-09-07 [TPU] Remove TopKTopPSampler dependency for TPU sampler (#24391)
6	2	tests/v1/tpu/test_topk_topp_sampler.py
2	51	vllm/v1/sample/ops/topk_topp_sampler.py
71	4	vllm/v1/sample/tpu/sampler.py

[62f66be1f] Jee Jee Li 2025-09-07 [Bugfix] Fix Qwen3-coder moe tuned config (#24072)
5	1	benchmarks/kernels/benchmark_moe.py
40	40	vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json

[81c53ef55] Ye (Charlotte) Qi 2025-09-06 [Misc] collect flashinfer version in collect_env.py (#24378)
2	0	vllm/collect_env.py

[75334956c] Saman A. Pour 2025-09-06 QWEN3 Thinking Fused MoE kernels Optimization configs (#24330)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json
18	18	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json

[77aec83b8] Jiangyun Zhu 2025-09-07 [Benchmark] add benchmark for custom activation op (#23908)
104	0	benchmarks/kernels/benchmark_activation.py

[e67597545] Aaron Pham 2025-09-06 [CI][Fix] deterministic seed for flaky CI runs on structured outputs (#24380)
3	3	tests/v1/entrypoints/llm/test_struct_output_generate.py

[37a6fa95f] Benji Beck 2025-09-06 Migrate Qwen2 inputs to TensorSchema (#23475)
31	21	vllm/model_executor/models/qwen2_5_omni_thinker.py
103	70	vllm/model_executor/models/qwen2_5_vl.py
29	11	vllm/model_executor/models/qwen2_audio.py
94	62	vllm/model_executor/models/qwen2_vl.py

[558f0907d] youkaichao 2025-09-07 [attention][DCP] use AttentionImpl.need_to_return_lse_for_decode (#24372)
26	0	vllm/attention/backends/abstract.py
0	4	vllm/v1/attention/backends/mla/common.py
2	0	vllm/v1/attention/backends/mla/flashmla.py
10	5	vllm/v1/worker/gpu_model_runner.py

[4172235ab] Woosuk Kwon 2025-09-06 [V0 deprecation] Deprecate V0 Neuron backend (#21159)
0	16	.buildkite/release-pipeline.yaml
0	64	.buildkite/scripts/hardware_ci/run-neuron-test.sh
0	1	MANIFEST.in
0	56	docker/Dockerfile.neuron
0	49	examples/offline_inference/neuron.py
0	61	examples/offline_inference/neuron_eagle.py
0	63	examples/offline_inference/neuron_int8_quantization.py
0	110	examples/offline_inference/neuron_multimodal.py
0	64	examples/offline_inference/neuron_speculation.py
0	9	requirements/neuron.txt
2	34	setup.py
0	9	tests/engine/test_arg_utils.py
0	43	tests/neuron/1_core/test_activation.py
0	154	tests/neuron/1_core/test_block_table.py
0	86	tests/neuron/1_core/test_cache.py
0	57	tests/neuron/1_core/test_layernorm.py
0	95	tests/neuron/1_core/test_logits_processor.py
0	127	tests/neuron/1_core/test_neuron_model_runner.py
0	12	tests/neuron/1_core/test_neuron_quant.py
0	514	tests/neuron/1_core/test_prefix_prefill.py
0	68	tests/neuron/1_core/test_rotary_embedding.py
0	101	tests/neuron/2_core/test_comm_ops.py
0	83	tests/neuron/2_core/test_eagle.py
0	64	tests/neuron/2_core/test_mistral.py
0	97	tests/neuron/2_core/test_multi_lora.py
0	903	vllm/attention/ops/nki_flash_attn.py
1	15	vllm/collect_env.py
2	20	vllm/config/__init__.py
2	3	vllm/config/cache.py
1	4	vllm/config/parallel.py
0	20	vllm/distributed/device_communicators/neuron_communicator.py
0	5	vllm/engine/arg_utils.py
1	1	vllm/envs.py
0	7	vllm/model_executor/custom_op.py
0	7	vllm/model_executor/layers/activation.py
0	3	vllm/model_executor/layers/quantization/__init__.py
0	76	vllm/model_executor/layers/quantization/neuron_quant.py
1	82	vllm/model_executor/layers/rotary_embedding/base.py
0	476	vllm/model_executor/model_loader/neuron.py
0	685	vllm/model_executor/model_loader/neuronx_distributed.py
0	25	vllm/platforms/__init__.py
0	4	vllm/platforms/interface.py
0	151	vllm/platforms/neuron.py
0	455	vllm/worker/neuron_model_runner.py
0	189	vllm/worker/neuron_worker.py
0	294	vllm/worker/neuronx_distributed_model_runner.py

[848562bd4] Bangsheng Tang 2025-09-06 break execute_model in gpu_model_runner into sub-functions for custom scopes (#24265)
4	0	vllm/envs.py
15	4	vllm/v1/utils.py
188	104	vllm/v1/worker/gpu_model_runner.py

[e68dc2f01] elvischenv 2025-09-07 [Bugfix] Fix unstable silu_mul+nvfp4 quant fusion test (#24370)
30	16	tests/compile/test_silu_mul_quant_fusion.py
8	0	tests/kernels/quantization/nvfp4_utils.py

[a3645ed94] Ye (Charlotte) Qi 2025-09-06 [Frontend][Responses API] Support reporting tool output tokens and fix reasoning token count (#24285)
425	0	tests/entrypoints/test_context.py
125	33	vllm/entrypoints/context.py
2	1	vllm/entrypoints/openai/protocol.py
5	2	vllm/entrypoints/openai/serving_responses.py

[fb691ee4e] Aaron Pham 2025-09-06 [Fix] [gpt-oss] fix non-tool calling path for chat completion (#24324)
52	18	tests/entrypoints/openai/test_serving_chat.py
31	20	vllm/entrypoints/openai/serving_chat.py

[6024d115c] Ashwin Phadke 2025-09-06 Lora bias(enable_lora_bias) deprecate warning (#24339)
7	2	vllm/config/__init__.py

[7555d6b34] Jee Jee Li 2025-09-07 [Bugfix] Fix test_mixtral_moe (#24371)
2	2	tests/kernels/moe/test_moe.py

[00a4e56d8] Isotr0py 2025-09-07 [Bugfix] Fix broken deepseek fp8 TP weights loading (#24367)
3	1	vllm/model_executor/layers/linear.py
2	1	vllm/model_executor/layers/quantization/fp8.py

[0eadaeff7] mohankku 2025-09-06 [Bugfix] Avoid uninitialized usage of azp_val when AZP is false. (#24335)
2	1	csrc/cpu/dnnl_kernels.cpp

[0077c8634] Benjamin Chislett 2025-09-06 Add @benchislett to codeowner for spec decode and structured outputs (#24362)
2	1	.github/CODEOWNERS

[b121ca22a] Roger Wang 2025-09-06 [CI] Disable flaky structured output test from CI (#24366)
3	2	tests/v1/entrypoints/llm/test_struct_output_generate.py

[eddaafc1c] Roger Wang 2025-09-06 [Multimodal] Improve max video embedding length estimation in V1 (#24312)
1	4	vllm/model_executor/models/llava_onevision.py
1	4	vllm/model_executor/models/qwen2_vl.py

[305a1cc0d] Andrew Sansom 2025-09-06 refactor: Turn GPUModelRunner.inputs_embeds to a CpuGpuBuffer (#24345)
17	4	vllm/v1/utils.py
20	10	vllm/v1/worker/gpu_model_runner.py

[6d6c6b05d] wang.yuqi 2025-09-06 [New Model]: google/embeddinggemma-300m (#24318)
1	0	docs/models/supported_models.md
16	2	tests/models/language/pooling/mteb_utils.py
6	1	tests/models/language/pooling/test_st_projector.py
1	0	tests/models/registry.py
2	0	vllm/config/__init__.py
17	15	vllm/model_executor/models/adapters.py
9	0	vllm/model_executor/models/config.py
20	11	vllm/model_executor/models/gemma3.py
1	0	vllm/model_executor/models/registry.py

[53b19ccdd] Isotr0py 2025-09-06 [Core] Allow disabling TP sharding for parallel Linear layer (#23024)
70	105	vllm/model_executor/layers/linear.py
19	4	vllm/model_executor/model_loader/bitsandbytes_loader.py
3	3	vllm/model_executor/models/deepseek_v2.py
50	78	vllm/model_executor/models/glm4_1v.py
25	37	vllm/model_executor/models/qwen2_5_vl.py
27	44	vllm/model_executor/models/step3_vl.py
11	11	vllm/model_executor/parameter.py

[6432739ef] Nick Hill 2025-09-05 [Bugfix] Catch and log invalid token ids in detokenizer (#24351)
5	0	vllm/v1/engine/detokenizer.py

[ac201a0ea] yzds 2025-09-06 [Feature] Support Decode Context Parallel (DCP) for MLA (#23734)
2	1	.buildkite/test-pipeline.yaml
0	7	csrc/cache.h
0	103	csrc/cache_kernels.cu
0	10	csrc/torch_bindings.cpp
263	0	tests/distributed/test_context_parallel.py
0	14	vllm/_custom_ops.py
139	0	vllm/attention/ops/common.py
3	1	vllm/attention/ops/flashmla.py
5	0	vllm/config/parallel.py
48	1	vllm/distributed/parallel_state.py
17	0	vllm/engine/arg_utils.py
309	25	vllm/v1/attention/backends/mla/common.py
11	7	vllm/v1/attention/backends/mla/cutlass_mla.py
9	4	vllm/v1/attention/backends/mla/flashattn_mla.py
9	9	vllm/v1/attention/backends/mla/flashmla.py
8	7	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
8	7	vllm/v1/attention/backends/mla/triton_mla.py
50	20	vllm/v1/core/kv_cache_coordinator.py
9	0	vllm/v1/core/kv_cache_manager.py
6	0	vllm/v1/core/kv_cache_utils.py
10	0	vllm/v1/core/sched/scheduler.py
17	2	vllm/v1/core/single_type_kv_cache_manager.py
8	0	vllm/v1/kv_cache_interface.py
50	9	vllm/v1/worker/block_table.py
11	0	vllm/v1/worker/gpu_model_runner.py
4	2	vllm/v1/worker/gpu_worker.py
4	2	vllm/worker/worker.py

[3c529fc99] Yong Hoon Shin 2025-09-05 [KV Sharing] Raise error if using eagle with fast prefill (#24350)
18	0	vllm/config/__init__.py
0	7	vllm/config/cache.py

[35bf19386] Didier Durand 2025-09-06 [Doc]: fix typos in Python comments (#24294)
1	1	csrc/quantization/machete/generate.py
1	1	docs/getting_started/installation/cpu.md
1	1	tests/models/multimodal/generation/vlm_utils/core.py
2	2	vllm/distributed/device_communicators/custom_all_reduce.py
3	3	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	1	vllm/envs.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
2	2	vllm/model_executor/layers/fused_moe/layer.py
2	2	vllm/model_executor/layers/quantization/gptq_marlin.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/engine/core.py
1	1	vllm/v1/worker/gpu_model_runner.py

[35efa7029] 22quinn 2025-09-05 Add @22quinn as code reviewer for RL related components (#24346)
4	3	.github/CODEOWNERS

[cee182b29] Benjamin Chislett 2025-09-05 [Perf][V1] Fully overlap model execution (#23569)
45	5	vllm/v1/executor/multiproc_executor.py
15	0	vllm/v1/outputs.py
5	0	vllm/v1/worker/gpu_input_batch.py
182	21	vllm/v1/worker/gpu_model_runner.py
5	5	vllm/v1/worker/gpu_worker.py

[c954c6629] Rafael Vasquez 2025-09-05 [CI] Add timeouts to tests (#24260)
102	44	.buildkite/test-pipeline.yaml

[9dfbeb41e] Shiyan Deng 2025-09-05 [RFC] allow cancelation after shutdown in blocking collective_rpc (#23390)
8	6	vllm/v1/executor/multiproc_executor.py

[eedb2a2a1] elvischenv 2025-09-06 [Bugfix] Fix silu_mul+quant fusion test (#24341)
2	1	tests/compile/test_silu_mul_quant_fusion.py

[23a6c5280] Chauncey 2025-09-06 [gpt-oss][Bugfix]Fix streamableparser for missing handling of certain token_ids (#24306)
3	3	vllm/entrypoints/context.py

[7812bcf27] youkaichao 2025-09-05 [docs] add shenzhen meetup (#24326)
3	2	README.md
1	0	docs/community/meetups.md

[006e7a34a] Louie Tsai 2025-09-05  Adding int4 and int8 models for CPU benchmarking (#23709)
414	6	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc2.json
621	6	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc3.json
31	1	docs/contributing/benchmarks.md

[e599e2c65] liuzhenwei 2025-09-05 [XPU][P/D] Add XPU support in NixlConnector (#22436)
1	0	requirements/xpu.txt
49	1	vllm/distributed/kv_transfer/kv_connector/utils.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
26	0	vllm/platforms/tpu.py
31	0	vllm/platforms/xpu.py
4	0	vllm/v1/worker/gpu_model_runner.py
2	70	vllm/v1/worker/tpu_model_runner.py

[c29fb540f] Aaron Pham 2025-09-04 [gpt-oss] tool parser supports for /chat/completions [1/n] (#22386)
162	1	tests/entrypoints/openai/test_serving_chat.py
147	0	tests/tool_use/test_openai_tool_parser.py
59	13	vllm/entrypoints/harmony_utils.py
95	37	vllm/entrypoints/openai/serving_chat.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
73	0	vllm/entrypoints/openai/tool_parsers/openai_tool_parser.py
1	1	vllm/model_executor/models/config.py
34	11	vllm/reasoning/gptoss_reasoning_parser.py

[65e038931] Nicolò Lucchesi 2025-09-05 [Frontend] Skip unnecessary detokenization when token_id is requested (#24236)
2	1	vllm/entrypoints/openai/serving_chat.py

[886ccbe5b] Zhuohan Li 2025-09-04 [CI/Build] Reduce the number of redundant cases to test for LoRA (#24276)
2	2	tests/lora/test_layers.py

[adc3ddb43] elvischenv 2025-09-05 [Bugfix][Misc] Fix silu_and_mul_nvfp4_quant issue and extract common utils for nvfp4 kernel source files (#23727)
2	2	.buildkite/test-pipeline.yaml
0	9	csrc/dispatch_utils.h
1	2	csrc/ops.h
28	184	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
16	0	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu
47	263	csrc/quantization/fp4/nvfp4_experts_quant.cu
18	0	csrc/quantization/fp4/nvfp4_quant_entry.cu
17	254	csrc/quantization/fp4/nvfp4_quant_kernels.cu
251	0	csrc/quantization/fp4/nvfp4_utils.cuh
1	2	csrc/torch_bindings.cpp
1	2	tests/kernels/quantization/test_silu_nvfp4_quant_fusion.py

[60b755cbc] Seiji Eicher 2025-09-04 [Misc] Have AsyncLLM `custom_stat_loggers` extend default logger list (#20952)
3	2	tests/v1/engine/test_async_llm.py
83	0	tests/v1/metrics/test_engine_logger_apis.py
7	1	vllm/v1/engine/async_llm.py
6	6	vllm/v1/metrics/loggers.py

[482e52f56] Saman A. Pour 2025-09-04 QWEN3 Coder Fused MoE kernels Optimization configs (#24266)
146	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=640,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=640,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=640,device_name=NVIDIA_H100,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_H100,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=40,N=2560,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=40,N=2560,device_name=NVIDIA_GB200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=40,N=2560,device_name=NVIDIA_H100,dtype=fp8_w8a8,block_shape=[128,128].json

[78336a0c3] Po-Han Huang (NVIDIA) 2025-09-05 Upgrade FlashInfer to v0.3.0 (#24086)
1	1	docker/Dockerfile
1	1	setup.py

[94866d7c9] Jee Jee Li 2025-09-05 [Misc] Slight improve deepgemm print (#24085)
3	2	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py

[83609ca91] Didier Durand 2025-09-04 [Doc]: fix typos in Python comments (#24173)
1	1	benchmarks/benchmark_dataset.py
1	1	benchmarks/kernels/benchmark_lora.py
1	1	benchmarks/multi_turn/benchmark_serving_multi_turn.py
1	1	examples/offline_inference/audio_language.py
1	1	tests/models/multimodal/generation/vlm_utils/builders.py
1	1	tests/models/multimodal/generation/vlm_utils/case_filtering.py
1	1	vllm/attention/backends/mla/common.py
1	1	vllm/engine/async_llm_engine.py
1	1	vllm/engine/multiprocessing/client.py
1	1	vllm/model_executor/layers/quantization/awq_triton.py
1	1	vllm/model_executor/layers/quantization/base_config.py
2	2	vllm/v1/attention/backends/mla/common.py

[e41a0fa37] Nick Hill 2025-09-04 [Perf] Freeze core engine proc heap after init (#24008)
6	0	vllm/v1/engine/core.py

[37241077d] nvjullin 2025-09-04 [Misc] Removed force_fp8_e4m3fnuz from FP8LinearOp (#23725)
16	11	tests/compile/test_fusion.py
15	12	tests/compile/test_silu_mul_quant_fusion.py
9	0	tests/utils.py
3	3	vllm/model_executor/layers/quantization/ptpc_fp8.py
2	4	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[c9f7081f9] Yash Pratap Singh 2025-09-04 [LoRA]: Add lora support to qwen-2.5-omni (#24231)
1	1	docs/models/supported_models.md
13	2	vllm/model_executor/models/qwen2_5_omni_thinker.py

[16ded21ee] Kunshang Ji 2025-09-04 [XPU] support Triton Attention backend on Intel GPU (#24149)
5	4	.buildkite/scripts/hardware_ci/run-xpu-test.sh
2	3	vllm/_ipex_ops.py
6	1	vllm/attention/ops/paged_attn.py
26	2	vllm/platforms/xpu.py
10	5	vllm/v1/attention/backends/triton_attn.py

[2b30afa44] nopperl 2025-09-04 Use hidden_size_per_head as head_size fallback (#24221)
5	0	vllm/config/__init__.py

[eafa8dcde] Jiangyun Zhu 2025-09-04 [Model] Add pp support for hunyuan (#24212)
2	2	docs/models/supported_models.md
2	2	vllm/model_executor/models/hunyuan_v1.py

[6c7af8110] TJian 2025-09-04 [Doc] Update vLLM Singapore Meetup info (#24234)
1	0	README.md
1	0	docs/community/meetups.md

[8f423e5f4] Kebe 2025-09-04 [Feature][Response API] Add streaming support for non-harmony (#23741)
16	0	tests/v1/entrypoints/openai/responses/test_basic.py
10	0	vllm/entrypoints/context.py
381	77	vllm/entrypoints/openai/serving_responses.py

[369a07956] Ignacio Sica 2025-09-04 [Hardware][Apple-CPU] Disable OneDNN build for Apple Silicon (#24200)
2	1	cmake/cpu_extension.cmake

[402759d47] Lucas Wilkinson 2025-09-04 [Attention] FlashAttn MLA (#14258)
4	4	.buildkite/check-wheel-size.py
1	1	cmake/external_projects/vllm_flash_attn.cmake
1	1	docker/Dockerfile
83	27	tests/kernels/attention/test_attention_selector.py
0	16	tests/v1/attention/test_attention_backends.py
101	101	tests/v1/attention/test_mla_backends.py
2	0	tests/v1/attention/utils.py
13	0	vllm/attention/utils/fa_utils.py
2	0	vllm/engine/arg_utils.py
1	0	vllm/envs.py
40	28	vllm/platforms/cuda.py
3	2	vllm/platforms/interface.py
2	1	vllm/v1/attention/backends/flashinfer.py
3	2	vllm/v1/attention/backends/linear_attn.py
3	2	vllm/v1/attention/backends/mamba1_attn.py
3	2	vllm/v1/attention/backends/mamba2_attn.py
12	5	vllm/v1/attention/backends/mla/common.py
189	0	vllm/v1/attention/backends/mla/flashattn_mla.py
6	4	vllm/v1/attention/backends/mla/flashmla.py
8	6	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
4	3	vllm/v1/attention/backends/short_conv_attn.py
10	6	vllm/v1/attention/backends/xformers.py

[2c301ee2e] Fanli Lin 2025-09-04 [Bugfix] Fix Incremental Detokenization with `tokenizers == 0.22.0` (#24159)
3	2	vllm/v1/engine/detokenizer.py

[3efb9f4d9] whx 2025-09-04 [Attention][Platform] Refactor MLA to support Custom Op (#23332)
158	0	vllm/model_executor/layers/mla.py
28	58	vllm/model_executor/models/deepseek_v2.py

[04f3c35cf] anthonsu 2025-09-04 Improve flexibility of auto_tune.sh execution. (#23766)
6	0	benchmarks/auto_tune/README.md
30	14	benchmarks/auto_tune/auto_tune.sh

[51d5e9be7] mgazz 2025-09-04 [Core][Model] Terratorch backend integration (#23513)
5	1	examples/offline_inference/prithvi_geospatial_mae.py
1	0	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
1	0	examples/online_serving/prithvi_geospatial_mae.py
1	1	requirements/test.in
1	1	requirements/test.txt
6	0	tests/distributed/test_pipeline_parallel.py
3	0	tests/distributed/test_sequence_parallel.py
3	1	tests/entrypoints/openai/test_chat_template.py
4	2	tests/entrypoints/openai/test_skip_tokenizer.py
9	3	tests/entrypoints/test_chat_utils.py
3	0	tests/models/multimodal/generation/vlm_utils/core.py
1	1	tests/models/multimodal/pooling/test_prithvi_mae.py
3	1	tests/models/multimodal/processing/test_common.py
3	1	tests/models/multimodal/processing/test_tensor_schema.py
3	1	tests/models/multimodal/test_mapping.py
37	3	tests/models/registry.py
4	1	tests/models/test_initialization.py
45	0	tests/models/test_terratorch.py
2	0	tests/models/utils.py
45	58	tests/plugins_tests/test_io_processor_plugins.py
4	1	vllm/config/__init__.py
12	3	vllm/model_executor/models/registry.py
109	129	vllm/model_executor/models/{prithvi_geospatial_mae.py => terratorch.py}

[e7fc70016] bingchen-mi 2025-09-04 [Model] Add MiDashengLM model support (#23652)
1	0	docs/models/supported_models.md
31	0	examples/offline_inference/audio_language.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
788	0	vllm/model_executor/models/midashenglm.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
101	0	vllm/transformers_utils/configs/midashenglm.py

[12e1e63cc] Weida Hong 2025-09-04 [Misc] Enhance output readability of helper script (#24214)
1	1	benchmarks/auto_tune/auto_tune.sh

[57b1ce94f] Li, Jiang 2025-09-04 [CPU] Refactor CPU unquantized linear (#24150)
177	0	csrc/cpu/dnnl_helper.cpp
74	0	csrc/cpu/dnnl_helper.h
54	0	csrc/cpu/dnnl_kernels.cpp
18	0	csrc/cpu/torch_bindings.cpp
70	0	tests/kernels/test_onednn.py
29	0	vllm/_custom_ops.py
4	21	vllm/model_executor/layers/linear.py
34	5	vllm/model_executor/layers/utils.py
6	0	vllm/model_executor/layers/vocab_parallel_embedding.py

[cb55ad86f] Benji Beck 2025-09-03 Migrate ultravox inputs to TensorSchema (#23503)
28	30	vllm/model_executor/models/ultravox.py

[712b273f6] Flora Feng 2025-09-03 [Refactor] Introduce basic Renderer for completion-style request (#24010)
163	0	tests/entrypoints/test_renderer.py
15	5	vllm/entrypoints/openai/serving_engine.py
12	14	vllm/entrypoints/openai/serving_pooling.py
7	8	vllm/entrypoints/openai/serving_tokenization.py
219	0	vllm/entrypoints/renderer.py

[e919d6f54] Qiming Zhang 2025-09-03 [Kernel][Bugfix] Fix grouped topk cu (#24146)
7	6	csrc/moe/grouped_topk_kernels.cu

[a38f8bd54] wuhang 2025-09-04 [Feature][Responses API]Support MCP tools with streaming mode + background mode (#23927)
18	1	tests/entrypoints/openai/test_response_api_with_harmony.py
14	2	vllm/entrypoints/openai/api_server.py
106	23	vllm/entrypoints/openai/serving_responses.py

[b5ee1e326] Peter Pan 2025-09-04 Remove deprecated `PyNcclConnector` (#24151)
2	2	benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh
2	2	benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
6	6	examples/offline_inference/disaggregated_prefill.py
2	2	examples/online_serving/disaggregated_prefill.sh
1	1	tests/kv_transfer/test_lookup_buffer.py
1	1	tests/kv_transfer/test_send_recv.py
1	1	vllm/config/__init__.py

[36c260dad] George Nagy II 2025-09-03 [Feature][gpt-oss] Add support for num_cached_tokens and num_reasoning_tokens tracking (#23460)
21	0	vllm/entrypoints/context.py

[a43a3f177] Kebe 2025-09-04 [Bugfix][DP] DP distribution does not require ray[default] (#23822)
10	14	vllm/v1/engine/utils.py

[6adaed42f] WeiQing Chen 2025-09-04 [Feature][P/D]: Optimize NIXL Connector xfer Launch (#23887)
18	13	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[a74232209] Matthew Bonanni 2025-09-03 [Attention] Blackwell FP8 MLA support with CUTLASS_MLA backend (#23289)
8	8	csrc/attention/mla/sm100_cutlass_mla_kernel.cu
167	83	tests/kernels/test_cutlass_mla_decode.py
2	2	vllm/platforms/cuda.py
9	14	vllm/v1/attention/backends/mla/cutlass_mla.py

[731a6940e] Benji Beck 2025-09-03 Migrate whisper inputs to TensorSchema (#23505)
12	4	vllm/model_executor/models/whisper.py

[e9b92dcd8] bnellnm 2025-09-03 [Kernels] Overlap shared experts with send/recv (#23273)
6	2	docs/design/fused_moe_modular_kernel.md
8	0	examples/offline_inference/data_parallel.py
74	12	tests/kernels/moe/test_pplx_moe.py
149	0	tests/kernels/moe/utils.py
1	6	vllm/distributed/device_communicators/all2all.py
4	1	vllm/distributed/device_communicators/base_device_communicator.py
91	39	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
41	9	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
1	3	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
1	3	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
146	37	vllm/model_executor/layers/fused_moe/layer.py
118	24	vllm/model_executor/layers/fused_moe/modular_kernel.py
71	6	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
1	3	vllm/model_executor/layers/fused_moe/prepare_finalize.py
2	2	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
6	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	2	vllm/model_executor/layers/quantization/experts_int8.py
2	2	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/layers/quantization/gguf.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	2	vllm/model_executor/layers/quantization/modelopt.py
2	2	vllm/model_executor/layers/quantization/moe_wna16.py
2	2	vllm/model_executor/layers/quantization/mxfp4.py
3	3	vllm/model_executor/layers/quantization/quark/quark_moe.py
2	2	vllm/model_executor/layers/quantization/rtn.py
6	0	vllm/model_executor/layers/shared_fused_moe/__init__.py
56	0	vllm/model_executor/layers/shared_fused_moe/shared_fused_moe.py
63	40	vllm/model_executor/models/deepseek_v2.py
2	0	vllm/model_executor/models/glm4_moe.py
16	13	vllm/model_executor/models/llama4.py
2	1	vllm/v1/worker/gpu_worker.py

[fa4311d85] nopperl 2025-09-04 [V1] v1 engine + full CUDA graph support for PLaMo2 (#23998)
1	1	docs/models/supported_models.md
1	1	docs/usage/v1_guide.md
3	2	tests/models/language/generation/test_hybrid.py
0	2	tests/models/registry.py
1	0	vllm/config/compilation.py
344	120	vllm/model_executor/models/plamo2.py

[6d80ae83e] Burkhard Ringlein 2025-09-03 [Bugfix] Fixing division by zero in triton_attn if query_heads/kv_heads > 16  (#23424)
2	1	vllm/attention/ops/triton_unified_attention.py

[4ba0c587b] dongbo910220 2025-09-03 FIX: Add libnuma-dev to Dockerfile for dev stage (#20388)
2	0	docker/Dockerfile

[6997a25ac] qscqesze 2025-09-03 [Model] Remove useless code from MiniMax implementation (#23982)
1	11	vllm/model_executor/layers/mamba/linear_attn.py

[28f350e14] Jakub Smid 2025-09-03 Support add_generation_prompt in embeddings endpoint with chat request (#23931)
8	0	vllm/entrypoints/openai/protocol.py
1	3	vllm/entrypoints/openai/serving_embedding.py

[51383bd47] wang.yuqi 2025-09-03 [CI] Accelerate mteb test by setting SentenceTransformers mteb score to a constant (#24088)
3	1	tests/entrypoints/openai/correctness/test_mteb_embed.py
15	16	tests/entrypoints/openai/correctness/test_mteb_score.py
1	4	tests/models/language/pooling/embed_utils.py
24	12	tests/models/language/pooling/mteb_utils.py
4	0	tests/models/language/pooling/test_baai.py
1	2	tests/models/language/pooling/test_bge_reranker_v2_gemma.py
2	0	tests/models/language/pooling/test_cross_encoder.py
1	4	tests/models/language/pooling/test_embedding.py
15	11	tests/models/language/pooling/test_gte.py
3	1	tests/models/language/pooling/test_intfloat.py
2	0	tests/models/language/pooling/test_jina.py
1	0	tests/models/language/pooling/test_mxbai_rerank.py
2	0	tests/models/language/pooling/test_nomic.py
1	0	tests/models/language/pooling/test_qwen3_reranker.py
6	1	tests/models/language/pooling/test_snowflake_arctic_embed.py
1	0	tests/models/language/pooling/test_st_projector.py
1	0	tests/models/utils.py

[9c99e4871] Isotr0py 2025-09-03 [Misc] Clean up deadcode for legacy processing pipeline (#24153)
0	3	tests/models/multimodal/processing/test_tensor_schema.py
1	4	vllm/multimodal/utils.py

[70549c124] dsinghvi 2025-09-03 [CI/Build] Serve images used by multimodal tests through local HTTP Server (#23907)
122	0	tests/conftest.py
3	2	tests/entrypoints/llm/test_chat.py
23	20	tests/entrypoints/openai/test_vision.py
10	9	tests/entrypoints/openai/test_vision_embedding.py
32	25	tests/models/multimodal/generation/test_pixtral.py
15	16	tests/multimodal/test_utils.py
16	14	tests/v1/entrypoints/openai/responses/test_image.py
7	6	tests/v1/tpu/test_multimodal.py
22	6	vllm/assets/image.py

[f0c503f66] Nicolò Lucchesi 2025-09-03 [Nixl] Heterogeneous TP support FlashInfer (#20189)
53	9	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[f38035c12] youkaichao 2025-09-03 [distributed][rl] remove nccl cumem env var override (#24141)
1	1	docs/usage/troubleshooting.md
0	18	vllm/env_override.py

[426cc8629] Yong Hoon Shin 2025-09-02 [BugFix] Fix routed_scaling_factor double mul for dots1 and glm4 MoE models (#24132)
2	1	vllm/model_executor/models/dots1.py
2	1	vllm/model_executor/models/glm4_moe.py

[e81d4e69c] Jiangyun Zhu 2025-09-03 [Misc] Add check for dual_chunk_attention (#24070)
6	1	vllm/config/__init__.py

[02d411fdb] Didier Durand 2025-09-03 [Doc]: fix typos in Python comments (#24115)
1	1	.buildkite/nightly-benchmarks/scripts/compare-json-results.py
1	1	benchmarks/benchmark_serving.py
1	1	benchmarks/benchmark_serving_structured_output.py
1	1	benchmarks/benchmark_throughput.py
1	1	tools/profiler/visualize_layerwise_profile.py
1	1	vllm/compilation/collective_fusion.py
1	1	vllm/engine/multiprocessing/engine.py
1	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
1	1	vllm/model_executor/model_loader/default_loader.py
1	1	vllm/v1/worker/xpu_worker.py
1	1	vllm/worker/worker.py

[d7e1e5997] Didier Durand 2025-09-03 [Doc]: fix typos in Python comments (#24093)
1	1	tests/core/test_scheduler.py
1	1	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py
1	1	tests/entrypoints/openai/test_return_token_ids.py
1	1	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/kernels/utils.py
2	2	tests/multimodal/test_utils.py
1	1	tests/v1/e2e/test_spec_decode.py
2	2	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
2	2	tests/v1/spec_decode/test_tree_attention.py
1	1	vllm/lora/utils.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
5	5	vllm/multimodal/utils.py
1	1	vllm/v1/attention/backends/utils.py
2	2	vllm/v1/structured_output/utils.py
1	1	vllm/v1/worker/tpu_worker.py

[c4ed78b14] Wentao Ye 2025-09-02 [Compile] Fix Compile Warning for `w4a8_mm_entry.cu` (#23660)
9	3	csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu

[1bd007f23] co63oc 2025-09-03 fix some typos (#24071)
1	1	benchmarks/benchmark_block_pool.py
1	1	benchmarks/benchmark_ngram_proposer.py
1	1	csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu
2	2	docs/configuration/optimization.md
1	1	docs/design/io_processor_plugins.md
1	1	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
1	1	examples/online_serving/prithvi_geospatial_mae.py
1	1	tests/compile/piecewise/test_multiple_graphs.py
1	1	tests/kernels/moe/test_mxfp4_moe.py
1	1	tests/models/multimodal/processing/test_mllama4.py
1	1	tests/quantization/test_modelopt.py
1	1	tests/samplers/test_beam_search.py
1	1	tests/v1/attention/test_chunked_local_attention.py
7	7	tests/v1/kv_connector/unit/test_shared_storage_connector.py
1	1	tests/v1/logits_processors/test_custom_offline.py
1	1	vllm/benchmarks/serve.py
1	1	vllm/config/compilation.py
1	1	vllm/config/parallel.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	1	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/model_executor/layers/activation.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/transform/module.py
1	1	vllm/model_executor/layers/quantization/mxfp4.py
1	1	vllm/model_executor/models/gemma3n_mm.py
1	1	vllm/model_executor/models/interns1.py
1	1	vllm/third_party/pynvml.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/core/kv_cache_utils.py
1	1	vllm/v1/worker/gpu_input_batch.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/kv_connector_model_runner_mixin.py

[136d853e6] afeldman-nm 2025-09-02 [V1] Wrapper which plumbs request-level logits processors into vLLM batch-level logits processing (#23656)
0	0	examples/offline_inference/{logits_processor.py => logits_processor/custom.py}
151	0	examples/offline_inference/logits_processor/custom_req.py
165	0	examples/offline_inference/logits_processor/custom_req_init.py
33	0	tests/v1/logits_processors/test_custom_offline.py
64	3	tests/v1/logits_processors/utils.py
111	2	vllm/v1/sample/logits_processor/__init__.py

[e32a0e867] Russell Bryant 2025-09-02 Upgrade xgrammar to 0.1.23 (#22988)
1	1	requirements/common.txt
1	8	vllm/v1/worker/gpu_model_runner.py

[42dc59dba] youkaichao 2025-09-03 Update release pipeline post PyTorch 2.8.0 update (#24073)
16	16	.buildkite/release-pipeline.yaml
12	10	.buildkite/scripts/upload-wheels.sh
1	1	tools/install_deepgemm.sh

[862f2ef89] Chaojun Zhang 2025-09-03 [XPU] Fix the bug of LoRA logits on the XPU platform (#24081)
1	1	vllm/lora/layers.py
10	3	vllm/lora/punica_wrapper/punica_xpu.py
4	1	vllm/platforms/xpu.py

[2fd1a40a5] Matthew Bonanni 2025-09-02 [CI/Build] Disable SiluMul NVFP4 quant fusion tests (#24121)
2	2	.buildkite/test-pipeline.yaml

[930a24144] Wentao Ye 2025-09-02 [Bug] R1 Accuracy: Fix `routed_scaling_factor` Double Mul Issue (#24119)
2	1	vllm/model_executor/models/deepseek_v2.py

[457e47197] rasmith 2025-09-02 [AMD][Kernel][Bugfix] Cast offsets tensor bn to tl.int64 to avoid GPU segfault (#23692)
3	3	vllm/attention/ops/prefix_prefill.py

[d328f7894] Thomas Parnell 2025-09-02 [CI] Enable all hf transformers baselines in test_hybrid (#23936)
22	52	tests/models/language/generation/test_hybrid.py
9	4	tests/models/registry.py

[98aee612a] Wentao Ye 2025-09-02 [Log] Only Print Profiler Results on Rank 0 (#23370)
4	2	vllm/v1/worker/gpu_worker.py
4	2	vllm/worker/worker.py

[598bd74cf] nathan 2025-09-02 Fix weights loading for Apertus (#24100)
6	0	vllm/model_executor/models/apertus.py

[241779847] Mark McLoughlin 2025-09-02 [Metrics] Deprecate TPOT in favor of ITL (#24110)
6	6	examples/online_serving/prometheus_grafana/grafana.json
15	7	tests/entrypoints/openai/test_metrics.py
4	4	vllm/engine/llm_engine.py
16	2	vllm/engine/metrics.py
1	1	vllm/engine/metrics_types.py
19	3	vllm/v1/metrics/loggers.py
3	3	vllm/v1/metrics/stats.py

[9480ae24e] Kyuyeun Kim 2025-09-02 [Bugfix] Fix packed_factor missing attribute error (#23902)
8	8	vllm/model_executor/layers/linear.py

[f399182e8] Chenheli Hua 2025-09-02 Run ruff format on a few files. (#24075)
578	452	tests/entrypoints/test_chat_utils.py
228	150	vllm/entrypoints/chat_utils.py
141	88	vllm/entrypoints/openai/serving_engine.py

[1c4131058] Kyle Sayers 2025-09-02 [Bugfix] Fix transform_config parsing in Compressed Tensors (#23945)
2	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[c83c4ff81] Jiangyun Zhu 2025-09-03 [Benchmark] Add support for local hf dataset path in benchmark (#23999)
6	1	benchmarks/README.md
50	14	vllm/benchmarks/datasets.py

[0e1759cd5] Peter Pan 2025-09-03 [docs] add SYS_NICE cap & `security-opt` for docker/k8s (#24017)
32	0	docs/getting_started/installation/cpu.md
4	0	docs/getting_started/installation/cpu/arm.inc.md
3	0	docs/getting_started/installation/cpu/s390x.inc.md
1	0	docs/getting_started/installation/cpu/x86.inc.md

[e66ed3e67] Michael Goin 2025-09-02 [CI Failure] Skip failing nvfp4 silu test (#23959)
2	1	tests/kernels/quantization/test_silu_nvfp4_quant_fusion.py

[e0653f6c0] wang.yuqi 2025-09-03 [Model] Classification models support logit_bias / sigmoid_normalize (#24031)
24	21	vllm/config/__init__.py
8	0	vllm/model_executor/layers/pooler.py
3	1	vllm/model_executor/models/config.py
3	8	vllm/model_executor/models/jina_vl.py

[38ba061f6] Kyungmin Lee 2025-09-02 [BugFix] Fix EXAONE4 rotary embeddings (#23918)
3	3	vllm/model_executor/models/exaone4.py

[0a74e9d0f] Nicolò Lucchesi 2025-09-02 [Gemma3n] Fix audio batching (#24052)
42	0	examples/online_serving/openai_chat_completion_client_for_multimodal.py
21	7	vllm/model_executor/models/gemma3n_mm.py

[8bd584498] Christian Berge 2025-09-02 correct LWS deployment yaml (#23104)
2	4	docs/deployment/frameworks/lws.md
1	1	examples/online_serving/multi-node-serving.sh

[ce30dca5c] Aziz 2025-09-02 [CI]: reduce HTTP calls inside entrypoints openai tests (#23646)
29	0	tests/entrypoints/conftest.py
0	2	tests/entrypoints/openai/test_chat.py
0	26	tests/entrypoints/openai/test_completion.py
1	26	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
0	8	tests/entrypoints/openai/test_lora_adapters.py
0	8	tests/entrypoints/openai/test_models.py
0	2	tests/entrypoints/openai/test_return_tokens_as_ids.py
0	2	tests/entrypoints/openai/test_tokenization.py

[2f0bab3f2] WeiQing Chen 2025-09-02 [Model] Support dp on ViT on GLM-4.5V (#23168)
1	0	docs/configuration/optimization.md
144	59	vllm/model_executor/models/glm4_1v.py

[fad73be1a] Didier Durand 2025-09-02 [Doc]: fix typos in Python comments (#24077)
1	1	tests/async_engine/test_api_server.py
2	2	tests/core/block/e2e/test_correctness.py
1	1	tests/engine/test_arg_utils.py
1	1	tests/kernels/moe/test_deepep_deepgemm_moe.py
2	2	tests/lora/test_add_lora.py
2	2	tests/lora/test_lora_allowed_token_ids.py
1	1	tests/models/language/generation/test_common.py
2	2	tests/models/language/generation/test_mistral.py
2	2	tests/models/multimodal/generation/test_qwen2_vl.py
1	1	tests/v1/core/test_kv_cache_utils.py
1	1	tests/v1/executor/test_executor.py
1	1	tests/v1/spec_decode/test_eagle.py
1	1	tests/v1/test_kv_sharing.py
1	1	tests/v1/worker/test_gpu_model_runner.py

[56d04089e] Benji Beck 2025-09-01 Migrate Interns1 inputs to TensorSchema (#23510)
50	51	vllm/model_executor/models/interns1.py

[7be0cb8e9] Yan Ma 2025-09-02 [XPU][Feature] fp8 online quantization support for XPU (#23148)
55	1	vllm/_ipex_ops.py
25	0	vllm/model_executor/layers/quantization/fp8.py
158	1	vllm/model_executor/layers/quantization/ipex_quant.py
4	0	vllm/platforms/xpu.py

[1fa1d6a9a] Benji Beck 2025-09-01 Migrate OvisImagePatchInputs to TensorSchema (#22024)
20	19	vllm/model_executor/models/ovis.py

[d59c98644] Maximilien de Bayser 2025-09-02 Remove runtime checks based on pooling params (#24051)
6	11	vllm/v1/worker/gpu_input_batch.py
8	12	vllm/v1/worker/gpu_model_runner.py

[04d0c6077] damon 2025-09-02 [Bugfix] Fix the issue that Blip2ForConditionalGeneration' object has… (#24028)
2	2	vllm/model_executor/models/blip2.py

[2b41cbbf0] Asaf Joseph Gardin 2025-09-02 [V1][Mamba1] - FP32 SSM Kernel Support (#23506)
47	25	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
6	1	tests/models/language/generation/test_hybrid.py
12	6	vllm/model_executor/layers/mamba/mamba_utils.py

[0235103cb] Didier Durand 2025-09-02 [Doc]: fix typos in Python comments (#24042)
1	1	vllm/distributed/device_communicators/quick_all_reduce.py
1	1	vllm/distributed/device_communicators/ray_communicator.py
1	1	vllm/entrypoints/openai/run_batch.py
1	1	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/executor/ray_utils.py
1	1	vllm/model_executor/layers/quantization/utils/quant_utils.py
1	1	vllm/model_executor/models/registry.py
1	1	vllm/model_executor/sampling_metadata.py
1	1	vllm/scalar_type.py
1	1	vllm/sequence.py
3	3	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/metrics/stats.py

[a344a5aa0] Lucia Fang 2025-09-01 [bugfix]fix MTP hidden states (#24056)
1	0	vllm/v1/spec_decode/eagle.py

[568537027] Woosuk Kwon 2025-09-01 [Chore][V0 Deprecation] Move LogProb to a separate file (#24055)
1	1	vllm/beam_search.py
1	1	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_engine.py
2	2	vllm/entrypoints/openai/serving_responses.py
28	0	vllm/logprobs.py
2	2	vllm/model_executor/layers/sampler.py
2	2	vllm/model_executor/model_loader/neuron.py
2	2	vllm/model_executor/model_loader/neuronx_distributed.py
3	2	vllm/outputs.py
1	24	vllm/sequence.py
3	2	vllm/transformers_utils/detokenizer.py
1	1	vllm/v1/engine/logprobs.py

[a0e0efd6b] WeiQing Chen 2025-09-02 [Model] Support DP for ViT on Kimi-VL-A3B-Thinking-2506 (#23817)
1	0	docs/configuration/optimization.md
12	6	tests/multimodal/test_utils.py
39	15	vllm/model_executor/models/kimi_vl.py
55	22	vllm/model_executor/models/moonvit.py
8	4	vllm/model_executor/models/qwen2_5_vl.py
42	15	vllm/multimodal/utils.py

[cf91a89dd] Christian Pinto 2025-09-01 [docs][misc] IOProcessor plugins fixes (#24046)
2	2	docs/design/io_processor_plugins.md
1	0	examples/online_serving/prithvi_geospatial_mae.py
1	18	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
1	0	tests/plugins_tests/test_io_processor_plugins.py
2	1	vllm/entrypoints/openai/protocol.py
7	2	vllm/plugins/io_processors/interface.py

[39a22dcaa] Woosuk Kwon 2025-09-01 [Misc] Minor code simplification for spec decode (#24053)
7	7	vllm/v1/core/sched/scheduler.py

[41c80698b] Julien Debache 2025-09-01 Document multi-proc method selection for profiling (#23802)
2	0	docs/contributing/profiling.md

[7c8271cd1] Kwai-Keye 2025-09-01 [Model]: support KeyeVL-1_5-8B (#23838)
2	1	docs/models/supported_models.md
32	0	examples/offline_inference/vision_language.py
38	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
129	0	vllm/model_executor/layers/rotary_embedding/mrope.py
174	134	vllm/model_executor/models/keye.py
601	0	vllm/model_executor/models/keye_vl1_5.py
1	0	vllm/model_executor/models/registry.py

[3e330fcb2] Kay Yan 2025-09-01 [Doc]: Fix CPU install docs: force torch-backend=cpu to avoid GPU torchvision errors (#24033)
2	2	docs/getting_started/installation/cpu/build.inc.md

[d46934b22] Nicolò Lucchesi 2025-09-01 [Frontend] Gemma3n audio `transcriptions`/`translations` endpoint (#23735)
27	0	tests/entrypoints/openai/conftest.py
19	16	tests/entrypoints/openai/test_transcription_validation.py
44	34	tests/entrypoints/openai/test_translation_validation.py
19	0	vllm/entrypoints/openai/protocol.py
6	1	vllm/entrypoints/openai/speech_to_text.py
61	4	vllm/model_executor/models/gemma3n_mm.py
4	2	vllm/model_executor/models/interfaces.py
5	3	vllm/model_executor/models/voxtral.py
4	3	vllm/model_executor/models/whisper.py

[107284959] Didier Durand 2025-09-01 [Doc]: fix typos in Python comments (#24026)
1	1	examples/offline_inference/multilora_inference.py
1	1	vllm/distributed/device_communicators/pynccl.py
2	2	vllm/distributed/parallel_state.py
1	1	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
1	1	vllm/model_executor/layers/fused_moe/moe_pallas.py
1	1	vllm/model_executor/models/ovis.py
3	3	vllm/model_executor/models/phi4mm_audio.py
1	1	vllm/model_executor/models/phi4mm_utils.py
1	1	vllm/third_party/pynvml.py
1	1	vllm/transformers_utils/configs/nemotron.py
1	1	vllm/transformers_utils/configs/nemotron_h.py
1	1	vllm/transformers_utils/processors/ovis.py
1	1	vllm/transformers_utils/processors/ovis2_5.py
1	1	vllm/v1/spec_decode/ngram_proposer.py

[dc1a53186] Jee Jee Li 2025-09-01 [Kernel] Update DeepGEMM to latest commit (#23915)
2	3	docker/Dockerfile
1	1	tools/install_deepgemm.sh

[55602bb2e] wang.yuqi 2025-09-01 [Frontend] Update the warning log when using VLLM_ALLOW_LONG_MAX_MODEL_LEN (#20904)
10	6	vllm/config/__init__.py

[d7fbc6dda] Isotr0py 2025-09-01 [Misc] Enable V1 FP16 inference on pre-Ampere GPUs (#24022)
0	11	vllm/engine/arg_utils.py

[5438967fb] Ning Xie 2025-09-01 [Misc] add hash_function doc string (#24014)
1	0	vllm/v1/core/kv_cache_utils.py

[422e793fa] Code Jesus 2025-08-31 [Bugfix] Add support for `<tool_call>` format in streaming mode for XLAM Tool Parser (#22769)
216	2	tests/tool_use/test_xlam_tool_parser.py
81	23	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[1cb39dbcd] Christian Pinto 2025-09-01 [Misc] IO Processor plugins for pooling models (#22820)
5	0	.buildkite/test-pipeline.yaml
78	0	docs/design/io_processor_plugins.md
2	0	docs/design/plugin_system.md
60	0	examples/offline_inference/prithvi_geospatial_mae_io_processor.py
54	0	examples/online_serving/prithvi_geospatial_mae.py
3	0	tests/conftest.py
8	0	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/__init__.py
449	0	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/prithvi_processor.py
59	0	tests/plugins/prithvi_io_processor_plugin/prithvi_io_processor/types.py
16	0	tests/plugins/prithvi_io_processor_plugin/setup.py
0	12	tests/plugins_tests/conftest.py
137	0	tests/plugins_tests/test_io_processor_plugins.py
9	0	tests/plugins_tests/test_platform_plugins.py
2	0	vllm/config/__init__.py
4	0	vllm/engine/arg_utils.py
4	0	vllm/engine/protocol.py
43	4	vllm/entrypoints/llm.py
3	2	vllm/entrypoints/openai/api_server.py
42	2	vllm/entrypoints/openai/protocol.py
6	4	vllm/entrypoints/openai/serving_engine.py
57	18	vllm/entrypoints/openai/serving_pooling.py
2	1	vllm/inputs/__init__.py
10	0	vllm/inputs/data.py
68	0	vllm/plugins/io_processors/__init__.py
62	0	vllm/plugins/io_processors/interface.py

[437c3ce02] Benji Beck 2025-08-31 Migrate Phi4 inputs to TensorSchema (#23471)
72	39	vllm/model_executor/models/phi4_multimodal.py
57	34	vllm/model_executor/models/phi4mm.py

[499b074bf] Ning Xie 2025-09-01 [Misc] refactor code by import as for torch._inductor.config (#23677)
4	4	vllm/v1/worker/cpu_model_runner.py

[ff0e59d83] Isotr0py 2025-09-01 [CI/Build] Improve Tensor Schema tests speed by avoid engine core initialization (#23357)
1	2	.buildkite/test-pipeline.yaml
52	80	tests/models/multimodal/processing/test_tensor_schema.py
1	1	vllm/model_executor/models/granite_speech.py
2	1	vllm/model_executor/models/mllama.py
1	1	vllm/model_executor/models/ovis.py
98	28	vllm/model_executor/models/ovis2_5.py
2	2	vllm/model_executor/models/phi4mm.py

[b55713683] Woosuk Kwon 2025-08-31 [Misc] Move fast prefill logic to separate method (#24013)
27	21	vllm/v1/worker/gpu_model_runner.py

[acc1a6e10] Jun-Howie 2025-09-01 Fix the bug related to loading GPTP INT3 weights. (#23328)
2	1	vllm/model_executor/layers/quantization/utils/gptq_utils.py

[8c742a66d] Woosuk Kwon 2025-08-31 [Misc] Avoid redundant copy for encoder-only models (#24012)
7	7	vllm/v1/worker/gpu_model_runner.py

[183a70967] JartX 2025-09-01 [BUGFIX] GPTQ quantization compatibility for Qwen3 MOE models (AutoGPTQ and AutoRound-GPTQ) (#23994)
7	1	vllm/model_executor/layers/quantization/gptq.py
3	0	vllm/model_executor/layers/quantization/gptq_marlin.py
7	3	vllm/model_executor/models/qwen3_moe.py

[14b4326b9] Or Ozeri 2025-09-01 v1: Support KV events from connectors (#19737)
2	0	examples/online_serving/kv_events_subscriber.py
5	0	vllm/distributed/kv_events.py
13	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
6	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
6	3	vllm/v1/core/block_pool.py
12	0	vllm/v1/core/sched/scheduler.py

[752d2e1c3] Nick Hill 2025-08-31 [Minor] Fix some random typos in comments (#24009)
1	1	vllm/utils/__init__.py
1	1	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/core/single_type_kv_cache_manager.py

[81eea3d34] Xiaodong Wang 2025-08-31 vllm fix check on max vocab size (#22471)
13	1	vllm/v1/engine/processor.py

[9701352e4] Didier Durand 2025-08-31 [Doc]: fix typos in Python comments (#24001)
1	1	vllm/compilation/monitor.py
1	1	vllm/core/evictor.py
1	1	vllm/engine/llm_engine.py
5	5	vllm/entrypoints/llm.py
1	1	vllm/executor/mp_distributed_executor.py
1	1	vllm/lora/layers.py
1	1	vllm/platforms/interface.py
1	1	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py

[749be00a9] Roger Wang 2025-08-30 [Core][Multimodal] Allow passing `multi_modal_uuids` as multimodal identifiers. (#23394)
35	0	docs/features/multimodal_inputs.md
229	0	tests/v1/engine/test_processor_multi_modal_uuids.py
1	1	vllm/entrypoints/openai/serving_engine.py
19	1	vllm/inputs/data.py
29	15	vllm/inputs/preprocess.py
3	3	vllm/multimodal/__init__.py
1	6	vllm/multimodal/hasher.py
13	2	vllm/multimodal/inputs.py
68	23	vllm/multimodal/processing.py
57	3	vllm/v1/engine/processor.py

[5b8077b8a] Gabriel Marinho 2025-08-30 Fix wrong truncate_prompt_tokens type hint (#22761)
18	23	vllm/entrypoints/llm.py
19	9	vllm/entrypoints/openai/protocol.py
0	1	vllm/entrypoints/openai/serving_chat.py
0	13	vllm/entrypoints/openai/serving_classification.py
0	1	vllm/entrypoints/openai/serving_completion.py
0	14	vllm/entrypoints/openai/serving_embedding.py
19	28	vllm/entrypoints/openai/serving_engine.py
0	2	vllm/entrypoints/openai/serving_pooling.py
3	3	vllm/entrypoints/openai/serving_score.py
20	2	vllm/inputs/preprocess.py
6	1	vllm/pooling_params.py
9	5	vllm/sampling_params.py
1	0	vllm/transformers_utils/tokenizer_group.py
6	0	vllm/utils/__init__.py

[038e9be4e] Andy Lo 2025-08-30 [LoRA] Much faster startup when LoRA is enabled (#23777)
8	3	vllm/v1/worker/gpu_model_runner.py
4	1	vllm/v1/worker/gpu_worker.py
21	9	vllm/v1/worker/lora_model_runner_mixin.py

[68a349114] Ning Xie 2025-08-30 [Misc] enhance type hint for rearrange return value (#23519)
10	7	vllm/distributed/eplb/eplb_state.py

[e80bca309] Ning Xie 2025-08-30 [Refactor] refactor freezing_value/cuda_event initialize outside try finally (#23758)
3	3	vllm/v1/worker/cpu_model_runner.py

[fb4983e11] Ning Xie 2025-08-30 [Misc] add reorder_batch AttentionMetadataBuilder (#23798)
17	0	vllm/v1/attention/backends/utils.py

[379ea2823] sadegh.shokatian 2025-08-30 Add LoRA support for DeepSeek models (V2, V3, R1-0528) (#23971)
3	3	docs/models/supported_models.md
6	2	vllm/model_executor/models/deepseek.py
3	2	vllm/model_executor/models/deepseek_v2.py

[3a6acad43] Jiangyun Zhu 2025-08-30 [Model] Enable encoder DP for MiniCPM-V (#23948)
1	1	docs/configuration/optimization.md
29	14	vllm/model_executor/models/minicpmv.py

[5490d633c] Ning Xie 2025-08-30 [UT] fix unify_kv_cache_configs when kv cache config needs sort (#23843)
8	2	tests/v1/core/test_kv_cache_utils.py

[628d00cd7] Jee Jee Li 2025-08-30 [Bugfix] Fix test_lora_resolvers.py (#23984)
1	0	tests/entrypoints/openai/test_lora_resolvers.py

[4071c76cf] Thomas Parnell 2025-08-30 [V1] [Hybrid] Move MiniMaxLinearAttention into layers/mamba (#23831)
442	0	vllm/model_executor/layers/mamba/linear_attn.py
6	410	vllm/model_executor/models/minimax_text_01.py

[f1bddbd85] Cyrus Leung 2025-08-30 [Core] Cleanup TPU model runner for MM (#23894)
1	31	vllm/v1/worker/tpu_model_runner.py

[9748c5198] Yong Hoon Shin 2025-08-30 [CI] Fix broken compile tests due to unsupported SiluMul+Nvfp4Quant fusion (#23973)
6	4	vllm/compilation/activation_quant_fusion.py

[ee52a3270] Roger Wang 2025-08-29 [CI] Move testing image from remote URL to S3 (#23980)
4	7	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
1	1	vllm/assets/image.py

[8fb85b7bb] Xin Yang 2025-08-29 Add routed_scaling_factor to MoE grouped topk (#23123)
12	0	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
4	3	vllm/model_executor/layers/fused_moe/fused_moe.py
18	0	vllm/model_executor/layers/fused_moe/layer.py
3	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	0	vllm/model_executor/layers/quantization/awq_marlin.py
2	0	vllm/model_executor/layers/quantization/bitsandbytes.py
10	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	0	vllm/model_executor/layers/quantization/experts_int8.py
3	1	vllm/model_executor/layers/quantization/fp8.py
2	0	vllm/model_executor/layers/quantization/gguf.py
2	0	vllm/model_executor/layers/quantization/gptq_marlin.py
4	0	vllm/model_executor/layers/quantization/modelopt.py
2	0	vllm/model_executor/layers/quantization/moe_wna16.py
2	0	vllm/model_executor/layers/quantization/mxfp4.py
4	0	vllm/model_executor/layers/quantization/quark/quark_moe.py
2	0	vllm/model_executor/layers/quantization/rtn.py
1	0	vllm/model_executor/models/deepseek_v2.py
1	0	vllm/model_executor/models/dots1.py
1	0	vllm/model_executor/models/glm4_moe.py

[5b31cb178] dubejf 2025-08-30 [Bugfix] Fix --config arg expansion called from api_server.py (#23944)
22	0	tests/entrypoints/openai/test_cli_args.py
5	2	vllm/utils/__init__.py

[d660c98c1] Roger Wang 2025-08-29 [CI] Fix unavailable image remote URL (#23966)
1	1	tests/models/multimodal/generation/vlm_utils/custom_inputs.py

[5674a4036] Harry Mellor 2025-08-29 [Misc] Make `download_weights_from_hf` more reliable (#23863)
33	18	vllm/model_executor/model_loader/weight_utils.py

[8c3e19999] Yong Hoon Shin 2025-08-29 Revert gemma3n fast prefill changes (#23897)
1	0	tests/v1/e2e/test_kv_sharing_fast_prefill.py
65	354	vllm/model_executor/models/gemma3n.py
1	1	vllm/model_executor/models/gemma3n_mm.py

[1c26b4229] Thomas Parnell 2025-08-29 [Docs] [V1] [Hybrid] Add new documentation re: contributing mamba-based models  (#23824)
28	0	docs/contributing/model/basic.md
5	7	docs/usage/v1_guide.md

[b7adf94c4] Michael Goin 2025-08-29 Tuned H100/H200 triton fp8 block configs for fused_qkv_a_proj (#23939)
1	0	benchmarks/kernels/bench_block_fp8_gemm.py
1	0	benchmarks/kernels/benchmark_w8a8_block_fp8.py
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2112,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
123	3	vllm/model_executor/layers/quantization/utils/configs/N=2112,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json

[4d7fe40fc] 22quinn 2025-08-29 [RL][BugFix] Fix missing tokenizer error for token-in-token-out (#23904)
73	0	tests/entrypoints/openai/test_token_in_token_out.py
5	1	vllm/entrypoints/openai/serving_completion.py
21	17	vllm/entrypoints/openai/serving_engine.py

[0dc953206] yzds 2025-08-30 [BUGFIX ] fix undefined silu_and_mul_nvfp4_quant (#23929)
2	2	csrc/ops.h
2	1	csrc/torch_bindings.cpp
3	1	vllm/compilation/fix_functionalization.py

[72a69132d] vllmellm 2025-08-29 [CI]  Add `aiter` to matching list of issue auto labeller for `rocm` tag (#23942)
4	0	.github/workflows/issue_autolabel.yml

[d90d8eb67] Nick Hill 2025-08-29 [BugFix] Async scheduling and PP compatibility with DP (#23770)
22	37	tests/v1/engine/test_engine_core.py
4	2	tests/v1/test_async_llm_dp.py
6	0	vllm/executor/ray_utils.py
36	33	vllm/v1/engine/core.py
5	4	vllm/v1/executor/abstract.py
12	3	vllm/v1/executor/multiproc_executor.py
22	21	vllm/v1/worker/gpu_worker.py

[0a2f4c079] Lukas Geiger 2025-08-29 [Models] Use in-place adds in Idefics2Vision (#23932)
3	3	vllm/model_executor/models/idefics2_vision_model.py

[1cf3753b9] EduardDurech 2025-08-29 [MODEL] `Apertus` and `XIELU` (#23068)
2	1	tests/models/language/generation/test_common.py
3	0	tests/models/registry.py
3	0	tests/models/test_registry.py
111	0	vllm/model_executor/layers/activation.py
576	0	vllm/model_executor/models/apertus.py
1	0	vllm/model_executor/models/registry.py

[4f7cde727] Adit Chawdhary 2025-08-29 Adds `json_count_leaves` utility function  (#23899)
33	3	tests/utils_/test_utils.py
27	5	vllm/multimodal/cache.py
12	2	vllm/utils/jsontree.py

[67c14906a] Huy Do 2025-08-29 Update PyTorch to 2.8.0 (#20358)
2	2	.buildkite/test-pipeline.yaml
2	2	CMakeLists.txt
1	1	pyproject.toml
2	1	requirements/build.txt
4	5	requirements/cpu.txt
5	5	requirements/cuda.txt
4	4	requirements/rocm-build.txt
3	3	requirements/test.in
18	18	requirements/test.txt
1	1	tests/distributed/test_sequence_parallel.py
5	1	tests/lora/test_chatglm3_tp.py
3	2	vllm/v1/attention/backends/flex_attention.py

[69f46359d] Flora Feng 2025-08-29 [Multimodal] Consolidate mm inputs into MultiModalFeatureSpec (#23779)
0	2	tests/tokenization/test_detokenize.py
13	9	tests/v1/core/test_kv_cache_utils.py
13	9	tests/v1/core/test_prefix_caching.py
14	12	tests/v1/core/test_scheduler.py
15	15	tests/v1/core/utils.py
1	3	tests/v1/engine/test_engine_core.py
1	3	tests/v1/engine/test_engine_core_client.py
8	10	tests/v1/engine/test_fast_incdec_prefix_err.py
10	20	tests/v1/engine/test_output_processor.py
1	3	tests/v1/kv_connector/unit/utils.py
13	3	vllm/multimodal/cache.py
23	0	vllm/multimodal/inputs.py
2	5	vllm/v1/engine/__init__.py
7	9	vllm/v1/engine/core.py
12	19	vllm/v1/engine/processor.py
10	24	vllm/v1/request.py

[d9e00dbd1] wang.yuqi 2025-08-29 [Performance] V1 Classify Models E2E Performance Optimization (#23541)
6	0	tests/entrypoints/llm/test_classify.py
30	0	tests/entrypoints/openai/test_classification.py
1	5	vllm/entrypoints/openai/api_server.py
32	28	vllm/model_executor/layers/pooler.py
11	4	vllm/v1/worker/gpu_model_runner.py

[ad39106b1] Li, Jiang 2025-08-29 [CPU] Enable data parallel for CPU backend (#23903)
20	4	.buildkite/scripts/hardware_ci/run-cpu-test.sh
2	1	docs/getting_started/installation/cpu.md
1	1	docs/getting_started/installation/cpu/x86.inc.md
8	0	vllm/platforms/cpu.py
6	1	vllm/v1/worker/cpu_model_runner.py
11	2	vllm/v1/worker/cpu_worker.py

[2554b27ba] Maximilien de Bayser 2025-08-29 [V0 Deprecation] Remove pooling model support in V0  (#23434)
6	2	tests/distributed/test_pipeline_parallel.py
0	8	tests/entrypoints/llm/test_classify.py
0	8	tests/entrypoints/llm/test_encode.py
0	8	tests/entrypoints/llm/test_reward.py
0	8	tests/entrypoints/llm/test_score.py
10	9	tests/entrypoints/offline_mode/test_offline_mode.py
0	8	tests/entrypoints/openai/test_embedding.py
0	8	tests/entrypoints/openai/test_rerank.py
0	9	tests/entrypoints/openai/test_score.py
3	17	tests/models/language/pooling/test_embedding.py
0	8	tests/models/language/pooling/test_reward.py
0	9	tests/models/language/pooling/test_scoring.py
11	12	tests/models/registry.py
0	54	tests/worker/test_model_input.py
0	1	vllm/core/scheduler.py
1	6	vllm/engine/arg_utils.py
14	97	vllm/engine/async_llm_engine.py
15	67	vllm/engine/llm_engine.py
1	0	vllm/engine/multiprocessing/__init__.py
13	38	vllm/engine/multiprocessing/client.py
3	2	vllm/engine/multiprocessing/engine.py
3	2	vllm/engine/protocol.py
1	2	vllm/entrypoints/llm.py
1	3	vllm/entrypoints/openai/serving_score.py
0	6	vllm/inputs/data.py
2	10	vllm/inputs/preprocess.py
7	25	vllm/model_executor/layers/pooler.py
1	1	vllm/model_executor/models/bert.py
1	1	vllm/model_executor/models/gritlm.py
1	1	vllm/model_executor/models/modernbert.py
0	90	vllm/model_executor/pooling_metadata.py
0	6	vllm/multimodal/inputs.py
0	11	vllm/sequence.py
2	3	vllm/worker/enc_dec_model_runner.py
0	29	vllm/worker/model_runner.py
2	12	vllm/worker/model_runner_base.py
0	222	vllm/worker/pooling_model_runner.py
1	5	vllm/worker/worker.py

[934bebf19] Harry Mellor 2025-08-29 Better errors for Transformers backend missing features (#23759)
23	2	vllm/model_executor/models/transformers.py

[885ca6d31] Jiangyun Zhu 2025-08-29 [Misc] Fix warnings for mistral model (#23552)
7	5	vllm/model_executor/models/pixtral.py
7	5	vllm/model_executor/models/voxtral.py
17	13	vllm/transformers_utils/tokenizers/mistral.py

[2d0afcc9d] Chenheli Hua 2025-08-28 [mrope][Qwen2-VL] Fix edge case where getting index of image/video token can potentially throw in default vl mrope implementation.  (#23895)
10	4	vllm/model_executor/layers/rotary_embedding/mrope.py

[b4f9e9631] Jee Jee Li 2025-08-29 [CI/Build] Clean up LoRA test (#23890)
0	1	.buildkite/scripts/hardware_ci/run-amd-test.sh
4	5	.buildkite/test-pipeline.yaml
0	80	tests/entrypoints/llm/test_generate_multiple_loras.py
36	1	tests/lora/{test_multi_loras_with_tp.py => test_llm_with_multi_loras.py}

[05d839c19] Raghavan 2025-08-29 Fix(async): Add support for truncate_prompt_tokens in AsyncLLM (#23800)
20	0	vllm/v1/engine/async_llm.py

[6597d7a45] wangxiyuan 2025-08-29 [Platform] import activation_quant_fusion for CUDA only (#23882)
1	1	vllm/compilation/pass_manager.py

[5264015d7] Jinghui Zhang 2025-08-28 [BugFix][AMD][Deepseek] fix a dtype mismatch error for deepseek running on AMD (#23864)
4	4	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[98ac0cb32] Isotr0py 2025-08-29 [Bugfix] Use `ReplicatedLinear` for SequenceClassification head (#23836)
2	5	tests/models/language/pooling/test_qwen3_reranker.py
2	3	vllm/model_executor/models/adapters.py

[c8b3b299c] Russell Bryant 2025-08-29 [tests] Improve speed and reliability of test_transcription_api_correctness (#23854)
7	4	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py

[006477e60] Charlie Fu 2025-08-28 [ROCm][Fix] Fix rocm build caused by #23791 (#23847)
0	1	csrc/cache_kernels.cu

[de533ab2a] Lukas Geiger 2025-08-29 [Models] Improve iteration over layers (#19497)
2	1	vllm/model_executor/models/arcee.py
2	1	vllm/model_executor/models/arctic.py
2	1	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bailing_moe.py
1	2	vllm/model_executor/models/bamba.py
2	1	vllm/model_executor/models/bloom.py
2	1	vllm/model_executor/models/chameleon.py
2	1	vllm/model_executor/models/chatglm.py
2	1	vllm/model_executor/models/commandr.py
2	1	vllm/model_executor/models/dbrx.py
3	2	vllm/model_executor/models/deepseek.py
2	1	vllm/model_executor/models/deepseek_v2.py
2	1	vllm/model_executor/models/dots1.py
2	2	vllm/model_executor/models/ernie45_moe.py
2	2	vllm/model_executor/models/ernie45_vl_moe.py
2	1	vllm/model_executor/models/exaone.py
2	1	vllm/model_executor/models/exaone4.py
2	1	vllm/model_executor/models/falcon.py
2	1	vllm/model_executor/models/gemma.py
2	1	vllm/model_executor/models/gemma2.py
2	1	vllm/model_executor/models/gemma3.py
2	2	vllm/model_executor/models/glm4_moe.py
2	1	vllm/model_executor/models/gpt2.py
2	1	vllm/model_executor/models/gpt_bigcode.py
3	2	vllm/model_executor/models/gpt_j.py
2	1	vllm/model_executor/models/gpt_neox.py
2	1	vllm/model_executor/models/granite.py
2	1	vllm/model_executor/models/granitemoe.py
1	2	vllm/model_executor/models/granitemoehybrid.py
2	2	vllm/model_executor/models/granitemoeshared.py
2	2	vllm/model_executor/models/grok1.py
2	1	vllm/model_executor/models/internlm2.py
2	1	vllm/model_executor/models/internlm2_ve.py
2	1	vllm/model_executor/models/jais.py
2	1	vllm/model_executor/models/jamba.py
3	2	vllm/model_executor/models/lfm2.py
2	1	vllm/model_executor/models/llama.py
1	3	vllm/model_executor/models/mamba2.py
2	1	vllm/model_executor/models/mimo.py
2	1	vllm/model_executor/models/minicpm.py
2	2	vllm/model_executor/models/minimax_text_01.py
2	1	vllm/model_executor/models/mixtral.py
2	1	vllm/model_executor/models/mixtral_quant.py
2	1	vllm/model_executor/models/molmo.py
2	1	vllm/model_executor/models/mpt.py
2	1	vllm/model_executor/models/nemotron.py
1	2	vllm/model_executor/models/nemotron_h.py
2	2	vllm/model_executor/models/nemotron_nas.py
2	1	vllm/model_executor/models/olmo.py
2	1	vllm/model_executor/models/olmo2.py
2	1	vllm/model_executor/models/olmoe.py
2	1	vllm/model_executor/models/opt.py
2	1	vllm/model_executor/models/orion.py
2	1	vllm/model_executor/models/persimmon.py
2	1	vllm/model_executor/models/phi.py
2	1	vllm/model_executor/models/phimoe.py
2	1	vllm/model_executor/models/plamo2.py
2	1	vllm/model_executor/models/qwen.py
2	1	vllm/model_executor/models/qwen2.py
2	1	vllm/model_executor/models/qwen2_moe.py
2	2	vllm/model_executor/models/qwen3_moe.py
2	1	vllm/model_executor/models/seed_oss.py
2	1	vllm/model_executor/models/stablelm.py
2	1	vllm/model_executor/models/starcoder2.py
2	2	vllm/model_executor/models/step3_text.py

[235c9db8a] Chaojun Zhang 2025-08-29 [XPU] support data parallel for MoE models on XPU (#22887)
11	0	vllm/distributed/device_communicators/xpu_communicator.py
2	0	vllm/model_executor/layers/fused_moe/layer.py

[b668055a1] Woosuk Kwon 2025-08-28 [V0 Deprecation] Remove V0 Samplers test (#23862)
0	769	tests/samplers/test_sampler.py
0	86	tests/samplers/test_seeded_generate.py

[d3d2aad5a] Wentao Ye 2025-08-28 [Log] Use Debug Once for DeepGEMM E8M0 When not Enabled (#23858)
1	1	vllm/utils/deep_gemm.py

[cb293f6a7] Yong Hoon Shin 2025-08-28 [V1] Enable prefill optimization for Gemma3n (#22628)
0	57	tests/v1/e2e/test_kv_sharing_fast_prefill.py
7	5	vllm/config/cache.py
354	65	vllm/model_executor/models/gemma3n.py
1	1	vllm/model_executor/models/gemma3n_mm.py
126	13	vllm/v1/attention/backends/utils.py
7	0	vllm/v1/engine/async_llm.py
53	43	vllm/v1/worker/gpu_model_runner.py
28	11	vllm/v1/worker/tpu_model_runner.py
7	33	vllm/v1/worker/utils.py

[7ffbf2723] Woosuk Kwon 2025-08-28 [BugFix][FlashInfer] Fix potential race condition for paged_kv_indptr_cpu (#23737)
10	2	vllm/v1/attention/backends/flashinfer.py

[27e88cee7] Simon Mo 2025-08-28 chore: build release image by default (#23852)
2	6	.buildkite/release-pipeline.yaml

[16a45b3a2] elvischenv 2025-08-29 [NVIDIA] Support SiluMul + NVFP4 quant fusion (#23671)
2	0	.buildkite/test-pipeline.yaml
2	0	CMakeLists.txt
16	0	csrc/dispatch_utils.h
8	0	csrc/ops.h
368	0	csrc/quantization/fp4/activation_nvfp4_quant_fusion_kernels.cu
7	0	csrc/torch_bindings.cpp
70	27	tests/compile/test_silu_mul_quant_fusion.py
126	0	tests/kernels/quantization/test_silu_nvfp4_quant_fusion.py
135	35	vllm/compilation/activation_quant_fusion.py
7	0	vllm/compilation/fix_functionalization.py
5	2	vllm/model_executor/layers/quantization/modelopt.py

[57d4ede52] Jingkai He 2025-08-29 [bugfix] [spec-decoding] fix data race in sample_recovered_tokens_kernel (vLLM v1) (#23829)
2	15	vllm/v1/sample/rejection_sampler.py

[04d1dd7f4] Divakar Verma 2025-08-28 [ROCm][Aiter] Add triton fp8 bmm kernel for mla (#23264)
8	0	vllm/envs.py
96	12	vllm/v1/attention/backends/mla/common.py

[f32a5bc50] Benji Beck 2025-08-28 Migrate Llama4ImagePatchInputs to TensorSchema (#22021)
23	18	vllm/model_executor/models/mllama4.py

[8805ad9fa] Jean Schmidt 2025-08-28 Add scale_config.yml file for Meta autoscalers for GH Actions (#23840)
21	0	.github/scale-config.yml

[0583578f4] Jean Schmidt 2025-08-28 [ci] breaks down V1 Test into 3 groups of approx 30 minutes runtime (#23757)
20	6	.buildkite/test-pipeline.yaml

[db74d6049] Angela Yi 2025-08-28 [Bugfix] Add fake mode around passes (#23349)
2	0	vllm/compilation/activation_quant_fusion.py
6	0	vllm/compilation/collective_fusion.py
2	0	vllm/compilation/fusion.py
34	41	vllm/compilation/fusion_attn.py
20	0	vllm/compilation/inductor_pass.py
2	0	vllm/compilation/sequence_parallelism.py

[95089607f] Po-Han Huang (NVIDIA) 2025-08-28 [Model][gpt-oss] Support DP+EP for GPT-OSS with FlashInfer trtllm-gen MoE (#23819)
8	7	vllm/model_executor/layers/fused_moe/config.py
4	4	vllm/model_executor/layers/fused_moe/layer.py
2	4	vllm/model_executor/layers/quantization/mxfp4.py

[1f096f9b9] Thomas Parnell 2025-08-28 [CI] Fix linting error on main (#23835)
2	1	vllm/v1/cudagraph_dispatcher.py

[66548f660] YUQI.CHENG 2025-08-28 [Bugfix] Fix benchmark_moe.py for blockwise fp8. (#23823)
4	1	benchmarks/kernels/benchmark_moe.py

[d3da2eea5] Didier Durand 2025-08-28 [Doc]: fix typos in Python scripts (#23828)
2	2	vllm/compilation/backends.py
1	1	vllm/config/cache.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/chat_utils.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	2	vllm/v1/cudagraph_dispatcher.py
1	1	vllm/v1/worker/block_table.py
1	1	vllm/v1/worker/cpu_model_runner.py

[bfab21964] Jiangyun Zhu 2025-08-28 [Model] [gpt-oss] fix gpt-oss pp support (#23815)
2	3	vllm/model_executor/models/gpt_oss.py

[a3432f18f] Woosuk Kwon 2025-08-28 [BugFix][Spec Decode] Use float64 for uniform_probs (#23803)
1	1	examples/offline_inference/spec_decode.py
6	1	vllm/v1/sample/rejection_sampler.py

[67cee40da] Li, Jiang 2025-08-28 [CI/Build][Bugfix] Fix Qwen VL tests on CPU (#23818)
10	10	.buildkite/scripts/hardware_ci/run-cpu-test.sh
4	4	vllm/model_executor/models/utils.py

[d99c3a4f7] Didier Durand 2025-08-28 [Doc]: fix typos in .md files (including those of #23751) (#23825)
1	1	docs/contributing/ci/update_pytorch_version.md
1	1	docs/contributing/model/multimodal.md
1	1	docs/deployment/frameworks/lobe-chat.md
1	1	docs/deployment/k8s.md
1	1	docs/design/fused_moe_modular_kernel.md
2	2	docs/design/metrics.md
1	1	docs/features/lora.md
1	1	docs/features/reasoning_outputs.md
1	1	docs/features/structured_outputs.md
2	2	docs/getting_started/installation/aws_neuron.md
1	1	docs/getting_started/installation/cpu/apple.inc.md
1	1	docs/getting_started/installation/gpu/cuda.inc.md
2	2	docs/getting_started/installation/gpu/rocm.inc.md
1	1	docs/models/pooling_models.md
1	1	docs/models/supported_models.md
1	1	docs/usage/usage_stats.md

[3462c1c52] JartX 2025-08-28 [FIXBUG] Add return_success parameter to moe_wna16_weight_loader function (#22797)
19	13	vllm/model_executor/layers/quantization/moe_wna16.py

[c5d004aaa] Isotr0py 2025-08-28 [Model] Add PP support and VLM backbone compatability for GPT-OSS (#23680)
1	1	docs/models/supported_models.md
86	33	vllm/model_executor/models/gpt_oss.py

[11a7fafaa] wang.yuqi 2025-08-28 [New Model]: Support GteNewModelForSequenceClassification (#23524)
4	0	docs/models/supported_models.md
2	3	tests/conftest.py
3	0	tests/models/language/pooling/embed_utils.py
6	0	tests/models/language/pooling/mteb_utils.py
9	15	tests/models/language/pooling/test_bge_reranker_v2_gemma.py
11	13	tests/models/language/pooling/test_gte.py
9	10	tests/models/language/pooling/test_mxbai_rerank.py
9	17	tests/models/language/pooling/test_qwen3_reranker.py
4	0	tests/models/registry.py
18	10	tests/models/utils.py
77	6	vllm/model_executor/models/bert_with_rope.py
1	0	vllm/model_executor/models/config.py
4	2	vllm/model_executor/models/registry.py

[186aced5f] yzds 2025-08-28 [Kernel] cuda kernels for upcoming decode context parallel feature (#23791)
16	1	csrc/cache.h
247	0	csrc/cache_kernels.cu
15	0	csrc/torch_bindings.cpp
72	0	tests/kernels/attention/test_cache.py
24	0	vllm/_custom_ops.py

[daa1273b1] rongfu.leng 2025-08-28 [Bugfix] when set offline model running error (#23711)
3	1	vllm/entrypoints/utils.py

[c07a73317] Jiangyun Zhu 2025-08-28 [CI] enable idefics3 and fuyu-8b test in multimodal test (#23790)
15	21	tests/models/multimodal/generation/test_common.py

[22feac8e9] Kyle Sayers 2025-08-28 [Transform] [Quantization] Add transforms to compressed tensors (#22486)
37	6	tests/conftest.py
22	0	tests/quantization/test_compressed_tensors.py
15	1	vllm/model_executor/layers/linear.py
37	15	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
227	0	vllm/model_executor/layers/quantization/compressed_tensors/transform/linear.py
135	0	vllm/model_executor/layers/quantization/compressed_tensors/transform/module.py
21	0	vllm/model_executor/layers/quantization/compressed_tensors/transform/schemes/linear_qutlass_nvfp4.py
13	0	vllm/model_executor/layers/quantization/compressed_tensors/transform/utils.py
154	14	vllm/model_executor/parameter.py

[c8851a472] Jinheng 2025-08-28 Add deprecation warning for lora_extra_vocab_size (#23635)
8	2	vllm/config/__init__.py

[f48a9af89] Alex 2025-08-27 [CI] make all multi-gpu weight loading tests run nightly (#23792)
1	0	.buildkite/test-pipeline.yaml

[a11adafdc] Jan Kessler 2025-08-28 Gracefully handle edge cases in harmony utils (#23155)
6	6	vllm/entrypoints/harmony_utils.py

[a781e84ec] Michael Goin 2025-08-27 [Perf] Tune configs for triton block fp8 gemm H100/H200 (#23748)
113	0	benchmarks/kernels/bench_block_fp8_gemm.py
146	0	vllm/model_executor/layers/quantization/utils/configs/N=12288,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=12288,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
33	33	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
35	35	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
22	22	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
24	24	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
32	32	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
34	34	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
41	41	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
43	43	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
31	31	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
33	33	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
30	30	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
37	37	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
38	38	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json
34	34	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json

[1b7b161a0] Shrey Gupta 2025-08-28 [Feature] models: pass layer prefix to replace_linear_class for per-layer quantization routing. Addresses #23239 (#23556)
8	4	vllm/model_executor/models/deepseek_vl2.py
10	4	vllm/model_executor/models/transformers.py

[a69693e38] Benji Beck 2025-08-27 Migrate Qwen inputs to TensorSchema (#23473)
25	26	vllm/model_executor/models/qwen_vl.py

[5da4f5d85] Hanchenli 2025-08-27 [Bugfix] Fix for V1 priority scheduling crashes at preemption (#23713)
88	3	tests/v1/core/test_scheduler.py
2	0	vllm/v1/core/sched/scheduler.py

[321938e9a] Wentao Ye 2025-08-27 [Feature] Add `VLLM_DISABLE_PAD_FOR_CUDAGRAPH` to Avoid Hang Issue (#23595)
7	0	vllm/envs.py
1	0	vllm/v1/worker/gpu_model_runner.py

[f9ca2b40a] Michael Goin 2025-08-27 [Bugfix] Fix Marlin NVFP4 for modelopt (#23659)
12	13	vllm/model_executor/layers/quantization/modelopt.py

[082cc07ef] Yongye Zhu 2025-08-27 DP/EP Support for gpt-oss with deepep-ht comm kernel on SM100 (#23608)
1	1	vllm/distributed/device_communicators/base_device_communicator.py
6	0	vllm/model_executor/layers/fused_moe/config.py
4	2	vllm/model_executor/layers/fused_moe/layer.py
197	0	vllm/model_executor/layers/fused_moe/trtllm_moe.py
16	0	vllm/model_executor/layers/fused_moe/utils.py
4	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	0	vllm/model_executor/layers/quantization/fp8.py
2	0	vllm/model_executor/layers/quantization/modelopt.py
110	0	vllm/model_executor/layers/quantization/mxfp4.py
4	5	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
20	0	vllm/model_executor/layers/quantization/utils/mxfp8_utils.py

[853c371fc] Asaf Joseph Gardin 2025-08-27 [V1][Mamba] - Enable V1 by default for Mamba Models (#23650)
71	80	tests/models/language/generation/test_hybrid.py
0	5	vllm/engine/arg_utils.py
1	0	vllm/model_executor/models/config.py

[8bf6266a1] Roger Wang 2025-08-27 [Multimodal] Generate mm_hash based on request metadata when caching is turned off (#23690)
63	8	vllm/inputs/preprocess.py
3	0	vllm/model_executor/models/deepseek_vl2.py
3	0	vllm/model_executor/models/h2ovl.py
6	2	vllm/model_executor/models/llava.py
6	2	vllm/model_executor/models/mllama.py
6	2	vllm/model_executor/models/paligemma.py
2	0	vllm/model_executor/models/pixtral.py
5	2	vllm/model_executor/models/prithvi_geospatial_mae.py
5	2	vllm/model_executor/models/transformers.py
2	0	vllm/model_executor/models/voxtral.py
30	6	vllm/multimodal/processing.py
48	0	vllm/v1/engine/processor.py

[0585a9e73] Harry Mellor 2025-08-27 Disable `torch.compile` for dynamic rope models in Transformers backend (#23738)
22	3	vllm/model_executor/models/transformers.py

[3c0ef769b] Eli Uriegas 2025-08-27 ci: Add arm64 docker build to release pipeline (#23210)
32	6	.buildkite/release-pipeline.yaml

[4e4d017b6] Hyogeun Oh (오효근) 2025-08-28 [Docs] Fix warnings in `mkdocs build` (continued) (#23743)
1	1	vllm/core/block/naive_block.py
1	1	vllm/core/block/prefix_caching_block.py
1	1	vllm/core/scheduler.py
2	1	vllm/v1/attention/backends/cpu_attn.py
2	1	vllm/v1/attention/backends/flash_attn.py
3	5	vllm/v1/attention/backends/flashinfer.py
2	1	vllm/v1/attention/backends/flex_attention.py
3	2	vllm/v1/attention/backends/pallas.py
2	1	vllm/v1/attention/backends/rocm_aiter_fa.py
2	1	vllm/v1/attention/backends/tree_attn.py
2	1	vllm/v1/attention/backends/triton_attn.py
2	1	vllm/v1/attention/backends/xformers.py
4	4	vllm/v1/core/encoder_cache_manager.py
2	1	vllm/v1/core/kv_cache_coordinator.py
6	5	vllm/v1/core/kv_cache_manager.py
2	1	vllm/v1/executor/ray_distributed_executor.py
1	1	vllm/v1/metrics/prometheus.py
2	2	vllm/v1/sample/logits_processor/interface.py
1	1	vllm/v1/sample/rejection_sampler.py
1	1	vllm/v1/sample/tpu/sampler.py
2	2	vllm/v1/structured_output/backend_types.py
0	3	vllm/v1/worker/gpu_input_batch.py
1	1	vllm/v1/worker/gpu_model_runner.py
5	5	vllm/v1/worker/tpu_model_runner.py
4	4	vllm/v1/worker/utils.py
2	2	vllm/v1/worker/worker_base.py

[dd5893228] Thomas Parnell 2025-08-27 [V1] [Hybrid] Enable compile and piecewise CUDA graph for MiniMax-Text models (#22589)
1	0	vllm/config/compilation.py
97	137	vllm/model_executor/models/minimax_text_01.py

[52883ed08] Cyrus Leung 2025-08-28 [Model] Merge `SupportsMultiModalWithRawInput` with `SupportsMultiModal` (#23749)
4	4	vllm/config/__init__.py
11	34	vllm/model_executor/models/interfaces.py
3	3	vllm/model_executor/models/prithvi_geospatial_mae.py
6	5	vllm/model_executor/models/registry.py
6	4	vllm/v1/worker/gpu_model_runner.py

[4f35be10a] Luka Govedič 2025-08-27 [BugFix] Fix topk_softmax assert (#19764)
1	1	csrc/moe/topk_softmax_kernels.cu

[2b61d2e22] Harry Mellor 2025-08-27 [Docs] Remove in-tree Gaudi install instructions (#23628)
0	1	docs/getting_started/installation/README.md
0	388	docs/getting_started/installation/intel_gaudi.md

[3ce8285d6] Nick Hill 2025-08-27 [LogitsProcs] Deduplicate built-in LP implementation logic (#23362)
11	27	examples/offline_inference/logits_processor.py
9	28	tests/v1/logits_processors/utils.py
62	86	vllm/v1/sample/logits_processor/builtin.py
13	2	vllm/v1/sample/logits_processor/interface.py

[83f555f63] Didier Durand 2025-08-27 [Doc]: upgrade version of crate-ci tool for improved typo detection (#23755)
1	1	.pre-commit-config.yaml

[841490434] Isotr0py 2025-08-27 [Model] Enable native HF format InternVL support (#23742)
1	0	docs/models/supported_models.md
14	15	tests/models/multimodal/generation/test_common.py
2	1	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py

[3af47c3cc] Wentao Ye 2025-08-27 [Feature] Add Hopper DeepGEMM E8M0 for DeepSeekV3.1 scale_fmt (#23666)
2	3	tests/kernels/moe/test_block_fp8.py
3	4	tests/kernels/moe/test_deepep_deepgemm_moe.py
7	1	vllm/envs.py
2	2	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
3	4	vllm/model_executor/layers/fused_moe/fused_moe.py
3	3	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
4	5	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py
18	0	vllm/transformers_utils/config.py
24	29	vllm/utils/deep_gemm.py

[513c1fe25] Harry Mellor 2025-08-27 Only run `get_attr_docs` if generating help text (#23723)
13	2	vllm/engine/arg_utils.py

[fe8d7b6f0] Cyrus Leung 2025-08-27 [Model] Interface to enable batch-level DP support (#23733)
5	2	docs/configuration/optimization.md
7	0	vllm/config/__init__.py
11	0	vllm/model_executor/models/interfaces.py
2	0	vllm/model_executor/models/minicpmv.py
2	0	vllm/model_executor/models/mllama4.py
2	0	vllm/model_executor/models/qwen2_5_vl.py
7	2	vllm/model_executor/models/registry.py
2	0	vllm/model_executor/models/step3_vl.py

[16dc4052b] Harry Mellor 2025-08-27 Fix pre-commit on main (#23747)
1	1	docs/community/meetups.md

[8dd2baa59] rebel-hongseok 2025-08-27 Add vLLM Korea Meetup in the README.md and meetups.md (#23746)
1	0	README.md
1	0	docs/community/meetups.md

[5eeef1b90] Cyrus Leung 2025-08-27 [Model] Explicit `default_pooling_type` interface (#23736)
2	2	vllm/model_executor/models/bert.py
3	2	vllm/model_executor/models/bert_with_rope.py
1	1	vllm/model_executor/models/gritlm.py
1	18	vllm/model_executor/models/interfaces.py
28	0	vllm/model_executor/models/interfaces_base.py
2	1	vllm/model_executor/models/internlm2.py
2	1	vllm/model_executor/models/modernbert.py
4	3	vllm/model_executor/models/prithvi_geospatial_mae.py
2	1	vllm/model_executor/models/qwen2_rm.py
4	3	vllm/model_executor/models/registry.py
2	1	vllm/model_executor/models/roberta.py

[704432af3] Thomas Parnell 2025-08-27 [V1] [Hybrid] Disable prefix caching by default for hybrid or mamba-based models  (#23716)
6	4	docs/usage/v1_guide.md
5	4	vllm/model_executor/models/config.py

[a403d0fa4] Nick Hill 2025-08-27 [Misc] Remove unnecessary `_send_reconfig_message()` in `core_client.py` (#23127)
9	22	vllm/v1/engine/core_client.py

[8c13820f0] cndoit18 2025-08-27 [Bugfix] Fix task field initialization when PYTHONOPTIMIZE is enabled (#23718)
3	2	vllm/worker/pooling_model_runner.py

[9d30de446] tc-mb 2025-08-27 [model] Support MiniCPM-V 4.5 (#23586)
1	1	docs/models/supported_models.md
1	1	tests/models/registry.py
301	13	vllm/model_executor/models/minicpmv.py
11	0	vllm/transformers_utils/chat_templates/registry.py
93	0	vllm/transformers_utils/chat_templates/template_minicpmv45.jinja

[1f7a9c95e] Michael Yao 2025-08-27 [Docs] Fix a 1-2-3 list and style issues in tpu.md (#23729)
8	8	docs/configuration/tpu.md

[8f0d7eaea] Fanli Lin 2025-08-27 [XPU] Fix OOM issue for data parallel with Ray backend (#22500)
18	9	vllm/v1/engine/core.py
31	4	vllm/v1/engine/utils.py

[e03940762] Jee Jee Li 2025-08-27 [CI/Build] Reduce LoRA layer test cases (#23721)
33	39	tests/lora/test_layers.py

[11eddf02f] Woosuk Kwon 2025-08-27 [FlashInfer] Cache hyper params in metadata builder (#23732)
15	15	vllm/v1/attention/backends/flashinfer.py

[04ff1e43f] Woosuk Kwon 2025-08-27 [Misc] Move CpuGpuBuffer to vllm/v1/utils.py (#23728)
29	0	vllm/v1/utils.py
1	1	vllm/v1/worker/cpu_model_runner.py
3	3	vllm/v1/worker/gpu_model_runner.py
0	29	vllm/v1/worker/utils.py

[6578e8736] Woosuk Kwon 2025-08-27 Optimize input preparation for FlashInfer [2/N] (#23174)
54	26	vllm/v1/attention/backends/flashinfer.py

[5bd9f8415] Michael Yao 2025-08-27 [Docs] Fix an admonition important (#23726)
1	1	docs/configuration/optimization.md

[91e382c93] Cyrus Leung 2025-08-27 [CI/Build] Remove redundant register in model init tests (#23715)
0	5	tests/models/test_initialization.py

[644667783] Kunshang Ji 2025-08-27 [XPU]fix cuda event used in XPU model runner (#23708)
21	1	vllm/v1/worker/xpu_model_runner.py

[69244e67e] Cyrus Leung 2025-08-27 [Core] Use key-only cache for `BaseMultiModalProcessor` (#23018)
1	1	docs/configuration/conserving_memory.md
35	9	docs/configuration/optimization.md
5	3	tests/models/multimodal/processing/test_common.py
174	8	tests/multimodal/test_cache.py
2	24	vllm/config/__init__.py
1	13	vllm/engine/arg_utils.py
10	5	vllm/engine/llm_engine.py
18	4	vllm/inputs/preprocess.py
9	3	vllm/inputs/registry.py
4	3	vllm/model_executor/models/hyperclovax_vision.py
4	4	vllm/model_executor/models/llava.py
39	1	vllm/model_executor/models/minicpmv.py
4	4	vllm/model_executor/models/mistral3.py
19	1	vllm/model_executor/models/phi3v.py
20	1	vllm/model_executor/models/phi4mm.py
4	3	vllm/model_executor/models/tarsier.py
390	15	vllm/multimodal/cache.py
28	10	vllm/multimodal/inputs.py
110	77	vllm/multimodal/processing.py
2	2	vllm/multimodal/profiling.py
36	54	vllm/multimodal/registry.py
1	2	vllm/v1/engine/async_llm.py
10	7	vllm/v1/engine/core.py
1	2	vllm/v1/engine/llm_engine.py
0	121	vllm/v1/engine/mm_input_cache.py
14	15	vllm/v1/engine/processor.py
3	0	vllm/v1/worker/gpu_model_runner.py
3	0	vllm/v1/worker/tpu_model_runner.py
7	2	vllm/v1/worker/utils.py

[8dbf6ed7b] rongfu.leng 2025-08-27 [Bugfix] fix when config.yaml config value is list parse error (#23528)
41	0	tests/utils_/test_utils.py
7	2	vllm/utils/__init__.py

[9de25c294] Jee Jee Li 2025-08-27 [CI/Build] Remove redundant LoRA model tests (#23706)
0	5	tests/lora/conftest.py
0	112	tests/lora/test_baichuan.py
0	71	tests/lora/test_phi.py

[fce10dbed] Kunshang Ji 2025-08-27 [XPU] Add xpu torch.compile support (#22609)
1	0	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	2	vllm/attention/layer.py
8	0	vllm/compilation/fix_functionalization.py
4	0	vllm/platforms/cpu.py
4	0	vllm/platforms/cuda.py
8	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py
6	9	vllm/platforms/xpu.py

[d272415e5] Dipika Sikka 2025-08-27 [Quantization] Expand compressed-tensors MoE matching logic to support NFP4 + FP8 MoEs (#22674)
7	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
33	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[142ac0803] Chen Zhang 2025-08-26 [Frontend] Optimize beam search performance by limiting concurrency (#23599)
0	1	benchmarks/benchmark_throughput.py
5	3	tests/conftest.py
53	0	tests/samplers/test_beam_search.py
85	67	vllm/entrypoints/llm.py

[321026442] Chen Zhang 2025-08-26 [Frontend] Add --log-error-stack to print stack trace for error response (#22960)
10	0	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/cli_args.py
3	1	vllm/entrypoints/openai/serving_chat.py
2	0	vllm/entrypoints/openai/serving_classification.py
2	0	vllm/entrypoints/openai/serving_completion.py
3	1	vllm/entrypoints/openai/serving_embedding.py
9	0	vllm/entrypoints/openai/serving_engine.py
3	1	vllm/entrypoints/openai/serving_pooling.py
2	0	vllm/entrypoints/openai/serving_responses.py
3	1	vllm/entrypoints/openai/serving_score.py
3	1	vllm/entrypoints/openai/serving_tokenization.py
6	2	vllm/entrypoints/openai/serving_transcription.py
3	1	vllm/entrypoints/openai/speech_to_text.py

[644d57d53] CSWYF3634076 2025-08-27 [Model] Add Ernie4.5 VL Model Support (#22514)
1	0	docs/models/supported_models.md
32	0	examples/offline_inference/vision_language.py
1	0	requirements/test.in
3	0	requirements/test.txt
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
72	0	vllm/model_executor/layers/rotary_embedding/ernie45_vl_rope.py
123	0	vllm/model_executor/layers/rotary_embedding/mrope.py
1504	0	vllm/model_executor/models/ernie45_vl.py
723	0	vllm/model_executor/models/ernie45_vl_moe.py
1	0	vllm/model_executor/models/registry.py

[c905684cf] Chenheli Hua 2025-08-26 [Core] Asynchronous h2d in merge_multimodal_embeddings via pinned memory. (#23686)
3	1	vllm/model_executor/models/utils.py

[786835807] Yiheng Xu 2025-08-27 [Bugfix]: Qwen3 Coder Tool Parser (#23099)
117	0	examples/tool_chat_template_qwen3coder.jinja
173	5	tests/tool_use/test_qwen3coder_tool_parser.py
286	243	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py

[fecbb7c78] Wei 2025-08-26 [Bugfix][gpt-oss] passing the cache config in gpt-oss (#23613)
6	1	vllm/model_executor/models/gpt_oss.py

[6dab89b8e] Harry Mellor 2025-08-27 [Docs] Fix math rendering in docs (#23676)
20	0	docs/mkdocs/javascript/mathjax.js
4	3	mkdocs.yaml
0	1	requirements/docs.txt

[de02b07db] Michael Goin 2025-08-26 [Bugfix] Lazy import gpt_oss_triton_kernels_moe for mxfp4 (#23678)
2	2	vllm/model_executor/layers/quantization/mxfp4.py

[eb1995167] Chen Zhang 2025-08-26 [gpt-oss] Enable unit test for response API harmony integration (#23533)
28	17	tests/entrypoints/openai/test_response_api_with_harmony.py

[2c2b140ae] czhu-cohere 2025-08-26 [quantization] use channel scales for w4a8 + misc fixes (#23570)
40	4	tests/quantization/test_compressed_tensors.py
11	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
11	8	vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py

[c7c80af08] yzds 2025-08-27 fix pynccl reduce_scatter (#23648)
3	3	vllm/distributed/device_communicators/cuda_communicator.py

[6891205b1] wuhang 2025-08-27 [Feature][Responses API] Support MCP tool in background mode (#23494)
26	5	vllm/entrypoints/context.py
138	131	vllm/entrypoints/openai/serving_responses.py

[b1625dbe9] zixuanzhang226 2025-08-26 feat: add triton fused moe config for GLM-4.5-Air-FP8 on B200 (#23695)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_B200,dtype=fp8_w8a8.json

[585e0bde3] Federico 2025-08-27 [Bugfix] UnboundLocalError when GptOss reasoning specified (#23054)
2	1	vllm/entrypoints/openai/serving_chat.py

[714872f1a] Wentao Ye 2025-08-26 [Compile] Fix Cmake Warning (#23689)
1	1	CMakeLists.txt

[5f1af97f8] Thomas Parnell 2025-08-27 [V1] [Hybrid] Enable Full CUDA graph by default for hybrid models in V1 (#22594)
42	0	vllm/model_executor/models/config.py

[c3b0fd1ee] Zhonghua Deng 2025-08-27 [V1][P/D]P2pNcclConnector supports flashinfer (#23536)
78	80	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py

[6421b66bf] Harry Mellor 2025-08-26 [Docs] Move quant supported hardware table to README (#23663)
47	1	docs/features/quantization/README.md
1	1	docs/features/quantization/bitblas.md
0	32	docs/features/quantization/supported_hardware.md

[2f13319f4] Huzaifa Sidhpurwala 2025-08-27 Enhance the pre-notification policy (#23532)
5	0	SECURITY.md

[d696f86e7] Chen Zhang 2025-08-26 [doc] Hybrid KV Cache Manager design doc (#22688)
-	-	docs/assets/design/hybrid_kv_cache_manager/basic_grouping_example.png
-	-	docs/assets/design/hybrid_kv_cache_manager/full_attn.png
-	-	docs/assets/design/hybrid_kv_cache_manager/memory_layout.png
-	-	docs/assets/design/hybrid_kv_cache_manager/overview.png
-	-	docs/assets/design/hybrid_kv_cache_manager/sw_attn.png
245	0	docs/design/hybrid_kv_cache_manager.md

[9816b81f5] Isotr0py 2025-08-27 [Model] Enable video support for InternVL3.5 models (#23658)
2	2	docs/models/supported_models.md
3	0	tests/models/multimodal/processing/test_common.py
6	1	tests/models/multimodal/processing/test_tensor_schema.py
4	1	tests/models/registry.py
7	3	vllm/model_executor/models/internvl.py

[c37c0af99] Jiangyun Zhu 2025-08-27 [Misc] Fix comments in `tests/kernels/quantization` (#23675)
1	1	tests/kernels/quantization/test_awq_triton.py
1	1	tests/kernels/quantization/test_cutlass_2of4_sparse.py
1	1	tests/kernels/quantization/test_cutlass_scaled_mm.py
1	1	tests/kernels/quantization/test_cutlass_w4a8.py
1	1	tests/kernels/quantization/test_machete_mm.py
1	1	tests/kernels/quantization/test_marlin_gemm.py
1	1	tests/kernels/quantization/test_triton_scaled_mm.py

[9715f7bb0] Cyrus Leung 2025-08-27 [Bugfix] Fix incorrect original shape in hashing (#23672)
4	3	tests/multimodal/test_hasher.py
8	2	vllm/multimodal/hasher.py

[98aa16ff4] Russell Bryant 2025-08-26 [v1] Add cross-attention KV cache support for encoder-decoder models (#23664)
19	0	vllm/multimodal/registry.py
25	9	vllm/v1/core/kv_cache_coordinator.py
4	2	vllm/v1/core/kv_cache_manager.py
36	1	vllm/v1/core/sched/scheduler.py
54	2	vllm/v1/core/single_type_kv_cache_manager.py
15	0	vllm/v1/kv_cache_interface.py

[227e231b5] Thomas Parnell 2025-08-26 [Docs] [V1] [Hybrid] Update docs to remove FlashInfer constraint for hybrid models (#23665)
2	3	docs/usage/v1_guide.md

[730d0ac8b] Hyogeun Oh (오효근) 2025-08-27 [Docs] Fix warnings in `mkdocs build` (#23649)
9	5	vllm/attention/backends/differential_flash_attn.py
3	2	vllm/attention/backends/flash_attn.py
6	5	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/attention/backends/utils.py
6	6	vllm/attention/backends/xformers.py
3	5	vllm/core/block_manager.py
2	2	vllm/engine/async_llm_engine.py
4	4	vllm/engine/llm_engine.py
5	5	vllm/entrypoints/llm.py
2	1	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py
10	1	vllm/model_executor/layers/lightning_attn.py
3	2	vllm/model_executor/layers/linear.py
2	2	vllm/outputs.py
10	17	vllm/sequence.py

[9b0187003] Li, Jiang 2025-08-27 [Bugfix] Fix cuda event usage with CPU model runner (#23643)
25	3	vllm/v1/worker/cpu_model_runner.py
1	1	vllm/v1/worker/gpu_model_runner.py

[44ac25eae] vllmellm 2025-08-27 [CI] [Doc]: Add GH Action for auto labeling issues with `rocm` tag (#20988)
305	0	.github/workflows/issue_autolabel.yml

[7ea22e42d] nvjullin 2025-08-26 [Misc] Add override for allreduce fusion thresholds (#23639)
13	0	vllm/compilation/collective_fusion.py
11	0	vllm/envs.py

[9d4183dd2] Yuekai Zhang 2025-08-26 [model] support qwen2audio embedding input (#23625)
7	6	vllm/model_executor/models/qwen2_5_omni_thinker.py
86	23	vllm/model_executor/models/qwen2_audio.py

[513298f1b] Yuekai Zhang 2025-08-26 [Bugfix] fix bf16 multimodal model hash (#23623)
13	1	vllm/multimodal/hasher.py

[379f828fb] Harry Mellor 2025-08-26 [Docs] Reduce requirements for docs build (#23651)
40	12	docs/mkdocs/hooks/generate_argparse.py
0	14	requirements/docs.txt
5	2	vllm/sequence.py
27	31	vllm/transformers_utils/config.py

[1fdc73241] Hongxia Yang 2025-08-26 [ROCm] Starting to add AMD code reviewers for ROCm components (#23496)
6	0	.github/CODEOWNERS

[f58675bfb] TianyuLi0 2025-08-26 [CPU] add cpu fused moe pytorch native implementation (#23146)
178	108	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
2	2	vllm/model_executor/layers/fused_moe/layer.py

[7c04779af] Didier Durand 2025-08-26 [Doc]: fix various spelling issues in multiple files (#23636)
1	1	.buildkite/nightly-benchmarks/README.md
1	1	benchmarks/README.md
2	2	docs/configuration/optimization.md
1	1	docs/configuration/tpu.md
3	3	docs/design/fused_moe_modular_kernel.md
2	2	vllm/distributed/kv_transfer/README.md

[f66673a39] nvjullin 2025-08-26 [Kernel] Added flashinfer fp8 per-tensor gemms (#22895)
1	0	.buildkite/test-pipeline.yaml
7	8	tests/compile/test_fusion.py
1	2	tests/compile/test_sequence_parallelism.py
6	7	tests/compile/test_silu_mul_quant_fusion.py
73	0	tests/kernels/quantization/test_flashinfer_scaled_mm.py
3	2	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/layers/quantization/ptpc_fp8.py
44	15	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
61	0	vllm/utils/flashinfer.py

[b78bed1bc] En Ouyang 2025-08-26 [Hardware][Mac] Fix the installation fail for Apple Silicon (CPU)  (#23565)
1	0	cmake/cpu_extension.cmake

[164b2273c] Harry Mellor 2025-08-26 [Docs] Fix broken links to `docs/api/summary.md` (#23637)
3	3	docs/examples/README.md
1	1	docs/models/generative_models.md
1	1	docs/models/pooling_models.md

[2b4fc9bd9] Chen Zhang 2025-08-26 Support FlashAttention Backend for Hybrid SSM Models (#23299)
0	3	tests/models/language/generation/test_hybrid.py
17	24	vllm/v1/worker/gpu_model_runner.py

[ebd5a77bb] Guillaume Calmettes 2025-08-26 feat: add usage to TranscriptionResponse (text and json response_format) (#23576)
10	4	tests/entrypoints/openai/test_transcription_validation.py
6	0	vllm/entrypoints/openai/protocol.py
16	1	vllm/entrypoints/openai/speech_to_text.py

[384dd1b0a] Matúš Námešný 2025-08-26 [Bugfix] Add missing enable_log_outputs parameter to init_app_state function (#23634)
2	0	vllm/entrypoints/openai/api_server.py

[fdeb3dac1] Jee Jee Li 2025-08-26 [Model] fix DeepSeek e_score_correction_bias dtype to fp32 (#23640)
1	1	vllm/model_executor/models/deepseek_v2.py

[d52358c1e] Michael Goin 2025-08-26 [Perf] Remove duplicated NVFP4 blockscales to save memory (#23379)
10	10	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
5	6	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
15	19	vllm/model_executor/layers/quantization/modelopt.py

[6ace2f72b] Huy Do 2025-08-26 Fix writing benchmark results with tuple keys (#23633)
6	1	vllm/benchmarks/lib/utils.py

[b00e69f8c] Harry Mellor 2025-08-26 Fix nits from #20059 (#23548)
6	8	vllm/config/compilation.py

[50fede663] Cyrus Leung 2025-08-26 [V1] Enable V1 for compute capability < 8.0 + FP32 (#23614)
8	8	vllm/engine/arg_utils.py

[b5d34af32] Roger Wang 2025-08-26 [Bugfix] Fix scheduling when repeated images in one request (#23544)
38	11	tests/v1/core/test_encoder_cache_manager.py
20	12	vllm/v1/core/encoder_cache_manager.py
38	16	vllm/v1/core/sched/scheduler.py

[9b5f64238] Jee Jee Li 2025-08-26 [Bugfix] Fix Qwen25VL packed_modules_mapping (#23604)
1	0	vllm/model_executor/models/qwen2_5_vl.py

[ff77764f8] Raghavan 2025-08-26 Fix CLI parameter documentation inconsistency in pooling_models.md (#23630)
2	2	docs/models/pooling_models.md

[bfc1edc9f] Harry Mellor 2025-08-26 [Docs] Fix titles for multi-file examples that are rendered in the docs (#23573)
16	4	docs/mkdocs/hooks/generate_examples.py

[3ecbb14b8] Jiangyun Zhu 2025-08-26 [Benchmarks] add benchmark for embedding models (#23000)
45	22	vllm/benchmarks/datasets.py
53	4	vllm/benchmarks/lib/endpoint_request_func.py
176	81	vllm/benchmarks/serve.py

[7d67a9d9f] Cyrus Leung 2025-08-26 [mypy] Fix incorrect type hint for EAGLE3 support (#23617)
3	3	vllm/model_executor/models/llama.py
3	3	vllm/model_executor/models/qwen2.py
2	2	vllm/model_executor/models/qwen3.py

[959783fb9] Bin Jia 2025-08-26 [fix] fix seed-oss-parser (#23560)
2	7	tests/tool_use/test_seed_oss_tool_parser.py
3	0	vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py

[ce0e9dbd4] Cyrus Leung 2025-08-26 [CI/Build] Fix typo in #23561 (#23616)
1	1	tests/entrypoints/openai/test_vision.py

[b395b3b0a] Zijing Liu 2025-08-25 [Disagg][Perf] Use CUDA event sync instead of blocking `tolist` to avoid unintentional copy ops blocking across different CUDA streams, improving disagg TTIT/TTFT (#22760)
23	2	vllm/v1/worker/gpu_model_runner.py

[6fad29b11] Copilot 2025-08-25 Remove graph_pool as member of VllmBackend and argument to CUDAGraphWrapper (#23385)
2	12	vllm/compilation/backends.py
1	4	vllm/compilation/base_static_graph.py
4	4	vllm/compilation/cuda_graph.py

[6fd45e7b8] Cyrus Leung 2025-08-26 [CI/Build] Use vLLM client's user agent to fetch images (#23561)
2	4	tests/entrypoints/openai/test_vision.py
1	2	tests/entrypoints/openai/test_vision_embedding.py

[56dcf4e7e] Wentao Ye 2025-08-25 [Bug] Fix DeepGEMM Env Control (#23591)
4	7	vllm/utils/deep_gemm.py

[ae067888d] weiliang 2025-08-26 Update Flashinfer to  0.2.14.post1 (#23537)
1	1	docker/Dockerfile
1	1	setup.py
2	1	vllm/compilation/collective_fusion.py
6	1	vllm/model_executor/layers/quantization/mxfp4.py
4	3	vllm/v1/worker/gpu_worker.py

[906e461ed] Michael Goin 2025-08-25 [CI Fix] Pin deepep and pplx tags in tools/ep_kernels/, gate multigpu tests (#23568)
1	0	.buildkite/test-pipeline.yaml
5	7	tests/distributed/test_comm_ops.py
3	0	tests/kernels/moe/test_deepep_deepgemm_moe.py
3	0	tests/kernels/moe/test_deepep_moe.py
2	0	tests/kernels/moe/test_modular_kernel_combinations.py
2	0	tests/kernels/moe/test_pplx_cutlass_moe.py
5	0	tests/kernels/moe/test_pplx_moe.py
6	3	tests/utils.py
13	2	tools/ep_kernels/install_python_libraries.sh

[2a97ffc33] Simon Mo 2025-08-25 [Misc] Add release note draft to PR template (#23598)
1	2	.github/PULL_REQUEST_TEMPLATE.md

[efc88cf64] Woosuk Kwon 2025-08-25 [Misc] Simplify FlashInfer attention metadata (#23585)
114	163	vllm/v1/attention/backends/flashinfer.py

[7b6a83727] Terrence Zhao 2025-08-25 [Docs] Update Documentation of Cohere Command-A Models (#23584)
1	1	docs/models/supported_models.md

[c34c82b7f] Pate Motter 2025-08-25 [TPU][Bugfix] Fixes prompt_token_ids error in tpu tests. (#23574)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[8a044754b] Chaojun Zhang 2025-08-26 [XPU] Delay BF16 check to worker init for spawn compatibility (#22979)
20	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py
20	0	vllm/platforms/rocm.py
11	26	vllm/platforms/xpu.py
1	21	vllm/v1/worker/gpu_worker.py
1	0	vllm/v1/worker/xpu_worker.py

[9188ae7cb] Zhonghua Deng 2025-08-26 [Bugfix][V1][P/D]Fix the issue where repeated requests for the same input produce abnormal outputs for P2pNcclConnector (#23403)
21	4	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
2	28	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
3	2	vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool.py

[8a3cd90af] Xin Yang 2025-08-25 [Kernel] Add fused grouped_topk kernel for MoE (#23274)
3	1	CMakeLists.txt
757	0	csrc/moe/grouped_topk_kernels.cu
5	0	csrc/moe/moe_ops.h
6	0	csrc/moe/torch_bindings.cpp
76	0	tests/kernels/moe/test_grouped_topk.py
11	0	vllm/_custom_ops.py
6	0	vllm/envs.py
45	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[2a167b2ee] 22quinn 2025-08-25 [test][RL] Add sleep level 2 test and fix reload with sleep mode (#23521)
31	0	tests/basic_correctness/test_cumem.py
1	2	vllm/v1/worker/gpu_worker.py

[0ff902f3b] Woosuk Kwon 2025-08-25 [Refactor] Refactor persistent buffers with CpuGpuBuffer  (#23515)
70	103	vllm/v1/worker/gpu_model_runner.py
29	0	vllm/v1/worker/utils.py

[a9082a4d1] Isotr0py 2025-08-25 [Bugfix] Fix Qwen3 MoE GPTQ inference (#23490)
18	6	vllm/model_executor/models/qwen3_moe.py

[e0329ed4b] Driss Guessous 2025-08-25 Updates to Flex + VLLm integration (#21416)
87	23	tests/kernels/test_flex_attention.py
18	12	tests/v1/attention/test_attention_backends.py
334	68	vllm/v1/attention/backends/flex_attention.py

[6879cd80a] Cyrus Leung 2025-08-25 [Refactor] Pass `tokenizer` explicitly instead of binding to prompt update (#23542)
8	13	tests/multimodal/test_processing.py
2	2	vllm/model_executor/models/gemma3_mm.py
2	2	vllm/model_executor/models/gemma3n_mm.py
83	127	vllm/multimodal/processing.py

[e269be2ba] Cyrus Leung 2025-08-25 [Doc] Add caution for API server scale-out (#23550)
7	0	docs/configuration/optimization.md

[5c4b6e66f] Ayush Satyam 2025-08-25 [Attention] Unify mamba and attention backend selection (#23171)
104	0	tests/v1/attention/test_attention_backends_selection.py
0	25	tests/v1/attention/test_mamba_selectors.py
2	1	vllm/attention/layer.py
23	0	vllm/model_executor/layers/attention_layer_base.py
13	2	vllm/model_executor/layers/mamba/abstract.py
9	1	vllm/model_executor/layers/mamba/mamba_mixer.py
9	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
9	1	vllm/model_executor/layers/mamba/short_conv.py
9	1	vllm/model_executor/models/minimax_text_01.py
0	22	vllm/v1/attention/backends/mamba_selectors.py
8	18	vllm/v1/worker/gpu_model_runner.py

[d0a4a3f64] youkaichao 2025-08-25 [misc] add shanghai meetup (#23535)
2	1	README.md
1	0	docs/community/meetups.md

[ebafb0936] Cyrus Leung 2025-08-25 [Bugfix] Allow dynamic number of patches for llava_onevision (#23525)
1	1	vllm/model_executor/models/llava_onevision.py

[0cb7b065c] Breno Baldas Skuk 2025-08-25 Feature/benchmark/random mm data/images (#23119)
77	0	benchmarks/README.md
344	0	tests/benchmarks/test_random_dataset.py
630	56	vllm/benchmarks/datasets.py

[2da02dd0d] ZiTian Zhao 2025-08-25 [Fix] DeepSeek V3.1 tool parser error message (#23492)
2	2	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py

[d765cf01f] Chenguang Zheng 2025-08-25 [Core][Multimodal] Track encode cache entries by mm_hash and enable embedding sharing between requests (#22711)
144	0	tests/v1/core/test_encoder_cache_manager.py
4	4	tests/v1/core/test_scheduler.py
5	1	tests/v1/core/utils.py
6	6	tests/v1/tpu/worker/test_tpu_model_runner.py
1	0	tests/v1/worker/test_gpu_input_batch.py
6	6	tests/v1/worker/test_gpu_model_runner.py
150	87	vllm/v1/core/encoder_cache_manager.py
3	3	vllm/v1/core/sched/output.py
6	4	vllm/v1/core/sched/scheduler.py
1	0	vllm/v1/worker/gpu_input_batch.py
21	26	vllm/v1/worker/gpu_model_runner.py
19	18	vllm/v1/worker/tpu_model_runner.py

[712d0f88d] Cyrus Leung 2025-08-25 [Refactor] Dynamic `target` and `content` for prompt updates (#23411)
54	58	tests/multimodal/test_processing.py
12	16	vllm/model_executor/models/gemma3_mm.py
12	16	vllm/model_executor/models/gemma3n_mm.py
3	10	vllm/model_executor/models/llava.py
6	9	vllm/model_executor/models/phi3v.py
4	8	vllm/model_executor/models/phi4_multimodal.py
7	17	vllm/model_executor/models/phi4mm.py
1	3	vllm/model_executor/models/qwen2_5_omni_thinker.py
373	328	vllm/multimodal/processing.py

[49ab23b3c] Yu Guo 2025-08-24 [gpt-oss] use reasoning channel for reasoning text in serving_chat (#22920)
6	6	vllm/entrypoints/openai/serving_chat.py

[c9abb1048] LIYIFAN_liyifan 2025-08-24 [Bugfix] Fix Dense module loading for sentence-transformers embedding models (simplified V2) (#23408)
22	0	tests/models/language/pooling/test_st_projector.py
11	0	vllm/model_executor/layers/activation.py
23	1	vllm/model_executor/layers/pooler.py
97	1	vllm/model_executor/models/adapters.py
22	0	vllm/transformers_utils/config.py

[787cdb382] Benji Beck 2025-08-24 Migrate DonutImagePixelInputs to TensorSchema (#23509)
19	30	vllm/model_executor/models/donut.py

[a5203d04d] Benji Beck 2025-08-24 Migrate skyworkr1v inputs to TensorSchema (#23499)
36	38	vllm/model_executor/models/skyworkr1v.py

[99f809440] Benji Beck 2025-08-24 Migrate tarsier inputs to TensorSchema (#23500)
23	21	vllm/model_executor/models/tarsier.py

[170e8ea9e] Jee Jee Li 2025-08-25 [Misc] Unified linear print info (#23516)
1	1	vllm/model_executor/layers/linear.py

[a71e4765c] zifeitong 2025-08-24 [Bugfix] Fix Qwen2.5-VL quantized model weights loading (#23512)
5	1	vllm/model_executor/models/qwen2_5_vl.py

[39971db3a] Noam Gat 2025-08-25 Frontend: Adding LM Format Enforcer support to V1 engine (#22564)
1	1	requirements/common.txt
7	3	tests/v1/entrypoints/llm/test_struct_output_generate.py
2	1	vllm/config/__init__.py
5	0	vllm/v1/engine/processor.py
8	0	vllm/v1/structured_output/__init__.py
167	0	vllm/v1/structured_output/backend_lm_format_enforcer.py

[504d91431] Ming Yang 2025-08-24 [Perf] Add Triton config for DeepSeek V3 FP8 EP32 H200 (#23504)
1	1	benchmarks/kernels/benchmark_w8a8_block_fp8.py
154	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=2112,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
3	0	vllm/model_executor/layers/quantization/utils/configs/README.md

[47455c424] Didier Durand 2025-08-25 [Doc: ]fix various typos in multiple files (#23487)
1	1	.buildkite/nightly-benchmarks/nightly-descriptions.md
1	1	docs/deployment/frameworks/anything-llm.md
1	1	docs/design/fused_moe_modular_kernel.md
1	1	docs/design/metrics.md
1	1	docs/design/paged_attention.md
1	1	docs/features/quantization/inc.md
3	3	docs/getting_started/installation/cpu.md
2	2	docs/getting_started/installation/intel_gaudi.md
1	1	vllm/config/cache.py

[c7fc6b135] Lucia Fang 2025-08-24 fix incompatibililty with non cuda platform for nvfp4 (#23478)
3	1	vllm/compilation/fusion.py

[ad7886845] Woosuk Kwon 2025-08-24 [Misc] Remove unused slot_mapping buffer (#23502)
0	3	vllm/v1/worker/gpu_model_runner.py

[e2db1164a] Cyrus Leung 2025-08-24 [Model] Enable BLOOM on V1 (#23488)
1	1	docs/models/supported_models.md
2	2	vllm/model_executor/models/bloom.py

[416f05929] 汪志鹏 2025-08-24 [New Model]Donut model (#23229)
1	0	docs/models/supported_models.md
311	0	examples/offline_inference/dolphin.py
46	0	examples/offline_inference/encoder_decoder_multimodal.py
2	0	tests/models/multimodal/processing/test_common.py
3	0	tests/models/registry.py
1	1	vllm/engine/llm_engine.py
398	0	vllm/model_executor/models/donut.py
1	0	vllm/model_executor/models/registry.py
475	0	vllm/model_executor/models/swin.py
1	1	vllm/multimodal/profiling.py
1	1	vllm/v1/engine/processor.py

[5e021b498] TeeKen Lau 2025-08-24 (Misc): add missing test for zero truncation size. (#23457)
22	0	tests/entrypoints/openai/test_truncation.py

[1b9b16649] rongfu.leng 2025-08-24 [Misc] update dict parse to EPLBConfig from json dumps to dict unpacking (#23305)
1	8	vllm/config/parallel.py
1	2	vllm/engine/arg_utils.py

[e76e23354] czhu-cohere 2025-08-24 [kernel] Support W4A8 on Hopper (#23198)
27	0	CMakeLists.txt
33	0	benchmarks/kernels/benchmark_machete.py
6	0	benchmarks/kernels/weight_shapes.py
418	0	csrc/quantization/cutlass_w4a8/w4a8_mm_entry.cu
20	0	csrc/torch_bindings.cpp
259	0	tests/kernels/quantization/test_cutlass_w4a8.py
48	0	vllm/_custom_ops.py
37	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
3	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
160	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_fp8.py
3	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
114	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/cutlass.py

[a75277285] Benji Beck 2025-08-23 Migrate Paligemma inputs to TensorSchema (#23470)
27	36	vllm/model_executor/models/paligemma.py

[9dc30b706] 22quinn 2025-08-23 [Bugfix] Add strong reference to CUDA pluggable allocator callbacks (#23477)
9	4	vllm/device_allocator/cumem.py

[053278a5d] Benji Beck 2025-08-23 Migrate Pixtral inputs to TensorSchema (#23472)
13	11	vllm/model_executor/models/pixtral.py

[c55c02899] Jiangyun Zhu 2025-08-24 [gpt-oss] Streaming Output for Python Tool (#23409)
44	26	vllm/entrypoints/openai/serving_responses.py

[65197a5fb] Jee Jee Li 2025-08-23 [Misc] Modify CacheConfig import (#23459)
1	1	vllm/attention/layers/encoder_only_attention.py

[b8f17f5d9] Xu Wenqing 2025-08-23 Support DeepSeek-V3.1 tool call (#23454)
8	0	docs/features/tool_calling.md
91	0	examples/tool_chat_template_deepseekv31.jinja
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
367	0	vllm/entrypoints/openai/tool_parsers/deepseekv31_tool_parser.py

[d9a55204b] Aziz 2025-08-23 fix(tests): Correct unreachable assertion in truncation test (#23425)
9	12	tests/entrypoints/openai/test_truncation.py

[b4e9fd811] Cyrus Leung 2025-08-23 Revert "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)" (#23396)
3	4	vllm/transformers_utils/tokenizer.py

[308fa287a] Chenxi Yang 2025-08-22 Add glm4.5v tp2,4 fp8 config on H100_80GB (#23443)
122	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=352,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
114	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=704,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json

[fa78de9dc] Daifeng Li 2025-08-23 Quantization: support FP4 quantized models on AMD CDNA2/CDNA3 GPUs (#22527)
1	1	requirements/rocm.txt
2	0	setup.py
15	3	vllm/config/__init__.py
1	0	vllm/model_executor/layers/linear.py
3	0	vllm/model_executor/layers/quantization/__init__.py
306	0	vllm/model_executor/layers/quantization/petit.py
122	0	vllm/model_executor/layers/quantization/utils/petit_utils.py
1	1	vllm/platforms/rocm.py

[f6818a92c] Michael Goin 2025-08-22 [UX] Move Dockerfile DeepGEMM install to tools/install_deepgemm.sh (#23360)
4	24	docker/Dockerfile
108	0	tools/install_deepgemm.sh

[23c939fd3] WeiQing Chen 2025-08-23 [Model] Support DP for ViT on MiniCPM-V-4 (#23327)
1	0	docs/configuration/optimization.md
96	27	vllm/model_executor/models/idefics2_vision_model.py
6	3	vllm/model_executor/models/minicpmv.py
2	0	vllm/multimodal/utils.py

[add1adfec] Nick Hill 2025-08-22 [BugFix] Fix `MinPLogitsProcessor.update_states()` (#23401)
21	13	vllm/v1/sample/logits_processor/builtin.py

[c80c53a30] Nick Hill 2025-08-22 [BugFix] Fix batch updates for pooling models (#23398)
16	4	vllm/v1/sample/logits_processor/state.py
76	70	vllm/v1/worker/gpu_input_batch.py
3	5	vllm/v1/worker/gpu_model_runner.py

[24d0c9e6e] elvischenv 2025-08-23 [NVIDIA][torch.compile] Support Flashinfer TRTLLM FP8-q/kv NVFP4-out Attention Kernel (#22703)
37	13	benchmarks/kernels/benchmark_trtllm_decode_attention.py
39	13	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
3	2	tests/compile/test_functionalization.py
5	5	tests/compile/test_fusion.py
113	39	tests/compile/test_fusion_attn.py
69	7	tests/kernels/attention/test_flashinfer_trtllm_attention.py
5	8	vllm/attention/backends/abstract.py
6	0	vllm/attention/backends/differential_flash_attn.py
2	1	vllm/attention/backends/dual_chunk_flash_attn.py
2	1	vllm/attention/backends/flash_attn.py
2	1	vllm/attention/backends/mla/common.py
9	5	vllm/attention/backends/rocm_flash_attn.py
2	1	vllm/attention/backends/xformers.py
5	2	vllm/attention/layer.py
18	48	vllm/compilation/fusion.py
151	34	vllm/compilation/fusion_attn.py
63	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
2	1	vllm/v1/attention/backends/cpu_attn.py
2	1	vllm/v1/attention/backends/flash_attn.py
49	13	vllm/v1/attention/backends/flashinfer.py
2	1	vllm/v1/attention/backends/flex_attention.py
2	1	vllm/v1/attention/backends/mla/common.py
2	1	vllm/v1/attention/backends/pallas.py
2	1	vllm/v1/attention/backends/rocm_aiter_fa.py
2	1	vllm/v1/attention/backends/tree_attn.py
2	1	vllm/v1/attention/backends/triton_attn.py
2	1	vllm/v1/attention/backends/xformers.py

[cc7ae5e7c] rasmith 2025-08-22 [BugFix][AMD][Quantization] Fix torch.compile issue where wvSplitKQ not being called when it should when using quantized FP8 model (#22281)
33	7	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[0313cf854] Ilya Markov 2025-08-22 [PERF] PyTorch Symmetric Memory All-Reduce (#20759)
1	1	docs/design/multiprocessing.md
108	0	tests/distributed/test_symm_mem_allreduce.py
1	1	tools/check_pickle_imports.py
33	0	vllm/distributed/device_communicators/{custom_all_reduce_utils.py => all_reduce_utils.py}
15	0	vllm/distributed/device_communicators/cuda_communicator.py
9	3	vllm/distributed/device_communicators/custom_all_reduce.py
111	0	vllm/distributed/device_communicators/symm_mem.py
5	0	vllm/envs.py

[0483fabc7] Zhewen Li 2025-08-22 [CI/Build] add EP dependencies to docker (#21976)
7	0	.buildkite/test-pipeline.yaml
8	0	docker/Dockerfile

[da65bec30] Shiyan Deng 2025-08-22 add an env var for path to pre-downloaded flashinfer cubin files (#22675)
6	0	vllm/envs.py
5	0	vllm/utils/flashinfer.py

[4645024d3] Isotr0py 2025-08-23 [Quantization] Allow GGUF quantization to skip unquantized layer (#23188)
11	2	vllm/model_executor/layers/quantization/gguf.py
13	1	vllm/model_executor/model_loader/gguf_loader.py
12	0	vllm/model_executor/model_loader/weight_utils.py

[cd7a3df26] Isotr0py 2025-08-23 [Bugfix] Fix broken Florence-2 model (#23426)
6	2	vllm/model_executor/models/florence2.py

[32d2b4064] Isotr0py 2025-08-23 [Model] Add Ovis2.5 PP support (#23405)
1	0	tests/distributed/test_pipeline_parallel.py
1	5	tests/models/multimodal/generation/test_common.py
1	3	tests/models/registry.py
20	16	vllm/model_executor/models/ovis2_5.py
162	81	vllm/model_executor/models/siglip2navit.py

[22cf679aa] Didier Durand 2025-08-22 [Doc]: fix various typos in multiple files (#23179)
1	1	vllm/beam_search.py
1	1	vllm/compilation/backends.py
3	3	vllm/engine/arg_utils.py
2	2	vllm/engine/multiprocessing/client.py
1	1	vllm/entrypoints/chat_utils.py
2	2	vllm/utils/__init__.py
2	2	vllm/v1/structured_output/__init__.py

[b6d7d34fc] Yong Hoon Shin 2025-08-22 Add unit tests for batched guided and non-guided requests (#23389)
82	0	tests/v1/entrypoints/llm/test_struct_output_generate.py

[341923b98] Aziz 2025-08-22 fix(tests): Ensure reliable CUDA cache clearing in MoE test (#23416)
1	1	tests/kernels/moe/test_moe.py

[424fb7a5d] bppps 2025-08-23 [BugFix] Fix the issue where image embeddings were incorrectly split.… (#23366)
5	2	vllm/model_executor/models/glm4_1v.py
54	32	vllm/model_executor/models/qwen2_5_omni_thinker.py
45	23	vllm/model_executor/models/qwen2_vl.py

[88491c1b6] PapaGoose 2025-08-22 [Speculators][Speculative Decoding] Fix Qwen 2 Eagle3 Support (#23337)
9	2	vllm/model_executor/models/qwen2.py

[613a23b57] Martin Hickey 2025-08-22 [Bugfix]: Installing dev environment due to pydantic incompatible version (#23353)
1	1	requirements/common.txt
1	1	requirements/test.txt

[51a215300] Burkhard Ringlein 2025-08-22 [Fix] Bump triton version in rocm-build requirements (#21630)
1	1	requirements/rocm-build.txt

[ebe14621e] Naman Lalit 2025-08-22 [Bug fix] Dynamically setting the backend variable for genai_perf_tests in the run-nightly-benchmark script (#23375)
1	1	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh

[325aa3dee] Ning Xie 2025-08-22 [Misc] local import code clean (#23420)
0	1	vllm/v1/worker/gpu_worker.py
1	2	vllm/worker/worker.py

[a073be6d8] Chen Zhang 2025-08-22 [Doc] Update the doc for log probs + prefix caching (#23399)
1	1	docs/usage/v1_guide.md

[695e7adcd] 杨朱 · Kiki 2025-08-22 [misc] Remove outdate comment about runai_model_streamer (#23421)
1	3	vllm/model_executor/model_loader/weight_utils.py

[281710ef9] Russell Bryant 2025-08-22 [Attention] Allow V1 flash_attn to support cross-attention (#23297)
7	10	vllm/v1/attention/backends/flash_attn.py

[808d2e9aa] Woosuk Kwon 2025-08-22 [Misc] Move M-RoPE init logic to _init_mrope_positions (#23422)
31	32	vllm/v1/worker/gpu_model_runner.py

[285178b3b] Jee Jee Li 2025-08-22 [V0 Deprecation] Remove V0 LoRA test (#23418)
4	27	tests/lora/conftest.py
4	7	tests/lora/test_add_lora.py
1	4	tests/lora/test_llama_tp.py
68	62	tests/lora/test_lora_manager.py
0	1	tests/lora/test_mixtral.py
5	15	tests/lora/test_worker.py
76	0	tests/lora/utils.py

[88016c372] Li, Jiang 2025-08-22 [Bugfix] Fix pooling models on CPU backend (#23392)
18	2	vllm/utils/__init__.py

[998720859] Benji Beck 2025-08-22 Migrate MiniCPMOAudioInputs to TensorSchema (#21847)
35	17	vllm/model_executor/models/minicpmo.py

[0ba1b54ac] Guillaume Calmettes 2025-08-22 [gpt-oss] add input/output usage in responses api when harmony context is leveraged (#22667)
26	2	vllm/entrypoints/context.py

[53415653f] Flora Feng 2025-08-21 [P/D][Nixl] Make kv cache register compatible with hybrid memory allocator (#23079)
85	1	tests/v1/kv_connector/unit/test_nixl_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/base.py
63	92	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[17373dcd9] Chen Zhang 2025-08-21 [Attention] Refactor AttentionMetadata Preparation for Encoder-only Models (#23154)
7	4	tests/v1/worker/test_gpu_model_runner.py
16	13	vllm/attention/layers/chunked_local_attention.py
86	0	vllm/attention/layers/encoder_only_attention.py
8	9	vllm/model_executor/models/bert.py
8	9	vllm/model_executor/models/bert_with_rope.py
5	1	vllm/model_executor/models/llama.py
7	7	vllm/model_executor/models/modernbert.py
4	1	vllm/model_executor/models/qwen2.py
1	31	vllm/v1/attention/backends/utils.py
8	0	vllm/v1/kv_cache_interface.py
73	140	vllm/v1/worker/gpu_model_runner.py
4	0	vllm/v1/worker/utils.py

[596406936] Bin Jia 2025-08-22 [New Model] Add Seed-Oss model (#23241)
1	0	docs/models/supported_models.md
3	0	tests/models/registry.py
459	0	tests/tool_use/test_seed_oss_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
676	0	vllm/entrypoints/openai/tool_parsers/seed_oss_tool_parser.py
1	0	vllm/model_executor/models/registry.py
487	0	vllm/model_executor/models/seed_oss.py

[de9c085e1] Philip Chung 2025-08-21 [Misc] Add gemma3 chat template with pythonic-style function calling (#17149)
123	0	examples/tool_chat_template_gemma3_pythonic.jinja

[111692bb8] Arjun Reddy 2025-08-21 [CI] Add end-to-end V1 min_tokens test coverage (#22495)
479	0	tests/v1/e2e/test_min_tokens.py

[394591e34] Wentao Ye 2025-08-22 [Feature] Enable DeepGEMM Linear on B200; 1.5% E2E throughput improvement (#23351)
6	16	vllm/model_executor/layers/quantization/utils/fp8_utils.py
7	0	vllm/utils/deep_gemm.py

[3ac849665] Isotr0py 2025-08-22 [CI/Build] Skip Idefics3 and SmolVLM generation test again (#23356)
4	4	tests/models/registry.py

[0b9cc56fa] Benji Beck 2025-08-21 Migrate MllamaImagePixelInputs to TensorSchema (#22020)
26	10	vllm/model_executor/models/mllama.py

[8896eb72e] Cyrus Leung 2025-08-22 [Deprecation] Remove `prompt_token_ids` arg fallback in `LLM.generate` and `LLM.embed` (#18800)
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
1	1	docker/Dockerfile.rocm
1	1	docs/features/quantization/fp8.md
1	1	docs/features/quantization/int4.md
1	1	docs/features/quantization/int8.md
1	1	docs/features/quantization/quark.md
3	1	examples/offline_inference/spec_decode.py
1	1	examples/offline_inference/structured_outputs.py
1	1	requirements/nightly_torch_test.txt
2	1	requirements/test.in
1	1	requirements/test.txt
6	9	tests/entrypoints/llm/test_chat.py
2	3	tests/entrypoints/llm/test_classify.py
2	3	tests/entrypoints/llm/test_embedding.py
3	49	tests/entrypoints/llm/test_encode.py
3	40	tests/entrypoints/llm/test_generate.py
2	3	tests/entrypoints/llm/test_generate_multiple_loras.py
2	3	tests/entrypoints/llm/test_reward.py
2	3	tests/entrypoints/llm/test_score.py
2	4	tests/quantization/test_fp8.py
1	1	tests/quantization/test_lm_head.py
55	49	tests/v1/entrypoints/llm/test_struct_output_generate.py
21	288	vllm/entrypoints/llm.py

[19fe1a051] Matthew Bonanni 2025-08-21 [Kernel] Add FP8 support with FlashMLA backend (#22668)
5	4	cmake/external_projects/flashmla.cmake
4	2	csrc/cache.h
29	28	csrc/cache_kernels.cu
9	4	csrc/torch_bindings.cpp
29	13	tests/kernels/attention/test_cache.py
51	18	tests/kernels/attention/test_flashmla.py
12	8	vllm/_custom_ops.py
9	5	vllm/attention/backends/mla/common.py
6	0	vllm/attention/ops/flashmla.py
1	2	vllm/engine/arg_utils.py
33	8	vllm/platforms/cuda.py
2	1	vllm/platforms/interface.py
2	1	vllm/platforms/rocm.py
2	1	vllm/platforms/tpu.py
31	6	vllm/v1/attention/backends/mla/common.py
2	1	vllm/v1/attention/backends/mla/cutlass_mla.py
4	6	vllm/v1/attention/backends/mla/flashmla.py
2	0	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
2	1	vllm/v1/attention/backends/mla/triton_mla.py

[480bdf5a7] 22quinn 2025-08-21 [Core] Support custom executor qualname (#23314)
1	0	.buildkite/test-pipeline.yaml
0	0	tests/v1/executor/__init__.py
116	0	tests/v1/executor/test_executor.py
7	7	vllm/config/parallel.py
1	1	vllm/engine/arg_utils.py
8	0	vllm/v1/executor/abstract.py

[5368f7685] Kebe 2025-08-22 [Feature][Responses API] Support logprobs(non-stream) (#23319)
13	0	tests/v1/entrypoints/openai/responses/test_basic.py
11	2	vllm/entrypoints/openai/protocol.py
62	2	vllm/entrypoints/openai/serving_responses.py

[8ef6b8a38] tvalentyn 2025-08-22 Always use cache mounts when installing vllm to avoid populating pip cache in the image. Also remove apt cache. (#23270)
8	3	docker/Dockerfile.tpu

[3bbe11cc1] Michael Goin 2025-08-21 [Perf] Small optimizations for silu_mul_fp8_quant_deep_gemm (#23265)
77	0	benchmarks/kernels/benchmark_silu_mul_fp8_quant.py
2	2	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
28	30	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[c5041f899] Simon Mo 2025-08-21 [CI] improve pr comments bot (#23380)
8	10	.github/workflows/reminder_comment.yml

[8b5fe6eb5] Simon Mo 2025-08-21 [CI] Clean up actions: remove helm, publish workflows and improve pr … (#23377)
0	89	.github/workflows/lint-and-deploy.yaml
0	111	.github/workflows/publish.yml
40	11	.github/workflows/reminder_comment.yml

[800349c2a] Woosuk Kwon 2025-08-21 [Structured Outputs] Refactor bitmask construction into get_grammar_bitmask (#23361)
33	22	vllm/v1/core/sched/scheduler.py

[044931f97] Elvir Crnčević 2025-08-21 Make sure that vectorize_with_alignment produced vectorized global loads (#23182)
10	5	csrc/quantization/vectorization_utils.cuh

[1d353b635] Pavani Majety 2025-08-21 [Core] Always use tensor cores for Flashinfer Decode Wrapper (#23214)
1	1	benchmarks/kernels/benchmark_trtllm_decode_attention.py
2	4	tests/kernels/attention/test_flashinfer.py
1	3	tests/kernels/attention/test_flashinfer_trtllm_attention.py
0	7	vllm/envs.py
28	50	vllm/v1/attention/backends/flashinfer.py

[349627466] Ning Xie 2025-08-22 [Misc] Convert VLLM_TORCH_PROFILER_DIR path to absolute (#23191)
2	1	vllm/envs.py

[8a1930317] Chen Zhang 2025-08-21 [BugFix][gpt-oss] Fix Chat Completion with Multiple Output Message (#23318)
5	9	vllm/entrypoints/harmony_utils.py

[603fbbbce] Nick Hill 2025-08-21 [Misc] Misc code cleanup/simplification (#23304)
1	1	vllm/v1/sample/sampler.py
5	4	vllm/v1/worker/gpu_input_batch.py
45	52	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/worker/worker_base.py

[10f535c08] Ming Yang 2025-08-21 [Bugfix] Fix port conflict by obtaining a list of open ports upfront (#21894)
18	6	vllm/config/parallel.py
14	6	vllm/utils/__init__.py
3	1	vllm/v1/engine/utils.py

[48bfb0c9b] Wentao Ye 2025-08-21 [Bug] Fix R1 Accuracy 0 Bug (#23294)
15	7	vllm/model_executor/layers/quantization/fp8.py

[f8ce02294] Lain 2025-08-21 add tg-mxfp4-moe-test (#22540)
1	0	.buildkite/test-pipeline.yaml
419	1	tests/kernels/moe/test_mxfp4_moe.py

[0278f1ac3] Yi Liu 2025-08-22 Fix nvfp4 swizzling (#23140)
3	24	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
2	2	vllm/model_executor/layers/quantization/utils/quant_utils.py

[a482e4e76] Benji Beck 2025-08-21 Migrate MolmoImageInputs to TensorSchema (#22022)
23	33	vllm/model_executor/models/molmo.py

[e0b056e44] youkaichao 2025-08-21 [ci/build] Fix abi tag for aarch64 (#23329)
7	2	.buildkite/generate_index.py
13	2	.buildkite/scripts/upload-wheels.sh
12	3	setup.py

[79f05e443] Roger Wang 2025-08-21 [Multimodal] Always enable hashing mm data (#23308)
0	9	vllm/config/__init__.py
11	42	vllm/inputs/preprocess.py
0	4	vllm/model_executor/models/deepseek_vl2.py
0	4	vllm/model_executor/models/h2ovl.py
1	2	vllm/model_executor/models/llava.py
1	2	vllm/model_executor/models/mllama.py
1	2	vllm/model_executor/models/paligemma.py
0	3	vllm/model_executor/models/pixtral.py
69	45	vllm/model_executor/models/prithvi_geospatial_mae.py
0	1	vllm/model_executor/models/transformers.py
0	3	vllm/model_executor/models/voxtral.py
1	1	vllm/multimodal/hasher.py
1	1	vllm/multimodal/inputs.py
4	16	vllm/multimodal/processing.py
6	14	vllm/v1/engine/processor.py

[f8daddcc4] jerryzhuang 2025-08-22 [Bugfix] set system_message in phi4mini chat template (#23309)
8	6	examples/tool_chat_template_phi4_mini.jinja

[c8e33c72c] Robert Shaw 2025-08-21 [V1] Remove unnecessary check for main thread (#23298)
0	6	vllm/engine/arg_utils.py

[d70a16625] wang.yuqi 2025-08-21 [Performance] V1 Pooling Models E2E Performance Optimization (#23162)
48	83	vllm/model_executor/layers/pooler.py
3	3	vllm/model_executor/models/bert.py
10	55	vllm/model_executor/models/roberta.py
17	6	vllm/model_executor/pooling_metadata.py
54	2	vllm/v1/pool/metadata.py
1	1	vllm/v1/worker/gpu_input_batch.py
20	16	vllm/v1/worker/gpu_model_runner.py
8	1	vllm/worker/pooling_model_runner.py

[5cc54f7c5] Cyrus Leung 2025-08-21 [Doc] Fix batch-level DP example (#23325)
6	5	docs/configuration/optimization.md

[0c6e40bba] Cyrus Leung 2025-08-21 [Refactor] Simplify code for MM budget (#23310)
32	24	vllm/v1/core/encoder_cache_manager.py
5	13	vllm/v1/worker/gpu_model_runner.py
3	10	vllm/v1/worker/tpu_model_runner.py
17	21	vllm/v1/worker/utils.py

[2e2000f35] Paul Pak 2025-08-21 [Model] Add LFM2 architecture (#22845)
1	0	docs/models/supported_models.md
25	8	tests/models/language/generation/test_hybrid.py
2	0	tests/models/registry.py
2	0	tests/models/test_initialization.py
1	0	vllm/config/compilation.py
24	0	vllm/model_executor/layers/mamba/mamba_utils.py
262	0	vllm/model_executor/layers/mamba/short_conv.py
557	0	vllm/model_executor/models/lfm2.py
1	0	vllm/model_executor/models/registry.py
4	0	vllm/v1/attention/backends/mamba_selectors.py
81	0	vllm/v1/attention/backends/short_conv_attn.py

[31282401b] Jared O'Connell 2025-08-21 [BugFix] Fix Python 3.9 Support (#23306)
2	2	vllm/benchmarks/lib/endpoint_request_func.py

[0c31e28e9] Cyrus Leung 2025-08-21 [Bugfix] Fix extra whitespace in strings caused by newline (#23272)
4	2	benchmarks/benchmark_dataset.py
7	8	examples/offline_inference/vision_language.py
4	2	vllm/benchmarks/datasets.py
6	5	vllm/model_executor/model_loader/tpu.py
4	5	vllm/model_executor/models/hyperclovax_vision.py
3	3	vllm/model_executor/models/phi4mm.py
2	2	vllm/transformers_utils/configs/eagle.py

[f571ff8eb] 22quinn 2025-08-20 [Sampler] Support returning final logprobs (#22387)
5	2	docs/usage/v1_guide.md
5	5	tests/v1/sample/test_logprobs.py
18	12	vllm/config/__init__.py
1	0	vllm/engine/arg_utils.py
35	30	vllm/v1/sample/ops/topk_topp_sampler.py
61	20	vllm/v1/sample/sampler.py
1	1	vllm/v1/sample/tpu/sampler.py

[f64ee61d9] Michael Goin 2025-08-21 [CI] Block the cu126 wheel build while broken (#23285)
5	0	.buildkite/release-pipeline.yaml

[8993073dc] QiliangCui 2025-08-21 [CI] Delete images older than 24h. (#23291)
1	1	.buildkite/scripts/tpu/cleanup_docker.sh

[655a09f65] 杨奇(yann qi) 2025-08-21 [Model][VLM] Support R-4B Model (#23246)
1	0	docs/models/supported_models.md
23	0	examples/offline_inference/vision_language.py
34	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py
103	0	vllm/model_executor/models/rvl.py

[f94bf9b92] Wentao Ye 2025-08-20 [Compile] Fix Compile Warning SM100 Cutlass MLA (#23287)
2	2	csrc/attention/mla/sm100_cutlass_mla_kernel.cu

[3663870c7] Asaf Joseph Gardin 2025-08-21 [V1][Mamba1] - Full CUDA and Piecewise CUDA Graphs Support (#23035)
1	1	docs/usage/v1_guide.md
6	14	tests/models/language/generation/test_hybrid.py
1	0	vllm/config/compilation.py
57	9	vllm/model_executor/layers/mamba/mamba_mixer.py
5	3	vllm/model_executor/models/jamba.py
5	2	vllm/model_executor/models/mamba.py
17	20	vllm/v1/attention/backends/mamba1_attn.py
7	38	vllm/v1/attention/backends/mamba2_attn.py
55	0	vllm/v1/attention/backends/mamba_attn.py

[2461d9e56] Cyrus Leung 2025-08-21 [CI/Build] Split out mm processor tests (#23260)
11	4	.buildkite/test-pipeline.yaml
3	4	tests/models/multimodal/{ => processing}/test_tensor_schema.py
2	0	vllm/model_executor/models/cohere2_vision.py

[7be5d113d] Li, Jiang 2025-08-21 [CPU] Refactor CPU W8A8 scaled_mm (#23071)
6	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh
20	39	cmake/cpu_extension.cmake
4	4	csrc/cpu/cpu_types_x86.hpp
346	0	csrc/cpu/dnnl_helper.cpp
169	0	csrc/cpu/dnnl_helper.h
0	206	csrc/cpu/dnnl_helper.hpp
494	0	csrc/cpu/dnnl_kernels.cpp
0	951	csrc/cpu/quant.cpp
32	64	csrc/cpu/torch_bindings.cpp
144	0	tests/kernels/test_onednn.py
83	0	vllm/_custom_ops.py
8	3	vllm/model_executor/layers/fused_moe/layer.py
4	4	vllm/model_executor/layers/linear.py
3	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
206	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/cpu.py
2	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
6	0	vllm/model_executor/layers/utils.py

[b029de990] Woosuk Kwon 2025-08-20 [Optimization] Make new_block_ids None if empty (#23262)
26	4	vllm/v1/core/kv_cache_manager.py
1	1	vllm/v1/core/sched/output.py
12	12	vllm/v1/core/sched/scheduler.py
9	5	vllm/v1/worker/gpu_model_runner.py
9	5	vllm/v1/worker/tpu_model_runner.py

[bbea1cefd] Michael Goin 2025-08-20 [CI Bugfix] Fix CI by fully removing --enable-prompt-adapter (#23284)
0	6	vllm/engine/arg_utils.py

[f5aa307d7] Russell Bryant 2025-08-20 Remove duplicate entry in vllm.attention.__all__ (#23296)
0	1	vllm/attention/__init__.py

[4b795020e] 22quinn 2025-08-20 [EP] Add logging for experts map (#22685)
26	0	vllm/model_executor/layers/fused_moe/layer.py

[c86af22f3] shixianc 2025-08-20 [Fix] remove is_marlin param in benchmark_moe (#23286)
[10cc12ba6] Matthew Bonanni 2025-08-20 Feature/mla tests (#23195)
11	15	tests/v1/attention/test_attention_backends.py
522	0	tests/v1/attention/test_mla_backends.py
10	1	tests/v1/attention/utils.py
8	8	vllm/v1/attention/backends/mla/common.py

[a4fbb32fa] Matthew Bonanni 2025-08-20 Remove chunked_prefill_enabled flag in V1 MLA (#23183)
23	27	vllm/v1/attention/backends/mla/common.py

[1b125004b] youkaichao 2025-08-21 [misc] fix multiple arch wheels for the nightly index (#23110)
16	2	.buildkite/generate_index.py

[4fbda0b20] rongfu.leng 2025-08-21 [Feature] use --eplb_config to set eplb param (#20562)
2	1	vllm/config/__init__.py
87	21	vllm/config/parallel.py
2	2	vllm/distributed/eplb/eplb_state.py
46	17	vllm/engine/arg_utils.py
2	2	vllm/model_executor/models/deepseek_v2.py
2	2	vllm/model_executor/models/glm4_moe.py
4	3	vllm/model_executor/models/qwen3_moe.py
2	2	vllm/v1/worker/gpu_model_runner.py
2	2	vllm/v1/worker/gpu_worker.py

[4e51fa8cb] Russell Bryant 2025-08-20 Do not use eval() to convert unknown types (#23266)
4	9	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py

[bf7c99dfc] Saurabh Misra 2025-08-20 [Perf] Speed up function `_convert_tokens_to_string_with_added_encoders` by 13.7x (#20413)
15	10	vllm/transformers_utils/detokenizer_utils.py

[b95697d73] Chen Zhang 2025-08-20 [Frontend] improve error logging of chat completion (#22957)
57	17	vllm/entrypoints/openai/api_server.py

[582bbe6bd] bigmoyan 2025-08-21 [Fix] correct tool_id for kimi-k2 when use tool_choice=required (#21259)
191	123	tests/entrypoints/openai/test_completion_with_function_calling.py
9	1	tests/utils.py
15	2	vllm/entrypoints/chat_utils.py
2	2	vllm/entrypoints/openai/protocol.py
46	18	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[0cdbf5e61] Michael Goin 2025-08-20 [Kernel/Quant] Remove the original marlin format and qqq (#23204)
0	12	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-QQQ.yaml
0	1	.buildkite/lm-eval-harness/configs/models-large.txt
0	2	CMakeLists.txt
1	22	benchmarks/kernels/benchmark_machete.py
73	72	csrc/quantization/machete/generate.py
0	209	csrc/quantization/marlin/dense/LICENSE
0	32	csrc/quantization/marlin/dense/common/base.h
0	89	csrc/quantization/marlin/dense/common/mem.h
0	1073	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
0	1248	csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu
0	17	csrc/torch_bindings.cpp
0	6	tests/compile/test_full_graph.py
17	17	tests/kernels/quantization/test_machete_mm.py
0	83	tests/kernels/quantization/test_marlin_gemm.py
0	10	tests/quantization/test_configs.py
1	5	tests/quantization/test_lm_head.py
0	4	tests/weight_loading/models.txt
0	36	vllm/_custom_ops.py
3	4	vllm/config/__init__.py
0	3	vllm/lora/layers.py
0	1	vllm/model_executor/layers/linear.py
0	6	vllm/model_executor/layers/quantization/__init__.py
0	263	vllm/model_executor/layers/quantization/marlin.py
0	275	vllm/model_executor/layers/quantization/qqq.py
0	126	vllm/model_executor/layers/quantization/utils/marlin_utils_test_qqq.py
0	85	vllm/model_executor/layers/quantization/utils/quant_utils.py

[ebe56a006] dongluw 2025-08-20 Small fix for Command-A-Vision (#23268)
1	1	vllm/model_executor/models/cohere2_vision.py

[f77a0802b] Russell Bryant 2025-08-20 Limit HTTP header count and size (#23267)
10	0	vllm/entrypoints/constants.py
21	0	vllm/entrypoints/launcher.py
2	0	vllm/entrypoints/openai/api_server.py
8	0	vllm/entrypoints/openai/cli_args.py

[c4477f55e] Benji Beck 2025-08-20 Migrate Mistral3ImagePixelInputs to TensorSchema (#21945)
17	21	vllm/model_executor/models/mistral3.py

[dfd238203] Yong Hoon Shin 2025-08-20 [torch.compile] Support conditional torch.compile per module (#22269)
2	0	.buildkite/test-pipeline.yaml
36	101	tests/compile/piecewise/test_multiple_graphs.py
251	0	tests/compile/test_decorator.py
19	2	vllm/compilation/decorators.py

[3b11b26b5] JartX 2025-08-20 [FIXBUG ] Allow disabling rocm_aiter_fa backend for ROCm GPUs not compatible with AITER (#22795)
45	35	vllm/v1/spec_decode/eagle.py

[d6d13bd49] Woosuk Kwon 2025-08-20 [Misc] Add max_seq_len to CommonAttentionMetadata  (#23216)
2	0	tests/v1/attention/utils.py
2	0	tests/v1/spec_decode/test_tree_attention.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/flex_attention.py
1	1	vllm/v1/attention/backends/rocm_aiter_fa.py
1	1	vllm/v1/attention/backends/tree_attn.py
1	1	vllm/v1/attention/backends/triton_attn.py
6	0	vllm/v1/attention/backends/utils.py
1	1	vllm/v1/attention/backends/xformers.py
1	0	vllm/v1/spec_decode/eagle.py
4	0	vllm/v1/worker/gpu_model_runner.py

[5efd6905b] Cyrus Leung 2025-08-20 [CLI][Doc] Formalize `--mm-encoder-tp-mode` (#23190)
45	0	docs/configuration/optimization.md
33	1	vllm/config/__init__.py
0	4	vllm/config/parallel.py
22	13	vllm/engine/arg_utils.py
2	2	vllm/model_executor/models/mllama4.py
1	2	vllm/model_executor/models/qwen2_5_vl.py
1	2	vllm/model_executor/models/step3_vl.py

[b17109bee] shixianc 2025-08-20 [Kernel] CUTLASS MoE FP8: Integrate cuda moe permute/unpermute (#23045)
34	1	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
14	19	csrc/moe/moe_permute_unpermute_op.cu
5	0	csrc/ops.h
4	2	csrc/quantization/cutlass_w8a8/moe/get_group_starts.cuh
50	15	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
24	0	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
13	0	csrc/torch_bindings.cpp
14	4	tests/kernels/moe/test_cutlass_moe.py
5	1	tests/kernels/moe/test_moe_permute_unpermute.py
21	1	tests/kernels/moe/test_pplx_cutlass_moe.py
1	1	tests/kernels/quantization/test_cutlass_scaled_mm.py
22	0	vllm/_custom_ops.py
111	68	vllm/model_executor/layers/fused_moe/cutlass_moe.py
20	9	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
31	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[444923584] Cyrus Leung 2025-08-20 [Bugfix] Ensure correctness of HCXVision processing (#23254)
1	1	tests/models/multimodal/processing/test_common.py
55	63	vllm/model_executor/models/hyperclovax_vision.py

[38217877a] rongfu.leng 2025-08-20 [Fix] fix offline env use local mode path (#22526)
35	0	tests/entrypoints/offline_mode/test_offline_mode.py
9	1	vllm/engine/arg_utils.py
21	2	vllm/transformers_utils/config.py

[c6d80a7a9] Jee Jee Li 2025-08-20 [Model] Improve olmo and olmo2 (#23228)
2	2	docs/models/supported_models.md
19	3	vllm/model_executor/models/olmo.py
15	2	vllm/model_executor/models/olmo2.py

[7cd17e22d] xyxinyang 2025-08-20 [Model][V1] Support Ernie MTP (#22169)
3	0	tests/models/registry.py
26	5	vllm/config/__init__.py
287	0	vllm/model_executor/models/ernie_mtp.py
1	0	vllm/model_executor/models/registry.py
1	1	vllm/v1/spec_decode/eagle.py
2	1	vllm/worker/worker.py

[50df09fe1] Michael Goin 2025-08-20 Update to flashinfer-python==0.2.12 and disable AOT compile for non-release image (#23129)
1	1	.buildkite/release-pipeline.yaml
33	19	docker/Dockerfile
1	1	setup.py

[68fcd3fa7] Cyrus Leung 2025-08-20 [Bugfix] Ensure correctness of Cohere2Vision processing (#23245)
1	0	tests/models/multimodal/processing/test_common.py
1	2	vllm/model_executor/models/aya_vision.py
54	17	vllm/model_executor/models/cohere2_vision.py

[83e69a09d] Xin Yang 2025-08-20 [Model] Support deepseek with eagle (#21086)
3	0	tests/models/registry.py
5	1	tests/v1/e2e/test_spec_decode.py
246	0	vllm/model_executor/models/deepseek_eagle.py
1	0	vllm/model_executor/models/registry.py

[3aa8c1003] Shiming Zhang 2025-08-20 Fix missing quotes (#23242)
1	1	docs/deployment/frameworks/dstack.md

[103f1ec8d] Calvin Chen 2025-08-20 [Model] use autoWeightsLoader for gptoss (#22446)
224	208	vllm/model_executor/models/gpt_oss.py

[d983769c4] who who who 2025-08-20 fix cuda graph (#22721)
4	3	vllm/v1/attention/backends/rocm_aiter_fa.py

[8fd920924] Nick Hill 2025-08-19 [BugFix] Fix stuck stats/metrics after requests are aborted (#22995)
94	1	tests/entrypoints/openai/test_metrics.py
6	1	vllm/v1/core/block_pool.py
6	3	vllm/v1/core/sched/scheduler.py

[de7b67a02] Cyrus Leung 2025-08-20 [CI/Build] Sync multimodal tests (#23181)
7	3	tests/models/multimodal/processing/test_common.py
11	13	tests/models/registry.py

[f72902327] Zhewen Li 2025-08-19 [CI/Build] Also check DP in benchmarks throughput script (#23038)
2	2	benchmarks/benchmark_throughput.py
8	0	vllm/benchmarks/throughput.py

[1a3079a15] 길재은 2025-08-20 chore: support pytorch format in lora  (#22790)
9	4	vllm/lora/models.py

[941f56858] Louie Tsai 2025-08-19 Fix a performance comparison issue in Benchmark Suite (#23047)
119	27	.buildkite/nightly-benchmarks/scripts/compare-json-results.py

[a634733f6] Zebing Lin 2025-08-19 [Attention] Optimize make_local_attention_virtual_batches for Flash Attention (#23185)
7	7	vllm/v1/attention/backends/utils.py

[64ab3c725] Cyrus Leung 2025-08-20 [Doc] Update V1 status of various pooling models (#23189)
13	13	docs/models/supported_models.md
5	4	tests/models/language/pooling/test_gritlm.py
3	3	vllm/model_executor/models/gritlm.py
7	4	vllm/model_executor/models/interfaces.py

[e58c5a976] Chenheli Hua 2025-08-19 [Core] Add torch profiler CPU traces for AsyncLLM. (#21794)
4	2	vllm/envs.py
31	2	vllm/v1/engine/async_llm.py

[d46d417b5] Michael Goin 2025-08-19 [CI Perf] Only test bfloat16 for tests/compile/test_fusion_all_reduce.py (#23132)
1	1	tests/compile/test_fusion_all_reduce.py

[0167efe20] 633WHU 2025-08-20 [Core] Optimize scheduler request removal for single completions (#21917)
6	8	vllm/v1/core/sched/scheduler.py
33	0	vllm/v1/core/sched/utils.py

[c32e6ad1f] Kyle Sayers 2025-08-19 [Quantization] Bump Compressed Tensors Version (#23202)
1	1	requirements/common.txt

[1630cc8d0] Chenheli Hua 2025-08-19 [Benchmarks] Add video inputs to ShareGPTDataset.  (#23199)
38	3	benchmarks/README.md
37	1	benchmarks/benchmark_dataset.py
38	2	vllm/benchmarks/datasets.py

[14e2b0730] Lucas Wilkinson 2025-08-19 [BugFix] fix CUTLASS MLA full cudagraph  (#23200)
1	1	vllm/v1/attention/backends/mla/cutlass_mla.py

[0f4f0191d] Michael Goin 2025-08-19 [CI/Build] Replace lm-eval gsm8k tests with faster implementation (#23002)
1	3	.buildkite/test-pipeline.yaml
35	0	tests/evals/gsm8k/README.md
2	0	tests/evals/gsm8k/__init__.py
5	0	tests/evals/gsm8k/configs/Llama-3-8B-Instruct-nonuniform-CT.yaml
5	0	tests/evals/gsm8k/configs/Llama-3.2-1B-Instruct-INT8-CT.yaml
5	0	tests/evals/gsm8k/configs/Qwen1.5-MoE-W4A16-CT.yaml
5	0	tests/evals/gsm8k/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
5	0	tests/evals/gsm8k/configs/Qwen3-0.6B-FP8.yaml
5	0	tests/evals/gsm8k/configs/models-small.txt
66	0	tests/evals/gsm8k/conftest.py
252	0	tests/evals/gsm8k/gsm8k_eval.py
90	0	tests/evals/gsm8k/test_gsm8k_correctness.py

[a38b8af4c] amirkl94 2025-08-20 [NVIDIA] Add SM100 Flashinfer Cutlass MoE fp8 backend (#22357)
2	0	.buildkite/test-pipeline.yaml
248	0	tests/kernels/moe/test_flashinfer.py
33	22	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
134	81	vllm/model_executor/layers/quantization/fp8.py
83	35	vllm/model_executor/layers/quantization/modelopt.py
112	0	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[21dce80ea] Michael Goin 2025-08-19 [CI/Build] Add support for Python 3.13 (#13164)
1	1	CMakeLists.txt
1	1	docs/getting_started/quickstart.md
2	1	pyproject.toml
11	1	vllm/config/__init__.py

[e61bac87e] Woosuk Kwon 2025-08-19 [Misc] Minor refactoring for FlashInfer backend (#23147)
65	91	vllm/v1/attention/backends/flashinfer.py

[80141bbf2] Marko Rosenmueller 2025-08-19 fix: use cache_salt for gpt-oss (#23186)
3	1	tests/entrypoints/openai/test_serving_chat.py
5	0	vllm/entrypoints/openai/serving_chat.py
5	0	vllm/entrypoints/openai/serving_responses.py

[b94faf9d5] bnellnm 2025-08-19 [Bugfix] Fix accuracy issue when using flashinfer cutlass moe, TP=1 and modelopt. (#23125)
49	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
2	0	vllm/model_executor/layers/fused_moe/layer.py
27	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
57	35	vllm/model_executor/layers/quantization/modelopt.py

[5b5f350d6] Woosuk Kwon 2025-08-19 [Misc] Enable yapf for FlashInfer backend (#23193)
24	13	vllm/v1/attention/backends/flashinfer.py

[f7cf5b512] 22quinn 2025-08-19 [Frontend] Add `/collective_rpc` API endpoint (#23075)
2	1	.buildkite/test-pipeline.yaml
88	0	tests/entrypoints/openai/test_collective_rpc.py
8	0	vllm/engine/protocol.py
28	0	vllm/entrypoints/openai/api_server.py

[03d4235fd] Ruixiang Tan 2025-08-20 [Misc] Fix the benchmark's README and improve the error messages for the benchmark's argument checks (#22654)
3	0	benchmarks/README.md
3	2	vllm/benchmarks/datasets.py

[d6a1a2097] Isotr0py 2025-08-20 [CI/Build] Update transformers to v4.55.2 (#23093)
1	1	requirements/common.txt
1	1	requirements/test.in
1	1	requirements/test.txt
9	8	tests/models/multimodal/generation/test_mllama.py

[a70d0bd0a] Benji Beck 2025-08-19 Migrate LlavaOnevisionMultiInputs to TensorSchema (#21844)
56	93	vllm/model_executor/models/llava_onevision.py

[24f4d1a22] Yuge Zhang 2025-08-20 Add return_token_ids parameter to OpenAI API endpoints (#22587)
38	25	tests/entrypoints/openai/test_openai_schema.py
374	0	tests/entrypoints/openai/test_return_token_ids.py
30	0	vllm/entrypoints/openai/protocol.py
18	4	vllm/entrypoints/openai/serving_chat.py
20	1	vllm/entrypoints/openai/serving_completion.py

[4f510bc2a] yiz-liu 2025-08-20 [Model] Removes redundant all-reduce operation in Qwen3MoeSparseMoeBlock (#23169)
1	5	vllm/model_executor/models/qwen3_moe.py

[1298c6779] TJian 2025-08-19 [FEAT] [Performance] Enable DP for ViT in Qwen2.5VL (#22742)
324	2	tests/multimodal/test_utils.py
1	1	vllm/model_executor/layers/linear.py
93	43	vllm/model_executor/models/qwen2_5_vl.py
0	2	vllm/model_executor/models/qwen2_vl.py
215	0	vllm/multimodal/utils.py

[4d9c61993] Jee Jee Li 2025-08-19 [Bugfix] Fix benchmark_moe.py  (#23177)
0	1	benchmarks/kernels/benchmark_moe.py

[b87cb97a5] myselvess 2025-08-19 [Model] support new model ovis2.5 (#23084)
1	0	docs/models/supported_models.md
33	0	examples/offline_inference/vision_language.py
31	0	examples/offline_inference/vision_language_multi_image.py
21	0	tests/models/multimodal/generation/test_common.py
58	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
2	0	tests/models/multimodal/processing/test_common.py
3	0	tests/models/registry.py
570	0	vllm/model_executor/models/ovis2_5.py
1	0	vllm/model_executor/models/registry.py
607	0	vllm/model_executor/models/siglip2navit.py
2	1	vllm/transformers_utils/processors/__init__.py
458	0	vllm/transformers_utils/processors/ovis2_5.py

[f856c33ce] wang.yuqi 2025-08-19 [Model] Add multi_label_classification support (#23173)
9	1	tests/conftest.py
33	0	tests/models/language/pooling/test_multilabel_classification_support.py
15	0	vllm/model_executor/layers/pooler.py

[03752dba8] elvischenv 2025-08-19 [NVIDIA] Support Flashinfer TRTLLM FP8-q/kv/out Attention Kernel (#21716)
2	0	.buildkite/test-pipeline.yaml
149	134	benchmarks/kernels/benchmark_trtllm_decode_attention.py
130	99	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
248	1	tests/compile/test_fusion_attn.py
172	128	tests/kernels/attention/test_flashinfer_trtllm_attention.py
9	2	vllm/attention/layer.py
43	34	vllm/compilation/fusion_attn.py
16	6	vllm/utils/flashinfer.py
80	29	vllm/v1/attention/backends/flashinfer.py

[40f26734b] Woosuk Kwon 2025-08-19 [Misc] Fix seq_lens for graph capture (#23175)
2	4	vllm/v1/worker/gpu_model_runner.py

[2c3f557f0] Tialo 2025-08-19 [Doc] use power of 2 (#23172)
1	1	docs/configuration/optimization.md

[21bcc8263] Woosuk Kwon 2025-08-19 [Misc] Avoid accessing req_ids inside a loop (#23159)
6	3	vllm/v1/worker/gpu_model_runner.py

[5bfe0dea7] qizixi 2025-08-19 [bug fix] Fix llama4 spec decoding (#22691)
4	2	vllm/model_executor/models/llama4.py

[31fd3265c] Isotr0py 2025-08-19 [Bugfix] Fix broken Minimax-01-VL model (#22116)
34	0	examples/offline_inference/vision_language.py
0	1	tests/models/multimodal/test_tensor_schema.py
89	31	vllm/model_executor/models/minimax_vl_01.py

[31436e8b4] hustxiayang 2025-08-19 [Misc] Add request_id into benchmark_serve.py (#23065)
21	2	benchmarks/backend_request_func.py
90	19	benchmarks/benchmark_dataset.py
21	2	benchmarks/benchmark_serving.py
92	21	vllm/benchmarks/datasets.py
7	0	vllm/benchmarks/lib/endpoint_request_func.py
12	2	vllm/benchmarks/serve.py

[4efd43e9b] qizixi 2025-08-19 Fix GLM-4.5V-FP8 numerical issue (#22949)
78	2	examples/offline_inference/vision_language.py
72	0	examples/offline_inference/vision_language_multi_image.py
4	3	vllm/model_executor/models/glm4_1v.py

[3c8a78724] Daniel Serebrenik 2025-08-19 [Benchmark] Add flag --served-model-name to benchmark_serving_multi_turn (#22889)
7	5	benchmarks/multi_turn/README.md
13	1	benchmarks/multi_turn/benchmark_serving_multi_turn.py

[01a08739e] Grace Ho 2025-08-19 [misc] split engine_model into json file for nsys profile tool (#23117)
26	27	tools/profiler/nsys_profile_tools/README.md
46	159	tools/profiler/nsys_profile_tools/gputrc2graph.py
63	0	tools/profiler/nsys_profile_tools/vllm_engine_model.json

[fda9537c5] Jiangyun Zhu 2025-08-19 [Model] Support Pipeline Parallelism for moonshotai/Kimi-VL-A3B-Thinking-2506 (#23114)
1	1	docs/models/supported_models.md
17	12	vllm/model_executor/models/kimi_vl.py

[90bbe0a5a] Wentao Ye 2025-08-19 [Log] Warning Once for Cutlass MLA  (#23137)
3	3	vllm/v1/attention/backends/mla/cutlass_mla.py

[e75f34226] Benji Beck 2025-08-18 Migrate InternVLImagePixelInputs (in nemotron_vl.py) to TensorSchema (#22023)
5	23	vllm/model_executor/models/nemotron_vl.py

[78dba404a] Nikhil Suryawanshi 2025-08-19 [Hardware][IBM Z]Enable v1 for s390x and s390x dockerfile fixes (#22725)
79	8	docker/Dockerfile.s390x
2	1	requirements/common.txt
2	2	requirements/cpu.txt
4	3	vllm/engine/arg_utils.py
3	2	vllm/platforms/cpu.py
3	0	vllm/platforms/interface.py
3	2	vllm/v1/worker/cpu_worker.py

[e9d6a3db6] Chengji Yao 2025-08-18 [TPU] make ptxla not imported when using tpu_commons (#23081)
13	14	vllm/distributed/device_communicators/tpu_communicator.py
1	1	vllm/model_executor/layers/fused_moe/moe_pallas.py
13	8	vllm/model_executor/model_loader/default_loader.py
3	0	vllm/platforms/tpu.py
51	46	vllm/v1/attention/backends/pallas.py
13	9	vllm/v1/worker/tpu_worker.py

[a4454e940] Xiao 2025-08-18 chore: disable enable_cpp_symbolic_shape_guards (#23048)
17	1	vllm/compilation/decorators.py

[14006840e] Woosuk Kwon 2025-08-18 [V0 Deprecation] Remove V0 FlashInfer attention backend (#22776)
1	8	tests/basic_correctness/test_basic_correctness.py
1	1	tests/compile/test_basic_correctness.py
2	6	tests/core/block/e2e/test_correctness_sliding_window.py
0	1	tests/distributed/test_pp_cudagraph.py
3	0	tests/kernels/attention/test_attention_selector.py
1	4	tests/models/quantization/test_fp8.py
0	1098	vllm/attention/backends/flashinfer.py
1	15	vllm/platforms/cuda.py

[660328873] Robert Shaw 2025-08-18 [CI][V0 Deprecation] Removed V0 Only Chunked Prefill and Prefix Caching Tests (#22871)
0	18	.buildkite/test-pipeline.yaml
0	1	.github/CODEOWNERS
0	296	tests/basic_correctness/test_chunked_prefill.py
0	0	tests/prefix_caching/__init__.py
0	49	tests/prefix_caching/test_disable_sliding_window.py
0	231	tests/prefix_caching/test_prefix_caching.py

[95e309513] Thomas Parnell 2025-08-19 [Misc] Add @tdoublep as a maintainer of hybrid model and Triton-attention related code (#23122)
9	0	.github/CODEOWNERS

[c9b38be8a] Woosuk Kwon 2025-08-18 [Spec Decode] Make `propose_draft_token_ids` non-blocking for lower TTFT (#23041)
0	1	tests/v1/core/test_async_scheduler.py
3	23	tests/v1/core/test_scheduler.py
0	1	tests/v1/kv_connector/unit/utils.py
9	1	vllm/v1/core/sched/interface.py
25	14	vllm/v1/core/sched/scheduler.py
10	0	vllm/v1/engine/core.py
6	2	vllm/v1/executor/abstract.py
7	1	vllm/v1/executor/multiproc_executor.py
9	4	vllm/v1/outputs.py
3	1	vllm/v1/spec_decode/medusa.py
23	14	vllm/v1/worker/gpu_model_runner.py
5	1	vllm/v1/worker/gpu_worker.py
0	1	vllm/v1/worker/tpu_model_runner.py

[0dd3f4f5a] Woosuk Kwon 2025-08-18 [Misc] Minor refactoring for prepare_inputs (#23116)
21	22	vllm/v1/worker/gpu_model_runner.py

[498259ccc] Xiang Xu 2025-08-18 Install tpu_info==0.4.0 to fix core dump for TPU (#23135)
1	0	requirements/tpu.txt

[6d25e3fd6] Michael Goin 2025-08-18 Use Blackwell FlashInfer MXFP4 MoE by default if available  (#23008)
5	5	vllm/model_executor/layers/fused_moe/layer.py
46	14	vllm/model_executor/layers/quantization/mxfp4.py

[ac6eb49de] Breno Baldas Skuk 2025-08-19 fix: OpenAI SDK compat (ResponseTextConfig) (#23126)
1	1	requirements/common.txt
1	1	requirements/docs.txt
9	1	vllm/entrypoints/openai/protocol.py

[bf756321c] Michael Goin 2025-08-18 [CI Bugfix] Pin `openai<1.100` to unblock CI (#23118)
1	1	requirements/common.txt
1	1	requirements/docs.txt

[0e3bb543f] Raushan Turganbay 2025-08-18 [Bugfix] Support compile for Transformers multimodal (#23095)
7	0	vllm/model_executor/models/transformers.py

[569aefd13] 杨朱 · Kiki 2025-08-18 chore: remove unnecessary patch_padding_side for the chatglm model (#23090)
0	2	tests/models/multimodal/generation/vlm_utils/model_utils.py
0	27	vllm/transformers_utils/tokenizer.py

[d3f71f122] Cyrus Leung 2025-08-18 [Refactor] Get prompt updates earlier (#23097)
3	3	vllm/model_executor/models/deepseek_vl2.py
4	4	vllm/model_executor/models/h2ovl.py
5	10	vllm/model_executor/models/pixtral.py
15	18	vllm/model_executor/models/qwen2_5_omni_thinker.py
5	6	vllm/model_executor/models/voxtral.py
52	28	vllm/multimodal/processing.py

[5a30bd10d] Ning Xie 2025-08-18 [Bugfix] fix IntermediateTensors equal method (#23027)
38	2	tests/test_sequence.py
7	1	vllm/sequence.py

[27e8d1ea3] Cyrus Leung 2025-08-18 [Refactor] Define MultiModalKwargsItems separate from MultiModalKwargs (#23053)
1	0	docs/api/README.md
2	2	docs/contributing/model/multimodal.md
10	4	tests/models/multimodal/processing/test_common.py
2	1	tests/models/multimodal/processing/test_glm4_1v.py
2	1	tests/models/multimodal/processing/test_h2ovl.py
2	1	tests/models/multimodal/processing/test_internvl.py
5	5	tests/models/multimodal/processing/test_llama4.py
3	3	tests/models/multimodal/processing/test_mllama.py
5	5	tests/models/multimodal/processing/test_mllama4.py
2	1	tests/models/multimodal/processing/test_nemotron_vl.py
2	1	tests/models/multimodal/processing/test_qwen2_vl.py
1	1	tests/models/multimodal/test_tensor_schema.py
6	5	tests/multimodal/test_cache.py
13	9	tests/v1/test_serial_utils.py
7	2	vllm/executor/msgspec_utils.py
2	2	vllm/model_executor/models/aria.py
2	2	vllm/model_executor/models/aya_vision.py
2	2	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/chameleon.py
2	2	vllm/model_executor/models/cohere2_vision.py
4	3	vllm/model_executor/models/deepseek_vl2.py
2	2	vllm/model_executor/models/florence2.py
2	2	vllm/model_executor/models/fuyu.py
2	2	vllm/model_executor/models/gemma3_mm.py
2	2	vllm/model_executor/models/gemma3n_mm.py
6	4	vllm/model_executor/models/glm4_1v.py
2	2	vllm/model_executor/models/glm4v.py
2	2	vllm/model_executor/models/granite_speech.py
9	7	vllm/model_executor/models/h2ovl.py
14	13	vllm/model_executor/models/hyperclovax_vision.py
2	2	vllm/model_executor/models/idefics3.py
7	6	vllm/model_executor/models/interns1.py
21	13	vllm/model_executor/models/internvl.py
4	3	vllm/model_executor/models/keye.py
2	2	vllm/model_executor/models/kimi_vl.py
3	3	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/llava_next_video.py
2	2	vllm/model_executor/models/llava_onevision.py
2	2	vllm/model_executor/models/minicpmo.py
2	2	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/mistral3.py
4	3	vllm/model_executor/models/mllama.py
4	8	vllm/model_executor/models/mllama4.py
2	2	vllm/model_executor/models/molmo.py
7	6	vllm/model_executor/models/nvlm_d.py
5	4	vllm/model_executor/models/ovis.py
2	2	vllm/model_executor/models/paligemma.py
2	2	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/phi4_multimodal.py
2	2	vllm/model_executor/models/phi4mm.py
4	3	vllm/model_executor/models/pixtral.py
4	3	vllm/model_executor/models/prithvi_geospatial_mae.py
8	7	vllm/model_executor/models/qwen2_5_omni_thinker.py
4	3	vllm/model_executor/models/qwen2_audio.py
4	3	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/model_executor/models/qwen_vl.py
7	6	vllm/model_executor/models/skyworkr1v.py
6	8	vllm/model_executor/models/step3_vl.py
2	2	vllm/model_executor/models/tarsier.py
3	3	vllm/model_executor/models/transformers.py
5	4	vllm/model_executor/models/ultravox.py
4	3	vllm/model_executor/models/voxtral.py
2	2	vllm/model_executor/models/whisper.py
3	1	vllm/multimodal/__init__.py
3	6	vllm/multimodal/base.py
13	8	vllm/multimodal/cache.py
72	100	vllm/multimodal/inputs.py
4	7	vllm/multimodal/parse.py
18	20	vllm/multimodal/processing.py
2	2	vllm/multimodal/profiling.py
16	9	vllm/multimodal/utils.py
2	4	vllm/sequence.py
1	1	vllm/v1/engine/processor.py
33	8	vllm/v1/serial_utils.py
6	4	vllm/v1/worker/gpu_input_batch.py
3	2	vllm/v1/worker/gpu_model_runner.py
3	2	vllm/v1/worker/tpu_model_runner.py

[5c79b0d64] Kunshang Ji 2025-08-18 [XPU][CI]add xpu env vars in CI scripts (#22946)
7	3	.buildkite/scripts/hardware_ci/run-xpu-test.sh

[5f5664b3e] Kunshang Ji 2025-08-18 [XPU] Fix compile size for xpu (#23069)
1	1	vllm/config/__init__.py

[89657a557] Roger Wang 2025-08-17 [Misc] Fix backward compatibility from #23030 (#23070)
6	3	vllm/multimodal/base.py
3	3	vllm/multimodal/inputs.py
3	1	vllm/sequence.py

[08d5f7113] Ning Xie 2025-08-18 [Misc] refactor function name (#23029)
1	1	vllm/platforms/cpu.py
1	1	vllm/v1/worker/cpu_worker.py

[b2fd0b81e] Andy Lo 2025-08-18 [Bugfix][CI] Machete kernels: deterministic ordering for more cache hits (#23055)
6	3	csrc/quantization/machete/generate.py

[9f1c64225] double7 2025-08-18 [Bugfix] fix Qwen2.5-Omni processor output mapping (#23058)
5	0	vllm/model_executor/models/qwen2_5_omni_thinker.py

[7be3a59d8] Ning Xie 2025-08-18 [Misc] enhance static type hint (#23059)
3	1	vllm/v1/worker/lora_model_runner_mixin.py

[8ea0c2753] Woosuk Kwon 2025-08-17 [Misc] Minor code cleanup for _get_prompt_logprobs_dict (#23064)
3	4	vllm/v1/worker/gpu_model_runner.py

[0fc8fa751] Simon Mo 2025-08-17 fix: gptq marlin weight loading failure (#23066)
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[21e39436c] Calvin Chen 2025-08-18 [XPU] fix xpu to set cudagraph batch sizes (#23044)
4	2	vllm/v1/worker/gpu_model_runner.py

[6d243efed] Woosuk Kwon 2025-08-17 [Misc] Convert use_structured_output property into constant (#23060)
4	7	vllm/v1/request.py

[c55bc1db2] Woosuk Kwon 2025-08-17 [Misc] Remove dead return (#23061)
0	1	vllm/model_executor/models/qwen2_vl.py

[292084e72] Lucas Wilkinson 2025-08-17 [BugFix] Fix for IMA in FA3 varlen combine (#22967)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[16bff144b] Kevinzz 2025-08-17 [Misc] fix typo in the multimodal doc (#23051)
1	1	docs/features/multimodal_inputs.md

[fe0411fc6] 947132885 2025-08-17 [Bugfix] should use stack instead of concat (#22972)
12	2	vllm/model_executor/models/transformers.py

[4d4061b6e] Jee Jee Li 2025-08-17 [Kernel] Add cuda kernel for gpt_oss activation (#22951)
59	0	csrc/activation_kernels.cu
2	0	csrc/ops.h
6	0	csrc/torch_bindings.cpp
39	6	tests/kernels/core/test_activation.py
38	3	vllm/model_executor/layers/activation.py
5	17	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
5	13	vllm/model_executor/layers/fused_moe/fused_moe.py
2	2	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	1	vllm/model_executor/models/gpt_oss.py

[87f48623a] Ning Xie 2025-08-17 [Misc] method name typo fix (#23042)
2	2	vllm/v1/worker/cpu_model_runner.py

[5c32143b9] Cyrus Leung 2025-08-17 [Refactor] Defer tensor data construction in MultiModalKwargs (#23030)
1	1	tests/multimodal/test_cache.py
1	33	tests/v1/test_serial_utils.py
1	1	vllm/inputs/registry.py
1	1	vllm/model_executor/models/prithvi_geospatial_mae.py
1	1	vllm/multimodal/base.py
1	1	vllm/multimodal/cache.py
54	42	vllm/multimodal/inputs.py
1	1	vllm/multimodal/processing.py
7	5	vllm/multimodal/utils.py
2	2	vllm/sequence.py
2	15	vllm/v1/serial_utils.py
1	1	vllm/v1/worker/gpu_input_batch.py

[94096a47c] Michael Goin 2025-08-16 [UX] Separate marlin moe config logic from triton moe (#23006)
6	14	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	8	vllm/model_executor/layers/fused_moe/fused_moe.py

[a258ad8bc] Jinzhen Lin 2025-08-17 [Bugfix] fix qwen3 moe fp8 accuracy issue (#23031)
4	0	vllm/model_executor/layers/quantization/fp8.py

[bf7f470b2] afeldman-nm 2025-08-16 [V1] Logits processors extensibility (#19912)
1	0	.buildkite/test-pipeline.yaml
147	0	examples/offline_inference/logits_processor.py
66	13	tests/utils.py
0	0	tests/v1/logits_processors/__init__.py
16	8	tests/v1/{sample/test_logits_processors.py => logits_processors/test_correctness.py}
237	0	tests/v1/logits_processors/test_custom_offline.py
180	0	tests/v1/logits_processors/test_custom_online.py
127	0	tests/v1/logits_processors/utils.py
2	2	tests/v1/sample/test_rejection_sampler.py
2	2	tests/v1/sample/test_sampler.py
2	2	tests/v1/worker/test_gpu_input_batch.py
5	0	vllm/config/__init__.py
8	0	vllm/engine/arg_utils.py
4	0	vllm/entrypoints/llm.py
1	1	vllm/utils/__init__.py
185	0	vllm/v1/sample/logits_processor/__init__.py
26	270	vllm/v1/sample/{logits_processor.py => logits_processor/builtin.py}
86	0	vllm/v1/sample/logits_processor/interface.py
149	0	vllm/v1/sample/logits_processor/state.py
2	2	vllm/v1/sample/metadata.py
58	33	vllm/v1/worker/gpu_input_batch.py
9	2	vllm/v1/worker/gpu_model_runner.py

[4fc722eca] Michael Goin 2025-08-16 [Kernel/Quant] Remove AQLM (#22943)
0	1	.buildkite/scripts/hardware_ci/run-amd-test.sh
0	1	CMakeLists.txt
0	345	benchmarks/kernels/benchmark_aqlm.py
0	9	csrc/ops.h
0	597	csrc/quantization/aqlm/gemm_kernels.cu
0	15	csrc/torch_bindings.cpp
0	1	docs/features/quantization/supported_hardware.md
0	1	docs/mkdocs/hooks/generate_examples.py
0	14	examples/offline_inference/basic/README.md
0	4	tests/compile/test_full_graph.py
0	40	tests/kernels/quantization/test_aqlm.py
0	68	tests/models/quantization/test_aqlm.py
0	41	vllm/_custom_ops.py
0	18	vllm/model_executor/layers/linear.py
0	3	vllm/model_executor/layers/quantization/__init__.py
0	376	vllm/model_executor/layers/quantization/aqlm.py

[3253ae765] Michael Goin 2025-08-16 [Flaky CI] Increase timeout tolerance for test_mp_crash_detection+test_default_mm_lora_chat_completions (#23028)
2	1	tests/entrypoints/openai/test_default_mm_loras.py
2	2	tests/mq_llm_engine/test_error_handling.py

[000cceca8] Michael Goin 2025-08-16 [Bugfix gpt-oss] Fix float32 convert for flashinfer sink support (#23016)
9	0	vllm/attention/layer.py
0	3	vllm/v1/attention/backends/flashinfer.py

[68373d312] Woonggi Min 2025-08-17 [Frontend] Added support for HermesToolParser for models without special tokens (#16890)
127	0	tests/entrypoints/openai/tool_parsers/test_hermes_tool_parser.py
64	17	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py

[52ce1420e] Maximilien de Bayser 2025-08-16 Fix handling of `max_num_batched_tokens` for pooling tasks (#23004)
0	3	vllm/config/__init__.py
5	5	vllm/engine/arg_utils.py

[829bbd788] 汪志鹏 2025-08-16 [New Model]mBART model (#22883)
4	0	docs/models/supported_models.md
147	86	examples/offline_inference/encoder_decoder.py
123	0	tests/models/language/generation/test_mbart.py
2	0	tests/models/registry.py
439	5	vllm/model_executor/models/bart.py
1	0	vllm/model_executor/models/registry.py

[4dff91c93] Cyrus Leung 2025-08-16 [Refactor] Allow optional MultiModalKwargsItem in IPC (#23022)
2	10	tests/v1/core/test_kv_cache_utils.py
2	10	tests/v1/core/test_prefix_caching.py
2	10	tests/v1/core/test_scheduler.py
2	10	tests/v1/core/utils.py
17	45	vllm/multimodal/inputs.py
2	1	vllm/v1/engine/__init__.py
18	15	vllm/v1/engine/mm_input_cache.py
7	3	vllm/v1/engine/processor.py
5	2	vllm/v1/request.py
2	2	vllm/v1/worker/gpu_model_runner.py

[de9cb6176] Seiji Eicher 2025-08-16 Add docs for PrefixRepetitionDataset + enable usage with `vllm bench throughput` (#23012)
21	1	benchmarks/README.md
52	5	vllm/benchmarks/throughput.py

[2dbccce8a] Isotr0py 2025-08-16 [CI][Bugfix] Skip Ovis2 generation test because of broken remote code (#22954)
11	4	tests/models/registry.py

[933f45334] Chengji Yao 2025-08-16 [Core] Make cudagraph check cuda platform only (#23005)
22	17	vllm/config/__init__.py

[cc826a202] Isotr0py 2025-08-16 [Multimodal] Update Tensor schema test to cover arbitrary shape mm inputs (#22867)
124	19	tests/models/multimodal/test_tensor_schema.py
14	8	vllm/model_executor/models/keye.py

[6d3da472b] Jee Jee Li 2025-08-16 [Misc] Add --save-dir option to benchmark_moe (#23020)
8	1	benchmarks/kernels/benchmark_moe.py

[78863f8c5] Andrew Sansom 2025-08-16 [BugFix] Add support for loading prompt embeds tensors serialized on unavailable devices and sparse tensors (#22962)
49	0	tests/entrypoints/openai/test_prompt_validation.py
4	2	vllm/entrypoints/openai/serving_engine.py

[5157827cf] Lucas Wilkinson 2025-08-16 [Build] Env var to disable sccache (#22968)
2	1	setup.py

[7caec10e7] Kunshang Ji 2025-08-16 [XPU]avoid circular import during XPU init (#23017)
2	1	vllm/platforms/xpu.py

[1f83e7d84] Grace Ho 2025-08-15 [misc] nsys profile output kernel classifier and visualizer (#22971)
175	0	tools/profiler/nsys_profile_tools/README.md
426	0	tools/profiler/nsys_profile_tools/gputrc2graph.py
-	-	tools/profiler/nsys_profile_tools/images/csv1.png
-	-	tools/profiler/nsys_profile_tools/images/html.png
-	-	tools/profiler/nsys_profile_tools/images/html_tbl.png

[e4e37ded5] Calvin Chen 2025-08-16 [V1] support min_tokens for detokener (#22014)
50	0	tests/detokenizer/test_min_tokens.py
8	3	vllm/v1/engine/detokenizer.py

[f6b504059] Nick Hill 2025-08-15 [Frontend] Avoid list copies in `serving_chat.py` (#22947)
15	14	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/reasoning/abs_reasoning_parsers.py

[fbd88728b] Benjamin Chislett 2025-08-15 [Bugfix] Fix DeepSeek MTP (#22934)
7	6	vllm/model_executor/models/deepseek_mtp.py
3	4	vllm/model_executor/models/glm4_moe_mtp.py
3	4	vllm/model_executor/models/mimo_mtp.py

[070da660c] Nicolò Lucchesi 2025-08-16 [Kernel] Simplify `get_kv_cache_layout` and cache `use_trtllm_attention` env-dependent bit (#22735)
31	15	vllm/utils/flashinfer.py
11	7	vllm/v1/attention/backends/utils.py

[ad0297d11] Nick Hill 2025-08-15 [Misc] Support passing multiple request ids at once to `AsyncLLM.abort()` (#22944)
76	1	tests/v1/engine/test_async_llm.py
4	1	vllm/engine/async_llm_engine.py
7	3	vllm/engine/multiprocessing/client.py
4	3	vllm/engine/protocol.py
5	0	vllm/utils/__init__.py
9	6	vllm/v1/engine/async_llm.py

[236b864e4] Yichen Yan 2025-08-16 [BugFix] Make `run_once` thread-safe (#22978)
8	4	vllm/utils/__init__.py

[3e2f7985a] Yong Hoon Shin 2025-08-15 Support multiple attention groups for KV sharing (#22672)
189	0	tests/v1/test_kv_sharing.py
24	16	vllm/v1/worker/utils.py

[c280066f9] Or Ozeri 2025-08-16 [v1] Move block_hashes from KVCacheManager to Request.block_hashes (#19728)
15	7	tests/v1/core/test_async_scheduler.py
27	23	tests/v1/core/test_kv_cache_utils.py
122	103	tests/v1/core/test_prefix_caching.py
16	13	tests/v1/core/test_scheduler.py
0	2	tests/v1/core/test_single_type_kv_cache_manager.py
16	1	tests/v1/core/utils.py
2	0	tests/v1/kv_connector/unit/test_nixl_connector.py
8	2	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
15	2	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
20	11	tests/v1/kv_connector/unit/utils.py
18	0	vllm/utils/__init__.py
17	58	vllm/v1/core/block_pool.py
12	21	vllm/v1/core/kv_cache_coordinator.py
4	47	vllm/v1/core/kv_cache_manager.py
50	30	vllm/v1/core/kv_cache_utils.py
0	2	vllm/v1/core/sched/scheduler.py
1	9	vllm/v1/core/single_type_kv_cache_manager.py
19	3	vllm/v1/engine/core.py
20	2	vllm/v1/request.py

[b9dc9d260] Nick Hill 2025-08-15 [BugFix] Handle case where async utility call is cancelled (#22996)
23	1	tests/v1/engine/test_engine_core_client.py
15	6	vllm/v1/engine/core_client.py

[1fc375dc0] rishitdholakia13 2025-08-15 [Structured Outputs] [Bug] Fix misalignment in apply_grammar_bitmask causing unintended masking and NaN logits (#22963)
2	2	vllm/v1/worker/gpu_model_runner.py

[76144adf7] Eli Uriegas 2025-08-15 ci: Add CUDA + arm64 release builds (#21201)
16	0	.buildkite/release-pipeline.yaml
2	15	docker/Dockerfile

[f5d412baf] Thomas Parnell 2025-08-16 [BugFix] Fix regression caused by mamba state dtype PR (#22998)
6	2	vllm/model_executor/models/phi4flash.py
6	2	vllm/model_executor/models/plamo2.py

[177e55e3b] Lucas Wilkinson 2025-08-15 [Attention] FA3 Attention Sinks Perf Boost (#22478)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[1723ef1aa] eigen 2025-08-15 minor: zero workspace buffer init for flashinfer trtllm-gen attn (#22603)
2	2	tests/kernels/attention/test_flashinfer_trtllm_attention.py
1	1	vllm/attention/backends/flashinfer.py
1	1	vllm/v1/attention/backends/flashinfer.py

[00d6cba0c] Seiji Eicher 2025-08-15 Add PrefixRepetitionRandomDataset to `vllm bench serve` datasets (#20638)
131	2	vllm/benchmarks/datasets.py

[7f89ed248] shixianc 2025-08-15 [Fix] enable swap_ab for pplx problem size computation (#22991)
32	13	csrc/quantization/cutlass_w8a8/moe/moe_data.cu

[8a87cd27d] Michael Goin 2025-08-15 [CI] Speed up Whisper tests by reusing server (#22859)
141	179	tests/entrypoints/openai/test_transcription_validation.py
122	112	tests/entrypoints/openai/test_translation_validation.py

[a344a1a7d] Michael Goin 2025-08-15 Use regex in convert-results-json-to-markdown.py (#22989)
1	1	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py

[79899b63f] nvjullin 2025-08-16 [Bugfix] Added more env vars to hash (#22449)
36	10	vllm/envs.py

[6e670778c] Zebing Lin 2025-08-15 [Core] direct indexing on self.block_table_np in compute_slot_mapping (#22940)
1	2	vllm/v1/worker/block_table.py

[df5afa82e] Wentao Ye 2025-08-15 [Log] Debug Once for Randomizing dummy data for DP Rank (#22860)
1	1	vllm/v1/worker/gpu_model_runner.py

[6cd69f51b] Chih-Chieh Yang 2025-08-15 [Model] Granite-4 support loading quantized checkpoint (#22925)
6	2	vllm/model_executor/models/granitemoehybrid.py

[8ad7285ea] bnellnm 2025-08-15 [Kernels] Clean up FusedMoeMethodBase and modular kernel setup.  Remove extra arguments from modular kernel methods. (#22035)
1	0	.buildkite/test-pipeline.yaml
9	1	docs/design/fused_moe_modular_kernel.md
22	1	examples/offline_inference/data_parallel.py
263	277	tests/kernels/moe/modular_kernel_tools/common.py
436	25	tests/kernels/moe/modular_kernel_tools/mk_objects.py
2	2	tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py
0	117	tests/kernels/moe/modular_kernel_tools/utils.py
2	2	tests/kernels/moe/test_batched_moe.py
17	14	tests/kernels/moe/test_block_fp8.py
8	7	tests/kernels/moe/test_block_int8.py
2	15	tests/kernels/moe/test_cutlass_grouped_gemm.py
4	2	tests/kernels/moe/test_deepep_deepgemm_moe.py
3	3	tests/kernels/moe/test_deepgemm.py
147	0	tests/kernels/moe/test_flashinfer_moe.py
88	53	tests/kernels/moe/test_modular_kernel_combinations.py
24	40	tests/kernels/moe/test_nvfp4_moe.py
4	7	tests/kernels/moe/test_pplx_cutlass_moe.py
2	2	tests/kernels/moe/test_pplx_moe.py
56	19	tests/kernels/moe/utils.py
4	3	vllm/distributed/device_communicators/base_device_communicator.py
3	1	vllm/model_executor/layers/fused_moe/__init__.py
23	13	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
24	14	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
6	5	vllm/model_executor/layers/fused_moe/config.py
193	135	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	2	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
19	11	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
20	12	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
23	36	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
24	28	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
63	35	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
3	4	vllm/model_executor/layers/fused_moe/fused_moe.py
7	8	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
50	41	vllm/model_executor/layers/fused_moe/layer.py
59	58	vllm/model_executor/layers/fused_moe/modular_kernel.py
21	12	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
18	25	vllm/model_executor/layers/fused_moe/prepare_finalize.py
23	14	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
2	16	vllm/model_executor/layers/fused_moe/utils.py
2	2	vllm/model_executor/layers/quantization/auto_round.py
1	1	vllm/model_executor/layers/quantization/awq.py
13	5	vllm/model_executor/layers/quantization/awq_marlin.py
9	3	vllm/model_executor/layers/quantization/bitsandbytes.py
112	56	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
13	4	vllm/model_executor/layers/quantization/experts_int8.py
28	15	vllm/model_executor/layers/quantization/fp8.py
12	3	vllm/model_executor/layers/quantization/gguf.py
11	3	vllm/model_executor/layers/quantization/gptq_marlin.py
66	33	vllm/model_executor/layers/quantization/modelopt.py
12	4	vllm/model_executor/layers/quantization/moe_wna16.py
1	1	vllm/model_executor/layers/quantization/mxfp4.py
30	9	vllm/model_executor/layers/quantization/quark/quark_moe.py
9	4	vllm/model_executor/layers/quantization/rtn.py
27	102	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py

[48b01fd4d] Shanshan Shen 2025-08-16 [Structured Output] Make the output of structured output example more complete (#22481)
13	3	examples/offline_inference/structured_outputs.py

[993d3d122] Chenheli Hua 2025-08-15 [Benchmarks] Include image data when ShareGPT4V dataset is used. (#22955)
49	0	benchmarks/README.md
7	1	benchmarks/benchmark_dataset.py
7	1	vllm/benchmarks/datasets.py

[68af77e51] JartX 2025-08-15 [FIXBUG] Correctly Apply Grammar Bitmask in Mixed Batches (#22896)
4	3	vllm/v1/worker/gpu_model_runner.py

[6b04039a7] sstamenk 2025-08-15 [BugFix] Skip the Q component for QKVParallelLinear in the case of QKVCrossParallelLinear since its width is 0 (#22369)
3	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[1c859a138] Woosuk Kwon 2025-08-15 [V0 Deprecation] Remove advance_step (#22969)
0	1	CMakeLists.txt
0	16	csrc/ops.h
0	336	csrc/prepare_inputs/advance_step.cu
0	19	csrc/prepare_inputs/advance_step.cuh
0	19	csrc/torch_bindings.cpp
0	32	vllm/_custom_ops.py
0	5	vllm/attention/backends/abstract.py
1	75	vllm/attention/backends/differential_flash_attn.py
1	75	vllm/attention/backends/flash_attn.py
2	63	vllm/attention/backends/flashinfer.py
1	14	vllm/attention/backends/flashmla.py
1	86	vllm/attention/backends/mla/common.py
1	61	vllm/attention/backends/placeholder_attn.py
0	21	vllm/attention/backends/rocm_aiter_mla.py
1	67	vllm/attention/backends/rocm_flash_attn.py
1	2	vllm/worker/model_runner.py

[74f441f4b] fhl2000 2025-08-15 [Core] Allow full cudagraph with separate attention routines and orthogonal to compilation, add support for FA2 and FlashInfer (#20059)
125	128	tests/compile/piecewise/test_full_cudagraph.py
25	8	tests/compile/piecewise/test_simple.py
29	7	tests/compile/piecewise/test_toy_llama.py
0	0	tests/v1/cudagraph/__init__.py
406	0	tests/v1/cudagraph/test_cudagraph_dispatch.py
187	0	tests/v1/cudagraph/test_cudagraph_mode.py
30	12	vllm/compilation/backends.py
0	72	vllm/compilation/base_piecewise_backend.py
54	0	vllm/compilation/base_static_graph.py
193	0	vllm/compilation/cuda_graph.py
16	117	vllm/compilation/cuda_piecewise_backend.py
18	0	vllm/compilation/monitor.py
4	3	vllm/compilation/wrapper.py
40	12	vllm/config/__init__.py
162	26	vllm/config/compilation.py
41	11	vllm/forward_context.py
8	5	vllm/platforms/cuda.py
15	4	vllm/platforms/interface.py
2	2	vllm/platforms/rocm.py
10	2	vllm/platforms/tpu.py
12	10	vllm/platforms/xpu.py
37	31	vllm/v1/attention/backends/flash_attn.py
5	8	vllm/v1/attention/backends/flashinfer.py
2	6	vllm/v1/attention/backends/mamba2_attn.py
1	5	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/cutlass_mla.py
7	4	vllm/v1/attention/backends/mla/flashmla.py
9	4	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
0	5	vllm/v1/attention/backends/rocm_aiter_fa.py
1	7	vllm/v1/attention/backends/triton_attn.py
10	14	vllm/v1/attention/backends/utils.py
120	0	vllm/v1/cudagraph_dispatcher.py
270	89	vllm/v1/worker/gpu_model_runner.py
0	5	vllm/v1/worker/gpu_worker.py

[a0632a3e0] Csrayz 2025-08-15 [Frontend] Expose do_log_stats interval to env (#22905)
1	0	docs/usage/troubleshooting.md
1	1	vllm/entrypoints/openai/api_server.py
7	0	vllm/envs.py

[e8b40c7fa] Harry Mellor 2025-08-15 [CI] Remove duplicated docs build from buildkite (#22924)
0	10	.buildkite/test-pipeline.yaml
4	7	docker/Dockerfile
1	1	tests/standalone_tests/python_only_compile.sh

[48f463692] Jee Jee Li 2025-08-15 [Misc] Ignore ep_kernels_workspace (#22807)
3	0	.gitignore

[75531a6c1] Thomas Parnell 2025-08-15 [V1] [Hybrid] Support using float32 for state in Hybrid Models (Mamba2, Mamba1, Minimax) (#22928)
62	0	tests/models/language/generation/test_hybrid.py
2	0	tests/v1/worker/test_gpu_model_runner.py
1	1	vllm/config/__init__.py
12	0	vllm/config/cache.py
14	6	vllm/engine/arg_utils.py
15	2	vllm/model_executor/layers/mamba/mamba_mixer.py
31	20	vllm/model_executor/layers/mamba/mamba_mixer2.py
52	0	vllm/model_executor/layers/mamba/mamba_utils.py
7	3	vllm/model_executor/layers/mamba/ops/ssd_combined.py
25	4	vllm/model_executor/models/bamba.py
1	1	vllm/model_executor/models/config.py
25	4	vllm/model_executor/models/falcon_h1.py
26	4	vllm/model_executor/models/granitemoehybrid.py
24	4	vllm/model_executor/models/jamba.py
23	4	vllm/model_executor/models/mamba.py
30	6	vllm/model_executor/models/mamba2.py
10	5	vllm/model_executor/models/mamba_cache.py
31	3	vllm/model_executor/models/minimax_text_01.py
28	4	vllm/model_executor/models/nemotron_h.py
33	5	vllm/model_executor/models/zamba2.py
1	0	vllm/utils/__init__.py
4	3	vllm/v1/kv_cache_interface.py
10	8	vllm/v1/worker/gpu_model_runner.py

[22341b996] Staszek Paśko 2025-08-15 Improve multimodal hasher performance for re-used Image prompts (#22825)
20	0	tests/multimodal/test_hasher.py
6	0	vllm/multimodal/hasher.py

[49252cf59] Roger Wang 2025-08-15 [MM] Allow skipping memory profiling for multimodal models. (#22950)
16	1	vllm/config/__init__.py
4	0	vllm/engine/arg_utils.py
45	39	vllm/v1/worker/gpu_model_runner.py
56	50	vllm/v1/worker/tpu_model_runner.py

[3e6dd4001] Jinzhen Lin 2025-08-15 [Bugfix] fix cuda 12.6 and 11.8 build (#22952)
12	6	CMakeLists.txt

[aa300c438] Sayandip Dutta 2025-08-15 [Bugfix] Unquote file uri before reading image (#22912)
26	0	tests/multimodal/test_utils.py
2	1	vllm/multimodal/utils.py

[fe91ce959] amirai21 2025-08-15 [V1] - Split Prefill and Decode for Mamba1 models (#22653)
13	0	tests/models/language/generation/test_hybrid.py
219	90	vllm/model_executor/layers/mamba/mamba_mixer.py
21	5	vllm/v1/attention/backends/mamba1_attn.py

[5406ebf5c] wang.yuqi 2025-08-15 [CI] Pooling models mteb test uses enforce_eager (#22878)
5	1	tests/models/language/pooling/mteb_utils.py

[b2c06509e] frankie 2025-08-15 [P/D]Provide bucket algorithm rate limiter  for proxy_server (#22643)
188	52	benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
45	0	benchmarks/disagg_benchmarks/rate_limiter.py
39	0	benchmarks/disagg_benchmarks/request_queue.py

[b2f6c247a] TJian 2025-08-14 Revert "[ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module." (#22956)
0	71	vllm/model_executor/layers/rotary_embedding/base.py
2	2	vllm/model_executor/layers/rotary_embedding/common.py
8	4	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
0	127	vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py

[3d232dbd1] Asaf Joseph Gardin 2025-08-15 [Mamba] - refactor: Renamed mamba_attn to mamba2_attn (#22818)
1	1	tests/kernels/mamba/test_mamba_ssm_ssd.py
1	1	tests/v1/attention/test_mamba_selectors.py
1	1	vllm/model_executor/layers/mamba/mamba2_metadata.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	0	vllm/v1/attention/backends/{mamba_attn.py => mamba2_attn.py}
1	1	vllm/v1/attention/backends/mamba_selectors.py

[5c3fbfe46] Wentao Ye 2025-08-15 [Feature] Full Cuda Graph Support for Cutlass MLA and 6% E2E Throughput Improvement (#22763)
74	0	tests/compile/piecewise/test_full_cudagraph.py
14	2	vllm/v1/attention/backends/mla/cutlass_mla.py

[b4cef5e6c] amirkl94 2025-08-15 refactor: Change scaling factors calculation for flashinfer FusedMoE (#22812)
12	17	vllm/model_executor/layers/fused_moe/fused_moe.py
3	2	vllm/model_executor/layers/quantization/fp8.py
3	2	vllm/model_executor/layers/quantization/modelopt.py
42	4	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py

[0fe85087a] Michael Goin 2025-08-14 [CI Perf] Prune tests in `tests/kernels/attention/` (#22936)
3	3	tests/kernels/attention/test_aiter_flash_attn.py
2	5	tests/kernels/attention/test_attention.py
2	2	tests/kernels/attention/test_cache.py
9	7	tests/kernels/attention/test_flash_attn.py
13	11	tests/kernels/attention/test_flashinfer.py
3	3	tests/kernels/attention/test_flashinfer_trtllm_attention.py
3	3	tests/kernels/attention/test_prefix_prefill.py
4	4	tests/kernels/attention/test_triton_unified_attention.py

[d2b0e97ea] Michael Goin 2025-08-14 [CI Perf] Prune tests in `tests/kernels/moe/` (#22939)
5	8	tests/kernels/moe/test_batched_moe.py
2	3	tests/kernels/moe/test_count_expert_num_tokens.py
23	10	tests/kernels/moe/test_moe.py
3	3	tests/kernels/moe/test_moe_align_block_size.py
4	4	tests/kernels/moe/test_moe_permute_unpermute.py
9	3	tests/kernels/moe/test_pplx_moe.py

[590bddbfc] Michael Goin 2025-08-14 [CI Perf] Prune tests in `tests/kernels/quantization/` (#22942)
3	5	tests/kernels/quantization/test_fp8_quant.py
3	4	tests/kernels/quantization/test_int8_quant.py
0	4	tests/kernels/quantization/test_machete_mm.py
0	4	tests/kernels/quantization/test_marlin_gemm.py
48	12	tests/kernels/quantization/test_rocm_skinny_gemms.py
12	4	tests/kernels/quantization/test_triton_scaled_mm.py

[ae05a6d83] Nick Hill 2025-08-14 [BugFix] Fix port lookup in internal DP LB tests (#22252)
32	22	tests/v1/test_internal_lb_dp.py

[0933f9d51] Nick Hill 2025-08-14 [BugFix][KVConn] Fix use of `get_required_kvcache_layout` (#22734)
4	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
3	2	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[f1f0d2fab] Simon Mo 2025-08-14 Revert "[Kernel]  Add cuda kernel for gpt_oss activation" (#22948)
0	59	csrc/activation_kernels.cu
0	2	csrc/ops.h
0	5	csrc/torch_bindings.cpp
6	39	tests/kernels/core/test_activation.py
3	38	vllm/model_executor/layers/activation.py
13	5	vllm/model_executor/layers/fused_moe/fused_moe.py
1	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	1	vllm/model_executor/models/gpt_oss.py

[81f4b9648] Jee Jee Li 2025-08-15 [Kernel]  Add cuda kernel for gpt_oss activation (#22538)
59	0	csrc/activation_kernels.cu
2	0	csrc/ops.h
5	0	csrc/torch_bindings.cpp
39	6	tests/kernels/core/test_activation.py
38	3	vllm/model_executor/layers/activation.py
5	13	vllm/model_executor/layers/fused_moe/fused_moe.py
1	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	1	vllm/model_executor/models/gpt_oss.py

[39cd09dc8] Yongye Zhu 2025-08-14 [Bugfix] use flash attn on sm90 (#22933)
1	1	vllm/platforms/cuda.py

[919234fe1] Nick Hill 2025-08-14 [BugFix] Fix initial DP request load imbalance (#22910)
2	4	vllm/v1/engine/core_client.py

[ebcce2cd3] Nick Hill 2025-08-14 [Core] Return final response for aborted requests from `AsyncLLM.generate` (#22283)
87	0	tests/v1/engine/test_async_llm.py
20	13	vllm/v1/engine/output_processor.py

[4121de512] Dipika Sikka 2025-08-14 [Quantization]: Support compressed-tensors mixed-precision model loading (#22468)
20	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[279a5f31b] nvjullin 2025-08-15 [Kernel] Add nvfp4 gemm flashinfer backends (#22346)
1	0	.buildkite/test-pipeline.yaml
139	0	tests/kernels/quantization/test_flashinfer_nvfp4_scaled_mm.py
3	0	tests/kernels/quantization/test_nvfp4_scaled_mm.py
7	0	vllm/envs.py
49	15	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
61	23	vllm/model_executor/layers/quantization/modelopt.py
38	1	vllm/model_executor/warmup/kernel_warmup.py
71	0	vllm/utils/flashinfer.py
2	2	vllm/v1/worker/gpu_worker.py

[b8ff05361] Lucas Wilkinson 2025-08-14 [CI] Temporarily disable flaky test  (#22930)
6	0	tests/v1/e2e/test_spec_decode.py

[637093ae2] Nir 2025-08-14 docs: update fastsafetensors usage instructions (#22891)
2	1	docs/models/extensions/fastsafetensor.md

[33c63e954] Jinzhen Lin 2025-08-15 [Kernel] [Quantization] Add MXFP4 and bias support for marlin kernel (#22428)
7	0	CMakeLists.txt
1	0	benchmarks/kernels/benchmark_machete.py
2	0	csrc/core/scalar_type.hpp
15	0	csrc/moe/marlin_moe_wna16/generate_kernels.py
14	12	csrc/moe/marlin_moe_wna16/kernel.h
111	26	csrc/moe/marlin_moe_wna16/marlin_template.h
120	61	csrc/moe/marlin_moe_wna16/ops.cu
2	1	csrc/moe/torch_bindings.cpp
19	4	csrc/quantization/gptq_marlin/dequant.h
16	1	csrc/quantization/gptq_marlin/generate_kernels.py
113	49	csrc/quantization/gptq_marlin/gptq_marlin.cu
4	1	csrc/quantization/gptq_marlin/kernel.h
109	30	csrc/quantization/gptq_marlin/marlin_template.h
1	0	csrc/torch_bindings.cpp
141	34	tests/kernels/moe/test_moe.py
74	21	tests/kernels/quantization/test_marlin_gemm.py
20	1	tests/kernels/utils.py
11	7	vllm/_custom_ops.py
11	0	vllm/envs.py
28	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
6	14	vllm/model_executor/layers/fused_moe/layer.py
12	1	vllm/model_executor/layers/quantization/awq_marlin.py
6	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	0	vllm/model_executor/layers/quantization/fp8.py
9	1	vllm/model_executor/layers/quantization/gptq_marlin.py
5	4	vllm/model_executor/layers/quantization/hqq_marlin.py
6	2	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
2	0	vllm/model_executor/layers/quantization/modelopt.py
81	10	vllm/model_executor/layers/quantization/mxfp4.py
9	6	vllm/model_executor/layers/quantization/utils/marlin_utils.py
141	28	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
25	5	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
1	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
2	0	vllm/scalar_type.py

[ab9f2cfd1] Thomas Parnell 2025-08-14 [CI] [Hybrid]  Bump min transformers version for Bamba and Jamba (#22908)
2	2	tests/models/registry.py

[dbe298046] Cyrus Leung 2025-08-14 [Bugfix] Fix parsing of `--disable-mm-preprocessor-cache` (#22909)
1	1	vllm/engine/arg_utils.py

[625ccd1c4] Jiangyun Zhu 2025-08-14 [Bugfix] Replace custom Encoding class with BatchEncoding in MistralTokenizer (#22786)
2	7	vllm/transformers_utils/tokenizers/mistral.py

[92ff41abe] Jee Jee Li 2025-08-14 [Model] Modify the gate implementation of glm4_moe (#22832)
1	1	docs/models/supported_models.md
10	10	vllm/model_executor/models/glm4_moe.py

[829b9a62d] Lucas Wilkinson 2025-08-14 [Perf] Dont create unnecessary pooling params (#22876)
3	3	vllm/v1/worker/gpu_model_runner.py

[540d54ca8] Nicolò Lucchesi 2025-08-14 [CI] Re-enable transcriptions `test_long_audio_request` (#22890)
0	3	tests/entrypoints/openai/test_transcription_validation.py

[0783f1396] Daniele 2025-08-14 [Doc] fix dead link (#22898)
1	1	docs/getting_started/installation/README.md

[7655dc3e4] iAmir97 2025-08-14 [Bugfix] Add reset prefix cache for online serving (#22726)
1	0	vllm/engine/async_llm_engine.py
1	0	vllm/v1/engine/async_llm.py

[f4efda821] Harry Mellor 2025-08-14 Remove Phi 4 Flash configuration workaround (#22723)
0	17	vllm/transformers_utils/config.py

[eb08487b1] Nick Hill 2025-08-14 [BugFix] Threadsafe close async zmq sockets (#22877)
22	2	vllm/utils/__init__.py
55	24	vllm/v1/engine/core_client.py

[7c3a0741c] Isotr0py 2025-08-14 [Bugfix] Fix `PixtralHFImagePixelInputs` dynamic shape check (#22827)
1	1	tests/models/multimodal/test_tensor_schema.py
3	2	vllm/model_executor/models/llava.py

[00e3f9da4] Louie Tsai 2025-08-14  vLLM Benchmark suite improvement (#22119)
12	20	.buildkite/nightly-benchmarks/README.md
162	13	.buildkite/nightly-benchmarks/scripts/compare-json-results.py
153	10	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
55	38	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
2	2	.buildkite/nightly-benchmarks/tests/latency-tests-cpu.json
24	25	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc2.json
26	26	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc3.json
15	15	.buildkite/nightly-benchmarks/tests/serving-tests-cpu.json
2	2	.buildkite/nightly-benchmarks/tests/throughput-tests-cpu.json
1	1	docs/contributing/benchmarks.md

[a353bd083] Robert Shaw 2025-08-14 [CI] remove flaky v0 test (#22864)
1	12	tests/entrypoints/openai/test_default_mm_loras.py

[1d20c3471] Ilya Markov 2025-08-14 [CI] Fix `tests/distributed/test_ca_buffer_sharing.py` (#22849)
1	1	vllm/distributed/device_communicators/custom_all_reduce.py

[b6af24fba] Will Eaton 2025-08-13 [CI][Entrypoints]: add filter to generation to filter out invalid tool calls (#22826)
32	16	tests/entrypoints/openai/test_openai_schema.py

[0ca2393b4] Cyrus Leung 2025-08-14 [CI/Build] Increase pooling tolerance to pass CI (#22844)
1	1	tests/models/language/pooling/test_intfloat.py
1	1	tests/models/language/pooling/test_snowflake_arctic_embed.py

[31a500c86] Jialin Ouyang 2025-08-13 [Core] [N-gram SD Optimization][1/n] Propose tokens with a single KMP (#22437)
74	0	benchmarks/benchmark_block_pool.py
112	0	benchmarks/benchmark_ngram_proposer.py
53	2	benchmarks/benchmark_utils.py
0	108	benchmarks/kv_cache/benchmark_block_pool.py
65	37	tests/v1/spec_decode/test_ngram.py
85	60	vllm/v1/spec_decode/ngram_proposer.py

[4e8614e88] Luka Govedič 2025-08-13 Move checklist in PR template (#22852)
12	8	.github/PULL_REQUEST_TEMPLATE.md
4	4	.github/scripts/cleanup_pr_body.sh

[c6cd5ca3d] kliuae 2025-08-14 [ROCm][Bugfix] Fix compilation error in topk softmax fused kernel (#22819)
19	2	csrc/moe/topk_softmax_kernels.cu

[df0e0f023] Isotr0py 2025-08-14 [CI/Build] Skip gpt_big model test because of broken HF model (#22848)
2	1	tests/models/registry.py

[b4b78d631] Cyrus Leung 2025-08-14 [CI/Build] Fix param mismatch in `test_eagle_correctness` (#22847)
5	1	tests/v1/e2e/test_spec_decode.py

[12817a8ac] Nicolò Lucchesi 2025-08-13 [CI] Fix `tests/v1/e2e/test_kv_sharing_fast_prefill.py` import on test (#22815)
7	5	tests/v1/e2e/test_kv_sharing_fast_prefill.py

[c9232d41f] Cyrus Leung 2025-08-14 [CI/Build] Update VLM common tests (#22841)
1	15	tests/models/multimodal/generation/test_common.py
1	18	vllm/model_executor/models/minicpmv.py

[9bd9294f0] HWH 2025-08-14 [Bugfix] Fix MiniCPMV Image input inference failed (#22813)
17	0	vllm/model_executor/models/minicpmv.py
43	27	vllm/utils/tensor_schema.py

[da2705198] Roger Wang 2025-08-13 [Misc] clear and separate error messages for input too long and input + max-tokens too long (#22803)
18	13	vllm/entrypoints/openai/serving_engine.py

[19b927e52] Cyrus Leung 2025-08-13 [Core] Use individual MM items in P0/P1 cache and model runner (#22570)
79	154	tests/multimodal/test_utils.py
30	18	tests/v1/core/test_kv_cache_utils.py
21	10	tests/v1/core/test_prefix_caching.py
15	6	tests/v1/core/test_scheduler.py
14	5	tests/v1/core/utils.py
1	1	tests/v1/engine/test_engine_core.py
1	1	tests/v1/engine/test_engine_core_client.py
5	5	tests/v1/engine/test_output_processor.py
1	1	tests/v1/kv_connector/unit/utils.py
1	1	tests/v1/tpu/worker/test_tpu_model_runner.py
1	1	tests/v1/worker/test_gpu_input_batch.py
1	1	tests/v1/worker/test_gpu_model_runner.py
120	21	vllm/multimodal/inputs.py
65	68	vllm/multimodal/utils.py
5	5	vllm/v1/core/sched/output.py
2	4	vllm/v1/engine/__init__.py
4	3	vllm/v1/engine/core.py
37	41	vllm/v1/engine/mm_input_cache.py
25	41	vllm/v1/engine/processor.py
10	11	vllm/v1/request.py
34	14	vllm/v1/serial_utils.py
11	2	vllm/v1/worker/gpu_input_batch.py
48	49	vllm/v1/worker/gpu_model_runner.py
17	22	vllm/v1/worker/tpu_model_runner.py

[20d65aa75] milesial 2025-08-13 [Frontend] Multithreaded async multimodal load_bytes (#22710)
7	0	vllm/envs.py
20	6	vllm/multimodal/utils.py

[b159c0a67] Gh0u1L5 2025-08-13 Fix GGUF loader for Qwen3 MoE. (#22785)
11	0	vllm/model_executor/model_loader/gguf_loader.py
1	0	vllm/model_executor/models/qwen3_moe.py

[6772bb0f7] Yuanyuan Chen 2025-08-13 Remove unnecessary CUDA sync of qwen image and video preprocess (#22792)
8	4	vllm/model_executor/models/qwen2_5_vl.py

[fceafaf58] Chen Zhang 2025-08-13 [Bugfix][mamba] Fix type annotation of Mamba2Metadata (#22787)
4	4	vllm/model_executor/layers/mamba/mamba_mixer2.py
22	17	vllm/v1/attention/backends/mamba_attn.py

[6b794c756] Nicolò Lucchesi 2025-08-13 [Nixl][CI] Fix tests (#22806)
3	0	tests/v1/kv_connector/unit/test_nixl_connector.py

[98deac387] Chi Zhang 2025-08-13 [FEATURE] support custom vllm tuned config path for fused moe triton kernels (#22791)
6	0	vllm/envs.py
20	8	vllm/model_executor/layers/fused_moe/fused_moe.py

[653124bd4] Kdump 2025-08-13 [Frontend] Add chunked processing to handle long inputs in embedding models (#22280)
186	0	examples/online_serving/openai_embedding_long_text/README.md
366	0	examples/online_serving/openai_embedding_long_text/client.py
137	0	examples/online_serving/openai_embedding_long_text/service.sh
441	0	tests/entrypoints/openai/test_embedding_long_text.py
19	0	vllm/config/__init__.py
454	3	vllm/entrypoints/openai/serving_embedding.py

[0b1bdac6a] wangxiyuan 2025-08-13 [Platform] Custom ops support for FusedMoe (#22509)
2	1	vllm/model_executor/layers/fused_moe/layer.py
6	6	vllm/model_executor/layers/linear.py
3	1	vllm/model_executor/layers/vocab_parallel_embedding.py

[d94e3026d] Giancarlo Delfin 2025-08-13 [V1] Add tree drafting tests for eagle spec decoding (#22705)
157	3	tests/v1/spec_decode/test_eagle.py
0	6	tests/v1/spec_decode/test_max_len.py
3	3	vllm/v1/attention/backends/tree_attn.py
18	43	vllm/v1/spec_decode/eagle.py

[3f52738dc] 633WHU 2025-08-13 [Doc] Add max_lora_rank configuration guide (#22782)
19	0	docs/features/lora.md

[a01e0018b] Duc-Viet Hoang 2025-08-13 [Bugfix] Fix Nemotron VL image processing (#22739)
4	4	tests/models/multimodal/processing/test_nemotron_vl.py
186	0	vllm/model_executor/models/nemotron_vl.py

[9e7e5baaa] Yuxuan Zhang 2025-08-13 [Model] Add missing prefix to glm4_1v (#22716)
7	1	vllm/model_executor/models/glm4_1v.py

[d16aa3dae] zzh142857 2025-08-13 [Model] Add option to run Step3VisionEncoder in DP (#22697)
91	41	vllm/model_executor/models/step3_vl.py

[6807af8f4] Chen Zhang 2025-08-12 [gpt-oss] upgrade gpt-oss to v0.0.3 and add version check (#22768)
34	17	vllm/entrypoints/tool.py

[4c558cf62] shixianc 2025-08-12 [Perf] Support topk softmax fused kernel for broader num_experts (#22211)
45	32	csrc/moe/topk_softmax_kernels.cu
1	1	tests/kernels/moe/test_moe.py

[77a6bf07a] Wentao Ye 2025-08-13 [Bug] Fix Unexpected Keyword Argument 'w1_bias' (#22757)
13	3	vllm/model_executor/layers/fused_moe/layer.py

[4082338a2] Michael Goin 2025-08-13 Remove unneeded ROCm platform import when using CUDA (#22765)
1	1	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/attention/ops/chunked_prefill_paged_decode.py

[c6b928798] Michael Goin 2025-08-13 Force TRTLLM attention for gpt-oss on SM100 (#22678)
1	4	vllm/model_executor/models/gpt_oss.py
8	0	vllm/utils/flashinfer.py
7	4	vllm/v1/attention/backends/flashinfer.py
4	1	vllm/v1/attention/backends/utils.py

[b1361c727] Michael Goin 2025-08-13 [Bugfix] Fix default enable for CUTLASS MLA on SM100 (#22738)
6	1	vllm/platforms/cuda.py

[4f0f844b1] Po-Han Huang (NVIDIA) 2025-08-13 Fix cuda illegal mem access with Llama4 TP8 + rms_norm custom op (#22701)
6	2	vllm/model_executor/models/llama4.py

[c5830381a] Woosuk Kwon 2025-08-12 [V0 Deprecation] Remove args for multi-step scheduling (#22779)
0	1	tests/utils_/test_utils.py
1	26	vllm/config/scheduler.py

[d31f97cf5] Woosuk Kwon 2025-08-12 [Misc] Remove tests/multi_step/__init__.py (#22778)
0	0	tests/multi_step/__init__.py

[71683ca6f] Woosuk Kwon 2025-08-12 [V0 Deprecation] Remove multi-step scheduling (#22138)
0	1	.buildkite/nightly-benchmarks/tests/genai-perf-tests.json
0	6	.buildkite/nightly-benchmarks/tests/nightly-tests.json
0	22	.buildkite/test-pipeline.yaml
0	1	.github/CODEOWNERS
0	409	tests/async_engine/test_async_llm_engine.py
0	1	tests/config/test_config.yaml
0	1	tests/config/test_config_with_model.yaml
3	7	tests/core/test_chunked_prefill_scheduler.py
4	20	tests/core/test_num_computed_tokens_update.py
0	274	tests/engine/test_multi_step_output_processor.py
0	3	tests/entrypoints/openai/correctness/test_lmeval.py
0	39	tests/metrics/test_metrics.py
0	26	tests/models/language/generation/test_hybrid.py
0	232	tests/multi_step/test_correctness_async_llm.py
0	383	tests/multi_step/test_correctness_llm.py
0	70	tests/samplers/test_logits_processor.py
0	1	tests/tpu/lora/test_lora.py
0	2	tests/utils_/test_utils.py
0	6	tests/v1/test_oracle.py
0	79	tests/worker/test_model_input.py
0	2	vllm/config/__init__.py
3	89	vllm/core/scheduler.py
7	36	vllm/engine/arg_utils.py
2	24	vllm/engine/async_llm_engine.py
16	162	vllm/engine/llm_engine.py
6	20	vllm/engine/output_processor/interfaces.py
0	211	vllm/engine/output_processor/multi_step.py
2	12	vllm/platforms/cuda.py
2	12	vllm/platforms/rocm.py
1	6	vllm/platforms/tpu.py
0	38	vllm/sequence.py
2	5	vllm/worker/model_runner.py
0	908	vllm/worker/multi_step_model_runner.py
0	84	vllm/worker/multi_step_neuron_model_runner.py
0	63	vllm/worker/multi_step_neuronx_distributed_model_runner.py
0	197	vllm/worker/multi_step_worker.py
9	13	vllm/worker/neuron_worker.py

[e18859298] Michael Goin 2025-08-12 Add hardware plugins to installation doc (#22732)
13	0	docs/getting_started/installation/README.md

[fde0b611a] Jee Jee Li 2025-08-13 [Model] Decouple glm4v (#22751)
1	1	docs/models/supported_models.md
21	5	vllm/model_executor/models/glm4_1v.py
1	1	vllm/model_executor/models/registry.py

[d0a630158] Harry Mellor 2025-08-13 Fix Transformers backend tensor parallel for multimodal models (#22673)
34	17	vllm/model_executor/models/transformers.py

[45c3936e9] Harry Mellor 2025-08-13 [Docs] Hide the navigation and toc sidebars on home page (#22749)
6	0	docs/README.md

[ba81acbdc] Frank Wang 2025-08-12 [Bugfix] Bump DeepGEMM Version to Fix SMXX Layout Issues (#22606)
1	1	docker/Dockerfile

[53c730286] RUTHLESS-BOT 2025-08-13 [Misc] parametrize 'dtype' in test_flash_mla (#22641)
3	4	tests/kernels/attention/test_flashmla.py

[6534d2fc9] zifeitong 2025-08-12 Fix torch version check for SM100 mxfp4  (#22535)
8	6	vllm/model_executor/layers/fused_moe/layer.py

[422f22e01] Nicolò Lucchesi 2025-08-12 [CI][Nixl] Check kv cache layout during handshake (#22745)
46	0	tests/v1/kv_connector/unit/test_nixl_connector.py
10	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[6bd8ebf02] Xiaozhu Meng 2025-08-12 [Kernel][AMD] Avoid D2H copy and cumsum kernel (#22683)
20	12	vllm/v1/attention/backends/rocm_aiter_fa.py

[dab4f9f76] Wentao Ye 2025-08-12 [Chore] Update CODEOWNERS to include @yewentao256 for CUDA kernels, attention backends, quantization, and related tests (#22741)
5	5	.github/CODEOWNERS

[c42fe0b63] TeeKen Lau 2025-08-13 Add more test scenario for tensor schema (#22733)
25	0	tests/utils_/test_tensor_schema.py

[5a4b4b372] Rahul Tuli 2025-08-12 Add: `SupportsEagle3` interface for explicit EAGLE3 support (#22642)
16	2	tests/speculative_decoding/speculators/test_eagle3.py
53	0	vllm/model_executor/models/interfaces.py
2	2	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/qwen3.py
8	2	vllm/v1/worker/gpu_model_runner.py

[e5d3d63c4] Daniel Serebrenik 2025-08-12 [Benchmark] Fix terminal colors in benchmark_serving_multi_turn (python 3.12) (#22730)
4	1	benchmarks/multi_turn/bench_utils.py

[3d9d40efd] Nicolò Lucchesi 2025-08-12 [Bugfix][CI] Fix `test_remote_decode_lifecycle.py::test_short_prompt_lifecycle` (#22727)
3	2	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py

[67c153b88] Po-Han Huang (NVIDIA) 2025-08-12 Fix Llama4 FlashInfer FP4 MoE issues (#22511)
0	2	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
6	1	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
3	2	vllm/model_executor/layers/quantization/modelopt.py

[f7ad6a1eb] wang.yuqi 2025-08-12 [CI Failure] fix tests/entrypoints/openai/test_skip_tokenizer.py (#22708)
9	4	vllm/model_executor/models/prithvi_geospatial_mae.py

[80bb1e8af] Harry Mellor 2025-08-12 Officially support SmolLM3 using the Transformers backend (#22665)
6	0	docs/models/supported_models.md
1	0	tests/models/registry.py
3	0	vllm/model_executor/models/registry.py

[d030b0154] Nicolò Lucchesi 2025-08-12 [BugFix][Nixl][PD] Fix heterogenous TP (#22663)
24	13	vllm/distributed/kv_transfer/kv_connector/factory.py
7	4	vllm/distributed/kv_transfer/kv_connector/utils.py

[767e63b86] Harry Mellor 2025-08-12 [Docs] Improve docs navigation (#22720)
2	1	.gitignore
7	15	docs/.nav.yml
11	0	docs/README.md
7	0	docs/examples/README.md
7	0	docs/mkdocs/stylesheets/extra.css
3	1	docs/usage/README.md
3	2	mkdocs.yaml

[007dd9085] Yongye Zhu 2025-08-12 [gpt-oss] Enable gpt-oss on ampere (#22714)
3	2	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
3	1	vllm/attention/layer.py
4	1	vllm/attention/selector.py
1	1	vllm/model_executor/layers/quantization/mxfp4.py
2	2	vllm/platforms/cpu.py
5	2	vllm/platforms/cuda.py
2	2	vllm/platforms/interface.py
2	2	vllm/platforms/rocm.py
2	2	vllm/platforms/tpu.py
2	2	vllm/platforms/xpu.py

[b8a9d0e42] Jee Jee Li 2025-08-12 [Misc] remove GH discussions link (#22722)
1	1	README.md

[50f2aae1b] zejunchen-zejun 2025-08-12 [LMCache][Example] Align the PYTHONHASHSEED for prefillers and decoders for KV chunks hashing (#21161)
8	0	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh

[46ae7f666] RishiAstra 2025-08-12 [Bugfix] Mamba2 SSD varlen bug fix initstates decay, improve test, assert chunk pwr 2 (#21783)
8	9	tests/kernels/mamba/test_mamba_ssm_ssd.py
2	4	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
5	0	vllm/model_executor/layers/mamba/ops/ssd_combined.py

[1ece7f30b] Jun-Howie 2025-08-12 Fix: AWQ Marlin get_quant_method does not recognize "modules_to_not_convert" (#21888)
6	2	vllm/model_executor/layers/quantization/awq_marlin.py

[bc8372efc] phantomlei 2025-08-12 [Bugfix] Fix erroneous randomly generated cases in bad word testing (#22170)
30	4	tests/v1/sample/test_sampler.py

[8d17fa633] Sugar-zsg 2025-08-12 [V0] Correct CUDA Graph capture for encoder-decoder models (#22630)
11	1	vllm/config/__init__.py

[9f909b899] dongluw 2025-08-12 [New Model] Support Command-A-Vision (#22660)
2	1	docs/models/supported_models.md
24	0	examples/offline_inference/vision_language.py
37	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/registry.py
445	0	vllm/model_executor/models/cohere2_vision.py
1	0	vllm/model_executor/models/registry.py

[59f3b9363] Chendi.Xue 2025-08-12 [DOC] update v1_guide with INTEL HW (#22679)
2	0	docs/usage/v1_guide.md

[78077d541] Harry Mellor 2025-08-12 Move `SchedulerConfig` from `config/__init__.py` to `config/scheduler.py` (#22626)
2	314	vllm/config/__init__.py
329	0	vllm/config/scheduler.py

[6d729c43f] wang.yuqi 2025-08-12 [Bugfix] Fix ModernBert load & Enable sliding window attention for bidirectional attention. (#22637)
18	3	tests/models/language/pooling/test_gte.py
15	16	vllm/model_executor/models/modernbert.py
2	0	vllm/v1/attention/backends/flash_attn.py
66	40	vllm/v1/worker/gpu_model_runner.py

[2f4657952] Sooraj S 2025-08-12 [doc] Update x86 CPU-inference installation doc to reflect optionality of AVX512f  (#22707)
3	2	docs/getting_started/installation/cpu/x86.inc.md

[3a7e3bbdd] Hongsheng Liu 2025-08-12 [Doc] Added unmentioned required option "method" in the usage of EAGLE-3 based models (#21737)
4	0	docs/features/spec_decode.md

[4fbd8bb59] Harry Mellor 2025-08-12 Fix passing `SpeculativeConfig` from the CLI (#22652)
4	0	vllm/engine/arg_utils.py

[ad344ef55] Chen Zhang 2025-08-11 [gpt-oss] Small bug fixes for frontend (#22512)
42	14	vllm/entrypoints/context.py
3	2	vllm/entrypoints/openai/protocol.py
16	13	vllm/entrypoints/openai/serving_responses.py
13	2	vllm/entrypoints/tool.py
3	2	vllm/entrypoints/tool_server.py

[bbaf9e9cb] Chen Zhang 2025-08-11 [gpt-oss] Fix mxfp4 support (#22700)
1	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py

[467850347] Benji Beck 2025-08-11 Migrate MiniCPMVImageInputs to TensorSchema (#21939)
36	29	vllm/model_executor/models/minicpmv.py

[93d065243] Michael Goin 2025-08-11 [CI] Increase timeout for test_completion_with_image_embeds (#22670)
2	1	tests/v1/entrypoints/openai/test_completion_with_image_embeds.py

[ea1292ad3] Michael Goin 2025-08-11 [CI Failure] Use float32 for tests/entrypoints/openai/test_audio.py (#22686)
2	0	tests/entrypoints/openai/test_audio.py

[dc5e4a653] Po-Han Huang (NVIDIA) 2025-08-12 Upgrade FlashInfer to v0.2.11 (#22613)
1	1	docker/Dockerfile
1	1	setup.py

[839ab0034] Harry Mellor 2025-08-12 Re-enable Xet on TPU tests now that `hf_xet` has been updated (#22666)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
0	3	tests/entrypoints/llm/test_accuracy.py

[9b94d6ec8] Andy Chen 2025-08-11 Enable 4bit bnb prequant MOE (#21548)
3	7	vllm/model_executor/model_loader/bitsandbytes_loader.py
1	1	vllm/model_executor/models/qwen3_moe.py

[1891a265d] Chen Zhang 2025-08-11 [gpt-oss] Add test for response API + harmony (but skipped) (#22554)
624	0	tests/entrypoints/openai/test_response_api_with_harmony.py

[95a935fc4] Chen Zhang 2025-08-11 [gpt-oss] Support streaming in response API (#22431)
445	5	vllm/entrypoints/openai/serving_responses.py

[458e74eb9] Harry Mellor 2025-08-11 Support more parallel styles in Transformers backend TP (#22651)
13	5	vllm/model_executor/models/transformers.py

[65abe111a] TJian 2025-08-11 [CI] Skip Tree Attn Test in `test_max_len.py` to unblock CI (#22664)
5	0	tests/v1/spec_decode/test_max_len.py

[807d21b80] 22quinn 2025-08-11 [BugFix] [Spec Decode] Remove LlamaForCausalLMEagle3 to fix CI (#22611)
5	4	tests/models/registry.py
24	21	tests/v1/e2e/test_spec_decode.py
2	1	vllm/model_executor/models/registry.py
1	1	vllm/transformers_utils/configs/eagle.py

[c90fb03df] Isotr0py 2025-08-12 [CI/Build] Skip Mllama HF runner tests with Transformers v4.55.0 (#22659)
17	0	tests/models/multimodal/generation/test_mllama.py

[84cf78ace] wang.yuqi 2025-08-12 [Model] Pooling models default to using chunked prefill & prefix caching if supported. (#20930)
6	0	tests/entrypoints/llm/test_classify.py
15	0	tests/entrypoints/openai/test_classification.py
10	2	tests/models/language/pooling/mteb_utils.py
93	0	tests/models/language/pooling/test_auto_prefix_cache_support.py
61	56	tests/models/language/pooling/test_baai.py
4	4	tests/models/language/pooling/test_bge_reranker_v2_gemma.py
7	5	tests/models/language/pooling/test_cross_encoder.py
44	43	tests/models/language/pooling/test_gte.py
22	22	tests/models/language/pooling/test_intfloat.py
8	6	tests/models/language/pooling/test_jina.py
8	7	tests/models/language/pooling/test_mxbai_rerank.py
14	13	tests/models/language/pooling/test_nomic.py
8	7	tests/models/language/pooling/test_qwen3_reranker.py
34	33	tests/models/language/pooling/test_snowflake_arctic_embed.py
18	0	tests/models/utils.py
14	0	tests/test_config.py
8	0	vllm/config/__init__.py
4	5	vllm/engine/arg_utils.py
4	0	vllm/entrypoints/llm.py
11	27	vllm/model_executor/layers/pooler.py
2	2	vllm/model_executor/models/adapters.py
8	8	vllm/model_executor/models/bert.py
3	1	vllm/model_executor/models/bert_with_rope.py
14	0	vllm/model_executor/models/interfaces.py
2	1	vllm/model_executor/models/internlm2.py
1	3	vllm/model_executor/models/jamba.py
4	2	vllm/model_executor/models/modernbert.py
6	10	vllm/model_executor/models/qwen2_rm.py
4	2	vllm/model_executor/models/registry.py
3	1	vllm/model_executor/models/roberta.py
12	1	vllm/v1/worker/gpu_model_runner.py

[16fb668b6] GuanLuo 2025-08-11 fix: NIXL connector transfers partial block to pass full multi-modal context (#21074)
8	10	tests/v1/kv_connector/unit/test_nixl_connector.py
14	9	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
99	5	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
9	17	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[f7dcce7a4] Wentao Ye 2025-08-11 [Feature] Add `VLLM_USE_DEEP_GEMM_E8M0` Env to Control E8M0 Scale (#21968)
3	2	tests/kernels/moe/test_block_fp8.py
3	3	tests/kernels/moe/test_deepep_deepgemm_moe.py
5	0	vllm/envs.py
2	2	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
3	3	vllm/model_executor/layers/fused_moe/fused_moe.py
3	3	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
7	12	vllm/model_executor/layers/quantization/fp8.py
2	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py
37	10	vllm/utils/deep_gemm.py

[8e13d9fe6] Isotr0py 2025-08-12 [Misc] Further clean up some redundant config definitions (#22649)
33	18	vllm/transformers_utils/config.py
1	5	vllm/transformers_utils/configs/__init__.py
0	31	vllm/transformers_utils/configs/mllama.py
0	31	vllm/transformers_utils/configs/nvlm_d.py

[3fa5b2584] Eric Curtin 2025-08-11 Document aarch64 CPU support works (#22646)
6	6	docs/usage/v1_guide.md

[14a5d903a] danielafrimi 2025-08-11 [Model] NemotronH Support  (#22349)
21	5	vllm/model_executor/models/nemotron_h.py
2	2	vllm/transformers_utils/configs/nemotron_h.py

[951b03829] Cyrus Leung 2025-08-11 [Misc] Move jsontree to utils (#22622)
1	1	vllm/inputs/registry.py
1	1	vllm/model_executor/models/aya_vision.py
1	1	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/minimax_vl_01.py
1	1	vllm/model_executor/models/tarsier.py
1	1	vllm/multimodal/cache.py
1	1	vllm/multimodal/inputs.py
0	0	vllm/{ => utils}/jsontree.py

[ebf7605b0] Cyrus Leung 2025-08-11 [Misc] Move tensor schema tests (#22612)
4	3	.buildkite/test-pipeline.yaml
6	0	tests/utils_/__init__.py
0	0	tests/{standalone_tests => utils_}/test_tensor_schema.py
1	2	tests/{ => utils_}/test_utils.py
1	1	tools/check_pickle_imports.py

[bc1d02ac8] Harry Mellor 2025-08-11 [Docs] Add comprehensive CLI reference for all large `vllm` subcommands (#22601)
4	6	docs/.nav.yml
0	0	docs/api/{summary.md => README.md}
1	0	docs/cli/.meta.yml
8	0	docs/cli/.nav.yml
43	42	docs/cli/README.md
9	0	docs/cli/bench/latency.md
9	0	docs/cli/bench/serve.md
9	0	docs/cli/bench/throughput.md
5	0	docs/cli/chat.md
5	0	docs/cli/complete.md
9	0	docs/cli/json_tip.inc.md
9	0	docs/cli/run-batch.md
9	0	docs/cli/serve.md
1	9	docs/configuration/engine_args.md
32	17	docs/mkdocs/hooks/generate_argparse.py
2	0	requirements/docs.txt
2	2	vllm/benchmarks/throughput.py
35	25	vllm/entrypoints/cli/openai.py
3	2	vllm/entrypoints/openai/run_batch.py
4	1	vllm/utils/__init__.py

[1e55dfa7e] JartX 2025-08-11 [BUGFIX] KeyError 'layers.14.mlp.gate.g_idx' for Qwen3-MoE with GPTQ on ROCm (#22017)
1	1	vllm/model_executor/models/qwen3_moe.py

[384a05297] Jee Jee Li 2025-08-11 [Misc] benchmark_moe supports expert parallel (#22251)
11	9	benchmarks/kernels/benchmark_moe.py

[39052dbca] Maximilien de Bayser 2025-08-11 Support token_type_ids in V1 with less code changes (#21985)
3	1	tests/entrypoints/openai/test_rerank.py
3	1	tests/entrypoints/openai/test_score.py
9	0	tests/models/language/pooling/test_scoring.py
26	28	vllm/entrypoints/llm.py
31	51	vllm/entrypoints/openai/serving_score.py
37	3	vllm/entrypoints/score_utils.py
63	25	vllm/model_executor/models/bert.py
17	19	vllm/model_executor/models/roberta.py
6	2	vllm/pooling_params.py
40	0	vllm/v1/worker/gpu_model_runner.py

[9c97a1c34] vllmellm 2025-08-11 [ROCm][AITER] Support AITER Rope ops in RotaryEmbedding Module. (#22521)
71	0	vllm/model_executor/layers/rotary_embedding/base.py
2	2	vllm/model_executor/layers/rotary_embedding/common.py
4	8	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
127	0	vllm/model_executor/layers/rotary_embedding/rocm_aiter_rope_ops.py

[f919d4cb8] Eugene Cheah 2025-08-10 [BugFix] Fix logits repetition penalty cuda check (#22592)
1	1	vllm/_custom_ops.py

[afa5b7ca0] Zhewen Li 2025-08-10 [Misc][gpt-oss] guard import when triton kernel when not up to date  (#22584)
12	3	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py

[1b9902806] Lifans 2025-08-10 [Misc][gpt-oss] Add rules to label gpt-oss related PRs (#22600)
14	0	.github/mergify.yml

[5898b135a] Nick Hill 2025-08-10 [BugFix] Fix KVConnectorOutput TPU breakage (#22598)
8	4	tests/v1/kv_connector/unit/utils.py
2	2	vllm/v1/core/sched/scheduler.py
9	4	vllm/v1/worker/tpu_model_runner.py

[b799f4b9e] 22quinn 2025-08-10 [CI/Build] Fix tensorizer test for load_format change (#22583)
0	1	.buildkite/test-pipeline.yaml
1	1	tests/entrypoints/openai/test_tensorizer_entrypoint.py
2	2	tests/tensorizer_loader/test_tensorizer.py

[06da44f0c] Benji Beck 2025-08-10 Migrate LlavaImageInputs to TensorSchema (#21770)
35	32	vllm/model_executor/models/llava.py

[a55499174] Benji Beck 2025-08-10 Migrate LlavaNextVideoPixelInputs to TensorSchema (#21843)
22	35	vllm/model_executor/models/llava_next_video.py

[d1af8b7be] Doug Smith 2025-08-10 enable Docker-aware precompiled wheel setup (#22106)
5	10	docker/Dockerfile
102	83	setup.py
9	2	vllm/envs.py

[68b254d67] Benji Beck 2025-08-10 Fix TensorSchema validation test for symbolic dims (#22366)
18	14	tests/standalone_tests/test_tensor_schema.py

[8c50d62f5] ZiTian Zhao 2025-08-11 Remove redundant row_indices unsqueeze operation in MiniCPMO (#22528)
0	1	vllm/model_executor/models/minicpmo.py

[b4e291672] Benji Beck 2025-08-10 Migrate LlavaNextImageInputs to TensorSchema (#21774)
31	63	vllm/model_executor/models/llava_next.py
3	0	vllm/utils/tensor_schema.py

[65a7917be] Breno Baldas Skuk 2025-08-10 Fix(benchmarks): allow multiple mm contents in OpenAI Chat Completion Benchmarks (#22534)
14	3	benchmarks/backend_request_func.py
1	1	benchmarks/benchmark_dataset.py
8	1	benchmarks/benchmark_serving.py
3	1	vllm/benchmarks/datasets.py
15	3	vllm/benchmarks/lib/endpoint_request_func.py
8	1	vllm/benchmarks/serve.py

[b76753f0b] Isotr0py 2025-08-11 [Bugfix][Kernel] Support partial rotary embedding for MRoPE triton kernel (#22593)
14	6	tests/kernels/{ => core}/test_mrope.py
16	12	vllm/model_executor/layers/rotary_embedding/mrope.py

[b81fe83b2] youkaichao 2025-08-10 [doc] add alibaba cloud as sponsor (#22597)
1	0	README.md
1	0	docs/community/sponsors.md

[0757551c9] youkaichao 2025-08-10 [doc] add beijing meetup links (#22596)
2	1	README.md
1	0	docs/community/meetups.md

[8290d15d2] Harry Mellor 2025-08-10 Move `CacheConfig` from `config/__init__.py` to `config/cache.py` (#22586)
4	186	vllm/config/__init__.py
204	0	vllm/config/cache.py

[049c24514] Isotr0py 2025-08-10 [Misc] Replace flaky image urls in pixtral test (#22574)
11	13	tests/models/multimodal/generation/test_pixtral.py
2	1	tests/models/utils.py

[00976db0c] Harry Mellor 2025-08-10 [Docs] Fix warnings in docs build (#22588)
0	2	docs/api/summary.md
1	1	docs/configuration/tpu.md
5	3	docs/contributing/model/multimodal.md
2	2	docs/models/generative_models.md
1	1	docs/models/pooling_models.md
1	1	docs/models/supported_models.md
0	0	vllm/attention/layers/__init__.py
6	4	vllm/inputs/__init__.py
0	0	vllm/model_executor/warmup/__init__.py
64	76	vllm/sampling_params.py

[d411df029] Cyrus Leung 2025-08-10 [Misc] Further refine type annotations in parallel state (#22499)
0	3	vllm/distributed/eplb/eplb_state.py
19	17	vllm/distributed/parallel_state.py

[010e0e39e] 22quinn 2025-08-10 [Doc] Fix API doc link in side navigation (#22585)
3	3	docs/.nav.yml

[326976291] Ning Xie 2025-08-10 [Misc] code clean duplicate set_current_vllm_config in _set_vllm_config (#22566)
0	1	tests/kernels/moe/modular_kernel_tools/parallel_utils.py

[7e8d68577] Isotr0py 2025-08-10 [Minor] Fix pre-commit error on main (#22579)
2	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[c49848396] Harry Mellor 2025-08-10 Refactor sliding window configuration to Transformers best practice (#21927)
1	1	docs/contributing/model/basic.md
0	22	tests/test_config.py
31	80	vllm/config/__init__.py
9	1	vllm/engine/arg_utils.py
7	15	vllm/model_executor/models/commandr.py
4	17	vllm/model_executor/models/exaone4.py
3	6	vllm/model_executor/models/gemma2.py
4	10	vllm/model_executor/models/gemma3.py
2	4	vllm/model_executor/models/gemma3_mm.py
6	7	vllm/model_executor/models/gemma3n.py
1	3	vllm/model_executor/models/gritlm.py
5	12	vllm/model_executor/models/llama.py
2	7	vllm/model_executor/models/phi4flash.py
2	2	vllm/model_executor/models/qwen2.py
7	45	vllm/model_executor/models/transformers.py
40	0	vllm/transformers_utils/config.py

[2a84fb422] Chengji Yao 2025-08-09 [TPU] kv cache update kernel doesn't need to be padded slices to multiple of num_slices_per_block (#22394)
0	5	tests/v1/tpu/test_kv_cache_update_kernel.py
10	6	vllm/attention/ops/pallas_kv_cache_update.py
9	10	vllm/v1/worker/tpu_model_runner.py

[534c45b96] ZiTian Zhao 2025-08-10 Improve fast_topk function with type hints and documentation (#22530)
17	1	vllm/model_executor/models/utils.py

[3d7363e61] Le Chen 2025-08-10 [Config] add "qwen" as a native eagle3 target supported model (#22333)
4	0	tests/models/registry.py
21	18	tests/v1/e2e/test_spec_decode.py
1	7	vllm/config/__init__.py
1	0	vllm/model_executor/models/registry.py
3	2	vllm/transformers_utils/configs/eagle.py

[0c5254b82] Jee Jee Li 2025-08-10 [oss] Init gpt-oss bf16 support (#22508)
5	1	vllm/model_executor/layers/fused_moe/config.py
160	109	vllm/model_executor/layers/fused_moe/fused_moe.py
28	12	vllm/model_executor/layers/fused_moe/layer.py
149	3	vllm/model_executor/models/gpt_oss.py

[61f67d8ac] Thomas Parnell 2025-08-10 [V1] [Hybrid] Enable Full CUDA Graph (decode-only) for Mamba layers (#21401)
60	0	tests/models/language/generation/test_hybrid.py
43	1	vllm/v1/attention/backends/mamba_attn.py

[42172ad18] TJian 2025-08-09 [FEAT] [Performance] Add triton mrope to replace the torch code path (#22375)
328	0	benchmarks/kernels/benchmark_mrope.py
207	0	tests/kernels/test_mrope.py
231	0	vllm/model_executor/layers/rotary_embedding/mrope.py

[fbd8595c5] Isotr0py 2025-08-10 [Bugfix] Fix basic models tests hanging due to mm processor creation (#22571)
23	5	vllm/multimodal/registry.py

[5a16fa614] Nicolò Lucchesi 2025-08-09 [Model] Gemma3n MM (#20495)
11	4	docs/models/supported_models.md
20	0	examples/offline_inference/audio_language.py
27	0	examples/offline_inference/vision_language.py
1	1	requirements/test.in
1	1	requirements/test.txt
4	1	tests/models/multimodal/processing/test_common.py
3	1	tests/models/registry.py
61	0	tests/test_test.py
34	45	vllm/model_executor/models/gemma3n.py
700	0	vllm/model_executor/models/gemma3n_mm.py
2	2	vllm/model_executor/models/registry.py

[2d18256e4] Harry Mellor 2025-08-09 Move `ParallelConfig` from `config/__init__.py` to `config/parallel.py` (#22565)
1	1	.github/CODEOWNERS
2	355	vllm/config/__init__.py
1	1	vllm/config/compilation.py
375	0	vllm/config/parallel.py

[56186474f] Harry Mellor 2025-08-09 [Docs] Reduce noise in docs and `--help` from the JSON tip (#22567)
10	0	docs/cli/README.md
10	0	docs/configuration/engine_args.md
2	21	vllm/engine/arg_utils.py
18	3	vllm/utils/__init__.py

[1bf5e1f25] Thomas Parnell 2025-08-09 [CI] [Hybrid] Speed up hybrid models test by removing large models  (#22563)
7	14	tests/models/language/generation/test_hybrid.py

[a6022e6fb] Yuxuan Zhang 2025-08-09 GLM-4.5V with new class name at transformers (#22520)
2	2	docs/models/supported_models.md
1	1	tests/models/registry.py
7	1	vllm/model_executor/models/glm4_moe.py
1	1	vllm/model_executor/models/registry.py
2	1	vllm/transformers_utils/config.py

[2be07a0db] Thomas Parnell 2025-08-09 Update docs for Minimax-Text support (#22562)
2	2	docs/models/supported_models.md
4	0	docs/usage/v1_guide.md

[0edc0cd52] Jee Jee Li 2025-08-09 [Bugfix] Fix CI moe kernel failure (#22556)
142	64	tests/kernels/moe/test_gpt_oss_triton_kernels.py

[7920e9b1c] Isotr0py 2025-08-09 [Bugfix] Fix failing GPT-OSS initialization test (#22557)
1	1	tests/models/registry.py
5	0	tests/models/test_initialization.py

[b7c0942b6] Charlie Fu 2025-08-09 [ROCm][Misc] Rename the context_len to seq_len in ROCm custom paged attention kernel (#22097)
87	92	csrc/rocm/attention.cu
2	2	csrc/rocm/ops.h
2	2	csrc/rocm/torch_bindings.cpp

[9a0c5ded5] Kyuyeun Kim 2025-08-08 [TPU] Add support for online w8a8 quantization (#22425)
2	0	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
73	0	tests/v1/tpu/test_tpu_int8.py
7	3	vllm/model_executor/layers/quantization/tpu_int8.py

[10a02535d] Eldar Kurtić 2025-08-09 Fix loading of quantized BigCode models (#22463)
14	4	vllm/model_executor/models/gpt_bigcode.py

[65552b476] Cyrus Leung 2025-08-09 [Misc] Use config definitions from Transformers library (#21913)
11	11	vllm/model_executor/models/aimv2.py
4	4	vllm/model_executor/models/commandr.py
7	7	vllm/model_executor/models/dbrx.py
9	6	vllm/model_executor/models/deepseek_v2.py
4	4	vllm/model_executor/models/dots1.py
3	3	vllm/model_executor/models/exaone4.py
5	5	vllm/model_executor/models/glm4_moe.py
3	3	vllm/model_executor/models/minimax_text_01.py
2	2	vllm/model_executor/models/olmoe.py
3	3	vllm/model_executor/models/qwen2_moe.py
3	3	vllm/model_executor/models/qwen3_moe.py

[7ad7adb67] Or Ozeri 2025-08-09 v1: Pass KVConnectorOutput to scheduler-side (#22157)
13	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
5	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
4	0	vllm/v1/core/sched/scheduler.py

[6ade99eaf] Thomas Parnell 2025-08-09 [V1] [Hybrid] Support Minimax-Text-01 in V1  (#22151)
1	1	vllm/model_executor/layers/lightning_attn.py
11	0	vllm/model_executor/layers/mamba/mamba_utils.py
152	40	vllm/model_executor/models/minimax_text_01.py
67	0	vllm/v1/attention/backends/linear_attn.py
3	1	vllm/v1/attention/backends/mamba_selectors.py

[3157aebb6] Wentao Ye 2025-08-09 [Log] Add Warning for Deprecation of DeepGEMM old version (#22194)
9	0	vllm/utils/deep_gemm.py

[8a0ffd628] Thomas Parnell 2025-08-09 Remove mamba_ssm from vLLM requirements; install inside test container using `--no-build-isolation` (#22541)
4	4	.buildkite/test-pipeline.yaml
0	13	docs/contributing/ci/update_pytorch_version.md
2	3	requirements/test.in
1	12	requirements/test.txt
9	7	tests/models/language/generation/test_hybrid.py
10	6	tests/models/registry.py

[23472ff51] Roger Wang 2025-08-08 [Doc] Add usage of implicit text-only mode  (#22561)
3	0	docs/models/supported_models.md

[08b751ba7] Roger Wang 2025-08-08 Implicit language-model-only mode via limit-mm-per-prompt (#22299)
38	0	tests/multimodal/test_registry.py
0	9	vllm/config/__init__.py
21	13	vllm/model_executor/models/llava.py
23	15	vllm/model_executor/models/mistral3.py
20	10	vllm/model_executor/models/mllama4.py
25	8	vllm/model_executor/models/qwen2_5_omni_thinker.py
15	7	vllm/model_executor/models/qwen2_5_vl.py
18	8	vllm/model_executor/models/qwen2_vl.py
38	22	vllm/model_executor/models/step3_vl.py
39	0	vllm/multimodal/registry.py
1	1	vllm/v1/core/encoder_cache_manager.py
2	1	vllm/v1/engine/core.py
7	5	vllm/v1/engine/mm_input_cache.py
1	1	vllm/v1/engine/processor.py
9	7	vllm/v1/worker/gpu_model_runner.py
14	9	vllm/v1/worker/tpu_model_runner.py

[429e4e2d4] Isotr0py 2025-08-09 [Bugfix] Fix ModernBert cuda graph capturing in v1 (#21901)
3	2	tests/models/language/pooling/mteb_utils.py
1	1	vllm/model_executor/models/bert.py
21	25	vllm/model_executor/models/bert_with_rope.py
11	11	vllm/model_executor/models/modernbert.py
3	3	vllm/model_executor/models/roberta.py

[35afe1b30] Pradyun92 2025-08-08 [BugFix] [P/D] Handle lookahead token count edge-case with Eagle Spec Decoding and P/D (#22317)
11	1	vllm/v1/core/sched/scheduler.py

[81c57f60a] Kunshang Ji 2025-08-09 [XPU] upgrade torch 2.8 on for XPU (#22300)
11	6	docker/Dockerfile.xpu
3	8	requirements/xpu.txt
0	9	vllm/plugins/__init__.py
1	1	vllm/v1/worker/xpu_worker.py

[311d87561] Russell Bryant 2025-08-08 Drop flaky test_healthcheck_response_time (#22539)
0	54	tests/entrypoints/openai/test_async_tokenization.py

[e3edc0a7a] Harry Mellor 2025-08-09 Extract `CompilationConfig` from `config.py` (#22524)
0	33	tests/engine/test_arg_utils.py
7	442	vllm/{config.py => config/__init__.py}
428	0	vllm/config/compilation.py
29	0	vllm/config/utils.py
3	5	vllm/engine/arg_utils.py

[baece8c3d] yyweiss 2025-08-09 [Frontend] Add unix domain socket support (#18097)
3	0	docs/cli/README.md
43	0	tests/entrypoints/openai/test_uds.py
19	8	tests/utils.py
19	8	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/cli_args.py

[2fcf6b27b] Guy Stone 2025-08-08 [Docs] fix broken links in metrics.md (#22315)
4	4	docs/design/metrics.md

[41b965575] Harry Mellor 2025-08-09 Skip Qwen 1 in CI because remote code is no longer compatible with Transformers (#22536)
2	0	tests/models/registry.py

[bd875d2eb] Thomas Parnell 2025-08-09 [Bugfix] Update FA commit hash (#22546)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[f703b923f] Varun Sundar Rabindranath 2025-08-08 [Misc] DeepGEMM : Avoid JIT generation in the hot-path (#22215)
0	12	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
30	25	vllm/model_executor/layers/fused_moe/fused_moe.py
219	0	vllm/model_executor/warmup/deep_gemm_warmup.py
20	0	vllm/model_executor/warmup/kernel_warmup.py
5	0	vllm/v1/worker/gpu_worker.py

[cd9b9de1f] Lucas Wilkinson 2025-08-08 [BugFix] Fix IMA FlashMLA full cuda-graph and DP + Update FlashMLA (#21691)
4	4	cmake/external_projects/flashmla.cmake
0	1	vllm/attention/ops/flashmla.py
38	22	vllm/v1/attention/backends/mla/flashmla.py

[fe6d8257a] Chen Zhang 2025-08-08 [gpt-oss] Support tool call and implement MCP tool server (#22427)
4	1	vllm/entrypoints/harmony_utils.py
5	1	vllm/entrypoints/openai/api_server.py
107	80	vllm/entrypoints/openai/serving_responses.py
118	1	vllm/entrypoints/tool_server.py

[e29059407] Ricardo Decal 2025-08-08 [Docs] Rename “Distributed inference and serving” to “Parallelism & Scaling” (#22466)
10	10	docs/models/supported_models.md
1	1	docs/serving/{distributed_serving.md => parallelism_scaling.md}
1	1	docs/usage/troubleshooting.md

[f756a682d] Yongye Zhu 2025-08-08 [gpt-oss] guard import when triton kernel is not installed (#22529)
19	10	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py

[f0964e29c] Daniel Serebrenik 2025-08-08 [Benchmark] Add benchmark tool for multi turn conversations (#20267)
71	0	benchmarks/multi_turn/README.md
493	0	benchmarks/multi_turn/bench_dataset.py
25	0	benchmarks/multi_turn/bench_utils.py
1557	0	benchmarks/multi_turn/benchmark_serving_multi_turn.py
354	0	benchmarks/multi_turn/convert_sharegpt_to_openai.py
35	0	benchmarks/multi_turn/generate_multi_turn.json
5	0	benchmarks/multi_turn/requirements.txt

[e789cad6b] Yongye Zhu 2025-08-08 [gpt-oss] triton kernel mxfp4 (#22421)
3	0	.gitignore
375	0	tests/kernels/moe/test_gpt_oss_triton_kernels.py
230	0	vllm/model_executor/layers/fused_moe/gpt_oss_triton_kernels_moe.py
12	5	vllm/model_executor/layers/fused_moe/layer.py
63	3	vllm/model_executor/layers/quantization/mxfp4.py
45	1	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
21	0	vllm/model_executor/layers/utils.py
6	0	vllm/utils/__init__.py

[e5ebeeba5] Harry Mellor 2025-08-08 Remove exception for Python 3.8 typing from linter (#22506)
0	2	pyproject.toml
2	2	vllm/utils/__init__.py

[7be7f3824] Harry Mellor 2025-08-08 [Docs] Improve API docs (+small tweaks) (#22459)
2	3	docs/.nav.yml
0	0	docs/api/{README.md => summary.md}
1	4	docs/features/quantization/inc.md
1	1	docs/mkdocs/hooks/generate_examples.py
1	5	mkdocs.yaml

[ccdae737a] Nick Hill 2025-08-08 [BugFix] Don't cancel asyncio tasks directly from destructors (#22476)
17	6	vllm/utils/__init__.py
2	3	vllm/v1/engine/async_llm.py
4	5	vllm/v1/engine/core_client.py

[904063907] rongfu.leng 2025-08-08 [Misc] fix openai version (#22485)
1	1	requirements/common.txt

[43c4f3d77] Cyrus Leung 2025-08-08 [Misc] Begin deprecation of `get_tensor_model_*_group` (#22494)
2	3	tests/distributed/test_custom_all_reduce.py
2	3	tests/distributed/test_quick_all_reduce.py
12	4	vllm/distributed/parallel_state.py

[1712543df] Cyrus Leung 2025-08-08 [CI/Build] Fix multimodal tests (#22491)
2	1	vllm/engine/llm_engine.py
13	12	vllm/multimodal/registry.py
1	1	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/llm_engine.py

[808a7b69d] lkchen 2025-08-07 [bench] Fix benchmark/serve.py to ignore unavailable results (#22382)
1	1	vllm/benchmarks/serve.py

[099c04646] iAmir97 2025-08-08 [Doc] Sleep mode documentation (#22310)
80	0	docs/features/sleep_mode.md

[af473f0a8] Po-Han Huang (NVIDIA) 2025-08-08 [bugfix] Fix Llama3/4 issues caused by FlashInfer 0.2.10 (#22426)
16	8	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
2	1	vllm/v1/attention/backends/flashinfer.py

[157f9c136] Cyrus Leung 2025-08-08 Fix pre-commit (#22487)
2	2	vllm/model_executor/models/minicpmo.py

[6f287915d] ZiTian Zhao 2025-08-08 Optimize MiniCPMO mask creation with vectorized implementation (#22464)
23	9	vllm/model_executor/models/minicpmo.py

[c152e2a8a] Yuxuan Zhang 2025-08-08 not tie_word_embeddings for glm-4.5 and glm-4.5v (#22460)
0	2	vllm/model_executor/models/glm4_moe.py

[17eaaef59] Chauncey 2025-08-08 [Bugfix] Fix RuntimeError: Index put requires the source and destination dtypes match (#22065)
103	0	tests/v1/entrypoints/openai/test_completion_with_image_embeds.py
3	2	vllm/model_executor/models/utils.py

[3303f134e] Junhao Li 2025-08-07 [Kernel] Add support for block FP8 on SM120 (NVIDIA 5090 and RTX PRO 6000) (#22131)
1	0	CMakeLists.txt
10	0	csrc/cutlass_extensions/common.hpp
23	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8.cu
183	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm120_fp8_dispatch.cuh
6	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp
6	18	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm120.cu

[b2c8ce57c] Shu Wang 2025-08-07 Fix Flashinfer CUTLASS MOE Allgather (#21963)
2	1	vllm/distributed/device_communicators/cuda_communicator.py
58	0	vllm/forward_context.py
4	20	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
7	6	vllm/model_executor/layers/fused_moe/layer.py

[a3b9c17b5] Shu Wang 2025-08-07 Support Tensorrt-LLM MoE fp4 for low-latency (#21331)
15	0	vllm/envs.py
2	1	vllm/model_executor/layers/fused_moe/config.py
4	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
252	32	vllm/model_executor/layers/quantization/modelopt.py
9	3	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
2	2	vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py
4	0	vllm/utils/flashinfer.py

[d57dc2364] Zhiyu 2025-08-07 Add ModelOpt Qwen3 nvfp4 support (#20101)
34	32	vllm/model_executor/model_loader/weight_utils.py
11	2	vllm/model_executor/models/qwen2.py
13	3	vllm/model_executor/models/qwen3_moe.py

[e2c8f1ede] Andrew Sansom 2025-08-07 [PERF] Use pybase64 to more quickly decode prompt embeddings (#22469)
3	2	vllm/entrypoints/openai/serving_engine.py

[1ee5ead5f] TJian 2025-08-07 [ROCm] [V1] [SpecDec] Enable Speculative Decoding on ROCm V1 Engine (#21496)
16	0	tests/utils.py
5	2	tests/v1/attention/utils.py
15	0	tests/v1/e2e/test_spec_decode.py
45	10	tests/v1/spec_decode/test_eagle.py
30	24	tests/v1/spec_decode/test_max_len.py
17	5	vllm/v1/spec_decode/eagle.py

[acf8aeb79] Ning Xie 2025-08-08 [Misc] normalize multiprocessing Queue usage (#22371)
18	2	tests/test_sharded_state_loader.py

[7e3a8dc90] Harry Mellor 2025-08-07 Remove `from_dict` from `SpeculativeConfig` (#22451)
6	7	tests/v1/spec_decode/test_ngram.py
0	5	vllm/config.py
3	16	vllm/engine/arg_utils.py

[139d15578] Cyrus Leung 2025-08-08 [Frontend] Use engine argument to control MM cache size (#22441)
1	1	docs/configuration/conserving_memory.md
9	3	docs/configuration/optimization.md
3	3	examples/offline_inference/mistral-small.py
2	2	examples/offline_inference/vision_language.py
1	3	tests/models/multimodal/generation/vlm_utils/core.py
3	3	tests/models/multimodal/processing/test_llama4.py
2	2	tests/models/utils.py
32	11	vllm/config.py
27	7	vllm/engine/arg_utils.py
3	4	vllm/entrypoints/cli/serve.py
1	1	vllm/envs.py
16	6	vllm/multimodal/registry.py
1	1	vllm/v1/core/kv_cache_utils.py

[8c9da6be2] Cyrus Leung 2025-08-08 [Core] Simplify mm processing cache (#22457)
6	6	vllm/model_executor/models/qwen2_5_omni_thinker.py
3	2	vllm/model_executor/models/transformers.py
69	179	vllm/multimodal/processing.py
19	19	vllm/v1/serial_utils.py

[399d2a10e] Woosuk Kwon 2025-08-07 Fix pre-commit error in main (#22462)
18	18	vllm/entrypoints/openai/serving_responses.py

[4815b00f5] Chen Zhang 2025-08-07 [gpt-oss] Generate ResponseOutputItem from Harmony Message (#22410)
1	1	tests/v1/entrypoints/openai/responses/test_basic.py
150	3	vllm/entrypoints/harmony_utils.py
19	12	vllm/entrypoints/openai/protocol.py
122	66	vllm/entrypoints/openai/serving_responses.py

[4da8bf20d] Chen Zhang 2025-08-07 [Tool] Fix auto tool call (#22434)
5	5	vllm/entrypoints/openai/serving_responses.py

[7e0b12181] fxmarty-amd 2025-08-07 [Bugfix] Add missing `packed_modules_mapping` to `DeepseekV2ForCausalLM` (#22352)
16	0	vllm/model_executor/models/deepseek_v2.py

[766bc8162] Cyrus Leung 2025-08-07 [Core] Store only the keys for multi-modal data in P0 (#22198)
15	17	docs/configuration/conserving_memory.md
29	44	docs/configuration/optimization.md
1	1	examples/offline_inference/mistral-small.py
1	1	examples/offline_inference/vision_language.py
3	2	tests/models/utils.py
51	0	tests/multimodal/test_cache.py
2	46	tests/multimodal/test_processing.py
27	3	vllm/config.py
11	11	vllm/engine/arg_utils.py
2	3	vllm/entrypoints/cli/serve.py
3	3	vllm/envs.py
95	0	vllm/multimodal/cache.py
4	49	vllm/multimodal/processing.py
2	2	vllm/v1/core/kv_cache_utils.py
3	4	vllm/v1/engine/core.py
63	34	vllm/v1/engine/mm_input_cache.py
8	9	vllm/v1/engine/processor.py

[289b18e67] WeiQing Chen 2025-08-07 [Docs] Update features/disagg_prefill, add v1 examples and development (#22165)
-	-	docs/assets/features/disagg_prefill/high_level_design.png
-	-	docs/assets/features/disagg_prefill/workflow.png
25	0	docs/features/disagg_prefill.md

[35171b117] Andrew Chan 2025-08-07 [Doc] update docs for nightly benchmarks (#12022)
4	4	.buildkite/nightly-benchmarks/README.md

[a2c6696bf] Ricardo Decal 2025-08-07 [Docs] Factor out troubleshooting to its own guide; add section for Ray Observability (#21578)
15	14	docs/serving/distributed_serving.md
16	0	docs/serving/distributed_troubleshooting.md

[5e8398805] Yong Hoon Shin 2025-08-07 [Doc] Fix link to prefix caching design (#22384)
1	1	docs/features/automatic_prefix_caching.md

[136825de7] Woosuk Kwon 2025-08-07 [Misc] Enhance code formatting in mxfp4.py  (#22423)
52	33	vllm/model_executor/layers/quantization/mxfp4.py

[c2dba2dba] JaceyShao 2025-08-07 Add H20-3e fused MoE kernel tuning configs for GLM-4.5 (#22433)
146	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_H20-3e.json

[434d2f3f7] Harry Mellor 2025-08-07 [Docs] Add missing dependency for docs build (#22435)
1	0	requirements/docs.txt

[8e8e0b6af] Adrián García García 2025-08-07 feat: Add --enable-log-outputs flag for logging model generations (#20707)
248	4	tests/test_logger.py
34	2	vllm/entrypoints/logger.py
6	3	vllm/entrypoints/openai/cli_args.py
104	17	vllm/entrypoints/openai/serving_chat.py
20	0	vllm/entrypoints/openai/serving_responses.py

[82216dc21] Ming Yang 2025-08-06 [Misc] Support routing logic simulation (#21990)
171	0	tests/test_routing_simulator.py
9	0	vllm/envs.py
12	0	vllm/model_executor/layers/fused_moe/layer.py
289	0	vllm/model_executor/layers/fused_moe/routing_simulator.py

[370661856] Moritz Sanft 2025-08-07 [Frontend] Update OpenAI error response to upstream format (#22099)
2	3	tests/entrypoints/openai/test_classification.py
4	4	tests/entrypoints/openai/test_lora_resolvers.py
8	8	tests/entrypoints/openai/test_serving_models.py
11	6	tests/entrypoints/openai/test_transcription_validation.py
3	2	tests/entrypoints/openai/test_translation_validation.py
26	24	vllm/entrypoints/openai/api_server.py
5	2	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/run_batch.py
9	12	vllm/entrypoints/openai/serving_engine.py
4	5	vllm/entrypoints/openai/serving_models.py

[cbc8457b2] vllmellm 2025-08-07 [Model] Switch to Fused RMS norm in Qwen2.5_VL model. (#22184)
7	7	vllm/model_executor/models/qwen2_5_vl.py

[4d4297e8f] lkchen 2025-08-06 [Bench] Split serve.py:main into async/async versions (#22405)
58	54	vllm/benchmarks/serve.py

[2a4c82552] wang.yuqi 2025-08-07 [CI] Skip the pooling models that do not support transformers v4.55 (#22411)
4	1	tests/models/language/pooling/test_embedding.py
9	0	tests/models/language/pooling/test_gte.py
4	0	tests/models/language/pooling/test_reward.py
11	0	tests/models/utils.py

[4be02a377] WeiQing Chen 2025-08-07 [Bugfix] EPLB load statistics problem (#22167)
24	26	vllm/distributed/eplb/eplb_state.py
2	15	vllm/model_executor/layers/fused_moe/layer.py

[f6278b624] Chen Zhang 2025-08-06 [gpt-oss] Convert user input to harmony format (#22402)
3	1	vllm/entrypoints/chat_utils.py
58	2	vllm/entrypoints/harmony_utils.py
7	2	vllm/entrypoints/openai/protocol.py
148	10	vllm/entrypoints/openai/serving_responses.py

[ad6c655dd] Lionel Villard 2025-08-06 preload heavy modules when mp method is forkserver (#22214)
3	1	vllm/benchmarks/latency.py
10	0	vllm/entrypoints/openai/api_server.py

[14bcf93a6] ZiTian.Zhao 2025-08-07 Optimize logger init performance by using module-level constants (#22373)
9	7	vllm/logger.py

[ecbea55ca] Harry Mellor 2025-08-07 Update `hf_xet` pin to resolve hangs (#22356)
0	1	requirements/common.txt
0	1	requirements/nightly_torch_test.txt
0	1	requirements/test.in
1	2	requirements/test.txt

[609b533cb] Syed Muhammad Bin Asif 2025-08-07 [Bugfix] Add proper comparison for package versions (#22314)
3	1	benchmarks/kernels/benchmark_bitblas.py
2	1	docs/design/arch_overview.md
3	1	vllm/attention/ops/triton_decode_attention.py
3	1	vllm/model_executor/layers/quantization/bitblas.py
5	2	vllm/model_executor/layers/quantization/bitsandbytes.py
2	1	vllm/model_executor/layers/quantization/deepspeedfp.py
3	1	vllm/model_executor/layers/quantization/gptq_bitblas.py
5	2	vllm/model_executor/layers/quantization/ipex_quant.py
3	1	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
3	1	vllm/model_executor/layers/quantization/utils/bitblas_utils.py
3	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
3	1	vllm/model_executor/model_loader/bitsandbytes_loader.py
2	1	vllm/v1/sample/ops/topk_topp_sampler.py

[5e9455ae8] qscqesze 2025-08-07 [Bugfix]: Fix the streaming output for function calls in the minimax (#22015)
845	1	tests/tool_use/test_minimax_tool_parser.py
649	203	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py

[a00d8b236] Michael Goin 2025-08-06 Use float32 for test_completion.py (#22385)
1	2	tests/v1/entrypoints/openai/test_completion.py

[04cf435d9] Cyrus Leung 2025-08-07 [Bugfix] Fix wrong method name in Intern-S1 image processor (#22417)
1	1	vllm/model_executor/models/interns1.py

[7377131a2] Tao He 2025-08-07 [Qwen3] Enable dual-chunk-attention support for Qwen3 models. (#21924)
40	24	vllm/model_executor/models/qwen3.py
20	7	vllm/model_executor/models/qwen3_moe.py

[6b47ef24d] Kunshang Ji 2025-08-07 [XPU]Fix `flash_attn_varlen_func` interface on xpu (#22350)
1	0	vllm/_ipex_ops.py

[1dc8a70b6] Lucas Wilkinson 2025-08-06 [Attention] Support multiple attention metadata builders per kv_cache_spec  + proper local attention no hybrid kv cache fix (#21588)
2	1	tests/v1/spec_decode/test_eagle.py
3	3	tests/v1/worker/test_gpu_model_runner.py
4	0	vllm/attention/backends/abstract.py
18	18	vllm/attention/layer.py
88	0	vllm/attention/layers/chunked_local_attention.py
1	1	vllm/attention/selector.py
6	4	vllm/model_executor/models/llama4.py
46	2	vllm/v1/attention/backends/utils.py
5	4	vllm/v1/spec_decode/eagle.py
4	4	vllm/v1/worker/cpu_model_runner.py
167	173	vllm/v1/worker/gpu_model_runner.py
3	2	vllm/v1/worker/tpu_model_runner.py
21	0	vllm/v1/worker/utils.py

[f825c6bd2] Maximilien de Bayser 2025-08-06 Support encoder_only attention for FlexAttention (#22273)
68	20	tests/kernels/test_flex_attention.py
70	27	vllm/v1/attention/backends/flex_attention.py

[41b67f426] tc-mb 2025-08-07 [model] Support MiniCPM-V 4.0 (#22166)
1	1	docs/models/supported_models.md
1	1	tests/models/registry.py
138	10	vllm/model_executor/models/minicpmv.py

[e8961e963] Michael Goin 2025-08-06 Update `flashinfer-python==0.2.10` (#22389)
1	1	docker/Dockerfile
1	1	setup.py

[9a3835aaa] Lain 2025-08-06 Fix trtllm-gen attention env and add attention sink (#22378)
4	9	vllm/envs.py
2	3	vllm/model_executor/models/gpt_oss.py
4	4	vllm/utils/flashinfer.py
9	8	vllm/v1/attention/backends/flashinfer.py
2	4	vllm/v1/attention/backends/utils.py

[5c7cc33f4] Yongye Zhu 2025-08-06 [gpt-oss] fix model config with hf_config (#22401)
3	3	vllm/model_executor/models/gpt_oss.py

[19c9365aa] Chen Zhang 2025-08-06 [gpt-oss] add demo tool server (#22393)
7	0	vllm/entrypoints/openai/api_server.py
4	0	vllm/entrypoints/openai/cli_args.py
4	0	vllm/entrypoints/openai/serving_responses.py
70	0	vllm/entrypoints/tool_server.py

[eec890c1c] Wentao Ye 2025-08-06 [Bug] Fix B200 DeepGEMM E8M0 Accuracy Issue (#22399)
2	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[46a13949d] Asaf Joseph Gardin 2025-08-07 [v1] - Mamba1 Attention Metadata (#21249)
3	0	csrc/mamba/mamba_ssm/selective_scan.h
14	4	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
2	2	docs/models/supported_models.md
5	7	docs/usage/v1_guide.md
2	0	tests/models/language/generation/test_hybrid.py
0	1	tests/v1/test_oracle.py
108	36	vllm/model_executor/layers/mamba/mamba_mixer.py
4	3	vllm/model_executor/layers/mamba/mamba_mixer2.py
67	50	vllm/model_executor/layers/mamba/mamba_utils.py
3	2	vllm/model_executor/models/bamba.py
3	2	vllm/model_executor/models/falcon_h1.py
3	2	vllm/model_executor/models/granitemoehybrid.py
37	25	vllm/model_executor/models/jamba.py
46	31	vllm/model_executor/models/mamba.py
3	2	vllm/model_executor/models/mamba2.py
3	2	vllm/model_executor/models/nemotron_h.py
3	2	vllm/model_executor/models/zamba2.py
67	0	vllm/v1/attention/backends/mamba1_attn.py
4	0	vllm/v1/attention/backends/mamba_selectors.py

[31f09c615] Yongye Zhu 2025-08-06 [gpt-oss] flashinfer mxfp4 (#22339)
12	0	vllm/envs.py
29	3	vllm/model_executor/layers/fused_moe/layer.py
3	0	vllm/model_executor/layers/quantization/__init__.py
387	0	vllm/model_executor/layers/quantization/mxfp4.py
22	0	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py

[31f5dc5b2] Yongye Zhu 2025-08-06 [gpt-oss] Enhance error msg on attention sink init (#22335)
9	5	vllm/v1/attention/backends/flashinfer.py

[ec7cb1922] Woosuk Kwon 2025-08-06 [gpt-oss] Add loop for built-in tool call (#22374)
56	0	vllm/entrypoints/openai/serving_engine.py
17	16	vllm/entrypoints/openai/serving_responses.py

[2435ea7ed] Gregory Shtrasberg 2025-08-06 [Bugfix] Make condition in triton kernel constexpr (#22370)
3	1	vllm/attention/ops/chunked_prefill_paged_decode.py
3	1	vllm/attention/ops/prefix_prefill.py

[4a6b72c2a] Lucas Wilkinson 2025-08-06 [BugFix] Fix triton compile error in `kernel_unified_attention_2/3d` caused by attention sinks (#22368)
15	8	vllm/attention/ops/triton_unified_attention.py

[b4b9813b5] Zhang Jason 2025-08-06 add the codes to check AMD Instinct GPU number (#22367)
8	2	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh

[2cb6ef899] Lucas Wilkinson 2025-08-06 [BugFix] Fix FA2 RuntimeError when sinks is provided (#22365)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[9edd1db02] Woosuk Kwon 2025-08-06 [Minor] Fix type  (#22347)
2	2	vllm/entrypoints/harmony_utils.py

[f263a4b53] Woosuk Kwon 2025-08-06 [gpt-oss] Support chat completion api (#22342)
34	0	vllm/entrypoints/harmony_utils.py
4	0	vllm/entrypoints/openai/protocol.py
145	24	vllm/entrypoints/openai/serving_chat.py

[54991c548] Roger Wang 2025-08-06 [gpt-oss] add model to supported models doc (#22336)
1	0	docs/models/supported_models.md

[178d03fbd] Woosuk Kwon 2025-08-06 [gpt-oss] Add Tool/ConversationContext classes and harmony_utils (#22340)
177	0	vllm/entrypoints/context.py
111	0	vllm/entrypoints/harmony_utils.py
87	0	vllm/entrypoints/tool.py

[fa00c5d75] Isotr0py 2025-08-06 [Misc] Clean up duplicated hf overrides (#22311)
3	48	tests/models/multimodal/test_tensor_schema.py
7	55	tests/models/test_initialization.py
61	0	tests/models/utils.py

[134a8ee8f] Woosuk Kwon 2025-08-06 [gpt-oss] Add openai-harmony as default dependency (#22332)
1	0	requirements/common.txt

[90ec00693] Yongye Zhu 2025-08-05 [gpt-oss] flashinfer attention sink init (#22330)
10	0	vllm/v1/attention/backends/flashinfer.py

[a47e6ffe9] Chen Zhang 2025-08-05 [GptOss] Add GptOss reasoning parser to support structure output (#22322)
3	3	vllm/model_executor/models/config.py
2	0	vllm/reasoning/__init__.py
64	0	vllm/reasoning/gptoss_reasoning_parser.py

[98a3a8102] Woosuk Kwon 2025-08-05 [ROCm] Add attention sink to use_rocm_custom_paged_attention (#22329)
6	5	vllm/platforms/rocm.py

[de98252f4] Woosuk Kwon 2025-08-05 Add GPT-OSS model code and config [1/N] (#22327)
1	0	tests/models/registry.py
29	0	vllm/model_executor/models/config.py
472	0	vllm/model_executor/models/gpt_oss.py
1	0	vllm/model_executor/models/registry.py

[796bae07c] Harry Mellor 2025-08-06 Update transformers to `v4.55` (#21931)
1	1	requirements/common.txt
1	1	requirements/test.in
3	3	requirements/test.txt
4	0	tests/models/multimodal/generation/test_common.py
15	9	tests/models/registry.py
4	0	tests/quantization/test_experts_int8.py
8	4	vllm/model_executor/models/interfaces_base.py
6	5	vllm/model_executor/models/qwen2_vl.py
10	7	vllm/model_executor/models/transformers.py
2	8	vllm/model_executor/models/utils.py
3	1	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
176	0	vllm/transformers_utils/configs/ovis.py

[6e2092435] Woosuk Kwon 2025-08-05 Add attention sink in attention backends (#22320)
27	6	vllm/attention/ops/chunked_prefill_paged_decode.py
15	3	vllm/attention/ops/prefix_prefill.py
28	2	vllm/attention/ops/triton_unified_attention.py
16	3	vllm/envs.py
10	0	vllm/v1/attention/backends/flash_attn.py
56	19	vllm/v1/attention/backends/triton_attn.py
24	12	vllm/v1/attention/backends/utils.py

[dd16bdc79] Woosuk Kwon 2025-08-05 Increase openai-python version (#22316)
1	1	requirements/common.txt

[e3c876dca] Woosuk Kwon 2025-08-05 Upgrade FA3 for attention sink (#22313)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[5d5d419ca] Gregory Shtrasberg 2025-08-05 [Bugfix][CI/Build][ROCm] Make sure to use the headers from the build folder on ROCm (#22264)
6	2	cmake/utils.cmake

[302962e80] Rui Qiao 2025-08-05 [Bugfix] Skip dead and non-GPU nodes for Ray DP engine allocation (#22275)
11	2	vllm/v1/engine/utils.py

[7e6544c79] Benjamin Chislett 2025-08-05 [Perf] Parallelize fill_bitmask to accelerate high-throughput guided decoding (#21862)
92	39	vllm/v1/structured_output/__init__.py
6	1	vllm/v1/structured_output/backend_xgrammar.py
7	2	vllm/v1/worker/gpu_model_runner.py

[8e6c7e873] Jee Jee Li 2025-08-06 [Bugfix] Fix MoE BNB version (#22260)
4	4	vllm/model_executor/layers/quantization/bitsandbytes.py

[6a5153043] Michael Goin 2025-08-05 [Bugfix] Fix 3D input passed into cutlass_scaled_mm (#22278)
20	15	vllm/_custom_ops.py

[35509fc5b] Michael Goin 2025-08-05 [Bugfix] Remove faulty test for oot attention backend (#22286)
0	10	tests/plugins_tests/test_platform_plugins.py

[4b29d2784] Siyuan Liu 2025-08-05 [CI][TPU] Fix docker clean up (#22271)
1	2	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
0	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
1	1	.buildkite/scripts/tpu/config_v6e_1.env
0	2	.buildkite/scripts/tpu/docker_run_bm.sh
1	1	.buildkite/scripts/tpu/quantized_v6e_1.env

[59a0b8554] youkaichao 2025-08-06 [bugfix] fix blackwell deepep installation (#22255)
5	5	tools/ep_kernels/README.md
7	1	tools/ep_kernels/install_python_libraries.sh

[469b3ffaa] Giancarlo Delfin 2025-08-05 [V1] port xformers backend to v1 (#21342)
2	0	tests/v1/attention/utils.py
1	0	vllm/engine/arg_utils.py
4	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
0	1	vllm/v1/attention/backends/tree_attn.py
430	0	vllm/v1/attention/backends/xformers.py

[ae87ddd04] Wentao Ye 2025-08-05 [Refactor] Remove Unused Environment Variable `VLLM_NO_DEPRECATION_WARNING` (#22199)
0	5	vllm/envs.py

[a7cb6101c] Michael Goin 2025-08-05 [CI/Build] Update flashinfer to 0.2.9 (#22233)
1	1	docker/Dockerfile
1	1	setup.py

[c494f96fb] Michael Goin 2025-08-05 Use UV_LINK_MODE=copy in Dockerfile to avoid hardlink fail (#22128)
10	0	docker/Dockerfile

[0c275ad5a] Nicolò Lucchesi 2025-08-05 [V0 Deprecation][TPU] Remove V1 flag check from tests (#22248)
0	7	tests/v1/tpu/test_mha_attn.py
0	7	tests/v1/tpu/test_multimodal.py
1	7	tests/v1/tpu/test_sampler.py

[74333ae2f] Ning Xie 2025-08-05 [Misc] correct static type check for GroupCoordinator (#21946)
1	0	vllm/distributed/device_communicators/ray_communicator.py
3	0	vllm/distributed/eplb/eplb_state.py
25	4	vllm/distributed/parallel_state.py

[83156c7b8] elvischenv 2025-08-05 [NVIDIA] Support Flashinfer TRT-LLM Prefill Attention Kernel (#22095)
1	1	.buildkite/test-pipeline.yaml
0	1	benchmarks/kernels/{benchmark_trtllm_attention.py => benchmark_trtllm_decode_attention.py}
250	0	benchmarks/kernels/benchmark_trtllm_prefill_attention.py
293	0	tests/kernels/attention/test_flashinfer_trtllm_attention.py
0	138	tests/kernels/attention/test_flashinfer_trtllm_decode_attention.py
2	2	vllm/attention/backends/flashinfer.py
3	3	vllm/envs.py
7	10	vllm/utils/flashinfer.py
145	80	vllm/v1/attention/backends/flashinfer.py

[4771df7b2] Wentao Ye 2025-08-05 [Feature] Non-contiguous Support for FP8 Quantization (#21961)
170	89	csrc/quantization/fp8/common.cu
0	107	csrc/quantization/fp8/common.cuh
33	0	tests/quantization/test_fp8.py
4	5	vllm/_custom_ops.py

[05fae0217] Benji Beck 2025-08-05 Migrate KimiVLImagePixelInputs to TensorSchema (#21769)
15	9	vllm/model_executor/models/kimi_vl.py

[d1bf1b971] Nicolò Lucchesi 2025-08-05 [Docs][TPU] Highlight TPU Software version selection (#22242)
4	1	docs/getting_started/installation/google_tpu.md

[586f28678] wang.yuqi 2025-08-05 [Model] Pooling model activation supports per request control by PoolingParams (#20538)
67	0	tests/entrypoints/llm/test_classify.py
56	0	tests/entrypoints/llm/test_embedding.py
66	0	tests/entrypoints/llm/test_reward.py
69	0	tests/entrypoints/llm/test_score.py
31	0	tests/entrypoints/openai/test_classification.py
34	0	tests/entrypoints/openai/test_embedding.py
38	0	tests/entrypoints/openai/test_rerank.py
41	0	tests/entrypoints/openai/test_score.py
127	0	tests/models/language/pooling/test_override_pooler_config.py
2	2	tests/models/language/pooling/test_reward.py
7	0	tests/models/utils.py
106	0	tests/test_pooling_params.py
15	15	vllm/config.py
19	3	vllm/entrypoints/llm.py
15	5	vllm/entrypoints/openai/protocol.py
106	116	vllm/model_executor/layers/pooler.py
32	0	vllm/model_executor/models/config.py
0	2	vllm/model_executor/models/jamba.py
1	4	vllm/model_executor/models/jina_vl.py
0	3	vllm/model_executor/models/qwen2_rm.py
116	23	vllm/pooling_params.py

[811ac13d0] Cyrus Leung 2025-08-05 [Core] Factor out common logic for MM budget calculation (#22228)
108	115	vllm/v1/worker/gpu_model_runner.py
89	108	vllm/v1/worker/tpu_model_runner.py
109	0	vllm/v1/worker/utils.py

[e79a12fc3] Michael Goin 2025-08-05 [UX] Fail if an invalid attention backend is specified (#22217)
5	15	tests/kernels/attention/test_attention_selector.py
4	0	vllm/attention/selector.py

[cdfd6871a] Cyrus Leung 2025-08-05 [Bugfix] Misaligned params in TreeAttentionImpl (#22226)
1	5	vllm/v1/attention/backends/tree_attn.py

[4b3e4474d] ZiTian.Zhao 2025-08-05 Optimize configuration access with LRU cache in custom ops (#22204)
9	1	vllm/config.py
4	4	vllm/model_executor/custom_op.py

[bd3db7f46] Ning Xie 2025-08-05 [Misc] log more detailed message for ensure_model_parallel_initialized (#22144)
6	6	vllm/distributed/parallel_state.py

[29b97c099] Ning Xie 2025-08-05 [Doc] add backend to doc string of initialize_model_parallel (#22142)
1	0	vllm/distributed/parallel_state.py

[7b455cf1c] elvischenv 2025-08-05 [Misc] Remove pass_config from CompilationConfig dump_json excluded (#21911)
9	1	vllm/config.py

[8a6e108e7] tlipoca9 2025-08-05 fix: kimi_k2 return empty tool call list (#22149)
4	4	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py

[d7b28f341] Wentao Ye 2025-08-04 [Log] DeepGEMM Update Log for Unaligned Problem Size (#22208)
19	2	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
2	4	vllm/model_executor/layers/fused_moe/fused_moe.py
2	2	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[6fa41e0c3] Yuxuan Zhang 2025-08-05 self.gate dtype update for GLM-4.5 (#22203)
1	1	docs/models/supported_models.md
1	1	tests/models/registry.py
2	1	vllm/model_executor/models/glm4_moe.py

[031ca762d] Gregory Shtrasberg 2025-08-04 [ROCm][Bugfix] Compilation passes fix (#22202)
4	2	vllm/compilation/pass_manager.py

[6ad6b8e11] TJian 2025-08-04 [FEAT] Refactor ROPE into module (#22192)
0	1967	vllm/model_executor/layers/rotary_embedding.py
190	0	vllm/model_executor/layers/rotary_embedding/__init__.py
237	0	vllm/model_executor/layers/rotary_embedding/base.py
105	0	vllm/model_executor/layers/rotary_embedding/common.py
131	0	vllm/model_executor/layers/rotary_embedding/deepseek_scaling_rope.py
188	0	vllm/model_executor/layers/rotary_embedding/dual_chunk_rope.py
41	0	vllm/model_executor/layers/rotary_embedding/dynamic_ntk_alpha_rope.py
67	0	vllm/model_executor/layers/rotary_embedding/dynamic_ntk_scaling_rope.py
115	0	vllm/model_executor/layers/rotary_embedding/linear_scaling_rope.py
54	0	vllm/model_executor/layers/rotary_embedding/llama3_rope.py
74	0	vllm/model_executor/layers/rotary_embedding/llama4_vision_rope.py
670	0	vllm/model_executor/layers/rotary_embedding/mrope.py
42	0	vllm/model_executor/layers/rotary_embedding/ntk_scaling_rope.py
129	0	vllm/model_executor/layers/rotary_embedding/phi3_long_rope_scaled_rope.py
68	0	vllm/model_executor/layers/rotary_embedding/yarn_scaling_rope.py

[f4f4e7ef2] lkchen 2025-08-04 [V0 deprecation][P/D] Deprecate v0 `KVConnectorBase` code (1/2) (#21785)
0	1	.buildkite/test-pipeline.yaml
0	120	tests/kv_transfer/test_disagg.py
4	136	vllm/distributed/kv_transfer/kv_connector/base.py
13	55	vllm/distributed/kv_transfer/kv_connector/factory.py
0	99	vllm/distributed/kv_transfer/kv_connector/lmcache_connector.py
0	203	vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py
0	329	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
4	5	vllm/distributed/kv_transfer/kv_connector/utils.py
4	4	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
0	77	vllm/distributed/kv_transfer/kv_connector_agent.py
2	7	vllm/distributed/kv_transfer/kv_transfer_state.py
1	1	vllm/v1/core/sched/scheduler.py
3	3	vllm/v1/worker/kv_connector_model_runner_mixin.py

[5ea71ff46] Giancarlo Delfin 2025-08-04 [V1] reduce block size for tree attention correctness test to fix 'ou… (#22207)
1	1	tests/v1/spec_decode/test_tree_attention.py

[717581763] Woosuk Kwon 2025-08-04 Revert "[Bugfix] V1 Fix the cursor leakage issue during request scheduling." (#22223)
2	95	tests/v1/core/test_scheduler.py
1	5	vllm/v1/core/sched/scheduler.py

[2dffac464] PiteXChen 2025-08-05 [Bugfix] V1 Fix the cursor leakage issue during request scheduling. (#21173)
95	2	tests/v1/core/test_scheduler.py
5	1	vllm/v1/core/sched/scheduler.py

[bdcb42e45] Po-Han Huang (NVIDIA) 2025-08-05 [NVIDIA] Auto detect modelopt quant and fix DSR1-FP4 weight loading (#22073)
15	0	vllm/config.py
38	15	vllm/model_executor/layers/fused_moe/layer.py
14	0	vllm/transformers_utils/config.py

[c09efff97] Zhonghua Deng 2025-08-05 [Bugfix][V1][P/D]Fix the uneven polling issue in the toy proxy for P2pNcclConnector (#21819)
3	2	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py

[309c1bb82] ericehanley 2025-08-04 [Bug] Update auto_tune.sh to separate benchmarking and profiling. (#21629)
80	43	benchmarks/auto_tune/auto_tune.sh

[9af654cc3] Woosuk Kwon 2025-08-04 [Responses API] Ignore `store=True` and process the request by default (#22185)
28	3	vllm/entrypoints/openai/serving_responses.py
2	1	vllm/envs.py

[a5fff3bd4] Raghav Ravishankar 2025-08-04 Fix Arcee model weight loading: Add custom load_weights (#21725)
1	2	tests/models/registry.py
79	4	vllm/model_executor/models/arcee.py

[1539ced93] Cyrus Leung 2025-08-04 [Doc] Update pooling model docs (#22186)
1	1	docs/models/pooling_models.md
42	27	docs/models/supported_models.md

[54de71d0d] 22quinn 2025-08-04 [Sampler] Support returning all logprobs or logits (#21792)
27	0	tests/v1/sample/test_logprobs.py
4	3	vllm/config.py
4	2	vllm/sampling_params.py
3	2	vllm/v1/engine/logprobs.py
4	1	vllm/v1/engine/processor.py
3	1	vllm/v1/worker/gpu_input_batch.py

[fed5849d3] Isotr0py 2025-08-04 [Bugfix] Fix failing GGUF models test (#22174)
15	6	vllm/transformers_utils/config.py

[c1b4eb048] Weixiao Huang 2025-08-04 [feat] move WEIGHT_SCALE_SUPPORTED into raise block to accelerate RLHF weight loading (#21164)
3	3	vllm/model_executor/layers/fused_moe/layer.py

[a7b8788d2] Jee Jee Li 2025-08-04 [Misc] Modify the organization of GLM series  (#22171)
5	5	docs/models/supported_models.md
2	2	examples/offline_inference/vision_language.py
2	2	tests/distributed/test_pipeline_parallel.py
1	1	tests/lora/test_add_lora.py
1	1	tests/lora/test_chatglm3_tp.py
1	1	tests/models/language/generation/test_common.py
3	3	tests/models/multimodal/generation/test_common.py
2	2	tests/models/multimodal/processing/test_common.py
1	1	tests/models/multimodal/processing/test_glm4_1v.py
5	5	tests/models/registry.py
1	1	tests/tokenization/test_cached_tokenizer.py
3	3	vllm/model_executor/models/chatglm.py
1	1	vllm/model_executor/models/glm4v.py
1	1	vllm/test_utils.py
1	1	vllm/transformers_utils/configs/chatglm.py
1	1	vllm/transformers_utils/tokenizer.py

[8ecb3e9e9] Tyler Michael Smith 2025-08-04 [CI Bugfix] Fix wNa16 kernel not found for test_shared_storage_connector_hashes (#22163)
1	1	tests/v1/kv_connector/unit/test_shared_storage_connector.py

[e5949e5ae] Chenxi Yang 2025-08-03 Remove index_put from MM embeddings merging (#22105)
24	18	vllm/model_executor/models/utils.py

[49bcd893e] ZiTian.Zhao 2025-08-04 [refactor] improve ConstantList exception specificity (#22156)
8	8	vllm/v1/utils.py

[aa7012eb6] Giancarlo Delfin 2025-08-03 Add tree attention backend for v1 (part 1) (#20401)
1	1	tests/v1/attention/test_attention_backends.py
4	2	tests/v1/attention/utils.py
4	3	tests/v1/spec_decode/test_eagle.py
299	0	tests/v1/spec_decode/test_tree_attention.py
48	0	vllm/attention/ops/triton_unified_attention.py
13	0	vllm/config.py
1	1	vllm/engine/arg_utils.py
4	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
452	0	vllm/v1/attention/backends/tree_attn.py
20	0	vllm/v1/attention/backends/utils.py
251	18	vllm/v1/spec_decode/eagle.py

[c2e75b3c1] Ning Xie 2025-08-04 remove duplicate code within cleanup_dist_env_and_memory (#22147)
0	2	vllm/distributed/parallel_state.py

[0d7db16a9] Abirdcfly 2025-08-04 [PD] add test for chat completions endpoint (#21925)
27	14	tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py
2	0	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py

[845420ac2] 22quinn 2025-08-03 [RLHF] Fix torch.dtype not serializable in example (#22158)
4	1	examples/offline_inference/rlhf.py
2	1	examples/offline_inference/rlhf_utils.py

[e27d25a0d] ZiTian.Zhao 2025-08-04 [fix] fix correct assertion syntax error in attention utils. (#22154)
3	1	vllm/v1/attention/backends/utils.py

[6f5478298] Seiji Eicher 2025-08-03 Use `aiohttp` connection pool for benchmarking (#21981)
238	241	vllm/benchmarks/lib/endpoint_request_func.py
3	1	vllm/benchmarks/lib/ready_checker.py
35	5	vllm/benchmarks/serve.py

[6a39ba85f] Isotr0py 2025-08-04 [Bugfix] Fix failing multimodal standard test (#22153)
2	0	tests/models/multimodal/test_tensor_schema.py
8	4	tests/models/registry.py

[d3c18c9cb] Yuxuan Zhang 2025-08-04 fuse fp32 for GLM-4.5 e_score_correction_bias (#22143)
2	3	vllm/model_executor/models/glm4_moe.py

[83f7bbb31] TankNee 2025-08-03 Add chat doc in quick start (#21213)
37	0	docs/getting_started/quickstart.md

[b5dfb94fa] Li, Jiang 2025-08-03 [CI/Build][Bugfix] Fix Qwen2.5 tests in CPU CI via fallback silu_and_mul to torch native implementation (#22145)
3	1	vllm/model_executor/layers/activation.py

[6d98843b3] Woosuk Kwon 2025-08-03 [Responses API] Disable response store by default (#22137)
8	4	tests/v1/entrypoints/openai/responses/conftest.py
5	2	tests/v1/entrypoints/openai/responses/test_image.py
21	4	vllm/entrypoints/openai/serving_responses.py
12	0	vllm/envs.py

[aefeea0fd] David Ben-David 2025-08-03 [V1] [P/D] Refactor KV Connector Path (#21980)
17	3	tests/v1/kv_connector/unit/test_output_aggreagator.py
5	3	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
5	3	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
5	3	tests/v1/kv_connector/unit/utils.py
7	9	vllm/distributed/kv_transfer/kv_connector/utils.py
7	6	vllm/sequence.py
7	5	vllm/v1/core/sched/scheduler.py
8	5	vllm/v1/outputs.py
9	19	vllm/v1/worker/gpu_model_runner.py
12	10	vllm/v1/worker/gpu_worker.py
54	9	vllm/v1/worker/kv_connector_model_runner_mixin.py
5	4	vllm/v1/worker/tpu_model_runner.py

[24d1dffbe] H 2025-08-03 [executor] feat: add supports_pp attr to executors (#21786)
12	8	vllm/engine/arg_utils.py
1	0	vllm/executor/executor_base.py
2	0	vllm/v1/executor/multiproc_executor.py
2	0	vllm/v1/executor/ray_distributed_executor.py

[7de45db9a] Ning Xie 2025-08-03 [Misc] update doc comment for send (#22026)
1	1	vllm/distributed/device_communicators/base_device_communicator.py
1	1	vllm/distributed/device_communicators/cuda_communicator.py
1	1	vllm/distributed/parallel_state.py

[789562c28] Roberto L. Castro 2025-08-03 Support CUTLASS NVFP4 (w4a4) for Blackwell Geforce GPUs (SM120) (#21309)
20	1	CMakeLists.txt
3	3	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu
8	6	csrc/quantization/fp4/nvfp4_quant_entry.cu
1	1	csrc/quantization/fp4/nvfp4_quant_kernels.cu
12	2	csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu
285	0	csrc/quantization/fp4/nvfp4_scaled_mm_sm120_kernels.cu

[3f36c325f] Ye (Charlotte) Qi 2025-08-03 [Benchmark] Support ready check timeout in `vllm bench serve` (#21696)
2	2	vllm/benchmarks/latency.py
3	0	vllm/benchmarks/lib/__init__.py
0	0	vllm/benchmarks/{ => lib}/endpoint_request_func.py
70	0	vllm/benchmarks/lib/ready_checker.py
0	0	vllm/benchmarks/{ => lib}/utils.py
17	7	vllm/benchmarks/serve.py
2	2	vllm/benchmarks/throughput.py

[3dddbf1f2] Isotr0py 2025-08-03 [Misc] Add tensor schema test coverage for multimodal models (#21754)
2	1	.buildkite/test-pipeline.yaml
1	1	tests/conftest.py
199	0	tests/models/multimodal/test_tensor_schema.py
4	3	tests/models/registry.py
2	1	vllm/model_executor/models/deepseek_vl2.py
11	6	vllm/model_executor/models/keye.py
3	3	vllm/transformers_utils/processors/deepseek_vl2.py

[337eb23bc] jiahanc 2025-08-03 [Fix] Fix llama4 modelopt weight loading error (#22107)
5	4	vllm/model_executor/models/mllama4.py

[2ff46b882] Rui Qiao 2025-08-02 [Misc] Bump ray to 2.48.0 (#22123)
1	1	requirements/cuda.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
15	7	requirements/test.txt

[554df8a6a] Xiao 2025-08-02 Revert "[compile][startup] Disable C++ compilation of symbolic shapes" (#22122)
2	9	vllm/compilation/decorators.py

[73e1b9b1d] Yan Ma 2025-08-02 [xpu]support moe models on XPU platform (#21643)
46	1	vllm/model_executor/layers/fused_moe/layer.py

[4abfd8796] Thomas Parnell 2025-08-02 [V1] [Hybrid] Validate compatibility of attention backend batch reordering at init time (#21557)
12	16	vllm/v1/attention/backends/flashinfer.py
6	14	vllm/v1/attention/backends/mamba_attn.py
7	15	vllm/v1/attention/backends/mla/common.py
0	3	vllm/v1/attention/backends/rocm_aiter_fa.py
4	8	vllm/v1/attention/backends/utils.py
33	1	vllm/v1/worker/cpu_model_runner.py
34	15	vllm/v1/worker/gpu_model_runner.py

[f5d0f4784] Cyrus Leung 2025-08-02 [Frontend] Improve error message for too many mm items (#22114)
2	8	tests/entrypoints/test_chat_utils.py
4	6	tests/multimodal/test_processing.py
10	17	vllm/entrypoints/chat_utils.py
35	19	vllm/multimodal/processing.py
1	1	vllm/multimodal/profiling.py

[b690e3482] Chih-Chieh Yang 2025-08-02 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)
40	34	tests/kernels/mamba/test_mamba_ssm.py
13	10	tests/kernels/mamba/test_mamba_ssm_ssd.py
4	2	vllm/model_executor/layers/mamba/mamba_mixer.py
32	22	vllm/model_executor/layers/mamba/mamba_mixer2.py
8	8	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
5	14	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
16	12	vllm/model_executor/layers/mamba/ops/ssd_combined.py
4	2	vllm/model_executor/models/phi4flash.py
22	14	vllm/model_executor/models/plamo2.py

[25373b6c6] Yuxuan Zhang 2025-08-02 for glm-4.1V update (#22000)
2	1	docs/models/supported_models.md
6	5	tests/models/registry.py
1	1	tests/tool_use/test_glm4_moe_tool_parser.py
1	1	vllm/model_executor/layers/rotary_embedding.py
13	8	vllm/model_executor/models/glm4_1v.py
1	0	vllm/model_executor/models/registry.py

[58eee5f2e] Vadim Gimpelson 2025-08-02 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)
4	3	vllm/transformers_utils/tokenizer.py

[067c34a15] Roger Wang 2025-08-02 docs: remove deprecated disable-log-requests flag (#22113)
0	1	.buildkite/scripts/tpu/run_bm.sh
5	5	benchmarks/README.md
0	1	benchmarks/auto_tune/auto_tune.sh
1	2	benchmarks/benchmark_serving.py
1	1	benchmarks/benchmark_serving_structured_output.py
0	8	docs/design/p2p_nccl_connector.md
1	1	docs/models/supported_models.md
0	2	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh
1	2	examples/online_serving/prometheus_grafana/README.md
0	2	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh
1	1	tests/entrypoints/openai/correctness/test_lmeval.py
0	2	tests/entrypoints/openai/test_chunked_prompt.py
0	1	tests/models/quantization/test_bitsandbytes.py
0	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
0	2	tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh
0	3	tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh
0	2	tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh
1	1	tests/v1/sample/test_logprobs_e2e.py
3	2	vllm/utils/__init__.py

[c64861d63] Chih-Chieh Yang 2025-08-02 [Bugfix] Mamba2 remove bugged initial state condition in chunk scan (#22034)
2	9	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py

[8564dc944] Yong Hoon Shin 2025-08-01 Fix test_kv_sharing_fast_prefill flakiness (#22038)
30	5	tests/v1/e2e/test_kv_sharing_fast_prefill.py

[4ac843735] Rui Qiao 2025-08-01 [Misc] Getting and passing ray runtime_env to workers (#22040)
33	0	tests/config/test_config_generation.py
5	0	vllm/config.py
11	0	vllm/engine/arg_utils.py
5	2	vllm/executor/ray_utils.py
22	0	vllm/ray/lazy_utils.py
1	11	vllm/utils/__init__.py

[d3a6f2120] vllmellm 2025-08-02 [FEAT][ROCm] Enable running Flash Attention as ViT attn backend for Qwen-VL models on ROCm platform. (#22069)
13	5	vllm/model_executor/models/qwen2_5_vl.py
13	5	vllm/model_executor/models/qwen2_vl.py
7	29	vllm/model_executor/models/vision.py
14	0	vllm/platforms/cuda.py
5	0	vllm/platforms/interface.py
12	0	vllm/platforms/rocm.py

[0edaf752d] Sage Moore 2025-08-01 [Attention][DBO] Add support for "splitting" the CommonAttentionMetadata (#21153)
157	0	tests/v1/attention/test_attention_splitting.py
83	0	vllm/v1/attention/backends/utils.py

[6e8d8c4af] Wentao Ye 2025-08-01 [Test] Add Unit Test for Batched DeepGEMM (#21559)
103	0	tests/kernels/moe/test_batched_deepgemm.py
2	6	tests/kernels/moe/test_deepgemm.py
2	2	vllm/utils/deep_gemm.py

[8d524ce79] Nick Hill 2025-08-02 [BugFix] Improve internal DP load balancing (#21617)
3	0	vllm/entrypoints/openai/api_server.py
4	0	vllm/v1/engine/async_llm.py
73	37	vllm/v1/engine/coordinator.py
8	5	vllm/v1/engine/core.py
29	17	vllm/v1/engine/core_client.py
4	0	vllm/v1/metrics/stats.py
1	0	vllm/v1/utils.py

[9f9c38c39] Dipika Sikka 2025-08-01 [Speculators][Speculative Decoding] Add Qwen Eagle3 Support (#21835)
12	2	tests/speculative_decoding/speculators/test_eagle3.py
12	3	vllm/config.py
15	6	vllm/model_executor/models/qwen2.py
7	0	vllm/model_executor/models/qwen3.py

[a65f46be5] Varun Sundar Rabindranath 2025-08-02 [Misc] DeepGemmExperts : Avoid JIT generation in the hot-path (#21955)
9	0	vllm/envs.py
76	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
7	0	vllm/utils/deep_gemm.py

[57393715e] Nicolò Lucchesi 2025-08-02 [Misc] `VLLM_TARGET_DEVICE.lower()` (#22101)
1	1	vllm/envs.py

[ee2eb6ecd] vllmellm 2025-08-02 [Model] Qwen2.5 VL SiLU-and-Mul (#22066)
21	23	vllm/model_executor/models/qwen2_5_vl.py

[23322431c] fhl2000 2025-08-02 [V1][CUDA] Full cudagraph support for FlashInfer (#21367)
5	2	vllm/v1/attention/backends/flash_attn.py
323	34	vllm/v1/attention/backends/flashinfer.py
3	1	vllm/v1/attention/backends/mla/flashmla.py
3	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
4	2	vllm/v1/attention/backends/triton_attn.py
17	1	vllm/v1/attention/backends/utils.py
17	7	vllm/v1/worker/gpu_model_runner.py
5	0	vllm/v1/worker/gpu_worker.py

[3654847db] JartX 2025-08-02 feat: Add Support GPTQ Quantization MOE on ROCM vllm serve (#21733)
2	2	vllm/model_executor/layers/fused_moe/fused_moe.py
19	3	vllm/model_executor/layers/quantization/gptq.py

[eefbf4a68] Wentao Ye 2025-08-01 [Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)
156	0	benchmarks/kernels/benchmark_reshape_and_cache_flash.py
69	23	csrc/cache_kernels.cu

[88faa466d] Michael Goin 2025-08-01 [CI] Initial tests for SM100 Blackwell runner (#21877)
21	3	.buildkite/test-pipeline.yaml
9	6	tests/compile/test_fusion_all_reduce.py
0	5	tests/kernels/quantization/test_cutlass_scaled_mm.py

[881e1af43] Nick Hill 2025-08-01 [BugFix] Harden distributed DP startup (#21538)
3	0	vllm/utils/__init__.py
12	0	vllm/v1/engine/coordinator.py
41	20	vllm/v1/engine/core.py

[d84b97a3e] XiongfeiWei 2025-08-01 Add lora test for tp>1 case for TPU. (#21970)
16	7	tests/tpu/lora/test_lora.py

[d33175948] Rui Qiao 2025-08-01 Introduce RayPPCommunicator for ray-based PP (#21660)
257	0	vllm/distributed/device_communicators/ray_communicator.py
8	0	vllm/envs.py
15	0	vllm/executor/ray_distributed_executor.py

[9659bc7f2] Animesh Jain 2025-08-01 [compile][startup] Disable C++ compilation of symbolic shapes (#20836)
9	2	vllm/compilation/decorators.py

[3277e8f9e] Michael Goin 2025-08-01 Fix pre-commit failure for SECURTIY.md (#22102)
5	1	SECURITY.md

[8d705996d] Jee Jee Li 2025-08-02 [Misc] Minor enhancement of benchmark_moe (#22068)
8	1	benchmarks/kernels/benchmark_moe.py

[38c8bce8b] Harry Mellor 2025-08-01 Enable headless models for pooling in the Transformers backend (#21767)
1	0	tests/models/registry.py
22	6	tests/models/test_transformers.py
7	2	vllm/config.py
2	1	vllm/model_executor/models/registry.py
12	0	vllm/model_executor/models/transformers.py

[ac45c44d9] Varun Sundar Rabindranath 2025-08-01 [Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)
8	5	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py

[d6664664b] Huzaifa Sidhpurwala 2025-08-01 security policy: take 1 (#21119)
32	4	SECURITY.md

[b879ecd6e] rongfu.leng 2025-08-02 [Bugfix] fix when skip tokenizer init (#21922)
26	0	tests/v1/engine/test_llm_engine.py
7	2	vllm/v1/engine/processor.py

[3f8e95217] Isotr0py 2025-08-02 [Bugfix] Fix glm4.1v video inference issue (#22067)
51	0	tests/models/multimodal/processing/test_glm4_1v.py
2	6	vllm/model_executor/models/glm4_1v.py

[326a1b001] Harry Mellor 2025-08-01 Improve documentation of `ModelConfig.try_get_generation_config` to prevent future confusion (#21526)
22	6	vllm/config.py

[2d7b09b99] Harry Mellor 2025-08-01 Deprecate `--disable-log-requests` and replace with `--enable-log-requests` (#21739)
0	1	.buildkite/nightly-benchmarks/README.md
0	1	.buildkite/nightly-benchmarks/tests/genai-perf-tests.json
0	6	.buildkite/nightly-benchmarks/tests/nightly-tests.json
0	6	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc2.json
0	6	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc3.json
0	5	.buildkite/nightly-benchmarks/tests/serving-tests-cpu.json
1	5	.buildkite/nightly-benchmarks/tests/serving-tests.json
0	1	tests/config/test_mp_reducer.py
1	1	tests/mq_llm_engine/test_load.py
1	3	tests/v1/engine/test_async_llm.py
0	1	tests/v1/test_async_llm_dp.py
26	4	vllm/engine/arg_utils.py
16	10	vllm/engine/async_llm_engine.py
19	8	vllm/engine/multiprocessing/engine.py
6	6	vllm/entrypoints/openai/api_server.py
3	3	vllm/entrypoints/openai/run_batch.py
6	0	vllm/utils/__init__.py
18	12	vllm/v1/engine/async_llm.py

[97608dc27] David Xia 2025-08-01 [Docs] use `uv` in CPU installation docs (#22089)
6	6	docs/getting_started/installation/cpu/apple.inc.md
14	8	docs/getting_started/installation/cpu/build.inc.md
28	17	docs/getting_started/installation/cpu/s390x.inc.md

[3146519ad] Nick Hill 2025-08-01 [BugFix] Don't change title of top-level process (#22032)
6	5	vllm/entrypoints/cli/serve.py
2	2	vllm/entrypoints/openai/api_server.py

[8026a335a] Richard Zou 2025-08-01 [BugFix] Update AttnFusionPass cache key (#21947)
3	0	vllm/compilation/fusion_attn.py
2	1	vllm/compilation/inductor_pass.py

[a59cd9d9f] Wentao Ye 2025-08-01 [Refactor] Fix Compile Warning #1444-D (#21462)
4	2	csrc/moe/topk_softmax_kernels.cu

[5c54d9759] Abirdcfly 2025-08-01 [Bugfix][PD] set max_completion_tokens=1 if req has this value (#21841)
2	0	examples/online_serving/disaggregated_serving/disagg_proxy_demo.py
2	0	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py

[0a6d305e0] Gamhang 2025-08-01 feat(multimodal): Add customizable background color for RGBA to RGB conversion (#22052)
44	0	docs/features/multimodal_inputs.md
114	1	tests/multimodal/test_image.py
32	5	vllm/multimodal/image.py

[f81c1bb05] Michael Goin 2025-08-01 [Bugfix] Check NVIDIA artifactory is accessible before using flashinfer cubin kernels (#21893)
2	44	vllm/attention/backends/flashinfer.py
80	1	vllm/utils/flashinfer.py
3	46	vllm/v1/attention/backends/flashinfer.py
8	8	vllm/v1/attention/backends/mla/common.py

[fb0e0d46f] Harry Mellor 2025-08-01 Fix `get_kwargs` for case where type hint is `list[Union[str, type]]` (#22016)
6	1	tests/engine/test_arg_utils.py
6	4	vllm/engine/arg_utils.py

[26b5f7bd2] TJian 2025-08-01 [BUG] [ROCm] Fix import bug on ROCm (#22083)
1	1	vllm/compilation/pass_manager.py

[dfbc1f888] Dipika Sikka 2025-08-01 [Speculative Decoding] Add `speculators` config support (#21345)
16	0	tests/speculative_decoding/speculators/test_eagle3.py
16	4	vllm/config.py
21	1	vllm/engine/arg_utils.py
23	3	vllm/model_executor/models/llama_eagle3.py
29	3	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
2	0	vllm/transformers_utils/configs/speculators/__init__.py
32	0	vllm/transformers_utils/configs/speculators/algos.py
91	0	vllm/transformers_utils/configs/speculators/base.py

[87c94bc87] Harry Mellor 2025-08-01 Revert "Update sampling_metadata.py (#21937)" (#22088)
6	6	vllm/model_executor/sampling_metadata.py

[28b18cc74] Jee Jee Li 2025-08-01 [Quantization] Enable BNB support for InternS1 (#21953)
25	14	vllm/model_executor/model_loader/bitsandbytes_loader.py
18	2	vllm/model_executor/utils.py

[493148698] WeiQing Chen 2025-08-01 [Doc] Added warning of speculating with draft model (#22047)
4	0	docs/features/spec_decode.md

[0f81b310d] Woosuk Kwon 2025-08-01 [Misc] Remove upper bound in openai package version (#22060)
1	1	requirements/common.txt

[e6680f9e2] wuhang 2025-08-01 [Bugfix] Add log prefix in non-dp mode engine core (#21889)
2	9	vllm/entrypoints/cli/serve.py
4	8	vllm/entrypoints/openai/api_server.py
4	38	vllm/executor/multiproc_worker_utils.py
54	1	vllm/utils/__init__.py
4	18	vllm/v1/engine/core.py
7	7	vllm/v1/executor/multiproc_executor.py

[27a145e89] Roger Wang 2025-08-01 [Doc] Add example for Step3-VL (#22061)
157	127	examples/offline_inference/vision_language.py
113	84	examples/offline_inference/vision_language_multi_image.py

[da31f6ad3] Simon Mo 2025-08-01 Revert precompile wheel changes (#22055)
10	17	docker/Dockerfile
8	16	requirements/test.txt
87	95	setup.py
2	9	vllm/envs.py

[98df153ab] Sungyoon Jeong 2025-08-01 [Frontend] Align tool_choice="required" behavior with OpenAI when tools is empty (#21052)
9	0	vllm/entrypoints/openai/protocol.py

[e0f63e4a3] Zebing Lin 2025-08-01 [Core] Avoid repeated len(block_token_ids) check in hash_request_tokens (#21781)
2	4	vllm/v1/core/kv_cache_utils.py

[b4e081cb1] Cyrus Leung 2025-08-01 [Bugfix] Disable multi-modal preprocessor cache for DP (#21896)
6	0	vllm/config.py
12	0	vllm/engine/arg_utils.py
3	2	vllm/entrypoints/cli/serve.py

[79731a79f] Hongsheng Liu 2025-08-01 [Doc] Fix a syntax error of example code in structured_outputs.md (#22045)
1	1	docs/features/structured_outputs.md

[53d7c3927] Aviad Rossmann 2025-08-01 Update sampling_metadata.py (#21937)
6	6	vllm/model_executor/sampling_metadata.py

[61dcc280f] Cyrus Leung 2025-08-01 [Doc] Add Voxtral to Supported Models page (#22059)
1	0	docs/models/supported_models.md

[0f46a780d] Kyle Sayers 2025-08-01 [Model] [Quantization] Support quantization for Gemma3n (#21974)
12	4	vllm/model_executor/models/gemma3n.py

[e1a7fe4af] Mickaël Seznec 2025-08-01 [BugFix] fix: aot passes kvcache dtype information (#19750)
21	4	vllm/v1/attention/backends/flash_attn.py

[82de9b9d4] Cyrus Leung 2025-08-01 [Misc] Automatically resolve HF processor init kwargs (#22005)
19	19	examples/offline_inference/vision_language.py
0	6	tests/lora/test_qwen2vl.py
26	1	tests/models/multimodal/generation/test_common.py
12	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	1	tests/models/multimodal/processing/test_transformers.py
1	2	tests/models/registry.py
70	37	tests/multimodal/test_processing.py
11	1	vllm/config.py
8	9	vllm/inputs/registry.py
3	9	vllm/model_executor/models/aya_vision.py
18	18	vllm/model_executor/models/deepseek_vl2.py
0	6	vllm/model_executor/models/florence2.py
2	2	vllm/model_executor/models/fuyu.py
4	4	vllm/model_executor/models/glm4_1v.py
1	15	vllm/model_executor/models/h2ovl.py
1	19	vllm/model_executor/models/hyperclovax_vision.py
1	9	vllm/model_executor/models/idefics3.py
3	25	vllm/model_executor/models/internvl.py
2	82	vllm/model_executor/models/keye.py
11	35	vllm/model_executor/models/llava.py
2	4	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/mllama4.py
3	21	vllm/model_executor/models/nemotron_vl.py
1	15	vllm/model_executor/models/nvlm_d.py
3	5	vllm/model_executor/models/ovis.py
0	11	vllm/model_executor/models/phi3v.py
8	14	vllm/model_executor/models/phi4_multimodal.py
6	15	vllm/model_executor/models/phi4mm.py
7	42	vllm/model_executor/models/qwen2_5_omni_thinker.py
2	17	vllm/model_executor/models/qwen2_5_vl.py
4	14	vllm/model_executor/models/qwen2_audio.py
4	78	vllm/model_executor/models/qwen2_vl.py
20	66	vllm/model_executor/models/skyworkr1v.py
1	9	vllm/model_executor/models/smolvlm.py
5	7	vllm/model_executor/models/tarsier.py
0	5	vllm/model_executor/models/transformers.py
5	15	vllm/model_executor/models/ultravox.py
7	8	vllm/model_executor/models/whisper.py
59	35	vllm/transformers_utils/processor.py
0	43	vllm/utils/__init__.py

[ad57f23f6] Charent 2025-08-01 [Bugfix] Fix: Fix multi loras with tp >=2 and LRU cache (#20873)
1	0	.buildkite/test-pipeline.yaml
158	0	tests/lora/test_multi_loras_with_tp.py
5	3	vllm/lora/layers.py

[370064201] Wentao Ye 2025-07-31 [Refactor] Remove Duplicate `per_block_cast_to_fp8`, Remove Dependencies of DeepGEMM (#21787)
6	39	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
3	28	tests/kernels/moe/modular_kernel_tools/utils.py
2	19	tests/kernels/moe/test_cutlass_grouped_gemm.py
6	2	tests/kernels/moe/test_deepgemm.py
2	2	tests/kernels/moe/utils.py
0	19	tests/kernels/quant_utils.py
1	1	tests/kernels/quantization/test_block_fp8.py
35	22	vllm/utils/deep_gemm.py

[0bd409cf0] Michael Goin 2025-07-31 Move flashinfer-python to optional extra `vllm[flashinfer]` (#21959)
1	3	requirements/cuda.txt
3	1	setup.py

[e360316ab] Matthew Bonanni 2025-07-31 Add DeepGEMM to Dockerfile in vllm-base image (#21533)
28	2	docker/Dockerfile
3	2	tests/kernels/moe/test_deepep_deepgemm_moe.py
3	3	tests/kernels/moe/test_deepgemm.py
12	0	vllm/utils/deep_gemm.py

[c3e0e9337] Wentao Ye 2025-07-31 [Feature] Add Flashinfer MoE Support for Compressed Tensor NVFP4 (#21639)
50	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
24	126	vllm/model_executor/layers/quantization/modelopt.py
154	0	vllm/model_executor/layers/quantization/utils/flashinfer_fp4_moe.py
59	0	vllm/model_executor/layers/quantization/utils/nvfp4_moe_support.py

[6e672daf6] Ilya Markov 2025-07-31 Add FlashInfer allreduce RMSNorm Quant fusion (#21069)
1	0	.buildkite/test-pipeline.yaml
104	22	tests/compile/test_fusion_all_reduce.py
12	0	tests/utils.py
488	45	vllm/compilation/collective_fusion.py
1	1	vllm/config.py

[2dff2e21d] Benjamin Chislett 2025-07-31 [Bugfix] Fix MTP weight loading  (#21941)
9	0	vllm/model_executor/models/deepseek_mtp.py

[71470bc4a] Yong Hoon Shin 2025-07-31 [Misc] Add unit tests for chunked local attention (#21692)
196	0	tests/v1/attention/test_chunked_local_attention.py
23	13	tests/v1/attention/utils.py

[9e0726e5b] zhiweiz 2025-07-31 [Meta] Official Eagle mm support, first enablement on llama4 (#20788)
59	7	examples/offline_inference/spec_decode.py
45	16	tests/v1/e2e/test_spec_decode.py
1	0	vllm/model_executor/models/llama4.py
31	4	vllm/model_executor/models/llama4_eagle.py
6	0	vllm/model_executor/models/llama_eagle.py
5	0	vllm/model_executor/models/llama_eagle3.py
50	9	vllm/v1/spec_decode/eagle.py
9	1	vllm/v1/worker/gpu_model_runner.py

[53c21e492] XiongfeiWei 2025-07-31 Update torch_xla pin to 20250730 (#21956)
1	1	docker/Dockerfile.tpu
4	4	requirements/tpu.txt

[0780bb578] Alexei-V-Ivanov-AMD 2025-07-31 Removing amdproduction Tests (#22027)
23	23	.buildkite/test-pipeline.yaml

[58bb90218] Doug Smith 2025-07-31 fix(setup): improve precompiled wheel setup for Docker builds (#22025)
1	0	docker/Dockerfile
16	8	requirements/test.txt
87	116	setup.py

[7349d5268] Zhengxu Chen 2025-07-31 [ez] Remove a trailing space from compilation/decorators.py (#22028)
1	1	vllm/compilation/decorators.py

[948464161] Song 2025-07-31 [Model] Add step3 vl (#21998)
1	0	docs/models/supported_models.md
6	0	tests/models/registry.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
296	0	vllm/entrypoints/openai/tool_parsers/step3_tool_parser.py
2	0	vllm/model_executor/models/registry.py
521	0	vllm/model_executor/models/step3_text.py
1052	0	vllm/model_executor/models/step3_vl.py
2	0	vllm/reasoning/__init__.py
109	0	vllm/reasoning/step3_reasoning_parser.py
4	1	vllm/transformers_utils/config.py
6	0	vllm/transformers_utils/configs/__init__.py
123	0	vllm/transformers_utils/configs/step3_vl.py

[207b750e1] amirkl94 2025-07-31 [NVIDIA] Add SM100 Flashinfer MoE per tensor scale fp8 backend (#21458)
95	18	vllm/model_executor/layers/fused_moe/fused_moe.py
44	31	vllm/model_executor/layers/quantization/fp8.py
28	0	vllm/model_executor/layers/quantization/modelopt.py
100	0	vllm/model_executor/layers/quantization/utils/flashinfer_utils.py
2	0	vllm/utils/flashinfer.py

[5daffe7cf] Nick Hill 2025-07-31 [BugFix] Fix case where `collective_rpc` returns `None` (#22006)
11	2	tests/v1/engine/test_engine_core_client.py
10	6	vllm/v1/serial_utils.py

[2836dd73f] wang.yuqi 2025-07-31 [Model][CI] Let more pooling models support v1 (#21747)
0	8	tests/models/language/pooling/test_classification.py
4	14	tests/models/language/pooling/test_gte.py
0	13	tests/models/language/pooling/test_jina.py
0	6	tests/models/language/pooling/test_qwen3_reranker.py
8	0	vllm/config.py
1	4	vllm/model_executor/models/bert_with_rope.py
1	1	vllm/model_executor/models/config.py
0	2	vllm/model_executor/models/modernbert.py

[d2aab336a] Daniele 2025-07-31 [CI/Build] get rid of unused VLLM_FA_CMAKE_GPU_ARCHES (#21599)
1	2	.buildkite/scripts/hardware_ci/run-gh200-test.sh
0	1	.github/workflows/scripts/build.sh
0	3	docker/Dockerfile
0	3	docker/Dockerfile.nightly_torch
1	2	docs/deployment/docker.md

[9532a6d56] Cyrus Leung 2025-07-31 [Deprecation] Remove deprecated args and methods (#21907)
4	28	vllm/entrypoints/chat_utils.py
0	25	vllm/multimodal/registry.py
1	6	vllm/worker/neuron_model_runner.py

[3e36fcbee] Ning Xie 2025-07-31 [Bugfix]: fix metadata file copy in test_sharded_state_loader (#21830)
5	3	tests/test_sharded_state_loader.py

[055bd3978] Michael Goin 2025-07-30 [CI Bugfix] Fix CI OOM for `test_shared_storage_connector_hashes` (#21973)
3	1	tests/v1/kv_connector/unit/test_shared_storage_connector.py

[0f7919fca] Jee Jee Li 2025-07-31 [Misc] Expand SUPPORTED_HIDDEN_SIZES  for DeepEP low-latency kernels (#21818)
1	1	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py

[61445453d] Michael Goin 2025-07-30 [UX] Rename CUTLASS_MLA_VLLM_V1 to CUTLASS_MLA (#21966)
1	1	vllm/engine/arg_utils.py
5	5	vllm/platforms/cuda.py
1	1	vllm/platforms/interface.py
1	1	vllm/v1/attention/backends/mla/cutlass_mla.py

[ec02e536d] Sanchit Gandhi 2025-07-31 [Bugfix] Relax lang pin for voxtral (#21833)
2	6	vllm/entrypoints/openai/speech_to_text.py
47	6	vllm/model_executor/models/interfaces.py
16	9	vllm/model_executor/models/voxtral.py
15	59	vllm/model_executor/models/whisper.py

[9cb497bfa] Michael Goin 2025-07-30 [Example] Add `async_llm_streaming.py` example for AsyncLLM streaming in python (#21763)
111	0	examples/offline_inference/async_llm_streaming.py

[ca9e2be3e] Zebing Lin 2025-07-30 [Core] Move EngineCoreRequest to Request conversion out of EngineCore (#21627)
26	18	tests/v1/engine/test_engine_core.py
45	29	vllm/v1/engine/core.py
2	1	vllm/v1/engine/core_client.py

[601f856d5] Bram 2025-07-30 [Bugfix] Fix None value handling in trace span creation for cancelled requests (#20272)
20	7	vllm/engine/llm_engine.py

[287f527f5] cascade 2025-07-30 [Feature] Add async tensor parallelism for scaled mm (#20155)
138	5	tests/compile/test_async_tp.py
242	2	vllm/compilation/collective_fusion.py
1	1	vllm/compilation/sequence_parallelism.py

[f12d9256b] Ming Yang 2025-07-30 [Misc] Use dracut on CentOS and skip clone if repo exists for EP kernel installation (#21635)
11	1	tools/ep_kernels/configure_system_drivers.sh
38	2	tools/ep_kernels/install_python_libraries.sh

[b9b753e7a] Doug Smith 2025-07-30 For VLLM_USE_PRECOMPILED, only compiled .so files should be extracted (#21964)
44	35	setup.py

[56bd537dd] Nick Hill 2025-07-30 [Misc] Support more collective_rpc return types (#21845)
64	1	tests/v1/engine/test_engine_core_client.py
8	1	vllm/v1/engine/__init__.py
3	3	vllm/v1/engine/core.py
2	1	vllm/v1/engine/core_client.py
44	0	vllm/v1/serial_utils.py

[8f0d51671] wenxindongwork 2025-07-30 [TPU] Support Pathways in vLLM (#21417)
5	0	vllm/envs.py
12	6	vllm/platforms/__init__.py

[f4135232b] wxsm 2025-07-31 feat(distributed): add `get_required_kvcache_layout` class method to kv connector api (#20433)
72	0	tests/distributed/test_kvlayout.py
15	1	vllm/distributed/kv_transfer/kv_connector/base.py
21	16	vllm/distributed/kv_transfer/kv_connector/factory.py
10	9	vllm/distributed/kv_transfer/kv_connector/utils.py
14	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
33	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
21	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[4904e53c3] Chenguang Zheng 2025-07-31 [Bugfix] SharedStorage Connector for V1 PD multimodal (#21611)
215	0	tests/v1/kv_connector/unit/test_shared_storage_connector.py
29	12	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py

[004203e95] Cyrus Leung 2025-07-31 [CI/Build] Fix registry tests (#21934)
11	5	tests/models/registry.py
10	10	vllm/model_executor/models/mpt.py
13	2	vllm/model_executor/models/telechat2.py
3	2	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
31	0	vllm/transformers_utils/configs/nvlm_d.py

[5c765aec6] 633WHU 2025-07-30 [Bugfix] Fix TypeError in scheduler when comparing mixed request_id types (#21816)
59	13	tests/v1/engine/test_engine_core.py
5	0	vllm/v1/engine/core.py

[ad510309e] Yong Hoon Shin 2025-07-30 Override attention metadata for fast prefill in some KV sharing setups (#21590)
143	0	tests/v1/e2e/test_kv_sharing_fast_prefill.py
15	0	vllm/config.py
6	0	vllm/engine/arg_utils.py
1	0	vllm/model_executor/models/gemma3n.py
33	2	vllm/v1/attention/backends/utils.py
89	24	vllm/v1/worker/gpu_model_runner.py

[366f6b3a4] Cyrus Leung 2025-07-30 [Bugfix] Fix multi-api server not working for text models (#21933)
1	14	vllm/config.py

[6e599eebe] Isotr0py 2025-07-30 [Bugfix] Fix OOM tests in initialization test (#21921)
8	6	tests/models/test_initialization.py
1	0	vllm/model_executor/models/glm4_1v.py

[88edf5994] Harry Mellor 2025-07-30 [Docs] Reduce the size of the built docs (#21920)
7	0	mkdocs.yaml
1	0	requirements/docs.txt

[ff08e5194] Po-Han Huang (NVIDIA) 2025-07-30 [NVIDIA] Fix Llama4 Scout FP4 functionality issues (#21499)
14	1	vllm/model_executor/layers/fused_moe/layer.py
0	2	vllm/model_executor/layers/quantization/modelopt.py
205	67	vllm/model_executor/models/llama4.py

[8f4a1c9a0] Ruixiang Tan 2025-07-30 [Misc] Improve code readability of KVCacheManager (#21673)
2	2	tests/v1/core/test_kv_cache_utils.py
1	1	vllm/v1/core/block_pool.py
6	3	vllm/v1/core/kv_cache_coordinator.py
1	4	vllm/v1/core/kv_cache_manager.py
0	8	vllm/v1/core/kv_cache_utils.py
8	4	vllm/v1/core/single_type_kv_cache_manager.py

[36ede4598] Harry Mellor 2025-07-30 Reduce time wasted in GitHub Actions using `concurrency` (#21919)
4	0	.github/workflows/lint-and-deploy.yaml
4	0	.github/workflows/pre-commit.yml

[0e40b2607] Cyrus Leung 2025-07-30 [CI/Build] Only run markdownlint in CI (#21892)
17	0	.github/workflows/matchers/markdownlint.json
1	0	.github/workflows/pre-commit.yml
2	1	.pre-commit-config.yaml

[0271c2ff2] Wentao Ye 2025-07-30 [Test] Add Benchmark and Unit Test for `per_token_group_quant` (#21860)
159	0	benchmarks/kernels/benchmark_per_token_group_quant.py
30	1	tests/kernels/quantization/test_per_token_group_quant.py

[e91d3c9cd] youkaichao 2025-07-30 [misc] skip p2p check by default (#21904)
7	5	vllm/envs.py

[bf668b5bf] Yan Pashkovsky 2025-07-30 [Feature] Support multiple api keys in server (#18548)
1	0	docs/getting_started/quickstart.md
6	6	vllm/entrypoints/openai/api_server.py
23	23	vllm/entrypoints/openai/cli_args.py

[da3e0bd6e] rongfu.leng 2025-07-30 [Bugfix] we should use metavar is not choices (#21902)
3	1	vllm/entrypoints/openai/cli_args.py

[fcfd1eb9c] Cyrus Leung 2025-07-30 [Doc] Remove vLLM prefix and add citation for PagedAttention (#21910)
-	-	docs/assets/{kernel => design/paged_attention}/k_vecs.png
-	-	docs/assets/{kernel => design/paged_attention}/key.png
-	-	docs/assets/{kernel => design/paged_attention}/logits_vec.png
-	-	docs/assets/{kernel => design/paged_attention}/q_vecs.png
-	-	docs/assets/{kernel => design/paged_attention}/query.png
-	-	docs/assets/{kernel => design/paged_attention}/v_vec.png
-	-	docs/assets/{kernel => design/paged_attention}/value.png
20	9	docs/design/paged_attention.md
1	1	docs/design/plugin_system.md
1	1	docs/design/torch_compile.md

[d979dd6be] aladerran 2025-07-30 [Feature][EPLB] Add eplb support for Qwen3 (#20815)
142	24	vllm/model_executor/models/qwen3_moe.py

[b876860c6] Eric Curtin 2025-07-30 [Hardware][CPU] Build fix for ARM without BF16 (#21848)
2	0	csrc/cpu/quant.cpp

[13986365a] Patrick von Platen 2025-07-30 Add @patrickvonplaten as maintainer of mistral's related files. (#21928)
8	0	.github/CODEOWNERS

[5c8fe389d] Hongsheng Liu 2025-07-30 [Docs] Fix the example code of streaming chat completions in reasoning (#21825)
6	7	docs/features/reasoning_outputs.md
6	7	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py

[5bbaf492a] Cyrus Leung 2025-07-30 [Doc] Update partial support (#21916)
4	3	docs/features/compatibility_matrix.md

[533db0935] Peter Pan 2025-07-30 [benchmark] add max-concurrency in result table (#21095)
4	0	benchmarks/benchmark_serving.py
4	0	benchmarks/benchmark_serving_structured_output.py
6	0	vllm/benchmarks/serve.py

[fc91da549] Jee Jee Li 2025-07-30 [Model] Remove DSV2 unused code (#21903)
0	14	vllm/model_executor/models/deepseek_v2.py

[547795232] Varun Vinayak Shenoy 2025-07-30 [Tests] Fixing bug inside MultiModalProfiler. (#21842)
67	0	tests/models/multimodal/processing/test_mllama4.py
3	1	tests/models/registry.py

[30ef30ed5] Kebe 2025-07-30 [CI] rollback lint-and-deploy pipeline using amd machine (#21912)
1	1	.github/workflows/lint-and-deploy.yaml

[02f82fe43] Jee Jee Li 2025-07-30 [Doc] Update Intern-S1 info  (#21908)
1	1	docs/models/supported_models.md

[2ca5f82c2] Cyrus Leung 2025-07-30 [Misc] Remove redundant config definitions (#21891)
11	11	vllm/model_executor/models/aimv2.py
7	7	vllm/model_executor/models/dbrx.py
4	4	vllm/model_executor/models/exaone.py
3	3	vllm/model_executor/models/exaone4.py
0	3	vllm/model_executor/models/keye.py
3	4	vllm/model_executor/models/minimax_vl_01.py
4	4	vllm/model_executor/models/mpt.py
4	9	vllm/model_executor/models/ovis.py
5	23	vllm/transformers_utils/config.py
6	24	vllm/transformers_utils/configs/__init__.py
0	195	vllm/transformers_utils/configs/cohere2.py
0	280	vllm/transformers_utils/configs/dbrx.py
0	190	vllm/transformers_utils/configs/exaone.py
0	252	vllm/transformers_utils/configs/exaone4.py
0	70	vllm/transformers_utils/configs/minimax_text_01.py
0	71	vllm/transformers_utils/configs/minimax_vl_01.py
0	180	vllm/transformers_utils/configs/mpt.py
0	31	vllm/transformers_utils/configs/nvlm_d.py
0	184	vllm/transformers_utils/configs/ovis.py
0	54	vllm/transformers_utils/configs/skyworkr1v.py
0	247	vllm/transformers_utils/configs/solar.py
0	64	vllm/transformers_utils/configs/telechat2.py
7	0	vllm/transformers_utils/processors/__init__.py

[6f8d26188] Louie Tsai 2025-07-29 Update vLLM Benchmark Suite for Xeon based on 0.9.2 release  (#21486)
1	0	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
1	1	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
209	0	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc2.json
211	0	.buildkite/nightly-benchmarks/tests/serving-tests-cpu-snc3.json
15	0	.buildkite/nightly-benchmarks/tests/serving-tests-cpu.json

[4cd7fe6ce] Ricardo Decal 2025-07-29 [Docs] Expand introduction to Ray in Multi-node deployment section (#21584)
11	1	docs/serving/distributed_serving.md

[16f325052] Cyrus Leung 2025-07-30 [CI/Build] Fix pre-commit failure in docs (#21897)
43	20	docs/design/fused_moe_modular_kernel.md

[e3bc17cee] Tao He 2025-07-30 Add @sighingnow as maintainer of qwen's related files. (#21895)
4	0	.github/CODEOWNERS

[05cbbe20c] Kunshang Ji 2025-07-30 [XPU] use `ZE_AFFINITY_MASK` for device select on xpu (#21815)
1	1	vllm/platforms/xpu.py

[65f311ce5] wang.yuqi 2025-07-30 [Frontend] Add LLM.reward specific to reward models (#21720)
53	28	docs/models/pooling_models.md
1	2	examples/offline_inference/basic/embed.py
53	0	examples/offline_inference/basic/reward.py
4	0	tests/conftest.py
1	1	tests/models/language/pooling/test_reward.py
3	3	tests/models/language/pooling/test_truncation_control.py
59	1	vllm/entrypoints/llm.py

[1b0a15553] Wentao Ye 2025-07-29 [Perf] Using `__nv_fp8_e4m3` instead of `c10::e4m3` for `per_token_group_quant` (#21867)
2	4	csrc/quantization/fp8/per_token_group_quant.cu

[44bc46da6] Cyrus Leung 2025-07-30 [Bugfix] Actually disable processing cache when API server is scaled out (#21839)
8	5	vllm/entrypoints/cli/serve.py

[b7b23da4d] MingzhenHan 2025-07-30 [Bugfix] Fix comment typo of get_num_common_prefix_blocks() (#21827)
2	2	vllm/v1/core/kv_cache_coordinator.py
1	1	vllm/v1/core/single_type_kv_cache_manager.py

[fdde18229] Areeb Syed 2025-07-30 [Bugfix] Fix shape mismatch assertion error when loading Gemma3n model with BitsAndBytes quantization (#21808)
24	7	vllm/model_executor/models/gemma3n.py

[b917da442] Csrayz 2025-07-30 Expose PyTorch profiler configuration to environment variables (#21803)
6	1	docs/contributing/profiling.md
29	0	vllm/envs.py
13	2	vllm/v1/worker/gpu_worker.py
12	1	vllm/v1/worker/xpu_worker.py

[fb58e3a65] Michael Goin 2025-07-29 [Docs] Update docker.md with HF_TOKEN, new model, and podman fix (#21856)
5	5	docs/deployment/docker.md

[76080cff7] Chen Zhang 2025-07-29 [DOC] Fix path of v1 related figures (#21868)
-	-	docs/assets/design/{v1 => }/metrics/intervals-1.png
-	-	docs/assets/design/{v1 => }/metrics/intervals-2.png
-	-	docs/assets/design/{v1 => }/metrics/intervals-3.png
-	-	docs/assets/design/{v1 => }/prefix_caching/example-time-1.png
-	-	docs/assets/design/{v1 => }/prefix_caching/example-time-3.png
-	-	docs/assets/design/{v1 => }/prefix_caching/example-time-4.png
-	-	docs/assets/design/{v1 => }/prefix_caching/example-time-5.png
-	-	docs/assets/design/{v1 => }/prefix_caching/example-time-6.png
-	-	docs/assets/design/{v1 => }/prefix_caching/example-time-7.png
-	-	docs/assets/design/{v1 => }/prefix_caching/free.png
-	-	docs/assets/design/{v1 => }/prefix_caching/overview.png
-	-	docs/assets/design/{v1 => }/tpu/most_model_len.png
1	1	docs/configuration/tpu.md
3	3	docs/design/metrics.md
8	8	docs/design/prefix_caching.md

[ba5c5e540] Harry Mellor 2025-07-30 [Docs] Switch to better markdown linting pre-commit hook (#21851)
5	0	.buildkite/nightly-benchmarks/README.md
11	10	.buildkite/nightly-benchmarks/nightly-annotation.md
17	17	.buildkite/nightly-benchmarks/nightly-descriptions.md
1	0	.buildkite/nightly-benchmarks/performance-benchmarks-descriptions.md
2	2	.github/PULL_REQUEST_TEMPLATE.md
13	0	.markdownlint.yaml
3	4	.pre-commit-config.yaml
7	0	README.md
4	1	RELEASE.md
56	43	benchmarks/README.md
6	2	benchmarks/auto_tune/README.md
2	2	benchmarks/kernels/deepgemm/README.md
3	2	csrc/quantization/cutlass_w8a8/Epilogues.md
2	2	docs/cli/README.md
11	4	docs/configuration/tpu.md
4	4	docs/contributing/ci/failures.md
3	1	docs/contributing/ci/update_pytorch_version.md
3	3	docs/contributing/deprecation_policy.md
2	2	docs/contributing/profiling.md
3	3	docs/contributing/vulnerability_management.md
6	6	docs/deployment/frameworks/anything-llm.md
5	5	docs/deployment/frameworks/chatbox.md
5	5	docs/deployment/frameworks/dify.md
0	2	docs/deployment/frameworks/haystack.md
1	0	docs/deployment/frameworks/retrieval_augmented_generation.md
5	4	docs/deployment/integrations/production-stack.md
1	1	docs/deployment/k8s.md
2	2	docs/design/metrics.md
3	1	docs/design/p2p_nccl_connector.md
7	4	docs/design/prefix_caching.md
3	3	docs/design/torch_compile.md
3	3	docs/features/compatibility_matrix.md
2	0	docs/features/lora.md
2	0	docs/features/multimodal_inputs.md
1	1	docs/features/quantization/auto_round.md
2	2	docs/features/quantization/int4.md
1	0	docs/features/quantization/quantized_kvcache.md
1	0	docs/features/quantization/quark.md
1	0	docs/features/quantization/torchao.md
3	3	docs/getting_started/installation/cpu.md
4	4	docs/getting_started/installation/intel_gaudi.md
3	2	docs/models/hardware_supported_models/tpu.md
7	7	docs/models/supported_models.md
1	1	docs/serving/distributed_serving.md
2	1	docs/serving/expert_parallel_deployment.md
1	0	docs/serving/openai_compatible_server.md
19	13	docs/usage/security.md
5	5	docs/usage/v1_guide.md
1	1	examples/offline_inference/disaggregated-prefill-v1/README.md
4	4	examples/offline_inference/openai_batch/README.md
4	0	examples/others/lmcache/README.md
3	3	examples/others/logging_configuration.md
0	10	pyproject.toml
6	3	tools/ep_kernels/README.md
2	1	vllm/plugins/lora_resolvers/README.md

[555e7225b] Chen Zhang 2025-07-29 [v1][attention] Support Hybrid Allocator + FlashInfer (#21412)
11	8	tests/v1/attention/test_attention_backends.py
1	0	tests/v1/spec_decode/test_eagle.py
2	1	tests/v1/worker/test_gpu_model_runner.py
24	8	vllm/config.py
2	2	vllm/v1/attention/backends/cpu_attn.py
2	2	vllm/v1/attention/backends/flash_attn.py
7	11	vllm/v1/attention/backends/flashinfer.py
2	2	vllm/v1/attention/backends/flex_attention.py
2	2	vllm/v1/attention/backends/mamba_attn.py
3	1	vllm/v1/attention/backends/mla/common.py
4	3	vllm/v1/attention/backends/mla/flashmla.py
4	3	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
2	2	vllm/v1/attention/backends/rocm_aiter_fa.py
2	2	vllm/v1/attention/backends/triton_attn.py
9	5	vllm/v1/attention/backends/utils.py
8	5	vllm/v1/worker/gpu_model_runner.py

[0e36abf99] milesial 2025-07-29 [Bugfix] Correct max tokens for non-contiguous embeds (#21798)
28	3	vllm/multimodal/profiling.py
1	1	vllm/multimodal/registry.py

[452b2a318] Simon Mo 2025-07-29 [ci] mark blackwell test optional for now (#21878)
1	1	.buildkite/test-pipeline.yaml

[0d0cc9e15] Simon Mo 2025-07-29 [ci] add b200 test placeholder (#21866)
11	0	.buildkite/test-pipeline.yaml

[9266d9804] Yong Hoon Shin 2025-07-29 [BugFix] Fix interleaved sliding window not set for Gemma3n (#21863)
7	2	vllm/config.py
7	2	vllm/model_executor/models/gemma3n.py

[176bbce1d] Gregory Shtrasberg 2025-07-29 Revert "[AMD][CI/Build] Fix the AMD issue caused by inappropriate of symbol exposure (#21647)" (#21850)
2	2	CMakeLists.txt

[a1873db23] Doug Smith 2025-07-29 docker: docker-aware precompiled wheel support (#21127)
16	10	docker/Dockerfile
43	15	setup.py
9	2	vllm/envs.py

[a33ea28b1] Michael Goin 2025-07-29 Add `flashinfer_python` to CUDA wheel requirements (#21389)
3	1	docker/Dockerfile
2	0	requirements/cuda.txt

[7b49cb1c6] David Xia 2025-07-29 [Doc] update Contributing page's testing section (#18272)
12	5	docs/contributing/README.md

[f03e9cf2b] Varun Sundar Rabindranath 2025-07-29 [Doc] Add FusedMoE Modular Kernel Documentation (#21623)
-	-	docs/assets/design/fused_moe_modular_kernel/fused_experts_blocks.png
-	-	docs/assets/design/fused_moe_modular_kernel/fused_moe_batched.png
-	-	docs/assets/design/fused_moe_modular_kernel/fused_moe_non_batched.png
-	-	docs/assets/design/fused_moe_modular_kernel/prepare_and_finalize_blocks.png
236	0	docs/design/fused_moe_modular_kernel.md

[37f86d904] David Xia 2025-07-29 [Docs] use `uv` in GPU installation docs (#20277)
44	40	docs/getting_started/installation/gpu/cuda.inc.md

[58b11b24a] elvischenv 2025-07-29 [Bugfix] Fix workspace buffer None issue for Flashinfer TRTLLM Backend (#21525)
28	14	benchmarks/kernels/benchmark_trtllm_attention.py
7	9	tests/kernels/attention/test_flashinfer_trtllm_decode_attention.py
11	4	vllm/attention/backends/flashinfer.py
15	15	vllm/v1/attention/backends/flashinfer.py

[ad341c519] Wenhua Cheng 2025-07-29 [Bugfix]fix mixed bits and visual language model quantization in AutoRound (#21802)
115	38	vllm/model_executor/layers/quantization/auto_round.py

[759b87ef3] Brittany 2025-07-29 [TPU] Add an optimization doc on TPU (#21155)
-	-	docs/assets/design/v1/tpu/most_model_len.png
104	0	docs/configuration/tpu.md

[f693b067a] Harry Mellor 2025-07-29 [Docs] Merge design docs for a V1 only future (#21832)
1	3	docs/.nav.yml
0	40	docs/design/automatic_prefix_caching.md
1	1	docs/design/huggingface_integration.md
0	0	docs/design/{v1 => }/metrics.md
0	0	docs/design/{v1 => }/multiprocessing.md
29	27	docs/design/{v1 => }/p2p_nccl_connector.md
4	0	docs/design/{kernel => }/paged_attention.md
0	0	docs/design/{v1 => }/prefix_caching.md
0	0	docs/design/{v1 => }/torch_compile.md

[04e38500e] Richard Zou 2025-07-29 [Bugfix] VLLM_V1 supports passing other compilation levels (#19340)
53	2	tests/compile/test_config.py
2	0	vllm/compilation/counter.py
19	2	vllm/config.py
12	1	vllm/v1/worker/gpu_model_runner.py
2	0	vllm/worker/model_runner.py

[ab714131e] Cyrus Leung 2025-07-29 [Doc] Update compatibility matrix for pooling and multimodal models (#21831)
7	5	docs/features/compatibility_matrix.md

[755fa8b65] Chen Zhang 2025-07-29 [KVCache] Make KVCacheSpec hashable (#21791)
33	1	tests/v1/core/test_kv_cache_utils.py
6	2	tests/v1/e2e/test_correctness_sliding_window.py
14	17	vllm/v1/core/kv_cache_coordinator.py
18	17	vllm/v1/core/kv_cache_utils.py
29	51	vllm/v1/kv_cache_interface.py

[247041911] Kay Yan 2025-07-29 [Docs] Fix the outdated URL for installing from vLLM binaries (#21523)
1	2	docs/contributing/ci/update_pytorch_version.md
4	4	docs/getting_started/installation/gpu/cuda.inc.md

[61a6905ab] Jee Jee Li 2025-07-29 [Model] Refactor JambaForCausalLM (#21394)
116	115	vllm/model_executor/models/jamba.py

[37efc63b6] Reza Barazesh 2025-07-29 [V0 deprecation] Guided decoding (#21347)
1	2	.buildkite/test-pipeline.yaml
0	3	.github/CODEOWNERS
0	3	.github/mergify.yml
0	552	tests/entrypoints/llm/test_guided_generate.py
11	41	tests/entrypoints/llm/test_lazy_outlines.py
20	131	tests/entrypoints/openai/test_chat.py
30	6	tests/entrypoints/openai/test_completion.py
5	21	tests/entrypoints/openai/test_prompt_validation.py
0	207	tests/model_executor/test_guided_processors.py
1	50	tests/models/language/generation/test_mistral.py
3	3	tests/samplers/test_no_bad_words.py
1	2	tests/test_sampling_params.py
0	7	tests/v1/test_oracle.py
0	1	tools/check_pickle_imports.py
4	16	vllm/config.py
8	16	vllm/engine/arg_utils.py
4	62	vllm/engine/async_llm_engine.py
7	41	vllm/engine/llm_engine.py
0	18	vllm/engine/multiprocessing/client.py
8	68	vllm/entrypoints/llm.py
0	192	vllm/model_executor/guided_decoding/__init__.py
0	63	vllm/model_executor/guided_decoding/guidance_decoding.py
0	104	vllm/model_executor/guided_decoding/guidance_logits_processors.py
0	41	vllm/model_executor/guided_decoding/guided_fields.py
0	67	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
0	117	vllm/model_executor/guided_decoding/outlines_decoding.py
0	307	vllm/model_executor/guided_decoding/outlines_logits_processors.py
0	242	vllm/model_executor/guided_decoding/utils.py
0	426	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[a4528f0ca] Isotr0py 2025-07-29 [Model]: Fused MoE for nomic-embed-text-v2-moe (#18321)
36	11	vllm/model_executor/layers/fused_moe/fused_moe.py
106	102	vllm/model_executor/models/bert_with_rope.py

[a2480251e] Cyrus Leung 2025-07-29 [Doc] Link to RFC for pooling optimizations (#21806)
3	3	docs/models/pooling_models.md

[7234fe268] Nick Hill 2025-07-29 [Misc] Rework process titles (#21780)
4	2	vllm/entrypoints/cli/serve.py
12	4	vllm/entrypoints/openai/api_server.py
12	4	vllm/utils/__init__.py
3	4	vllm/v1/engine/coordinator.py
4	3	vllm/v1/engine/core.py
10	6	vllm/v1/executor/multiproc_executor.py
3	3	vllm/v1/utils.py

[f1e2c095e] Benji Beck 2025-07-28 Migrate InternVLImageInputs and InternVLVideoInputs to TensorSchema (#21684)
49	62	vllm/model_executor/models/internvl.py

[12a223ef9] Gregory Shtrasberg 2025-07-28 [AMD][CI/Build][Bugfix] Guarding CUDA specific functions by ifndef ROCM (#21766)
6	2	csrc/quantization/compressed_tensors/int8_quant_kernels.cu

[e18f08510] Calvin Chen 2025-07-29 skip fusedmoe layer for start_load_kv (#21378)
10	2	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py

[afa260759] Michael Goin 2025-07-28 [CI] Parallelize Kernels MoE Test (#21764)
4	3	.buildkite/test-pipeline.yaml

[48b763d6b] Wentao Ye 2025-07-28 [Refactor] Merge Compressed Tensor FP8 `CompressedTensorsW8A8Fp8MoEMethod` and `CompressedTensorsW8A8Fp8MoECutlassMethod` (#21775)
100	289	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[947e982ed] Michael Goin 2025-07-28 [Docs] Minimize spacing for supported_hardware.md table (#21779)
14	7	docs/features/quantization/supported_hardware.md

[c6c9122d5] lyrisz 2025-07-28 [Kernel] SM90 CUTLASS FP8 GEMM: add support for swap AB + kernel tuning (#20396)
4	5	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu
272	46	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8_dispatch.cuh
1	1	tests/kernels/quantization/test_cutlass_scaled_mm.py

[8aa1485fc] Lucas Wilkinson 2025-07-28 [Perf] Disable chunked local attention by default with llama4 (#21761)
17	6	vllm/config.py
12	0	vllm/envs.py

[89ac266b2] Nikhil Gupta 2025-07-28 [Feat]: Add support for Dynamic Quant 4 bit CPU kleidiai kernels (#17112)
31	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
135	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a8_int.py
9	5	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
92	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/dynamic_4bit.py

[c6f36cfa2] Clayton Coleman 2025-07-28 [Bugfix] DeepGEMM is not enabled on B200 due to `_lazy_init()` (#21472)
8	4	vllm/utils/deep_gemm.py

[b18b417fb] Kuntai Du 2025-07-28 Revert "[V1] Exception Handling when Loading KV Cache from Remote Store" (#21778)
0	120	tests/v1/kv_connector/kv_load_exception_handling/random_drop_connector.py
0	16	tests/v1/kv_connector/kv_load_exception_handling/test.sh
1	15	vllm/distributed/kv_transfer/kv_connector/utils.py
0	20	vllm/distributed/kv_transfer/kv_connector/v1/base.py
0	2	vllm/sequence.py
0	30	vllm/v1/core/sched/scheduler.py
0	3	vllm/v1/outputs.py
2	8	vllm/v1/worker/gpu_model_runner.py
1	3	vllm/v1/worker/gpu_worker.py
1	12	vllm/v1/worker/kv_connector_model_runner_mixin.py

[9ba1c88a9] Lu Fang 2025-07-28 [AMD][CI/Build] Fix the AMD issue caused by inappropriate of symbol exposure (#21647)
2	2	CMakeLists.txt

[e0e58f972] Wentao Ye 2025-07-28 [Bug] Enforce contiguous input for `dynamic_scaled_fp8_quant` and `static_scaled_fp8_quant` (#21773)
3	2	vllm/_custom_ops.py

[b361f14e3] rasmith 2025-07-28 [AMD][BugFix] Fix omission  of wvSplitK kernel for small batch sizes (1-4) due to torch.compile (#21350)
28	4	vllm/model_executor/layers/utils.py

[01c753ed9] weiliang 2025-07-29 update flashinfer to v0.2.9rc2 (#21701)
1	1	docker/Dockerfile

[94b71ae10] Harry Mellor 2025-07-28 Use `metavar` to list the choices for a CLI arg when custom values are also accepted (#21760)
5	0	docs/mkdocs/hooks/generate_argparse.py
4	0	tests/engine/test_arg_utils.py
11	7	vllm/engine/arg_utils.py

[7d44c691b] Nick Hill 2025-07-28 [P/D] Log warnings related to prefill KV expiry (#21753)
12	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[e17a4d3bf] Cyrus Leung 2025-07-29 [Bugfix] Fix granite speech shape validation (#21762)
4	3	vllm/model_executor/models/granite_speech.py

[ec261b029] Chaojun Zhang 2025-07-29 [XPU] IPEX-optimized Punica Wrapper on XPU (#21703)
7	0	vllm/lora/ops/ipex_ops/__init__.py
44	0	vllm/lora/ops/ipex_ops/lora_ops.py
269	0	vllm/lora/punica_wrapper/punica_xpu.py
1	1	vllm/platforms/xpu.py

[04fe61aa3] Cyrus Leung 2025-07-28 [CI/Build] Fix plugin tests (#21758)
1	4	tests/models/test_oot_registration.py

[25708d317] Michard Hugo 2025-07-28 [Bugfix] Mistral crashes on tool with no description (#21167)
4	1	vllm/transformers_utils/tokenizers/mistral.py

[0e18a5d05] Cyrus Leung 2025-07-28 [Misc] Reduce logs for model resolution (#21765)
12	6	vllm/config.py

[34a20c49b] Michael Goin 2025-07-28 [Logs] Change flashinfer sampler logs to once (#21759)
8	7	vllm/v1/sample/ops/topk_topp_sampler.py

[31084b3b1] Isotr0py 2025-07-28 [Bugfix][CI/Build] Update peft version in test requirement (#21729)
1	1	requirements/test.in
1	1	requirements/test.txt

[bccc43c03] wuhang 2025-07-28 [Bugfix]check health for engine core process exiting unexpectedly (#21728)
41	0	vllm/v1/engine/core_client.py
43	2	vllm/v1/executor/multiproc_executor.py

[1395dd9c2] Harry Mellor 2025-07-28 [Docs] Add revision date to rendered docs (#21752)
3	0	.readthedocs.yaml
5	0	mkdocs.yaml
1	0	requirements/docs.txt

[9ace2eaf3] Keyang Ru 2025-07-28 [Bugfix] Improve JSON extraction in LlamaToolParser (#19024)
132	0	tests/entrypoints/openai/tool_parsers/test_llama3_json_tool_parser.py
42	40	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py

[656c24f1b] Anton Vlasjuk 2025-07-28 [`Ernie 4.5`] Name Change for Base 0.3B Model (#21735)
5	5	docs/models/supported_models.md
1	1	tests/models/registry.py
1	1	vllm/model_executor/models/ernie45.py
1	1	vllm/model_executor/models/registry.py

[63fe3a700] Chauncey 2025-07-28 [PD] let p2p nccl toy proxy handle /chat/completions (#21734)
3	2	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py

[0ae970ed1] Isotr0py 2025-07-28 [Bugfix] Fix glm4.1v video_grid_thw tensor shape scheme (#21744)
3	6	vllm/model_executor/models/glm4_1v.py

[65e8466c3] Li, Jiang 2025-07-28 [Bugfix] Fix environment variable setting in CPU Dockerfile (#21730)
6	6	.buildkite/scripts/hardware_ci/run-cpu-test.sh
12	13	docker/Dockerfile.cpu

[1b769dccf] Jee Jee Li 2025-07-28 [Bugfix] Fix Ernie4_5_MoeForCausalLM shared experts (#21717)
6	5	vllm/model_executor/models/ernie45_moe.py

[2cc571199] rongfu.leng 2025-07-28 [feature] add log non default args in LLM (#21680)
4	1	vllm/entrypoints/llm.py
2	3	vllm/entrypoints/openai/api_server.py
0	9	vllm/entrypoints/openai/cli_args.py
29	0	vllm/entrypoints/utils.py

[a4ed73154] Cyrus Leung 2025-07-28 [Model] Prioritize Transformers fallback over suffix matching (#21719)
2	1	tests/models/multimodal/generation/test_common.py
36	22	vllm/model_executor/models/registry.py

[d128d0d55] Benji Beck 2025-07-28 Migrate KeyeImageInputs and KeyeVideoInputs to TensorSchema (#21686)
41	65	vllm/model_executor/models/keye.py

[a6c050286] Asaf Joseph Gardin 2025-07-28 [v1][mamba] Added mamba_type into MambaSpec (#21715)
25	0	tests/v1/attention/test_mamba_selectors.py
5	0	vllm/model_executor/layers/mamba/abstract.py
4	0	vllm/model_executor/layers/mamba/mamba_mixer2.py
12	0	vllm/v1/attention/backends/mamba_selectors.py
2	1	vllm/v1/kv_cache_interface.py
4	3	vllm/v1/worker/gpu_model_runner.py

[139a7f07b] Lucas Wilkinson 2025-07-28 [BugFix] Fix ChunkedLocalAttention when the hybrid kv-cache is disabled (#21707)
25	0	vllm/v1/worker/gpu_model_runner.py

[150d9e633] Ning Xie 2025-07-28 [Bugfix] fix max-file-size type from str to int (#21675)
1	1	examples/offline_inference/save_sharded_state.py

[139a97ec5] Cyrus Leung 2025-07-28 [Bugfix] Fix shape checking for Fuyu (#21709)
5	4	vllm/model_executor/models/fuyu.py

[18cc33dd6] rongfu.leng 2025-07-28 [bugfix] fix profile impact benchmark results (#21507)
14	14	benchmarks/benchmark_serving.py
14	14	benchmarks/benchmark_serving_structured_output.py
13	14	vllm/benchmarks/serve.py

[7656cf4cf] Hongsheng Liu 2025-07-28 [Bugfix] [issue-21565] Fix the incompatibility issue with stream and named function calling when Thinking is disabled (#21573)
11	2	tests/entrypoints/openai/test_completion_with_function_calling.py
12	5	vllm/entrypoints/openai/serving_chat.py

[3ea57a56d] Benji Beck 2025-07-27 Migrate Idefics3ImagePixelInputs and Idefics3ImageEmbeddingInputs to … (#21683)
28	41	vllm/model_executor/models/idefics3.py

[75856bc2c] Benji Beck 2025-07-27 Migrate GraniteSpeechAudioInputs to TensorSchema (#21682)
18	8	vllm/model_executor/models/granite_speech.py

[304dcdf57] Benji Beck 2025-07-27 Migrate GLMVImagePixelInputs to TensorSchema (#21679)
20	23	vllm/model_executor/models/glm4v.py

[88e46c7c8] Benji Beck 2025-07-27 Migrate Glm4vImageInputs, Glm4vVideoInputs to TensorSchema (#21678)
23	1	tests/standalone_tests/test_tensor_schema.py
46	65	vllm/model_executor/models/glm4_1v.py

[d8937de4c] Benji Beck 2025-07-27 Migrate Gemma3ImagePixelInputs to TensorSchema (#21676)
21	25	vllm/model_executor/models/gemma3_mm.py

[e626d286f] TJian 2025-07-27 [FEAT] [ROCm] [AITER]: Add AITER HIP block quant kernel (#21242)
13	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[c7ffe93d9] Shinichi Hemmi 2025-07-28 [Model] Support TP/PP/mamba2 kernel for PLaMo2 (#19674)
1	1	docs/models/supported_models.md
1	0	tests/distributed/test_pipeline_parallel.py
1	1	tests/quantization/test_experts_int8.py
362	211	vllm/model_executor/models/plamo2.py

[15a72ac47] Adeline 2025-07-28 [V1] Exception Handling when Loading KV Cache from Remote Store (#21534)
120	0	tests/v1/kv_connector/kv_load_exception_handling/random_drop_connector.py
16	0	tests/v1/kv_connector/kv_load_exception_handling/test.sh
15	1	vllm/distributed/kv_transfer/kv_connector/utils.py
20	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
2	0	vllm/sequence.py
30	0	vllm/v1/core/sched/scheduler.py
3	0	vllm/v1/outputs.py
8	2	vllm/v1/worker/gpu_model_runner.py
3	1	vllm/v1/worker/gpu_worker.py
12	1	vllm/v1/worker/kv_connector_model_runner_mixin.py

[04ff4be31] Jee Jee Li 2025-07-28 [Misc]  Add fused_moe configs for Qwen3-Coder-480B-A35B-Instruct-FP8 (#21700)
146	0	vllm/model_executor/layers/fused_moe/configs/E=20,N=2560,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json

[93269bb43] Yuxuan Zhang 2025-07-28 Fix GLM tool parser (#21668)
113	330	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py

[82acf2184] Joachim Studnia 2025-07-27 Fix typo for limit-mm-per-prompt in docs (#21697)
1	1	vllm/config.py

[86ae693f2] Cyrus Leung 2025-07-28 [Deprecation][2/N] Replace `--task` with `--runner` and `--convert` (#21470)
2	2	docs/features/multimodal_inputs.md
1	1	docs/features/prompt_embeds.md
10	3	docs/models/generative_models.md
49	28	docs/models/pooling_models.md
51	50	docs/models/supported_models.md
12	12	docs/serving/openai_compatible_server.md
4	2	examples/offline_inference/basic/classify.py
2	2	examples/offline_inference/basic/embed.py
4	2	examples/offline_inference/basic/score.py
4	2	examples/offline_inference/embed_jina_embeddings_v3.py
4	2	examples/offline_inference/embed_matryoshka_fy.py
2	2	examples/offline_inference/qwen3_reranker.py
3	3	examples/offline_inference/vision_language_pooling.py
1	1	examples/online_serving/openai_chat_completion_client_for_multimodal.py
1	1	examples/online_serving/openai_chat_embedding_client_for_multimodal.py
1	1	examples/online_serving/openai_cross_encoder_score.py
1	1	examples/online_serving/openai_cross_encoder_score_for_multimodal.py
1	1	examples/online_serving/openai_pooling_client.py
1	1	examples/online_serving/prompt_embed_inference_with_openai_client.py
0	3	tests/compile/test_async_tp.py
3	3	tests/compile/test_basic_correctness.py
0	3	tests/compile/test_fusion_all_reduce.py
0	3	tests/compile/test_sequence_parallelism.py
5	3	tests/conftest.py
13	13	tests/distributed/test_expert_parallel.py
22	22	tests/distributed/test_pipeline_parallel.py
15	15	tests/distributed/test_sequence_parallel.py
2	1	tests/entrypoints/openai/correctness/test_mteb_embed.py
2	1	tests/entrypoints/openai/correctness/test_mteb_score.py
0	4	tests/entrypoints/openai/test_chat_logit_bias_validation.py
1	0	tests/entrypoints/openai/test_chat_template.py
2	2	tests/entrypoints/openai/test_embedding.py
2	2	tests/entrypoints/openai/test_embedding_dimensions.py
1	1	tests/entrypoints/openai/test_openai_schema.py
2	2	tests/entrypoints/openai/test_optional_middleware.py
2	2	tests/entrypoints/openai/test_pooling.py
2	2	tests/entrypoints/openai/test_skip_tokenizer.py
2	2	tests/entrypoints/openai/test_truncation.py
1	1	tests/entrypoints/openai/test_video.py
1	1	tests/entrypoints/openai/test_vision.py
2	2	tests/entrypoints/openai/test_vision_embedding.py
9	33	tests/entrypoints/test_chat_utils.py
0	5	tests/lora/test_worker.py
2	8	tests/model_executor/test_guided_processors.py
0	2	tests/model_executor/test_model_load_with_params.py
1	1	tests/models/language/pooling/embed_utils.py
2	5	tests/models/language/pooling/mteb_utils.py
1	1	tests/models/language/pooling/test_embedding.py
5	8	tests/models/language/pooling/test_gritlm.py
6	1	tests/models/language/pooling/test_jina.py
11	9	tests/models/language/pooling/test_nomic_max_model_len.py
12	6	tests/models/language/pooling/test_scoring.py
3	3	tests/models/language/pooling/test_truncation_control.py
2	3	tests/models/multimodal/generation/test_common.py
1	1	tests/models/multimodal/generation/test_granite_speech.py
1	1	tests/models/multimodal/generation/test_interleaved.py
1	1	tests/models/multimodal/generation/test_phi4mm.py
1	1	tests/models/multimodal/generation/test_qwen2_vl.py
3	3	tests/models/multimodal/generation/vlm_utils/core.py
3	3	tests/models/multimodal/generation/vlm_utils/types.py
1	1	tests/models/multimodal/pooling/test_dse_qwen2_vl.py
1	1	tests/models/multimodal/pooling/test_jinavl_reranker.py
1	1	tests/models/multimodal/pooling/test_llava_next.py
1	1	tests/models/multimodal/pooling/test_phi3v.py
1	1	tests/models/multimodal/pooling/test_prithvi_mae.py
1	4	tests/models/multimodal/processing/test_common.py
1	4	tests/models/multimodal/test_mapping.py
1	1	tests/models/quantization/test_bitsandbytes.py
5	1	tests/models/test_initialization.py
12	9	tests/models/test_registry.py
13	1	tests/models/test_transformers.py
4	3	tests/models/utils.py
1	24	tests/multimodal/test_processing.py
1	9	tests/quantization/test_configs.py
95	225	tests/test_config.py
0	5	tests/test_sampling_params.py
2	10	tests/v1/core/test_kv_cache_utils.py
0	3	tests/v1/core/test_scheduler.py
0	3	tests/v1/core/utils.py
0	3	tests/v1/kv_connector/unit/utils.py
2	7	tests/v1/spec_decode/test_eagle.py
1	8	tests/v1/spec_decode/test_ngram.py
0	4	tests/v1/tpu/worker/test_tpu_model_runner.py
0	4	tests/v1/worker/test_gpu_model_runner.py
321	205	vllm/config.py
19	10	vllm/engine/arg_utils.py
37	56	vllm/entrypoints/llm.py
0	1	vllm/entrypoints/openai/api_server.py
27	104	vllm/model_executor/model_loader/utils.py
4	2	vllm/model_executor/models/config.py
187	62	vllm/model_executor/models/registry.py
60	0	vllm/transformers_utils/dynamic_module.py
11	1	vllm/transformers_utils/tokenizer_group.py
3	3	vllm/v1/worker/gpu_model_runner.py

[8f605ee30] Alexander Matveev 2025-07-27 [Attention] Make CutlassMLA the default backend for SM100 (blackwell) (#21626)
22	7	vllm/platforms/cuda.py

[a9b2a1d70] Ning Xie 2025-07-28 [Misc] Refactor vllm config str (#21666)
12	12	vllm/config.py

[57c22e57f] Caleb_Du 2025-07-27 Fix CUDA permute/unpermute for use with DeepGemm Moe (#17934)
43	33	benchmarks/kernels/benchmark_moe_permute_unpermute.py
35	38	csrc/moe/moe_permute_unpermute_op.cu
1	1	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
4	16	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h
18	21	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.inl
5	6	csrc/moe/torch_bindings.cpp
82	50	tests/kernels/moe/test_moe_permute_unpermute.py
48	44	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py

[bda9d0535] Wentao Ye 2025-07-27 [Refactor] Refactor MOE NVFP4 Code Base: ModelOpt + Compressed Tensor (#21631)
3	1	tests/quantization/test_compressed_tensors.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
8	31	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
5	61	vllm/model_executor/layers/quantization/modelopt.py
3	12	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py
55	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[3d847a312] Isotr0py 2025-07-27 [VLM] Add video support for Intern-S1 (#21671)
1	1	docs/models/supported_models.md
5	3	examples/offline_inference/vision_language.py
1	0	tests/models/multimodal/processing/test_common.py
166	45	vllm/model_executor/models/interns1.py
0	1	vllm/model_executor/models/internvl.py

[5f8c9a425] Benji Beck 2025-07-27 Migrate Florence2ImagePixelInputs to TensorSchema (#21663)
24	28	vllm/model_executor/models/florence2.py

[1cbf951ba] Ning Xie 2025-07-27 [Misc] add default value for file pattern arg (#21659)
5	1	examples/offline_inference/save_sharded_state.py

[a8936e519] ZiTian.Zhao 2025-07-27 Refactor: Remove numpy dependency from LoggingStatLogger (#20529)
9	8	vllm/v1/metrics/loggers.py

[01a395e9e] Ye (Charlotte) Qi 2025-07-26 [CI/Build][Doc] Clean up more docs that point to old bench scripts (#21667)
5	5	.buildkite/nightly-benchmarks/README.md
3	3	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
1	1	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
4	4	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
1	1	benchmarks/auto_tune/README.md
30	30	benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh
15	15	benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
6	3	docs/contributing/profiling.md
1	1	examples/online_serving/prometheus_grafana/README.md

[971948b84] Huy Do 2025-07-26 Handle non-serializable objects in vllm bench (#21665)
6	1	vllm/benchmarks/utils.py

[eed2f463b] Isotr0py 2025-07-27 [VLM] Support HF format Phi-4-MM model (#17121)
1	0	docs/models/supported_models.md
32	0	examples/offline_inference/audio_language.py
36	0	examples/offline_inference/vision_language.py
35	0	examples/offline_inference/vision_language_multi_image.py
252	0	tests/models/multimodal/generation/test_phi4_multimodal.py
31	3	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
1455	0	vllm/model_executor/models/phi4_multimodal.py
2	1	vllm/model_executor/models/registry.py
1	1	vllm/transformers_utils/tokenizer.py

[20950b29f] Benji Beck 2025-07-26 Migrate ChameleonImagePixelInputs to TensorSchema (#21657)
21	27	vllm/model_executor/models/chameleon.py

[3339cba3f] Benji Beck 2025-07-26 Migrate FuyuImagePatchInputs to TensorSchema (#21662)
22	0	tests/standalone_tests/test_tensor_schema.py
19	36	vllm/model_executor/models/fuyu.py
16	9	vllm/utils/tensor_schema.py

[0b8caf909] Benji Beck 2025-07-26 Migrate DeepseekVL2ImageInputs to TensorSchema (#21658)
31	72	vllm/model_executor/models/deepseek_vl2.py

[ccf27cc4d] Benji Beck 2025-07-26 Migrate Blip2ImagePixelInputs and Blip2ImageEmbeddingInputs to TensorSchema (#21656)
30	42	vllm/model_executor/models/blip2.py

[c65736984] Jinzhen Lin 2025-07-27 support `torch.compile` for bailing moe (#21664)
2	0	vllm/model_executor/models/bailing_moe.py

[6c66f28fa] Wenchen Lo 2025-07-26 Remove xformers requirement for Mistral-format Pixtral and Mistral3 (#21154)
18	3	vllm/model_executor/models/pixtral.py

[de509ae8e] Kaixi Hou 2025-07-26 [NVIDIA] Explicitly disable shuffled weights for flashinfer blockscale moe fp8 kernels (#21411)
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py

[e7c4f9ee8] Ye (Charlotte) Qi 2025-07-26 [CI/Build][Doc] Move existing benchmark scripts in CI/document/example to vllm bench CLI (#21355)
12	12	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
3	3	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
5	5	.buildkite/scripts/hardware_ci/run-cpu-test.sh
3	3	.buildkite/scripts/run-benchmarks.sh
1	1	.buildkite/scripts/tpu/run_bm.sh
33	33	benchmarks/README.md
9	9	benchmarks/auto_tune/auto_tune.sh
5	0	benchmarks/benchmark_latency.py
5	0	benchmarks/benchmark_serving.py
5	0	benchmarks/benchmark_throughput.py
5	5	docs/contributing/profiling.md
9	9	docs/design/v1/p2p_nccl_connector.md
5	5	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh
2	2	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh

[9094d11c5] Yeju Zhou 2025-07-26 [Bugfix][Apple Silicon] fix missing symbols when build from source on Mac with Apple Silicon (#21380)
1	1	csrc/cpu/torch_bindings.cpp

[56e544f24] Wentao Ye 2025-07-26 [Refactor] Remove `moe_align_block_size_triton` (#21335)
4	86	benchmarks/kernels/benchmark_moe_align_block_size.py
2	138	vllm/model_executor/layers/fused_moe/moe_align_block_size.py

[97d6c30cc] WeiQing Chen 2025-07-26 [BugFix] Fix shared storage connector load kv only load attention layer (#21428)
10	2	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py

[a40a8506d] Ye (Charlotte) Qi 2025-07-26 [Misc] Improve memory profiling debug message (#21429)
14	4	vllm/v1/worker/gpu_worker.py

[c215f5c87] Wentao Ye 2025-07-26 [Bug] Fix `has_flashinfer_moe` Import Error when it is not installed (#21634)
2	1	vllm/utils/flashinfer.py

[1cd6eaba5] Maximilien de Bayser 2025-07-26 Support encoder-only models without KV-Cache (#21270)
1	1	examples/offline_inference/prithvi_geospatial_mae.py
11	2	tests/conftest.py
9	3	tests/model_executor/test_model_load_with_params.py
3	11	tests/models/language/pooling/test_embedding.py
8	0	tests/models/language/pooling/test_jina.py
1	0	tests/v1/attention/utils.py
0	1	tests/v1/test_oracle.py
1	2	tests/v1/test_utils.py
2	1	vllm/engine/arg_utils.py
7	11	vllm/model_executor/models/bert.py
64	21	vllm/model_executor/models/roberta.py
84	7	vllm/v1/attention/backends/flash_attn.py
3	0	vllm/v1/attention/backends/utils.py
6	0	vllm/v1/engine/core.py
1	0	vllm/v1/spec_decode/eagle.py
4	0	vllm/v1/worker/cpu_model_runner.py
147	39	vllm/v1/worker/gpu_model_runner.py

[f27fdfc3e] Isotr0py 2025-07-26 [Bugfix] Investigate Qwen2-VL failing test (#21527)
1	0	tests/models/multimodal/generation/test_common.py

[de10ff0b7] Benji Beck 2025-07-26 Migrate AyaVisionImagePixelInputs to TensorSchema for shape validation (#21622)
29	38	vllm/model_executor/models/aya_vision.py

[9d197280f] Benji Beck 2025-07-26 Migrate AriaImagePixelInputs to TensorSchema for shape validation (#21620)
21	29	vllm/model_executor/models/aria.py

[e98def439] Huy Do 2025-07-26 [Take 2] Correctly kill vLLM processes after benchmarks (#21646)
2	1	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
2	0	benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh
2	0	benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh

[05c1126f2] Reid 2025-07-26 [Misc] remove unused try-except in pooling config check (#21618)
5	7	vllm/transformers_utils/config.py

[875af38e0] Lyu Han 2025-07-26 Support Intern-S1 (#21628)
1	0	docs/models/supported_models.md
32	0	examples/offline_inference/vision_language.py
28	0	examples/offline_inference/vision_language_multi_image.py
2	0	tests/models/registry.py
711	0	vllm/model_executor/models/interns1.py
421	0	vllm/model_executor/models/interns1_vit.py
1	0	vllm/model_executor/models/registry.py

[7728dd77b] QiliangCui 2025-07-25 [TPU][Test] Divide TPU v1 Test into 2 parts. (#21431)
166	0	.buildkite/scripts/hardware_ci/run-tpu-v1-test-part2.sh
0	12	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[2f6e6b33f] Alexandre JUAN 2025-07-26 [Bugfix] Fix isinstance check for tensor types in _load_prompt_embeds to use dtype comparison (#21612)
5	3	vllm/entrypoints/openai/serving_engine.py

[a55c95096] Huy Do 2025-07-25 Correctly kill vLLM processes after finishing serving benchmarks (#21641)
8	6	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh

[97349fe2b] WeiQing Chen 2025-07-26 [Docs] add offline serving multi-modal video input expamle Qwen2.5-VL (#21530)
64	0	docs/features/multimodal_inputs.md

[62965de5f] Farzad Abdolhosseini 2025-07-26 [Model] Ultravox: Support Llama 4 and Gemma 3 backends (#17818)
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py
23	15	vllm/model_executor/models/ultravox.py
13	9	vllm/transformers_utils/configs/ultravox.py

[7ae75fa6d] Alex Kogan 2025-07-25 [Feature] Add support for MoE models in the calibration-free RTN-based quantization (#20766)
4	1	tests/quantization/test_rtn.py
197	37	vllm/model_executor/layers/quantization/rtn.py

[f1b286b2f] Chengji Yao 2025-07-25 [TPU] Update ptxla nightly version to 20250724 (#21555)
1	1	docker/Dockerfile.tpu
4	4	requirements/tpu.txt

[c7742d611] Rui Qiao 2025-07-25 [Bugfix] Always set RAY_ADDRESS for Ray actor before spawn (#21540)
10	9	vllm/utils/__init__.py

[cea96a015] Rui Qiao 2025-07-25 [Bugfix] Fix sync_and_slice_intermediate_tensors (#21537)
1	1	vllm/v1/worker/gpu_model_runner.py

[2eddd437b] Yong Hoon Shin 2025-07-25 Add interleaved RoPE test for Llama4 (Maverick) (#21478)
74	20	tests/models/multimodal/generation/test_maverick.py

[75d29cf4e] Wentao Ye 2025-07-25 [Perf] Cuda Kernel for Int8 Per Token Group Quant (#21476)
5	0	csrc/ops.h
10	0	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
5	1	csrc/quantization/fp8/per_token_group_quant.cu
10	0	csrc/quantization/per_token_group_quant_8bit.h
8	0	csrc/torch_bindings.cpp
9	2	vllm/model_executor/layers/quantization/utils/int8_utils.py

[41d3082c4] Daniel Han 2025-07-25 Add Unsloth to RLHF.md (#21636)
5	1	docs/training/rlhf.md

[7cfea0df3] QiliangCui 2025-07-25 [TPU][Test] Rollback PR-21550. (#21619)
1	1	tests/v1/tpu/test_basic.py

[5ac3168ee] Wenhua Cheng 2025-07-25 [Docs] add auto-round quantization readme  (#21600)
1	0	docs/features/quantization/README.md
103	0	docs/features/quantization/auto_round.md

[396ee9418] Kebe 2025-07-25 [CI] Unifying Dockerfiles for ARM and X86 Builds (#21343)
1	1	.github/workflows/lint-and-deploy.yaml
0	62	docker/Dockerfile.arm
22	2	docker/Dockerfile.cpu
1	1	docs/getting_started/installation/cpu/arm.inc.md
5	1	requirements/cpu.txt

[e189b50f5] mgazz 2025-07-25 Add support for Prithvi in Online serving mode (#21518)
93	0	tests/entrypoints/openai/test_skip_tokenizer.py
14	6	vllm/engine/multiprocessing/client.py
12	2	vllm/entrypoints/openai/serving_engine.py
5	1	vllm/entrypoints/openai/serving_pooling.py
4	1	vllm/model_executor/models/prithvi_geospatial_mae.py

[136d750f5] czhu-cohere 2025-07-25 [Kernel] Improve machete memory bound perf (#21556)
6	2	csrc/quantization/machete/machete_prepacked_layout.cuh

[b3caeb82e] who who who 2025-07-25 [ROCm][AITER] Enable fp8 kv cache on rocm aiter backend. (#20295)
191	0	tests/kernels/attention/test_aiter_flash_attn.py
129	96	vllm/v1/attention/backends/rocm_aiter_fa.py

[eab2f3980] Chih-Chieh Yang 2025-07-25 [Model] Replace Mamba2 RMSNorm Gated with Fused Triton Kernel (#20839)
8	13	vllm/model_executor/layers/mamba/mamba_mixer2.py
168	0	vllm/model_executor/layers/mamba/ops/layernorm_gated.py

[9fe98d425] kourosh hakhamaneshi 2025-07-25 [Frontend] Add request_id to the Request object so they can be controlled better via external load balancers (#21009)
21	0	vllm/entrypoints/openai/protocol.py
3	1	vllm/entrypoints/openai/serving_completion.py
3	2	vllm/entrypoints/openai/serving_embedding.py

[29c6fbe58] bigshanedogg 2025-07-25 [MODEL] New model support for naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B (#20931)
1	0	docs/models/supported_models.md
80	0	examples/offline_inference/vision_language.py
48	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
3	0	tests/models/registry.py
1231	0	vllm/model_executor/models/hyperclovax_vision.py
1	0	vllm/model_executor/models/registry.py

[c72f049cb] xyxinyang 2025-07-25 [Model] Fix Ernie4.5MoE e_score_correction_bias parameter (#21586)
17	8	vllm/model_executor/models/ernie45_moe.py

[f3a683b7c] Mengqing Cao 2025-07-25 [Bugfix][Logprobs] Fix logprobs op to support more backend (#21591)
3	1	vllm/v1/sample/ops/logprobs.py

[46d81d695] Cyrus Leung 2025-07-25 [V1] Get supported tasks from model runner instead of model config (#21585)
17	7	vllm/entrypoints/llm.py
19	13	vllm/entrypoints/openai/api_server.py
14	7	vllm/entrypoints/openai/run_batch.py
4	4	vllm/executor/executor_base.py
2	1	vllm/model_executor/layers/pooler.py
1	1	vllm/model_executor/models/bert.py
1	1	vllm/model_executor/models/gritlm.py
1	1	vllm/model_executor/models/modernbert.py
2	3	vllm/pooling_params.py
11	0	vllm/tasks.py
4	0	vllm/v1/engine/async_llm.py
9	2	vllm/v1/engine/core.py
16	0	vllm/v1/engine/core_client.py
4	0	vllm/v1/engine/llm_engine.py
31	4	vllm/v1/worker/gpu_model_runner.py
3	3	vllm/v1/worker/gpu_worker.py
29	2	vllm/v1/worker/tpu_model_runner.py
3	3	vllm/v1/worker/tpu_worker.py
29	2	vllm/worker/model_runner_base.py

[5c3f2628d] Jee Jee Li 2025-07-25 [Quantization] Enable BNB support for more MoE models (#21370)
78	70	vllm/model_executor/models/dots1.py
15	10	vllm/model_executor/models/glm4_moe.py

[7311f7446] Kebe 2025-07-25 [Bugfix] GGUF: fix AttributeError: 'PosixPath' object has no attribute 'startswith' (#21579)
3	3	vllm/transformers_utils/config.py

[8ed01e32f] Xu Wenqing 2025-07-25 Add H20-3e fused MoE kernel tuning configs for Qwen3-Coder-480B-A35B-Instruct (#21598)
146	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=320,device_name=NVIDIA_H20-3e.json

[e38e96a3c] Nick Hill 2025-07-25 [Tests] Harden DP tests (#21508)
5	4	tests/v1/test_external_lb_dp.py
29	31	tests/v1/test_hybrid_lb_dp.py
48	20	tests/v1/test_internal_lb_dp.py

[40d86ee41] Chengji Yao 2025-07-24 [TPU][Bugfix] fix OOM issue in CI test (#21550)
1	1	tests/v1/tpu/test_basic.py

[85d051f02] Yang Chen 2025-07-24 [Misc] Removed undefined cmake variables MOE_PERMUTE_ARCHS (#21262)
8	11	CMakeLists.txt

[5140f54b8] Ignacio Sica 2025-07-25 [CI/Build] fix cpu_extension for apple silicon (#21195)
20	5	cmake/cpu_extension.cmake

[947edd099] Chengji Yao 2025-07-24 [Misc][Tools] make max-model-len a parameter in auto_tune script (#21321)
4	0	benchmarks/auto_tune/README.md
11	3	benchmarks/auto_tune/auto_tune.sh

[fde60ee77] hfan 2025-07-25 [Model] Fix a check for None but the return value was empty list in Gemma3 MM vision_embeddings (#21479)
1	1	vllm/model_executor/models/gemma3_mm.py

[b38bc652a] Jason Gu 2025-07-25 [Model] Support tensor parallel for timm ViT in Deepseek_vl2 (#21494)
38	2	vllm/model_executor/models/deepseek_vl2.py

[adaf2c6d4] Ning Xie 2025-07-25 [Bugfix] fix modelscope snapshot_download serialization (#21536)
6	6	vllm/model_executor/model_loader/default_loader.py

[42343f1f8] Li, Jiang 2025-07-25 [CI] Update CODEOWNERS for CPU and Intel GPU (#21582)
12	0	.github/CODEOWNERS

[965bc71b0] Benji Beck 2025-07-24 Integrate TensorSchema with shape validation for Phi3VImagePixelInputs (#21232)
126	0	tests/standalone_tests/test_tensor_schema.py
36	72	vllm/model_executor/models/phi3v.py
210	0	vllm/utils/tensor_schema.py

[807a328bb] Zhou Fang 2025-07-24 [Docs] Add `requirements/common.txt` to run unit tests (#21572)
1	1	docs/contributing/README.md

[e0be2c4d0] QiliangCui 2025-07-24 [TPU][Test] Temporarily suspend this MoE model in test_basic.py. (#21560)
2	1	tests/v1/tpu/test_basic.py

[9c8b2c2a8] Nick Hill 2025-07-25 [DP] Support api-server-count > 0 in hybrid DP LB mode (#21510)
1	1	tests/v1/test_hybrid_lb_dp.py
4	8	vllm/entrypoints/cli/serve.py

[2212cd6cf] Varun Sundar Rabindranath 2025-07-25 [Bugfix] DeepGemm utils : Fix hardcoded type-cast (#21517)
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py

[ce3a9b137] Burkhard Ringlein 2025-07-25 [Kernel] adding fused_moe configs for upcoming granite4 (#21332)
146	0	vllm/model_executor/layers/fused_moe/configs/E=62,N=256,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=62,N=512,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=72,N=384,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=72,N=768,device_name=NVIDIA_H100_80GB_HBM3.json

[2ce90e5b0] Yuxuan Zhang 2025-07-25 Fix GLM-4 PP Missing Layer When using with PP. (#21531)
9	3	vllm/model_executor/models/glm4_moe.py

[633f6e804] Wentao Ye 2025-07-24 [Bug] Fix DeepGemm Init Error (#21554)
5	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[b57296bb9] Harry Mellor 2025-07-25 [Docs] Fix `site_url` for RunLLM (#21564)
1	1	mkdocs.yaml

[34ddcf9ff] Cyrus Leung 2025-07-25 [Frontend] `run-batch` supports V1 (#21541)
2	1	benchmarks/benchmark_throughput.py
2	3	tests/entrypoints/openai/test_metrics.py
3	1	vllm/benchmarks/throughput.py
18	4	vllm/entrypoints/openai/api_server.py
29	14	vllm/entrypoints/openai/run_batch.py

[fe56180c7] Woosuk Kwon 2025-07-24 [MoE] More balanced expert sharding (#21497)
10	12	vllm/model_executor/layers/fused_moe/layer.py

[07d80d7b0] QiliangCui 2025-07-24 [TPU][TEST] HF_HUB_DISABLE_XET=1 the test 3. (#21539)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
0	3	tests/entrypoints/llm/test_accuracy.py

[2dd72d23d] weiliang 2025-07-25 update flashinfer to v0.2.9rc1 (#21485)
1	1	docker/Dockerfile
3	7	vllm/attention/backends/flashinfer.py
2	7	vllm/v1/attention/backends/flashinfer.py

[a6c7fb8cf] Simon Mo 2025-07-24 [Docs] Add Expert Parallelism Initial Documentation (#21373)
244	0	docs/serving/expert_parallel_deployment.md

[a7272c23d] Ricardo Decal 2025-07-24 [Docs][minor] Fix broken gh-file link in distributed serving docs (#21543)
1	1	docs/serving/distributed_serving.md

[606628491] Juncheng Gu 2025-07-24 [P/D] Support CPU Transfer in NixlConnector (#18293)
1	0	requirements/tpu.txt
162	0	tests/v1/kv_connector/nixl_integration/run_tpu_disagg_accuracy_test.sh
128	0	tests/v1/kv_connector/nixl_integration/run_tpu_edge_case_test.sh
162	0	tests/v1/kv_connector/nixl_integration/test_disagg_accuracy.py
6	3	tests/v1/kv_connector/nixl_integration/test_edge_cases.py
3	3	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
14	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
229	43	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
6	52	vllm/v1/worker/gpu_model_runner.py
70	0	vllm/v1/worker/kv_connector_model_runner_mixin.py
101	4	vllm/v1/worker/tpu_model_runner.py
11	4	vllm/v1/worker/tpu_worker.py

[1e9ea8e69] Rui Qiao 2025-07-24 [P/D] Move FakeNixlWrapper to test dir (#21328)
136	33	tests/v1/kv_connector/unit/test_nixl_connector.py
4	6	vllm/distributed/kv_transfer/kv_connector/utils.py
0	0	vllm/mocks/__init__.py
0	76	vllm/mocks/mock_nixl_connector.py

[d9f9a3fd9] Chaojun Zhang 2025-07-24 [XPU] Conditionally import CUDA-specific passes to avoid import errors on xpu platform (#21036)
6	3	vllm/compilation/pass_manager.py

[1b25f1fe7] Shu Wang 2025-07-24 Update flashinfer CUTLASS MoE Kernel (#21408)
2	2	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
2	2	vllm/model_executor/layers/quantization/modelopt.py
4	4	vllm/utils/flashinfer.py

[e8cb0d049] Wentao Ye 2025-07-24 [Bug] Fix Compressed Tensor NVFP4 `cutlass_fp4_group_mm` illegal memory access (#21465)
15	12	csrc/quantization/cutlass_w8a8/moe/moe_data.cu

[684174115] Ricardo Decal 2025-07-24 [Docs] Rewrite Distributed Inference and Serving guide (#20593)
79	52	docs/serving/distributed_serving.md

[cdb79ee63] Sanger Steel 2025-07-24 [Docs] Update Tensorizer usage documentation (#21190)
94	5	docs/models/extensions/tensorizer.md
16	13	examples/others/tensorize_vllm_model.py

[5a19a6c67] elvischenv 2025-07-24 [Fix] Update mamba_ssm to 2.2.5 (#21421)
0	8	docker/Dockerfile
1	1	docs/contributing/ci/update_pytorch_version.md
1	1	requirements/test.in
4	2	requirements/test.txt

[2ded067fd] Ming Yang 2025-07-24 [Bugfix] Fix CUDA arch flags for MoE permute (#21426)
3	3	CMakeLists.txt
294	0	tests/kernels/test_shuffle_rows.py

[13abd0eaf] Harry Mellor 2025-07-24 [Model] Officially support Emu3 with Transformers backend (#21319)
6	0	docs/models/supported_models.md
10	7	tests/models/multimodal/test_mapping.py
3	2	tests/models/registry.py
3	3	vllm/model_executor/model_loader/utils.py
7	2	vllm/model_executor/models/registry.py

[61b8cea3b] Lucas Wilkinson 2025-07-24 [Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)
10	3	tests/v1/attention/test_attention_backends.py
1	1	tests/v1/attention/utils.py
83	74	vllm/v1/attention/backends/flashinfer.py

[526078a96] cjackal 2025-07-24 bump `flashinfer` to `v0.2.8` (#21385)
1	1	docker/Dockerfile

[6da007852] Chauncey 2025-07-24 [Feat] Allow custom naming of vLLM processes (#21445)
1	0	requirements/common.txt
1	0	requirements/docs.txt
2	2	vllm/entrypoints/cli/serve.py
4	3	vllm/entrypoints/openai/api_server.py
6	0	vllm/envs.py
14	0	vllm/utils/__init__.py
6	5	vllm/v1/engine/coordinator.py
3	1	vllm/v1/engine/core.py
6	3	vllm/v1/executor/multiproc_executor.py
3	3	vllm/v1/utils.py

[73e3949d0] Rui Qiao 2025-07-24 [Misc] Improve comment for DPEngineCoreActor._set_cuda_visible_devices() (#21501)
7	2	vllm/v1/engine/core.py

[6eca337ce] Shintarou Okada 2025-07-24 Replace `--expand-tools-even-if-tool-choice-none` with `--exclude-tools-when-tool-choice-none` for v0.10.0 (#20544)
2	1	docs/features/tool_calling.md
2	0	vllm/entrypoints/openai/api_server.py
3	0	vllm/entrypoints/openai/cli_args.py
6	1	vllm/entrypoints/openai/serving_chat.py

[85bda9e7d] Yuxuan Zhang 2025-07-24 remove GLM-4.5 quantization wrong Code (#21435)
1	1	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
0	1	vllm/model_executor/models/glm4_moe.py
1	1	vllm/reasoning/glm4_moe_reasoning_parser.py

[610852a42] 22quinn 2025-07-24 [Core] Support model loader plugins (#21067)
1	3	tests/fastsafetensors_loader/test_fastsafetensors_loader.py
0	0	tests/model_executor/model_loader/__init__.py
37	0	tests/model_executor/model_loader/test_registry.py
4	3	tests/runai_model_streamer_test/test_runai_model_streamer_loader.py
6	24	vllm/config.py
13	15	vllm/engine/arg_utils.py
87	27	vllm/model_executor/model_loader/__init__.py
9	9	vllm/model_executor/model_loader/default_loader.py
2	5	vllm/model_executor/model_loader/sharded_state_loader.py

[f0f4de8f2] Nick Hill 2025-07-24 [Misc] Fix duplicate FusedMoEConfig debug messages (#21455)
2	2	vllm/model_executor/layers/fused_moe/config.py

[fc5f756db] Zhou Fang 2025-07-24 [v1][Core] Clean up usages of `SpecializedManager` (#21407)
0	0	tests/v1/core/{test_specialized_manager.py => test_single_type_kv_cache_manager.py}
1	1	vllm/v1/core/single_type_kv_cache_manager.py

[e74bfc70e] Chengji Yao 2025-07-24 [TPU][Bugfix] fix moe layer (#21340)
1	0	tests/v1/tpu/test_basic.py
18	1	vllm/model_executor/layers/fused_moe/layer.py

[90eeea8f8] Gregory Shtrasberg 2025-07-24 [Bugfix][ROCm] Fix for warp_size uses on host (#21205)
1	1	csrc/attention/attention_kernels.cuh
2	3	csrc/attention/paged_attention_v1.cu
2	3	csrc/attention/paged_attention_v2.cu
29	2	csrc/cuda_compat.h
29	18	csrc/moe/topk_softmax_kernels.cu
1	1	csrc/quantization/activation_kernels.cu
1	1	csrc/quantization/gguf/gguf_kernel.cu
1	1	csrc/rocm/attention.cu
1	1	csrc/rocm/skinny_gemms.cu

[dde295a93] Harry Mellor 2025-07-24 Deduplicate Transformers backend code using inheritance (#21461)
49	150	vllm/model_executor/models/transformers.py

[6d8d0a24c] Julien Denize 2025-07-24 Add think chunk (#21333)
1	1	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
5	2	requirements/test.txt
167	0	tests/entrypoints/test_chat_utils.py
341	0	tests/reasoning/test_mistral_reasoning_parser.py
59	0	tests/reasoning/utils.py
27	2	vllm/entrypoints/chat_utils.py
2	0	vllm/reasoning/__init__.py
47	0	vllm/reasoning/mistral_reasoning_parser.py
31	6	vllm/transformers_utils/tokenizers/mistral.py

[11ef7a611] Yinghai Lu 2025-07-23 [BugFix] Set CUDA_VISIBLE_DEVICES before spawning the subprocesses (#21211)
31	20	vllm/v1/engine/core.py
38	6	vllm/v1/engine/utils.py

[dc2f159f8] Woosuk Kwon 2025-07-23 Dump input metadata on crash for async scheduling (#21258)
14	4	vllm/v1/engine/core.py

[d5b981f8b] Robert Shaw 2025-07-23 [DP] Internal Load Balancing Per Node [`one-pod-per-node`] (#21238)
2	0	.buildkite/test-pipeline.yaml
2	2	tests/v1/engine/test_engine_core_client.py
352	0	tests/v1/test_hybrid_lb_dp.py
10	2	vllm/config.py
38	0	vllm/engine/arg_utils.py
10	9	vllm/entrypoints/cli/serve.py
0	7	vllm/entrypoints/openai/cli_args.py
1	1	vllm/v1/engine/async_llm.py
3	2	vllm/v1/engine/coordinator.py
14	5	vllm/v1/engine/core.py
19	8	vllm/v1/engine/core_client.py
35	9	vllm/v1/engine/utils.py

[eec694201] Nick Hill 2025-07-24 [BugFix] Fix KVConnector TP worker aggregation (#21473)
10	8	vllm/v1/worker/gpu_worker.py

[fd48d99ff] KazusatoOoko 2025-07-23 [BugFix]: Batch generation from prompt_embeds fails for long prompts (#21390)
22	14	vllm/worker/model_runner.py

[f8c15c4ef] WeiQing Chen 2025-07-24 [Bugfix] Fix example disagg_example_p2p_nccl_xpyd.sh zombie process (#21437)
1	0	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh

[aa08a954f] Matthew Bonanni 2025-07-23 [Bugfix] Fix casing warning (#21468)
1	1	docker/Dockerfile

[13e4ee1dc] Liangliang Ma 2025-07-24 [XPU][UT] increase intel xpu CI test scope (#21492)
9	0	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	1	docker/Dockerfile.xpu
3	2	tests/entrypoints/openai/correctness/test_lmeval.py

[772ce5af9] Ming Yang 2025-07-23 [Misc] Add dummy maverick test to CI (#21324)
1	0	.buildkite/test-pipeline.yaml
3	0	tests/models/multimodal/generation/test_maverick.py

[63d92abb7] deven-labovitch 2025-07-23 [Frontend] Set MAX_AUDIO_CLIP_FILESIZE_MB via env var instead of hardcoding (#21374)
5	0	docs/serving/openai_compatible_server.md
4	5	vllm/entrypoints/openai/speech_to_text.py
7	0	vllm/envs.py

[11599b0e1] Hardik Gupta 2025-07-23 feat(gguf_loader): accept HF repo paths & URLs for GGUF (#20793)
12	1	vllm/model_executor/model_loader/gguf_loader.py

[f3137cdd8] Michael Goin 2025-07-23 [Core] Freeze gc during cuda graph capture to speed up init (#21146)
7	0	vllm/envs.py
16	1	vllm/v1/worker/gpu_model_runner.py

[82ec66f51] Michael Goin 2025-07-23 [V0 Deprecation] Remove Prompt Adapters (#20588)
0	1	docs/api/README.md
16	18	docs/features/compatibility_matrix.md
0	1	pyproject.toml
30	42	tests/entrypoints/openai/test_completion.py
0	1	tests/entrypoints/openai/test_return_tokens_as_ids.py
1	2	tests/entrypoints/openai/test_serving_models.py
0	48	tests/prompt_adapter/test_bloom.py
0	56	tests/prompt_adapter/test_multi_adapter_inference.py
0	64	tests/prompt_adapter/test_pa_lora.py
0	1	tools/mypy.sh
0	62	vllm/config.py
0	12	vllm/core/scheduler.py
11	38	vllm/engine/arg_utils.py
0	10	vllm/engine/async_llm_engine.py
18	50	vllm/engine/llm_engine.py
0	4	vllm/engine/multiprocessing/__init__.py
1	8	vllm/engine/multiprocessing/client.py
6	8	vllm/engine/multiprocessing/engine.py
0	2	vllm/engine/protocol.py
2	44	vllm/entrypoints/llm.py
2	5	vllm/entrypoints/logger.py
0	1	vllm/entrypoints/openai/api_server.py
1	35	vllm/entrypoints/openai/cli_args.py
0	1	vllm/entrypoints/openai/run_batch.py
3	8	vllm/entrypoints/openai/serving_chat.py
1	9	vllm/entrypoints/openai/serving_classification.py
1	6	vllm/entrypoints/openai/serving_completion.py
1	8	vllm/entrypoints/openai/serving_embedding.py
8	23	vllm/entrypoints/openai/serving_engine.py
0	31	vllm/entrypoints/openai/serving_models.py
2	10	vllm/entrypoints/openai/serving_pooling.py
2	7	vllm/entrypoints/openai/serving_responses.py
3	19	vllm/entrypoints/openai/serving_score.py
4	17	vllm/entrypoints/openai/serving_tokenization.py
2	10	vllm/entrypoints/openai/speech_to_text.py
0	31	vllm/executor/executor_base.py
2	33	vllm/inputs/preprocess.py
0	0	vllm/prompt_adapter/__init__.py
0	83	vllm/prompt_adapter/layers.py
0	358	vllm/prompt_adapter/models.py
0	37	vllm/prompt_adapter/request.py
0	98	vllm/prompt_adapter/utils.py
0	179	vllm/prompt_adapter/worker_manager.py
2	37	vllm/sequence.py
0	5	vllm/utils/__init__.py
1	6	vllm/v1/engine/async_llm.py
1	4	vllm/v1/engine/llm_engine.py
0	6	vllm/v1/engine/processor.py
0	2	vllm/v1/utils.py
0	1	vllm/v1/worker/gpu_model_runner.py
0	1	vllm/v1/worker/tpu_model_runner.py
0	1	vllm/v1/worker/tpu_worker.py
3	4	vllm/worker/enc_dec_model_runner.py
2	149	vllm/worker/model_runner.py
0	1	vllm/worker/model_runner_base.py
0	3	vllm/worker/multi_step_model_runner.py
0	7	vllm/worker/pooling_model_runner.py
0	4	vllm/worker/utils.py
0	14	vllm/worker/worker.py
0	1	vllm/worker/worker_base.py

[78c13e30e] Yong Hoon Shin 2025-07-23 [V1] Fix local chunked attention always disabled (#21419)
2	1	vllm/attention/layer.py

[5c9b807b3] 22quinn 2025-07-23 [Core] Add `reload_weights` RPC method (#20096)
6	1	tests/v1/worker/test_gpu_model_runner.py
10	11	vllm/v1/worker/gpu_model_runner.py
21	12	vllm/v1/worker/gpu_worker.py
11	10	vllm/v1/worker/tpu_model_runner.py
3	0	vllm/v1/worker/tpu_worker.py

[14bf19e39] QiliangCui 2025-07-23 [TPU][TEST] Fix the downloading issue in TPU v1 test 11.  (#21418)
3	2	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[4ac7713e3] Yong Hoon Shin 2025-07-23 Add test case for compiling multiple graphs (#21044)
350	0	tests/compile/piecewise/test_multiple_graphs.py
6	0	vllm/compilation/compiler_interface.py
34	1	vllm/compilation/decorators.py

[8560a5b25] Christian Pinto 2025-07-23  [Core][Model] PrithviMAE Enablement on vLLM v1 engine (#20577)
70	175	examples/offline_inference/prithvi_geospatial_mae.py
1	0	requirements/test.in
360	14	requirements/test.txt
63	0	tests/models/multimodal/pooling/test_prithvi_mae.py
4	2	vllm/config.py
5	5	vllm/engine/llm_engine.py
34	0	vllm/model_executor/models/interfaces.py
57	17	vllm/model_executor/models/prithvi_geospatial_mae.py
11	2	vllm/model_executor/models/registry.py
1	1	vllm/multimodal/registry.py
12	5	vllm/v1/engine/async_llm.py
8	5	vllm/v1/engine/llm_engine.py
10	8	vllm/v1/engine/output_processor.py
8	4	vllm/v1/engine/processor.py
60	0	vllm/v1/worker/gpu_model_runner.py

[316b1bf70] Nick Hill 2025-07-23 [Tests] Add tests for headless internal DP LB (#21450)
2	0	.buildkite/test-pipeline.yaml
3	120	tests/v1/entrypoints/openai/test_multi_api_servers.py
639	0	tests/v1/test_internal_lb_dp.py
124	0	tests/v1/test_utils.py

[7c734ee09] Tao He 2025-07-23 [Bugfix][Qwen][DCA] fixes bug in dual-chunk-flash-attn backend for qwen 1m models. (#21364)
0	8	vllm/attention/backends/dual_chunk_flash_attn.py

[f59ec35b7] Cyrus Leung 2025-07-23 [V1] Check all pooling tasks during profiling (#21299)
7	0	vllm/sequence.py
40	23	vllm/v1/worker/gpu_model_runner.py

[2671334d4] Asher 2025-07-23 [Model] add Hunyuan V1 Dense Model support. (#21368)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
52	18	vllm/model_executor/models/{hunyuan_v1_moe.py => hunyuan_v1.py}
2	1	vllm/model_executor/models/registry.py

[2cc5016a1] Michael Yao 2025-07-23 [Docs] Clean up v1/metrics.md (#21449)
73	92	docs/design/v1/metrics.md

[6929f8b43] Yang Chen 2025-07-23 [Misc] fixed nvfp4_moe test failures due to invalid kwargs (#21246)
2	2	tests/kernels/moe/test_nvfp4_moe.py

[32ec9e2f2] Yu Chin Fabian Lim 2025-07-23 Mamba V2 Test not Asserting Failures.  (#21379)
5	4	tests/kernels/mamba/test_mamba_mixer2.py
20	6	tests/kernels/mamba/test_mamba_ssm_ssd.py

[accac8292] Lu Fang 2025-07-23 [Sampler] Introduce logprobs mode for logging (#21398)
43	0	tests/v1/sample/test_logprobs.py
9	0	vllm/config.py
11	7	vllm/engine/arg_utils.py
15	2	vllm/v1/sample/sampler.py
1	0	vllm/v1/sample/tpu/sampler.py
2	2	vllm/v1/worker/gpu_input_batch.py
2	2	vllm/v1/worker/gpu_model_runner.py

[23637dcde] Michael Yao 2025-07-23 [Docs] Fix bullets and grammars in tool_calling.md (#21440)
35	31	docs/features/tool_calling.md

[6364af92f] Sergio Paniego Blanco 2025-07-23 Fixed typo in profiling logs (#21441)
1	1	vllm/multimodal/profiling.py

[7aaa2bd5a] Guillaume Calmettes 2025-07-23 [Bugfix] ensure tool_choice is popped when `tool_choice:null` is passed in json payload (#19679)
2	2	vllm/entrypoints/openai/protocol.py

[2f5c14de6] youkaichao 2025-07-23 add clear messages for deprecated models (#21424)
10	1	vllm/model_executor/model_loader/utils.py
2	0	vllm/model_executor/models/registry.py

[f002e9a87] Michael Goin 2025-07-23 [Cleanup] Only log MoE DP setup warning if DP is enabled (#21315)
5	4	vllm/model_executor/layers/fused_moe/config.py

[a1f3610fc] Jialin Ouyang 2025-07-23 [Core] Add basic unit test for maybe_evict_cached_block (#21400)
67	0	tests/v1/core/test_prefix_caching.py

[4ecedd180] Isotr0py 2025-07-23 [Bugfix] Fix nightly transformers CI failure (#21427)
6	6	tests/models/registry.py
1	5	vllm/model_executor/models/tarsier.py
2	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
56	0	vllm/transformers_utils/configs/nemotron_vl.py

[107111a85] Alexei-V-Ivanov-AMD 2025-07-22 Changing "amdproduction" allocation. (#21409)
11	11	.buildkite/test-pipeline.yaml

[2dec7c1a5] elvischenv 2025-07-23 [Bugfix][CUDA] fixes CUDA FP8 kv cache dtype supported (#21420)
13	13	vllm/platforms/cuda.py

[08d2bd78d] Chendi.Xue 2025-07-22 [BUGFIX] deepseek-v2-lite failed due to fused_qkv_a_proj name update (#21414)
5	2	vllm/model_executor/models/deepseek_v2.py

[4f76a05f4] ericehanley 2025-07-22 [BugFix] Update python to python3 calls for image; fix prefix & input calculations. (#21391)
5	4	benchmarks/auto_tune/auto_tune.sh

[f154bb9ff] Harry Mellor 2025-07-23 Simplify weight loading in Transformers backend (#21382)
2	2	tests/distributed/test_pipeline_parallel.py
1	1	tests/lora/test_transformers_model.py
1	1	tests/models/registry.py
1	1	tests/models/test_transformers.py
3	7	vllm/model_executor/models/interfaces.py
44	63	vllm/model_executor/models/transformers.py
1	1	vllm/test_utils.py

[3ec7170ff] Gregory Shtrasberg 2025-07-22 [Bugfix][ROCm][Build] Fix build regression on ROCm (#21393)
2	2	CMakeLists.txt
5	5	csrc/ops.h
9	9	csrc/torch_bindings.cpp

[c401c64b4] Cyrus Leung 2025-07-23 [CI/Build] Fix model executor tests (#21387)
0	1	.buildkite/test-pipeline.yaml
9	4	tests/model_executor/test_model_load_with_params.py

[b77c7d327] Joe Runde 2025-07-22 [BugFix] Fix ray import error mem cleanup bug (#21381)
3	2	vllm/config.py
5	3	vllm/executor/ray_utils.py

[35bc8bd5f] Rui Qiao 2025-07-22 [Misc] Copy HF_TOKEN env var to Ray workers (#21406)
5	1	vllm/executor/ray_distributed_executor.py
3	2	vllm/ray/ray_env.py

[4594fc3b2] Yiheng Xu 2025-07-22 [Model] Add Qwen3CoderToolParser (#21396)
618	0	tests/tool_use/test_qwen3coder_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
669	0	vllm/entrypoints/openai/tool_parsers/qwen3coder_tool_parser.py

[ae268b632] Xin Li 2025-07-22 Fix Flashinfer Allreduce+Norm enable disable calculation based on `fi_allreduce_fusion_max_token_num` (#21325)
13	6	vllm/compilation/collective_fusion.py

[35366ae57] Cyrus Leung 2025-07-22 [CI/Build] Fix test failure due to updated model repo (#21375)
2	2	tests/models/registry.py

[2226d5bd8] Aritra Roy Gosthipaty 2025-07-22 [Bugfix] Decode Tokenized IDs to Strings for `hf_processor` in `llm.chat()` with `model_impl=transformers` (#21353)
40	0	tests/models/multimodal/processing/test_transformers.py
5	0	vllm/model_executor/models/transformers.py

[44554a006] Wang Yijun 2025-07-22 Add tokenization_kwargs to encode for embedding model truncation (#21033)
6	0	vllm/engine/async_llm_engine.py
12	3	vllm/entrypoints/llm.py
2	0	vllm/v1/engine/async_llm.py

[226b452a2] Wentao Ye 2025-07-22 Revert "[Refactor] Fix Compile Warning #1444-D (#21208)" (#21384)
1	2	csrc/moe/topk_softmax_kernels.cu

[f38ee34a0] Raushan Turganbay 2025-07-22 [feat] Enable mm caching for transformers backend (#21358)
1	1	docs/models/supported_models.md
0	8	tests/models/multimodal/generation/test_common.py
3	6	vllm/model_executor/models/transformers.py
3	3	vllm/v1/core/kv_cache_utils.py

[b194557a6] Benjamin Bartels 2025-07-22 Adds parallel model weight loading for runai_streamer (#21330)
2	1	setup.py
14	8	vllm/model_executor/model_loader/weight_utils.py

[774d0c014] Wentao Ye 2025-07-22 [Perf] Cuda Kernel for Per Token Group Quant (#21083)
1	0	CMakeLists.txt
5	0	csrc/ops.h
213	0	csrc/quantization/fp8/per_token_group_quant.cu
9	0	csrc/torch_bindings.cpp
44	0	tests/kernels/quantization/test_per_token_group_quant.py
13	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[2c8db17cf] Duncan Moss 2025-07-22 [feat]: add SM100 support for cutlass FP8 groupGEMM (#20447)
21	1	CMakeLists.txt
9	4	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
140	0	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x_sm100.cu
17	13	csrc/quantization/cutlass_w8a8/moe/{grouped_mm_c3x.cu => grouped_mm_c3x_sm90.cu}
1	1	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
34	11	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
6	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
27	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[4fb56914c] Mickaël Seznec 2025-07-22 [perf] Add fused MLA QKV + strided layernorm (#21116)
40	23	csrc/layernorm_kernels.cu
25	14	csrc/layernorm_quant_kernels.cu
4	0	csrc/quantization/fp8/common.cu
19	7	tests/kernels/core/test_layernorm.py
77	1	vllm/model_executor/layers/linear.py
10	3	vllm/model_executor/layers/quantization/fp8.py
39	18	vllm/model_executor/models/deepseek_v2.py

[0df4d9b06] Ning Xie 2025-07-22 [Misc] unify variable for LLM instance v2 (#21356)
4	4	tests/models/language/generation/test_gemma.py

[ed2505457] Jialin Ouyang 2025-07-22 [Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool (#21222)
105	0	tests/v1/core/test_kv_cache_utils.py
20	20	vllm/v1/core/block_pool.py
58	0	vllm/v1/core/kv_cache_utils.py

[10904e6d7] Jialin Ouyang 2025-07-22 [benchmark] Port benchmark request sent optimization to benchmark_serving (#21209)
2	96	benchmarks/benchmark_serving.py
5	5	vllm/benchmarks/serve.py

[a32237665] Jialin Ouyang 2025-07-22 [Core] Optimize update checks in LogitsProcessor (#21245)
13	5	vllm/v1/sample/logits_processor.py

[bc8a8ce5e] Kebe 2025-07-22 [Misc] Remove deprecated args in v0.10 (#21349)
0	1	examples/offline_inference/neuron_speculation.py
0	1	tests/neuron/2_core/test_mistral.py
0	2	tests/neuron/2_core/test_multi_lora.py
0	21	vllm/engine/arg_utils.py

[32142b3c6] Simon Mo 2025-07-22 [Bugfix] Fix eviction cached blocked logic (#21357)
1	1	vllm/v1/core/block_pool.py

[82b8027be] Raghav Ravishankar 2025-07-22 Add arcee model (#21296)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
347	0	vllm/model_executor/models/arcee.py
1	0	vllm/model_executor/models/registry.py

[3779eb8c8] rongfu.leng 2025-07-22 [Feature][eplb] add verify ep or tp or dp (#21102)
9	0	vllm/config.py

[9e23ad965] Shu Wang 2025-07-22 Update fp4 quantize API (#21327)
5	5	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
2	2	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
4	4	vllm/utils/flashinfer.py

[e69a92a1c] Wentao Ye 2025-07-22 [Bug] DeepGemm: Fix Cuda Init Error (#21312)
32	22	vllm/utils/deep_gemm.py

[8425f785a] Varun Sundar Rabindranath 2025-07-22 [Misc] DeepEPHighThroughtput - Enable Inductor pass (#21311)
0	3	vllm/platforms/cuda.py

[c17231e82] Konrad Zawora 2025-07-22 Fix kv_cache_dtype handling for out-of-tree HPU plugin (#21302)
2	16	vllm/engine/arg_utils.py
13	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py
4	0	vllm/platforms/tpu.py

[6e5b5ca58] Wentao Ye 2025-07-22 [Refactor] Fix Compile Warning #1444-D (#21208)
2	1	csrc/moe/topk_softmax_kernels.cu

[488d8a986] Thomas Parnell 2025-07-22 [V1] [Hybrid] Add new test to verify that hybrid views into KVCacheTensor are compatible (#21300)
149	1	tests/v1/worker/test_gpu_model_runner.py

[af376ca19] Jialin Ouyang 2025-07-21 [Core] Minimize number of dict lookup in _maybe_evict_cached_block (#21281)
21	16	vllm/v1/core/block_pool.py

[e7b204268] Ming Yang 2025-07-21 Revert "[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)
1	34	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
11	42	csrc/moe/moe_permute_unpermute_op.cu
2	12	tests/kernels/moe/test_cutlass_moe.py
0	22	tests/kernels/moe/test_pplx_cutlass_moe.py
23	39	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	25	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[90f1e5542] Ratnam Parikh 2025-07-21 [Intel GPU] Ray Compiled Graph avoid NCCL for Intel GPU (#21338)
2	2	vllm/executor/ray_distributed_executor.py

[5e70dcd6e] Li, Jiang 2025-07-22 [Doc] Fix CPU doc format (#21316)
10	9	docs/getting_started/installation/cpu.md

[25d585ab7] Chaojun Zhang 2025-07-22 [XPU] Enable external_launcher to serve as an executor via torchrun (#21021)
3	1	vllm/v1/worker/xpu_worker.py

[8d0a01a5f] Lu Fang 2025-07-21 [v1][sampler] Inplace logprobs comparison to get the token rank (#21283)
24	0	vllm/v1/sample/ops/logprobs.py
2	1	vllm/v1/sample/sampler.py

[0ec82edda] Himanshu Jaju 2025-07-21 [perf] Speed up align sum kernels (#21079)
2	5	benchmarks/kernels/benchmark_moe_align_block_size.py
55	16	csrc/moe/moe_align_sum_kernels.cu
3	4	vllm/model_executor/layers/fused_moe/moe_align_block_size.py

[005ae9be6] Michael Goin 2025-07-21 Fix bad lm-eval fork (#21318)
1	1	.buildkite/test-pipeline.yaml

[29d1ffc5b] Robert Shaw 2025-07-21 [DP] Fix Prometheus Logging (#21257)
4	3	tests/v1/engine/test_async_llm.py
4	2	tests/v1/test_async_llm_dp.py
26	43	vllm/v1/engine/async_llm.py
5	4	vllm/v1/engine/core_client.py
339	202	vllm/v1/metrics/loggers.py
0	4	vllm/v1/metrics/ray_wrappers.py

[304dce7ec] Lucas Wilkinson 2025-07-21 [Attention] Clean up iRoPE in V1 (#21188)
7	0	vllm/attention/layer.py
0	5	vllm/v1/attention/backends/cpu_attn.py
0	2	vllm/v1/attention/backends/flash_attn.py
0	2	vllm/v1/attention/backends/flashinfer.py
0	5	vllm/v1/attention/backends/pallas.py
0	2	vllm/v1/attention/backends/rocm_aiter_fa.py
0	6	vllm/v1/attention/backends/triton_attn.py
3	4	vllm/v1/worker/gpu_model_runner.py
4	0	vllm/v1/worker/tpu_model_runner.py

[6ece16c4f] Ming Yang 2025-07-21 [Misc] Add dummy maverick test (#21199)
649	0	tests/models/multimodal/generation/test_maverick.py

[a0e827e07] simpx 2025-07-22 [BugFix] make utils.current_stream thread-safety (#21252) (#21253)
41	3	tests/test_utils.py
7	8	vllm/utils/__init__.py

[a15a50fc1] Li, Jiang 2025-07-22 [CPU] Enable shared-memory based pipeline parallel for CPU backend (#21289)
9	9	.buildkite/scripts/hardware_ci/run-cpu-test.sh
48	21	csrc/cpu/shm.cpp
14	0	docs/getting_started/installation/cpu.md
58	2	vllm/distributed/device_communicators/cpu_communicator.py
12	0	vllm/distributed/parallel_state.py
5	4	vllm/engine/arg_utils.py
4	3	vllm/envs.py
15	20	vllm/platforms/cpu.py

[6dda13c86] Woosuk Kwon 2025-07-21 [Misc] Add sliding window to flashinfer test (#21282)
31	18	tests/kernels/attention/test_flashinfer.py

[6b46c4b65] Zhiyu 2025-07-21 Add Nvidia ModelOpt config adaptation (#19815)
91	0	tests/quantization/test_modelopt.py
13	7	vllm/config.py
183	25	vllm/model_executor/layers/quantization/modelopt.py

[d97841078] Ning Xie 2025-07-21 [Misc] unify variable for LLM instance (#20996)
1	1	docs/configuration/model_resolution.md
2	2	docs/features/lora.md
6	4	docs/features/quantization/fp8.md
2	1	docs/features/quantization/int4.md
2	1	docs/features/quantization/int8.md
5	5	docs/models/pooling_models.md
2	2	examples/offline_inference/basic/classify.py
2	2	examples/offline_inference/basic/embed.py
2	2	examples/offline_inference/basic/score.py
2	2	examples/offline_inference/embed_jina_embeddings_v3.py
2	2	examples/offline_inference/embed_matryoshka_fy.py
6	6	examples/offline_inference/neuron_speculation.py
2	2	examples/offline_inference/prithvi_geospatial_mae.py
4	4	examples/offline_inference/qwen3_reranker.py
2	2	tests/basic_correctness/test_basic_correctness.py
5	5	tests/basic_correctness/test_preemption.py
16	16	tests/conftest.py
1	1	tests/core/test_num_computed_tokens_update.py
1	1	tests/detokenizer/test_stop_reason.py
21	21	tests/detokenizer/test_stop_strings.py
10	10	tests/lora/test_llama_tp.py
7	7	tests/metrics/test_metrics.py
5	5	tests/model_executor/test_model_load_with_params.py
1	1	tests/models/language/generation/test_hybrid.py
7	7	tests/models/language/generation/test_mistral.py
9	9	tests/models/language/pooling/mteb_utils.py
2	2	tests/models/language/pooling/test_gritlm.py
2	2	tests/models/language/pooling/test_jina.py
3	3	tests/models/language/pooling/test_nomic_max_model_len.py
3	3	tests/models/language/pooling/test_truncation_control.py
2	3	tests/models/multimodal/generation/test_pixtral.py
1	1	tests/models/multimodal/generation/test_whisper.py
1	1	tests/models/multimodal/generation/vlm_utils/core.py
1	1	tests/models/multimodal/pooling/test_dse_qwen2_vl.py
1	1	tests/models/multimodal/pooling/test_jinavl_reranker.py
3	3	tests/models/quantization/test_modelopt.py
3	3	tests/models/quantization/test_nvfp4.py
11	11	tests/prefix_caching/test_disable_sliding_window.py
3	3	tests/prefix_caching/test_prefix_caching.py
1	1	tests/quantization/test_gptq_dynamic.py
2	2	tests/quantization/test_quark.py
1	1	tests/quantization/test_register_quantization_config.py
1	1	tests/samplers/test_ignore_eos.py
5	5	tests/samplers/test_logits_processor.py
2	2	tests/samplers/test_logprobs.py
6	6	tests/samplers/test_no_bad_words.py
1	1	tests/samplers/test_seeded_generate.py
1	1	tests/tokenization/test_detokenize.py
6	6	tests/v1/core/test_scheduler_e2e.py
7	7	tests/v1/engine/test_llm_engine.py
4	4	tests/v1/sample/test_logprobs.py
36	38	tests/v1/sample/test_sampling_params_e2e.py
3	3	tests/v1/test_oracle.py

[e6b90a280] Harry Mellor 2025-07-21 [Docs] Make tables more space efficient in `supported_models.md` (#21291)
7	0	docs/models/supported_models.md

[be54a951a] Harry Mellor 2025-07-21 [Docs] Fix hardcoded links in docs (#21287)
2	3	docs/design/v1/metrics.md
1	1	docs/features/multimodal_inputs.md
1	1	docs/features/quantization/bitblas.md
1	1	docs/features/tool_calling.md
1	1	docs/models/extensions/tensorizer.md

[042af0c8d] Cyrus Leung 2025-07-21 [Model][1/N] Support multiple poolers at model level (#21227)
39	14	docs/models/pooling_models.md
1	1	tests/models/test_transformers.py
8	7	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
4	4	vllm/config.py
1	1	vllm/entrypoints/openai/api_server.py
175	171	vllm/model_executor/layers/pooler.py
51	57	vllm/model_executor/models/adapters.py
99	33	vllm/model_executor/models/bert.py
10	6	vllm/model_executor/models/gpt2.py
19	20	vllm/model_executor/models/gritlm.py
5	7	vllm/model_executor/models/internlm2.py
14	15	vllm/model_executor/models/jamba.py
12	6	vllm/model_executor/models/jina_vl.py
38	12	vllm/model_executor/models/modernbert.py
21	14	vllm/model_executor/models/qwen2_rm.py
27	17	vllm/model_executor/models/roberta.py
7	0	vllm/model_executor/pooling_metadata.py
8	0	vllm/v1/pool/metadata.py
4	12	vllm/v1/worker/gpu_model_runner.py
2	5	vllm/v1/worker/tpu_model_runner.py
2	5	vllm/worker/model_runner_base.py
3	7	vllm/worker/pooling_model_runner.py

[378d33c39] Cyrus Leung 2025-07-21 [Bugfix] Fix missing placeholder in logger debug (#21280)
1	1	vllm/transformers_utils/configs/mistral.py

[940af1f03] Huy Do 2025-07-20 Add the instruction to run e2e validation manually before release (#21023)
33	0	RELEASE.md

[92615d7fe] Simon Mo 2025-07-20 [Docs] Add RFC Meeting to Issue Template (#21279)
1	1	.github/ISSUE_TEMPLATE/750-RFC.yml

[8188196a1] Kay Yan 2025-07-21 [CI] Cleanup modelscope version constraint in Dockerfile (#21243)
1	1	docker/Dockerfile
1	1	docker/Dockerfile.xpu

[7ba34b124] Jiayi Yan 2025-07-21 [bugfix] fix syntax warning caused by backslash (#21251)
1	1	examples/offline_inference/neuron_eagle.py
1	1	tests/v1/kv_connector/unit/test_nixl_connector.py

[9499e26e2] Raushan Turganbay 2025-07-20 [Model] Support VLMs with transformers backend (#20543)
6	3	docs/models/supported_models.md
75	0	tests/models/multimodal/generation/test_common.py
1	0	tests/models/registry.py
30	9	vllm/config.py
26	23	vllm/model_executor/model_loader/utils.py
9	3	vllm/model_executor/models/registry.py
478	49	vllm/model_executor/models/transformers.py

[51ba83955] Calvin Chen 2025-07-20 [Model] use AutoWeightsLoader for bart (#18299)
71	101	vllm/model_executor/models/bart.py

[d1fb65bde] Seiji Eicher 2025-07-19 Enable v1 metrics tests (#20953)
1	0	.buildkite/test-pipeline.yaml
12	6	tests/v1/metrics/test_ray_metrics.py
7	1	vllm/v1/metrics/ray_wrappers.py

[3a1d8940a] Chengji Yao 2025-07-19 [TPU] support fp8 kv cache quantization (#19292)
30	10	tests/entrypoints/llm/test_accuracy.py
2	0	tests/v1/tpu/test_pallas.py
4	4	vllm/engine/arg_utils.py
3	1	vllm/platforms/tpu.py
50	8	vllm/v1/attention/backends/pallas.py
6	5	vllm/v1/worker/tpu_model_runner.py

[2b504eb77] Thomas Parnell 2025-07-20 [Docs] [V1] Update docs to remove enforce_eager limitation for hybrid models. (#21233)
2	3	docs/usage/v1_guide.md

[10eb24cc9] Yuxuan Zhang 2025-07-20 GLM-4 Update (#20736)
5	1	benchmarks/kernels/benchmark_moe.py
1	0	benchmarks/kernels/benchmark_moe_permute_unpermute.py
1	0	docs/models/supported_models.md
7	0	tests/models/registry.py
410	0	tests/tool_use/test_glm4_moe_tool_parser.py
12	3	vllm/config.py
19	6	vllm/entrypoints/openai/tool_parsers/__init__.py
402	0	vllm/entrypoints/openai/tool_parsers/glm4_moe_tool_parser.py
685	0	vllm/model_executor/models/glm4_moe.py
307	0	vllm/model_executor/models/glm4_moe_mtp.py
2	0	vllm/model_executor/models/registry.py
2	0	vllm/reasoning/__init__.py
151	0	vllm/reasoning/glm4_moe_reasoning_parser.py
2	1	vllm/worker/worker.py

[2e8cbb58f] fhl2000 2025-07-20 [BugFix] Fix full cuda graph slot_mapping (#21228)
1	1	vllm/v1/worker/gpu_model_runner.py

[752c6ade2] Woosuk Kwon 2025-07-19 [V0 Deprecation] Deprecate BlockSparse Attention & Phi3-Small (#21217)
0	1	.buildkite/scripts/hardware_ci/run-amd-test.sh
0	1	docs/models/supported_models.md
0	441	tests/kernels/attention/test_blocksparse_attention.py
24	8	tests/kernels/attention/test_rocm_attention_selector.py
0	4	tests/models/registry.py
0	1	vllm/attention/backends/abstract.py
0	466	vllm/attention/backends/blocksparse_attn.py
0	4	vllm/attention/backends/differential_flash_attn.py
0	1	vllm/attention/backends/dual_chunk_flash_attn.py
1	5	vllm/attention/backends/flash_attn.py
0	1	vllm/attention/backends/flashinfer.py
4	8	vllm/attention/backends/flashmla.py
0	1	vllm/attention/backends/mla/common.py
4	8	vllm/attention/backends/rocm_aiter_mla.py
1	5	vllm/attention/backends/rocm_flash_attn.py
4	8	vllm/attention/backends/triton_mla.py
1	5	vllm/attention/backends/xformers.py
2	4	vllm/attention/layer.py
0	0	vllm/attention/ops/blocksparse_attention/__init__.py
0	433	vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
0	239	vllm/attention/ops/blocksparse_attention/interface.py
0	246	vllm/attention/ops/blocksparse_attention/utils.py
0	9	vllm/attention/selector.py
0	465	vllm/model_executor/models/phi3_small.py
0	1	vllm/model_executor/models/registry.py
0	1	vllm/platforms/interface.py
1	5	vllm/v1/attention/backends/cpu_attn.py
1	5	vllm/v1/attention/backends/flash_attn.py
1	2	vllm/v1/attention/backends/flashinfer.py
1	6	vllm/v1/attention/backends/flex_attention.py
1	2	vllm/v1/attention/backends/mla/common.py
4	8	vllm/v1/attention/backends/mla/cutlass_mla.py
4	8	vllm/v1/attention/backends/mla/flashmla.py
4	8	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
4	8	vllm/v1/attention/backends/mla/triton_mla.py
1	7	vllm/v1/attention/backends/pallas.py
1	5	vllm/v1/attention/backends/rocm_aiter_fa.py
1	5	vllm/v1/attention/backends/triton_attn.py

[881e3cbe3] Thomas Parnell 2025-07-19 [V1] [Hybrid] Enable piecewise CUDA Graph for mamba layers  (#21194)
0	1	tests/models/language/generation/test_hybrid.py
1	0	vllm/config.py
66	9	vllm/model_executor/layers/mamba/mamba_mixer2.py
6	5	vllm/model_executor/models/bamba.py
6	2	vllm/model_executor/models/falcon_h1.py
5	3	vllm/model_executor/models/granitemoehybrid.py
5	3	vllm/model_executor/models/mamba2.py
5	3	vllm/model_executor/models/nemotron_h.py
6	2	vllm/model_executor/models/zamba2.py
0	3	vllm/v1/worker/gpu_model_runner.py

[9f414a12a] kourosh hakhamaneshi 2025-07-19 [BugFix] Make PD work with Ray (#21072)
42	75	tests/v1/kv_connector/unit/test_nixl_connector.py
9	28	tests/v1/{executor/test_multiproc_executor.py => kv_connector/unit/test_output_aggreagator.py}
90	0	vllm/distributed/kv_transfer/kv_connector/utils.py
1	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
0	0	vllm/mocks/__init__.py
76	0	vllm/mocks/mock_nixl_connector.py
6	0	vllm/sequence.py
7	79	vllm/v1/executor/multiproc_executor.py
45	12	vllm/v1/executor/ray_distributed_executor.py
43	6	vllm/v1/worker/gpu_model_runner.py
10	20	vllm/v1/worker/gpu_worker.py

[6a971ed69] Jiayi Yan 2025-07-19 [Docs] Update the link to the 'Prometheus/Grafana' example (#21225)
1	1	docs/design/v1/metrics.md

[da6579bf4] Sungjae Lee 2025-07-19 [CI/CD][bugfix]fix: error argument to loads has incompatible type (#21223)
2	2	vllm/engine/arg_utils.py

[c81259d33] Rabi Mishra 2025-07-19 Fix/remove some broken model executor tests (#21224)
0	13	tests/model_executor/test_guided_processors.py
3	3	tests/model_executor/test_model_load_with_params.py

[e3a0e43d7] Li, Jiang 2025-07-19 [bugfix] Fix auto thread-binding when world_size > 1 in CPU backend and refactor code (#21032)
2	2	.buildkite/scripts/hardware_ci/run-cpu-test.sh
7	3	docs/getting_started/installation/cpu.md
0	2	requirements/cpu.txt
3	2	vllm/envs.py
64	0	vllm/platforms/cpu.py
4	3	vllm/v1/worker/cpu_model_runner.py
64	138	vllm/v1/worker/cpu_worker.py

[b3d82108e] 22quinn 2025-07-19 [Bugfix][Frontend] Fix openai CLI arg `middleware` (#21220)
10	0	tests/entrypoints/openai/test_cli_args.py
4	0	vllm/entrypoints/openai/cli_args.py

[6d0734c56] Kaixi Hou 2025-07-19 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)
8	3	vllm/envs.py
1	1	vllm/model_executor/layers/fused_moe/config.py
99	1	vllm/model_executor/layers/fused_moe/fused_moe.py
63	19	vllm/model_executor/layers/quantization/fp8.py
4	5	vllm/model_executor/layers/quantization/modelopt.py
12	2	vllm/utils/flashinfer.py

[7d9457713] shixianc 2025-07-19 Add torch golden impl for moe_align_block_size kernel test (#20653)
296	71	tests/kernels/moe/test_moe_align_block_size.py

[59f935300] Lucas Wilkinson 2025-07-19 [BugFix] Fix potential cuda-graph IMA (#21196)
0	5	vllm/v1/attention/backends/utils.py
6	1	vllm/v1/worker/gpu_model_runner.py

[18e519ec8] Isotr0py 2025-07-19 [Bugfix] Fix ndarray video color from VideoAsset (#21064)
80	23	tests/multimodal/test_video.py
46	0	tests/multimodal/utils.py
4	5	vllm/assets/video.py

[1eaff2781] Jee Jee Li 2025-07-19 [V0 deprecation] Remove long context LoRA (#21169)
0	5	tests/lora/conftest.py
4	7	tests/lora/test_peft_helper.py
1	13	vllm/config.py
0	5	vllm/engine/arg_utils.py
0	90	vllm/lora/layers.py
10	70	vllm/lora/models.py
0	9	vllm/lora/peft_helper.py
9	36	vllm/lora/punica_wrapper/punica_base.py
5	16	vllm/lora/punica_wrapper/punica_gpu.py
0	14	vllm/lora/punica_wrapper/punica_tpu.py
5	33	vllm/lora/punica_wrapper/utils.py
0	2	vllm/lora/utils.py
1	1	vllm/lora/worker_manager.py

[cf8cc3267] Huy Do 2025-07-19 Fix a couple of Voxtral tests (#21218)
5	1	tests/models/registry.py

[3a2cb2649] Chenyaaang 2025-07-19 [Misc][Tools][Benchmark] Add readme file for auto_tune script (#20779)
137	0	benchmarks/auto_tune/README.md
1	30	benchmarks/{ => auto_tune}/auto_tune.sh

[3e04107d9] 김종곤 2025-07-19 [Model] EXAONE 4.0 model support (#21060)
1	0	docs/models/supported_models.md
1	0	tests/models/registry.py
547	0	vllm/model_executor/models/exaone4.py
1	0	vllm/model_executor/models/registry.py
5	3	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
252	0	vllm/transformers_utils/configs/exaone4.py

[37bd8d6e4] Wentao Ye 2025-07-19 [Bug] DeepGemm: Fix TypeError: per_block_cast_to_fp8() missing 1 required positional argument: 'use_ue8m0' for SM100 (#21187)
1	1	vllm/utils/deep_gemm.py

[468e2400f] Lucas Wilkinson 2025-07-19 [BugFix][CPU] Fix `TorchSDPABackendImpl` doesn't have `use_irope`  (#21200)
2	1	vllm/v1/worker/gpu_model_runner.py

[dcc6cfb99] Varun Sundar Rabindranath 2025-07-19 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)
4	5	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[dd572c0ab] Woosuk Kwon 2025-07-18 [V0 Deprecation] Remove V0 Spec Decode workers (#21152)
0	14	.buildkite/test-pipeline.yaml
0	1	.github/CODEOWNERS
0	3	.github/mergify.yml
0	1	pyproject.toml
1	1	tests/core/test_serialization.py
131	3	tests/core/utils.py
0	146	tests/metrics/test_metrics.py
4	4	tests/models/registry.py
9	5	tests/models/test_registry.py
0	577	tests/samplers/test_rejection_sampler.py
0	480	tests/samplers/test_typical_acceptance_sampler.py
0	0	tests/spec_decode/__init__.py
0	12	tests/spec_decode/conftest.py
0	0	tests/spec_decode/e2e/__init__.py
0	307	tests/spec_decode/e2e/conftest.py
0	66	tests/spec_decode/e2e/test_compatibility.py
0	480	tests/spec_decode/e2e/test_eagle_correctness.py
0	161	tests/spec_decode/e2e/test_integration.py
0	247	tests/spec_decode/e2e/test_integration_dist_tp2.py
0	123	tests/spec_decode/e2e/test_integration_dist_tp4.py
0	315	tests/spec_decode/e2e/test_logprobs.py
0	417	tests/spec_decode/e2e/test_medusa_correctness.py
0	533	tests/spec_decode/e2e/test_mlp_correctness.py
0	333	tests/spec_decode/e2e/test_mtp_correctness.py
0	842	tests/spec_decode/e2e/test_multistep_correctness.py
0	392	tests/spec_decode/e2e/test_ngram_correctness.py
0	70	tests/spec_decode/e2e/test_seed.py
0	110	tests/spec_decode/test_batch_expansion.py
0	90	tests/spec_decode/test_dynamic_spec_decode.py
0	91	tests/spec_decode/test_memory_usage.py
0	205	tests/spec_decode/test_metrics.py
0	838	tests/spec_decode/test_multi_step_worker.py
0	221	tests/spec_decode/test_ngram_worker.py
0	116	tests/spec_decode/test_scorer.py
0	945	tests/spec_decode/test_spec_decode_worker.py
0	150	tests/spec_decode/test_utils.py
0	290	tests/spec_decode/utils.py
0	1	tests/test_sequence.py
0	6	tests/v1/test_oracle.py
0	1	tools/mypy.sh
7	54	vllm/config.py
6	22	vllm/engine/arg_utils.py
0	8	vllm/engine/llm_engine.py
0	66	vllm/engine/metrics.py
1	11	vllm/engine/metrics_types.py
0	5	vllm/engine/output_processor/multi_step.py
0	406	vllm/model_executor/layers/rejection_sampler.py
3	9	vllm/model_executor/layers/sampler.py
0	259	vllm/model_executor/layers/spec_decode_base_sampler.py
0	166	vllm/model_executor/layers/typical_acceptance_sampler.py
0	261	vllm/model_executor/models/eagle.py
3	2	vllm/model_executor/models/registry.py
4	8	vllm/platforms/cuda.py
3	8	vllm/platforms/rocm.py
1	13	vllm/sequence.py
0	0	vllm/spec_decode/__init__.py
0	506	vllm/spec_decode/batch_expansion.py
0	349	vllm/spec_decode/draft_model_runner.py
0	99	vllm/spec_decode/interfaces.py
0	138	vllm/spec_decode/medusa_worker.py
0	213	vllm/spec_decode/metrics.py
0	94	vllm/spec_decode/mlp_speculator_worker.py
0	160	vllm/spec_decode/mqa_scorer.py
0	423	vllm/spec_decode/multi_step_worker.py
0	196	vllm/spec_decode/ngram_worker.py
0	59	vllm/spec_decode/proposer_worker_base.py
0	196	vllm/spec_decode/smaller_tp_proposer_worker.py
0	1326	vllm/spec_decode/spec_decode_worker.py
0	45	vllm/spec_decode/target_model_runner.py
0	275	vllm/spec_decode/top1_proposer.py
0	277	vllm/spec_decode/util.py
18	22	vllm/transformers_utils/configs/eagle.py
0	2	vllm/worker/worker_base.py

[9ffe905a4] Varun Sundar Rabindranath 2025-07-19 [Bugfix][Model] Fix LoRA for Mistral-Small-3.1-24B-Instruct-2503 (#21183)
17	2	vllm/lora/models.py
10	6	vllm/lora/utils.py

[9a9fda142] Lucia Fang 2025-07-19 [Core] Support Local Chunked Attention for Hybrid KV Cache (#19351)
155	2	tests/v1/core/test_specialized_manager.py
1	0	vllm/attention/layer.py
7	0	vllm/config.py
2	1	vllm/v1/attention/backends/flash_attn.py
1	0	vllm/v1/attention/backends/utils.py
16	3	vllm/v1/core/kv_cache_utils.py
124	1	vllm/v1/core/single_type_kv_cache_manager.py
37	12	vllm/v1/kv_cache_interface.py
8	0	vllm/v1/worker/gpu_model_runner.py

[466e878f2] Jee Jee Li 2025-07-19 [Quantization] Enable BNB support for more MoE models (#21100)
4	4	docs/models/supported_models.md
14	7	vllm/model_executor/models/bailing_moe.py
84	69	vllm/model_executor/models/ernie45_moe.py
14	10	vllm/model_executor/models/grok1.py
107	91	vllm/model_executor/models/hunyuan_v1_moe.py

[217937221] Rui Qiao 2025-07-18 Elastic Expert Parallel Initial Support (#20775)
57	0	examples/online_serving/elastic_ep/bench.sh
53	0	examples/online_serving/elastic_ep/scale.py
72	0	examples/online_serving/elastic_ep/serve_deepseek_v2.sh
92	0	tools/ep_kernels/elastic_ep/eep_nvshmem.patch
86	0	tools/ep_kernels/elastic_ep/install_eep_libraries.sh
13	0	vllm/config.py
219	33	vllm/distributed/eplb/eplb_state.py
117	0	vllm/distributed/eplb/rebalance_execute.py
6	0	vllm/engine/protocol.py
105	0	vllm/entrypoints/openai/api_server.py
9	0	vllm/executor/uniproc_executor.py
31	8	vllm/model_executor/layers/fused_moe/layer.py
20	3	vllm/model_executor/models/deepseek_v2.py
7	0	vllm/model_executor/models/interfaces.py
16	0	vllm/v1/engine/__init__.py
58	0	vllm/v1/engine/async_llm.py
31	1	vllm/v1/engine/coordinator.py
64	5	vllm/v1/engine/core.py
182	7	vllm/v1/engine/core_client.py
217	8	vllm/v1/engine/utils.py
9	0	vllm/v1/executor/ray_distributed_executor.py
1	1	vllm/v1/worker/cpu_model_runner.py
36	1	vllm/v1/worker/gpu_model_runner.py
158	1	vllm/v1/worker/gpu_worker.py

[5782581ac] hax0r31337 2025-07-19 [Bugfix] Voxtral on Blackwell GPUs (RTX 50 series) (#21077)
33	0	vllm/attention/layer.py

[0f199f197] JialinOuyang-Meta 2025-07-18 [Core] Avoid KVCacheBlock.__eq__ invocations in FreeKVCacheBlockQueue (#21005)
108	0	benchmarks/kv_cache/benchmark_block_pool.py
15	13	tests/v1/core/test_kv_cache_utils.py
13	13	tests/v1/core/test_prefix_caching.py
74	32	vllm/v1/core/kv_cache_utils.py

[b2eb2b5ad] Richard Zou 2025-07-18 [Kernel] Apply torch.Tag.needs_fixed_stride_order only for torch==2.6.0 (#19346)
8	4	csrc/torch_bindings.cpp
6	2	vllm/attention/ops/rocm_aiter_mla.py
5	3	vllm/model_executor/layers/fused_moe/fused_moe.py

[21274ab47] Richard Zou 2025-07-18 [CI] Update CODEOWNERS for vllm/compilation (#21185)
1	1	.github/CODEOWNERS

[ed8cbfedf] Thomas Parnell 2025-07-18 Let GraniteMoeAttention use YaRN (#21174)
5	1	vllm/model_executor/models/granitemoe.py
2	0	vllm/model_executor/models/granitemoeshared.py

[45badd05d] Cyrus Leung 2025-07-18 [Core] Set pooling params based on task and model (#21128)
11	15	tests/models/language/pooling/test_gritlm.py
33	16	vllm/entrypoints/llm.py
4	4	vllm/entrypoints/openai/protocol.py
32	0	vllm/entrypoints/openai/serving_classification.py
15	3	vllm/entrypoints/openai/serving_embedding.py
13	5	vllm/entrypoints/openai/serving_engine.py
5	0	vllm/entrypoints/openai/serving_pooling.py
21	9	vllm/entrypoints/openai/serving_score.py
7	0	vllm/executor/executor_base.py
95	54	vllm/model_executor/layers/pooler.py
8	4	vllm/model_executor/models/bert.py
116	69	vllm/model_executor/models/gritlm.py
0	7	vllm/model_executor/models/interfaces.py
8	4	vllm/model_executor/models/modernbert.py
25	16	vllm/pooling_params.py
6	0	vllm/v1/engine/core.py
0	4	vllm/v1/worker/cpu_model_runner.py
11	8	vllm/v1/worker/gpu_input_batch.py
39	9	vllm/v1/worker/gpu_model_runner.py
4	0	vllm/v1/worker/gpu_worker.py
13	1	vllm/v1/worker/tpu_model_runner.py
4	0	vllm/v1/worker/tpu_worker.py
13	1	vllm/worker/model_runner_base.py
15	1	vllm/worker/pooling_model_runner.py

[4adc66f64] ElizaWszola 2025-07-18 [Bugfix] Allocate less memory in non-batched CUTLASS MoE (#21121)
2	2	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[55ad64871] Cyrus Leung 2025-07-18 [Doc] Fix typo in model name (#21178)
1	1	docs/models/supported_models.md

[5895afd78] wang.yuqi 2025-07-18 [Bugfix] The special_tokens in tokenizer should also be controlled by do_lower_case in encoder_config. (#20750)
18	0	tests/tokenization/test_do_lower_case.py
14	0	vllm/transformers_utils/tokenizer.py

[ca4eb82bc] wang.yuqi 2025-07-18 [Model] Re-add the implicit conversion feature for as_seq_cls_model (#21103)
20	12	tests/models/registry.py
21	8	tests/models/test_initialization.py
35	0	tests/models/test_transformers.py
25	21	vllm/config.py
26	4	vllm/model_executor/model_loader/utils.py
9	6	vllm/model_executor/models/adapters.py
0	4	vllm/model_executor/models/gemma.py
0	4	vllm/model_executor/models/llama.py
0	4	vllm/model_executor/models/qwen2.py
0	4	vllm/model_executor/models/qwen3.py
29	8	vllm/model_executor/models/registry.py

[ba2dfbb0c] Roger Wang 2025-07-18 [Misc] Make MM embedding merge interface explicit in model runner (#21147)
4	5	vllm/v1/worker/gpu_model_runner.py
4	5	vllm/v1/worker/tpu_model_runner.py

[1bf65138f] Jialin Ouyang 2025-07-17 [benchmark] Sending request strictly follows the random intervals (#21108)
40	17	vllm/benchmarks/serve.py

[54cf1cae6] Woosuk Kwon 2025-07-17 [Misc] Do not print async output warning for v1 (#21151)
1	1	vllm/platforms/cuda.py
1	1	vllm/platforms/rocm.py

[5780121c9] shixianc 2025-07-17 [Perf] Add swap_ab to SM90 FP8 non-block CUTLASS moe grouped gemm (#20911)
37	12	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu
48	19	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
49	19	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
1	0	tests/kernels/moe/test_cutlass_moe.py

[c7d8724e7] Shu Wang 2025-07-17 [Core] FlashInfer CUTLASS fused MoE backend (NVFP4) (#20037)
9	13	vllm/_custom_ops.py
5	0	vllm/envs.py
13	23	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
4	3	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
16	0	vllm/model_executor/layers/fused_moe/config.py
238	46	vllm/model_executor/layers/fused_moe/cutlass_moe.py
2	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
8	11	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
8	11	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
198	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_moe.py
114	0	vllm/model_executor/layers/fused_moe/flashinfer_cutlass_prepare_finalize.py
15	21	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py
31	5	vllm/model_executor/layers/fused_moe/layer.py
60	39	vllm/model_executor/layers/fused_moe/modular_kernel.py
11	19	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
26	18	vllm/model_executor/layers/fused_moe/prepare_finalize.py
14	23	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
31	1	vllm/model_executor/layers/fused_moe/utils.py
5	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
179	32	vllm/model_executor/layers/quantization/modelopt.py
107	0	vllm/utils/flashinfer.py

[b38baabcf] 22quinn 2025-07-17 [Doc] Add inplace weights loading example (#19640)
53	0	examples/offline_inference/skip_loading_weights_in_engine_init.py

[89cab4d01] Lucas Wilkinson 2025-07-18 [Attention] Make local attention backend agnostic (#21093)
10	74	vllm/v1/attention/backends/flash_attn.py
1	4	vllm/v1/attention/backends/flashinfer.py
7	90	vllm/v1/attention/backends/rocm_aiter_fa.py
7	61	vllm/v1/attention/backends/triton_attn.py
24	6	vllm/v1/attention/backends/utils.py
7	3	vllm/v1/core/single_type_kv_cache_manager.py
15	0	vllm/v1/kv_cache_interface.py
23	4	vllm/v1/worker/gpu_model_runner.py

[b9a21e917] Lucia Fang 2025-07-18 [Docs] Update supported models documentation with missing models (#20844)
2	0	docs/models/supported_models.md

[c4e3b1252] Ricardo Decal 2025-07-17 [Docs] Add minimal demo of Ray Data API usage (#21080)
26	3	docs/serving/offline_inference.md

[8dfb45ca3] elvischenv 2025-07-18 [Bugfix] Fix the tensor non-contiguous issue for Flashinfer TRT-LLM backend attention kernel (#21133)
23	11	vllm/v1/attention/backends/flashinfer.py

[8a8fc9463] Wentao Ye 2025-07-17 [Log] Debugging Log with more Information (#20770)
17	9	vllm/model_executor/layers/fused_moe/cutlass_moe.py
20	4	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py

[4de714635] Woosuk Kwon 2025-07-17 [V0 deprecation] Remove V0 HPU backend (#21131)
0	21	docker/Dockerfile.hpu
0	12	requirements/hpu.txt
2	34	setup.py
1	2	vllm/_custom_ops.py
0	319	vllm/attention/backends/hpu_attn.py
0	88	vllm/attention/ops/hpu_paged_attn.py
1	1	vllm/config.py
1	3	vllm/core/block/cpu_gpu_block_allocator.py
0	46	vllm/distributed/device_communicators/hpu_communicator.py
2	3	vllm/engine/arg_utils.py
0	15	vllm/envs.py
0	4	vllm/lora/layers.py
0	145	vllm/lora/punica_wrapper/punica_hpu.py
0	7	vllm/model_executor/custom_op.py
0	36	vllm/model_executor/layers/fused_moe/layer.py
0	20	vllm/model_executor/layers/layernorm.py
0	58	vllm/model_executor/layers/rotary_embedding.py
2	14	vllm/model_executor/layers/vocab_parallel_embedding.py
1	10	vllm/model_executor/model_loader/bitsandbytes_loader.py
0	10	vllm/model_executor/model_loader/default_loader.py
0	18	vllm/platforms/__init__.py
0	114	vllm/platforms/hpu.py
0	5	vllm/platforms/interface.py
0	13	vllm/plugins/__init__.py
0	2320	vllm/worker/hpu_model_runner.py
0	485	vllm/worker/hpu_worker.py
0	123	vllm/worker/multi_step_hpu_worker.py

[ac9fb732a] Eric Curtin 2025-07-17 On environments where numa cannot be detected we get 0 (#21115)
111	77	vllm/v1/worker/cpu_worker.py

[a3a6c695f] Jee Jee Li 2025-07-18 [Misc] Qwen MoE model supports LoRA (#20932)
2	2	docs/models/supported_models.md
13	0	vllm/lora/models.py
3	4	vllm/model_executor/models/qwen2_moe.py
2	2	vllm/model_executor/models/qwen3_moe.py

[90bd2ab6e] Cyrus Leung 2025-07-18 [Model] Update pooling model interface (#21058)
5	10	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
5	29	vllm/entrypoints/openai/protocol.py
112	64	vllm/model_executor/layers/pooler.py
8	23	vllm/model_executor/models/adapters.py
18	19	vllm/model_executor/models/bert.py
4	10	vllm/model_executor/models/gpt2.py
3	9	vllm/model_executor/models/gritlm.py
12	74	vllm/model_executor/models/interfaces.py
16	17	vllm/model_executor/models/interfaces_base.py
4	10	vllm/model_executor/models/internlm2.py
4	10	vllm/model_executor/models/jamba.py
4	11	vllm/model_executor/models/jina_vl.py
12	12	vllm/model_executor/models/modernbert.py
8	12	vllm/model_executor/models/prithvi_geospatial_mae.py
9	14	vllm/model_executor/models/qwen2_rm.py
3	10	vllm/model_executor/models/roberta.py
20	11	vllm/pooling_params.py

[9fb2d2203] ElizaWszola 2025-07-17 [Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762)
34	1	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
42	11	csrc/moe/moe_permute_unpermute_op.cu
12	2	tests/kernels/moe/test_cutlass_moe.py
22	0	tests/kernels/moe/test_pplx_cutlass_moe.py
39	23	vllm/model_executor/layers/fused_moe/cutlass_moe.py
25	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[2d6a38209] Harry Mellor 2025-07-17 [Docs] Move code block out of admonition now that it's short (#21118)
3	5	docs/design/v1/p2p_nccl_connector.md

[89e3c4e9b] wangxiyuan 2025-07-17 [Misc] Avoid unnecessary import (#21106)
1	1	vllm/entrypoints/openai/speech_to_text.py
12	8	vllm/lora/utils.py

[fe8a2c544] Harry Mellor 2025-07-17 [Docs] Improve docstring formatting for `FusedMoEParallelConfig.make` (#21117)
34	28	vllm/model_executor/layers/fused_moe/config.py

[4ef00b5ca] kYLe 2025-07-17 [VLM] Add Nemotron-Nano-VL-8B-V1 support (#20349)
1	1	docker/Dockerfile.cpu
1	0	docs/models/supported_models.md
39	0	examples/offline_inference/vision_language.py
1	0	requirements/test.in
15	1	requirements/test.txt
1	0	tests/models/multimodal/processing/test_common.py
134	0	tests/models/multimodal/processing/test_nemotron_vl.py
2	0	tests/models/registry.py
505	0	vllm/model_executor/models/nemotron_vl.py
1	0	vllm/model_executor/models/registry.py
1	1	vllm/transformers_utils/configs/nemotron.py

[5a7fb3ab9] Asher 2025-07-17 [Model] Add ToolParser and MoE Config for Hunyuan A13B  (#20820)
5	0	benchmarks/kernels/benchmark_moe.py
1	0	docs/features/reasoning_outputs.md
10	0	docs/features/tool_calling.md
113	0	examples/tool_chat_template_hunyuan_a13b.jinja
153	0	tests/entrypoints/openai/tool_parsers/test_hunyuan_a13b_tool_parser.py
11	0	tests/reasoning/test_hunyuan_reasoning_parser.py
16	3	vllm/entrypoints/openai/serving_chat.py
2	1	vllm/entrypoints/openai/tool_parsers/__init__.py
372	0	vllm/entrypoints/openai/tool_parsers/hunyuan_a13b_tool_parser.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1536,device_name=NVIDIA_H20,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=3072,device_name=NVIDIA_H20,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=3072,device_name=NVIDIA_H20.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=384,device_name=NVIDIA_H20,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=384,device_name=NVIDIA_H20.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=768,device_name=NVIDIA_H20,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=768,device_name=NVIDIA_H20.json
7	0	vllm/reasoning/hunyuan_a13b_reasoning_parser.py

[11dfdf21b] Varun Sundar Rabindranath 2025-07-17 [Kernel] DeepGemm MoE : Integrate triton permute / unpermute kernels  (#20903)
0	1	tests/kernels/moe/modular_kernel_tools/cli_args.py
1	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
5	2	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/cutlass_moe.py
53	48	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
413	0	vllm/model_executor/layers/fused_moe/deep_gemm_utils.py
2	0	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py
10	6	vllm/model_executor/layers/fused_moe/modular_kernel.py
5	2	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[fdc5b43d2] Chauncey 2025-07-17 [Bugfix]: Fix final_res_batch list index out of range error (#21055)
17	1	tests/v1/entrypoints/openai/test_completion.py
61	39	vllm/entrypoints/openai/serving_completion.py

[c5b8b5953] Jee Jee Li 2025-07-17 [Misc] Fix PhiMoE expert mapping (#21085)
1	6	vllm/model_executor/models/phimoe.py

[4fcef49ec] David Ben-David 2025-07-17 [V1] [KVConnector] Fix MultiprocExecutor worker output aggregation (#21048)
127	0	tests/v1/executor/test_multiproc_executor.py
2	4	vllm/v1/executor/multiproc_executor.py

[8a4e5c5f3] Zhonghua Deng 2025-07-17 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)
30	62	docs/design/v1/p2p_nccl_connector.md
35	4	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
7	31	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
194	159	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py

[76b494444] Lucas Wilkinson 2025-07-17 [Attention] Refactor attention metadata builder interface (#20466)
466	0	tests/v1/attention/test_attention_backends.py
229	0	tests/v1/attention/utils.py
44	24	tests/v1/spec_decode/test_eagle.py
34	31	vllm/v1/attention/backends/cpu_attn.py
48	53	vllm/v1/attention/backends/flash_attn.py
51	106	vllm/v1/attention/backends/flashinfer.py
26	33	vllm/v1/attention/backends/flex_attention.py
38	92	vllm/v1/attention/backends/mamba_attn.py
64	119	vllm/v1/attention/backends/mla/common.py
8	7	vllm/v1/attention/backends/mla/flashmla.py
19	16	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
39	50	vllm/v1/attention/backends/rocm_aiter_fa.py
34	39	vllm/v1/attention/backends/triton_attn.py
138	2	vllm/v1/attention/backends/utils.py
108	90	vllm/v1/spec_decode/eagle.py
0	27	vllm/v1/spec_decode/utils.py
37	4	vllm/v1/worker/block_table.py
64	85	vllm/v1/worker/gpu_model_runner.py

[28a6d5423] Michael Goin 2025-07-16 [Bugfix] Fix Machete zero point issue for GPTQ models on SM90 (#21066)
5	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py

[58760e12b] XiongfeiWei 2025-07-16 [TPU] Start using python 3.12 (#21000)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
2	2	docker/Dockerfile.tpu
2	2	docs/getting_started/installation/google_tpu.md
4	5	requirements/tpu.txt

[a50d91822] Michael Goin 2025-07-16 [Docker] Allow FlashInfer to be built in the ARM CUDA Dockerfile (#21013)
27	41	docker/Dockerfile

[c9ba8104e] Kevin_Xiong 2025-07-17 [Bugfix] weight loading use correct tp_group with patch_tensor_parallel_group (#21024)
25	28	vllm/model_executor/layers/linear.py

[4e7dfbe7b] Michael Goin 2025-07-16 Update PyTorch to `torch==2.7.1` for CUDA (#21011)
1	1	CMakeLists.txt
1	1	pyproject.toml
1	1	requirements/build.txt
5	5	requirements/cuda.txt
3	3	requirements/test.in
4	4	requirements/test.txt
2	2	tests/entrypoints/openai/test_vision.py

[72ad27358] QiliangCui 2025-07-16 Remove torch_xla.tpu.version() from pallas.py. (#21065)
0	4	vllm/v1/attention/backends/pallas.py

[01513a334] Nir David 2025-07-16 Support FP8 Quantization and Inference Run on Intel Gaudi (HPU) using INC (Intel Neural Compressor) (#12010)
1	0	docs/features/quantization/README.md
56	0	docs/features/quantization/inc.md
13	12	docs/features/quantization/supported_hardware.md
3	2	docs/getting_started/installation/intel_gaudi.md
8	5	vllm/config.py
9	1	vllm/engine/arg_utils.py
5	2	vllm/model_executor/layers/quantization/__init__.py
61	0	vllm/model_executor/layers/quantization/inc.py
9	1	vllm/model_executor/model_loader/base_loader.py
2	2	vllm/model_executor/model_loader/weight_utils.py
1	0	vllm/utils/__init__.py

[ac2bf41e5] Cyrus Leung 2025-07-17 [Model] Remove model sampler (#21059)
0	10	vllm/model_executor/models/bailing_moe.py
0	2	vllm/model_executor/models/granite_speech.py
0	10	vllm/model_executor/models/hunyuan_v1_moe.py
0	2	vllm/model_executor/models/mimo.py
0	11	vllm/model_executor/models/mimo_mtp.py
0	10	vllm/model_executor/models/phi4flash.py

[a931b4cdc] Harry Mellor 2025-07-16 Remove Qwen Omni workaround that's no longer necessary (#21057)
0	7	vllm/transformers_utils/config.py

[a0f8a7964] Avshalom Manevich 2025-07-16 [fix] fix qwen image_embeds input (#21049)
2	2	vllm/model_executor/models/qwen2_5_vl.py

[18bdcf411] Mac Misiura 2025-07-16 feat - add a new endpoint `get_tokenizer_info` to provide tokenizer/chat-template information (#20575)
104	0	tests/entrypoints/openai/test_tokenization.py
14	0	vllm/entrypoints/openai/api_server.py
3	0	vllm/entrypoints/openai/cli_args.py
10	0	vllm/entrypoints/openai/protocol.py
51	3	vllm/entrypoints/openai/serving_tokenization.py

[1c3198b6c] Cyrus Leung 2025-07-16 [Model] Consolidate pooler implementations (#20927)
434	247	vllm/model_executor/layers/pooler.py
48	51	vllm/model_executor/models/adapters.py
16	9	vllm/model_executor/models/bert.py
2	2	vllm/model_executor/models/gritlm.py
1	1	vllm/model_executor/models/interfaces.py
26	13	vllm/model_executor/models/jamba.py
18	15	vllm/model_executor/models/modernbert.py
8	5	vllm/model_executor/models/roberta.py
0	24	vllm/transformers_utils/config.py

[260127ea5] Michael Yao 2025-07-16 [Docs] Add intro and fix 1-2-3 list in frameworks/open-webui.md (#19199)
-	-	docs/assets/deployment/open_webui.png
33	17	docs/deployment/frameworks/open-webui.md

[d0dc4cfca] Seiji Eicher 2025-07-16 Fix inadvertently silenced PP tests for `mp`, add DeepSeek V2/V3 model family to PP tests (#20831)
17	7	tests/distributed/test_pipeline_parallel.py

[d31a64712] Lucas Wilkinson 2025-07-16 [BugFix] Fix import error on non-blackwell machines (#21020)
10	0	csrc/attention/mla/sm100_cutlass_mla_kernel.cu
0	13	csrc/ops.h
2	3	csrc/torch_bindings.cpp

[85431bd9a] Chengji Yao 2025-07-15 [TPU] fix kv_cache_update kernel block size choosing logic (#21007)
48	1	vllm/v1/attention/backends/pallas.py
3	2	vllm/v1/worker/tpu_model_runner.py

[c11013db8] zhiweiz 2025-07-15 [Meta] Llama4 EAGLE Support (#20591)
1	0	examples/offline_inference/spec_decode.py
5	0	tests/models/registry.py
5	0	tests/models/test_initialization.py
31	17	tests/v1/e2e/test_spec_decode.py
214	0	vllm/model_executor/models/llama4_eagle.py
1	0	vllm/model_executor/models/registry.py

[1eb2b9c10] Peter Pan 2025-07-16 [CI] update typos config for CI pre-commit and fix some spells (#20919)
1	1	.pre-commit-config.yaml
1	1	csrc/cpu/sgl-kernels/common.h
1	1	csrc/cpu/sgl-kernels/gemm.h
1	1	csrc/cpu/sgl-kernels/gemm_int8.cpp
1	1	csrc/cpu/sgl-kernels/vec.h
1	1	docker/Dockerfile
1	1	docs/usage/v1_guide.md
183	0	pyproject.toml
1	1	tests/kernels/moe/modular_kernel_tools/common.py
1	1	tests/kernels/moe/test_deepgemm.py
1	1	tests/models/test_initialization.py
1	1	tests/v1/test_external_lb_dp.py
0	179	typos.toml
1	1	vllm/attention/backends/differential_flash_attn.py
1	1	vllm/entrypoints/openai/serving_responses.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
1	1	vllm/model_executor/models/phi4flash.py
1	1	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/worker/tpu_model_runner.py

[6ebf31379] Maximilien de Bayser 2025-07-16 Avoid direct comparison of floating point numbers (#21002)
5	1	tests/entrypoints/openai/test_classification.py
15	2	tests/entrypoints/openai/test_embedding.py
14	2	tests/entrypoints/openai/test_pooling.py
5	1	tests/entrypoints/openai/test_rerank.py
5	1	tests/entrypoints/openai/test_score.py

[cfbcb9ed8] Patrick von Platen 2025-07-16 [Voxtral] Add more tests (#21010)
9	4	tests/conftest.py
0	3	tests/entrypoints/openai/test_transcription_validation.py
115	0	tests/models/multimodal/generation/test_voxtral.py
1	1	tests/models/registry.py

[76ddeff29] Wentao Ye 2025-07-15 [Doc] Remove duplicate docstring (#21012)
0	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[f46098335] Michael Goin 2025-07-15 [Bugfix] Fix Mistral3 support on SM100/SM120 (#20998)
7	2	vllm/model_executor/models/pixtral.py

[e9534c720] Chendi.Xue 2025-07-15 [CI][HPU] update for v0 deprecate by switching to VLLM_TARGET_DEVICE=empty (#21006)
3	5	.buildkite/scripts/hardware_ci/run-hpu-test.sh

[797644601] Doug Smith 2025-07-15 Add Dockerfile argument for VLLM_USE_PRECOMPILED environment (#20943)
13	0	docker/Dockerfile

[fcb9f879c] Ming Yang 2025-07-15 [Bugfix] Correct per_act_token in CompressedTensorsW8A8Fp8MoECutlassM… (#20937)
4	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[3ed94f9d0] Ricardo Decal 2025-07-15 [Docs] Enhance Anyscale documentation, add quickstart links for vLLM (#21018)
11	2	docs/deployment/frameworks/anyscale.md

[fa839565f] Reid 2025-07-16 [Misc] Refactor: Improve argument handling for `conda` command (#20481)
25	20	vllm/collect_env.py

[75a99b98b] Brayden Zhong 2025-07-15 [Chore] Remove outdated transformers check (#20989)
4	11	vllm/model_executor/models/idefics3.py

[b5c3b6835] Chauncey 2025-07-16 [Misc] bump xgrammar version to v0.1.21 (#20992)
1	1	requirements/common.txt

[6cbc4d4be] Thomas Parnell 2025-07-16 [Model] Add ModelConfig class for GraniteMoeHybrid to override default max_seq_len_to_capture (#20923)
14	0	vllm/model_executor/models/config.py

[153c6f1e6] Michael Goin 2025-07-15 [Frontend] Remove print left in FrontendArgs.add_cli_args (#21004)
0	1	vllm/entrypoints/openai/cli_args.py

[34cda778a] Chauncey 2025-07-16 [Frontend] OpenAI Responses API supports input image (#20975)
166	0	tests/v1/entrypoints/openai/responses/test_image.py
6	3	vllm/entrypoints/chat_utils.py

[30800b01c] Elfie Guo 2025-07-15 [Nvidia] Integrate SM100 cudnn prefill API to MLA prefill (#20411)
5	0	vllm/envs.py
108	5	vllm/v1/attention/backends/mla/common.py

[10be20949] Chen LI 2025-07-15 [Bug Fix] get_distributed_init_method should get the ip from get_ip i… (#20889)
5	0	vllm/envs.py
27	0	vllm/utils/__init__.py
4	4	vllm/v1/executor/multiproc_executor.py

[19c863068] Marko Rosenmueller 2025-07-15 [Frontend] Support cache_salt in /v1/completions and /v1/responses (#20981)
1	0	vllm/entrypoints/openai/api_server.py
49	3	vllm/entrypoints/openai/protocol.py
17	0	vllm/entrypoints/openai/serving_completion.py
10	1	vllm/entrypoints/openai/serving_engine.py

[f29fd8a7f] Tuan, Hoang-Trong 2025-07-15 [BugFix] fix 3 issues: (1) using metadata for causal-conv1d, (2) indexing overflow in v1 vLLM, and (3) init_states in v0 (#20838)
11	5	vllm/model_executor/layers/mamba/mamba_mixer2.py
3	4	vllm/model_executor/layers/mamba/ops/causal_conv1d.py

[ed10f3cea] Gregory Shtrasberg 2025-07-15 [ROCm] warpSize is being made non constexpr in ROCm 7.0 (#20330)
1	7	csrc/attention/attention_kernels.cuh
1	7	csrc/attention/paged_attention_v1.cu
1	7	csrc/attention/paged_attention_v2.cu
3	3	csrc/cuda_compat.h

[b637e9dcb] Harry Mellor 2025-07-15 Add full serve CLI reference back to docs (#20978)
8	0	docs/cli/README.md
1	1	docs/configuration/serve_args.md
20	3	docs/mkdocs/hooks/generate_argparse.py
1	0	requirements/docs.txt
0	31	vllm/entrypoints/cli/serve.py
28	0	vllm/entrypoints/openai/cli_args.py

[1e36c8687] Harry Mellor 2025-07-15 [Deprecation] Remove `nullable_kvs` (#20969)
4	52	tests/engine/test_arg_utils.py
2	1	tests/entrypoints/openai/test_openai_schema.py
1	40	vllm/engine/arg_utils.py

[5bac61362] Harry Mellor 2025-07-15 Configure Gemini (#20971)
6	0	.gemini/config.yaml

[313ae8c16] Harry Mellor 2025-07-15 [Deprecation] Remove everything scheduled for removal in v0.10.0 (#20979)
1	3	docs/features/tool_calling.md
1	34	vllm/config.py
0	27	vllm/engine/arg_utils.py
0	4	vllm/entrypoints/openai/api_server.py
0	12	vllm/entrypoints/openai/cli_args.py
0	17	vllm/entrypoints/openai/serving_chat.py
0	1	vllm/entrypoints/openai/serving_responses.py
0	22	vllm/sampling_params.py

[c847e34b3] Cyrus Leung 2025-07-15 [CI/Build] Fix wrong path in Transformers Nightly Models Test (#20994)
1	1	.buildkite/test-pipeline.yaml

[e7e3e6d26] Patrick von Platen 2025-07-15 Voxtral (#20970)
75	10	examples/offline_inference/audio_language.py
1	1	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
7	1	requirements/test.txt
2	1	setup.py
23	5	tests/entrypoints/openai/test_transcription_validation.py
2	1	tests/models/registry.py
1	0	vllm/entrypoints/openai/speech_to_text.py
2	1	vllm/model_executor/models/interfaces.py
1	0	vllm/model_executor/models/registry.py
691	0	vllm/model_executor/models/voxtral.py
59	22	vllm/model_executor/models/whisper.py
47	3	vllm/transformers_utils/configs/mistral.py

[4ffd963fa] Christian Pinto 2025-07-15 [v1][core] Support for attention free models (#20811)
6	1	vllm/v1/core/kv_cache_manager.py
20	1	vllm/v1/core/kv_cache_utils.py
7	1	vllm/v1/engine/core.py

[56fe4bedd] Harry Mellor 2025-07-15 [Deprecation] Remove `TokenizerPoolConfig` (#20968)
0	1	docs/api/README.md
2	6	tests/async_engine/test_api_server.py
0	33	vllm/config.py
2	22	vllm/engine/arg_utils.py

[d91278181] Rui Qiao 2025-07-15 [doc] Add more details for Ray-based DP (#20948)
10	2	docs/serving/data_parallel_deployment.md

[20149d84d] Li Wang 2025-07-15 [MISC] Add init files for python package (#20908)
0	0	vllm/attention/utils/__init__.py
0	0	vllm/ray/__init__.py

[3534c39a2] Thomas Parnell 2025-07-15 [V1] [Hybrid] Refactor mamba state shape calculation; enable V1 via cli  (#20840)
1	2	docs/usage/v1_guide.md
1	15	tests/models/language/generation/test_hybrid.py
8	1	vllm/config.py
10	38	vllm/model_executor/layers/mamba/mamba_mixer2.py
55	0	vllm/model_executor/layers/mamba/mamba_utils.py
42	39	vllm/model_executor/models/bamba.py
90	0	vllm/model_executor/models/config.py
43	37	vllm/model_executor/models/falcon_h1.py
42	38	vllm/model_executor/models/granitemoehybrid.py
20	0	vllm/model_executor/models/interfaces.py
41	39	vllm/model_executor/models/mamba2.py
42	40	vllm/model_executor/models/nemotron_h.py
43	49	vllm/model_executor/models/zamba2.py
3	55	vllm/v1/worker/gpu_model_runner.py

[c586b5566] Yifei Teng 2025-07-15 [TPU] Optimize kv cache update kernel (#20415)
7	0	vllm/utils/__init__.py
6	0	vllm/v1/attention/backends/pallas.py
50	16	vllm/v1/worker/tpu_model_runner.py

[33d560001] Ricardo Decal 2025-07-15 [Docs] Improve documentation for ray cluster launcher helper script (#20602)
62	12	examples/online_serving/run_cluster.sh

[f148c44c6] kourosh hakhamaneshi 2025-07-15 [frontend] Refactor CLI Args for a better modular integration (#20206)
1	1	.pre-commit-config.yaml
166	211	vllm/entrypoints/openai/cli_args.py

[235bfd5df] Ricardo Decal 2025-07-15 [Docs] Improve documentation for RLHF example (#20598)
49	36	examples/offline_inference/rlhf.py

[68d28e37b] Reid 2025-07-15 [frontend] Add --help=page option for paginated help output (#20961)
3	0	docs/cli/README.md
36	8	vllm/entrypoints/utils.py

[37a7d5d74] Ilya Markov 2025-07-15 [Misc] Refactor AllReduceFusionPass. Remove parameter (#20918)
1	3	tests/compile/test_fusion_all_reduce.py
5	3	vllm/compilation/collective_fusion.py
1	4	vllm/compilation/pass_manager.py

[d4d309409] Woosuk Kwon 2025-07-14 Implement Async Scheduling (#19970)
0	0	tests/v1/core/__init__.py
228	0	tests/v1/core/test_async_scheduler.py
1	127	tests/v1/core/test_scheduler.py
152	0	tests/v1/core/utils.py
11	0	vllm/config.py
25	0	vllm/engine/arg_utils.py
47	0	vllm/v1/core/sched/async_scheduler.py
39	21	vllm/v1/core/sched/scheduler.py
2	0	vllm/v1/executor/multiproc_executor.py
2	0	vllm/v1/executor/ray_distributed_executor.py
1	0	vllm/v1/request.py

[85bd6599e] Jennifer He 2025-07-15 [Model] Add AutoWeightsLoader support for BERT, RoBERTa (#20534)
37	48	vllm/model_executor/models/bert.py
22	52	vllm/model_executor/models/roberta.py

[91b3d190a] Boyuan Feng 2025-07-14 [cold start] replace VLLM_COMPILE_DEPYF with debug_dump_dir (#20940)
7	15	vllm/compilation/wrapper.py
0	6	vllm/envs.py

[fc017915f] Isotr0py 2025-07-15 [Doc] Clearer mistral3 and pixtral model support description (#20926)
3	3	docs/models/supported_models.md

[9ad0a4588] Pavani Majety 2025-07-14 [Bugfix] Switch bailout logic for kv-cache-dtype with SM100 Flashinfer (#20934)
4	3	vllm/engine/arg_utils.py

[016b8d1b7] Ruheena Suhani Shaik 2025-07-15 Enabled BnB NF4 inference on Gaudi (#20172)
6	6	vllm/model_executor/layers/quantization/bitsandbytes.py
12	2	vllm/model_executor/model_loader/bitsandbytes_loader.py

[80305c1b2] Nicolò Lucchesi 2025-07-15 [CI] Fix flaky `test_streaming_response` test (#20913)
4	2	tests/entrypoints/openai/test_transcription_validation.py

[37e2ecace] Reid 2025-07-15 feat: add image zoom to improve image viewing experience (#20763)
1	0	mkdocs.yaml
1	0	requirements/docs.txt

[054c8657e] Ricardo Decal 2025-07-14 [Docs] Add Kuberay to deployment integrations (#20592)
20	0	docs/deployment/integrations/kuberay.md
1	0	docs/deployment/k8s.md

[d4170fad3] XiongfeiWei 2025-07-14 Use w8a8 quantized matmul Pallas kernel (#19170)
5	5	requirements/tpu.txt
4	4	tests/tpu/test_quantization_accuracy.py
32	0	tests/v1/tpu/test_basic.py
9	10	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py

[946aadb4a] Michael Goin 2025-07-15 [CI/Build] Split Entrypoints Test into LLM and API Server (#20945)
14	4	.buildkite/test-pipeline.yaml

[bcdfb2a33] Michael Goin 2025-07-15 [Bugfix] Fix incorrect dispatch for CutlassBlockScaledGroupedGemm and DeepGEMM (#20933)
10	5	vllm/model_executor/layers/quantization/fp8.py

[ba8c30001] Richard Zou 2025-07-14 [BugFix] VLLM_DISABLE_COMPILE_CACHE=1 should disable all reads and writes from the cache (#20942)
24	0	tests/compile/test_config.py
2	1	vllm/compilation/backends.py
3	1	vllm/compilation/compiler_interface.py
4	0	vllm/compilation/counter.py

[8cdc37121] Alexander Matveev 2025-07-14 SM100 Cutlass MLA decode with unrestricted num_heads (< 128) for DeepSeek TP (#20769)
2	1	CMakeLists.txt
372	0	csrc/attention/mla/cutlass_sm100_mla/device/sm100_mla.hpp
203	0	csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_fmha_mla_reduction.hpp
2023	0	csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_fmha_mla_tma_warpspecialized.hpp
165	0	csrc/attention/mla/cutlass_sm100_mla/kernel/sm100_mla_tile_scheduler.hpp
273	0	csrc/attention/mla/sm100_cutlass_mla_kernel.cu
13	0	csrc/ops.h
17	0	csrc/torch_bindings.cpp
20	0	vllm/_custom_ops.py
7	0	vllm/platforms/cuda.py
5	0	vllm/v1/attention/backends/mla/common.py
183	1	vllm/v1/attention/backends/mla/cutlass_mla.py

[61e20828d] Yong Hoon Shin 2025-07-14 Fall back if flashinfer comm module not found (#20936)
8	5	vllm/compilation/collective_fusion.py

[55e1c66da] Kuntai Du 2025-07-14 [Docs] remove outdated performance benchmark (#20935)
0	2	README.md

[86f3ac21c] Thomas Parnell 2025-07-14 Fix overflow indexing in causal_conv1d kernel (#20938)
2	1	vllm/model_executor/layers/mamba/ops/causal_conv1d.py

[149f2435a] Nicolò Lucchesi 2025-07-14 [Misc] Relax translations tests (#20856)
3	4	tests/entrypoints/openai/test_translation_validation.py

[c0569dbc8] Varun Sundar Rabindranath 2025-07-15 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)
2	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
16	24	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
11	20	vllm/model_executor/layers/fused_moe/cutlass_moe.py
21	10	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
8	6	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
38	33	vllm/model_executor/layers/fused_moe/fused_moe.py
91	59	vllm/model_executor/layers/fused_moe/modular_kernel.py
12	5	vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
4	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[8bb43b9c9] Michael Goin 2025-07-15 Add benchmark dataset for mlperf llama tasks (#20338)
82	0	vllm/benchmarks/datasets.py

[559756214] Tyler Michael Smith 2025-07-14 Change default model to Qwen3-0.6B (#20335)
1	1	vllm/config.py

[6d0cf239c] Isotr0py 2025-07-15 [CI/Build] Add Transformers nightly tests in CI (#20924)
12	0	.buildkite/test-pipeline.yaml

[3fc964433] Isotr0py 2025-07-14 [Misc] Clean up Aimv2 config registration in Ovis config (#20921)
5	5	vllm/transformers_utils/configs/ovis.py

[0caf61c08] Lu Fang 2025-07-14 [CI] Update codeowner for compilation code (#20929)
1	1	.github/CODEOWNERS

[667624659] Richard Zou 2025-07-14 [CI] cc folks on changes to vllm/compilation (#20925)
1	0	.github/CODEOWNERS

[38efa2827] ant-yy 2025-07-14 [Model] Add Ling implementation (#20680)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
530	0	vllm/model_executor/models/bailing_moe.py
1	0	vllm/model_executor/models/registry.py

[e8cc53af5] Cyrus Leung 2025-07-14 [Misc] Log the reason for falling back to FlexAttention (#20699)
40	9	vllm/attention/selector.py
35	22	vllm/platforms/cuda.py
1	1	vllm/reasoning/hunyuan_a13b_reasoning_parser.py
4	0	vllm/v1/attention/backends/cpu_attn.py
4	0	vllm/v1/attention/backends/flash_attn.py
4	0	vllm/v1/attention/backends/flashinfer.py
4	0	vllm/v1/attention/backends/flex_attention.py
4	0	vllm/v1/attention/backends/mla/common.py
4	0	vllm/v1/attention/backends/rocm_aiter_fa.py
4	0	vllm/v1/attention/backends/triton_attn.py

[a4851cfe6] Chauncey 2025-07-14 [Bugfix]: Fix messy code when using logprobs (#20910)
2	5	vllm/transformers_utils/detokenizer_utils.py

[9887e8ec5] Reid 2025-07-14 [Misc] Remove unused function (#20909)
0	11	vllm/entrypoints/cli/main.py

[f326ab9c8] 22quinn 2025-07-14 [Bugfix] Bump up mistral_common to support v13 tokenizer (#20905)
1	1	requirements/test.in
1	1	requirements/test.txt

[dcf2a5e20] Cyrus Leung 2025-07-14 [CI/Build] Fix OOM issue in Jina-VL test (#20907)
85	58	tests/models/multimodal/pooling/test_jinavl_reranker.py

[1e9438e0b] wangxiyuan 2025-07-14 [MISC] Move bind_kv_cache to worker module (#20900)
1	1	tests/v1/test_utils.py
0	48	vllm/v1/utils.py
2	2	vllm/v1/worker/gpu_model_runner.py
1	2	vllm/v1/worker/tpu_model_runner.py
2	1	vllm/v1/worker/tpu_worker.py
51	1	vllm/v1/worker/utils.py

[697ef765e] Aaron Pham 2025-07-14 [Refactor][V1] Move outlines utils for V1 imports (#20878)
5	4	vllm/v1/structured_output/backend_outlines.py
199	1	vllm/v1/structured_output/utils.py

[a99b9f7de] Jee Jee Li 2025-07-14 [Quantization] add BNB for MixtralForCausalLM (#20893)
6	1	vllm/model_executor/model_loader/utils.py
102	3	vllm/model_executor/models/granitemoe.py
2	3	vllm/model_executor/models/granitemoeshared.py
13	8	vllm/model_executor/models/mixtral.py
2	1	vllm/model_executor/models/olmoe.py
2	1	vllm/model_executor/models/qwen2_moe.py
1	3	vllm/model_executor/models/qwen3_moe.py

[c488b928a] TJian 2025-07-14 [ROCm] [Bugfix] [Critical]: Fix mamba compilation bug (#20883)
10	1	csrc/mamba/mamba_ssm/selective_scan_fwd.cu

[2c7fa4716] Reid 2025-07-14 Fix: Add missing EOFError handling in CLI complete command (#20896)
6	3	vllm/entrypoints/cli/openai.py

[88fc8a97e] Daniel song 2025-07-14 Removing redundant python version check (#20888)
0	5	vllm/entrypoints/openai/serving_engine.py

[66f6fbd39] Maroon Ayoub 2025-07-14 [Prefix Cache] Add reproducible prefix-cache block hashing using SHA-256 + CBOR (64bit) (#20511)
1	0	requirements/common.txt
1	0	requirements/docs.txt
20	10	tests/v1/core/test_kv_cache_utils.py
9	5	tests/v1/core/test_prefix_caching.py
7	2	vllm/config.py
24	0	vllm/utils/__init__.py
6	3	vllm/v1/core/kv_cache_manager.py
20	8	vllm/v1/core/kv_cache_utils.py

[8632e831b] 22quinn 2025-07-13 [Core] Add `update_config` RPC method (#20095)
29	1	tests/test_config.py
14	2	tests/v1/worker/test_gpu_model_runner.py
20	1	vllm/config.py
11	1	vllm/v1/worker/gpu_model_runner.py
4	1	vllm/v1/worker/gpu_worker.py
15	2	vllm/v1/worker/tpu_model_runner.py
4	1	vllm/v1/worker/tpu_worker.py

[4bbfc36b1] nopperl 2025-07-14 [V1] Hybrid allocator without prefix caching (#20661)
33	0	vllm/v1/core/kv_cache_coordinator.py

[80d38b8ac] TJian 2025-07-13 [V1] [ROCm] [AITER] Upgrade AITER to commit `916bf3c` and bugfix APIs (#20880)
1	1	docker/Dockerfile.rocm_base
46	3	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[211b6a611] Liuchenlong 2025-07-13 [Bugfix] fix define of RerankDocument (#20877)
3	2	vllm/entrypoints/openai/protocol.py

[247102f07] Wang Siyuan 2025-07-13 [Bugfix] Fix: add patch_rope_scaling after hf override (#20857)
7	11	vllm/config.py
10	0	vllm/transformers_utils/config.py

[bd4c1e6fd] Minkyu Kim 2025-07-13 Support for LlamaForSequenceClassification (#20807)
1	0	tests/models/registry.py
4	0	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/registry.py

[99b4f080d] QiliangCui 2025-07-12 Renable google/gemma-3-1b-it accuracy test. (#20866)
2	3	tests/entrypoints/llm/test_accuracy.py

[020f58abc] Nicolò Lucchesi 2025-07-13 [Core] Support multiple tasks per model (#20771)
46	3	tests/test_config.py
160	96	vllm/config.py
31	30	vllm/entrypoints/llm.py
14	12	vllm/entrypoints/openai/api_server.py
8	6	vllm/entrypoints/openai/run_batch.py
6	0	vllm/model_executor/models/interfaces.py
10	0	vllm/model_executor/models/registry.py
3	0	vllm/model_executor/models/whisper.py

[c1acd6d7d] Wentao Ye 2025-07-12 [Refactor] Change the way of import triton (#20774)
1	1	tests/kernels/moe/test_batched_moe.py
1	2	vllm/attention/ops/triton_unified_attention.py
1	2	vllm/lora/ops/triton_ops/lora_expand_op.py
1	2	vllm/lora/ops/triton_ops/lora_shrink_op.py
1	2	vllm/model_executor/layers/fused_moe/fused_batched_moe.py

[3b3b778d4] ElizaWszola 2025-07-13 [Bugfix] Fix a couple PPLX+CUTLASS MoE bugs (#20825)
2	2	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
35	18	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[42d440c22] Wentao Ye 2025-07-12 [Perf] Use Triton instead of Torch for DeepGEMM Per Token Group Quant (#20841)
4	3	tests/kernels/moe/test_deepgemm.py
2	3	tests/kernels/quantization/test_block_fp8.py
7	6	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	6	vllm/model_executor/layers/fused_moe/utils.py
12	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py
0	21	vllm/utils/deep_gemm.py

[f45a33288] Woosuk Kwon 2025-07-12 [Sched] Enhance the logic to remove stopped requests from queues (#20739)
1	1	requirements/common.txt
62	0	tests/v1/core/test_scheduler.py
29	16	vllm/v1/core/sched/scheduler.py

[6e2c176e1] Michael Goin 2025-07-13 [Bugfix] Restrict Machete to only run on Hopper (#20830)
3	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py

[a86754a12] Reid 2025-07-12 [docs] convert supported configs to table (#20858)
14	30	docs/getting_started/installation/intel_gaudi.md

[c2a2f19ab] Alex Brooks 2025-07-12 [Bugfix] Fix Tensor Parallelism Padding Consistency in Granite Models (#20843)
4	0	vllm/model_executor/models/granite.py

[2c11a738b] Congcong Chen 2025-07-12 [Model] New model support for microsoft/Phi-4-mini-flash-reasoning (#20702)
25	24	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
1	0	docs/models/supported_models.md
4	0	tests/models/registry.py
3	0	tests/models/test_initialization.py
25	0	tests/test_utils.py
2	1	vllm/attention/backends/blocksparse_attn.py
1000	0	vllm/attention/backends/differential_flash_attn.py
2	1	vllm/attention/backends/dual_chunk_flash_attn.py
2	1	vllm/attention/backends/flash_attn.py
2	1	vllm/attention/backends/flashinfer.py
2	1	vllm/attention/backends/hpu_attn.py
2	1	vllm/attention/backends/rocm_flash_attn.py
2	1	vllm/attention/backends/xformers.py
0	4	vllm/attention/layer.py
2	1	vllm/model_executor/layers/logits_processor.py
746	0	vllm/model_executor/models/phi4flash.py
1	0	vllm/model_executor/models/registry.py
4	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
15	3	vllm/utils/__init__.py
4	0	vllm/worker/model_runner.py
24	2	vllm/worker/worker.py

[b639327ad] Michael Goin 2025-07-12 Revert "Use NVCC --compress-mode to reduce binary size by 30% #20694" (#20853)
0	10	CMakeLists.txt

[4afe687a8] Zhiyu 2025-07-11 Enable ModelOpt Llama4 fp8 checkpoint deployment (#20419)
31	6	vllm/model_executor/layers/fused_moe/layer.py
261	5	vllm/model_executor/layers/quantization/modelopt.py
10	0	vllm/model_executor/model_loader/weight_utils.py
55	4	vllm/model_executor/models/llama4.py
144	20	vllm/model_executor/models/mllama4.py

[5de8d9f11] Maximilien de Bayser 2025-07-12 Remove extra tensor on CPU (#20693)
13	5	vllm/v1/sample/logits_processor.py

[c1c8ca57f] Boyuan Feng 2025-07-11 [cold start time] add envs.VLLM_COMPILE_DEPYF to guard decompile (#20790)
13	3	vllm/compilation/wrapper.py
6	0	vllm/envs.py

[a3a5a47e4] Richard Zou 2025-07-12 [Bugfix] Fix torch.compile x LoRA for PyTorch 2.8  (#20823)
8	6	vllm/lora/layers.py

[fb25e9568] Lucia Fang 2025-07-12 [Docs] Update basic.md (#20846)
2	0	docs/contributing/model/basic.md

[0d4891cd0] Wentao Ye 2025-07-12 [Bug] Fix DeepGemm for EP low latency case (#20833)
10	9	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py

[f56d2996c] lkchen 2025-07-11 [Misc] Respect `no_use_tqdm_on_load` flag while capturing CUDA graph (#20834)
4	2	vllm/v1/worker/gpu_model_runner.py
1	0	vllm/worker/model_runner.py

[147afb448] Isotr0py 2025-07-12 [Bugfix] Replace unavailable video url in multimodal test (#20854)
1	1	tests/multimodal/test_utils.py

[3c7d942da] Nicolò Lucchesi 2025-07-12 [Frontend] Abstract prompt and SpeechToTextConfig for transcriptions models (#20637)
31	0	vllm/config.py
33	50	vllm/entrypoints/openai/speech_to_text.py
29	3	vllm/model_executor/models/interfaces.py
48	7	vllm/model_executor/models/whisper.py

[890323dc1] Varun Sundar Rabindranath 2025-07-12 [Bugfix] : Fix typo - logger.warn_once -> logger.warning_once (#20852)
1	1	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py

[01cae3771] Isotr0py 2025-07-12 [CI/Build] Ensure compatability with Transformers v4.53 (#20541)
1	1	requirements/test.in
1	1	requirements/test.txt
2	2	tests/models/multimodal/generation/test_common.py
1	0	tests/models/multimodal/processing/test_common.py
10	2	tests/models/test_initialization.py
1	7	vllm/inputs/registry.py
5	2	vllm/model_executor/models/commandr.py
18	7	vllm/model_executor/models/fuyu.py
6	3	vllm/model_executor/models/gemma3.py
11	10	vllm/model_executor/models/minicpmo.py
1	1	vllm/model_executor/models/paligemma.py
9	1	vllm/model_executor/models/qwen2_5_omni_thinker.py
8	1	vllm/model_executor/models/whisper.py

[11c019861] yurhett 2025-07-12 [Bugfix] Fix tensor parallel issue in Qwen3 reranker weight loading (#20682)
3	2	tests/models/language/pooling/mteb_utils.py
27	0	tests/models/language/pooling/test_qwen3_reranker.py
8	5	vllm/model_executor/models/adapters.py

[b1235c3e1] Li, Jiang 2025-07-12 [Bugfix] Lazy import fused_experts in BitsAndBytesMoEMethod to avoid break not-cuda-alike devices  (#20822)
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py

[44d02f54d] Jee Jee Li 2025-07-12 [Misc] Restrict deep_gemm's log output (#20827)
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py

[a8593237c] Trevor Morris 2025-07-11 Add pynccl all-gatherv and reducescatterv (#20154)
70	0	tests/distributed/test_pynccl.py
15	1	vllm/distributed/device_communicators/base_device_communicator.py
82	1	vllm/distributed/device_communicators/cuda_communicator.py
72	0	vllm/distributed/device_communicators/pynccl.py
33	0	vllm/distributed/device_communicators/pynccl_wrapper.py
12	0	vllm/distributed/parallel_state.py

[fc0f41d10] Ilya Markov 2025-07-12 Integration SM100 FlashInfer fused allreduce RMSNorm (#20691)
152	0	tests/compile/test_fusion_all_reduce.py
352	4	vllm/compilation/collective_fusion.py
6	2	vllm/compilation/pass_manager.py
4	0	vllm/config.py

[7b828e30d] Wentao Ye 2025-07-11 [CI Bug] Fix Async Engine, Inputs, Utils, Worker Test: 'State' object has no attribute 'enable_server_load_tracking' (#20845)
12	5	vllm/entrypoints/utils.py

[5f0af36af] bigmoyan 2025-07-12 Update kimi-k2 tool calling docs, enable unit tests (#20821)
8	0	docs/features/tool_calling.md
0	2	tests/tool_use/test_kimi_k2_tool_parser.py

[0d21b2664] Isotr0py 2025-07-12 [Bugfix] Fix OOM in language generation test (#20814)
1	1	tests/models/language/generation/test_common.py

[9907fc449] Nick Hill 2025-07-11 [Docs] Data Parallel deployment documentation (#20768)
1	1	README.md
1	1	docs/README.md
-	-	docs/assets/deployment/dp_external_lb.png
-	-	docs/assets/deployment/dp_internal_lb.png
112	0	docs/serving/data_parallel_deployment.md
4	0	docs/serving/distributed_serving.md

[d47661f0c] Michael Goin 2025-07-12 [Kernel] Basic tuned configs for NVFP4 CUTLASS dense GEMM (#20646)
85	50	csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu

[53fa45739] Varun Sundar Rabindranath 2025-07-11 [Misc] Add unit tests for MoE ModularKernel combinations + Profiling utility (#20449)
0	0	tests/kernels/moe/modular_kernel_tools/__init__.py
160	0	tests/kernels/moe/modular_kernel_tools/cli_args.py
641	0	tests/kernels/moe/modular_kernel_tools/common.py
173	0	tests/kernels/moe/modular_kernel_tools/make_feature_matrix.py
87	0	tests/kernels/moe/modular_kernel_tools/mk_objects.py
138	0	tests/kernels/moe/modular_kernel_tools/parallel_utils.py
127	0	tests/kernels/moe/modular_kernel_tools/profile_modular_kernel.py
142	0	tests/kernels/moe/modular_kernel_tools/utils.py
2	4	tests/kernels/moe/parallel_utils.py
214	0	tests/kernels/moe/test_modular_kernel_combinations.py
25	5	tests/kernels/utils.py
1	2	vllm/distributed/device_communicators/base_device_communicator.py
0	1	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
11	7	vllm/model_executor/layers/fused_moe/layer.py
6	3	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[6fb162447] Reid 2025-07-11 [doc] fix ordered list issue (#20819)
6	6	docs/features/spec_decode.md

[66177189c] Li, Jiang 2025-07-11 [Bugfix] Add missing field to TritonLanguagePlaceholder (#20812)
1	0	vllm/triton_utils/importing.py

[b4f0b5f9a] QiliangCui 2025-07-11 Temporarily suspend google/gemma-3-1b-it. (#20722)
6	0	tests/entrypoints/llm/test_accuracy.py

[cbd14ed56] Cyrus Leung 2025-07-11 [Bugfix] Refactor `/invocations` to be task-agnostic (#20764)
33	4	tests/entrypoints/openai/test_chat.py
22	0	tests/entrypoints/openai/test_classification.py
25	0	tests/entrypoints/openai/test_completion.py
60	0	tests/entrypoints/openai/test_embedding.py
91	22	tests/entrypoints/openai/test_pooling.py
27	0	tests/entrypoints/openai/test_rerank.py
25	0	tests/entrypoints/openai/test_score.py
45	47	vllm/entrypoints/openai/api_server.py
24	2	vllm/entrypoints/openai/protocol.py

[7bd4c37ae] Pavani Majety 2025-07-11 [Core] Add Flashinfer TRTLLM Backend for Flashinfer decode path (SM100).  (#19825)
240	0	benchmarks/kernels/benchmark_trtllm_attention.py
140	0	tests/kernels/attention/test_flashinfer_trtllm_decode_attention.py
107	16	vllm/attention/backends/flashinfer.py
2	0	vllm/engine/arg_utils.py
5	1	vllm/envs.py
17	2	vllm/platforms/cuda.py
147	36	vllm/v1/attention/backends/flashinfer.py
9	1	vllm/v1/attention/backends/utils.py

[8020e98c9] Jee Jee Li 2025-07-11 [Quantization][1/N] MoE support BNB-Inflight Quantization (#20061)
39	6	tests/models/quantization/test_bitsandbytes.py
32	4	vllm/model_executor/layers/fused_moe/layer.py
223	9	vllm/model_executor/layers/quantization/bitsandbytes.py
193	45	vllm/model_executor/model_loader/bitsandbytes_loader.py
24	9	vllm/model_executor/models/olmoe.py
11	0	vllm/model_executor/models/phimoe.py
26	9	vllm/model_executor/models/qwen2_moe.py
13	6	vllm/model_executor/models/qwen3_moe.py

[762be26a8] Luka Govedič 2025-07-11 [Bugfix] Upgrade depyf to 0.19 and streamline custom pass logging (#20777)
1	1	requirements/common.txt
6	0	tests/compile/test_full_graph.py
4	24	vllm/compilation/vllm_inductor_pass.py
2	11	vllm/config.py

[6a9e6b2ab] Reid 2025-07-11 [doc] fold long code block (#20795)
53	53	docs/features/lora.md

[5d09152ff] nopperl 2025-07-11 [V1] Enable Mamba2 layers other than MambaMixer2 in the v1 engine (#20660)
11	0	vllm/config.py
29	0	vllm/model_executor/layers/mamba/abstract.py
17	20	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	2	vllm/model_executor/models/bamba.py
0	1	vllm/model_executor/models/falcon_h1.py
1	2	vllm/model_executor/models/granitemoehybrid.py
1	2	vllm/model_executor/models/mamba2.py
0	1	vllm/model_executor/models/nemotron_h.py
1	2	vllm/model_executor/models/zamba2.py
4	11	vllm/v1/attention/backends/mamba_attn.py
3	4	vllm/v1/worker/gpu_model_runner.py

[31d5c1797] Luka Govedič 2025-07-11 [Perf][fp8] Use CustomOp abstraction for fp8 quant for better perf (#19830)
98	0	benchmarks/kernels/bench_per_token_quant_fp8.py
7	4	tests/compile/test_fusion.py
2	0	tests/compile/test_fusion_attn.py
30	7	tests/compile/test_silu_mul_quant_fusion.py
4	2	vllm/attention/backends/abstract.py
4	2	vllm/attention/backends/rocm_flash_attn.py
3	22	vllm/compilation/fusion.py
2	0	vllm/model_executor/layers/fused_moe/utils.py
16	8	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
7	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
3	3	vllm/model_executor/layers/quantization/fbgemm_fp8.py
11	3	vllm/model_executor/layers/quantization/fp8.py
103	0	vllm/model_executor/layers/quantization/input_quant_fp8.py
3	2	vllm/model_executor/layers/quantization/modelopt.py
5	3	vllm/model_executor/layers/quantization/ptpc_fp8.py
11	5	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
28	7	vllm/model_executor/layers/quantization/utils/quant_utils.py
31	35	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[35514b682] Ratnam Parikh 2025-07-10 [XPU] XCCL support enabled in torch 2.8.0.dev nightly builds (#20705)
2	2	vllm/utils/__init__.py

[e2de455c3] Wentao Ye 2025-07-10  [Feature] Integrate SM100 DeepGEMM support (#20087)
3	0	benchmarks/kernels/benchmark_moe.py
8	8	tests/kernels/moe/test_block_fp8.py
5	0	tests/kernels/moe/test_deepep_deepgemm_moe.py
8	47	tests/kernels/moe/test_deepgemm.py
11	16	tests/kernels/quantization/test_block_fp8.py
9	12	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
11	11	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
9	3	vllm/model_executor/layers/fused_moe/fused_moe.py
0	1	vllm/model_executor/layers/fused_moe/prepare_finalize.py
5	2	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
6	1	vllm/model_executor/layers/fused_moe/utils.py
1	2	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
3	5	vllm/model_executor/layers/quantization/deepgemm.py
42	4	vllm/model_executor/layers/quantization/fp8.py
124	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py
152	0	vllm/utils/deep_gemm.py

[5b032352c] Alexander Matveev 2025-07-10 [Attention] MLA - Flashinfer Ragged Prefill (#20034)
0	0	tests/v1/kv_connector/__init__.py
15	72	tests/v1/kv_connector/unit/test_multi_connector.py
62	0	tests/v1/kv_connector/unit/utils.py
1	1	vllm/attention/layer.py
33	0	vllm/attention/utils/kv_sharing_utils.py
14	0	vllm/logger.py
5	68	vllm/v1/attention/backends/flashinfer.py
222	40	vllm/v1/attention/backends/mla/common.py
1	0	vllm/v1/attention/backends/mla/cutlass_mla.py
69	34	vllm/v1/attention/backends/utils.py

[922f31644] Michael Goin 2025-07-11 [Model] Support HF format of minimax (#20211)
2	0	tests/models/registry.py
33	11	vllm/model_executor/models/minimax_text_01.py
1	0	vllm/model_executor/models/registry.py

[5923ab952] Duncan Moss 2025-07-10 [fix]: disable cutlass block scaled group gemm for EP (#20781)
4	5	csrc/quantization/cutlass_w8a8/moe/blockwise_scaled_group_mm_sm100.cu
27	2	vllm/model_executor/layers/fused_moe/cutlass_moe.py
3	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[0cf893cae] bigmoyan 2025-07-11 Add kimi-k2 tool parser (#20789)
195	0	tests/tool_use/test_kimi_k2_tool_parser.py
1	1	vllm/config.py
3	1	vllm/entrypoints/openai/tool_parsers/__init__.py
377	0	vllm/entrypoints/openai/tool_parsers/kimi_k2_tool_parser.py

[cf75cd209] Michael Goin 2025-07-11 [CI Bugfix] Specify same TORCH_CUDA_ARCH_LIST for flashinfer aot and install (#20772)
2	1	docker/Dockerfile

[b854321ff] Simon Mo 2025-07-10 [Docs] Lazy import gguf (#20785)
5	1	vllm/entrypoints/score_utils.py
5	1	vllm/model_executor/model_loader/weight_utils.py

[5b6fe23d0] Kuntai Du 2025-07-10 [Bugfix][Benchmark] Make sure the output length > 0 when testing prefill workload. (#20786)
3	0	benchmarks/benchmark_dataset.py

[f0c98cae2] Varun Sundar Rabindranath 2025-07-10 [Misc] MoE ModularKernel : Introduce TopKWeightAndReduce  (#20648)
3	0	tests/kernels/moe/test_pplx_moe.py
6	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
19	0	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
6	0	vllm/model_executor/layers/fused_moe/cutlass_moe.py
6	0	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
10	29	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
7	2	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
20	18	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
6	0	vllm/model_executor/layers/fused_moe/fused_moe.py
38	6	vllm/model_executor/layers/fused_moe/modular_kernel.py
7	0	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
11	4	vllm/model_executor/layers/fused_moe/prepare_finalize.py
139	0	vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
19	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[574ad60db] Nick Hill 2025-07-10 [KVConnector] Always call connector `clear_metadata()` at end of step (#20756)
6	3	vllm/distributed/kv_transfer/kv_connector/v1/base.py
15	19	vllm/v1/executor/multiproc_executor.py
0	4	vllm/v1/worker/gpu_model_runner.py
4	0	vllm/v1/worker/gpu_worker.py

[fdadb6f43] Varun Sundar Rabindranath 2025-07-10 [Bugfix] Fused MoE Modular Kernel chunking loop (#20392)
140	0	tests/kernels/moe/test_count_expert_num_tokens.py
2	2	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
190	105	vllm/model_executor/layers/fused_moe/modular_kernel.py
72	0	vllm/model_executor/layers/fused_moe/utils.py

[41060c6e0] Alex Brooks 2025-07-10 [Core] Add Support for Default Modality Specific LoRAs [generate / chat completions] (#19126)
77	0	docs/features/lora.md
107	0	tests/entrypoints/openai/test_default_mm_loras.py
118	0	tests/lora/test_default_mm_loras.py
11	0	vllm/config.py
10	0	vllm/engine/arg_utils.py
81	0	vllm/entrypoints/llm.py
19	1	vllm/entrypoints/openai/api_server.py
2	1	vllm/entrypoints/openai/serving_chat.py
57	3	vllm/entrypoints/openai/serving_engine.py

[3de2ed767] Ming Yang 2025-07-10 [Bugfix] Remove assertion of expert_map being None (#20714)
12	3	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py

[299252ea8] Wentao Ye 2025-07-10 [CI] Fix pre commit issue (#20782)
2	2	vllm/entrypoints/openai/serving_score.py

[d6902ce79] Nathan Hoos 2025-07-10 [V0][V1][Core] Add outlines integration for V1, and update V0 integration. (#15975)
3	1	requirements/common.txt
20	13	tests/entrypoints/llm/test_guided_generate.py
9	21	tests/model_executor/test_guided_processors.py
1	1	tests/tool_use/test_tool_choice_required.py
171	162	tests/v1/entrypoints/llm/test_struct_output_generate.py
2	1	vllm/config.py
7	0	vllm/envs.py
21	10	vllm/model_executor/guided_decoding/__init__.py
12	50	vllm/model_executor/guided_decoding/outlines_decoding.py
228	205	vllm/model_executor/guided_decoding/outlines_logits_processors.py
5	0	vllm/v1/engine/processor.py
9	0	vllm/v1/structured_output/__init__.py
319	0	vllm/v1/structured_output/backend_outlines.py

[5e53c89a7] Sanger Steel 2025-07-10 [Bugfix] [CI] Fix Tensorizer LoRA test (#20760)
3	8	tests/lora/test_llama_tp.py
2	2	vllm/lora/peft_helper.py
9	9	vllm/model_executor/model_loader/tensorizer.py

[c66e38ea4] QiliangCui 2025-07-10 [Test] Remove docker build from test. (#20542)
0	10	.buildkite/scripts/tpu/docker_run_bm.sh

[251595368] sfbemerk 2025-07-10 Fix DeepSeek-R1-0528 chat template (#20717)
16	16	examples/tool_chat_template_deepseekr1.jinja

[4bed16776] shineran96 2025-07-11 [Model][VLM] Support JinaVL Reranker (#20260)
1	1	.buildkite/test-pipeline.yaml
8	0	docs/models/supported_models.md
51	3	docs/serving/openai_compatible_server.md
89	7	examples/offline_inference/{vision_language_embedding.py => vision_language_pooling.py}
60	0	examples/online_serving/openai_cross_encoder_score_for_multimodal.py
160	0	tests/models/multimodal/pooling/test_jinavl_reranker.py
3	0	tests/models/registry.py
122	61	vllm/entrypoints/llm.py
19	5	vllm/entrypoints/openai/protocol.py
110	48	vllm/entrypoints/openai/serving_score.py
152	8	vllm/entrypoints/score_utils.py
10	0	vllm/model_executor/models/config.py
57	0	vllm/model_executor/models/interfaces.py
150	0	vllm/model_executor/models/jina_vl.py
1	0	vllm/model_executor/models/registry.py

[b140416ab] Asher 2025-07-11 [Model] Add reason parser for Hunyuan A13B Model. (#20625)
162	0	tests/reasoning/test_hunyuan_reasoning_parser.py
2	0	vllm/reasoning/__init__.py
238	0	vllm/reasoning/hunyuan_a13b_reasoning_parser.py

[5b8366b61] Gregory Shtrasberg 2025-07-10 [ROCm][Regression] Remove tensor creation that harms performance on ROCm (#20741)
0	4	vllm/platforms/rocm.py

[c7753a980] nishith-fujitsu 2025-07-10 [Hardware][CPU] Vllm int8 quantization enablement for ARM CPU (#14129)
24	4	cmake/cpu_extension.cmake
264	3	csrc/cpu/cpu_types_arm.hpp
45	13	csrc/cpu/dnnl_helper.hpp
12	9	csrc/cpu/quant.cpp
2	1	csrc/cpu/torch_bindings.cpp

[4b9a9435b] Michael Goin 2025-07-11 Update Dockerfile FlashInfer to v0.2.8rc1 (#20718)
5	2	docker/Dockerfile

[3482fd7e4] Harry Mellor 2025-07-10 [Doc] Add engine args back in to the docs (#20674)
1	0	.gitignore
10	5	docs/configuration/engine_args.md
105	0	docs/mkdocs/hooks/generate_argparse.py
1	1	docs/mkdocs/hooks/generate_examples.py
21	0	docs/mkdocs/overrides/partials/toc-item.html
2	0	mkdocs.yaml
15	0	requirements/docs.txt
26	16	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/chat_utils.py
14	7	vllm/inputs/registry.py
1	1	vllm/model_executor/models/registry.py
1	1	vllm/platforms/cpu.py
12	4	vllm/reasoning/abs_reasoning_parsers.py
8	4	vllm/transformers_utils/tokenizer.py

[77f77a951] Isotr0py 2025-07-10 [Misc] Clean up mark to fork process in BNB tests (#20692)
11	18	tests/{ => models}/quantization/test_bitsandbytes.py

[1a4f35e2e] Michael Goin 2025-07-10 Normalize lm-eval command between baseline and correctness test (#18560)
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
3	1	.buildkite/lm-eval-harness/test_lm_eval_correctness.py

[be1e128df] Michael Goin 2025-07-10 [CI Bugfix] Skip failing Tensorizer+LoRA test (#20724)
4	0	tests/lora/test_llama_tp.py

[65393ee06] Reid 2025-07-10 [doc] fix ordered list (#20749)
6	6	docs/contributing/incremental_build.md

[dc221ad72] Gregory Shtrasberg 2025-07-10 [Bugfix][Build][Non-CUDA] Only referencing CMAKE_CUDA_COMPILER_VERSION on CUDA where it is defined (#20738)
4	2	CMakeLists.txt

[7571a4a7e] Jee Jee Li 2025-07-10 [CI/Build] Fix Basic Models Test (#20728)
9	0	tests/models/test_initialization.py

[f67d986dd] Isotr0py 2025-07-10 [Misc] loose new-model tagger conditions (#20747)
0	2	.github/mergify.yml

[cc876d0f2] Or Ozeri 2025-07-10 [KVConnector] Aggregate finished requests on the scheduler (#19555)
3	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
4	61	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
105	5	vllm/v1/executor/multiproc_executor.py
6	40	vllm/v1/worker/gpu_model_runner.py
21	3	vllm/v1/worker/gpu_worker.py

[fdfd409f8] Chenyaaang 2025-07-10 [TPU][Core]Make load weight exceed hbm error more instructive for customers (#20644)
25	18	vllm/v1/worker/tpu_model_runner.py

[ffbcc9e75] Nick Hill 2025-07-10 [BugFix] Fix `VllmConfig()` construction on all platforms (#20695)
0	1	vllm/config.py
4	3	vllm/platforms/cpu.py
5	3	vllm/platforms/cuda.py
6	4	vllm/platforms/tpu.py
4	5	vllm/platforms/xpu.py

[59389c927] Nick Hill 2025-07-10 [BugFix][CPU] Fix CPU worker dependency on cumem_allocator (#20696)
9	1	vllm/v1/worker/gpu_worker.py

[8f2720def] Chauncey 2025-07-10 [Frontend] Support Tool Calling with both `tool_choice='required'` and `$defs`. (#20629)
35	0	tests/entrypoints/openai/test_completion_with_function_calling.py
21	0	vllm/entrypoints/openai/protocol.py

[ad6c2e1a0] Seiji Eicher 2025-07-09 Correct PPMissingLayer handling in Deepseek-V2-Lite PP deployment (#20665)
9	3	vllm/model_executor/models/deepseek_v2.py

[49e8c7ea2] Michael Goin 2025-07-10 Use NVCC `--compress-mode` to reduce binary size by 30% (#20694)
19	12	CMakeLists.txt

[805d62ca8] Varun Sundar Rabindranath 2025-07-09 [Misc] DP : Add ExpertTokensMetadata (#20332)
4	2	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
2	2	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
7	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
17	9	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
7	3	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
32	44	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
34	9	vllm/model_executor/layers/fused_moe/modular_kernel.py
7	3	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
3	2	vllm/model_executor/layers/fused_moe/prepare_finalize.py
2	2	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[b7d9e9416] Michael Goin 2025-07-10 [CI/Build] Fix FlashInfer double build in Dockerfile (#20651)
12	16	docker/Dockerfile

[7c12a765a] Woosuk Kwon 2025-07-09 [Misc] Simplify the prefix caching logic on draft tokens (#20701)
10	6	vllm/v1/core/kv_cache_manager.py
0	5	vllm/v1/core/sched/scheduler.py

[cd587c93e] Yiming 2025-07-10 [BugFix]: Properly set engine_id when using multi connector (#19487)
3	0	tests/v1/kv_connector/unit/test_multi_connector.py
4	2	tests/v1/kv_connector/unit/test_nixl_connector.py
5	2	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
36	27	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[332d4cb17] fxmarty-amd 2025-07-09 [Feature][Quantization] MXFP4 support for MOE models (#17888)
25	0	docs/features/quantization/quark.md
1	0	tests/kernels/moe/test_moe.py
57	0	tests/kernels/moe/test_mxfp4_moe.py
287	0	tests/quantization/reference_mxfp4.py
171	0	tests/quantization/test_quark.py
0	9	vllm/envs.py
6	1	vllm/model_executor/layers/fused_moe/config.py
8	0	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
44	9	vllm/model_executor/layers/fused_moe/fused_moe.py
3	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
22	2	vllm/model_executor/layers/fused_moe/utils.py
0	6	vllm/model_executor/layers/quantization/quark/quark.py
167	3	vllm/model_executor/layers/quantization/quark/quark_moe.py
32	46	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
52	30	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py

[bf03ff357] Jacob Manning 2025-07-09 [Kernel] Add Conch backend for mixed-precision linear layer (#19818)
1	0	requirements/rocm.txt
4	1	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
92	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/conch.py
4	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
4	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py

[47043eb67] Tuan, Hoang-Trong 2025-07-09 [Kernel] Triton implementation of causal-conv1d for Mamba-based models (#18218)
0	1	CMakeLists.txt
0	656	csrc/mamba/causal_conv1d/causal_conv1d.cu
0	159	csrc/mamba/causal_conv1d/causal_conv1d.h
0	28	csrc/mamba/causal_conv1d/static_switch.h
0	16	csrc/ops.h
0	22	csrc/torch_bindings.cpp
44	114	tests/kernels/mamba/test_causal_conv1d.py
2	2	tests/kernels/mamba/test_mamba_ssm_ssd.py
5	29	vllm/_custom_ops.py
103	42	vllm/model_executor/layers/mamba/mamba2_metadata.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer.py
17	9	vllm/model_executor/layers/mamba/mamba_mixer2.py
902	61	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
4	2	vllm/model_executor/models/mamba_cache.py
42	3	vllm/v1/attention/backends/mamba_attn.py

[31b96d1c6] Michael Goin 2025-07-10 Support Llama 4 for cutlass_moe_fp4 (#20453)
28	9	vllm/model_executor/layers/fused_moe/cutlass_moe.py
20	20	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
32	45	vllm/model_executor/layers/quantization/modelopt.py

[e59ba9e14] Li, Jiang 2025-07-10 [CI/Build] Enlarge tolerance for a CPU multi-modal test (#20684)
1	0	tests/models/multimodal/generation/test_common.py

[403b48157] Harry Mellor 2025-07-09 Remove heading form installation `inc.md` file (#20697)
16	17	docs/getting_started/installation/cpu/apple.inc.md

[138709f8d] Li, Jiang 2025-07-10 [Doc] Update CPU doc (#20676)
35	70	docs/getting_started/installation/cpu.md
16	1	docs/getting_started/installation/cpu/arm.inc.md
5	2	docs/getting_started/installation/cpu/build.inc.md
17	0	docs/getting_started/installation/cpu/s390x.inc.md
27	12	docs/getting_started/installation/cpu/x86.inc.md

[0bbac1c1b] Michael Goin 2025-07-10 [Bench] Add NVFP4 GEMM benchmark script (#20578)
141	0	benchmarks/kernels/bench_nvfp4_gemm.py

[a3e4e85ec] Liangliang Ma 2025-07-10 [XPU][CI] enhance xpu test support (#20652)
3	2	tests/conftest.py
3	0	vllm/distributed/device_communicators/xpu_communicator.py
6	4	vllm/distributed/parallel_state.py
5	5	vllm/platforms/xpu.py
1	1	vllm/v1/worker/xpu_model_runner.py

[eb58f5953] Chengji Yao 2025-07-09 [TPU][Bugfix] fix test_pallas (#20666)
2	0	tests/v1/tpu/test_pallas.py

[4ac9c33f7] Sanger Steel 2025-07-09 [Bugfix] Fix handling of Tensorizer arguments for LoadConfig (#20643)
0	19	tests/tensorizer_loader/test_tensorizer.py
15	29	vllm/engine/arg_utils.py
5	3	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/model_executor/model_loader/tensorizer_loader.py

[efe73d057] Reid 2025-07-09 [doc] update doc format (#20673)
51	27	docs/contributing/ci/update_pytorch_version.md

[853487bc1] Ricardo Decal 2025-07-09 [Docs] Improve docs for RLHF co-location example (#20599)
74	41	examples/offline_inference/rlhf_colocate.py

[9ff2af6d2] Li Wang 2025-07-09 [Benchmark] Parameterization of streaming loading of multimodal datasets (#20528)
3	1	benchmarks/benchmark_dataset.py
6	0	benchmarks/benchmark_serving.py
6	0	benchmarks/benchmark_throughput.py
9	1	vllm/benchmarks/datasets.py

[70ca5484f] Cyrus Leung 2025-07-09 [Doc] Update notes (#20668)
6	3	docs/deployment/integrations/production-stack.md
6	9	docs/features/tool_calling.md
2	2	docs/models/supported_models.md

[5358cce5f] Thomas Parnell 2025-07-09 [V1] [Doc] Update V1 docs for Mamba models (#20499)
6	6	docs/models/supported_models.md
11	3	docs/usage/v1_guide.md

[2155e95ef] Chauncey 2025-07-09 [Bugfix] Fix the issue where `reasoning_content` is `None` when Thinkng is enabled and `tool_choice` is set to `'required'`. (#20662)
5	1	tests/entrypoints/openai/test_completion_with_function_calling.py
1	0	vllm/entrypoints/openai/serving_chat.py

[f95570a52] qscqesze 2025-07-09 [Docs] fix minimax tool_calling docs error (#20667)
3	3	docs/features/tool_calling.md

[b6e7e3d58] Kunshang Ji 2025-07-09 [Intel GPU] support ray as distributed executor backend for XPU. (#20659)
2	0	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	1	vllm/executor/ray_distributed_executor.py

[e760fcef2] Dmitry Rogozhkin 2025-07-09 [XPU] Use spawn with XPU multiprocessing (#20649)
4	3	tests/utils.py
2	2	tests/v1/e2e/test_cascade_attention.py
9	0	vllm/utils/__init__.py

[6bbf1795b] B-201 2025-07-09 [Misc] Fix the size of batched_dummy_mm_inputs in profile_run (#20434)
2	1	tests/models/registry.py
7	5	vllm/v1/worker/gpu_model_runner.py

[9e0ef888f] Michael Goin 2025-07-09 Fix bullets in incremental_build.md (#20642)
1	0	docs/contributing/incremental_build.md

[97abeb1da] Duncan Moss 2025-07-08 [feat] enable SM100 CUTLASS block scaled group gemm for smaller batch sizes (#20640)
4	6	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[34dad19e7] zhrrr 2025-07-09 [Bugfix] set default set cuda_graph_sizes to min(self.max_num_seqs * 2, 512) (#20628)
12	4	vllm/config.py

[6db31e7a2] Akash kaothalkar 2025-07-09 [Hardware][PPC64LE] Enable V1 for ppc64le and ARM (#20554)
9	6	vllm/engine/arg_utils.py
3	2	vllm/platforms/cpu.py
4	2	vllm/v1/attention/backends/cpu_attn.py
61	3	vllm/v1/worker/cpu_worker.py

[977180c91] Ricardo Decal 2025-07-08 [Docs] Improve documentation for multi-node service helper script (#20600)
32	7	examples/online_serving/multi-node-serving.sh

[c40784c79] Ratnam Parikh 2025-07-08 [BugFix][Intel GPU] Use refactored API for dist_backend in V1 worker (#20596)
2	2	vllm/v1/worker/xpu_worker.py

[baed180aa] kourosh hakhamaneshi 2025-07-08 [tech debt] Revisit lora request model checker (#20636)
2	1	tests/entrypoints/openai/test_serving_models.py
3	6	vllm/entrypoints/openai/serving_engine.py
60	55	vllm/entrypoints/openai/serving_models.py

[0b407479e] Kunshang Ji 2025-07-09 [misc]refactor `Platform.set_device` method (#20262)
7	0	vllm/platforms/cpu.py
1	1	vllm/platforms/cuda.py
7	0	vllm/platforms/hpu.py
1	1	vllm/platforms/interface.py
11	0	vllm/platforms/rocm.py
7	0	vllm/platforms/tpu.py
7	0	vllm/platforms/xpu.py
1	1	vllm/v1/worker/gpu_worker.py
1	1	vllm/v1/worker/xpu_worker.py

[5eaf57005] Wenxin Cheng 2025-07-08 Replace `multiply_add` with `homogeneous_multiply_add` to Address Clang Template Parameter Issue (#20142)
3	3	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp
4	4	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp

[d8ee5a2ca] QiliangCui 2025-07-08 [TPU][Bugfix] disable phi-3 test (#20632)
1	0	tests/v1/tpu/test_basic.py

[b9fca8325] Isotr0py 2025-07-09 [Bugfix] Fix GLM-4.1-V video prompt update (#20635)
5	2	vllm/model_executor/models/glm4_1v.py

[32dffc277] Cyrus Leung 2025-07-09 [Core] Rename `get_max_tokens_per_item` for backward compatibility (#20630)
5	4	vllm/model_executor/models/qwen2_vl.py
18	13	vllm/multimodal/processing.py
7	2	vllm/multimodal/profiling.py

[c438183e9] Ming Yang 2025-07-08 [Bugfix] Fix topk_ids indices_type for CUTLASS w8a8 FP8 MoE (#20166)
4	4	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
4	2	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
9	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[baba0389f] wang.yuqi 2025-07-08 [CI] Increase the threshold of the MTEB RERANK tests (#20615)
1	1	tests/models/language/pooling/mteb_utils.py
0	1	tests/models/language/pooling/test_baai.py
2	5	tests/models/language/pooling/test_jina.py
0	2	tests/models/language/pooling/test_qwen3_reranker.py

[c6c22f16d] viravera 2025-07-08 Revert invalid spellchecker fix on deepseek_vl2 (#20618)
4	4	vllm/model_executor/models/deepseek_vl2.py

[dd382e0fe] Cyrus Leung 2025-07-08 [Model] Implement missing `get_language_model` for Keye-VL (#20631)
3	0	vllm/model_executor/models/keye.py

[849590a2a] XiongfeiWei 2025-07-08 Update torch/xla pin to 20250703 (#20589)
5	5	requirements/tpu.txt

[a4c23314c] Yan Ma 2025-07-08 [xpu]feat: support multi-lora on xpu (#20616)
2	0	vllm/lora/ops/triton_ops/lora_expand_op.py
2	0	vllm/lora/ops/triton_ops/lora_shrink_op.py
9	3	vllm/lora/ops/triton_ops/utils.py
4	1	vllm/model_executor/model_loader/tensorizer.py
11	0	vllm/platforms/xpu.py

[b942c094e] Harry Mellor 2025-07-08 Stop using title frontmatter and fix doc that can only be reached by search (#20623)
1	0	docs/.nav.yml
1	3	docs/community/contact_us.md
1	3	docs/community/meetups.md
1	3	docs/configuration/engine_args.md
1	3	docs/configuration/serve_args.md
1	3	docs/contributing/benchmarks.md
0	0	docs/contributing/{ci-failures.md => ci/failures.md}
1	3	docs/{ => contributing}/ci/update_pytorch_version.md
1	3	docs/contributing/model/README.md
1	3	docs/contributing/model/basic.md
1	3	docs/contributing/model/multimodal.md
1	3	docs/contributing/model/registration.md
1	3	docs/contributing/model/tests.md
1	3	docs/deployment/docker.md
2	3	docs/deployment/frameworks/anyscale.md
1	3	docs/deployment/frameworks/anything-llm.md
1	3	docs/deployment/frameworks/autogen.md
1	3	docs/deployment/frameworks/bentoml.md
1	3	docs/deployment/frameworks/cerebrium.md
1	3	docs/deployment/frameworks/chatbox.md
1	3	docs/deployment/frameworks/dify.md
1	3	docs/deployment/frameworks/dstack.md
1	3	docs/deployment/frameworks/haystack.md
1	3	docs/deployment/frameworks/helm.md
1	3	docs/deployment/frameworks/litellm.md
1	3	docs/deployment/frameworks/lobe-chat.md
1	3	docs/deployment/frameworks/lws.md
1	3	docs/deployment/frameworks/modal.md
1	3	docs/deployment/frameworks/open-webui.md
1	3	docs/deployment/frameworks/retrieval_augmented_generation.md
1	3	docs/deployment/frameworks/skypilot.md
1	3	docs/deployment/frameworks/streamlit.md
1	3	docs/deployment/frameworks/triton.md
1	3	docs/deployment/integrations/kserve.md
1	3	docs/deployment/integrations/kubeai.md
1	3	docs/deployment/integrations/llamastack.md
1	3	docs/deployment/integrations/llmaz.md
1	3	docs/deployment/integrations/production-stack.md
1	3	docs/deployment/k8s.md
1	3	docs/deployment/nginx.md
1	3	docs/design/arch_overview.md
1	3	docs/design/automatic_prefix_caching.md
1	3	docs/design/huggingface_integration.md
1	3	docs/design/kernel/paged_attention.md
1	3	docs/design/mm_processing.md
1	3	docs/design/plugin_system.md
1	3	docs/features/automatic_prefix_caching.md
1	3	docs/features/compatibility_matrix.md
1	3	docs/features/disagg_prefill.md
1	3	docs/features/lora.md
1	3	docs/features/multimodal_inputs.md
1	3	docs/features/quantization/README.md
1	3	docs/features/quantization/auto_awq.md
1	3	docs/features/quantization/bitblas.md
1	3	docs/features/quantization/bnb.md
1	3	docs/features/quantization/fp8.md
1	3	docs/features/quantization/gguf.md
1	3	docs/features/quantization/gptqmodel.md
1	3	docs/features/quantization/int4.md
1	3	docs/features/quantization/int8.md
1	3	docs/features/quantization/quantized_kvcache.md
1	3	docs/features/quantization/quark.md
1	3	docs/features/quantization/supported_hardware.md
1	3	docs/features/reasoning_outputs.md
1	3	docs/features/spec_decode.md
1	3	docs/features/structured_outputs.md
1	3	docs/getting_started/installation/README.md
1	3	docs/getting_started/quickstart.md
1	3	docs/models/extensions/runai_model_streamer.md
1	3	docs/models/extensions/tensorizer.md
1	3	docs/models/generative_models.md
1	3	docs/models/hardware_supported_models/tpu.md
1	3	docs/models/pooling_models.md
1	3	docs/models/supported_models.md
1	3	docs/serving/distributed_serving.md
1	3	docs/serving/integrations/langchain.md
1	3	docs/serving/integrations/llamaindex.md
2	4	docs/serving/offline_inference.md
1	3	docs/serving/openai_compatible_server.md
1	3	docs/usage/faq.md
1	3	docs/usage/troubleshooting.md

[b4bab8166] Harry Mellor 2025-07-08 Remove unnecessary explicit title anchors and use relative links instead (#20620)
1	1	docs/README.md
1	1	docs/api/README.md
0	1	docs/community/contact_us.md
0	1	docs/community/meetups.md
1	1	docs/configuration/conserving_memory.md
2	3	docs/configuration/engine_args.md
1	1	docs/configuration/model_resolution.md
1	2	docs/configuration/serve_args.md
0	1	docs/contributing/benchmarks.md
1	1	docs/contributing/dockerfile/dockerfile.md
1	2	docs/contributing/model/README.md
1	2	docs/contributing/model/basic.md
4	5	docs/contributing/model/multimodal.md
5	6	docs/contributing/model/registration.md
0	1	docs/contributing/model/tests.md
1	2	docs/deployment/docker.md
0	1	docs/deployment/frameworks/anything-llm.md
0	1	docs/deployment/frameworks/autogen.md
0	1	docs/deployment/frameworks/bentoml.md
0	1	docs/deployment/frameworks/cerebrium.md
0	1	docs/deployment/frameworks/chatbox.md
0	1	docs/deployment/frameworks/dify.md
0	1	docs/deployment/frameworks/dstack.md
0	1	docs/deployment/frameworks/haystack.md
0	1	docs/deployment/frameworks/helm.md
0	1	docs/deployment/frameworks/litellm.md
0	1	docs/deployment/frameworks/lobe-chat.md
0	1	docs/deployment/frameworks/lws.md
0	1	docs/deployment/frameworks/modal.md
0	1	docs/deployment/frameworks/open-webui.md
0	1	docs/deployment/frameworks/retrieval_augmented_generation.md
0	1	docs/deployment/frameworks/skypilot.md
0	1	docs/deployment/frameworks/streamlit.md
0	1	docs/deployment/frameworks/triton.md
0	1	docs/deployment/integrations/kserve.md
0	1	docs/deployment/integrations/kubeai.md
0	1	docs/deployment/integrations/llamastack.md
0	1	docs/deployment/integrations/llmaz.md
0	1	docs/deployment/integrations/production-stack.md
0	1	docs/deployment/k8s.md
0	1	docs/deployment/nginx.md
2	3	docs/design/arch_overview.md
0	1	docs/design/automatic_prefix_caching.md
0	1	docs/design/huggingface_integration.md
0	1	docs/design/kernel/paged_attention.md
1	2	docs/design/mm_processing.md
1	2	docs/design/plugin_system.md
1	2	docs/features/automatic_prefix_caching.md
7	8	docs/features/compatibility_matrix.md
0	1	docs/features/disagg_prefill.md
0	1	docs/features/lora.md
0	1	docs/features/multimodal_inputs.md
0	1	docs/features/quantization/README.md
0	1	docs/features/quantization/auto_awq.md
0	1	docs/features/quantization/bitblas.md
0	1	docs/features/quantization/bnb.md
0	1	docs/features/quantization/fp8.md
0	1	docs/features/quantization/gguf.md
0	1	docs/features/quantization/gptqmodel.md
0	1	docs/features/quantization/int4.md
0	1	docs/features/quantization/int8.md
0	1	docs/features/quantization/quantized_kvcache.md
0	1	docs/features/quantization/quark.md
0	1	docs/features/quantization/supported_hardware.md
0	1	docs/features/reasoning_outputs.md
2	3	docs/features/spec_decode.md
1	2	docs/features/structured_outputs.md
0	1	docs/getting_started/installation/README.md
2	2	docs/getting_started/installation/intel_gaudi.md
2	3	docs/getting_started/quickstart.md
9	4	docs/mkdocs/hooks/generate_examples.py
0	1	docs/models/extensions/runai_model_streamer.md
0	1	docs/models/extensions/tensorizer.md
2	3	docs/models/generative_models.md
0	1	docs/models/hardware_supported_models/tpu.md
3	4	docs/models/pooling_models.md
14	15	docs/models/supported_models.md
0	1	docs/serving/distributed_serving.md
0	1	docs/serving/integrations/langchain.md
0	1	docs/serving/integrations/llamaindex.md
2	3	docs/serving/offline_inference.md
2	3	docs/serving/openai_compatible_server.md
1	2	docs/usage/faq.md
1	1	docs/usage/metrics.md
1	2	docs/usage/troubleshooting.md
1	1	docs/usage/v1_guide.md

[b91cb3fa5] Ricardo Decal 2025-07-08 [Docs] Improve documentation for Deepseek R1 on Ray Serve LLM (#20601)
19	12	examples/online_serving/ray_serve_deepseek.py

[71d1d75b7] Nicolò Lucchesi 2025-07-08 [PD][Nixl] Remote consumer READ timeout for clearing request blocks  (#20139)
74	4	tests/v1/kv_connector/unit/test_nixl_connector.py
32	5	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
9	1	vllm/envs.py

[72d14d0ee] Sanger Steel 2025-07-08 [Frontend] [Core] Integrate Tensorizer in to S3 loading machinery, allow passing arbitrary arguments during save/load (#19619)
83	25	examples/others/tensorize_vllm_model.py
1	1	requirements/nightly_torch_test.txt
1	1	requirements/rocm.txt
1	1	requirements/test.in
1	1	requirements/test.txt
1	1	setup.py
10	8	tests/entrypoints/openai/test_tensorizer_entrypoint.py
3	2	tests/lora/test_llama_tp.py
85	0	tests/tensorizer_loader/conftest.py
301	19	tests/tensorizer_loader/test_tensorizer.py
7	3	vllm/config.py
34	2	vllm/engine/arg_utils.py
4	3	vllm/lora/models.py
1	1	vllm/lora/peft_helper.py
257	121	vllm/model_executor/model_loader/tensorizer.py
15	0	vllm/model_executor/model_loader/tensorizer_loader.py
1	0	vllm/v1/worker/gpu_model_runner.py
1	0	vllm/worker/model_runner.py

[e34d130c1] Chenyaaang 2025-07-07 [TPU] Temporary fix vmem oom for long model len by reducing page size (#20278)
6	0	vllm/v1/attention/backends/pallas.py

[7721ef178] Li, Jiang 2025-07-08 [CI/Build][CPU] Fix CPU CI and remove all CPU V0 files (#20560)
12	12	.buildkite/scripts/hardware_ci/run-cpu-test.sh
0	58	tests/basic_correctness/test_chunked_prefill.py
6	2	tests/models/language/generation/test_common.py
11	12	tests/models/language/pooling/test_embedding.py
5	0	tests/models/language/pooling/test_reward.py
2	1	tests/quantization/test_compressed_tensors.py
0	546	vllm/attention/backends/torch_sdpa.py
0	195	vllm/attention/ops/ipex_attn.py
749	13	vllm/v1/attention/backends/cpu_attn.py

[8369b7c2a] Reid 2025-07-08 [Misc] improve error msg (#20604)
6	3	vllm/entrypoints/cli/serve.py

[3eb4ad53f] Ricardo Decal 2025-07-07 [Docs] Add Anyscale to frameworks (#20590)
9	0	docs/deployment/frameworks/anyscale.md

[90a2769f2] Ricardo Decal 2025-07-07 [Docs] Add Ray Serve LLM section to openai compatible server guide (#20595)
14	0	docs/serving/openai_compatible_server.md

[e60d422f1] Ricardo Decal 2025-07-07 [Docs] Improve docstring for ray data llm example (#20597)
11	9	examples/offline_inference/batch_llm_inference.py

[0d914c81a] Ricardo Decal 2025-07-07 [Docs] Rewrite offline inference guide (#20594)
19	8	docs/serving/offline_inference.md

[6e428cdd7] Harry Mellor 2025-07-08 [Doc] Syntax highlight request responses as JSON instead of bash (#20582)
6	6	docs/serving/openai_compatible_server.md

[93b9d9f49] Chauncey 2025-07-08 [Bugfix]: Fix messy code when using logprobs (#19209)
14	0	tests/test_utils.py
1	1	tests/v1/engine/test_output_processor.py
10	2	vllm/transformers_utils/detokenizer_utils.py

[af107d5a0] Harry Mellor 2025-07-08 Make distinct `code` and `console` admonitions so readers are less likely to miss them (#20585)
1	1	docs/cli/README.md
2	2	docs/configuration/conserving_memory.md
1	1	docs/configuration/env_vars.md
1	1	docs/contributing/README.md
1	1	docs/contributing/model/basic.md
20	20	docs/contributing/model/multimodal.md
1	1	docs/contributing/profiling.md
1	1	docs/deployment/docker.md
1	1	docs/deployment/frameworks/autogen.md
3	3	docs/deployment/frameworks/cerebrium.md
3	3	docs/deployment/frameworks/dstack.md
1	1	docs/deployment/frameworks/haystack.md
1	1	docs/deployment/frameworks/litellm.md
2	2	docs/deployment/frameworks/lws.md
6	6	docs/deployment/frameworks/skypilot.md
3	3	docs/deployment/integrations/production-stack.md
2	2	docs/deployment/k8s.md
2	2	docs/deployment/nginx.md
2	2	docs/design/arch_overview.md
1	1	docs/design/kernel/paged_attention.md
1	1	docs/design/plugin_system.md
10	10	docs/design/v1/p2p_nccl_connector.md
2	2	docs/design/v1/torch_compile.md
4	4	docs/features/lora.md
10	10	docs/features/multimodal_inputs.md
2	2	docs/features/quantization/auto_awq.md
1	1	docs/features/quantization/bitblas.md
1	1	docs/features/quantization/fp8.md
1	1	docs/features/quantization/gguf.md
2	2	docs/features/quantization/gptqmodel.md
3	3	docs/features/quantization/int4.md
2	2	docs/features/quantization/int8.md
2	2	docs/features/quantization/modelopt.md
2	2	docs/features/quantization/quantized_kvcache.md
5	5	docs/features/quantization/quark.md
1	1	docs/features/quantization/torchao.md
6	6	docs/features/reasoning_outputs.md
5	5	docs/features/spec_decode.md
8	8	docs/features/structured_outputs.md
2	2	docs/features/tool_calling.md
2	2	docs/getting_started/installation/cpu.md
2	2	docs/getting_started/installation/gpu/rocm.inc.md
2	2	docs/getting_started/installation/intel_gaudi.md
2	2	docs/getting_started/quickstart.md
30	0	docs/mkdocs/stylesheets/extra.css
1	1	docs/models/generative_models.md
1	1	docs/models/supported_models.md
1	1	docs/serving/integrations/langchain.md
20	20	docs/serving/openai_compatible_server.md
2	2	docs/usage/metrics.md
3	3	docs/usage/troubleshooting.md
1	1	docs/usage/usage_stats.md

[31c5d0a1b] Woosuk Kwon 2025-07-07 [Optimize] Don't send token ids when kv connector is not used (#20586)
5	1	vllm/v1/core/sched/scheduler.py

[afb7cff1b] Ming Yang 2025-07-07 [Bugfix] Fix Maverick correctness by filling zero to cache space in cutlass_moe (#20167)
117	20	tests/kernels/moe/test_cutlass_moe.py
6	2	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[d2e841a10] Kyle Yu 2025-07-07 [Misc] Improve logging for dynamic shape cache compilation (#20573)
36	15	vllm/compilation/backends.py

[14601f5fb] Patrick von Platen 2025-07-08 [Config] Refactor mistral configs  (#20570)
3	0	vllm/model_executor/models/llama.py
44	113	vllm/transformers_utils/config.py
120	0	vllm/transformers_utils/configs/mistral.py

[042d131f3] Harry Mellor 2025-07-07 Fix links in multi-modal model contributing page (#18615)
1	1	docs/contributing/model/multimodal.md
11	0	vllm/model_executor/models/interfaces.py

[8e807cdfa] rongfu.leng 2025-07-08 [Misc] feat output content in stream response (#19608)
148	2	vllm/entrypoints/openai/api_server.py

[e601efcb1] Anton 2025-07-07 [Misc] Add fully interleaved support for multimodal 'string' content format (#14047)
350	2	tests/entrypoints/test_chat_utils.py
13	1	vllm/config.py
5	0	vllm/engine/arg_utils.py
111	41	vllm/entrypoints/chat_utils.py

[22dd9c273] jvlunteren 2025-07-07 [Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel (#20308)
13	1	vllm/attention/ops/triton_unified_attention.py

[a6d795d59] Rui Qiao 2025-07-07 [DP] Copy environment variables to Ray DPEngineCoreActors (#20344)
5	28	vllm/executor/ray_distributed_executor.py
71	0	vllm/ray/ray_env.py
17	7	vllm/v1/engine/utils.py

[a37d75bbe] ztang2370 2025-07-08 [Front-end] microbatch tokenization (#19334)
23	16	tests/entrypoints/openai/test_serving_chat.py
73	48	vllm/entrypoints/openai/serving_engine.py
192	0	vllm/utils/__init__.py

[edd270bc7] Peter Pan 2025-07-08 [Bugfix] Prevent IndexError for cached requests when pipeline parallelism is disabled (#20486)
2	0	vllm/v1/core/sched/scheduler.py

[110df7433] wang.yuqi 2025-07-07 [Model][Last/4] Automatic conversion of CrossEncoding model (#19675)
8	0	docs/models/supported_models.md
134	0	examples/offline_inference/convert_model_to_seq_cls.py
3	2	tests/models/language/pooling/mteb_utils.py
140	0	tests/models/language/pooling/test_bge_reranker_v2_gemma.py
0	2	tests/models/language/pooling/test_mxbai_rerank.py
6	1	tests/models/registry.py
6	0	vllm/config.py
8	4	vllm/entrypoints/llm.py
14	4	vllm/entrypoints/openai/serving_score.py
48	0	vllm/model_executor/models/adapters.py
4	0	vllm/model_executor/models/gemma.py
2	1	vllm/model_executor/models/registry.py

[1ad69e837] Harry Mellor 2025-07-07 [Doc] Fix some MkDocs snippets used in the installation docs (#20572)
0	3	docs/getting_started/installation/cpu/apple.inc.md
0	3	docs/getting_started/installation/cpu/arm.inc.md
0	3	docs/getting_started/installation/cpu/s390x.inc.md
0	3	docs/getting_started/installation/cpu/x86.inc.md
2	2	docs/getting_started/installation/gpu.md
0	4	docs/getting_started/installation/gpu/cuda.inc.md
6	4	docs/getting_started/installation/gpu/rocm.inc.md
2	4	docs/getting_started/installation/gpu/xpu.inc.md

[b8a498c9b] Harry Mellor 2025-07-07 [Doc] Add outline for content tabs (#20571)
10	0	docs/mkdocs/stylesheets/extra.css

[923147b5e] Harry Mellor 2025-07-07 [Doc] Fix internal links so they don't always point to latest (#20563)
3	3	docs/features/structured_outputs.md

[45877ef74] Harry Mellor 2025-07-07 [Doc] Use `gh-pr` and `gh-issue` everywhere we can in the docs (#20564)
5	7	docs/ci/update_pytorch_version.md
3	3	docs/features/spec_decode.md
2	2	docs/usage/troubleshooting.md
12	12	docs/usage/v1_guide.md

[6e4bef1be] Harry Mellor 2025-07-07 [Doc] Remove extra whitespace from CI failures doc (#20565)
20	20	docs/contributing/ci-failures.md

[4ff79a136] Jee Jee Li 2025-07-07 [Misc] Set the minimum openai version (#20539)
1	1	requirements/common.txt

[448acad31] Abirdcfly 2025-07-07 [Misc] remove unused jinaai_serving_reranking (#18878)
0	5	vllm/entrypoints/openai/api_server.py

[eb0b2d2f0] Michael Yao 2025-07-07 [Docs] Clean up tables in supported_models.md (#20552)
160	160	docs/models/supported_models.md

[3112271f6] Yan Ma 2025-07-07 [XPU] log clean up for XPU platform (#20553)
2	1	vllm/_custom_ops.py
2	3	vllm/platforms/xpu.py

[1fd471e95] Michael Yao 2025-07-07 Add docstrings to url_schemes.py to improve readability (#20545)
69	1	docs/mkdocs/hooks/url_schemes.py

[2c5ebec06] Liangliang Ma 2025-07-07 [XPU][CI] add v1/core test in xpu hardware ci (#20537)
4	2	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	1	docker/Dockerfile.xpu
1	5	vllm/platforms/xpu.py

[2e610deb7] Jee Jee Li 2025-07-07 [CI/Build] Enable phi2 lora test (#20540)
0	5	tests/lora/test_phi.py

[6e2c19ce2] Yang Yang 2025-07-07 [Refactor]Abstract Platform Interface for Distributed Backend and Add xccl Support for Intel XPU (#19410)
5	0	docs/getting_started/installation/gpu/xpu.inc.md
11	2	vllm/platforms/__init__.py
1	0	vllm/platforms/cpu.py
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
3	0	vllm/platforms/interface.py
1	0	vllm/platforms/neuron.py
1	0	vllm/platforms/rocm.py
1	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py
6	0	vllm/utils/__init__.py
3	1	vllm/v1/worker/cpu_worker.py
2	1	vllm/v1/worker/gpu_worker.py
2	1	vllm/v1/worker/tpu_worker.py
2	1	vllm/worker/hpu_worker.py
1	1	vllm/worker/neuron_worker.py
2	1	vllm/worker/worker.py

[47db8c2c1] Reid 2025-07-07 [Misc] add a tip for pre-commit (#20536)
1	1	.pre-commit-config.yaml

[462b26928] Woosuk Kwon 2025-07-06 Implement OpenAI Responses API [1/N] (#20504)
4	0	tests/entrypoints/openai/test_openai_schema.py
0	0	tests/v1/entrypoints/openai/responses/__init__.py
32	0	tests/v1/entrypoints/openai/responses/conftest.py
75	0	tests/v1/entrypoints/openai/responses/test_basic.py
137	0	tests/v1/entrypoints/openai/responses/test_stateful.py
92	0	tests/v1/entrypoints/openai/responses/test_structured_output.py
3	1	vllm/entrypoints/chat_utils.py
89	2	vllm/entrypoints/openai/api_server.py
201	0	vllm/entrypoints/openai/protocol.py
5	3	vllm/entrypoints/openai/serving_engine.py
464	0	vllm/entrypoints/openai/serving_responses.py
4	2	vllm/reasoning/abs_reasoning_parsers.py

[c18b3b8e8] Cyrus Leung 2025-07-07 [Bugfix] Add `use_cross_encoder` flag to use correct activation in `ClassifierPooler` (#20527)
1	1	vllm/entrypoints/llm.py
6	4	vllm/entrypoints/openai/protocol.py
4	6	vllm/entrypoints/openai/serving_score.py
31	11	vllm/model_executor/layers/pooler.py
0	5	vllm/model_executor/models/bert.py
0	5	vllm/model_executor/models/roberta.py
3	0	vllm/pooling_params.py
11	9	vllm/transformers_utils/config.py

[9528e3a05] Woosuk Kwon 2025-07-06 [BugFix][Spec Decode] Fix spec token ids in model runner (#20530)
12	11	vllm/v1/worker/gpu_model_runner.py

[9fb52e523] Cyrus Leung 2025-07-07 [V1] Support any head size for FlexAttention backend (#20467)
1	2	.buildkite/scripts/hardware_ci/run-amd-test.sh
2	8	docs/models/supported_models.md
2	2	examples/offline_inference/vision_language.py
12	1	tests/kernels/attention/test_attention_selector.py
2	7	tests/models/multimodal/generation/test_common.py
1	1	tests/models/quantization/test_gguf.py
6	9	tests/models/registry.py
2	1	tests/models/test_initialization.py
2	1	vllm/attention/layer.py
30	3	vllm/attention/selector.py
1	1	vllm/config.py
28	16	vllm/platforms/cuda.py
18	2	vllm/v1/attention/backends/cpu_attn.py
14	8	vllm/v1/attention/backends/flash_attn.py
16	10	vllm/v1/attention/backends/flashinfer.py
20	19	vllm/v1/attention/backends/flex_attention.py
15	8	vllm/v1/attention/backends/mla/common.py
14	10	vllm/v1/attention/backends/rocm_aiter_fa.py
14	7	vllm/v1/attention/backends/triton_attn.py
2	2	vllm/v1/spec_decode/eagle.py

[e202dd273] Woosuk Kwon 2025-07-06 [V0 deprecation] Remove V0 CPU/XPU/TPU backends (#20412)
4	4	.buildkite/scripts/hardware_ci/run-cpu-test.sh
0	2	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	1	examples/online_serving/chart-helm/values.yaml
10	7	tests/kernels/attention/test_attention_selector.py
0	307	vllm/attention/backends/cpu_mla.py
0	403	vllm/attention/backends/ipex_attn.py
0	356	vllm/attention/backends/pallas.py
3	164	vllm/attention/backends/torch_sdpa.py
6	20	vllm/platforms/cpu.py
16	35	vllm/platforms/tpu.py
6	15	vllm/platforms/xpu.py
0	326	vllm/worker/cpu_enc_dec_model_runner.py
0	671	vllm/worker/cpu_model_runner.py
0	125	vllm/worker/cpu_pooling_model_runner.py
0	452	vllm/worker/cpu_worker.py
0	108	vllm/worker/multi_step_tpu_worker.py
0	909	vllm/worker/tpu_model_runner.py
0	337	vllm/worker/tpu_worker.py
0	606	vllm/worker/xpu_model_runner.py
0	186	vllm/worker/xpu_worker.py

[43813e636] Reid 2025-07-06 [Misc] call the pre-defined func (#20518)
2	2	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
2	1	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[cede942b8] Brayden Zhong 2025-07-06 [Benchmark] Add support for multiple batch size benchmark through CLI in `benchmark_moe.py` (#20516)
2	2	benchmarks/kernels/benchmark_moe.py
147	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_B200,dtype=fp8_w8a8.json

[fe1e92481] Flora Feng 2025-07-05 [Frontend] Support image object in llm.chat (#19635)
43	0	docs/features/multimodal_inputs.md
5	2	examples/offline_inference/mistral-small.py
4	8	tests/entrypoints/test_chat_utils.py
45	3	vllm/entrypoints/chat_utils.py

[4548c03c5] Chengji Yao 2025-07-05 [TPU][Bugfix] fix the MoE OOM issue (#20339)
7	2	vllm/model_executor/layers/fused_moe/layer.py

[40b86aa05] Lucas Wilkinson 2025-07-06 [BugFix] Fix: ImportError when building on hopper systems (#20513)
1	1	.github/CODEOWNERS
0	5	csrc/ops.h
8	1	csrc/quantization/cutlass_w8a8/moe/blockwise_scaled_group_mm_sm100.cu
1	2	csrc/torch_bindings.cpp

[432870829] Lucia Fang 2025-07-06 [Bugfix] Fix missing per_act_token parameter in compressed_tensors_moe (#20509)
4	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[f73d02aad] Vadim Gimpelson 2025-07-06 [BUG] Fix #20484. Support empty sequence in cuda penalty kernel (#20491)
2	0	csrc/sampler.cu
48	0	tests/kernels/test_apply_repetition_penalties.py

[c5ebe040a] Jeremy Reizenstein 2025-07-06 test_attention compat with coming xformers change (#20487)
2	1	tests/kernels/attention/test_attention.py

[8d763cb89] Reid 2025-07-06 [Misc] remove unused import (#20517)
1	1	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[cf4cd5398] Reid 2025-07-05 [Misc] Add logger.exception for TPU information collection failures (#20510)
4	1	vllm/usage/usage_lib.py

[32c9be220] Isotr0py 2025-07-05 [v1] Re-add fp32 support to v1 engine through FlexAttention (#19754)
1	1	.github/workflows/lint-and-deploy.yaml
28	0	tests/kernels/attention/test_attention_selector.py
5	0	tests/v1/worker/test_gpu_model_runner.py
0	7	vllm/engine/arg_utils.py
6	2	vllm/model_executor/model_loader/tensorizer_loader.py
4	0	vllm/platforms/cuda.py
11	1	vllm/v1/attention/backends/flex_attention.py
4	1	vllm/v1/sample/ops/topk_topp_sampler.py

[8aeaa910a] Lucia Fang 2025-07-05 Fix unknown attribute of topk_indices_dtype in CompressedTensorsW8A8Fp8MoECutlassMethod (#20507)
2	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[906e05d84] Jee Jee Li 2025-07-05 [Misc] Remove the unused LoRA test code (#20494)
0	17	tests/lora/conftest.py
1	1	vllm/multimodal/utils.py

[ef9a2990a] Reid 2025-07-05 [doc] small fix (#20506)
1	1	docs/contributing/incremental_build.md

[7e9087049] Reid 2025-07-05 [Misc] Add security warning for development mode endpoints (#20508)
2	0	vllm/entrypoints/openai/api_server.py

[d3f05c924] Guy Stone 2025-07-05 [Doc] fix mutltimodal_inputs.md gh examples link (#20497)
1	1	docs/features/multimodal_inputs.md

[c108781c8] Michael Goin 2025-07-05 [CI Bugfix] Fix pre-commit failures on main (#20502)
1	0	tests/kernels/moe/test_cutlass_grouped_gemm.py
3	2	vllm/multimodal/utils.py

[3d184b95b] Duncan Moss 2025-07-04 [feat]: CUTLASS block scaled group gemm for SM100 (#19757)
21	1	CMakeLists.txt
0	1	csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp
5	0	csrc/ops.h
2	1	csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh
367	0	csrc/quantization/cutlass_w8a8/moe/blockwise_scaled_group_mm_sm100.cu
0	1	csrc/quantization/machete/machete_mainloop.cuh
2	1	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh
9	0	csrc/torch_bindings.cpp
115	0	tests/kernels/moe/test_cutlass_grouped_gemm.py
14	0	vllm/_custom_ops.py
133	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py
39	23	vllm/model_executor/layers/fused_moe/fused_moe.py
19	1	vllm/model_executor/layers/quantization/fp8.py

[2f35a022e] Thomas Parnell 2025-07-04 Enable V1 for Hybrid SSM/Attention Models (#20016)
60	10	tests/models/language/generation/test_hybrid.py
1	1	tests/models/registry.py
0	1	tests/v1/test_oracle.py
1	1	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
30	15	vllm/model_executor/models/bamba.py
40	17	vllm/model_executor/models/falcon_h1.py
32	17	vllm/model_executor/models/granitemoehybrid.py
30	15	vllm/model_executor/models/nemotron_h.py
64	37	vllm/model_executor/models/zamba2.py
8	3	vllm/v1/core/kv_cache_coordinator.py
10	6	vllm/v1/core/kv_cache_manager.py
4	2	vllm/v1/core/kv_cache_utils.py
6	1	vllm/v1/kv_cache_interface.py
115	10	vllm/v1/worker/gpu_model_runner.py

[ffe00ef77] Chenheli Hua 2025-07-04 [Misc] Small: Remove global media connector. Each test should have its own test connector object. (#20395)
57	9	vllm/multimodal/utils.py

[5561681d0] Peter Pan 2025-07-04 [CI] add kvcache-connector dependency definition and add into CI build (#18193)
1	1	.buildkite/release-pipeline.yaml
12	0	docker/Dockerfile
1	0	requirements/kv_connectors.txt

[fbd62d875] Cyrus Leung 2025-07-04 [Doc] Fix classification table in list of supported models (#20489)
1	0	docs/models/supported_models.md

[2e26f9156] wang.yuqi 2025-07-04 [Model][3/N] Automatic conversion of CrossEncoding model (#20168)
15	6	docs/models/supported_models.md
9	1	tests/models/language/pooling/test_embedding.py
12	4	tests/models/language/pooling/test_gte.py
84	0	tests/models/language/pooling/test_mxbai_rerank.py
10	3	vllm/config.py
99	3	vllm/model_executor/models/adapters.py
1	1	vllm/model_executor/models/config.py
4	115	vllm/model_executor/models/qwen3.py

[9e5452ee3] sangbumlikeagod 2025-07-04 [Bug][Frontend] Fix structure of transcription's decoder_prompt (#18809)
28	11	tests/entrypoints/openai/test_transcription_validation.py
3	2	vllm/model_executor/models/whisper.py

[0e3fe896e] Michael Goin 2025-07-04 Support Llama 4 for fused_marlin_moe (#20457)
4	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	5	vllm/model_executor/layers/quantization/awq_marlin.py
3	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	2	vllm/model_executor/layers/quantization/fp8.py
1	4	vllm/model_executor/layers/quantization/gptq_marlin.py
1	0	vllm/model_executor/layers/quantization/modelopt.py

[1caca5a58] Jee Jee Li 2025-07-04 [Misc] Add SPDX-FileCopyrightText (#20428)
1	0	benchmarks/kernels/bench_fp8_gemm.py
1	0	examples/offline_inference/spec_decode.py
1	0	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
1	0	examples/online_serving/multi_instance_data_parallel.py
1	0	examples/online_serving/openai_chat_completion_client_with_tools_xlam.py
1	0	examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming.py
1	0	tests/compile/test_fusion_attn.py
1	0	tests/kernels/moe/parallel_utils.py
1	0	tests/kernels/moe/test_deepep_deepgemm_moe.py
1	0	tests/kernels/moe/test_deepep_moe.py
1	0	tests/kernels/moe/test_deepgemm.py
1	0	tests/kernels/test_apply_repetition_penalties.py
1	0	tests/kernels/test_flex_attention.py
1	0	tests/models/language/pooling/test_intfloat.py
1	0	tests/quantization/test_rtn.py
1	0	tests/tool_use/test_minimax_tool_parser.py
1	0	tests/tool_use/test_xlam_tool_parser.py
1	0	tests/v1/sample/test_logits_processors.py
1	0	tests/v1/test_request.py
1	0	tests/v1/tpu/test_spmd_model_weight_loading.py
1	0	tests/v1/tpu/test_tpu_qkv_linear.py
1	0	tools/check_pickle_imports.py
121	26	tools/check_spdx_header.py
1	0	vllm/compilation/fusion_attn.py
1	0	vllm/distributed/eplb/__init__.py
1	0	vllm/distributed/eplb/eplb_state.py
1	0	vllm/distributed/eplb/rebalance_algo.py
1	0	vllm/distributed/eplb/rebalance_execute.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool.py
1	0	vllm/distributed/tpu_distributed_utils.py
1	0	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py
1	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
1	0	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
1	0	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
1	0	vllm/model_executor/layers/quantization/deepgemm.py
1	0	vllm/model_executor/layers/quantization/rtn.py
1	0	vllm/model_executor/model_loader/tpu.py
2	2	vllm/model_executor/models/aya_vision.py
1	0	vllm/model_executor/models/dots1.py
1	0	vllm/model_executor/models/glm4_1v.py
1	0	vllm/model_executor/models/tarsier.py
1	0	vllm/transformers_utils/configs/nemotron_h.py
1	0	vllm/v1/attention/backends/cpu_attn.py
1	0	vllm/v1/attention/backends/flex_attention.py
1	0	vllm/v1/attention/backends/mla/cutlass_mla.py
1	0	vllm/v1/attention/backends/rocm_aiter_fa.py
1	0	vllm/v1/core/kv_cache_coordinator.py
1	0	vllm/v1/pool/metadata.py
1	0	vllm/v1/sample/logits_processor.py
1	0	vllm/v1/worker/cpu_model_runner.py
1	0	vllm/v1/worker/cpu_worker.py
1	0	vllm/v1/worker/xpu_model_runner.py
1	0	vllm/v1/worker/xpu_worker.py

[783921d88] Wentao Ye 2025-07-04 [Perf] Optimize Vectorization Utils for Int 8 Quantization Kernels (#20331)
9	7	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
97	0	csrc/quantization/vectorization_utils.cuh

[4a98edff1] Aaron Pham 2025-07-04 [Structured Outputs][V1] Skipping with models doesn't contain tokenizers (#20365)
48	9	tests/v1/core/test_scheduler.py
56	6	tests/v1/engine/test_llm_engine.py
5	0	vllm/v1/engine/processor.py
19	16	vllm/v1/structured_output/__init__.py

[a7bab0c9e] Reid 2025-07-04 [Misc] small update (#20462)
4	1	examples/offline_inference/profiling_tpu/README.md
7	3	examples/online_serving/structured_outputs/README.md
7	5	examples/others/tensorize_vllm_model.py

[25950dca9] 汪志鹏 2025-07-04 Add ignore consolidated file in mistral example code (#20420)
1	0	examples/offline_inference/vision_language.py
1	0	examples/offline_inference/vision_language_multi_image.py

[a4113b035] Gabriel Marinho 2025-07-03 [Platform] Add custom default max tokens (#18557)
10	49	vllm/entrypoints/openai/protocol.py
13	5	vllm/entrypoints/openai/serving_chat.py
12	4	vllm/entrypoints/openai/serving_completion.py
20	2	vllm/entrypoints/utils.py
4	0	vllm/platforms/interface.py

[7e1665b08] Michael Goin 2025-07-04 [Misc] Change warn_for_unimplemented_methods to debug (#20455)
2	2	vllm/utils/__init__.py

[8d1096e7d] Seiji Eicher 2025-07-03 [Bugfix] Register reducer even if transformers_modules not available (#19510)
57	0	tests/config/test_mp_reducer.py
16	15	vllm/transformers_utils/config.py

[8d775dd30] Nicolò Lucchesi 2025-07-03 [Misc] Fix `Unable to detect current VLLM config. Defaulting to NHD kv cache layout` warning (#20400)
2	2	vllm/distributed/kv_transfer/kv_connector/utils.py
1	1	vllm/v1/attention/backends/utils.py

[78fe77534] bnellnm 2025-07-03 [Kernel] Enable fp8 support for pplx and BatchedTritonExperts. (#18864)
3	7	tests/kernels/moe/parallel_utils.py
54	30	tests/kernels/moe/test_batched_moe.py
1	2	tests/kernels/moe/test_deepep_deepgemm_moe.py
3	2	tests/kernels/moe/test_deepep_moe.py
44	35	tests/kernels/moe/test_pplx_cutlass_moe.py
488	244	tests/kernels/moe/test_pplx_moe.py
6	8	tests/kernels/moe/utils.py
18	0	tests/kernels/quant_utils.py
6	4	tests/kernels/utils.py
6	8	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
11	23	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
55	9	vllm/model_executor/layers/fused_moe/config.py
12	6	vllm/model_executor/layers/fused_moe/cutlass_moe.py
6	17	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
7	8	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
391	192	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	0	vllm/model_executor/layers/fused_moe/fused_moe.py
12	22	vllm/model_executor/layers/fused_moe/layer.py
4	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
61	34	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
3	0	vllm/model_executor/layers/fused_moe/prepare_finalize.py
34	3	vllm/model_executor/layers/fused_moe/utils.py
46	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	4	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/models/qwen3_moe.py

[2f2fcb31b] Yuxuan Zhang 2025-07-04 [Misc] Remove _maybe_ignore_quant_config from GLM4.1v (#20432)
9	18	vllm/model_executor/models/glm4_1v.py

[1dba2c4eb] Ning Xie 2025-07-04 [Misc] adjust for ipv6 for mookcacke url parse (#20107)
46	4	tests/test_utils.py
29	19	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
24	4	vllm/utils/__init__.py

[71d6de3a2] Isotr0py 2025-07-04 [Misc] Clean up InternVL family config registration (#19992)
22	6	vllm/transformers_utils/config.py
0	4	vllm/transformers_utils/configs/__init__.py
0	16	vllm/transformers_utils/configs/h2ovl.py
0	54	vllm/transformers_utils/configs/internvl.py
18	2	vllm/transformers_utils/configs/nvlm_d.py

[536fd3300] Alexei-V-Ivanov-AMD 2025-07-03 [CI] Trimming some failing test groups from AMDPRODUCTION. (#20390)
6	6	.buildkite/test-pipeline.yaml

[619b9f5c7] Reid 2025-07-03 [Frontend] fix duplicate output for bench subcmd (#20446)
1	0	vllm/entrypoints/cli/benchmark/main.py

[d1b689c44] Nicolò Lucchesi 2025-07-03 [Bugfix] Fix flaky `test_streaming_response` test (#20363)
1	1	requirements/common.txt

[9854dc904] Reid 2025-07-03 [Frontend] improve vllm bench <bench_type> --help display (#20430)
5	0	vllm/entrypoints/cli/benchmark/main.py
1	1	vllm/entrypoints/cli/run_batch.py
1	1	vllm/entrypoints/cli/serve.py
11	4	vllm/entrypoints/utils.py

[ff5c60fad] Isotr0py 2025-07-03 [Misc] Automatically tag PRs to add new models (#20222)
13	2	.github/mergify.yml

[6f1229f91] wang.yuqi 2025-07-03 [Model][2/N] Automatic conversion of CrossEncoding model (#19978)
1	1	docs/models/supported_models.md
1	1	docs/serving/openai_compatible_server.md
14	11	tests/entrypoints/openai/correctness/test_mteb_score.py
31	25	tests/models/language/pooling/mteb_utils.py
4	4	tests/models/test_registry.py
27	1	tests/test_config.py
22	12	vllm/config.py
7	3	vllm/entrypoints/llm.py
11	8	vllm/entrypoints/openai/api_server.py
5	1	vllm/entrypoints/openai/run_batch.py
17	6	vllm/model_executor/layers/pooler.py
4	3	vllm/model_executor/model_loader/utils.py
44	13	vllm/model_executor/models/adapters.py
4	0	vllm/model_executor/models/qwen2.py
3	3	vllm/model_executor/models/registry.py
5	1	vllm/outputs.py

[1819fbda6] Jee Jee Li 2025-07-03 [Quantization] Bump to use latest bitsandbytes (#20424)
1	1	docker/Dockerfile
1	1	docs/features/quantization/bnb.md
1	1	requirements/nightly_torch_test.txt
1	1	requirements/test.in
1	1	requirements/test.txt
1	1	vllm/config.py
4	4	vllm/model_executor/layers/quantization/bitsandbytes.py
4	4	vllm/model_executor/model_loader/bitsandbytes_loader.py

[7f0367109] Li, Jiang 2025-07-03 [CI/Build][CPU] Enable cross compilation in CPU release pipeline (#20423)
1	1	.buildkite/release-pipeline.yaml
17	7	cmake/cpu_extension.cmake
9	1	docker/Dockerfile.cpu

[fb14d53cf] Ning Xie 2025-07-03 [Kernel] refactor cpu worker v0 cache dtype (#20080)
20	18	vllm/worker/cpu_worker.py

[b024a42e9] Cyrus Leung 2025-07-03 [Core] Move multimodal placeholder from chat utils to model definition (#20355)
16	0	docs/contributing/model/multimodal.md
0	1	tests/async_engine/test_async_llm_engine.py
0	20	tests/engine/test_arg_utils.py
0	1	tests/entrypoints/openai/test_serving_chat.py
1	7	vllm/config.py
0	6	vllm/engine/arg_utils.py
9	85	vllm/entrypoints/chat_utils.py
12	6	vllm/entrypoints/openai/speech_to_text.py
2	1	vllm/model_executor/model_loader/__init__.py
5	3	vllm/model_executor/model_loader/tensorizer.py
4	0	vllm/model_executor/model_loader/utils.py
5	3	vllm/model_executor/models/__init__.py
7	0	vllm/model_executor/models/aria.py
7	0	vllm/model_executor/models/aya_vision.py
7	0	vllm/model_executor/models/blip2.py
7	0	vllm/model_executor/models/chameleon.py
7	0	vllm/model_executor/models/deepseek_vl2.py
7	0	vllm/model_executor/models/florence2.py
7	0	vllm/model_executor/models/fuyu.py
7	0	vllm/model_executor/models/gemma3_mm.py
9	0	vllm/model_executor/models/glm4_1v.py
7	0	vllm/model_executor/models/glm4v.py
7	0	vllm/model_executor/models/granite_speech.py
7	0	vllm/model_executor/models/idefics3.py
7	0	vllm/model_executor/models/interfaces.py
9	0	vllm/model_executor/models/internvl.py
9	0	vllm/model_executor/models/keye.py
7	0	vllm/model_executor/models/kimi_vl.py
7	0	vllm/model_executor/models/llava.py
7	0	vllm/model_executor/models/llava_next.py
9	0	vllm/model_executor/models/llava_next_video.py
9	0	vllm/model_executor/models/llava_onevision.py
11	0	vllm/model_executor/models/minicpmo.py
9	0	vllm/model_executor/models/minicpmv.py
7	0	vllm/model_executor/models/minimax_vl_01.py
7	0	vllm/model_executor/models/mistral3.py
7	0	vllm/model_executor/models/mllama.py
7	0	vllm/model_executor/models/mllama4.py
7	0	vllm/model_executor/models/molmo.py
7	0	vllm/model_executor/models/ovis.py
7	0	vllm/model_executor/models/paligemma.py
7	0	vllm/model_executor/models/phi3v.py
9	0	vllm/model_executor/models/phi4mm.py
7	0	vllm/model_executor/models/pixtral.py
7	0	vllm/model_executor/models/prithvi_geospatial_mae.py
11	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
9	0	vllm/model_executor/models/qwen2_5_vl.py
7	0	vllm/model_executor/models/qwen2_audio.py
9	0	vllm/model_executor/models/qwen2_vl.py
7	0	vllm/model_executor/models/qwen_vl.py
7	0	vllm/model_executor/models/skyworkr1v.py
7	0	vllm/model_executor/models/tarsier.py
7	0	vllm/model_executor/models/ultravox.py
29	22	vllm/model_executor/models/whisper.py

[cb97f2bfc] Michael Yao 2025-07-03 [Docs] Replace two list with tables in intel_gaudi.md (#20414)
24	19	docs/getting_started/installation/intel_gaudi.md

[359200f6a] Reid 2025-07-03 [doc] fix link (#20417)
1	1	examples/offline_inference/profiling_tpu/README.md

[220aee902] Lifans 2025-07-02 [Misc] Add rules to label Speculative Decoding Related PRs (#20406)
6	0	.github/mergify.yml

[67d25eca0] Nick Hill 2025-07-03 [Tests] Update online DP tests to verify that requests are balanced (#20157)
1	1	tests/v1/entrypoints/openai/test_completion.py
126	0	tests/v1/entrypoints/openai/test_multi_api_servers.py
43	8	tests/v1/test_async_llm_dp.py

[363528de2] qscqesze 2025-07-03 [Feature] Support MiniMax-M1 function calls features (#20297)
9	0	docs/features/tool_calling.md
91	0	examples/tool_chat_template_minimax_m1.jinja
371	0	tests/tool_use/test_minimax_tool_parser.py
2	1	vllm/entrypoints/openai/tool_parsers/__init__.py
369	0	vllm/entrypoints/openai/tool_parsers/minimax_tool_parser.py

[4ff61abab] QiliangCui 2025-07-02 [TPU] Add a case to cover RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w8a8 (#20385)
14	0	.buildkite/scripts/tpu/quantized_v6e_1.env

[0ec3779df] Li, Jiang 2025-07-03 [Bugfix][CI/CD][CPU] Fix CPU CI tests (#20383)
7	6	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[b616f6a53] Chenheli Hua 2025-07-02 [Misc] Small: Fix video loader return type annotations. (#20389)
4	3	tests/multimodal/test_utils.py
2	2	vllm/multimodal/utils.py
8	6	vllm/multimodal/video.py

[2e25bb12a] bnellnm 2025-07-02 [Bugfix] Fix import of CutlassExpertsFp8 in compressed_tensors_moe.py (#20381)
5	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
6	4	vllm/model_executor/layers/quantization/fp8.py

[9965c47d0] Louie Tsai 2025-07-02 Enable CPU nightly performance benchmark and its Markdown report (#18444)
40	2	.buildkite/nightly-benchmarks/README.md
12	4	.buildkite/nightly-benchmarks/performance-benchmarks-descriptions.md
66	0	.buildkite/nightly-benchmarks/scripts/compare-json-results.py
53	10	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
101	27	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
30	0	.buildkite/nightly-benchmarks/tests/latency-tests-cpu.json
158	0	.buildkite/nightly-benchmarks/tests/serving-tests-cpu.json
32	0	.buildkite/nightly-benchmarks/tests/throughput-tests-cpu.json
2	1	docker/Dockerfile.cpu

[059d4cdb4] Nick Hill 2025-07-03 [BugFix] Fix DP headless mode arg validation (#20398)
4	4	vllm/entrypoints/cli/serve.py

[bdb84e26b] Tyler Michael Smith 2025-07-02 [Bugfix] Fixes for FlashInfer's TORCH_CUDA_ARCH_LIST (#20136)
38	17	docker/Dockerfile

[3dd359147] Nicolò Lucchesi 2025-07-03 [Docs] Update EAGLE example (#20375)
1	0	docs/features/spec_decode.md

[657f2f301] Nick Hill 2025-07-02 [DP] Support external DP Load Balancer mode (#19790)
5	1	.buildkite/test-pipeline.yaml
3	3	tests/v1/engine/test_engine_core_client.py
312	0	tests/v1/test_external_lb_dp.py
13	0	vllm/config.py
21	7	vllm/engine/arg_utils.py
59	118	vllm/entrypoints/cli/serve.py
35	13	vllm/v1/engine/coordinator.py
113	36	vllm/v1/engine/core.py
126	222	vllm/v1/engine/core_client.py
546	0	vllm/v1/engine/utils.py
17	383	vllm/v1/utils.py

[a1aafc827] vllmellm 2025-07-03 [ROCm][FEAT] Enable Full Graph Mode in AITER MLA V1 Attn Backend (Decode Phase only) (#20254)
59	31	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[139508a41] rongfu.leng 2025-07-03 [Misc] add handler HF_TOKEN is emptry string (#20369)
22	6	vllm/transformers_utils/config.py

[d265414db] Nick Hill 2025-07-02 [Minor] Clean up incorrect comment in test (#20382)
0	3	tests/engine/test_options.py

[48fb076cb] afeldman-nm 2025-07-02 [V1] LogitsProcessor programming model (#16728)
626	0	tests/v1/sample/test_logits_processors.py
3	2	tests/v1/sample/test_logprobs_e2e.py
2	3	tests/v1/sample/test_rejection_sampler.py
5	149	tests/v1/sample/test_sampler.py
80	1	tests/v1/sample/utils.py
29	26	tests/v1/worker/test_gpu_input_batch.py
516	0	vllm/v1/sample/logits_processor.py
5	6	vllm/v1/sample/metadata.py
0	16	vllm/v1/sample/ops/penalties.py
11	71	vllm/v1/sample/sampler.py
12	17	vllm/v1/spec_decode/utils.py
95	63	vllm/v1/worker/gpu_input_batch.py
19	41	vllm/v1/worker/gpu_model_runner.py

[c1909e7e8] bnellnm 2025-07-02 [Kernels] MoE refactor (#19636)
8	3	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
190	0	tests/kernels/moe/parallel_utils.py
218	35	tests/kernels/moe/test_batched_moe.py
296	0	tests/kernels/moe/test_block_fp8.py
147	0	tests/kernels/moe/test_block_int8.py
13	9	tests/kernels/moe/test_cutlass_moe.py
28	74	tests/kernels/moe/test_deepep_deepgemm_moe.py
69	37	tests/kernels/moe/test_deepep_moe.py
13	1	tests/kernels/moe/test_moe.py
1	1	tests/kernels/moe/test_nvfp4_moe.py
2	4	tests/kernels/moe/test_pplx_cutlass_moe.py
96	49	tests/kernels/moe/test_pplx_moe.py
242	187	tests/kernels/moe/utils.py
166	8	tests/kernels/quant_utils.py
4	367	tests/kernels/quantization/test_block_fp8.py
1	135	tests/kernels/quantization/test_block_int8.py
64	16	tests/kernels/utils.py
1	1	vllm/_custom_ops.py
25	1	vllm/model_executor/layers/fused_moe/__init__.py
26	8	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
44	21	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
410	0	vllm/model_executor/layers/fused_moe/config.py
47	26	vllm/model_executor/layers/fused_moe/cutlass_moe.py
22	7	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
36	35	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
34	37	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
94	39	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
44	53	vllm/model_executor/layers/fused_moe/fused_moe.py
77	311	vllm/model_executor/layers/fused_moe/layer.py
84	3	vllm/model_executor/layers/fused_moe/modular_kernel.py
77	30	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
9	15	vllm/model_executor/layers/fused_moe/prepare_finalize.py
46	22	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
22	7	vllm/model_executor/layers/fused_moe/utils.py
25	30	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
32	27	vllm/model_executor/layers/quantization/fp8.py

[b95877509] cronoik-inceptionai 2025-07-02 Documentation update tool_calling: mapping back to function from response (#20373)
1	1	docs/features/tool_calling.md

[706ff1322] zichongli5 2025-07-02 [Model] Adds support for SlimMoE models Phi-tiny-MoE-instruct (#20286)
10	1	vllm/model_executor/models/phimoe.py

[ccbfb1d1c] WangHuaqiang 2025-07-02 [Bugfix] Fix the max_seq_len limit of 16384 for DeepSeek models (#20322)
2	0	tests/test_config.py
11	4	vllm/config.py

[9e5552aa1] Joonchen Liau 2025-07-02 [NVIDIA] Support Cutlass w8a8 FP8 for Blackwell Geforce GPUs (sm120) (#17280)
30	0	CMakeLists.txt
61	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh
6	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp
24	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm120_fp8.cu
67	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm120_fp8_dispatch.cuh
34	0	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm120.cu
16	1	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[0c600b9ab] Lu Fang 2025-07-02 [Build/CI] Automatically tag DeepSeek related PRs (#20370)
16	0	.github/mergify.yml

[e303dcf52] CSWYF3634076 2025-07-02 [Model] Add Ernie4.5 and Ernie4.5MoE Model Support (#20220)
2	0	docs/models/supported_models.md
4	0	tests/models/registry.py
43	0	vllm/model_executor/models/ernie45.py
583	0	vllm/model_executor/models/ernie45_moe.py
2	0	vllm/model_executor/models/registry.py

[ae9c4d416] Michael Yao 2025-07-02 [Docs] Make TPU ref prettier in google_tpu.md (#20356)
3	3	docs/getting_started/installation/google_tpu.md

[d853520b3] Michael Yao 2025-07-02 [Docs] Fix indentations for 2-level items in deprecation_policy.md (#20352)
8	8	docs/contributing/deprecation_policy.md

[ba51aea65] Cyrus Leung 2025-07-02 [Bugfix] Keye-VL compatibility with `tok_kwargs` (#20058) (#20353)
3	1	vllm/model_executor/models/keye.py

[8452946c0] Kwai-Keye 2025-07-02 [Model][VLM] Support Keye-VL-8B-Preview (#20126)
1	0	docs/models/supported_models.md
32	0	examples/offline_inference/vision_language.py
38	0	examples/offline_inference/vision_language_multi_image.py
2	0	tests/models/registry.py
2	2	vllm/entrypoints/chat_utils.py
1725	0	vllm/model_executor/models/keye.py
1	0	vllm/model_executor/models/registry.py

[2e7cbf2d7] Chenheli Hua 2025-07-01 [Frontend] Support configurable mm placeholder strings & flexible video sampling policies via CLI flags. (#20105)
4	2	tests/async_engine/test_async_llm_engine.py
52	0	tests/engine/test_arg_utils.py
4	2	tests/entrypoints/openai/test_serving_chat.py
6	6	tests/multimodal/test_utils.py
47	1	tests/multimodal/test_video.py
16	0	vllm/config.py
12	0	vllm/engine/arg_utils.py
8	3	vllm/entrypoints/chat_utils.py
10	0	vllm/multimodal/audio.py
7	1	vllm/multimodal/image.py
17	10	vllm/multimodal/utils.py
16	4	vllm/multimodal/video.py

[7da296be0] Chengji Yao 2025-07-01 [TPU] kv cache update kernel supports dynamic grid (#20235)
6	2	tests/v1/tpu/test_kv_cache_update_kernel.py
6	3	vllm/attention/ops/pallas_kv_cache_update.py
22	12	vllm/v1/attention/backends/pallas.py
8	0	vllm/v1/worker/tpu_model_runner.py

[b205e8467] QiliangCui 2025-07-01 [Doc][TPU] Add models and features supporting matrix. (#20230)
1	0	docs/.nav.yml
17	17	docs/features/compatibility_matrix.md
36	0	docs/models/hardware_supported_models/tpu.md

[be0cfb2b6] yyzxw 2025-07-02 fix[Docs]: link anchor is incorrect #20309 (#20315)
1	1	docs/configuration/engine_args.md
1	1	docs/design/arch_overview.md
1	1	docs/features/structured_outputs.md
1	1	docs/getting_started/installation/intel_gaudi.md
1	1	docs/models/generative_models.md
1	1	docs/models/pooling_models.md
3	3	docs/models/supported_models.md
1	1	docs/serving/openai_compatible_server.md

[1a03dd496] Cyrus Leung 2025-07-02 [Bugfix] Fix dynamic rotary embedding (#20343)
7	4	vllm/model_executor/layers/rotary_embedding.py

[27b801763] Kunshang Ji 2025-07-02 [FIX][Intel GPU]fix ipex flash_attn_varlen_func api missing parameter (#20348)
1	0	vllm/_ipex_ops.py

[9ec1e3065] Lifans 2025-07-01 [Misc][Doc] Add missing comment for LLM (#20285)
20	12	vllm/entrypoints/llm.py

[9dae7d46b] Wentao Ye 2025-07-01 [Refactor] Remove Unused Env `VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON` (#20334)
0	7	vllm/envs.py
0	1	vllm/model_executor/layers/fused_moe/moe_align_block_size.py

[7058d7dd5] Wentao Ye 2025-07-01 [Refactor] Remove duplicate `find_free_port` (#20333)
2	2	tests/kernels/moe/utils.py
0	9	vllm/model_executor/layers/fused_moe/utils.py

[a0389e055] Liangliang Ma 2025-07-02 [UT][intel GPU] use current_platform instead of device hardcode in v1 tests (#20169)
2	2	tests/conftest.py
5	4	tests/v1/sample/test_rejection_sampler.py
6	5	tests/v1/sample/test_topk_topp_sampler.py
13	10	tests/v1/spec_decode/test_eagle.py
3	1	tests/v1/worker/test_gpu_input_batch.py
2	1	tests/v1/worker/test_gpu_model_runner.py
5	1	vllm/platforms/cuda.py
5	0	vllm/platforms/rocm.py
1	1	vllm/platforms/xpu.py
2	1	vllm/v1/attention/backends/mla/common.py

[3be8d312a] Tyler Michael Smith 2025-07-01 [Kernel][Bugfix] Fixup some warnings in nvfp4_blockwise_moe when CUDA < 12.8 (#20324)
2	0	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu

[3abfe2215] czhu-cohere 2025-07-01 Enable group size 64 for Machete (#20290)
4	4	tests/kernels/quantization/test_machete_mm.py
3	3	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
18	1	vllm/model_executor/layers/quantization/utils/machete_utils.py

[e81fbefe8] Wentao Ye 2025-07-01 [Refactor] Refactor import utils (#20269)
1	1	pyproject.toml
1	1	tests/build_cython.py
1	2	tools/check_pickle_imports.py
7	7	vllm/{utils.py => utils/__init__.py}

[9290de566] 周周周 2025-07-02 remove unused variables in marlin_template.h (#20236)
0	2	csrc/moe/marlin_moe_wna16/marlin_template.h
0	2	csrc/quantization/gptq_marlin/marlin_template.h

[7f280d69c] Woosuk Kwon 2025-07-01 [Optimization] Cache sampled token ids in model runner (#20291)
6	6	tests/v1/worker/test_gpu_model_runner.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	0	vllm/v1/core/sched/output.py
13	5	vllm/v1/core/sched/scheduler.py
68	32	vllm/v1/worker/gpu_model_runner.py

[02cabff20] TJian 2025-07-01 [V1] [ROCm] Enable EP with AITER Fused MoE (#20270)
1	1	vllm/model_executor/layers/fused_moe/layer.py
9	2	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
3	1	vllm/model_executor/layers/quantization/fp8.py

[3d19d47d9] Shintarou Okada 2025-07-02 [Frontend] Expand tools even if tool_choice="none" (#17177)
8	0	docs/features/tool_calling.md
2	0	vllm/entrypoints/openai/api_server.py
11	0	vllm/entrypoints/openai/cli_args.py
1	3	vllm/entrypoints/openai/protocol.py
21	3	vllm/entrypoints/openai/serving_chat.py

[8acb4bade] Woosuk Kwon 2025-07-01 [CUDA graphs] Enable full cuda graphs with FA3 AoT scheduling (#20301)
1	1	cmake/external_projects/vllm_flash_attn.cmake
53	6	vllm/v1/attention/backends/flash_attn.py

[314af8617] Nicolò Lucchesi 2025-07-01 [Docs] Update transcriptions API to use openai client with `stream=True`  (#20271)
28	33	examples/online_serving/openai_transcription_client.py
3	4	vllm/entrypoints/openai/protocol.py

[0e96cc9b7] Woosuk Kwon 2025-07-01 [Misc] Minor refactoring for scheduler (#20299)
36	15	vllm/v1/core/sched/scheduler.py

[ecad851cb] aiyiwang2025 2025-07-01 [Model]Add Tencent HunYuanMoEV1 Model Support (#20114)
2	1	docs/models/supported_models.md
3	1	tests/models/registry.py
2	1	tests/models/test_initialization.py
44	3	vllm/model_executor/layers/rotary_embedding.py
897	0	vllm/model_executor/models/hunyuan_v1_moe.py
1	0	vllm/model_executor/models/registry.py

[ed70f3c64] Yuxuan Zhang 2025-07-01 Add GLM4.1V model (Draft) (#19331)
2	1	docs/models/supported_models.md
39	1	examples/offline_inference/vision_language.py
1	1	tests/entrypoints/openai/test_video.py
28	0	tests/models/multimodal/generation/test_common.py
20	0	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
24	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
24	0	tests/models/multimodal/processing/test_common.py
1	0	tests/models/registry.py
3	1	tests/multimodal/test_utils.py
25	1	vllm/assets/video.py
4	0	vllm/entrypoints/chat_utils.py
119	0	vllm/model_executor/layers/rotary_embedding.py
1589	0	vllm/model_executor/models/glm4_1v.py
1	0	vllm/model_executor/models/registry.py
5	3	vllm/multimodal/inputs.py
40	2	vllm/multimodal/parse.py
21	6	vllm/multimodal/video.py

[650d5dbd0] Nicolò Lucchesi 2025-07-01 [Misc] Minor refactor of NIXL background handshake (#20068)
32	28	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[9025a9a70] Kyle Sayers 2025-07-01 [Quant] [Bugfix] Fix quantization config matching with `hf_to_vllm_mapper` (#20046)
1	0	tests/quantization/test_register_quantization_config.py
1	1	vllm/lora/models.py
1	4	vllm/lora/worker_manager.py
13	0	vllm/model_executor/layers/quantization/base_config.py
1	0	vllm/model_executor/layers/quantization/bitblas.py
16	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
9	1	vllm/model_executor/layers/quantization/fp8.py
1	0	vllm/model_executor/layers/quantization/gptq_bitblas.py
2	0	vllm/model_executor/layers/quantization/marlin.py
1	0	vllm/model_executor/layers/quantization/modelopt.py
1	0	vllm/model_executor/layers/quantization/torchao.py
13	9	vllm/model_executor/model_loader/utils.py
20	3	vllm/model_executor/models/interfaces.py
7	7	vllm/model_executor/models/qwen2_5_vl.py
1	0	vllm/model_executor/models/transformers.py
14	1	vllm/model_executor/models/utils.py
5	2	vllm/model_executor/utils.py

[c05596f1a] Lionel Villard 2025-07-01 [Perf] Validate @config in pre-commit instead of dynamically (#20200)
7	0	.pre-commit-config.yaml
1	34	tests/test_config.py
0	0	tests/tools/__init__.py
49	0	tests/tools/test_config_validator.py
158	0	tools/validate_config.py
5	23	vllm/config.py

[787b13389] Reid 2025-07-01 [doc] fix the incorrect logo in dark mode (#20289)
2	1	docs/README.md
9	0	docs/mkdocs/stylesheets/extra.css

[96453cfa8] TY-AMD 2025-07-01 [BugFix][V1][ROCm] Triton MLA uses V0 backend on V1 engine (#19067)
2	4	tests/kernels/attention/test_attention_selector.py
4	2	tests/kernels/attention/test_rocm_attention_selector.py
8	2	vllm/platforms/rocm.py
7	2	vllm/v1/attention/backends/mla/common.py
57	0	vllm/v1/attention/backends/mla/triton_mla.py

[b1c1fe35a] Kebe 2025-07-01 [Misc] remove redundant char (#20287)
1	1	benchmarks/benchmark_serving.py
1	1	vllm/benchmarks/serve.py

[08d81f101] Varun Sundar Rabindranath 2025-07-01 [Bugfix] Fix deepep tests (#20288)
1	1	tests/kernels/moe/test_deepep_deepgemm_moe.py
1	1	tests/kernels/moe/test_deepep_moe.py

[6cc1e7d96] Li, Jiang 2025-07-01 [CPU] Update custom ops for the CPU backend (#20255)
2	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh
20	0	cmake/cpu_extension.cmake
238	0	csrc/cpu/sgl-kernels/common.h
464	0	csrc/cpu/sgl-kernels/gemm.cpp
266	0	csrc/cpu/sgl-kernels/gemm.h
530	0	csrc/cpu/sgl-kernels/gemm_fp8.cpp
440	0	csrc/cpu/sgl-kernels/gemm_int8.cpp
1330	0	csrc/cpu/sgl-kernels/moe.cpp
502	0	csrc/cpu/sgl-kernels/moe_fp8.cpp
769	0	csrc/cpu/sgl-kernels/moe_int8.cpp
308	0	csrc/cpu/sgl-kernels/vec.h
94	84	csrc/cpu/shm.cpp
43	0	csrc/cpu/torch_bindings.cpp
1	0	docs/getting_started/installation/cpu.md
2	1	tests/models/language/generation/test_common.py
49	0	vllm/_custom_ops.py
5	0	vllm/envs.py
214	0	vllm/model_executor/layers/fused_moe/cpu_fused_moe.py
30	11	vllm/model_executor/layers/fused_moe/layer.py
24	1	vllm/model_executor/layers/linear.py
23	2	vllm/model_executor/layers/utils.py
1	1	vllm/model_executor/layers/vocab_parallel_embedding.py
2	0	vllm/platforms/cpu.py

[9909726d2] czhu-cohere 2025-07-01 Enable ZP Support for Machete (#20268)
2	0	benchmarks/kernels/benchmark_machete.py
1	1	tests/kernels/quantization/test_machete_mm.py
16	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py

[22e9d4204] Prashant Gupta 2025-07-01 [Misc] add xgrammar for arm64 (#18359)
1	1	requirements/common.txt

[86debab54] Richard Barnes 2025-07-01 Fix `numel()` downcast in vllm/csrc/moe/moe_align_sum_kernels.cu +2 (#17082)
1	1	csrc/moe/moe_align_sum_kernels.cu
1	1	csrc/moe/topk_softmax_kernels.cu

[be250bbc6] Michael Goin 2025-07-01 [V1] Only print cudagraph tqdm on rank 0 with `is_global_first_rank` (#19516)
31	0	vllm/distributed/parallel_state.py
7	4	vllm/v1/worker/gpu_model_runner.py

[27949354f] Alex Kogan 2025-07-01 [Feature] A calibration-free RTN-based quantization for accurate and accelerated INT4/INT8 inference (#18768)
28	0	tests/quantization/test_rtn.py
3	0	vllm/model_executor/layers/quantization/__init__.py
288	0	vllm/model_executor/layers/quantization/rtn.py

[bd5038af0] Ernest Wong 2025-06-30 [Doc] add config and troubleshooting guide for NCCL & GPUDirect RDMA (#15897)
44	1	docs/serving/distributed_serving.md
21	0	docs/usage/troubleshooting.md

[a2f14dc8f] Chendi.Xue 2025-06-30 [CI][Intel Gaudi][vllm-Plugin]Add CI for hpu-plugin-v1-test (#20196)
40	8	.buildkite/scripts/hardware_ci/run-hpu-test.sh

[92ee7baaf] Kuntai Du 2025-06-30 [Example] add one-click runnable example for P2P NCCL XpYd (#20246)
245	0	examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_example_p2p_nccl_xpyd.sh
0	0	examples/online_serving/{disagg_xpyd/disagg_prefill_proxy_xpyd.py => disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py}

[7151f9224] Woosuk Kwon 2025-06-30 [Misc] Fix spec decode example (#20296)
0	2	examples/offline_inference/spec_decode.py

[e28533a16] fyuan1316 2025-07-01 [Bugfix] Fix include prompt in stream response when echo=true (#15233)
54	0	tests/entrypoints/openai/test_completion.py
17	4	vllm/entrypoints/openai/serving_completion.py

[6d42ce831] Luka Govedič 2025-06-30 [CLI] Improve CLI arg parsing for `-O`/`--compilation-config` (#20156)
18	10	tests/engine/test_arg_utils.py
47	0	tests/test_utils.py
9	10	vllm/config.py
4	1	vllm/engine/arg_utils.py
46	19	vllm/utils.py

[ded1fb635] Zhonghua Deng 2025-07-01 [Bugfix][V1][P/D]Fix the issue of occasional garbled output  for P2pNcclConnector (#20263)
5	4	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py

[97d9524fe] Wentao Ye 2025-06-30 [Refactor] Remove useless pdb comment (#20266)
0	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py

[d8cf819a9] Kyle Sayers 2025-06-30 [Core] [Bugfix] [Multimodal] Fix multimodal profiling and generation for SFT/PTQed models (#20058)
7	0	docs/contributing/model/multimodal.md
1	0	tests/multimodal/test_processing.py
8	0	vllm/entrypoints/llm.py
4	0	vllm/entrypoints/utils.py
22	7	vllm/inputs/preprocess.py
2	0	vllm/model_executor/models/aya_vision.py
2	0	vllm/model_executor/models/blip2.py
2	0	vllm/model_executor/models/chameleon.py
5	1	vllm/model_executor/models/deepseek_vl2.py
3	1	vllm/model_executor/models/florence2.py
2	0	vllm/model_executor/models/fuyu.py
2	0	vllm/model_executor/models/gemma3_mm.py
1	0	vllm/model_executor/models/glm4v.py
2	0	vllm/model_executor/models/granite_speech.py
3	0	vllm/model_executor/models/h2ovl.py
2	0	vllm/model_executor/models/idefics3.py
4	1	vllm/model_executor/models/internvl.py
4	1	vllm/model_executor/models/llava.py
7	0	vllm/model_executor/models/llava_onevision.py
6	4	vllm/model_executor/models/minicpmo.py
14	4	vllm/model_executor/models/minicpmv.py
2	0	vllm/model_executor/models/minimax_vl_01.py
2	0	vllm/model_executor/models/mistral3.py
4	2	vllm/model_executor/models/mllama.py
2	0	vllm/model_executor/models/mllama4.py
2	0	vllm/model_executor/models/ovis.py
4	1	vllm/model_executor/models/paligemma.py
2	0	vllm/model_executor/models/phi3v.py
2	1	vllm/model_executor/models/phi4mm.py
6	1	vllm/model_executor/models/pixtral.py
1	0	vllm/model_executor/models/prithvi_geospatial_mae.py
7	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
2	0	vllm/model_executor/models/qwen2_audio.py
3	1	vllm/model_executor/models/qwen2_vl.py
3	0	vllm/model_executor/models/qwen_vl.py
2	0	vllm/model_executor/models/skyworkr1v.py
6	0	vllm/model_executor/models/ultravox.py
3	1	vllm/model_executor/models/whisper.py
43	11	vllm/multimodal/processing.py
6	1	vllm/multimodal/profiling.py
2	0	vllm/utils.py

[551ef1631] Wentao Ye 2025-06-30 [Unit Test] Add unit test for deep gemm (#20090)
225	0	tests/kernels/moe/test_deepgemm.py

[2863befce] Woosuk Kwon 2025-06-30 [Optimization] Use Shared `CachedRequestData` Instance Across All Requests (#20232)
67	63	tests/v1/core/test_scheduler.py
2	2	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
6	6	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
0	1	tests/v1/kv_connector/unit/utils.py
11	11	tests/v1/tpu/worker/test_tpu_model_runner.py
11	11	tests/v1/worker/test_gpu_model_runner.py
23	20	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
12	7	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
16	18	vllm/v1/core/sched/output.py
43	65	vllm/v1/core/sched/scheduler.py
17	17	vllm/v1/worker/gpu_model_runner.py
13	11	vllm/v1/worker/tpu_model_runner.py

[2965c99c8] Woosuk Kwon 2025-06-30 [Spec Decode] Clean up spec decode example (#20240)
0	144	examples/offline_inference/eagle.py
21	19	examples/offline_inference/spec_decode.py

[2062c0723] Woosuk Kwon 2025-06-30 [Spec Decode] Refactor spec decoding into a separate function (#20238)
60	33	vllm/v1/worker/gpu_model_runner.py

[1c50e100a] li haoyang 2025-06-30 [Bugfix] fix quark ptpc (#20251)
1	5	vllm/model_executor/layers/quantization/quark/quark.py
22	11	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py

[3ee56e26b] Michael Yao 2025-06-30 [Docs] Fix 1-2-3 list in v1/prefix_caching.md (#20243)
11	11	docs/design/v1/prefix_caching.md

[8fe7fc863] Jee Jee Li 2025-06-30 [Quantization] Improve BitsAndBytesModelLoader (#20242)
72	51	vllm/model_executor/model_loader/bitsandbytes_loader.py

[e936e401d] Isotr0py 2025-06-30 [Bugfix] Fix processor initialization in transformers 4.53.0 (#20244)
7	1	vllm/inputs/registry.py

[f5dfa0753] noiji 2025-06-30 [Bugfix] Skip loading extra parameters for modelopt Qwen3 MoE model (#19598)
15	9	vllm/model_executor/models/qwen3_moe.py

[022c58b80] Reid 2025-06-30 [doc] Add Slack and Forum to the top navigation (#20208)
56	0	docs/mkdocs/javascript/slack_and_forum.js
26	0	docs/mkdocs/stylesheets/extra.css
1	0	mkdocs.yaml

[19108ef31] Woosuk Kwon 2025-06-29 [Misc] Fix import (#20233)
1	2	vllm/v1/worker/gpu_model_runner.py

[5a52f389d] Chendi.Xue 2025-06-29 [BUGFIX][DEEPSEEK][MODEL_LOAD] fix w13, w2 weight not initialized assert (#20202)
1	0	vllm/model_executor/models/deepseek_v2.py

[65b1cbb13] redmoe-moutain 2025-06-30 [Model] support dots1 (#18254)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
535	0	vllm/model_executor/models/dots1.py
1	0	vllm/model_executor/models/registry.py

[6c9837a76] Huy Do 2025-06-29 Fix cuda_archs_loose_intersection when handling sm_*a (#20207)
12	2	CMakeLists.txt
14	19	cmake/utils.cmake

[6f2f53a82] Dipika Sikka 2025-06-30 [Quantization] Add compressed-tensors NVFP4 MoE Support (#19990)
3	3	tests/quantization/test_compressed_tensors.py
3	1	vllm/model_executor/layers/fused_moe/layer.py
3	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
271	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	12	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
14	1	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py

[7b1895e6c] Michael Goin 2025-06-29 [CI Fix] Try fixing eagle e2e test OOM by reducing block allocation (#20213)
8	0	tests/spec_decode/e2e/test_eagle_correctness.py

[4d3669368] Wentao Ye 2025-06-28 [Refactor] Create a function util and cache the results for `has_deepgemm`, `has_deepep`, `has_pplx` (#20187)
7	13	tests/kernels/moe/test_deepep_deepgemm_moe.py
3	5	tests/kernels/moe/test_deepep_moe.py
5	5	vllm/distributed/device_communicators/all2all.py
0	3	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
5	7	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
3	7	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/utils.py
3	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	4	vllm/model_executor/layers/quantization/deepgemm.py
2	4	vllm/model_executor/layers/quantization/fp8.py
2	4	vllm/model_executor/layers/quantization/utils/fp8_utils.py
28	0	vllm/utils.py

[daec9dea6] Stan Wozniak 2025-06-28 [Bugfix] Correct behavior of GraniteMoeHybrid for TensorParallel execution (#20137)
0	42	tests/models/language/generation/test_granitemoehybrid.py
3	2	tests/models/language/generation/test_hybrid.py
73	37	vllm/model_executor/models/granitemoehybrid.py

[daceac57c] Nicolò Lucchesi 2025-06-28 [Frontend] Generalize `v1/audio/transcriptions` endpoint (#20179)
14	128	vllm/entrypoints/openai/speech_to_text.py
11	0	vllm/model_executor/models/interfaces.py
129	0	vllm/model_executor/models/whisper.py

[8615d9776] Thomas Parnell 2025-06-28 [CI/Build] Add new CI job to validate Hybrid Models for every PR  (#20147)
12	1	.buildkite/test-pipeline.yaml
1	0	pyproject.toml
3	0	tests/models/language/generation/test_hybrid.py

[7b460c25f] Jiayi Yan 2025-06-28 [BugFix] Fix the incorrect func name in the comments. (config.py) (#20185)
1	1	vllm/config.py

[f71977228] Michael Goin 2025-06-28 [Bugfix] Properly reject requests with empty list guided_choice (#20195)
6	0	vllm/v1/engine/processor.py

[d45417b80] Wentao Ye 2025-06-28 fix ci issue distributed 4 gpu test (#20204)
18	0	examples/offline_inference/data_parallel.py

[a29e62ea3] Michael Goin 2025-06-28 Fix num_token_padding support for static per-tensor scaled_fp8_quant (#20188)
1	2	vllm/_custom_ops.py

[e53be6f00] Chales Xu 2025-06-28 [Misc] Add type assertion of request_id for LLMEngine.add_request (#19700)
5	5	tests/mq_llm_engine/test_error_handling.py
4	0	vllm/engine/llm_engine.py
5	0	vllm/v1/engine/llm_engine.py

[c329ceca6] Michael Goin 2025-06-28 [CI Fix] Pin tests/models/registry.py MiniMaxText01ForCausalLM to revision due to model changes (#20199)
8	1	tests/models/registry.py
1	0	tests/models/test_initialization.py

[3c545c0c3] Fabien Dupont 2025-06-27 [CI/Build] Allow hermetic builds (#18064)
158	30	docker/Dockerfile

[e8c3bd2cd] Tyler Michael Smith 2025-06-27 [Bugfix] Fix some narrowing conversion warnings (#20141)
1	1	csrc/attention/mla/cutlass_mla_kernels.cu
2	6	csrc/mamba/causal_conv1d/causal_conv1d.cu
1	3	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
2	2	csrc/quantization/fp4/nvfp4_experts_quant.cu
1	1	csrc/quantization/fp4/nvfp4_quant_kernels.cu
1	1	csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu

[c6c983053] bnellnm 2025-06-27 [Bugfix] Mark 'hidden_states' as mutable in moe_forward registration. (#20152)
2	1	vllm/model_executor/layers/fused_moe/layer.py

[aafabaa0d] Luka Govedič 2025-06-27 [Fix][torch.compile] Enable custom ops by default when Inductor off (#20102)
26	19	tests/model_executor/test_enabled_custom_ops.py
9	18	vllm/config.py
6	6	vllm/model_executor/custom_op.py

[94a55c768] Hosang 2025-06-27 [Fix][ROCm] Remove unused variables to fix build error on GFX11/12 (#19891)
0	4	csrc/rocm/attention.cu

[aa0dc77ef] Ilya Lavrenov 2025-06-27 [Perf] Improved perf for resolve_chat_template_content_format (#20065)
1	0	vllm/entrypoints/chat_utils.py

[4ab3ac285] Michael Goin 2025-06-27 [Bugfix] Fix flaky failure when getting DP ports (#20151)
32	9	vllm/config.py

[d1c956dc0] Robert Shaw 2025-06-27 Gemma3n (Text-only) (#20134)
4	0	docs/models/supported_models.md
2	0	tests/models/registry.py
51	0	vllm/model_executor/layers/activation.py
811	0	vllm/model_executor/models/gemma3n.py
2	0	vllm/model_executor/models/registry.py

[dec197e3e] Chendi.Xue 2025-06-27 Quick Fix by adding conditional import for flash_attn_varlen_func in flash_attn (#20143)
4	0	vllm/attention/utils/fa_utils.py
7	3	vllm/v1/attention/backends/flash_attn.py

[6e244ae09] Yazan Sharaya 2025-06-27 [Perf][Frontend] eliminate api_key and x_request_id headers middleware overhead (#19946)
0	5	docs/serving/openai_compatible_server.md
116	0	tests/entrypoints/openai/test_optional_middleware.py
73	27	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/cli_args.py

[cd4cfee68] wang.yuqi 2025-06-27 [Model][1/N] Automatic conversion of CrossEncoding model (#20012)
7	4	tests/models/language/pooling/mteb_utils.py
28	1	vllm/config.py
1	148	vllm/model_executor/models/bert_with_rope.py
200	0	vllm/model_executor/models/config.py
3	14	vllm/model_executor/models/qwen3.py

[e11093068] Thomas Parnell 2025-06-27 [Fix] Fix gemma CI test failing on main (#20124)
18	11	tests/models/language/generation/test_gemma.py

[8b64c895c] Yang Wang 2025-06-26 [CI] Sync test dependency with test.in for torch nightly (#19632)
11	1	.buildkite/test-pipeline.yaml
5	0	.pre-commit-config.yaml
37	34	requirements/nightly_torch_test.txt
2	1	requirements/test.in
42	0	tests/standalone_tests/pytorch_nightly_dependency.sh
34	0	tools/generate_nightly_torch_test.py

[0740e29b6] li haoyang 2025-06-27 [Feature] add quick all reduce (#19744)
8	0	CMakeLists.txt
114	0	csrc/custom_quickreduce.cu
11	0	csrc/ops.h
338	0	csrc/quickreduce/base.h
196	0	csrc/quickreduce/quick_reduce.h
698	0	csrc/quickreduce/quick_reduce_impl.cuh
18	0	csrc/torch_bindings.cpp
138	0	tests/distributed/test_quick_all_reduce.py
32	0	vllm/_custom_ops.py
20	2	vllm/distributed/device_communicators/cuda_communicator.py
278	0	vllm/distributed/device_communicators/quick_all_reduce.py
28	0	vllm/envs.py

[44d2e6af6] Michael Goin 2025-06-27 [Bugfix] Build moe_data for both sm100 and sm90 (#20086)
12	2	CMakeLists.txt
5	4	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[2d7779f88] Ilya Markov 2025-06-27 [Perf] SM100 FP8 GEMM Optimizations after cutlass_profiler (#20071)
20	20	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh

[a57d57fa7] Dipika Sikka 2025-06-26 [Quantization] Bump to use latest `compressed-tensors` (#20033)
1	1	requirements/common.txt

[71799fd00] Michael Goin 2025-06-27 [CI Failure] Fix OOM with test_oot_registration_embedding (#20144)
3	1	tests/models/test_oot_registration.py

[e9fd658a7] Bowen Wang 2025-06-26 [Feature] Expert Parallelism Load Balancer (EPLB) (#18343)
17	0	.buildkite/test-pipeline.yaml
292	0	tests/distributed/test_eplb_algo.py
504	0	tests/distributed/test_eplb_execute.py
10	2	tests/models/test_initialization.py
33	0	vllm/config.py
7	0	vllm/distributed/eplb/__init__.py
431	0	vllm/distributed/eplb/eplb_state.py
233	0	vllm/distributed/eplb/rebalance_algo.py
306	0	vllm/distributed/eplb/rebalance_execute.py
20	0	vllm/engine/arg_utils.py
236	28	vllm/model_executor/layers/fused_moe/layer.py
8	0	vllm/model_executor/layers/quantization/awq_marlin.py
42	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
8	0	vllm/model_executor/layers/quantization/experts_int8.py
14	0	vllm/model_executor/layers/quantization/fp8.py
8	0	vllm/model_executor/layers/quantization/gguf.py
8	0	vllm/model_executor/layers/quantization/gptq_marlin.py
8	0	vllm/model_executor/layers/quantization/modelopt.py
8	0	vllm/model_executor/layers/quantization/moe_wna16.py
8	0	vllm/model_executor/layers/quantization/quark/quark_moe.py
109	18	vllm/model_executor/models/deepseek_v2.py
68	0	vllm/model_executor/models/interfaces.py
61	4	vllm/v1/worker/gpu_model_runner.py
7	2	vllm/v1/worker/gpu_worker.py

[07b8fae21] Kyle Yu 2025-06-26 [Doc] correct LoRA capitalization (#20135)
1	1	docs/README.md
1	1	docs/models/supported_models.md

[562308816] Wentao Ye 2025-06-26 [Refactor] Rename commnication utils (#20091)
1	1	tests/kernels/moe/test_deepep_deepgemm_moe.py
1	1	tests/kernels/moe/test_deepep_moe.py
1	1	tests/kernels/moe/test_pplx_cutlass_moe.py
1	1	tests/kernels/moe/test_pplx_moe.py
0	0	tests/kernels/moe/{deepep_utils.py => utils.py}

[04e1642e3] Chengji Yao 2025-06-26 [TPU] add kv cache update kernel (#19928)
2	0	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
71	0	tests/v1/tpu/test_kv_cache_update_kernel.py
2	1	tests/v1/tpu/test_pallas.py
117	0	vllm/attention/ops/pallas_kv_cache_update.py
50	5	vllm/v1/attention/backends/pallas.py
100	32	vllm/v1/worker/tpu_model_runner.py

[b69781f10] Kunshang Ji 2025-06-27 [Hardware][Intel GPU] Add v1 Intel GPU support with Flash attention backend. (#19560)
1	0	.buildkite/scripts/hardware_ci/run-xpu-test.sh
1	0	docker/Dockerfile.xpu
1	0	requirements/xpu.txt
105	0	vllm/_ipex_ops.py
14	1	vllm/attention/utils/fa_utils.py
1	1	vllm/executor/ray_distributed_executor.py
70	34	vllm/platforms/xpu.py
5	7	vllm/v1/attention/backends/flash_attn.py
32	0	vllm/v1/worker/xpu_model_runner.py
164	0	vllm/v1/worker/xpu_worker.py

[0bceac981] Tyler Michael Smith 2025-06-26 Spam folks if config.py changes (#20131)
4	0	.github/CODEOWNERS

[34878a0b4] Cyrus Leung 2025-06-26 [Doc] Rename page titles (#20130)
1	1	docs/contributing/incremental_build.md
3	3	docs/contributing/model/README.md
1	1	docs/contributing/model/basic.md
1	1	docs/contributing/model/registration.md
1	1	docs/contributing/model/tests.md

[6393b0398] Cyrus Leung 2025-06-26 [Doc] Auto sign-off for VSCode (#20132)
6	3	docs/contributing/README.md

[0907d507b] wang.yuqi 2025-06-26 [Doc] Automatically signed-off by PyCharm (#20120)
5	0	docs/contributing/README.md

[c894c5dc1] Wentao Ye 2025-06-26 [Bug Fix] Fix address/port already in use error for deep_ep test (#20094)
4	1	tests/kernels/moe/deepep_utils.py
9	0	vllm/model_executor/layers/fused_moe/utils.py

[1f5d178e9] Michael Goin 2025-06-26 Revert "[Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine" (#20128)
4	9	vllm/config.py

[27c065df5] TJian 2025-06-26 [Bugfix][V1][ROCm] Fix AITER Flash Attention Backend (Fix API Break and Local Attention Logic: affecting Llama4) (#19904)
9	5	vllm/attention/layer.py
37	18	vllm/v1/attention/backends/rocm_aiter_fa.py

[84c260cae] Michael Yao 2025-06-26 [Docs] Improve frameworks/helm.md (#20113)
64	56	docs/deployment/frameworks/helm.md

[167aca45c] Reid 2025-06-26 [Misc] Use collapsible blocks for benchmark examples. (#20017)
60	34	benchmarks/README.md

[0567c8249] Li, Jiang 2025-06-26 [CPU] Fix torch version in x86 CPU backend (#19258)
9	4	csrc/cpu/torch_bindings.cpp
20	13	docker/Dockerfile.cpu
12	0	requirements/cpu-build.txt
3	2	requirements/cpu.txt
2	0	tests/models/multimodal/generation/test_common.py
3	0	tests/models/multimodal/generation/vlm_utils/builders.py
2	0	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/ipex_quant.py

[d188913d9] Wentao Ye 2025-06-26 [Refactor] Remove unused library (#20099)
0	1	vllm/_custom_ops.py

[1d7c29f5f] Cyrus Leung 2025-06-26 [Doc] Update docs for New Model Implementation (#20115)
6	1	docs/.nav.yml
13	11	docs/contributing/model/README.md

[65397e40f] Seiji Eicher 2025-06-26 [Bugfix] Allow `CUDA_VISIBLE_DEVICES=''` in `Platform.device_id_to_physical_device_id` (#18979)
38	0	tests/config/test_config_generation.py
71	0	tests/v1/engine/test_engine_core_client.py
5	10	vllm/platforms/interface.py

[9502c3813] Ekagra Ranjan 2025-06-26 [Benchmark][Bug] Fix multiple bugs in bench and add args to spec_decode offline (#20083)
2	1	benchmarks/benchmark_dataset.py
13	7	examples/offline_inference/spec_decode.py
7	3	vllm/benchmarks/datasets.py
6	0	vllm/benchmarks/serve.py

[258268356] Nicolò Lucchesi 2025-06-26 [PD] Skip `tp_size` exchange with rank0 (#19413)
23	6	tests/v1/kv_connector/unit/test_nixl_connector.py
49	60	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[754b00edb] Michael Goin 2025-06-26 [Bugfix] Fix Mistral tool-parser regex for nested JSON (#20093)
51	0	tests/models/language/generation/test_mistral.py
2	2	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[296ce95d8] Michael Goin 2025-06-26 [CI] Add SM120 to the Dockerfile (#19794)
3	3	docker/Dockerfile

[2d7620c3e] Chenyaaang 2025-06-25 [TPU] Add TPU specific var VLLM_TPU_MOST_MODEL_LEN (#19919)
14	0	tests/v1/tpu/worker/test_tpu_model_runner.py
3	0	vllm/envs.py
0	10	vllm/platforms/tpu.py
5	0	vllm/v1/attention/backends/pallas.py
163	67	vllm/v1/worker/tpu_model_runner.py

[55c65ab49] Nick Hill 2025-06-25 [P/D] Avoid stranding blocks in P when aborted in D's waiting queue (#19223)
14	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[2cc206997] Chengji Yao 2025-06-25 [TPU][Bugfix] fix kv cache padding (#20048)
1	7	vllm/v1/attention/backends/pallas.py
13	2	vllm/v1/worker/tpu_worker.py

[9f0608fc1] zhrrr 2025-06-26 [Bugfix] default set cuda_graph_sizes to max_num_seqs for v1 engine (#20062)
9	4	vllm/config.py

[4e0db57ff] QiliangCui 2025-06-25 Fix the path to the testing script. (#20082)
1	1	.buildkite/scripts/tpu/docker_run_bm.sh

[c40692bf9] Nick Hill 2025-06-25 [Misc] Add parallel state `node_count` function (#20045)
2	0	.buildkite/test-pipeline.yaml
43	0	tests/distributed/test_node_count.py
53	2	vllm/distributed/parallel_state.py

[4734704b3] lkchen 2025-06-25 [PD] let toy proxy handle /chat/completions (#19730)
15	7	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py

[8b8c209e3] Eldar Kurtić 2025-06-25 static_scaled_fp8_quant should not run when scale.numel is not 1 (#20076)
1	1	vllm/_custom_ops.py

[23a04e089] lsz05 2025-06-26 [Fix] Support cls pooling in ModernBertPooler (#20067)
8	1	vllm/model_executor/models/modernbert.py

[02c97d9a9] Dipika Sikka 2025-06-25 [Quantization] Add compressed-tensors emulations support for NVFP4 (#19879)
7	0	vllm/envs.py
3	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
16	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py

[e795d723e] Nicolò Lucchesi 2025-06-25 [Frontend] Add `/v1/audio/translations` OpenAI API endpoint (#19615)
30	0	docs/serving/openai_compatible_server.md
20	21	examples/online_serving/openai_transcription_client.py
75	0	examples/online_serving/openai_translation_client.py
2	0	tests/entrypoints/openai/test_transcription_validation.py
172	0	tests/entrypoints/openai/test_translation_validation.py
54	1	vllm/entrypoints/openai/api_server.py
187	0	vllm/entrypoints/openai/protocol.py
4	4	vllm/entrypoints/openai/serving_engine.py
80	435	vllm/entrypoints/openai/serving_transcription.py
503	0	vllm/entrypoints/openai/speech_to_text.py

[8359f4c8d] cjackal 2025-06-26 [V1][Speculative Decoding] Fix DeepSeek MTP (#20022)
20	8	vllm/model_executor/models/deepseek_mtp.py
1	1	vllm/v1/spec_decode/eagle.py

[bf5181583] Michael Goin 2025-06-25 [Doc] Guide for Incremental Compilation Workflow (#19109)
3	0	docs/contributing/README.md
138	0	docs/contributing/incremental_build.md
3	0	docs/getting_started/installation/gpu/cuda.inc.md
169	0	tools/generate_cmake_presets.py

[c53fec1fc] Reid 2025-06-25 [doc] add reference link for Intel XPU (#20064)
1	1	docs/getting_started/installation/gpu/xpu.inc.md

[0f9e7354f] Lucas Wilkinson 2025-06-25 [BugFix] Fix full-cuda-graph illegal memory access in FA3 (#20057)
7	18	vllm/v1/attention/backends/flash_attn.py

[ba7ba35cd] Aaron Pham 2025-06-25 [Chore] debloat some initial logs (#19438)
4	4	vllm/config.py

[015fab8c2] bnellnm 2025-06-25 [Kernels][Bugfix] Use torch op for all kernels in FusedMoE forward.  Add additional testing for cudagraphs. (#19717)
10	1	tests/kernels/moe/test_cutlass_moe.py
202	90	tests/kernels/moe/test_moe.py
1	1	tests/kernels/moe/test_nvfp4_moe.py
3	19	tests/kernels/moe/test_pplx_cutlass_moe.py
10	27	tests/kernels/moe/test_pplx_moe.py
42	7	tests/kernels/quantization/test_block_fp8.py
25	3	tests/kernels/utils.py
1	0	vllm/envs.py
35	35	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	1	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
42	42	vllm/model_executor/layers/fused_moe/fused_moe.py
7	12	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py

[f59fc60fb] Max Wittig 2025-06-25 [Feat][CLI] enforce-include-usage (#19695)
2	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/entrypoints/openai/cli_args.py
15	4	vllm/entrypoints/openai/serving_chat.py
8	3	vllm/entrypoints/openai/serving_completion.py
4	2	vllm/entrypoints/openai/serving_engine.py

[879f69bed] Wentao Ye 2025-06-25 [Refactor] Remove duplicate `ceil_div` (#20023)
3	8	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
1	4	tests/kernels/attention/test_mla_decode_cpu.py
1	4	tests/kernels/attention/test_triton_decode_attention.py
4	5	tests/neuron/1_core/test_prefix_prefill.py
6	9	vllm/attention/ops/nki_flash_attn.py
2	6	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
3	6	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[710893414] David Xia 2025-06-25 [Frontend] speed up import time of vllm.config (#18036)
27	17	vllm/config.py

[3443aaf8d] h-avsha 2025-06-25 Move to a faster base64 implementation (#19984)
1	0	requirements/common.txt
5	5	vllm/multimodal/image.py

[2273ec322] Isotr0py 2025-06-25 Revert "Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor" (#20030)
0	1	vllm/model_executor/models/gemma3_mm.py

[a6c4b87fb] Wentao Ye 2025-06-24 Revert "[Feature] Integrate new deepgemm (#19820)" (#20049)
0	3	benchmarks/kernels/benchmark_moe.py
174	133	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
21	2	tests/kernels/moe/test_deepep_deepgemm_moe.py
41	14	tests/kernels/quantization/test_block_fp8.py
11	8	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
6	4	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/layers/quantization/deepgemm.py
3	68	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[1afa9948f] Brayden Zhong 2025-06-24 [Llama4] Update `attn_temperature_tuning` (#19997)
1	2	vllm/model_executor/models/llama4.py

[0d06b533a] Eli Uriegas 2025-06-24 cmake: Update vllm_flash_attn for vllm_kernels (#20032)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[c01d1c5ab] Boyuan Feng 2025-06-24 use .dev for version comparison with pytorch nightly release (#20031)
9	0	tests/compile/test_config.py
1	1	vllm/compilation/backends.py
2	2	vllm/model_executor/layers/quantization/torchao.py
7	2	vllm/utils.py

[ead369845] Brayden Zhong 2025-06-24 [Easy] Remove submodule added in #19463 (#20039)
0	1	test-qwen

[c6e3bba8e] Wentao Ye 2025-06-24 [Feature] Integrate new deepgemm (#19820)
3	0	benchmarks/kernels/benchmark_moe.py
134	186	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
2	21	tests/kernels/moe/test_deepep_deepgemm_moe.py
14	41	tests/kernels/quantization/test_block_fp8.py
8	11	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
4	5	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	1	vllm/model_executor/layers/quantization/deepgemm.py
68	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[91f7d9d0b] lkchen 2025-06-24 [P/D] Asynchronously do _nixl_handshake (#19836)
168	58	tests/v1/kv_connector/unit/test_nixl_connector.py
96	38	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[8619e7158] Nick Hill 2025-06-24 [BugFix] Fix multi-node offline data parallel (#19937)
3	0	.buildkite/test-pipeline.yaml
2	0	vllm/entrypoints/llm.py
6	2	vllm/v1/engine/core.py
19	1	vllm/v1/engine/core_client.py
1	1	vllm/v1/engine/llm_engine.py

[c635c5f74] d.transposed 2025-06-24 [Misc][Benchmarking] Add variable request-rate ("ramp-up") to the benchmarking client. (#19423)
15	0	benchmarks/README.md
161	17	benchmarks/benchmark_serving.py
154	17	vllm/benchmarks/serve.py

[a045b7e89] Lucas Wilkinson 2025-06-24 [Perf] Improve/Fix-regression for FA3 in High QPS regimes (#19463)
1	1	cmake/external_projects/vllm_flash_attn.cmake
1	0	test-qwen

[981eeca41] amit 2025-06-24 [Fix][V1] Remove --scheduling-policy oracle (#20010)
0	6	tests/v1/test_oracle.py
0	5	vllm/engine/arg_utils.py

[26d34eb67] Reid 2025-06-24 refactor example - qwen3_reranker (#19847)
22	10	examples/offline_inference/qwen3_reranker.py

[53da4cd39] Li, Jiang 2025-06-24 [Bugfix][CPU] Fix InputBatch for pooling models in the CPU v1 (#20014)
1	1	tests/models/language/pooling/test_reward.py
4	0	vllm/v1/worker/cpu_model_runner.py

[9a3b88328] Vadim Gimpelson 2025-06-24 [PERF] Speedup of MRoPE prepare inputs (#19939)
9	9	vllm/model_executor/layers/rotary_embedding.py
8	9	vllm/v1/worker/gpu_model_runner.py

[3014c920d] Reid 2025-06-24 add some examples for other benchmark scripts (#19893)
175	0	benchmarks/README.md

[0eed51695] Kay Yan 2025-06-24 [doc] Fix broken link in the installation for CPU (#19980)
1	1	docs/getting_started/installation/cpu/arm.inc.md
1	1	docs/getting_started/installation/cpu/x86.inc.md

[ee5ad8d2c] Chenyaaang 2025-06-23 [Misc][Tools][Benchmark] Add profile to autotune script (#19711)
37	5	benchmarks/auto_tune.sh

[a738dbb2a] QiliangCui 2025-06-23 Update test case parameter to have the throughput above 8.0 (#19994)
2	2	.buildkite/scripts/tpu/config_v6e_1.env

[33d5e29be] Chenyaaang 2025-06-23 [TPU] Fix tpu model runner test (#19995)
2	0	tests/v1/tpu/worker/test_tpu_model_runner.py

[4671ac6e2] 22quinn 2025-06-23 [Bugfix][Benchmark] Fix Marlin benchmark (#19929)
150	79	benchmarks/kernels/benchmark_marlin.py

[dd2ccf8dd] Jun-Howie 2025-06-24 Feat Dynamic Quantization for MoE Layers in GPTQ Marlin Backend (#19395)
29	3	vllm/model_executor/layers/quantization/gptq_marlin.py

[a3bc76e4b] 22quinn 2025-06-23 [CI/Build] Push latest tag for cpu and neuron docker image (#19897)
2	0	.buildkite/release-pipeline.yaml

[e6327c9b3] cascade 2025-06-23 [Feature] Support sequence parallelism for static fp8 quantization (#19181)
144	17	tests/compile/test_sequence_parallelism.py
52	56	tests/distributed/test_sequence_parallel.py
2	1	tests/models/registry.py
2	2	vllm/compilation/fusion.py
4	4	vllm/compilation/pass_manager.py
328	114	vllm/compilation/sequence_parallelism.py
2	4	vllm/config.py

[d0132f025] lkchen 2025-06-23 [Misc] Add type alias `ReqId` and `EngineId` for better readability (#19880)
20	17	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[61f4fc5dc] Isotr0py 2025-06-24 [Bugfix][v1] Fix step pooler implementation and step pooling usage in v1 (#19956)
11	7	tests/conftest.py
6	6	tests/model_executor/test_model_load_with_params.py
1	1	tests/models/language/pooling/embed_utils.py
1	1	tests/models/language/pooling/test_embedding.py
2	2	tests/models/language/pooling/test_jina.py
104	0	tests/models/language/pooling/test_reward.py
2	2	tests/models/multimodal/pooling/test_dse_qwen2_vl.py
1	1	tests/models/multimodal/pooling/test_llava_next.py
1	1	tests/models/multimodal/pooling/test_phi3v.py
1	1	tests/quantization/test_bitsandbytes.py
6	7	vllm/model_executor/layers/pooler.py
6	0	vllm/model_executor/models/interfaces.py
19	11	vllm/v1/worker/gpu_input_batch.py
3	0	vllm/v1/worker/gpu_model_runner.py

[68aaeb374] Tyler Michael Smith 2025-06-23 [EP+DP] Optimize the little operations in the DeepGEMM + DeepEP low latency case (#19885)
83	0	tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py
170	16	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
10	2	vllm/model_executor/layers/fused_moe/layer.py

[c3649e4fe] Lukas Geiger 2025-06-23 [Docs] Fix syntax highlighting of shell commands (#19870)
1	1	.buildkite/nightly-benchmarks/nightly-annotation.md
6	6	docs/deployment/docker.md
1	1	docs/deployment/frameworks/anything-llm.md
2	2	docs/deployment/frameworks/autogen.md
3	3	docs/deployment/frameworks/cerebrium.md
1	1	docs/deployment/frameworks/chatbox.md
2	2	docs/deployment/frameworks/dify.md
2	2	docs/deployment/frameworks/dstack.md
2	2	docs/deployment/frameworks/haystack.md
2	2	docs/deployment/frameworks/helm.md
3	3	docs/deployment/frameworks/litellm.md
2	2	docs/deployment/frameworks/open-webui.md
6	6	docs/deployment/frameworks/retrieval_augmented_generation.md
8	8	docs/deployment/frameworks/skypilot.md
3	3	docs/deployment/frameworks/streamlit.md
1	1	docs/deployment/integrations/llamastack.md
3	3	docs/deployment/k8s.md
8	8	docs/deployment/nginx.md
3	3	docs/features/multimodal_inputs.md
2	2	docs/features/quantization/auto_awq.md
1	1	docs/features/quantization/bitblas.md
2	2	docs/features/quantization/bnb.md
5	5	docs/features/quantization/fp8.md
3	3	docs/features/quantization/gguf.md
2	2	docs/features/quantization/gptqmodel.md
4	4	docs/features/quantization/int4.md
4	4	docs/features/quantization/int8.md
1	1	docs/features/quantization/modelopt.md
1	1	docs/features/quantization/quantized_kvcache.md
6	6	docs/features/quantization/quark.md
1	1	docs/features/quantization/torchao.md
1	1	docs/features/tool_calling.md
5	5	docs/getting_started/installation/aws_neuron.md
9	9	docs/getting_started/installation/cpu.md
2	2	docs/getting_started/installation/cpu/apple.inc.md
5	5	docs/getting_started/installation/cpu/build.inc.md
3	3	docs/getting_started/installation/cpu/s390x.inc.md
4	4	docs/getting_started/installation/google_tpu.md
17	17	docs/getting_started/installation/gpu/cuda.inc.md
12	12	docs/getting_started/installation/gpu/rocm.inc.md
6	6	docs/getting_started/installation/gpu/xpu.inc.md
5	5	docs/getting_started/installation/intel_gaudi.md
1	1	docs/getting_started/installation/python_env_setup.inc.md
7	7	docs/getting_started/quickstart.md
9	9	docs/models/extensions/runai_model_streamer.md
2	2	docs/models/supported_models.md
8	8	docs/serving/distributed_serving.md
1	1	docs/serving/integrations/langchain.md
1	1	docs/serving/integrations/llamaindex.md
1	1	docs/usage/metrics.md
2	2	docs/usage/troubleshooting.md
20	20	examples/offline_inference/openai_batch/README.md
8	8	examples/online_serving/opentelemetry/README.md

[53243e5c4] Reid 2025-06-23 [doc] improve readability for long commands (#19920)
33	5	docs/contributing/profiling.md
11	3	docs/getting_started/installation/cpu.md
4	1	docs/usage/troubleshooting.md

[a6e6604d3] Jee Jee Li 2025-06-23 [Bugfix] Fix CI bitsandbytes failure (#19969)
1	0	tests/quantization/test_bitsandbytes.py

[b82e0f82c] Reid 2025-06-23 [doc] use MkDocs collapsible blocks - supplement (#19973)
188	168	docs/design/v1/p2p_nccl_connector.md
45	39	docs/design/v1/torch_compile.md
57	57	examples/others/logging_configuration.md

[5111642a6] Isotr0py 2025-06-23 [Doc] Update V1 status for decoder-only embedding models (#19952)
10	9	docs/models/supported_models.md
8	18	vllm/model_executor/models/qwen2_rm.py

[1bcd15edc] lkchen 2025-06-22 [BugFix][P/D] Fix for cases where _recving_transfers can be cleaned up when *all* transfer done (#19874)
173	1	tests/v1/kv_connector/unit/test_nixl_connector.py
6	3	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[2ebff5b77] Nicolò Lucchesi 2025-06-23 [P/D][NixlConnector] Support `tp_size > num_kv_heads` deployments (#19691)
10	5	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[f17aec0d6] Reid 2025-06-23 [doc] Fold long code blocks to improve readability (#19926)
3	3	docs/ci/update_pytorch_version.md
22	27	docs/cli/README.md
31	27	docs/configuration/conserving_memory.md
5	3	docs/configuration/env_vars.md
16	14	docs/contributing/README.md
29	27	docs/contributing/model/basic.md
424	385	docs/contributing/model/multimodal.md
20	20	docs/contributing/profiling.md
15	13	docs/deployment/docker.md
47	45	docs/deployment/frameworks/autogen.md
59	53	docs/deployment/frameworks/cerebrium.md
69	63	docs/deployment/frameworks/dstack.md
23	23	docs/deployment/frameworks/haystack.md
15	13	docs/deployment/frameworks/litellm.md
117	113	docs/deployment/frameworks/lws.md
200	206	docs/deployment/frameworks/skypilot.md
43	41	docs/deployment/integrations/production-stack.md
98	74	docs/deployment/k8s.md
44	40	docs/deployment/nginx.md
55	51	docs/design/arch_overview.md
20	18	docs/design/kernel/paged_attention.md
24	22	docs/design/plugin_system.md
103	97	docs/features/lora.md
360	340	docs/features/multimodal_inputs.md
46	42	docs/features/quantization/auto_awq.md
16	14	docs/features/quantization/bitblas.md
15	13	docs/features/quantization/fp8.md
41	39	docs/features/quantization/gguf.md
48	44	docs/features/quantization/gptqmodel.md
68	62	docs/features/quantization/int4.md
45	39	docs/features/quantization/int8.md
39	35	docs/features/quantization/modelopt.md
75	71	docs/features/quantization/quantized_kvcache.md
122	112	docs/features/quantization/quark.md
23	21	docs/features/quantization/torchao.md
208	196	docs/features/reasoning_outputs.md
133	123	docs/features/spec_decode.md
195	181	docs/features/structured_outputs.md
84	80	docs/features/tool_calling.md
45	41	docs/getting_started/installation/cpu.md
35	31	docs/getting_started/installation/gpu/rocm.inc.md
50	46	docs/getting_started/installation/intel_gaudi.md
38	34	docs/getting_started/quickstart.md
31	29	docs/models/generative_models.md
40	23	docs/models/supported_models.md
16	14	docs/serving/integrations/langchain.md
286	270	docs/serving/openai_compatible_server.md
23	19	docs/usage/metrics.md
91	85	docs/usage/troubleshooting.md
32	30	docs/usage/usage_stats.md

[493c27535] Vensen 2025-06-23 Fix(models/siglip): Add compatibility for Gemma models quantized by llm-compressor (#19643)
1	0	vllm/model_executor/models/gemma3_mm.py

[f39ab2d4b] jinqinn 2025-06-23 [Misc] Configurable timeout for execute_model RPC calls via env var (#19544)
6	0	vllm/envs.py
6	11	vllm/v1/executor/multiproc_executor.py

[4a0f7888a] amit 2025-06-23 [Core] feat: Implement Priority Scheduling in V1 Engine (#19057)
12	0	docs/usage/v1_guide.md
589	1	tests/v1/core/test_scheduler.py
224	0	vllm/v1/core/sched/request_queue.py
61	27	vllm/v1/core/sched/scheduler.py
1	0	vllm/v1/engine/__init__.py
1	2	vllm/v1/engine/processor.py
8	0	vllm/v1/request.py

[c4cf26067] Aaron Pham 2025-06-22 [Perf][CLI] Improve overall startup time (#19941)
5	0	.pre-commit-config.yaml
108	0	tools/check_init_lazy_imports.py
59	16	vllm/__init__.py
16	14	vllm/config.py
14	1	vllm/engine/arg_utils.py
7	2	vllm/entrypoints/cli/benchmark/main.py
8	7	vllm/entrypoints/cli/collect_env.py
28	22	vllm/entrypoints/cli/main.py
9	13	vllm/entrypoints/cli/openai.py
19	12	vllm/entrypoints/cli/run_batch.py
4	7	vllm/entrypoints/cli/serve.py
5	1	vllm/entrypoints/cli/types.py
11	5	vllm/entrypoints/openai/run_batch.py
0	3	vllm/utils.py

[33d51f599] Ye (Charlotte) Qi 2025-06-22 [BugFix] Add an env to disable moe chunking to work around compile incompatibility (#19642)
7	0	vllm/envs.py
5	1	vllm/model_executor/layers/fused_moe/modular_kernel.py

[e91386cde] Aaron Pham 2025-06-22 [Chore] dedup logs (#19955)
1	1	vllm/config.py
0	4	vllm/triton_utils/importing.py

[2c11a29f0] Ye (Charlotte) Qi 2025-06-22 [Misc] Simplify vllm bench cli subcommand implementation (#19948)
12	0	vllm/entrypoints/cli/__init__.py
3	17	vllm/entrypoints/cli/benchmark/base.py
4	13	vllm/entrypoints/cli/benchmark/latency.py
17	24	vllm/entrypoints/cli/benchmark/main.py
4	13	vllm/entrypoints/cli/benchmark/serve.py
4	13	vllm/entrypoints/cli/benchmark/throughput.py

[c76a506bd] Roger Wang 2025-06-22 [Misc] Update model-specific PR tagging (#19949)
1	1	.github/mergify.yml

[ec0db6f51] Reid 2025-06-22 [doc] use snippets for contact us (#19944)
2	1	README.md
1	6	docs/community/contact_us.md

[c305a2109] 22quinn 2025-06-22 [CI/Build] Auto tag perf benchmarks related PRs (#19943)
13	0	.github/mergify.yml

[202c5df93] Wang, Yi 2025-06-22 [Benchmark] fix request loss if "ping" is returned (#19535)
7	1	benchmarks/backend_request_func.py
16	4	vllm/benchmarks/endpoint_request_func.py

[2bb246b8f] Ning Xie 2025-06-22 [MISC] add cpu_kvcache_space_bytes to CacheConfig (#19812)
2	0	vllm/config.py

[4c409cabc] Ning Xie 2025-06-22 [Misc] add vllm_config in __init__ (#19866)
1	0	vllm/worker/worker_base.py

[3b1e4c6a2] Adrian 2025-06-21 [Docs] Add GPT2ForSequenceClassification to supported models in docs (#19932)
1	1	docs/models/supported_models.md

[2c5302fad] Woosuk Kwon 2025-06-21 [Multimodal] Optimize Qwen2/2.5-VL startup time (#19756)
8	0	vllm/model_executor/models/qwen2_vl.py
21	0	vllm/multimodal/processing.py
21	1	vllm/multimodal/profiling.py

[caa680fd2] Reid 2025-06-22 [doc] add contact us in community (#19922)
1	0	README.md
11	0	docs/community/contact_us.md

[c3bf9bad1] 汪志鹏 2025-06-21 [New model support]Support Tarsier2 (#19887)
1	0	docs/models/supported_models.md
32	0	examples/offline_inference/vision_language.py
27	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
88	1	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/registry.py

[6f170f11d] Isotr0py 2025-06-21 [Bugfix] Fix bnb 8bit model weights loading (#19917)
2	2	vllm/model_executor/model_loader/bitsandbytes_loader.py

[8ca81bb06] Rabin Adhikari 2025-06-21 Fix: Check the type of params to be a Sequence not list. (#19910)
3	3	vllm/entrypoints/llm.py

[e773a9e1c] wangxiyuan 2025-06-21 [Misc] Clean up useless code (#19889)
0	11	vllm/config.py

[71baf85ae] Ning Xie 2025-06-21 [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError (#19749)
1	1	vllm/attention/backends/torch_sdpa.py

[79f2f1c2a] Li, Jiang 2025-06-20 [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests (#19901)
6	1	tests/models/language/pooling/test_embedding.py
7	0	vllm/engine/arg_utils.py

[2e3e3c86d] Vlad Tiberiu Mihailescu 2025-06-20 Export NaNs in logits to scheduler_stats if output is corrupted (#18777)
49	0	tests/v1/worker/test_gpu_model_runner.py
8	1	vllm/envs.py
7	0	vllm/v1/core/sched/scheduler.py
2	0	vllm/v1/metrics/stats.py
5	1	vllm/v1/outputs.py
8	0	vllm/v1/request.py
25	0	vllm/v1/worker/gpu_model_runner.py

[7e8977fcd] Chendi.Xue 2025-06-20 [custom_op][vllm-plugin] update custom_op class to use op_registry (#19164)
3	1	tests/plugins/vllm_add_dummy_platform/setup.py
4	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/__init__.py
3	2	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py
20	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_custom_ops.py
20	3	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
14	0	tests/plugins_tests/test_platform_plugins.py
56	0	vllm/model_executor/custom_op.py

[f1e840e84] Adrian 2025-06-20 [Model] GPT2ForSequenceClassification model (#19663)
1	0	tests/models/registry.py
55	1	vllm/model_executor/models/gpt2.py
1	0	vllm/model_executor/models/registry.py

[7771d1de8] Thomas Parnell 2025-06-20 [Fix] import regex instead of re (#19875)
2	1	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[71d121954] Ning Xie 2025-06-20 [Kernel] correct cpu worker function parameter type (#19745)
1	1	vllm/attention/ops/ipex_attn.py
4	4	vllm/worker/cpu_worker.py

[e384f2f10] Reid 2025-06-20 [Misc] refactor example - openai_transcription_client (#19851)
34	5	examples/online_serving/openai_transcription_client.py

[089a306f1] Reid 2025-06-20 [Misc] update cuda version (#19526)
2	1	examples/others/lmcache/cpu_offload_lmcache.py

[5e666f72c] kourosh hakhamaneshi 2025-06-19 [Bugfix][Ray] Set the cuda context eagerly in the ray worker  (#19583)
9	0	.buildkite/test-pipeline.yaml
80	0	tests/cuda/test_cuda_context.py
11	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py

[e3a3e4db4] qli88 2025-06-19 [Bugfix] Enable PP with AITER+V1 (#19822)
0	1	vllm/model_executor/layers/layernorm.py
3	10	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[e41bf15cd] Xerxes 2025-06-20 [Chore]: qwen3-moe-type-hints-mistake (#19860)
1	1	vllm/model_executor/models/qwen3_moe.py

[5aa4a015c] Brayden Zhong 2025-06-20 [Benchmark] Fix `Value of type "SampleRequest" is not indexable` (#18032)
1	1	benchmarks/benchmark_throughput.py
1	1	vllm/benchmarks/throughput.py

[b6bad3d18] Elaine Zhao 2025-06-19 [CI][Neuron] Fail and exit on first error (#19622)
2	1	.buildkite/scripts/hardware_ci/run-neuron-test.sh

[ee9a1531a] Isotr0py 2025-06-20 [CI/Build][Bugfix] Fix deadlock on v1 engine test CI (#19872)
2	1	tests/v1/engine/test_async_llm.py

[10d82f9ac] Robert Shaw 2025-06-19 [Benchmark][Bugfix] Fix Dataset Length Calculation (#19868)
1	1	benchmarks/benchmark_dataset.py

[ea10dd9d9] xzbdmw 2025-06-20 [Frontend] early return chat format resolution when specified (#19735)
4	1	vllm/entrypoints/chat_utils.py

[ead211029] Alex Brooks 2025-06-19 [Core][Bugfix] Fix Online MM Beam Search (#19688)
27	4	tests/entrypoints/openai/test_vision.py
11	2	vllm/engine/protocol.py
7	6	vllm/entrypoints/llm.py

[01220ce89] Li, Jiang 2025-06-19 [CI][CPU] Improve dummy Triton interfaces and fix the CPU CI (#19838)
0	3	requirements/cpu.txt
2	0	vllm/triton_utils/importing.py

[6f68c4922] 22quinn 2025-06-19 [Doc] Update V1 user guide for embedding models (#19842)
6	6	docs/usage/v1_guide.md

[471946064] Alexei-V-Ivanov-AMD 2025-06-19 Fixing Chunked Prefill Test. (#19762)
1	1	.buildkite/test-pipeline.yaml
16	2	tests/basic_correctness/test_chunked_prefill.py

[466166dcf] NekoMimiUnagi 2025-06-19 [Frontend] Add optional token-level progress bar to `LLM.beam_search` (#19301)
14	1	vllm/entrypoints/llm.py

[1d0ae26c8] Zuxin 2025-06-18 Add xLAM tool parser support (#17148)
19	0	docs/features/tool_calling.md
244	0	examples/online_serving/openai_chat_completion_client_with_tools_xlam.py
272	0	examples/online_serving/openai_chat_completion_client_with_tools_xlam_streaming.py
77	0	examples/tool_chat_template_xlam_llama.jinja
66	0	examples/tool_chat_template_xlam_qwen.jinja
246	0	tests/tool_use/test_xlam_tool_parser.py
2	1	vllm/entrypoints/openai/tool_parsers/__init__.py
463	0	vllm/entrypoints/openai/tool_parsers/xlam_tool_parser.py

[602199957] Isotr0py 2025-06-19 [Minor] Allow redirecting model path for HfRunner in test (#19795)
2	0	tests/conftest.py

[c7b370c60] Ning Xie 2025-06-19 raise exception for pin_lora (#19809)
2	3	vllm/worker/worker_base.py

[aa20d10a9] zsolt-borbely-htec 2025-06-19 [Misc] [ROCm] Prevent surplus tensor reshape (#19803)
1	1	vllm/v1/attention/backends/triton_attn.py

[2de12be42] TJian 2025-06-18 [ROCm] [AITER] [Bugfix] Patch for AITER commit `648764942e552a8bb5fe16026703716a81f05374` (#18990)
1	1	docker/Dockerfile.rocm_base
3	2	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[83ca9ae47] Yu-Hang "Maxin" Tang 2025-06-18 Mark invariant normalizer in Gemma as non-persistent (#19788)
20	0	tests/models/language/generation/test_gemma.py
3	1	vllm/model_executor/models/gemma.py
3	1	vllm/model_executor/models/gemma2.py
3	1	vllm/model_executor/models/gemma3.py

[e2148dc5e] kourosh hakhamaneshi 2025-06-18 [Bugfix] Add check_health to v1 async client. (#19821)
29	0	tests/v1/engine/test_async_llm.py
2	0	vllm/v1/engine/async_llm.py

[b1098b407] Lu Fang 2025-06-19 [Bugfix] Fix the linter (#19826)
5	5	vllm/model_executor/models/qwen2_5_omni_thinker.py
5	5	vllm/model_executor/models/qwen2_5_vl.py
5	5	vllm/model_executor/models/qwen2_vl.py

[799397ee4] Maximilien de Bayser 2025-06-19 Support embedding models in V1 (#16188)
4	1	examples/offline_inference/basic/embed.py
1	0	examples/offline_inference/vision_language_embedding.py
18	14	tests/compile/test_basic_correctness.py
3	0	tests/conftest.py
20	4	tests/entrypoints/llm/test_encode.py
8	0	tests/entrypoints/openai/test_embedding.py
11	4	tests/entrypoints/openai/test_pooling.py
8	0	tests/entrypoints/openai/test_rerank.py
9	0	tests/entrypoints/openai/test_score.py
9	1	tests/models/language/pooling/test_classification.py
27	7	tests/models/language/pooling/test_embedding.py
11	11	tests/models/registry.py
1	0	tests/tokenization/test_detokenize.py
1	0	tests/v1/core/test_kv_cache_utils.py
1	0	tests/v1/core/test_prefix_caching.py
22	4	tests/v1/core/test_scheduler.py
1	0	tests/v1/engine/test_engine_core.py
1	0	tests/v1/engine/test_engine_core_client.py
1	0	tests/v1/engine/test_fast_incdec_prefix_err.py
9	4	tests/v1/engine/test_output_processor.py
2	0	tests/v1/kv_connector/unit/utils.py
3	1	tests/v1/worker/test_gpu_input_batch.py
1	0	tests/v1/worker/test_gpu_model_runner.py
23	3	vllm/config.py
31	14	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/llm.py
3	1	vllm/entrypoints/openai/serving_pooling.py
116	33	vllm/model_executor/layers/pooler.py
2	2	vllm/model_executor/models/bert.py
3	2	vllm/model_executor/models/modernbert.py
6	1	vllm/model_executor/models/qwen3.py
7	0	vllm/pooling_params.py
2	1	vllm/v1/core/kv_cache_manager.py
4	1	vllm/v1/core/sched/output.py
25	3	vllm/v1/core/sched/scheduler.py
14	1	vllm/v1/core/sched/utils.py
6	1	vllm/v1/engine/__init__.py
73	7	vllm/v1/engine/async_llm.py
0	1	vllm/v1/engine/core.py
7	3	vllm/v1/engine/detokenizer.py
2	2	vllm/v1/engine/llm_engine.py
1	0	vllm/v1/engine/logprobs.py
93	44	vllm/v1/engine/output_processor.py
19	14	vllm/v1/engine/processor.py
3	2	vllm/v1/metrics/loggers.py
0	1	vllm/v1/metrics/stats.py
4	0	vllm/v1/outputs.py
0	0	vllm/v1/pool/__init__.py
16	0	vllm/v1/pool/metadata.py
30	13	vllm/v1/request.py
3	1	vllm/v1/structured_output/__init__.py
101	70	vllm/v1/worker/gpu_input_batch.py
115	11	vllm/v1/worker/gpu_model_runner.py
8	3	vllm/v1/worker/gpu_worker.py
1	0	vllm/v1/worker/tpu_input_batch.py
4	0	vllm/v1/worker/tpu_model_runner.py

[495991508] Jee Jee Li 2025-06-19 [Quantization] Modify the logic of BNB double quantization (#19742)
25	3	vllm/model_executor/model_loader/bitsandbytes_loader.py

[8d1e89d94] Lu Fang 2025-06-19 [Misc][ROCm] Enforce no unused variable in ROCm C++ files (#19796)
1	0	cmake/utils.cmake

[36239f79d] Michael Goin 2025-06-19 Fix FA2 fallback for Blackwell V1 (#19781)
1	1	vllm/platforms/cuda.py

[dfada85ee] afeldman-nm 2025-06-18 [Frontend] Expose custom args in OpenAI APIs (#16862)
1	1	benchmarks/kernels/benchmark_moe_align_block_size.py
41	11	vllm/entrypoints/openai/protocol.py
2	2	vllm/sampling_params.py

[ed3334973] Richard Zou 2025-06-18 [BugFix] Fix use_cudagraph=False (#19612)
21	24	tests/compile/test_config.py
3	0	vllm/compilation/counter.py
11	5	vllm/v1/worker/gpu_model_runner.py

[d49adea1f] Woosuk Kwon 2025-06-18 [Multimodal] Use fast processor for Qwen2/2.5-VL (#19789)
1	1	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
2	1	vllm/model_executor/models/qwen2_vl.py

[14fdd21d3] Russell Bryant 2025-06-18 [Core] More fixes to MultiModalEmbeddings type handling (#19715)
2	1	vllm/model_executor/models/aria.py
2	1	vllm/model_executor/models/aya_vision.py
2	1	vllm/model_executor/models/blip2.py
2	1	vllm/model_executor/models/chameleon.py
2	1	vllm/model_executor/models/deepseek_vl2.py
2	1	vllm/model_executor/models/florence2.py
2	1	vllm/model_executor/models/fuyu.py
2	1	vllm/model_executor/models/gemma3_mm.py
2	1	vllm/model_executor/models/glm4v.py
2	1	vllm/model_executor/models/granite_speech.py
2	1	vllm/model_executor/models/idefics3.py
2	1	vllm/model_executor/models/internvl.py
2	1	vllm/model_executor/models/kimi_vl.py
2	1	vllm/model_executor/models/llava.py
2	1	vllm/model_executor/models/llava_next.py
2	1	vllm/model_executor/models/llava_next_video.py
2	1	vllm/model_executor/models/llava_onevision.py
2	1	vllm/model_executor/models/minicpmv.py
2	1	vllm/model_executor/models/minimax_vl_01.py
2	1	vllm/model_executor/models/mistral3.py
2	1	vllm/model_executor/models/mllama4.py
2	1	vllm/model_executor/models/molmo.py
2	1	vllm/model_executor/models/ovis.py
2	1	vllm/model_executor/models/paligemma.py
2	1	vllm/model_executor/models/phi3v.py
2	1	vllm/model_executor/models/phi4mm.py
2	1	vllm/model_executor/models/pixtral.py
3	2	vllm/model_executor/models/qwen2_5_omni_thinker.py
2	1	vllm/model_executor/models/qwen2_5_vl.py
2	1	vllm/model_executor/models/qwen2_audio.py
2	1	vllm/model_executor/models/qwen2_vl.py
2	1	vllm/model_executor/models/qwen_vl.py
2	1	vllm/model_executor/models/skyworkr1v.py
2	1	vllm/model_executor/models/tarsier.py
2	1	vllm/model_executor/models/ultravox.py

[04fefe7c9] QiliangCui 2025-06-18 [TPU] Update torch-xla version to include paged attention tuned block change (#19813)
5	5	requirements/tpu.txt

[3b523e38d] Lukas Geiger 2025-06-18 [Core] Do not copy array during hashing (#19484)
12	0	tests/multimodal/test_hasher.py
12	10	vllm/multimodal/hasher.py
1	1	vllm/v1/serial_utils.py

[16c16301c] afeldman-nm 2025-06-18 Disable "Forbid direct 'import triton'" check for `vllm/triton_utils/importing.py` in an extensible way (#19783)
10	0	tools/check_triton_import.py

[9206d0ff0] Nathan Weinberg 2025-06-18 docs: fix Slack bulletpoint in README (#19811)
1	1	README.md

[a89209b78] Chen Zhang 2025-06-19 [v1] Support mamba2 (#19327)
42	11	tests/models/language/generation/test_hybrid.py
1	1	tests/v1/test_oracle.py
6	1	vllm/engine/arg_utils.py
175	60	vllm/model_executor/layers/mamba/mamba_mixer2.py
33	21	vllm/model_executor/models/mamba2.py
192	0	vllm/v1/attention/backends/mamba_attn.py
42	1	vllm/v1/core/single_type_kv_cache_manager.py
24	0	vllm/v1/kv_cache_interface.py
68	26	vllm/v1/worker/gpu_model_runner.py

[ffacb222c] Russell Bryant 2025-06-18 [Docs] Add Huzaifa Sidhpurwala to vuln mgmt team doc (#19808)
1	0	docs/contributing/vulnerability_management.md

[12575cfa7] Chauncey 2025-06-19 [Bugfix] fix RAY_CGRAPH_get_timeout is not set successfully (#19725)
10	10	vllm/executor/ray_distributed_executor.py

[8b6e1d639] Zzz9990 2025-06-18 [Hardware][AMD] integrate aiter chunked prefill into vllm (#18596)
8	0	vllm/envs.py
9	3	vllm/platforms/rocm.py
585	0	vllm/v1/attention/backends/rocm_aiter_fa.py

[735a9de71] Lu Fang 2025-06-18 [Qwen] Add tagging rule for Qwen related PRs (#19799)
15	0	.github/mergify.yml

[257ab9543] wangxiyuan 2025-06-18 [Platform] Allow platform use V1 Engine by default (#19792)
5	9	vllm/engine/arg_utils.py
8	0	vllm/platforms/cpu.py
7	0	vllm/platforms/interface.py

[cca91a7a1] Reid 2025-06-18 [doc] fix the incorrect label (#19787)
1	1	docs/getting_started/installation/gpu.md
0	2	docs/getting_started/installation/gpu/cuda.inc.md

[f04d60456] Woosuk Kwon 2025-06-17 [Minor] Zero-initialize attn output buffer (#19784)
1	1	vllm/attention/layer.py

[19a53b278] afeldman-nm 2025-06-18 [V1] Decouple GPU and TPU `InputBatch` (#19778)
1	1	vllm/v1/sample/tpu/metadata.py
6	1	vllm/v1/worker/gpu_input_batch.py
5	1	vllm/v1/worker/lora_model_runner_mixin.py
584	0	vllm/v1/worker/tpu_input_batch.py
1	1	vllm/v1/worker/tpu_model_runner.py

[eccdc8318] Zhonghua Deng 2025-06-18 [V1][P/D] An native implementation of xPyD based on P2P NCCL (#18242)
337	0	docs/design/v1/p2p_nccl_connector.md
154	0	examples/online_serving/disagg_xpyd/disagg_prefill_proxy_xpyd.py
8	0	vllm/distributed/device_communicators/pynccl_wrapper.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
0	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/__init__.py
481	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
531	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
264	0	vllm/distributed/kv_transfer/kv_connector/v1/p2p/tensor_memory_pool.py

[5f52a8468] Russell Bryant 2025-06-18 [V1] Add API docs for EncoderCacheManager (#19294)
106	2	vllm/v1/core/encoder_cache_manager.py

[d4629dc43] lkchen 2025-06-17 [Misc] Add __str__ for RequestStatus (#19780)
15	0	tests/v1/test_request.py
3	0	vllm/v1/request.py

[6e9cc73f6] Ning Xie 2025-06-18 [MISC] correct DeviceConfig device field static type analysis (#19699)
5	2	vllm/config.py
2	1	vllm/engine/arg_utils.py

[c53711bd6] Ning Xie 2025-06-18 [MISC] correct copy_blocks src_to_dists param type (#19696)
2	2	vllm/attention/ops/ipex_attn.py

[dac8cc49f] Chenyaaang 2025-06-17 [TPU] Update torch version to include paged attention kernel change (#19706)
5	5	requirements/tpu.txt

[a44b1c951] Charlie Fu 2025-06-17 [Feature][ROCm] Add full graph capture support for TritonAttentionBackend (#19158)
1	0	tests/compile/piecewise/test_full_cudagraph.py
3	2	vllm/platforms/rocm.py
3	169	vllm/v1/attention/backends/flash_attn.py
159	7	vllm/v1/attention/backends/triton_attn.py
168	0	vllm/v1/attention/backends/utils.py

[b447624ee] Michael Goin 2025-06-18 [Bugfix] Fix faulty triton importing logic when using Ray for DP (#19734)
17	1	vllm/triton_utils/importing.py

[cda92307c] Jiayi Yao 2025-06-17 [Misc] Update lmcache connector with the latest connector apis (#19441)
34	1	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py

[bf57ccc5c] Michael Goin 2025-06-18 Remove sm120 arch from sm100 cutlass kernel arch list (#19716)
3	3	CMakeLists.txt

[ffb2cd6b5] Wentao Ye 2025-06-17 [Perf] Optimize `moe_align_block_size` CUDA kernel (#19572)
159	0	benchmarks/kernels/benchmark_moe_align_block_size.py
133	317	csrc/moe/moe_align_sum_kernels.cu
0	6	csrc/moe/moe_ops.h
0	9	csrc/moe/torch_bindings.cpp
90	0	tests/kernels/moe/test_moe_align_block_size.py
0	9	vllm/_custom_ops.py
4	24	vllm/model_executor/layers/fused_moe/moe_align_block_size.py

[ca94d7fa0] Isotr0py 2025-06-17 [Bugfix] Update multimodel models mapping to fit new checkpoint after Transformers v4.52 (#19151)
86	0	tests/models/multimodal/test_mapping.py
5	0	vllm/model_executor/models/aria.py
13	3	vllm/model_executor/models/aya_vision.py
8	1	vllm/model_executor/models/fuyu.py
13	3	vllm/model_executor/models/gemma3_mm.py
13	3	vllm/model_executor/models/llava.py
13	3	vllm/model_executor/models/llava_next.py
14	3	vllm/model_executor/models/llava_next_video.py
14	3	vllm/model_executor/models/llava_onevision.py
13	3	vllm/model_executor/models/mistral3.py
99	50	vllm/model_executor/models/mllama.py
13	3	vllm/model_executor/models/paligemma.py

[5a1c2e15d] CYJiang 2025-06-17 [Mis] remove duplicate engine status checks (#19647)
3	5	vllm/v1/engine/core_client.py

[4c8f64faa] Nicolò Lucchesi 2025-06-17 [V1][Kernel] Flashinfer HND KV cache layout (#19280)
1	3	vllm/attention/backends/flashinfer.py
5	4	vllm/distributed/kv_transfer/kv_connector/utils.py
11	0	vllm/envs.py
5	7	vllm/v1/attention/backends/flash_attn.py
21	6	vllm/v1/attention/backends/flashinfer.py
21	0	vllm/v1/attention/backends/utils.py

[93aee29fd] David Xia 2025-06-17 [doc] split "Other AI Accelerators" tabs (#19708)
3	1	docs/getting_started/installation/.nav.yml
3	4	docs/getting_started/installation/README.md
0	117	docs/getting_started/installation/ai_accelerator.md
38	46	docs/getting_started/installation/{ai_accelerator/neuron.inc.md => aws_neuron.md}
11	25	docs/getting_started/installation/{ai_accelerator/tpu.inc.md => google_tpu.md}
14	24	docs/getting_started/installation/{ai_accelerator/hpu-gaudi.inc.md => intel_gaudi.md}

[154d063b9] Reid 2025-06-17 [doc][mkdocs] Add edit  button to documentation (#19637)
47	0	docs/mkdocs/javascript/edit_and_feedback.js
37	0	docs/mkdocs/stylesheets/extra.css
3	0	mkdocs.yaml

[ccd7c0508] jvlunteren 2025-06-17 [Kernel] Add Split-KV Support to Unified Triton Attention Kernel (#19152)
456	52	vllm/attention/ops/triton_unified_attention.py

[c48c6c400] Huy Do 2025-06-17 Add a doc on how to update PyTorch version (#19705)
134	0	docs/ci/update_pytorch_version.md

[aed846864] Isotr0py 2025-06-17 [Doc] Add missing llava family multi-image examples (#19698)
103	0	examples/offline_inference/vision_language_multi_image.py

[5c76b9cda] quanliu 2025-06-17 [Core] add remove_seq_from_computed_blocks_tracker to BlockSpaceManager (#19686)
4	0	vllm/core/interfaces.py
3	0	vllm/core/placeholder_block_space_manager.py

[ddfed314f] Driss Guessous 2025-06-16 Fixes IMA for TP w/ flex-attention (#19712)
0	2	tests/kernels/test_flex_attention.py
2	8	vllm/v1/attention/backends/flex_attention.py

[5b3ad5ecf] Di Liu 2025-06-17 [DOC] fix doc typos (#19600)
4	4	vllm/model_executor/layers/vocab_parallel_embedding.py

[ede5c4ebd] nguyenhoangthuan99 2025-06-17 [Frontend] add chunking audio for > 30s audio (#19597)
23	13	tests/entrypoints/openai/test_transcription_validation.py
145	82	vllm/entrypoints/openai/serving_transcription.py

[07334959d] Lucas Wilkinson 2025-06-16 [Wheel Size] Only build FA2 8.0+PTX (#19336)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[119f68394] David Xia 2025-06-16 [doc] add project flag to gcloud TPU command (#19664)
2	2	docs/getting_started/installation/ai_accelerator/tpu.inc.md

[0860087af] Conroy Cheers 2025-06-17 [Fix] Fall back to Gloo when NCCL backend is unavailable (#19641)
7	0	vllm/distributed/parallel_state.py

[6bc7b5731] Dipika Sikka 2025-06-16 [Quantization] Remove FP4 emulation; Fall-back to marlin for device < 100 (#19563)
7	1	tests/quantization/test_compressed_tensors.py
8	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
13	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
22	57	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
29	0	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py

[90f9c2eb5] Russell Bryant 2025-06-16 [V1] Change return type on get_multimodal_embeddings() (#19446)
3	3	vllm/model_executor/models/aria.py
3	3	vllm/model_executor/models/aya_vision.py
3	3	vllm/model_executor/models/blip2.py
3	3	vllm/model_executor/models/chameleon.py
3	3	vllm/model_executor/models/deepseek_vl2.py
3	3	vllm/model_executor/models/florence2.py
3	3	vllm/model_executor/models/fuyu.py
3	3	vllm/model_executor/models/gemma3_mm.py
3	3	vllm/model_executor/models/glm4v.py
2	1	vllm/model_executor/models/granite_speech.py
3	3	vllm/model_executor/models/idefics3.py
2	2	vllm/model_executor/models/interfaces.py
3	2	vllm/model_executor/models/internvl.py
3	3	vllm/model_executor/models/llava.py
4	4	vllm/model_executor/models/llava_next.py
3	3	vllm/model_executor/models/llava_next_video.py
3	2	vllm/model_executor/models/llava_onevision.py
3	3	vllm/model_executor/models/minicpmv.py
3	3	vllm/model_executor/models/minimax_vl_01.py
3	3	vllm/model_executor/models/mistral3.py
2	3	vllm/model_executor/models/mllama4.py
3	3	vllm/model_executor/models/molmo.py
3	3	vllm/model_executor/models/ovis.py
3	3	vllm/model_executor/models/paligemma.py
4	4	vllm/model_executor/models/phi3v.py
3	2	vllm/model_executor/models/phi4mm.py
3	3	vllm/model_executor/models/pixtral.py
3	3	vllm/model_executor/models/qwen2_5_omni_thinker.py
3	3	vllm/model_executor/models/qwen2_5_vl.py
3	3	vllm/model_executor/models/qwen2_audio.py
3	2	vllm/model_executor/models/qwen2_vl.py
3	3	vllm/model_executor/models/qwen_vl.py
3	3	vllm/model_executor/models/skyworkr1v.py
3	3	vllm/model_executor/models/tarsier.py
3	3	vllm/model_executor/models/ultravox.py
2	2	vllm/model_executor/models/whisper.py
2	1	vllm/v1/worker/utils.py

[387bdf0ab] qscqesze 2025-06-17 [Model] Add support for MiniMaxM1ForCausalLM (shares architecture with MiniMaxText01ForCausalLM) (#19677)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py

[5e5baa91a] bnellnm 2025-06-16 [Kernels] Use empty for modular MoE workspaces (#19667)
3	0	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	2	vllm/model_executor/layers/fused_moe/modular_kernel.py

[836d4ce14] Chauncey 2025-06-16 [Bugfix] fix missing 'finish_reason': null in streaming chat (#19662)
1	1	vllm/entrypoints/openai/serving_chat.py

[c3fec47bb] Ning Xie 2025-06-16 [MISC] bump huggingface_hub pkg to 0.33.0 (#19547)
1	1	requirements/common.txt
1	1	requirements/test.in
2	2	requirements/test.txt

[1173804dc] Isotr0py 2025-06-16 [Bugfix] Fix TP inference for Flex attention backend (#19657)
35	1	tests/v1/engine/test_engine_core.py
7	1	vllm/v1/attention/backends/flex_attention.py
2	0	vllm/v1/engine/core.py
5	0	vllm/v1/worker/gpu_worker.py
5	0	vllm/v1/worker/tpu_worker.py

[4d5424029] Shawn Tan 2025-06-16 [Feature]:Allow for Granite MoE Hybrid models with _only_ shared experts. (#19652)
40	24	vllm/model_executor/models/granitemoehybrid.py

[3e7506975] Navanit Dubey 2025-06-16 [DOC] Add reasoning capability to vLLM streamlit code (#19557)
165	43	examples/online_serving/streamlit_openai_chatbot_webserver.py

[ee35e96ac] Nick Hill 2025-06-16 [BugFix] Don't catch BaseException when dumping execute_model errors (#19626)
7	11	vllm/logging_utils/dump_input.py
5	2	vllm/v1/engine/core.py

[dec66d253] Szymon Ożóg 2025-06-16 [Kernel] GGUF MMVQ kernel for multiple input vectors (#18754)
24	23	csrc/quantization/gguf/gguf_kernel.cu
65	62	csrc/quantization/gguf/mmvq.cuh
1	1	vllm/_custom_ops.py
5	1	vllm/model_executor/layers/quantization/gguf.py

[8d120701f] Russell Bryant 2025-06-16 [Docs] Move multiproc doc to v1 dir (#19651)
0	0	docs/design/{ => v1}/multiprocessing.md

[f40f763f1] wang.yuqi 2025-06-16 [CI] Add mteb testing for rerank models (#19344)
1	1	requirements/test.in
6	0	requirements/test.txt
9	3	tests/conftest.py
6	10	tests/entrypoints/openai/correctness/{test_mteb.py => test_mteb_embed.py}
59	0	tests/entrypoints/openai/correctness/test_mteb_score.py
183	9	tests/models/language/pooling/mteb_utils.py
23	2	tests/models/language/pooling/test_baai.py
18	0	tests/models/language/pooling/test_cross_encoder.py
16	62	tests/models/language/pooling/test_jina.py
84	80	tests/models/language/pooling/test_qwen3_reranker.py
0	73	tests/models/language/pooling/test_qwen3_reranker_seq_cls.py
7	0	tests/models/utils.py
11	1	vllm/model_executor/layers/pooler.py
4	9	vllm/model_executor/models/bert.py
1	6	vllm/model_executor/models/bert_with_rope.py

[26bc46ef8] Ning Xie 2025-06-16 [MISC] typo fix (#19672)
1	1	vllm/config.py

[a77aea59f] Chengji Yao 2025-06-15 [TPU] support attention head dim smaller than 128 (#19620)
37	0	tests/v1/tpu/test_basic.py
28	7	vllm/v1/attention/backends/pallas.py

[b692e9cd0] Ye (Charlotte) Qi 2025-06-15 [Misc] Fix skipped max-model-len validation when deriving max model length from tokenizer config (#19660)
28	0	tests/test_config.py
15	13	vllm/config.py

[367871a46] Francesco Bertolotti 2025-06-16 [Misc][Frontend] passthrough `bad_words` (#19564)
2	0	vllm/entrypoints/openai/protocol.py

[92183b41f] quanliu 2025-06-16 [Bugfix][Core] Prefix caching causes incorrect outputs due to outdated ComputedBlocksTracker (#18957)
294	0	tests/core/test_scheduler.py
4	0	vllm/core/block_manager.py
33	0	vllm/core/scheduler.py

[c6703d1e0] Lu Fang 2025-06-16 [MISC] Remove unused variableds in C++ (#19609)
1	4	csrc/attention/paged_attention_v1.cu
1	4	csrc/attention/paged_attention_v2.cu
0	1	csrc/prepare_inputs/advance_step.cu
0	2	csrc/quantization/fp8/amd/quant_utils.cuh
0	8	csrc/quantization/gptq/q_gemm.cu
0	20	csrc/rocm/attention.cu

[a5e7242d5] Isotr0py 2025-06-16 [Misc] Remove duplicate multiproc method setting for CPU platform (#19649)
0	3	vllm/platforms/cpu.py

[91b2c17a5] Richard Zou 2025-06-15 [CI/Build] Fix torch nightly CI dependencies part 2 (#19589)
1	0	requirements/nightly_torch_test.txt

[055915e6c] Woosuk Kwon 2025-06-15 Enable prefix caching with full cuda graphs (#19617)
0	1	vllm/config.py

[3d330c4c0] Wentao Ye 2025-06-15 [Benchmark] Refactor benchmark script for fp8 & int8 (#19627)
92	157	benchmarks/kernels/bench_fp8_gemm.py
92	123	benchmarks/kernels/bench_int8_gemm.py

[0b73736a0] 22quinn 2025-06-14 [Kernel] Raise verbose error and consolidate `num_heads/num_kv_heads` divisibility check (#19339)
16	0	tests/kernels/attention/test_attention.py
1	3	vllm/attention/backends/blocksparse_attn.py
0	1	vllm/attention/backends/dual_chunk_flash_attn.py
0	1	vllm/attention/backends/flash_attn.py
0	1	vllm/attention/backends/flashinfer.py
0	1	vllm/attention/backends/hpu_attn.py
0	1	vllm/attention/backends/ipex_attn.py
1	2	vllm/attention/backends/pallas.py
0	1	vllm/attention/backends/rocm_flash_attn.py
0	1	vllm/attention/backends/torch_sdpa.py
0	1	vllm/attention/backends/xformers.py
6	1	vllm/attention/layer.py
0	1	vllm/v1/attention/backends/flash_attn.py
0	1	vllm/v1/attention/backends/flashinfer.py
0	1	vllm/v1/attention/backends/flex_attention.py
0	1	vllm/v1/attention/backends/pallas.py
0	1	vllm/v1/attention/backends/triton_attn.py

[ee1531bc3] Lu Fang 2025-06-15 [Bugfix][2/n] Fix speculative decoding CI - Fix test_ngram_e2e_greedy_correctness (#19644)
10	1	tests/spec_decode/e2e/test_integration.py
17	1	tests/spec_decode/e2e/test_logprobs.py
3	0	tests/spec_decode/e2e/test_mlp_correctness.py
18	0	tests/spec_decode/e2e/test_ngram_correctness.py
2	1	vllm/model_executor/models/eagle.py

[e13945f9d] Ilya Markov 2025-06-15 [Perf] Further tunings for SM100 FP8 CUTLASS kernel (#19566)
25	5	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh

[08500011d] maobaolong 2025-06-15 [Fix] Convert kv_transfer_config from dict to KVTransferConfig (#19262)
18	0	vllm/entrypoints/llm.py

[861a0a0a3] Konrad Zawora 2025-06-14 [Bugfix] Don't attempt to use triton if no driver is active (#19561)
30	0	vllm/triton_utils/importing.py

[bc956b38d] Huy Do 2025-06-14 Only build CUTLASS MoE kernels on Hopper (#19648)
2	2	CMakeLists.txt

[294fc1e2c] jiahanc 2025-06-14 [Hardware][NVIDIA][kernel] Fp4 MOE quant kernel optimization (#19500)
226	48	csrc/quantization/fp4/nvfp4_experts_quant.cu

[2db9044ab] Isotr0py 2025-06-14 [Bugfix] Fix auto dtype casting for BatchFeature (#19316)
16	9	tests/v1/engine/test_async_llm.py
13	9	tests/v1/engine/test_engine_core.py
35	28	tests/v1/engine/test_engine_core_client.py
3	1	vllm/inputs/registry.py
4	5	vllm/model_executor/models/qwen2_5_vl.py
4	5	vllm/model_executor/models/qwen2_vl.py
10	0	vllm/utils.py

[6fa718a46] Reid 2025-06-14 [Misc] Modularize CLI Argument Parsing in Benchmark Scripts (#19593)
7	1	benchmarks/benchmark_latency.py
7	1	benchmarks/benchmark_long_document_qa_throughput.py
7	1	benchmarks/benchmark_prefix_caching.py
7	1	benchmarks/benchmark_prioritization.py
6	2	benchmarks/benchmark_serving.py
6	1	benchmarks/benchmark_serving_structured_output.py
7	1	benchmarks/benchmark_throughput.py

[06be85882] Lu Fang 2025-06-14 [Bugfix] Fix the speculative decoding test by setting the target dtype (#19633)
24	0	tests/spec_decode/e2e/test_multistep_correctness.py

[d1e34cc9a] Saheli Bhattacharjee 2025-06-14 [V1][Metrics] Deprecate metrics with gpu_ prefix for non GPU specific metrics. (#18354)
1	1	vllm/v1/core/sched/scheduler.py
39	6	vllm/v1/metrics/loggers.py
1	1	vllm/v1/metrics/stats.py

[bd517eb9f] Nick Hill 2025-06-13 [BugFix] Fix DP Coordinator incorrect debug log message (#19624)
13	10	vllm/v1/engine/coordinator.py

[d65668b4e] Concurrensee 2025-06-13 Adding "AMD: Multi-step Tests" to amdproduction. (#19508)
1	1	.buildkite/test-pipeline.yaml
10	0	tests/multi_step/test_correctness_llm.py

[aafbbd981] Woosuk Kwon 2025-06-13 [torch.compile] Use custom ops when use_inductor=False (#19618)
15	3	vllm/config.py

[0f0874515] Anna Pendleton 2025-06-13 [Doc] Add troubleshooting section to k8s deployment (#19377)
24	10	docs/deployment/k8s.md

[3597b06a4] Luka Govedič 2025-06-13 [CUDA] Enable full cudagraph for FlashMLA (#18581)
107	51	tests/compile/piecewise/test_full_cudagraph.py
5	11	tests/compile/piecewise/test_simple.py
21	24	tests/compile/piecewise/test_toy_llama.py
21	9	tests/utils.py
5	1	vllm/compilation/cuda_piecewise_backend.py
2	1	vllm/entrypoints/llm.py
12	6	vllm/forward_context.py
8	4	vllm/v1/attention/backends/cpu_attn.py
18	11	vllm/v1/attention/backends/flash_attn.py
7	4	vllm/v1/attention/backends/flashinfer.py
9	13	vllm/v1/attention/backends/flex_attention.py
36	8	vllm/v1/attention/backends/mla/common.py
31	3	vllm/v1/attention/backends/mla/flashmla.py
1	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
71	2	vllm/v1/attention/backends/utils.py
6	4	vllm/v1/spec_decode/eagle.py
93	67	vllm/v1/worker/gpu_model_runner.py

[1015296b7] Reid 2025-06-14 [doc][mkdocs] fix the  duplicate Supported features sections in GPU docs (#19606)
4	1	docs/getting_started/installation/gpu/cuda.inc.md
4	1	docs/getting_started/installation/gpu/rocm.inc.md
4	1	docs/getting_started/installation/gpu/xpu.inc.md

[ce9dc02c9] Wentao Ye 2025-06-13 [Refactor] Remove unused variables in `moe_permute_unpermute_kernel.inl` (#19573)
0	5	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.inl

[a24cb9160] qscqesze 2025-06-13 [Model] Fix minimax model cache & lm_head precision (#19592)
3	3	vllm/model_executor/models/minimax_text_01.py

[7e8d97dd3] Nick Hill 2025-06-13 [BugFix] Honor `enable_caching` in connector-delayed kvcache load case (#19435)
5	4	vllm/v1/core/kv_cache_manager.py
1	0	vllm/v1/core/sched/scheduler.py

[d70bc7c02] youkaichao 2025-06-13 [torch.compile] reorganize the cache directory to support compiling multiple models (#19064)
56	9	vllm/compilation/backends.py
29	9	vllm/compilation/compiler_interface.py
17	2	vllm/config.py
6	2	vllm/model_executor/model_loader/utils.py
4	2	vllm/v1/spec_decode/eagle.py
5	3	vllm/v1/spec_decode/medusa.py

[ce688ad46] Boyuan Feng 2025-06-13 use base version for version comparison (#19587)
1	1	vllm/compilation/backends.py
2	2	vllm/model_executor/layers/quantization/torchao.py

[cefdb9962] 汪志鹏 2025-06-13 [Fix] The zip function in Python 3.9 does not have the strict argument (#19549)
2	4	vllm/v1/worker/gpu_model_runner.py
2	4	vllm/v1/worker/tpu_model_runner.py

[ace5cdaff] 汪志鹏 2025-06-13 [Fix] bump mistral common to support magistral (#19533)
1	1	requirements/common.txt
1	1	requirements/test.in
1	1	requirements/test.txt

[645872110] Li, Jiang 2025-06-13 [CPU] Refine default config for the CPU backend (#19539)
12	3	.buildkite/scripts/hardware_ci/run-cpu-test.sh
22	4	vllm/engine/arg_utils.py
14	6	vllm/platforms/cpu.py
12	12	vllm/v1/worker/cpu_model_runner.py

[bb4a0dece] Hyogeun Oh (오효근) 2025-06-13 [Misc] Correct broken docs link (#19553)
3	4	vllm/entrypoints/cli/serve.py

[c707cfc12] Reid 2025-06-13 [doc] fix incorrect link (#19586)
1	1	docs/design/multiprocessing.md

[7b3c9ff91] Aaron Pham 2025-06-12 [Doc] uses absolute links for structured outputs (#19582)
3	3	docs/features/structured_outputs.md
1	1	examples/online_serving/structured_outputs/README.md

[c68698b32] qizixi 2025-06-12 [Bugfix] Fix EAGLE vocab embedding for multimodal target model (#19570)
12	7	vllm/v1/spec_decode/eagle.py

[e3b12667d] Varun Sundar Rabindranath 2025-06-12 [BugFix] : Fix Batched DeepGemm Experts (#19515)
11	8	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
4	3	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
2	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py
5	8	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
7	3	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
2	1	vllm/model_executor/layers/fused_moe/fused_moe.py
10	5	vllm/model_executor/layers/fused_moe/modular_kernel.py
6	0	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
5	3	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py

[e6aab5de2] kourosh hakhamaneshi 2025-06-12 Revert "[Build/CI] Add tracing deps to vllm container image (#15224)" (#19378)
5	0	.buildkite/test-pipeline.yaml
0	4	requirements/common.txt

[c57bb199b] Russell Bryant 2025-06-12 [V1] Resolve failed concurrent structured output requests (#19565)
8	1	vllm/v1/worker/gpu_model_runner.py

[dba68f915] Aaron Pham 2025-06-12 [Doc] Unify structured outputs examples (#18196)
0	45	docs/features/reasoning_outputs.md
63	17	docs/features/structured_outputs.md
0	175	examples/online_serving/openai_chat_completion_structured_outputs.py
0	87	examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py
0	167	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
54	0	examples/online_serving/structured_outputs/README.md
8	0	examples/online_serving/structured_outputs/pyproject.toml
272	0	examples/online_serving/structured_outputs/structured_outputs.py

[a3319f4f0] Michael Goin 2025-06-12 [Bugfix] Enforce contiguous input for dynamic_per_token FP8/INT8 quant (#19452)
3	3	vllm/_custom_ops.py

[9d880f594] Varun Sundar Rabindranath 2025-06-12 [Misc] Turn MOE_DP_CHUNK_SIZE into an env var (#19506)
9	0	vllm/envs.py
9	8	vllm/model_executor/layers/fused_moe/layer.py

[017ef648e] Ekagra Ranjan 2025-06-12 [Spec Decode][Benchmark] Generalize spec decode offline benchmark to more methods and datasets (#18847)
4	0	examples/offline_inference/eagle.py
137	0	examples/offline_inference/spec_decode.py
2	0	tests/benchmarks/test_serve_cli.py
252	0	vllm/benchmarks/datasets.py
8	223	vllm/benchmarks/serve.py

[4b25ab14e] Reid 2025-06-12 [doc] Make top navigation sticky (#19540)
1	0	mkdocs.yaml

[f98548b9d] Luka Govedič 2025-06-12 [torch.compile][ROCm] Fuse quantization onto attention using a torch.compile pass (#16756)
1	0	.buildkite/test-pipeline.yaml
16	14	tests/compile/backend.py
1	2	tests/compile/test_async_tp.py
5	7	tests/compile/test_fusion.py
131	0	tests/compile/test_fusion_attn.py
7	1	vllm/_custom_ops.py
17	0	vllm/attention/backends/abstract.py
6	0	vllm/attention/backends/blocksparse_attn.py
9	0	vllm/attention/backends/dual_chunk_flash_attn.py
6	0	vllm/attention/backends/flash_attn.py
6	0	vllm/attention/backends/flashinfer.py
6	0	vllm/attention/backends/hpu_attn.py
6	0	vllm/attention/backends/ipex_attn.py
6	0	vllm/attention/backends/mla/common.py
6	0	vllm/attention/backends/pallas.py
37	4	vllm/attention/backends/rocm_flash_attn.py
5	0	vllm/attention/backends/torch_sdpa.py
6	0	vllm/attention/backends/xformers.py
4	1	vllm/attention/layer.py
63	36	vllm/compilation/fusion.py
165	0	vllm/compilation/fusion_attn.py
23	1	vllm/compilation/fx_utils.py
29	1	vllm/compilation/noop_elimination.py
4	0	vllm/compilation/pass_manager.py
3	0	vllm/compilation/vllm_inductor_pass.py
16	11	vllm/config.py
1	1	vllm/envs.py
6	0	vllm/v1/attention/backends/flash_attn.py
6	0	vllm/v1/attention/backends/flashinfer.py
7	0	vllm/v1/attention/backends/flex_attention.py
6	0	vllm/v1/attention/backends/mla/common.py
6	0	vllm/v1/attention/backends/pallas.py
6	0	vllm/v1/attention/backends/triton_attn.py

[96846bb36] mobicham 2025-06-12 Fix TorchAOConfig skip layers (#19265)
15	0	tests/quantization/test_torchao.py
57	7	vllm/model_executor/layers/quantization/torchao.py

[b6efafd9e] Wentao Ye 2025-06-12 [Perf] Vectorize static / dynamic INT8 quant kernels (#19233)
200	0	benchmarks/kernels/bench_int8_gemm.py
139	101	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
75	0	csrc/quantization/vectorization_utils.cuh
1	0	tests/kernels/quantization/test_int8_quant.py

[1129e2b1a] Nicolò Lucchesi 2025-06-12 [V1][NixlConnector] Drop `num_blocks` check  (#19532)
0	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[c742438f8] Cyrus Leung 2025-06-12 [Doc] Add V1 column to supported models list (#19523)
143	141	docs/models/supported_models.md
25	19	docs/usage/v1_guide.md

[73e2e0118] Jee Jee Li 2025-06-12 [Quantization] Improve AWQ logic (#19431)
38	4	vllm/model_executor/layers/quantization/awq.py

[c9280e634] jmswen 2025-06-12 [Bugfix] Respect num-gpu-blocks-override in v1 (#19503)
16	0	tests/v1/core/test_kv_cache_utils.py
1	0	vllm/v1/core/kv_cache_utils.py

[af09b3f0a] Michael Goin 2025-06-12 [Bugfix][V1] Allow manual FlashAttention for Blackwell (#19492)
13	4	vllm/platforms/cuda.py

[4f6c42fa0] Russell Bryant 2025-06-12 [Security] Prevent new imports of (cloud)pickle (#18018)
7	0	.pre-commit-config.yaml
152	0	tools/check_pickle_imports.py

[dff680001] niu_he 2025-06-12 Fix typo (#19525)
1	1	examples/offline_inference/basic/README.md

[2e090bd5d] rasmith 2025-06-12 [AMD][Kernel][BugFix] fix test_rocm_compressed_tensors_w8a8 for rocm (#19509)
2	5	vllm/_custom_ops.py
4	4	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py

[1b0b065eb] wonjun Jang 2025-06-12 [BugFix] Handle missing sep_token for Qwen3-Reranker in Score API (#19522)
2	2	vllm/entrypoints/openai/serving_score.py

[d5bdf899e] Nick Hill 2025-06-11 [BugFix] Work-around incremental detokenization edge case error (#19449)
80	0	tests/v1/engine/test_fast_incdec_prefix_err.py
33	6	vllm/v1/engine/detokenizer.py

[7e3e74c97] 22quinn 2025-06-11 [Frontend] Improve error message in tool_choice validation (#19239)
15	11	vllm/entrypoints/openai/protocol.py

[3f6341bf7] Brayden Zhong 2025-06-12 Add Triton Fused MoE kernel config for E=16 on B200 (#19518)
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_B200.json

[e5d35d62f] Varun Sundar Rabindranath 2025-06-12 [BugFix] Force registration of w8a8_block_fp8_matmul_deepgemm via lazy import (#19514)
1	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[2f1c19b24] Ning Xie 2025-06-12 [CI] change spell checker from codespell to typos (#18711)
1	1	.gitignore
3	5	.pre-commit-config.yaml
3	3	csrc/cpu/attention.cpp
5	5	csrc/cpu/cpu_types_x86.hpp
8	8	csrc/moe/moe_permute_unpermute_op.cu
3	3	csrc/moe/topk_softmax_kernels.cu
1	1	csrc/moe/torch_bindings.cpp
3	3	csrc/quantization/machete/machete_mainloop.cuh
7	7	csrc/rocm/skinny_gemms.cu
1	1	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu
0	4	pyproject.toml
2	2	tests/compile/test_async_tp.py
2	2	tests/core/block/e2e/test_correctness.py
3	3	tests/core/block/e2e/test_correctness_sliding_window.py
2	2	tests/core/test_scheduler.py
2	2	tests/entrypoints/openai/test_chat_template.py
8	8	tests/kernels/attention/test_cache.py
1	1	tests/kernels/attention/test_encoder_decoder_attn.py
4	4	tests/kernels/core/test_rotary_embedding.py
13	14	tests/kernels/mamba/test_mamba_ssm_ssd.py
0	0	tests/lora/{test_transfomers_model.py => test_transformers_model.py}
1	1	tests/models/language/generation/test_bart.py
1	1	tests/samplers/test_typical_acceptance_sampler.py
1	1	tests/spec_decode/e2e/test_eagle_correctness.py
1	1	tests/spec_decode/e2e/test_medusa_correctness.py
1	1	tests/spec_decode/e2e/test_mtp_correctness.py
2	2	tests/spec_decode/e2e/test_ngram_correctness.py
1	1	tests/v1/e2e/test_correctness_sliding_window.py
2	2	tests/v1/kv_connector/unit/test_nixl_connector.py
1	1	tests/v1/sample/test_logprobs_e2e.py
13	13	tests/worker/test_model_input.py
1	1	tools/report_build_time_ninja.py
179	0	typos.toml
3	3	vllm/_custom_ops.py
2	2	vllm/attention/backends/utils.py
4	4	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	1	vllm/lora/layers.py
2	2	vllm/lora/punica_wrapper/utils.py
2	2	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	1	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
1	1	vllm/model_executor/layers/quantization/utils/int8_utils.py
1	1	vllm/model_executor/model_loader/bitsandbytes_loader.py
3	3	vllm/model_executor/models/baichuan.py
3	3	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/eagle.py
4	4	vllm/model_executor/models/gemma3_mm.py
1	1	vllm/model_executor/models/llama4.py
5	5	vllm/model_executor/models/mixtral_quant.py
1	1	vllm/model_executor/models/ovis.py
5	5	vllm/model_executor/models/phi3_small.py
4	4	vllm/model_executor/models/phi4mm_audio.py
3	3	vllm/model_executor/models/phimoe.py
1	1	vllm/multimodal/utils.py
1	1	vllm/transformers_utils/processors/ovis.py
1	1	vllm/worker/hpu_model_runner.py
1	1	vllm/worker/multi_step_model_runner.py
8	8	vllm/worker/tpu_model_runner.py

[42f52cc95] Richard Zou 2025-06-11 [CI/Build] Fix torch nightly CI dependencies (#19505)
2	2	requirements/nightly_torch_test.txt

[97a9465bb] Robert Shaw 2025-06-11 [UX] Add Feedback During CUDAGraph Capture (#19501)
4	1	vllm/v1/worker/gpu_model_runner.py

[c7ea0b56c] rasmith 2025-06-11 [AMD] [Quantization] Add override flag for attention dtype instead of using kv_cache_dtype trigger (#17331)
9	1	vllm/attention/backends/rocm_flash_attn.py
8	0	vllm/config.py
4	0	vllm/engine/arg_utils.py

[29fa5cac1] bnellnm 2025-06-11 [Kernels] Add activation chunking logic to FusedMoEModularKernel (#19168)
1	0	tests/kernels/moe/test_cutlass_moe.py
22	1	tests/kernels/moe/test_moe.py
33	10	tests/kernels/moe/test_pplx_cutlass_moe.py
67	22	tests/kernels/moe/test_pplx_moe.py
25	2	tests/kernels/quantization/test_block_fp8.py
0	123	tests/pplx_utils.py
11	9	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
16	6	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
49	28	vllm/model_executor/layers/fused_moe/cutlass_moe.py
11	11	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
29	44	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
11	11	vllm/model_executor/layers/fused_moe/fused_moe.py
143	85	vllm/model_executor/layers/fused_moe/modular_kernel.py
35	42	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
5	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[b2d9be6f7] Woosuk Kwon 2025-06-11 [Docs] Remove WIP features in V1 guide (#19498)
1	9	docs/usage/v1_guide.md

[04a55612d] Jee Jee Li 2025-06-12 [Misc] Fix  misleading ROCm warning (#19486)
6	1	vllm/attention/ops/triton_flash_attention.py

[89b0f84e1] David Xia 2025-06-11 [doc] fix "Other AI accelerators" getting started page (#19457)
3	3	docs/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
28	27	docs/getting_started/installation/ai_accelerator/neuron.inc.md
19	13	docs/getting_started/installation/ai_accelerator/tpu.inc.md

[497a91e9f] Michael Goin 2025-06-11 [CI] Update FlashInfer to 0.2.6.post1 (#19297)
17	15	docker/Dockerfile

[943ffa570] runzhen 2025-06-11 [Bugfix] Update the example code, make it work with the latest lmcache (#19453)
1	1	examples/others/lmcache/cpu_offload_lmcache.py
2	2	examples/others/lmcache/kv_cache_sharing_lmcache_v1.py

[5c8d34a42] Louie Tsai 2025-06-11 Support no privileged mode on CPU for docker and kubernetes deployments (#19241)
2	3	csrc/cpu/utils.cpp

[3c8694eab] Ximingwang-09 2025-06-11 Fix some typo (#19475)
3	3	vllm/model_executor/layers/rejection_sampler.py

[7484e1fce] Michael Goin 2025-06-11 Add cache to cuda get_device_capability (#19436)
3	1	vllm/platforms/cuda.py

[a2142f019] Cyrus Leung 2025-06-11 Support non-string values in JSON keys from CLI (#19471)
17	17	tests/test_config.py
15	0	tests/test_utils.py
16	7	vllm/utils.py

[871d6b7c7] Lu Fang 2025-06-11 [Misc] Reduce warning message introduced in env_override (#19476)
1	1	vllm/env_override.py

[29a38f035] Cyrus Leung 2025-06-11 [Doc] Support "important" and "announcement" admonitions (#19479)
1	1	docs/contributing/README.md
4	4	docs/contributing/model/multimodal.md
2	2	docs/contributing/model/registration.md
1	1	docs/contributing/model/tests.md
1	1	docs/design/multiprocessing.md
1	1	docs/features/multimodal_inputs.md
3	2	docs/getting_started/quickstart.md
37	0	docs/mkdocs/stylesheets/extra.css
2	2	docs/models/generative_models.md
4	4	docs/models/supported_models.md
4	4	docs/serving/openai_compatible_server.md
1	1	docs/usage/v1_guide.md

[a5115f4ff] Cyrus Leung 2025-06-11 [Doc] Fix quantization link titles (#19478)
13	13	docs/features/quantization/README.md
1	1	docs/features/quantization/quark.md

[68b4a2614] Cyrus Leung 2025-06-11 [Doc] Update V1 User Guide for Hardware and Models (#19474)
81	67	docs/usage/v1_guide.md
2	1	vllm/engine/arg_utils.py

[b8e809a05] artetaout 2025-06-11 [Kernel] Support deep_gemm for linear methods (#19085)
84	0	vllm/model_executor/layers/quantization/deepgemm.py
1	0	vllm/model_executor/layers/quantization/fp8.py
39	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[5039ec233] Lu Fang 2025-06-11 [ROCm] Add rules to automatically label ROCm related PRs (#19405)
20	0	.github/mergify.yml

[7c644ab6d] leopardracer 2025-06-11 Fix Typo in Documentation and Function Name (#19442)
1	1	tests/kernels/attention/test_encoder_decoder_attn.py
1	1	tests/v1/sample/test_topk_topp_sampler.py

[2d40665fe] Junhao Li 2025-06-11 Add fused MOE config for Qwen3 30B A3B on B200 (#19455)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json

[96ada386b] Lukas Geiger 2025-06-11 [Misc] Remove unused `MultiModalHasher.hash_prompt_mm_data` (#19422)
0	29	vllm/multimodal/hasher.py

[1e473b301] Michael Goin 2025-06-11 [CI] Disable failing GGUF model test (#19454)
1	1	tests/models/quantization/test_gguf.py

[2b1e2111b] Lu Fang 2025-06-11 Fix test_max_model_len in tests/entrypoints/llm/test_generate.py (#19451)
4	1	tests/entrypoints/llm/test_generate.py

[a45b979d9] niu_he 2025-06-11 [BugFix] Fix docker build cpu-dev image error (#19394)
4	0	docker/Dockerfile.cpu

[3952731e8] wang.yuqi 2025-06-11 [New Model]: Support Qwen3 Embedding & Reranker  (#19260)
25	17	docs/models/supported_models.md
77	0	examples/offline_inference/qwen3_reranker.py
9	0	tests/models/language/pooling/test_gte.py
87	0	tests/models/language/pooling/test_qwen3_reranker.py
73	0	tests/models/language/pooling/test_qwen3_reranker_seq_cls.py
1	0	tests/models/registry.py
123	2	vllm/model_executor/models/qwen3.py
1	0	vllm/model_executor/models/registry.py

[77f0d465d] Richard Zou 2025-06-10 [BugFix] Allow use_cudagraph to work with dynamic VLLM_USE_V1 (#19390)
10	0	tests/compile/test_config.py
1	1	vllm/config.py

[22c3c0aa4] Xu Wenqing 2025-06-11 Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B-FP8 (#19401)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json

[33f8dba7c] py-andy-c 2025-06-10 [Model] use AutoWeightsLoader for commandr (#19399)
62	63	vllm/model_executor/models/commandr.py

[5241ca50d] Gregory Shtrasberg 2025-06-10 [ROCm][V1] Adding ROCm to the list of plaforms using V1 by default (#19440)
1	1	vllm/engine/arg_utils.py

[da9b523ce] Russell Bryant 2025-06-10 [Docs] Note that alternative structured output backends are supported (#19426)
1	8	docs/usage/v1_guide.md

[b6553be1b] Jee Jee Li 2025-06-10 [Misc] Slight improvement of the BNB  (#19418)
1	3	vllm/model_executor/layers/quantization/bitsandbytes.py
2	1	vllm/model_executor/model_loader/bitsandbytes_loader.py

[64a9af5af] youkaichao 2025-06-10 Simplify ep kernels installation (#19412)
4	6	tools/ep_kernels/README.md
7	0	tools/ep_kernels/configure_system_drivers.sh
15	21	tools/ep_kernels/install_python_libraries.sh
0	24	tools/ep_kernels/install_system_drivers.sh
0	18	tools/ep_kernels/install_system_libraries.sh

[e4248849e] Li, Jiang 2025-06-10 [BugFix][CPU] Fix CPU CI by ignore collecting test_pixtral (#19411)
4	1	.buildkite/scripts/hardware_ci/run-cpu-test.sh

[467bef18a] Rachel Guo 2025-06-10 [BugFix][FlashInfer] Fix attention backend interface mismatch with unexpected keyword `use_irope` (#19134)
5	0	vllm/v1/attention/backends/flashinfer.py

[5f1ac1e1d] Isotr0py 2025-06-10 Revert "[v1] Add fp32 support to v1 engine through flex attn" (#19404)
0	28	tests/kernels/attention/test_attention_selector.py
0	6	tests/v1/worker/test_gpu_model_runner.py
7	0	vllm/engine/arg_utils.py
0	4	vllm/platforms/cuda.py

[9368cc90b] Louie Tsai 2025-06-09 Automatically bind CPU OMP Threads of a rank to CPU ids of a NUMA node. (#17930)
18	3	docs/getting_started/installation/cpu.md
2	0	requirements/cpu.txt
7	1	vllm/envs.py
3	0	vllm/platforms/cpu.py
53	2	vllm/v1/worker/cpu_worker.py
51	2	vllm/worker/cpu_worker.py

[32b3946bb] Anna Pendleton 2025-06-09 Add clear documentation around the impact of debugging flag (#19369)
1	1	docs/usage/troubleshooting.md

[6b1391ca7] Reid 2025-06-10 [Misc] refactor neuron_multimodal and profiling (#19397)
5	1	examples/offline_inference/neuron_multimodal.py
6	2	examples/offline_inference/profiling_tpu/profiling.py

[a3f66e75d] Russell Bryant 2025-06-10 Add security warning to bug report template (#19365)
10	0	.github/ISSUE_TEMPLATE/400-bug-report.yml

[319cb1e35] Lukas Geiger 2025-06-10 [Core] Batch multi modal input using pinned memory (#19169)
14	5	vllm/multimodal/inputs.py
4	2	vllm/v1/worker/gpu_model_runner.py

[1efef7164] Li Wang 2025-06-10 [Bugfix] Fix modelscope token passed in (#19389)
3	1	vllm/transformers_utils/config.py

[646d62f63] Nick Hill 2025-06-09 [Core] Use tuple for kv cache group block ids (#19175)
22	22	tests/v1/core/test_prefix_caching.py
4	4	tests/v1/tpu/worker/test_tpu_model_runner.py
1	1	tests/v1/worker/test_gpu_input_batch.py
2	2	tests/v1/worker/test_gpu_model_runner.py
4	4	vllm/v1/core/block_pool.py
53	48	vllm/v1/core/kv_cache_coordinator.py
18	20	vllm/v1/core/kv_cache_manager.py
4	4	vllm/v1/core/sched/output.py
4	8	vllm/v1/core/sched/scheduler.py
24	26	vllm/v1/core/single_type_kv_cache_manager.py
3	2	vllm/v1/worker/block_table.py
1	1	vllm/v1/worker/gpu_input_batch.py

[6cd4ae8ac] Reid 2025-06-10 [Frontend] Add tqdm_leave_pbar to control progress bar visibility (#19357)
52	30	vllm/entrypoints/llm.py

[c016047ed] Harry Mellor 2025-06-10 Fix docs/mkdocs/hooks/remove_announcement.py (#19382)
4	4	docs/mkdocs/hooks/remove_announcement.py

[9af6d22e4] XiongfeiWei 2025-06-09 Use xla flag to improve the quantized model performance (#19303)
4	1	vllm/v1/worker/tpu_worker.py

[4589b9403] Tianyu Guo 2025-06-10 [Bugfix] Fix benchmark_moe.py (#19016)
4	10	benchmarks/kernels/benchmark_moe.py

[cc867be19] Ye (Charlotte) Qi 2025-06-09 [V1] Reuse V0's memory_profiling util for gpu worker memory profiling (#19312)
16	2	vllm/utils.py
35	51	vllm/v1/worker/gpu_worker.py

[3a7cd627a] Siyuan Liu 2025-06-09 [Misc] Fix a config typo in disable_hybrid_kv_cache_manager configuration (#19383)
3	3	vllm/config.py

[8058c9110] Pavani Majety 2025-06-09 [HOT-FIX] Add `kv_sharing_target_layer_name` argument to cutlass_mla backend (#19374)
2	1	vllm/v1/attention/backends/mla/cutlass_mla.py

[7d44c469f] Siyuan Liu 2025-06-09 [TPU]Fix KV cache sharing tests (#19371)
52	60	tests/v1/tpu/worker/test_tpu_model_runner.py

[31f58be96] liusiqian-tal 2025-06-10 [Frontend] Make TIMEOUT_KEEP_ALIVE configurable through env var (#18472)
6	6	tests/async_engine/api_server_async_engine.py
2	2	vllm/entrypoints/api_server.py
1	3	vllm/entrypoints/openai/api_server.py
5	0	vllm/envs.py

[ebb2f383b] Kyle Sayers 2025-06-09 [Quantization] Bump compressed-tensors version (#19295)
1	1	requirements/common.txt

[c1c7dbbee] 22quinn 2025-06-09 [Bugfix][Core] Prevent token lengths exceeding `max_model_len` in V0 (#19348)
22	0	tests/entrypoints/llm/test_generate.py
1	1	vllm/engine/output_processor/stop_checker.py

[5cf2daea9] Varun Sundar Rabindranath 2025-06-09 [Misc] Fixes and Optimizations for DeepEP + DeepGEMM combination. (#19298)
1	1	tests/kernels/moe/test_pplx_moe.py
5	10	vllm/distributed/device_communicators/all2all.py
5	0	vllm/envs.py
18	11	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
10	6	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
1	1	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
26	6	vllm/model_executor/layers/quantization/utils/fp8_utils.py
32	1	vllm/v1/worker/gpu_model_runner.py

[b8089195b] Isotr0py 2025-06-09 [v1] Add fp32 support to v1 engine through flex attn (#19319)
28	0	tests/kernels/attention/test_attention_selector.py
6	0	tests/v1/worker/test_gpu_model_runner.py
0	7	vllm/engine/arg_utils.py
4	0	vllm/platforms/cuda.py

[770e5dcdb] Yinghai Lu 2025-06-09 [full_graph] Fix query_start_loc padding (#19321)
4	1	vllm/v1/worker/gpu_model_runner.py

[c57c9415b] Michael Yao 2025-06-09 [Docs] Fix a bullet list in usage/security.md (#19358)
1	0	docs/usage/security.md

[01810f923] Lu Fang 2025-06-09 [CI] Introduce rules for llama auto-label (#19323)
14	0	.github/mergify.yml

[59abbd84f] Conroy Cheers 2025-06-09 [Fix] Allow kernel compilation for CUDA capability 8.7 (#19328)
3	3	CMakeLists.txt

[95a6568b5] Jee Jee Li 2025-06-09 [CI/Build] Fix LoRA test (#19350)
0	5	tests/lora/conftest.py
0	34	tests/lora/test_llama_tp.py
0	7	tests/lora/test_lora_functions.py
1	9	tests/lora/test_phi.py
10	8	tests/lora/test_worker.py

[0eca5eacd] Se7en 2025-06-09 [Doc] Fix description in the Automatic Prefix Caching design doc (#19333)
1	1	docs/design/v1/prefix_caching.md

[12e582922] Reid 2025-06-09 [doc] improve ci doc (#19307)
17	0	.buildkite/scripts/ci-clean-log.sh
18	0	.buildkite/scripts/rerun-test.sh
7	9	docs/contributing/ci-failures.md

[3a4d41770] Richard Zou 2025-06-09 [Misc] Cleanup compilation tests (#19343)
0	9	tests/compile/test_config.py

[8335667c2] Kseniya Parkhamchuk 2025-06-08 [Frontend] Remove unreachable code from llm.py (#19288)
0	1	vllm/entrypoints/llm.py

[e1c4380d4] Isotr0py 2025-06-09 [Misc] Add documentation update reminder to PR template (#19289)
3	0	.github/PULL_REQUEST_TEMPLATE.md

[e31ae3de3] Cyrus Leung 2025-06-09 [Deprecation] Remove `inputs` arg fallback in Engine classes (#18799)
9	98	vllm/engine/async_llm_engine.py
3	45	vllm/engine/llm_engine.py
2	43	vllm/engine/multiprocessing/__init__.py
7	82	vllm/engine/multiprocessing/client.py

[2ffb9b6e0] wang.yuqi 2025-06-08 [Bugfix] model_max_length should consider max_model_len in tokenizer_config (#19201)
14	1	vllm/config.py
16	0	vllm/transformers_utils/config.py

[cda10fa3e] jennyyyyzhen 2025-06-08 [Multi Modal] Add an env var for message queue max chunk bytes  (#19242)
7	0	vllm/envs.py
5	1	vllm/v1/executor/multiproc_executor.py

[c123bc33f] Dipika Sikka 2025-06-08 [Quantization] Add compressed-tensors NVFP4 support (#18312)
12	7	tests/quantization/test_compressed_tensors.py
27	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
3	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
178	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a4_nvfp4.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
46	4	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py

[b9a1791e2] Akash kaothalkar 2025-06-08 [Hardware][POWER] Add IBM POWER11 Support to CPU Extension Detection (#19082)
13	6	cmake/cpu_extension.cmake

[989dcee98] Xu Wenqing 2025-06-08 Add H20-3e fused MoE kernel tuning configs for Qwen3-235B-A22B (#19315)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H20-3e.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20-3e.json

[3d64d366e] Richard Zou 2025-06-08 [Misc] Change tests/compile to use VLLM_V1 by default (#19302)
0	15	tests/compile/conftest.py
2	0	tests/compile/piecewise/test_simple.py

[eaa2e5108] Richard Zou 2025-06-07 [Bugfix] Re-enable use_cudagraph in vLLM v1 (#19299)
1	1	tests/compile/piecewise/test_simple.py
3	3	tests/compile/piecewise/test_toy_llama.py
43	0	tests/compile/test_config.py
1	1	vllm/compilation/counter.py
1	1	vllm/compilation/cuda_piecewise_backend.py
3	2	vllm/config.py

[d77f7fb87] Chauncey 2025-06-08 [Bugfix]: Fix TypeError: 'float' object cannot be interpreted as an integer (#19283)
1	1	vllm/v1/utils.py

[2d8476e46] Luka Govedič 2025-06-07 [BugFix][V1] Fix memory profiling bug (#18974)
2	0	tests/models/test_initialization.py
8	5	tests/v1/sample/test_logprobs.py
43	11	vllm/v1/worker/gpu_worker.py

[88be823d5] pramenku 2025-06-07 [AMD] Update compatible packaging version (#19309)
1	1	docker/Dockerfile.rocm
2	1	requirements/rocm.txt

[4e4f63ad4] Lifans 2025-06-07 [Nit][Benchmark]Fix example in benchmark_serving_structured_output.py (#19311)
0	1	benchmarks/benchmark_serving_structured_output.py

[d2f0e7e61] Isotr0py 2025-06-07 [CI/Build] Improve Llama GGUF test robustness (#19287)
1	1	tests/models/quantization/test_gguf.py

[122cdca5f] Reid 2025-06-07 [Misc] refactor context extension (#19246)
51	30	examples/offline_inference/context_extension.py

[cf02f9b28] Driss Guessous 2025-06-07 Add FlexAttention to V1 (#16078)
93	0	tests/kernels/test_flex_attention.py
1	0	vllm/engine/arg_utils.py
3	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
477	0	vllm/v1/attention/backends/flex_attention.py

[c4296b1a2] Aaruni Aggarwal 2025-06-07 [CI][PowerPC] Use a more appropriate way to select testcase in tests/models/language/pooling/test_embedding.py (#19253)
1	1	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh

[66c508b13] QiliangCui 2025-06-06 [TPU][Test] Add script to run benchmark on TPU for buildkite (#19039)
24	0	.buildkite/scripts/tpu/cleanup_docker.sh
14	0	.buildkite/scripts/tpu/config_v6e_1.env
102	0	.buildkite/scripts/tpu/docker_run_bm.sh
94	0	.buildkite/scripts/tpu/run_bm.sh

[84166fee9] ElizaWszola 2025-06-07 [Kernel] Integrate CUTLASS MoE kernel with PPLX (#18762)
2	2	CMakeLists.txt
11	50	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
10	1	csrc/ops.h
19	10	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu
2	4	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
38	4	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
36	3	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
18	1	csrc/torch_bindings.cpp
2	6	tests/kernels/moe/test_cutlass_moe.py
287	0	tests/kernels/moe/test_pplx_cutlass_moe.py
7	124	tests/kernels/moe/test_pplx_moe.py
2	1	tests/kernels/quantization/test_cutlass_scaled_mm.py
123	0	tests/pplx_utils.py
27	2	vllm/_custom_ops.py
1	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
3	2	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
220	143	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	0	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
2	0	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py
47	11	vllm/model_executor/layers/fused_moe/layer.py
2	1	vllm/model_executor/layers/fused_moe/modular_kernel.py
13	8	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
3	2	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
47	40	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/fp8.py

[6e0cd10f7] Lu Fang 2025-06-07 [Easy][Test] Simplify test_function_tool_use with multiple parametrizes (#19269)
16	57	tests/entrypoints/openai/test_completion_with_function_calling.py

[e010688f5] Alexei-V-Ivanov-AMD 2025-06-06 [Build][ROCm] Update Dockerfile.rocm (#19296)
2	1	docker/Dockerfile.rocm

[441b65d8c] Chenyaaang 2025-06-06 [Misc][Tools][Benchmark] Fix and improve auto tune script (#19163)
98	66	benchmarks/auto_tune.sh

[46ecc5797] Nick Hill 2025-06-06 [BugFix] Fix tpu_model_runner block_id concatenation (#19228)
1	1	tests/v1/tpu/worker/test_tpu_model_runner.py
5	2	vllm/v1/worker/gpu_model_runner.py
5	1	vllm/v1/worker/tpu_model_runner.py

[b6a3a9f76] Nicolò Lucchesi 2025-06-07 [Core] Fix abrupt request abort (#18485)
1	1	vllm/v1/core/kv_cache_coordinator.py
8	6	vllm/v1/core/sched/scheduler.py

[ca27f0f9c] Adolfo Victoria 2025-06-06 [Bugfix][Core] Update cancellation logic in `generate()` to handle Generator exits (#19225)
126	43	tests/v1/engine/test_async_llm.py
3	2	vllm/v1/engine/async_llm.py

[aad30bd30] Nick Hill 2025-06-06 [BugFix] Fix MultiConnector test after HMA changes (#19291)
8	7	tests/v1/kv_connector/unit/test_multi_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
4	0	vllm/v1/core/kv_cache_manager.py

[94ecee628] Nishidha 2025-06-07 Fixed ppc build when it runs on non-RHEL based linux distros (#18422)
1	0	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh
70	26	docker/Dockerfile.ppc64le

[8267f9916] Yu Guo 2025-06-06 improve logits bias (#19041)
16	1	vllm/v1/sample/sampler.py

[7353492a4] jmswen 2025-06-06 [Core] Raise when non-multi-instance DP clients target a DP rank (#19227)
22	0	tests/async_engine/test_async_llm_engine.py
29	0	tests/v1/engine/test_async_llm.py
16	9	tests/v1/test_async_llm_dp.py
4	0	vllm/engine/async_llm_engine.py
0	3	vllm/v1/engine/core_client.py
6	0	vllm/v1/engine/processor.py

[7661e92ef] Jee Jee Li 2025-06-06 [Model] Optimize nemotron_h implementation (#19249)
16	8	vllm/model_executor/models/nemotron_h.py

[f168b8572] Siqi Yan 2025-06-06 Unit Test for run_dp_sharded_vision_model (#19103)
97	1	tests/multimodal/test_utils.py

[da511d54d] Richard Zou 2025-06-06 Fix CompilationConfig repr (#19091)
13	0	tests/test_config.py
20	15	vllm/config.py

[65c69444b] Nick Hill 2025-06-06 [Docs] Improve V1 KVConnector interface documentation (#19172)
26	3	vllm/distributed/kv_transfer/kv_connector/v1/base.py
6	6	vllm/v1/core/sched/scheduler.py

[94870359c] Dipika Sikka 2025-06-06 [Quantization] Bump compressed-tensors version; update NVFP4A16 test model (#19224)
1	1	requirements/common.txt
1	2	tests/quantization/test_compressed_tensors.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[0d49483ea] Chengji Yao 2025-06-06 [TPU] fix kv cache dtype in model runner (#19244)
9	3	vllm/v1/worker/tpu_model_runner.py

[90b78ec5f] Jinghui Zhang 2025-06-05 [v1][P/D] Fix a edge case in kv cache schedule (#19182)
2	0	vllm/v1/core/sched/scheduler.py

[91a2ef98e] Aaron Pham 2025-06-06 [Chore] update CODEOWNERS (#19247)
9	7	.github/CODEOWNERS

[3da2313d7] Xu Song 2025-06-06 Support allowed_token_ids in ChatCompletionRequest (#19143)
2	0	vllm/entrypoints/openai/protocol.py

[b61dc5f97] Chengji Yao 2025-06-05 [TPU] update torch_xla pin (#19231)
5	5	requirements/tpu.txt
1	1	tests/tpu/test_moe_pallas.py
2	1	vllm/v1/worker/tpu_worker.py

[f8a1a2d10] Chen Zhang 2025-06-06 [v1] Hybrid Memory Allocator (#17996)
210	35	tests/v1/core/test_kv_cache_utils.py
350	100	tests/v1/core/test_prefix_caching.py
8	8	tests/v1/core/test_scheduler.py
16	11	tests/v1/core/test_specialized_manager.py
2	2	tests/v1/e2e/test_correctness_sliding_window.py
2	2	tests/v1/kv_connector/unit/test_nixl_connector.py
2	2	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
12	12	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
5	5	tests/v1/kv_connector/unit/utils.py
14	13	tests/v1/worker/test_gpu_model_runner.py
21	0	vllm/config.py
8	0	vllm/engine/arg_utils.py
44	25	vllm/v1/core/block_pool.py
358	0	vllm/v1/core/kv_cache_coordinator.py
76	59	vllm/v1/core/kv_cache_manager.py
271	51	vllm/v1/core/kv_cache_utils.py
4	3	vllm/v1/core/sched/scheduler.py
98	53	vllm/v1/core/single_type_kv_cache_manager.py
9	24	vllm/v1/kv_cache_interface.py
87	33	vllm/v1/worker/gpu_model_runner.py
10	4	vllm/v1/worker/tpu_model_runner.py

[3465b87ef] Benjamin Chislett 2025-06-05 [Bugfix] Fix EAGLE vocab embedding construction for Llama 70B (#19033)
1	1	benchmarks/kernels/bench_fp8_gemm.py
40	24	tests/v1/spec_decode/test_eagle.py
6	8	vllm/model_executor/models/llama_eagle.py
14	10	vllm/model_executor/models/llama_eagle3.py
1	0	vllm/platforms/cuda.py
1	0	vllm/utils.py
7	4	vllm/v1/spec_decode/eagle.py

[c8134bea1] Jerry Zhang 2025-06-05 Fix AOPerModuleConfig name changes (#18869)
3	0	.buildkite/test-pipeline.yaml
3	3	tests/quantization/test_torchao.py
19	2	vllm/model_executor/layers/quantization/torchao.py

[cb6d572e8] Luis Vega 2025-06-05 [Model] NemotronH support (#18863)
1	0	docs/models/supported_models.md
2	0	tests/models/registry.py
565	0	vllm/model_executor/models/nemotron_h.py
1	0	vllm/model_executor/models/registry.py
2	0	vllm/transformers_utils/configs/__init__.py
258	0	vllm/transformers_utils/configs/nemotron_h.py

[87360308b] Michael Goin 2025-06-05 [V1] Use FlashInfer by default on Blackwell GPUs (#19118)
15	0	vllm/platforms/cuda.py
24	0	vllm/platforms/interface.py

[aa49f1483] Dipika Sikka 2025-06-05 [Quantization] Skip Fp4 Test for `compressed-tensors` (#19217)
1	0	tests/quantization/test_compressed_tensors.py

[9ef9173cf] Nicolò Lucchesi 2025-06-05 [P/D][NixlConnector] Enable FlashInfer backend (#19090)
50	15	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	0	vllm/platforms/interface.py

[85e2b7bb1] Povilas Kanapickas 2025-06-05 [MISC][Bugfix] Use less CPU when message queue has been empty for some time (#16226)
19	9	tests/basic_correctness/test_basic_correctness.py
45	2	vllm/distributed/device_communicators/shm_broadcast.py
6	0	vllm/envs.py

[61059bee4] Chiyue Wei 2025-06-05 [Hardware][NVIDIA] FP4 MoE kernel optimization (#19110)
1	1	benchmarks/kernels/benchmark_cutlass_fp4_moe.py
5	1	csrc/moe/moe_ops.h
56	0	csrc/moe/moe_permute_unpermute_op.cu
12	6	csrc/moe/permute_unpermute_kernels/dispatch.h
6	0	csrc/moe/torch_bindings.cpp
2	1	csrc/ops.h
31	5	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
6	3	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
1	1	csrc/torch_bindings.cpp
4	1	tests/kernels/moe/test_nvfp4_moe.py
33	12	vllm/_custom_ops.py
8	7	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[ec89524f5] Xu Wenqing 2025-06-06 Add H20-3e fused MoE kernel tuning configs for DeepSeek-R1/V3 (#19205)
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H20-3e,dtype=fp8_w8a8,block_shape=[128,128].json

[f20f9f063] Patrick von Platen 2025-06-05 [mistral_common] Add v11 tokenizer (#19193)
30	4	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
2	0	vllm/transformers_utils/tokenizers/mistral.py

[9bc8bb07c] Guillaume Calmettes 2025-06-05 [Bugfix] properly catch PIL-related errors for vision models when incorrect data urls are provided (#19202)
13	0	tests/multimodal/test_utils.py
19	11	vllm/multimodal/utils.py

[1aeb925f3] Reid 2025-06-05 [Frontend] improve vllm run-batch --help display (#19187)
2	2	vllm/entrypoints/cli/main.py
7	1	vllm/entrypoints/cli/run_batch.py
3	3	vllm/entrypoints/cli/serve.py
11	3	vllm/entrypoints/utils.py

[188a4590d] 22quinn 2025-06-05 [Misc] Do not override NCCL_CUMEM_ENABLE if set explicitly (#19105)
12	6	vllm/env_override.py

[18093084b] vllmellm 2025-06-05 [Misc] Remove unnecessary fallback to prefill-decode attention (#19138)
1	4	vllm/v1/attention/backends/triton_attn.py

[da4038021] Simon Mo 2025-06-04 [Build] Annotate wheel and container path for release workflow (#19162)
18	1	.buildkite/release-pipeline.yaml
31	0	.buildkite/scripts/annotate-release.sh

[8fc57501d] Chauncey 2025-06-05 [Bugfix]: Fix the incompatibility issue with stream when Thinking is disabled (#19135)
86	30	tests/entrypoints/openai/test_completion_with_function_calling.py
15	1	vllm/entrypoints/openai/serving_chat.py

[af7fc84fd] Woosuk Kwon 2025-06-04 [BugFix][Minor] Fix full cuda graph bug when max_num_seqs < 512 (#19171)
2	2	vllm/v1/worker/gpu_model_runner.py

[0678b5225] Huy Do 2025-06-04 Handle non-serializable objects when dumping benchmark results (#19114)
2	0	.pre-commit-config.yaml
6	1	benchmarks/benchmark_utils.py

[25b918eee] Yang Wang 2025-06-04 [Torch Nightly]add missing dependency (#18770)
3	0	docker/Dockerfile.nightly_torch
7	2	requirements/nightly_torch_test.txt

[a408820f2] Michael Goin 2025-06-04 [Bugfix] Fix port handling in make_zmq_path (#19117)
1	1	vllm/utils.py

[c56ed8bb0] Robert Shaw 2025-06-04 [Bugfix][Nixl] Fix full prefix cache hit bug (#18632)
47	29	tests/v1/kv_connector/unit/test_multi_connector.py
26	15	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
20	33	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
4	4	vllm/v1/core/sched/scheduler.py

[78dcf56cb] Reid 2025-06-05 [doc] small fix (#19167)
1	1	docs/models/extensions/tensorizer.md

[b2fac6713] Nicolò Lucchesi 2025-06-05 [P/D] Heterogeneous TP (#18833)
8	3	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
1	0	tests/v1/kv_connector/nixl_integration/test_accuracy.py
17	1	vllm/distributed/kv_transfer/kv_connector/utils.py
243	95	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
16	0	vllm/v1/attention/backends/flash_attn.py
2	1	vllm/worker/worker_base.py

[23027e2da] CYJiang 2025-06-05 [Misc] refactor: simplify EngineCoreClient.make_async_mp_client in AsyncLLM (#18817)
3	11	vllm/v1/engine/async_llm.py
19	6	vllm/v1/engine/core_client.py

[c3fd4d669] Varun Sundar Rabindranath 2025-06-04 [Kernel] Integrate batched/masked deepgemm kernel (#19111)
4	1	tests/kernels/moe/deepep_utils.py
166	24	tests/kernels/moe/test_deepep_deepgemm_moe.py
124	0	vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
116	0	vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py
55	21	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
7	5	vllm/model_executor/layers/quantization/fp8.py

[ef3f98b59] Kebe 2025-06-05 [Bugfix] fix v1 cpu worker fails on macOS (#19121)
3	2	vllm/engine/arg_utils.py
11	1	vllm/platforms/cpu.py

[7ee259047] Siyuan Liu 2025-06-04 [TPU] Update dynamo dump file name in compilation test (#19108)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
3	2	tests/tpu/test_compilation.py
15	10	tests/v1/tpu/worker/test_tpu_model_runner.py

[53a5a0ce3] Michael Goin 2025-06-04 [Perf] Tunings for SM100 FP8 CUTLASS kernel (#18778)
51	2	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh

[d459fae0a] Tyler Michael Smith 2025-06-04 [Bugfix][EP+DP] Fix internode check (#19112)
0	6	vllm/distributed/device_communicators/all2all.py
1	2	vllm/distributed/device_communicators/base_device_communicator.py

[c8dcc1592] jmswen 2025-06-04 Allow AsyncLLMEngine.generate to target a specific DP rank (#19102)
58	0	examples/online_serving/multi_instance_data_parallel.py
2	1	tests/tokenization/test_detokenize.py
1	0	tests/v1/engine/test_engine_core.py
1	0	tests/v1/engine/test_engine_core_client.py
5	0	tests/v1/engine/test_output_processor.py
11	1	vllm/engine/async_llm_engine.py
1	0	vllm/v1/engine/__init__.py
4	1	vllm/v1/engine/async_llm.py
12	2	vllm/v1/engine/core_client.py
2	0	vllm/v1/engine/processor.py

[8f4ffbd37] Cyrus Leung 2025-06-04 [Doc] Update V1 Guide for embedding models (#19141)
3	3	docs/usage/v1_guide.md

[5f2cd251d] Lain 2025-06-04 Sm100 blockwise fp8 swap ab (#18564)
0	4	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8.cu
140	66	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh
0	14	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[02658c2df] Xu Wenqing 2025-06-04 Add DeepSeek-R1-0528 function call chat template (#18874)
4	2	docs/features/tool_calling.md
92	0	examples/tool_chat_template_deepseekr1.jinja

[01dc9a76d] Cyrus Leung 2025-06-04 [CI/Build][Bugfix] Ensure compatibility with transformers 4.52 (#18678)
1	1	requirements/test.in
1	1	requirements/test.txt
7	2	tests/models/multimodal/generation/test_common.py
2	0	tests/models/multimodal/generation/test_florence2.py
1	1	tests/models/multimodal/generation/test_granite_speech.py
4	0	tests/models/multimodal/generation/test_phi4mm.py
17	1	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	1	tests/models/multimodal/processing/test_common.py
15	32	tests/models/registry.py
11	0	tests/models/test_initialization.py
2	0	vllm/config.py
8	4	vllm/model_executor/models/aya_vision.py
12	4	vllm/model_executor/models/idefics3.py

[35cf32df3] wang.yuqi 2025-06-04 Improve the output precision of embedding models (#19092)
1	5	tests/models/language/pooling/embed_utils.py
6	6	tests/models/language/pooling/mteb_utils.py
0	7	tests/models/language/pooling/test_gte.py
46	0	tests/models/language/pooling/test_intfloat.py
1	2	tests/models/language/pooling/test_jina.py
0	3	tests/models/language/pooling/test_nomic.py
9	4	vllm/model_executor/models/bert.py
6	1	vllm/model_executor/models/bert_with_rope.py

[8711bc5e6] Isotr0py 2025-06-04 [Misc] Add packages for benchmark as extra dependency (#19089)
2	0	docs/cli/README.md
1	0	setup.py
17	22	vllm/benchmarks/datasets.py

[2669a0d7b] Seiji Eicher 2025-06-04 Fix ValueError: Missing value for tag key(s): model_name,engine. (#19113)
4	1	tests/v1/metrics/test_ray_metrics.py
10	0	vllm/v1/metrics/ray_wrappers.py

[8e972d9c4] Siyuan Liu 2025-06-04 [TPU] Skip hanging tests (#19115)
1	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
8	5	tests/v1/tpu/test_spmd_model_weight_loading.py

[3336c8cfb] 汪志鹏 2025-06-04 Fix #19130 (#19132)
26	10	examples/offline_inference/vision_language_multi_image.py

[b124e1085] Woosuk Kwon 2025-06-03 [Bugfix] Fix FA3 full cuda graph correctness (#19106)
1	0	.buildkite/test-pipeline.yaml
5	2	tests/compile/piecewise/test_full_cudagraph.py
21	8	vllm/v1/attention/backends/flash_attn.py
5	0	vllm/v1/worker/gpu_model_runner.py

[41aa57842] Kaixi Hou 2025-06-04 [NVIDIA] Add Cutlass MLA backend (#17625)
1	1	csrc/attention/mla/cutlass_mla_kernels.cu
3	1	tests/kernels/test_cutlass_mla_decode.py
1	0	vllm/engine/arg_utils.py
8	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
1	1	vllm/v1/attention/backends/mla/common.py
96	0	vllm/v1/attention/backends/mla/cutlass_mla.py

[8d646c2e5] Calvin Chen 2025-06-04 [Cleanup][v1]:remote guided-decoding-backend for example (#19059)
0	1	examples/online_serving/openai_chat_completion_structured_outputs.py

[5d6d1adf1] Vadim Gimpelson 2025-06-04 [KERNEL] Sampler. CUDA kernel for applying repetition penalty (#18437)
1	0	CMakeLists.txt
5	0	csrc/ops.h
86	0	csrc/sampler.cu
7	0	csrc/torch_bindings.cpp
76	0	tests/kernels/test_apply_repetition_penalties.py
39	0	vllm/_custom_ops.py
4	9	vllm/model_executor/layers/utils.py

[1409ef913] Lukas Geiger 2025-06-04 [Core] Cast multimodal input in hf processor (#18862)
24	2	vllm/inputs/registry.py
1	7	vllm/multimodal/inputs.py
0	1	vllm/spec_decode/draft_model_runner.py
0	2	vllm/v1/worker/gpu_model_runner.py
0	2	vllm/v1/worker/tpu_model_runner.py
0	1	vllm/worker/cpu_enc_dec_model_runner.py
0	1	vllm/worker/cpu_model_runner.py
0	1	vllm/worker/cpu_pooling_model_runner.py
0	1	vllm/worker/enc_dec_model_runner.py
0	1	vllm/worker/model_runner.py
0	1	vllm/worker/multi_step_neuron_model_runner.py
0	1	vllm/worker/multi_step_neuronx_distributed_model_runner.py
0	2	vllm/worker/neuron_model_runner.py
0	1	vllm/worker/pooling_model_runner.py
0	1	vllm/worker/xpu_model_runner.py

[4555143ea] Li, Jiang 2025-06-04 [CPU] V1 support for the CPU backend (#16441)
5	8	.buildkite/scripts/hardware_ci/run-cpu-test.sh
2	0	docs/usage/v1_guide.md
3	0	requirements/cpu.txt
4	1	tests/kernels/attention/test_attention_selector.py
0	1	tests/models/language/generation/test_common.py
3	3	vllm/attention/backends/cpu_mla.py
12	4	vllm/attention/backends/torch_sdpa.py
6	1	vllm/compilation/wrapper.py
3	1	vllm/engine/arg_utils.py
57	10	vllm/platforms/cpu.py
163	0	vllm/v1/attention/backends/cpu_attn.py
86	0	vllm/v1/worker/cpu_model_runner.py
101	0	vllm/v1/worker/cpu_worker.py
18	10	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/gpu_worker.py

[52dceb172] Russell Bryant 2025-06-03 [Docs] Add developer doc about CI failures (#18782)
120	0	docs/contributing/ci-failures.md

[abd7df2fc] Jiaxin Shan 2025-06-03 [Misc] Fix path and python alias errors in disagg_prefill exmaples (#18919)
3	3	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh

[b712be98c] Yan Ru Pei 2025-06-03 feat: add data parallel rank to KVEventBatch (#18925)
2	0	.buildkite/test-pipeline.yaml
66	41	tests/distributed/conftest.py
67	2	tests/distributed/test_events.py
156	33	tests/v1/engine/test_engine_core_client.py
68	9	vllm/distributed/kv_events.py
3	1	vllm/v1/core/sched/scheduler.py

[a8da78eac] Chen Zhang 2025-06-04 [Bugfix] Max concurrency estimation and check_enough_kv_cache_memory for models with sliding window layers (#19029)
83	7	tests/v1/core/test_kv_cache_utils.py
42	19	vllm/v1/core/kv_cache_utils.py

[5d96533e2] Nicolò Lucchesi 2025-06-04 [Bugfix][P/D] Fix Prefix Cache Bug (#18411)
2	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[4de790fca] Chauncey 2025-06-04 [Bugfix]: Fix the incompatibility issue with tool_choice 'required' when Thinking is enabled (#19075)
1	1	tests/entrypoints/openai/test_completion_with_function_calling.py
15	3	vllm/entrypoints/openai/serving_chat.py

[b5fd9506c] Chen Zhang 2025-06-04 [Bugfix] get_num_blocks_to_allocate with null_block (#19031)
23	0	tests/v1/core/test_specialized_manager.py
3	2	vllm/v1/core/block_pool.py
3	0	vllm/v1/core/kv_cache_utils.py
3	2	vllm/v1/core/single_type_kv_cache_manager.py

[135cf55cd] Ekagra Ranjan 2025-06-03 [V1][Spec Decode][Ngram] 1.35x gain -> 1.95x gain on InstructCoder with prompt fix (#18971)
9	1	benchmarks/benchmark_dataset.py
13	1	vllm/benchmarks/datasets.py

[6cac54f4d] Chen Zhang 2025-06-04 [v1] Re-init input batch for multiple kv cache groups (#18654)
3	26	tests/v1/worker/test_gpu_input_batch.py
3	1	tests/v1/worker/test_gpu_model_runner.py
2	1	vllm/v1/worker/block_table.py
9	9	vllm/v1/worker/gpu_input_batch.py
40	6	vllm/v1/worker/gpu_model_runner.py
4	3	vllm/v1/worker/tpu_model_runner.py

[6865fe007] Harry Mellor 2025-06-03 Fix interaction between `Optional` and `Annotated` in CLI typing (#19093)
15	3	tests/engine/test_arg_utils.py
19	7	vllm/engine/arg_utils.py

[e31446b6c] Michael Goin 2025-06-03 [Perf] Tune `scaled_fp8_quant` by increasing vectorization (#18844)
19	16	csrc/quantization/fp8/common.cu
35	33	csrc/quantization/fp8/common.cuh
50	49	csrc/quantization/fused_kernels/layernorm_utils.cuh
11	12	csrc/quantization/vectorization.cuh

[bdf13965a] Yong Hoon Shin 2025-06-03 [V1] Support cross-layer KV sharing (#18212)
226	1	tests/v1/tpu/worker/test_tpu_model_runner.py
237	7	tests/v1/worker/test_gpu_model_runner.py
1	0	vllm/attention/backends/abstract.py
3	0	vllm/attention/backends/blocksparse_attn.py
2	1	vllm/attention/backends/cpu_mla.py
3	0	vllm/attention/backends/dual_chunk_flash_attn.py
3	0	vllm/attention/backends/flash_attn.py
3	0	vllm/attention/backends/flashinfer.py
2	1	vllm/attention/backends/flashmla.py
3	0	vllm/attention/backends/hpu_attn.py
3	0	vllm/attention/backends/ipex_attn.py
3	0	vllm/attention/backends/mla/common.py
3	0	vllm/attention/backends/pallas.py
2	1	vllm/attention/backends/rocm_aiter_mla.py
3	0	vllm/attention/backends/rocm_flash_attn.py
3	0	vllm/attention/backends/torch_sdpa.py
2	1	vllm/attention/backends/triton_mla.py
3	0	vllm/attention/backends/xformers.py
16	1	vllm/attention/layer.py
21	15	vllm/v1/attention/backends/flash_attn.py
21	15	vllm/v1/attention/backends/flashinfer.py
4	0	vllm/v1/attention/backends/mla/common.py
2	1	vllm/v1/attention/backends/mla/flashmla.py
2	1	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
2	1	vllm/v1/attention/backends/mla/triton_mla.py
5	1	vllm/v1/attention/backends/pallas.py
28	23	vllm/v1/attention/backends/triton_attn.py
33	0	vllm/v1/attention/backends/utils.py
29	2	vllm/v1/worker/gpu_model_runner.py
29	1	vllm/v1/worker/tpu_model_runner.py
36	0	vllm/v1/worker/utils.py

[fa98d7777] Varun Sundar Rabindranath 2025-06-03 [Kernel] DeepEP dispatch-combine kernel integration (#18434)
14	2	csrc/moe/topk_softmax_kernels.cu
0	0	tests/kernels/moe/__init__.py
188	0	tests/kernels/moe/deepep_utils.py
371	0	tests/kernels/moe/test_deepep_deepgemm_moe.py
459	0	tests/kernels/moe/test_deepep_moe.py
2	0	vllm/config.py
145	1	vllm/distributed/device_communicators/all2all.py
8	0	vllm/distributed/device_communicators/cuda_communicator.py
2	0	vllm/envs.py
18	14	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
236	0	vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
152	0	vllm/model_executor/layers/fused_moe/deepep_ll_prepare_finalize.py
37	20	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
120	28	vllm/model_executor/layers/fused_moe/layer.py
121	41	vllm/model_executor/layers/fused_moe/modular_kernel.py
3	2	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
9	2	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
10	2	vllm/model_executor/layers/fused_moe/prepare_finalize.py
5	2	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
2	2	vllm/model_executor/layers/fused_moe/utils.py
34	7	vllm/model_executor/layers/quantization/fp8.py
15	0	vllm/platforms/cuda.py

[01eee4053] Reid 2025-06-04 [doc] update docker version (#19074)
2	2	docs/deployment/docker.md

[19bdaf32b] SorenDreano 2025-06-03 [Doc] Readme standardization (#18695)
5	5	README.md

[02f0c7b22] Simon Mo 2025-06-03 [Misc] Add SPDX-FileCopyrightText  (#19100)
1	0	.buildkite/check-wheel-size.py
1	0	.buildkite/generate_index.py
1	0	.buildkite/lm-eval-harness/conftest.py
1	0	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
1	0	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
1	0	.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
1	0	.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
1	0	.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
1	0	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
1	0	benchmarks/backend_request_func.py
1	0	benchmarks/benchmark_dataset.py
1	0	benchmarks/benchmark_latency.py
1	0	benchmarks/benchmark_long_document_qa_throughput.py
1	0	benchmarks/benchmark_prefix_caching.py
1	0	benchmarks/benchmark_prioritization.py
1	0	benchmarks/benchmark_serving.py
1	0	benchmarks/benchmark_serving_structured_output.py
1	0	benchmarks/benchmark_throughput.py
1	0	benchmarks/benchmark_utils.py
1	0	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
1	0	benchmarks/cutlass_benchmarks/utils.py
1	0	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
1	0	benchmarks/cutlass_benchmarks/weight_shapes.py
1	0	benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
1	0	benchmarks/disagg_benchmarks/round_robin_proxy.py
1	0	benchmarks/disagg_benchmarks/visualize_benchmark_results.py
1	0	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
1	0	benchmarks/kernels/bench_fp8_gemm.py
1	0	benchmarks/kernels/benchmark_aqlm.py
1	0	benchmarks/kernels/benchmark_bitblas.py
1	0	benchmarks/kernels/benchmark_cutlass_fp4_moe.py
1	0	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
1	0	benchmarks/kernels/benchmark_layernorm.py
1	0	benchmarks/kernels/benchmark_lora.py
1	0	benchmarks/kernels/benchmark_machete.py
1	0	benchmarks/kernels/benchmark_marlin.py
1	0	benchmarks/kernels/benchmark_moe.py
1	0	benchmarks/kernels/benchmark_moe_permute_unpermute.py
1	0	benchmarks/kernels/benchmark_paged_attention.py
1	0	benchmarks/kernels/benchmark_quant.py
1	0	benchmarks/kernels/benchmark_rmsnorm.py
1	0	benchmarks/kernels/benchmark_rope.py
1	0	benchmarks/kernels/benchmark_shapes.py
1	0	benchmarks/kernels/benchmark_w8a8_block_fp8.py
1	0	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
1	0	benchmarks/kernels/graph_machete_bench.py
1	0	benchmarks/kernels/utils.py
1	0	benchmarks/kernels/weight_shapes.py
1	0	benchmarks/overheads/benchmark_hashing.py
1	0	cmake/hipify.py
1	0	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
1	0	csrc/moe/marlin_moe_wna16/generate_kernels.py
1	0	csrc/quantization/gptq_marlin/generate_kernels.py
1	0	csrc/quantization/machete/generate.py
1	0	docs/mkdocs/hooks/generate_examples.py
1	0	docs/mkdocs/hooks/remove_announcement.py
1	0	docs/mkdocs/hooks/url_schemes.py
1	0	examples/offline_inference/audio_language.py
1	0	examples/offline_inference/automatic_prefix_caching.py
1	0	examples/offline_inference/basic/basic.py
1	0	examples/offline_inference/basic/chat.py
1	0	examples/offline_inference/basic/classify.py
1	0	examples/offline_inference/basic/embed.py
1	0	examples/offline_inference/basic/generate.py
1	0	examples/offline_inference/basic/score.py
1	0	examples/offline_inference/batch_llm_inference.py
1	0	examples/offline_inference/chat_with_tools.py
1	0	examples/offline_inference/context_extension.py
1	0	examples/offline_inference/data_parallel.py
1	0	examples/offline_inference/disaggregated-prefill-v1/decode_example.py
1	0	examples/offline_inference/disaggregated-prefill-v1/prefill_example.py
1	0	examples/offline_inference/disaggregated_prefill.py
1	0	examples/offline_inference/eagle.py
1	0	examples/offline_inference/embed_jina_embeddings_v3.py
1	0	examples/offline_inference/embed_matryoshka_fy.py
1	0	examples/offline_inference/encoder_decoder.py
1	0	examples/offline_inference/encoder_decoder_multimodal.py
1	0	examples/offline_inference/llm_engine_example.py
1	0	examples/offline_inference/load_sharded_state.py
1	0	examples/offline_inference/lora_with_quantization_inference.py
1	0	examples/offline_inference/metrics.py
1	0	examples/offline_inference/mistral-small.py
1	0	examples/offline_inference/mlpspeculator.py
1	0	examples/offline_inference/multilora_inference.py
1	0	examples/offline_inference/neuron.py
1	0	examples/offline_inference/neuron_eagle.py
1	0	examples/offline_inference/neuron_int8_quantization.py
1	0	examples/offline_inference/neuron_multimodal.py
1	0	examples/offline_inference/neuron_speculation.py
1	0	examples/offline_inference/prefix_caching.py
1	0	examples/offline_inference/prithvi_geospatial_mae.py
1	0	examples/offline_inference/profiling.py
1	0	examples/offline_inference/profiling_tpu/profiling.py
1	0	examples/offline_inference/prompt_embed_inference.py
1	0	examples/offline_inference/qwen2_5_omni/only_thinker.py
1	0	examples/offline_inference/qwen_1m.py
1	0	examples/offline_inference/reproducibility.py
1	0	examples/offline_inference/rlhf.py
1	0	examples/offline_inference/rlhf_colocate.py
1	0	examples/offline_inference/rlhf_utils.py
1	0	examples/offline_inference/save_sharded_state.py
1	0	examples/offline_inference/simple_profiling.py
1	0	examples/offline_inference/structured_outputs.py
1	0	examples/offline_inference/torchrun_example.py
1	0	examples/offline_inference/tpu.py
1	0	examples/offline_inference/vision_language.py
1	0	examples/offline_inference/vision_language_embedding.py
1	0	examples/offline_inference/vision_language_multi_image.py
1	0	examples/online_serving/api_client.py
1	0	examples/online_serving/cohere_rerank_client.py
1	0	examples/online_serving/disaggregated_serving/disagg_proxy_demo.py
1	0	examples/online_serving/gradio_openai_chatbot_webserver.py
1	0	examples/online_serving/gradio_webserver.py
1	0	examples/online_serving/jinaai_rerank_client.py
1	0	examples/online_serving/kv_events_subscriber.py
1	0	examples/online_serving/openai_chat_completion_client.py
1	0	examples/online_serving/openai_chat_completion_client_for_multimodal.py
1	0	examples/online_serving/openai_chat_completion_client_with_tools.py
1	0	examples/online_serving/openai_chat_completion_client_with_tools_required.py
1	0	examples/online_serving/openai_chat_completion_structured_outputs.py
1	0	examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py
1	0	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
1	0	examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py
1	0	examples/online_serving/openai_chat_completion_with_reasoning.py
1	0	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
1	0	examples/online_serving/openai_chat_embedding_client_for_multimodal.py
1	0	examples/online_serving/openai_classification_client.py
1	0	examples/online_serving/openai_completion_client.py
1	0	examples/online_serving/openai_cross_encoder_score.py
1	0	examples/online_serving/openai_embedding_client.py
1	0	examples/online_serving/openai_embedding_matryoshka_fy.py
1	0	examples/online_serving/openai_pooling_client.py
1	0	examples/online_serving/openai_transcription_client.py
1	0	examples/online_serving/opentelemetry/dummy_client.py
1	0	examples/online_serving/prompt_embed_inference_with_openai_client.py
1	0	examples/online_serving/ray_serve_deepseek.py
1	0	examples/online_serving/retrieval_augmented_generation_with_langchain.py
1	0	examples/online_serving/retrieval_augmented_generation_with_llamaindex.py
1	0	examples/online_serving/streamlit_openai_chatbot_webserver.py
1	0	examples/online_serving/utils.py
1	0	examples/others/lmcache/cpu_offload_lmcache.py
1	0	examples/others/lmcache/disagg_prefill_lmcache_v0.py
1	0	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py
1	0	examples/others/lmcache/kv_cache_sharing_lmcache_v1.py
1	0	examples/others/tensorize_vllm_model.py
1	0	find_cuda_init.py
1	0	setup.py
1	0	tests/async_engine/api_server_async_engine.py
1	0	tests/async_engine/conftest.py
1	0	tests/async_engine/test_api_server.py
1	0	tests/async_engine/test_async_llm_engine.py
1	0	tests/async_engine/test_request_tracker.py
1	0	tests/basic_correctness/test_basic_correctness.py
1	0	tests/basic_correctness/test_chunked_prefill.py
1	0	tests/basic_correctness/test_cpu_offload.py
1	0	tests/basic_correctness/test_cumem.py
1	0	tests/basic_correctness/test_preemption.py
1	0	tests/benchmarks/test_latency_cli.py
1	0	tests/benchmarks/test_serve_cli.py
1	0	tests/benchmarks/test_throughput_cli.py
1	0	tests/build_cython.py
1	0	tests/compile/backend.py
1	0	tests/compile/conftest.py
1	0	tests/compile/piecewise/test_full_cudagraph.py
1	0	tests/compile/piecewise/test_simple.py
1	0	tests/compile/piecewise/test_toy_llama.py
1	0	tests/compile/test_async_tp.py
1	0	tests/compile/test_basic_correctness.py
1	0	tests/compile/test_full_graph.py
1	0	tests/compile/test_functionalization.py
1	0	tests/compile/test_fusion.py
1	0	tests/compile/test_pass_manager.py
1	0	tests/compile/test_sequence_parallelism.py
1	0	tests/compile/test_silu_mul_quant_fusion.py
1	0	tests/compile/test_wrapper.py
1	0	tests/conftest.py
1	0	tests/core/block/conftest.py
1	0	tests/core/block/e2e/conftest.py
1	0	tests/core/block/e2e/test_correctness.py
1	0	tests/core/block/e2e/test_correctness_sliding_window.py
1	0	tests/core/block/test_block_manager.py
1	0	tests/core/block/test_block_table.py
1	0	tests/core/block/test_common.py
1	0	tests/core/block/test_cpu_gpu_block_allocator.py
1	0	tests/core/block/test_naive_block.py
1	0	tests/core/block/test_prefix_caching_block.py
1	0	tests/core/conftest.py
1	0	tests/core/test_chunked_prefill_scheduler.py
1	0	tests/core/test_num_computed_tokens_update.py
1	0	tests/core/test_scheduler.py
1	0	tests/core/test_scheduler_encoder_decoder.py
1	0	tests/core/test_serialization.py
1	0	tests/core/utils.py
1	0	tests/detokenizer/conftest.py
1	0	tests/detokenizer/test_disable_detokenization.py
1	0	tests/detokenizer/test_stop_checker.py
1	0	tests/detokenizer/test_stop_reason.py
1	0	tests/detokenizer/test_stop_strings.py
1	0	tests/distributed/conftest.py
1	0	tests/distributed/test_ca_buffer_sharing.py
1	0	tests/distributed/test_comm_ops.py
1	0	tests/distributed/test_custom_all_reduce.py
1	0	tests/distributed/test_distributed_oot.py
1	0	tests/distributed/test_events.py
1	0	tests/distributed/test_expert_parallel.py
1	0	tests/distributed/test_multi_node_assignment.py
1	0	tests/distributed/test_pipeline_parallel.py
1	0	tests/distributed/test_pipeline_partition.py
1	0	tests/distributed/test_pp_cudagraph.py
1	0	tests/distributed/test_pynccl.py
1	0	tests/distributed/test_same_node.py
1	0	tests/distributed/test_sequence_parallel.py
1	0	tests/distributed/test_shm_broadcast.py
1	0	tests/distributed/test_torchrun_example.py
1	0	tests/distributed/test_utils.py
1	0	tests/encoder_decoder/test_e2e_correctness.py
1	0	tests/engine/conftest.py
1	0	tests/engine/test_arg_utils.py
1	0	tests/engine/test_computed_prefix_blocks.py
1	0	tests/engine/test_executor.py
1	0	tests/engine/test_multi_step_output_processor.py
1	0	tests/engine/test_multiproc_workers.py
1	0	tests/engine/test_options.py
1	0	tests/engine/test_short_mm_context.py
1	0	tests/entrypoints/conftest.py
1	0	tests/entrypoints/llm/test_accuracy.py
1	0	tests/entrypoints/llm/test_chat.py
1	0	tests/entrypoints/llm/test_collective_rpc.py
1	0	tests/entrypoints/llm/test_encode.py
1	0	tests/entrypoints/llm/test_generate.py
1	0	tests/entrypoints/llm/test_generate_multiple_loras.py
1	0	tests/entrypoints/llm/test_gpu_utilization.py
1	0	tests/entrypoints/llm/test_guided_generate.py
1	0	tests/entrypoints/llm/test_lazy_outlines.py
1	0	tests/entrypoints/llm/test_prompt_validation.py
1	0	tests/entrypoints/offline_mode/test_offline_mode.py
1	0	tests/entrypoints/openai/correctness/test_lmeval.py
1	0	tests/entrypoints/openai/correctness/test_mteb.py
1	0	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py
1	0	tests/entrypoints/openai/test_async_tokenization.py
1	0	tests/entrypoints/openai/test_audio.py
1	0	tests/entrypoints/openai/test_basic.py
1	0	tests/entrypoints/openai/test_chat.py
1	0	tests/entrypoints/openai/test_chat_echo.py
1	0	tests/entrypoints/openai/test_chat_logit_bias_validation.py
1	0	tests/entrypoints/openai/test_chat_template.py
1	0	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
1	0	tests/entrypoints/openai/test_chunked_prompt.py
1	0	tests/entrypoints/openai/test_classification.py
1	0	tests/entrypoints/openai/test_cli_args.py
1	0	tests/entrypoints/openai/test_completion.py
1	0	tests/entrypoints/openai/test_completion_with_function_calling.py
1	0	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
1	0	tests/entrypoints/openai/test_embedding.py
1	0	tests/entrypoints/openai/test_embedding_dimensions.py
1	0	tests/entrypoints/openai/test_encoder_decoder.py
1	0	tests/entrypoints/openai/test_lora_adapters.py
1	0	tests/entrypoints/openai/test_lora_resolvers.py
1	0	tests/entrypoints/openai/test_metrics.py
1	0	tests/entrypoints/openai/test_models.py
1	0	tests/entrypoints/openai/test_oot_registration.py
1	0	tests/entrypoints/openai/test_openai_schema.py
1	0	tests/entrypoints/openai/test_pooling.py
1	0	tests/entrypoints/openai/test_prompt_validation.py
1	0	tests/entrypoints/openai/test_rerank.py
1	0	tests/entrypoints/openai/test_return_tokens_as_ids.py
1	0	tests/entrypoints/openai/test_root_path.py
1	0	tests/entrypoints/openai/test_run_batch.py
1	0	tests/entrypoints/openai/test_score.py
1	0	tests/entrypoints/openai/test_serving_chat.py
1	0	tests/entrypoints/openai/test_serving_models.py
1	0	tests/entrypoints/openai/test_shutdown.py
1	0	tests/entrypoints/openai/test_sleep.py
1	0	tests/entrypoints/openai/test_tensorizer_entrypoint.py
1	0	tests/entrypoints/openai/test_tokenization.py
1	0	tests/entrypoints/openai/test_transcription_validation.py
1	0	tests/entrypoints/openai/test_truncation.py
1	0	tests/entrypoints/openai/test_video.py
1	0	tests/entrypoints/openai/test_vision.py
1	0	tests/entrypoints/openai/test_vision_embedding.py
1	0	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
1	0	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
1	0	tests/entrypoints/openai/tool_parsers/utils.py
1	0	tests/entrypoints/test_api_server_process_manager.py
1	0	tests/entrypoints/test_chat_utils.py
1	0	tests/entrypoints/test_ssl_cert_refresher.py
1	0	tests/fastsafetensors_loader/test_fastsafetensors_loader.py
1	0	tests/fastsafetensors_loader/test_weight_utils.py
1	0	tests/kernels/allclose_default.py
1	0	tests/kernels/attention/conftest.py
1	0	tests/kernels/attention/test_attention.py
1	0	tests/kernels/attention/test_attention_selector.py
1	0	tests/kernels/attention/test_blocksparse_attention.py
1	0	tests/kernels/attention/test_cache.py
1	0	tests/kernels/attention/test_cascade_flash_attn.py
1	0	tests/kernels/attention/test_encoder_decoder_attn.py
1	0	tests/kernels/attention/test_flash_attn.py
1	0	tests/kernels/attention/test_flashinfer.py
1	0	tests/kernels/attention/test_flashmla.py
1	0	tests/kernels/attention/test_lightning_attn.py
1	0	tests/kernels/attention/test_merge_attn_states.py
1	0	tests/kernels/attention/test_mha_attn.py
1	0	tests/kernels/attention/test_mla_decode_cpu.py
1	0	tests/kernels/attention/test_prefix_prefill.py
1	0	tests/kernels/attention/test_rocm_attention_selector.py
1	0	tests/kernels/attention/test_triton_decode_attention.py
1	0	tests/kernels/attention/test_triton_unified_attention.py
1	0	tests/kernels/core/test_activation.py
1	0	tests/kernels/core/test_fused_quant_layernorm.py
1	0	tests/kernels/core/test_layernorm.py
1	0	tests/kernels/core/test_opcheck.py
1	0	tests/kernels/core/test_permute_cols.py
1	0	tests/kernels/core/test_pos_encoding.py
1	0	tests/kernels/core/test_rotary_embedding.py
1	0	tests/kernels/core/test_uva.py
1	0	tests/kernels/mamba/test_causal_conv1d.py
1	0	tests/kernels/mamba/test_mamba_mixer2.py
1	0	tests/kernels/mamba/test_mamba_ssm.py
1	0	tests/kernels/mamba/test_mamba_ssm_ssd.py
1	0	tests/kernels/moe/test_batched_moe.py
1	0	tests/kernels/moe/test_cutlass_moe.py
1	0	tests/kernels/moe/test_moe.py
1	0	tests/kernels/moe/test_moe_permute_unpermute.py
1	0	tests/kernels/moe/test_nvfp4_moe.py
1	0	tests/kernels/moe/test_pplx_moe.py
1	0	tests/kernels/moe/test_rocm_aiter_topk.py
1	0	tests/kernels/moe/test_triton_moe_ptpc_fp8.py
1	0	tests/kernels/quant_utils.py
1	0	tests/kernels/quantization/nvfp4_utils.py
1	0	tests/kernels/quantization/test_allspark_gemm.py
1	0	tests/kernels/quantization/test_aqlm.py
1	0	tests/kernels/quantization/test_awq.py
1	0	tests/kernels/quantization/test_awq_triton.py
1	0	tests/kernels/quantization/test_block_fp8.py
1	0	tests/kernels/quantization/test_block_int8.py
1	0	tests/kernels/quantization/test_cutlass_2of4_sparse.py
1	0	tests/kernels/quantization/test_cutlass_scaled_mm.py
1	0	tests/kernels/quantization/test_fp8_quant.py
1	0	tests/kernels/quantization/test_ggml.py
1	0	tests/kernels/quantization/test_gguf.py
1	0	tests/kernels/quantization/test_gptq.py
1	0	tests/kernels/quantization/test_int8_kernel.py
1	0	tests/kernels/quantization/test_int8_quant.py
1	0	tests/kernels/quantization/test_machete_mm.py
1	0	tests/kernels/quantization/test_marlin_gemm.py
1	0	tests/kernels/quantization/test_nvfp4_quant.py
1	0	tests/kernels/quantization/test_nvfp4_scaled_mm.py
1	0	tests/kernels/quantization/test_rocm_skinny_gemms.py
1	0	tests/kernels/quantization/test_triton_scaled_mm.py
1	0	tests/kernels/test_cutlass_mla_decode.py
1	0	tests/kernels/test_fused_quant_activation.py
1	0	tests/kernels/test_triton_flash_attention.py
1	0	tests/kernels/utils.py
1	0	tests/kv_transfer/test_disagg.py
1	0	tests/kv_transfer/test_lookup_buffer.py
1	0	tests/kv_transfer/test_module.py
1	0	tests/kv_transfer/test_send_recv.py
1	0	tests/lora/conftest.py
1	0	tests/lora/test_add_lora.py
1	0	tests/lora/test_baichuan.py
1	0	tests/lora/test_chatglm3_tp.py
1	0	tests/lora/test_layers.py
1	0	tests/lora/test_llama_tp.py
1	0	tests/lora/test_lora_allowed_token_ids.py
1	0	tests/lora/test_lora_checkpoints.py
1	0	tests/lora/test_lora_functions.py
1	0	tests/lora/test_lora_huggingface.py
1	0	tests/lora/test_lora_manager.py
1	0	tests/lora/test_minicpmv_tp.py
1	0	tests/lora/test_mixtral.py
1	0	tests/lora/test_peft_helper.py
1	0	tests/lora/test_phi.py
1	0	tests/lora/test_punica_ops.py
1	0	tests/lora/test_quant_model.py
1	0	tests/lora/test_qwen2vl.py
1	0	tests/lora/test_resolver.py
1	0	tests/lora/test_tokenizer_group.py
1	0	tests/lora/test_transfomers_model.py
1	0	tests/lora/test_utils.py
1	0	tests/lora/test_worker.py
1	0	tests/lora/utils.py
1	0	tests/metrics/test_metrics.py
1	0	tests/mistral_tool_use/conftest.py
1	0	tests/mistral_tool_use/test_mistral_tool_calls.py
1	0	tests/mistral_tool_use/utils.py
1	0	tests/model_executor/conftest.py
1	0	tests/model_executor/test_enabled_custom_ops.py
1	0	tests/model_executor/test_guided_processors.py
1	0	tests/model_executor/test_logits_processor.py
1	0	tests/model_executor/test_model_load_with_params.py
1	0	tests/model_executor/test_weight_utils.py
1	0	tests/models/language/generation/test_bart.py
1	0	tests/models/language/generation/test_common.py
1	0	tests/models/language/generation/test_granite.py
1	0	tests/models/language/generation/test_granitemoehybrid.py
1	0	tests/models/language/generation/test_hybrid.py
1	0	tests/models/language/generation/test_mistral.py
1	0	tests/models/language/generation/test_phimoe.py
1	0	tests/models/language/pooling/embed_utils.py
1	0	tests/models/language/pooling/mteb_utils.py
1	0	tests/models/language/pooling/test_baai.py
1	0	tests/models/language/pooling/test_classification.py
1	0	tests/models/language/pooling/test_embedding.py
1	0	tests/models/language/pooling/test_gritlm.py
1	0	tests/models/language/pooling/test_gte.py
1	0	tests/models/language/pooling/test_jina.py
1	0	tests/models/language/pooling/test_nomic.py
1	0	tests/models/language/pooling/test_nomic_max_model_len.py
1	0	tests/models/language/pooling/test_scoring.py
1	0	tests/models/language/pooling/test_snowflake_arctic_embed.py
1	0	tests/models/language/pooling/test_truncation_control.py
1	0	tests/models/multimodal/generation/test_common.py
1	0	tests/models/multimodal/generation/test_florence2.py
1	0	tests/models/multimodal/generation/test_granite_speech.py
1	0	tests/models/multimodal/generation/test_interleaved.py
1	0	tests/models/multimodal/generation/test_mllama.py
1	0	tests/models/multimodal/generation/test_phi4mm.py
1	0	tests/models/multimodal/generation/test_pixtral.py
1	0	tests/models/multimodal/generation/test_qwen2_vl.py
1	0	tests/models/multimodal/generation/test_ultravox.py
1	0	tests/models/multimodal/generation/test_whisper.py
1	0	tests/models/multimodal/generation/vlm_utils/builders.py
1	0	tests/models/multimodal/generation/vlm_utils/case_filtering.py
1	0	tests/models/multimodal/generation/vlm_utils/core.py
1	0	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
1	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	0	tests/models/multimodal/generation/vlm_utils/runners.py
1	0	tests/models/multimodal/generation/vlm_utils/types.py
1	0	tests/models/multimodal/pooling/test_dse_qwen2_vl.py
1	0	tests/models/multimodal/pooling/test_intern_vit.py
1	0	tests/models/multimodal/pooling/test_llava_next.py
1	0	tests/models/multimodal/pooling/test_phi3v.py
1	0	tests/models/multimodal/processing/test_common.py
1	0	tests/models/multimodal/processing/test_h2ovl.py
1	0	tests/models/multimodal/processing/test_idefics3.py
1	0	tests/models/multimodal/processing/test_internvl.py
1	0	tests/models/multimodal/processing/test_llama4.py
1	0	tests/models/multimodal/processing/test_llava_next.py
1	0	tests/models/multimodal/processing/test_llava_onevision.py
1	0	tests/models/multimodal/processing/test_minimax_vl_01.py
1	0	tests/models/multimodal/processing/test_mllama.py
1	0	tests/models/multimodal/processing/test_phi3v.py
1	0	tests/models/multimodal/processing/test_phi4mm.py
1	0	tests/models/multimodal/processing/test_qwen2_vl.py
1	0	tests/models/multimodal/processing/test_smolvlm.py
1	0	tests/models/quantization/test_aqlm.py
1	0	tests/models/quantization/test_awq.py
1	0	tests/models/quantization/test_bitblas.py
1	0	tests/models/quantization/test_fp8.py
1	0	tests/models/quantization/test_gguf.py
1	0	tests/models/quantization/test_gptq_bitblas.py
1	0	tests/models/quantization/test_gptq_marlin.py
1	0	tests/models/quantization/test_gptq_marlin_24.py
1	0	tests/models/quantization/test_modelopt.py
1	0	tests/models/quantization/test_mxfp4.py
1	0	tests/models/quantization/test_nvfp4.py
1	0	tests/models/registry.py
1	0	tests/models/test_initialization.py
1	0	tests/models/test_oot_registration.py
1	0	tests/models/test_registry.py
1	0	tests/models/test_transformers.py
1	0	tests/models/test_utils.py
1	0	tests/models/test_vision.py
1	0	tests/models/utils.py
1	0	tests/mq_llm_engine/conftest.py
1	0	tests/mq_llm_engine/test_abort.py
1	0	tests/mq_llm_engine/test_error_handling.py
1	0	tests/mq_llm_engine/test_load.py
1	0	tests/mq_llm_engine/utils.py
1	0	tests/multi_step/test_correctness_async_llm.py
1	0	tests/multi_step/test_correctness_llm.py
1	0	tests/multimodal/test_hasher.py
1	0	tests/multimodal/test_image.py
1	0	tests/multimodal/test_inputs.py
1	0	tests/multimodal/test_processing.py
1	0	tests/multimodal/test_utils.py
1	0	tests/multimodal/test_video.py
1	0	tests/multimodal/utils.py
1	0	tests/neuron/1_core/test_activation.py
1	0	tests/neuron/1_core/test_block_table.py
1	0	tests/neuron/1_core/test_cache.py
1	0	tests/neuron/1_core/test_layernorm.py
1	0	tests/neuron/1_core/test_logits_processor.py
1	0	tests/neuron/1_core/test_neuron_model_runner.py
1	0	tests/neuron/1_core/test_neuron_quant.py
1	0	tests/neuron/1_core/test_prefix_prefill.py
1	0	tests/neuron/1_core/test_rotary_embedding.py
1	0	tests/neuron/2_core/test_comm_ops.py
1	0	tests/neuron/2_core/test_eagle.py
1	0	tests/neuron/2_core/test_mistral.py
1	0	tests/neuron/2_core/test_multi_lora.py
1	0	tests/plugins/lora_resolvers/test_filesystem_resolver.py
1	0	tests/plugins/vllm_add_dummy_model/setup.py
1	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py
1	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
1	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
1	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py
1	0	tests/plugins/vllm_add_dummy_platform/setup.py
1	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/__init__.py
1	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py
1	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
1	0	tests/plugins_tests/conftest.py
1	0	tests/plugins_tests/test_platform_plugins.py
1	0	tests/plugins_tests/test_scheduler_plugins.py
1	0	tests/prefix_caching/test_disable_sliding_window.py
1	0	tests/prefix_caching/test_prefix_caching.py
1	0	tests/prompt_adapter/test_bloom.py
1	0	tests/prompt_adapter/test_multi_adapter_inference.py
1	0	tests/prompt_adapter/test_pa_lora.py
1	0	tests/quantization/test_auto_round.py
1	0	tests/quantization/test_bitsandbytes.py
1	0	tests/quantization/test_compressed_tensors.py
1	0	tests/quantization/test_configs.py
2	1	tests/quantization/test_cpu_offload.py
1	0	tests/quantization/test_experts_int8.py
1	0	tests/quantization/test_fp8.py
1	0	tests/quantization/test_gptq_dynamic.py
1	0	tests/quantization/test_ipex_quant.py
1	0	tests/quantization/test_lm_head.py
1	0	tests/quantization/test_ptpc_fp8.py
1	0	tests/quantization/test_quark.py
1	0	tests/quantization/test_register_quantization_config.py
1	0	tests/quantization/test_torchao.py
1	0	tests/quantization/utils.py
1	0	tests/reasoning/test_deepseekr1_reasoning_parser.py
1	0	tests/reasoning/test_granite_reasoning_parser.py
1	0	tests/reasoning/test_qwen3_reasoning_parser.py
1	0	tests/reasoning/utils.py
1	0	tests/runai_model_streamer_test/test_runai_model_streamer_loader.py
1	0	tests/runai_model_streamer_test/test_weight_utils.py
1	0	tests/samplers/test_beam_search.py
1	0	tests/samplers/test_ignore_eos.py
1	0	tests/samplers/test_logits_processor.py
1	0	tests/samplers/test_logprobs.py
1	0	tests/samplers/test_no_bad_words.py
1	0	tests/samplers/test_ranks.py
1	0	tests/samplers/test_rejection_sampler.py
1	0	tests/samplers/test_sampler.py
1	0	tests/samplers/test_seeded_generate.py
1	0	tests/samplers/test_typical_acceptance_sampler.py
1	0	tests/spec_decode/conftest.py
1	0	tests/spec_decode/e2e/conftest.py
1	0	tests/spec_decode/e2e/test_compatibility.py
1	0	tests/spec_decode/e2e/test_eagle_correctness.py
1	0	tests/spec_decode/e2e/test_integration.py
1	0	tests/spec_decode/e2e/test_integration_dist_tp2.py
1	0	tests/spec_decode/e2e/test_integration_dist_tp4.py
1	0	tests/spec_decode/e2e/test_logprobs.py
1	0	tests/spec_decode/e2e/test_medusa_correctness.py
1	0	tests/spec_decode/e2e/test_mlp_correctness.py
1	0	tests/spec_decode/e2e/test_mtp_correctness.py
1	0	tests/spec_decode/e2e/test_multistep_correctness.py
1	0	tests/spec_decode/e2e/test_ngram_correctness.py
1	0	tests/spec_decode/e2e/test_seed.py
1	0	tests/spec_decode/test_batch_expansion.py
1	0	tests/spec_decode/test_dynamic_spec_decode.py
1	0	tests/spec_decode/test_memory_usage.py
1	0	tests/spec_decode/test_metrics.py
1	0	tests/spec_decode/test_multi_step_worker.py
1	0	tests/spec_decode/test_ngram_worker.py
1	0	tests/spec_decode/test_scorer.py
1	0	tests/spec_decode/test_spec_decode_worker.py
1	0	tests/spec_decode/test_utils.py
1	0	tests/spec_decode/utils.py
1	0	tests/standalone_tests/lazy_imports.py
1	0	tests/tensorizer_loader/conftest.py
1	0	tests/tensorizer_loader/test_tensorizer.py
1	0	tests/test_cache_block_hashing.py
1	0	tests/test_config.py
1	0	tests/test_embedded_commit.py
1	0	tests/test_inputs.py
1	0	tests/test_logger.py
1	0	tests/test_outputs.py
1	0	tests/test_regression.py
1	0	tests/test_sampling_params.py
1	0	tests/test_scalartype.py
2	1	tests/test_seed_behavior.py
1	0	tests/test_sequence.py
1	0	tests/test_sharded_state_loader.py
1	0	tests/test_triton_utils.py
1	0	tests/test_utils.py
1	0	tests/test_version.py
1	0	tests/test_vllm_port.py
1	0	tests/tokenization/test_cached_tokenizer.py
1	0	tests/tokenization/test_detokenize.py
1	0	tests/tokenization/test_get_eos.py
1	0	tests/tokenization/test_mistral_tokenizer.py
1	0	tests/tokenization/test_tokenizer.py
1	0	tests/tokenization/test_tokenizer_group.py
1	0	tests/tokenization/test_tokenizer_registry.py
1	0	tests/tool_use/conftest.py
1	0	tests/tool_use/test_chat_completion_request_validations.py
1	0	tests/tool_use/test_chat_completions.py
1	0	tests/tool_use/test_jamba_tool_parser.py
1	0	tests/tool_use/test_parallel_tool_calls.py
1	0	tests/tool_use/test_tool_calls.py
1	0	tests/tool_use/test_tool_choice_required.py
1	0	tests/tool_use/utils.py
1	0	tests/tpu/lora/test_lora.py
1	0	tests/tpu/test_compilation.py
1	0	tests/tpu/test_custom_dispatcher.py
1	0	tests/tpu/test_moe_pallas.py
1	0	tests/tpu/test_quantization_accuracy.py
1	0	tests/tracing/test_tracing.py
1	0	tests/utils.py
1	0	tests/v1/core/test_kv_cache_utils.py
1	0	tests/v1/core/test_prefix_caching.py
1	0	tests/v1/core/test_scheduler.py
1	0	tests/v1/core/test_scheduler_e2e.py
1	0	tests/v1/core/test_specialized_manager.py
1	0	tests/v1/e2e/test_cascade_attention.py
1	0	tests/v1/e2e/test_correctness_sliding_window.py
1	0	tests/v1/e2e/test_spec_decode.py
1	0	tests/v1/engine/conftest.py
1	0	tests/v1/engine/test_async_llm.py
1	0	tests/v1/engine/test_engine_args.py
1	0	tests/v1/engine/test_engine_core.py
1	0	tests/v1/engine/test_engine_core_client.py
1	0	tests/v1/engine/test_llm_engine.py
1	0	tests/v1/engine/test_output_processor.py
1	0	tests/v1/engine/utils.py
1	0	tests/v1/entrypoints/conftest.py
1	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	0	tests/v1/entrypoints/openai/test_chat_completion.py
1	0	tests/v1/entrypoints/openai/test_completion.py
1	0	tests/v1/entrypoints/openai/test_multi_api_servers.py
1	0	tests/v1/kv_connector/nixl_integration/test_accuracy.py
1	0	tests/v1/kv_connector/nixl_integration/test_edge_cases.py
1	0	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
1	0	tests/v1/kv_connector/unit/test_multi_connector.py
1	0	tests/v1/kv_connector/unit/test_nixl_connector.py
1	0	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
1	0	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
1	0	tests/v1/kv_connector/unit/utils.py
1	0	tests/v1/metrics/test_ray_metrics.py
1	0	tests/v1/sample/test_logprobs.py
1	0	tests/v1/sample/test_logprobs_e2e.py
1	0	tests/v1/sample/test_rejection_sampler.py
1	0	tests/v1/sample/test_sampler.py
1	0	tests/v1/sample/test_sampling_params_e2e.py
1	0	tests/v1/sample/test_topk_topp_sampler.py
1	0	tests/v1/sample/utils.py
1	0	tests/v1/shutdown/test_delete.py
1	0	tests/v1/shutdown/test_forward_error.py
1	0	tests/v1/shutdown/test_processor_error.py
1	0	tests/v1/shutdown/test_startup_error.py
1	0	tests/v1/shutdown/utils.py
1	0	tests/v1/spec_decode/test_eagle.py
1	0	tests/v1/spec_decode/test_max_len.py
1	0	tests/v1/spec_decode/test_ngram.py
1	0	tests/v1/structured_output/test_utils.py
1	0	tests/v1/test_async_llm_dp.py
1	0	tests/v1/test_metrics_reader.py
1	0	tests/v1/test_oracle.py
1	0	tests/v1/test_serial_utils.py
1	0	tests/v1/test_utils.py
1	0	tests/v1/tpu/test_basic.py
1	0	tests/v1/tpu/test_mha_attn.py
1	0	tests/v1/tpu/test_multimodal.py
1	0	tests/v1/tpu/test_pallas.py
1	0	tests/v1/tpu/test_perf.py
1	0	tests/v1/tpu/test_sampler.py
1	0	tests/v1/tpu/test_topk_topp_sampler.py
1	0	tests/v1/tpu/worker/test_tpu_model_runner.py
1	0	tests/v1/worker/test_gpu_input_batch.py
1	0	tests/v1/worker/test_gpu_model_runner.py
1	0	tests/vllm_test_utils/setup.py
1	0	tests/vllm_test_utils/vllm_test_utils/__init__.py
1	0	tests/vllm_test_utils/vllm_test_utils/blame.py
1	0	tests/vllm_test_utils/vllm_test_utils/monitor.py
1	0	tests/weight_loading/test_weight_loading.py
1	0	tests/worker/conftest.py
1	0	tests/worker/test_encoder_decoder_model_runner.py
1	0	tests/worker/test_model_input.py
1	0	tests/worker/test_model_runner.py
1	0	tests/worker/test_profile.py
1	0	tests/worker/test_swap.py
4	1	tools/check_spdx_header.py
1	0	tools/check_triton_import.py
1	0	tools/enforce_regex_import.py
1	0	tools/profiler/print_layerwise_table.py
1	0	tools/profiler/visualize_layerwise_profile.py
1	0	tools/report_build_time_ninja.py
1	0	use_existing_torch.py
1	0	vllm/__init__.py
1	0	vllm/_custom_ops.py
1	0	vllm/_ipex_ops.py
1	0	vllm/adapter_commons/layers.py
1	0	vllm/adapter_commons/models.py
1	0	vllm/adapter_commons/request.py
1	0	vllm/adapter_commons/utils.py
1	0	vllm/adapter_commons/worker_manager.py
1	0	vllm/assets/audio.py
1	0	vllm/assets/base.py
1	0	vllm/assets/image.py
1	0	vllm/assets/video.py
1	0	vllm/attention/__init__.py
1	0	vllm/attention/backends/abstract.py
1	0	vllm/attention/backends/blocksparse_attn.py
1	0	vllm/attention/backends/cpu_mla.py
1	0	vllm/attention/backends/dual_chunk_flash_attn.py
1	0	vllm/attention/backends/flash_attn.py
1	0	vllm/attention/backends/flashinfer.py
1	0	vllm/attention/backends/flashmla.py
1	0	vllm/attention/backends/hpu_attn.py
1	0	vllm/attention/backends/ipex_attn.py
1	0	vllm/attention/backends/mla/common.py
1	0	vllm/attention/backends/pallas.py
1	0	vllm/attention/backends/placeholder_attn.py
1	0	vllm/attention/backends/rocm_aiter_mla.py
1	0	vllm/attention/backends/rocm_flash_attn.py
1	0	vllm/attention/backends/torch_sdpa.py
1	0	vllm/attention/backends/triton_mla.py
1	0	vllm/attention/backends/utils.py
1	0	vllm/attention/backends/xformers.py
1	0	vllm/attention/layer.py
1	0	vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
1	0	vllm/attention/ops/blocksparse_attention/interface.py
1	0	vllm/attention/ops/blocksparse_attention/utils.py
1	0	vllm/attention/ops/chunked_prefill_paged_decode.py
1	0	vllm/attention/ops/flashmla.py
1	0	vllm/attention/ops/hpu_paged_attn.py
1	0	vllm/attention/ops/ipex_attn.py
1	0	vllm/attention/ops/merge_attn_states.py
1	0	vllm/attention/ops/nki_flash_attn.py
1	0	vllm/attention/ops/paged_attn.py
1	0	vllm/attention/ops/prefix_prefill.py
1	0	vllm/attention/ops/rocm_aiter_mla.py
1	0	vllm/attention/ops/rocm_aiter_paged_attn.py
1	0	vllm/attention/ops/triton_decode_attention.py
1	0	vllm/attention/ops/triton_flash_attention.py
1	0	vllm/attention/ops/triton_merge_attn_states.py
1	0	vllm/attention/ops/triton_unified_attention.py
1	0	vllm/attention/selector.py
1	0	vllm/attention/utils/fa_utils.py
1	0	vllm/beam_search.py
1	0	vllm/benchmarks/datasets.py
1	0	vllm/benchmarks/endpoint_request_func.py
1	0	vllm/benchmarks/latency.py
1	0	vllm/benchmarks/serve.py
1	0	vllm/benchmarks/throughput.py
1	0	vllm/benchmarks/utils.py
4	2	vllm/collect_env.py
1	0	vllm/compilation/activation_quant_fusion.py
1	0	vllm/compilation/backends.py
1	0	vllm/compilation/base_piecewise_backend.py
1	0	vllm/compilation/collective_fusion.py
1	0	vllm/compilation/compiler_interface.py
1	0	vllm/compilation/counter.py
1	0	vllm/compilation/cuda_piecewise_backend.py
1	0	vllm/compilation/decorators.py
1	0	vllm/compilation/fix_functionalization.py
1	0	vllm/compilation/fusion.py
1	0	vllm/compilation/fx_utils.py
1	0	vllm/compilation/inductor_pass.py
1	0	vllm/compilation/monitor.py
1	0	vllm/compilation/multi_output_match.py
1	0	vllm/compilation/noop_elimination.py
1	0	vllm/compilation/pass_manager.py
1	0	vllm/compilation/sequence_parallelism.py
1	0	vllm/compilation/torch25_custom_graph_pass.py
1	0	vllm/compilation/vllm_inductor_pass.py
1	0	vllm/compilation/wrapper.py
1	0	vllm/config.py
1	0	vllm/connections.py
1	0	vllm/core/block/block_table.py
1	0	vllm/core/block/common.py
1	0	vllm/core/block/cpu_gpu_block_allocator.py
1	0	vllm/core/block/interfaces.py
1	0	vllm/core/block/naive_block.py
1	0	vllm/core/block/prefix_caching_block.py
1	0	vllm/core/block/utils.py
1	0	vllm/core/block_manager.py
1	0	vllm/core/evictor.py
1	0	vllm/core/interfaces.py
1	0	vllm/core/placeholder_block_space_manager.py
1	0	vllm/core/scheduler.py
1	0	vllm/device_allocator/cumem.py
1	0	vllm/distributed/__init__.py
1	0	vllm/distributed/communication_op.py
1	0	vllm/distributed/device_communicators/all2all.py
1	0	vllm/distributed/device_communicators/base_device_communicator.py
1	0	vllm/distributed/device_communicators/cpu_communicator.py
1	0	vllm/distributed/device_communicators/cuda_communicator.py
1	0	vllm/distributed/device_communicators/cuda_wrapper.py
1	0	vllm/distributed/device_communicators/custom_all_reduce.py
1	0	vllm/distributed/device_communicators/custom_all_reduce_utils.py
1	0	vllm/distributed/device_communicators/hpu_communicator.py
1	0	vllm/distributed/device_communicators/neuron_communicator.py
1	0	vllm/distributed/device_communicators/pynccl.py
1	0	vllm/distributed/device_communicators/pynccl_wrapper.py
1	0	vllm/distributed/device_communicators/shm_broadcast.py
1	0	vllm/distributed/device_communicators/tpu_communicator.py
1	0	vllm/distributed/device_communicators/xpu_communicator.py
1	0	vllm/distributed/kv_events.py
1	0	vllm/distributed/kv_transfer/__init__.py
1	0	vllm/distributed/kv_transfer/kv_connector/base.py
1	0	vllm/distributed/kv_transfer/kv_connector/factory.py
1	0	vllm/distributed/kv_transfer/kv_connector/lmcache_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/utils.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
1	0	vllm/distributed/kv_transfer/kv_connector_agent.py
1	0	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
1	0	vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py
1	0	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
1	0	vllm/distributed/kv_transfer/kv_pipe/base.py
1	0	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
1	0	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
1	0	vllm/distributed/kv_transfer/kv_transfer_state.py
1	0	vllm/distributed/parallel_state.py
1	0	vllm/distributed/utils.py
1	0	vllm/engine/arg_utils.py
1	0	vllm/engine/async_llm_engine.py
1	0	vllm/engine/async_timeout.py
1	0	vllm/engine/llm_engine.py
1	0	vllm/engine/metrics.py
1	0	vllm/engine/metrics_types.py
1	0	vllm/engine/multiprocessing/__init__.py
1	0	vllm/engine/multiprocessing/client.py
1	0	vllm/engine/multiprocessing/engine.py
1	0	vllm/engine/output_processor/interfaces.py
1	0	vllm/engine/output_processor/multi_step.py
1	0	vllm/engine/output_processor/single_step.py
1	0	vllm/engine/output_processor/stop_checker.py
1	0	vllm/engine/output_processor/util.py
1	0	vllm/engine/protocol.py
1	0	vllm/entrypoints/api_server.py
1	0	vllm/entrypoints/chat_utils.py
1	0	vllm/entrypoints/cli/benchmark/base.py
1	0	vllm/entrypoints/cli/benchmark/latency.py
1	0	vllm/entrypoints/cli/benchmark/main.py
1	0	vllm/entrypoints/cli/benchmark/serve.py
1	0	vllm/entrypoints/cli/benchmark/throughput.py
1	0	vllm/entrypoints/cli/collect_env.py
1	0	vllm/entrypoints/cli/main.py
1	0	vllm/entrypoints/cli/openai.py
1	0	vllm/entrypoints/cli/run_batch.py
1	0	vllm/entrypoints/cli/serve.py
1	0	vllm/entrypoints/cli/types.py
1	0	vllm/entrypoints/launcher.py
1	0	vllm/entrypoints/llm.py
1	0	vllm/entrypoints/logger.py
1	0	vllm/entrypoints/openai/api_server.py
1	0	vllm/entrypoints/openai/cli_args.py
1	0	vllm/entrypoints/openai/logits_processors.py
1	0	vllm/entrypoints/openai/protocol.py
1	0	vllm/entrypoints/openai/run_batch.py
1	0	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/entrypoints/openai/serving_classification.py
1	0	vllm/entrypoints/openai/serving_completion.py
1	0	vllm/entrypoints/openai/serving_embedding.py
1	0	vllm/entrypoints/openai/serving_engine.py
1	0	vllm/entrypoints/openai/serving_models.py
1	0	vllm/entrypoints/openai/serving_pooling.py
1	0	vllm/entrypoints/openai/serving_score.py
1	0	vllm/entrypoints/openai/serving_tokenization.py
1	0	vllm/entrypoints/openai/serving_transcription.py
1	0	vllm/entrypoints/openai/tool_parsers/__init__.py
1	0	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
1	0	vllm/entrypoints/openai/tool_parsers/utils.py
1	0	vllm/entrypoints/score_utils.py
1	0	vllm/entrypoints/ssl.py
1	0	vllm/entrypoints/utils.py
1	0	vllm/env_override.py
1	0	vllm/envs.py
1	0	vllm/executor/executor_base.py
1	0	vllm/executor/mp_distributed_executor.py
1	0	vllm/executor/msgspec_utils.py
1	0	vllm/executor/multiproc_worker_utils.py
1	0	vllm/executor/ray_distributed_executor.py
1	0	vllm/executor/ray_utils.py
1	0	vllm/executor/uniproc_executor.py
1	0	vllm/forward_context.py
1	0	vllm/inputs/__init__.py
1	0	vllm/inputs/data.py
1	0	vllm/inputs/parse.py
1	0	vllm/inputs/preprocess.py
1	0	vllm/inputs/registry.py
1	0	vllm/jsontree.py
1	0	vllm/logger.py
1	0	vllm/logging_utils/__init__.py
1	0	vllm/logging_utils/dump_input.py
1	0	vllm/logging_utils/formatter.py
1	0	vllm/logits_process.py
1	0	vllm/lora/fully_sharded_layers.py
1	0	vllm/lora/layers.py
1	0	vllm/lora/lora.py
1	0	vllm/lora/models.py
1	0	vllm/lora/ops/torch_ops/__init__.py
1	0	vllm/lora/ops/torch_ops/lora_ops.py
1	0	vllm/lora/ops/triton_ops/__init__.py
1	0	vllm/lora/ops/triton_ops/kernel_utils.py
1	0	vllm/lora/ops/triton_ops/lora_expand_op.py
1	0	vllm/lora/ops/triton_ops/lora_kernel_metadata.py
1	0	vllm/lora/ops/triton_ops/lora_shrink_op.py
1	0	vllm/lora/ops/triton_ops/utils.py
1	0	vllm/lora/ops/xla_ops/__init__.py
1	0	vllm/lora/ops/xla_ops/lora_ops.py
1	0	vllm/lora/peft_helper.py
1	0	vllm/lora/punica_wrapper/__init__.py
1	0	vllm/lora/punica_wrapper/punica_base.py
1	0	vllm/lora/punica_wrapper/punica_cpu.py
1	0	vllm/lora/punica_wrapper/punica_gpu.py
1	0	vllm/lora/punica_wrapper/punica_hpu.py
1	0	vllm/lora/punica_wrapper/punica_selector.py
1	0	vllm/lora/punica_wrapper/punica_tpu.py
1	0	vllm/lora/punica_wrapper/utils.py
1	0	vllm/lora/request.py
1	0	vllm/lora/resolver.py
1	0	vllm/lora/utils.py
1	0	vllm/lora/worker_manager.py
1	0	vllm/model_executor/__init__.py
1	0	vllm/model_executor/custom_op.py
1	0	vllm/model_executor/guided_decoding/__init__.py
1	0	vllm/model_executor/guided_decoding/guidance_decoding.py
1	0	vllm/model_executor/guided_decoding/guidance_logits_processors.py
1	0	vllm/model_executor/guided_decoding/guided_fields.py
1	0	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
1	0	vllm/model_executor/guided_decoding/outlines_decoding.py
1	0	vllm/model_executor/guided_decoding/outlines_logits_processors.py
1	0	vllm/model_executor/guided_decoding/utils.py
1	0	vllm/model_executor/guided_decoding/xgrammar_decoding.py
1	0	vllm/model_executor/layers/activation.py
1	0	vllm/model_executor/layers/fused_moe/__init__.py
1	0	vllm/model_executor/layers/fused_moe/cutlass_moe.py
1	0	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py
1	0	vllm/model_executor/layers/fused_moe/layer.py
1	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
1	0	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
1	0	vllm/model_executor/layers/fused_moe/moe_pallas.py
1	0	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
1	0	vllm/model_executor/layers/fused_moe/moe_torch_iterative.py
1	0	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
1	0	vllm/model_executor/layers/fused_moe/prepare_finalize.py
1	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
1	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
1	0	vllm/model_executor/layers/fused_moe/utils.py
1	0	vllm/model_executor/layers/layernorm.py
1	0	vllm/model_executor/layers/lightning_attn.py
1	0	vllm/model_executor/layers/linear.py
1	0	vllm/model_executor/layers/logits_processor.py
1	0	vllm/model_executor/layers/mamba/mamba2_metadata.py
1	0	vllm/model_executor/layers/mamba/mamba_mixer.py
1	0	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	0	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
1	0	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
1	0	vllm/model_executor/layers/mamba/ops/ssd_bmm.py
1	0	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
1	0	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
1	0	vllm/model_executor/layers/mamba/ops/ssd_combined.py
1	0	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
1	0	vllm/model_executor/layers/pooler.py
1	0	vllm/model_executor/layers/quantization/__init__.py
1	0	vllm/model_executor/layers/quantization/aqlm.py
1	0	vllm/model_executor/layers/quantization/auto_round.py
1	0	vllm/model_executor/layers/quantization/awq.py
1	0	vllm/model_executor/layers/quantization/awq_marlin.py
1	0	vllm/model_executor/layers/quantization/awq_triton.py
1	0	vllm/model_executor/layers/quantization/base_config.py
1	0	vllm/model_executor/layers/quantization/bitblas.py
1	0	vllm/model_executor/layers/quantization/bitsandbytes.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
1	0	vllm/model_executor/layers/quantization/deepspeedfp.py
1	0	vllm/model_executor/layers/quantization/experts_int8.py
1	0	vllm/model_executor/layers/quantization/fbgemm_fp8.py
1	0	vllm/model_executor/layers/quantization/fp8.py
1	0	vllm/model_executor/layers/quantization/gguf.py
1	0	vllm/model_executor/layers/quantization/gptq.py
1	0	vllm/model_executor/layers/quantization/gptq_bitblas.py
1	0	vllm/model_executor/layers/quantization/gptq_marlin.py
1	0	vllm/model_executor/layers/quantization/gptq_marlin_24.py
1	0	vllm/model_executor/layers/quantization/hqq_marlin.py
1	0	vllm/model_executor/layers/quantization/ipex_quant.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
1	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
1	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
1	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
1	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
1	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
1	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
1	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
1	0	vllm/model_executor/layers/quantization/kv_cache.py
1	0	vllm/model_executor/layers/quantization/marlin.py
1	0	vllm/model_executor/layers/quantization/modelopt.py
1	0	vllm/model_executor/layers/quantization/moe_wna16.py
1	0	vllm/model_executor/layers/quantization/neuron_quant.py
1	0	vllm/model_executor/layers/quantization/ptpc_fp8.py
1	0	vllm/model_executor/layers/quantization/qqq.py
1	0	vllm/model_executor/layers/quantization/quark/quark.py
1	0	vllm/model_executor/layers/quantization/quark/quark_moe.py
1	0	vllm/model_executor/layers/quantization/quark/schemes/__init__.py
1	0	vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py
1	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
1	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
1	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
1	0	vllm/model_executor/layers/quantization/quark/utils.py
1	0	vllm/model_executor/layers/quantization/schema.py
1	0	vllm/model_executor/layers/quantization/torchao.py
1	0	vllm/model_executor/layers/quantization/tpu_int8.py
1	0	vllm/model_executor/layers/quantization/utils/__init__.py
1	0	vllm/model_executor/layers/quantization/utils/allspark_utils.py
1	0	vllm/model_executor/layers/quantization/utils/bitblas_utils.py
1	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	0	vllm/model_executor/layers/quantization/utils/gptq_utils.py
1	0	vllm/model_executor/layers/quantization/utils/int8_utils.py
1	0	vllm/model_executor/layers/quantization/utils/layer_utils.py
1	0	vllm/model_executor/layers/quantization/utils/machete_utils.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test_qqq.py
1	0	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	0	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py
1	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
1	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
1	0	vllm/model_executor/layers/rejection_sampler.py
1	0	vllm/model_executor/layers/resampler.py
1	0	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/layers/sampler.py
1	0	vllm/model_executor/layers/spec_decode_base_sampler.py
1	0	vllm/model_executor/layers/typical_acceptance_sampler.py
1	0	vllm/model_executor/layers/utils.py
1	0	vllm/model_executor/layers/vocab_parallel_embedding.py
1	0	vllm/model_executor/model_loader/__init__.py
1	0	vllm/model_executor/model_loader/base_loader.py
1	0	vllm/model_executor/model_loader/bitsandbytes_loader.py
1	0	vllm/model_executor/model_loader/default_loader.py
1	0	vllm/model_executor/model_loader/dummy_loader.py
1	0	vllm/model_executor/model_loader/gguf_loader.py
1	0	vllm/model_executor/model_loader/neuron.py
1	0	vllm/model_executor/model_loader/neuronx_distributed.py
1	0	vllm/model_executor/model_loader/runai_streamer_loader.py
1	0	vllm/model_executor/model_loader/sharded_state_loader.py
1	0	vllm/model_executor/model_loader/tensorizer.py
1	0	vllm/model_executor/model_loader/tensorizer_loader.py
1	0	vllm/model_executor/model_loader/utils.py
1	0	vllm/model_executor/model_loader/weight_utils.py
1	0	vllm/model_executor/models/__init__.py
1	0	vllm/model_executor/models/adapters.py
1	0	vllm/model_executor/models/aimv2.py
1	0	vllm/model_executor/models/arctic.py
1	0	vllm/model_executor/models/aria.py
2	1	vllm/model_executor/models/aya_vision.py
1	0	vllm/model_executor/models/baichuan.py
1	0	vllm/model_executor/models/bamba.py
1	0	vllm/model_executor/models/bart.py
1	0	vllm/model_executor/models/bert.py
1	0	vllm/model_executor/models/bert_with_rope.py
1	0	vllm/model_executor/models/blip.py
1	0	vllm/model_executor/models/blip2.py
1	0	vllm/model_executor/models/bloom.py
1	0	vllm/model_executor/models/chameleon.py
1	0	vllm/model_executor/models/chatglm.py
1	0	vllm/model_executor/models/clip.py
1	0	vllm/model_executor/models/commandr.py
1	0	vllm/model_executor/models/constant_size_cache.py
1	0	vllm/model_executor/models/dbrx.py
1	0	vllm/model_executor/models/deepseek.py
1	0	vllm/model_executor/models/deepseek_mtp.py
1	0	vllm/model_executor/models/deepseek_v2.py
1	0	vllm/model_executor/models/deepseek_vl2.py
1	0	vllm/model_executor/models/eagle.py
1	0	vllm/model_executor/models/exaone.py
1	0	vllm/model_executor/models/fairseq2_llama.py
1	0	vllm/model_executor/models/falcon.py
1	0	vllm/model_executor/models/falcon_h1.py
1	0	vllm/model_executor/models/florence2.py
1	0	vllm/model_executor/models/fuyu.py
1	0	vllm/model_executor/models/gemma.py
1	0	vllm/model_executor/models/gemma2.py
1	0	vllm/model_executor/models/gemma3.py
1	0	vllm/model_executor/models/gemma3_mm.py
1	0	vllm/model_executor/models/glm.py
1	0	vllm/model_executor/models/glm4.py
1	0	vllm/model_executor/models/glm4v.py
1	0	vllm/model_executor/models/gpt2.py
1	0	vllm/model_executor/models/gpt_bigcode.py
1	0	vllm/model_executor/models/gpt_j.py
1	0	vllm/model_executor/models/gpt_neox.py
1	0	vllm/model_executor/models/granite.py
1	0	vllm/model_executor/models/granite_speech.py
1	0	vllm/model_executor/models/granitemoe.py
1	0	vllm/model_executor/models/granitemoehybrid.py
1	0	vllm/model_executor/models/granitemoeshared.py
1	0	vllm/model_executor/models/gritlm.py
1	0	vllm/model_executor/models/grok1.py
1	0	vllm/model_executor/models/h2ovl.py
1	0	vllm/model_executor/models/idefics2_vision_model.py
1	0	vllm/model_executor/models/idefics3.py
1	0	vllm/model_executor/models/interfaces.py
1	0	vllm/model_executor/models/interfaces_base.py
1	0	vllm/model_executor/models/intern_vit.py
1	0	vllm/model_executor/models/internlm2.py
1	0	vllm/model_executor/models/internlm2_ve.py
1	0	vllm/model_executor/models/internvl.py
1	0	vllm/model_executor/models/jais.py
1	0	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/kimi_vl.py
1	0	vllm/model_executor/models/llama.py
1	0	vllm/model_executor/models/llama4.py
1	0	vllm/model_executor/models/llama_eagle.py
1	0	vllm/model_executor/models/llama_eagle3.py
1	0	vllm/model_executor/models/llava.py
1	0	vllm/model_executor/models/llava_next.py
1	0	vllm/model_executor/models/llava_next_video.py
1	0	vllm/model_executor/models/llava_onevision.py
1	0	vllm/model_executor/models/mamba.py
1	0	vllm/model_executor/models/mamba2.py
1	0	vllm/model_executor/models/mamba_cache.py
1	0	vllm/model_executor/models/medusa.py
1	0	vllm/model_executor/models/mimo.py
1	0	vllm/model_executor/models/mimo_mtp.py
1	0	vllm/model_executor/models/minicpm.py
1	0	vllm/model_executor/models/minicpm3.py
1	0	vllm/model_executor/models/minicpm_eagle.py
1	0	vllm/model_executor/models/minicpmo.py
1	0	vllm/model_executor/models/minicpmv.py
1	0	vllm/model_executor/models/minimax_cache.py
1	0	vllm/model_executor/models/minimax_text_01.py
1	0	vllm/model_executor/models/minimax_vl_01.py
1	0	vllm/model_executor/models/mistral3.py
1	0	vllm/model_executor/models/mixtral.py
1	0	vllm/model_executor/models/mixtral_quant.py
1	0	vllm/model_executor/models/mllama.py
1	0	vllm/model_executor/models/mllama4.py
1	0	vllm/model_executor/models/mlp_speculator.py
1	0	vllm/model_executor/models/modernbert.py
1	0	vllm/model_executor/models/module_mapping.py
1	0	vllm/model_executor/models/molmo.py
1	0	vllm/model_executor/models/moonvit.py
1	0	vllm/model_executor/models/mpt.py
1	0	vllm/model_executor/models/nemotron.py
1	0	vllm/model_executor/models/nemotron_nas.py
1	0	vllm/model_executor/models/nvlm_d.py
1	0	vllm/model_executor/models/olmo.py
1	0	vllm/model_executor/models/olmo2.py
1	0	vllm/model_executor/models/olmoe.py
1	0	vllm/model_executor/models/opt.py
1	0	vllm/model_executor/models/orion.py
1	0	vllm/model_executor/models/ovis.py
1	0	vllm/model_executor/models/paligemma.py
1	0	vllm/model_executor/models/persimmon.py
1	0	vllm/model_executor/models/phi.py
1	0	vllm/model_executor/models/phi3.py
1	0	vllm/model_executor/models/phi3_small.py
1	0	vllm/model_executor/models/phi3v.py
1	0	vllm/model_executor/models/phi4mm.py
1	0	vllm/model_executor/models/phi4mm_audio.py
1	0	vllm/model_executor/models/phi4mm_utils.py
1	0	vllm/model_executor/models/phimoe.py
1	0	vllm/model_executor/models/pixtral.py
1	0	vllm/model_executor/models/plamo2.py
1	0	vllm/model_executor/models/prithvi_geospatial_mae.py
1	0	vllm/model_executor/models/qwen.py
1	0	vllm/model_executor/models/qwen2.py
1	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	0	vllm/model_executor/models/qwen2_5_vl.py
1	0	vllm/model_executor/models/qwen2_audio.py
1	0	vllm/model_executor/models/qwen2_moe.py
1	0	vllm/model_executor/models/qwen2_rm.py
1	0	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/qwen3.py
1	0	vllm/model_executor/models/qwen3_moe.py
1	0	vllm/model_executor/models/qwen_vl.py
1	0	vllm/model_executor/models/registry.py
1	0	vllm/model_executor/models/roberta.py
1	0	vllm/model_executor/models/siglip.py
1	0	vllm/model_executor/models/skyworkr1v.py
1	0	vllm/model_executor/models/smolvlm.py
1	0	vllm/model_executor/models/solar.py
1	0	vllm/model_executor/models/stablelm.py
1	0	vllm/model_executor/models/starcoder2.py
1	0	vllm/model_executor/models/telechat2.py
1	0	vllm/model_executor/models/teleflm.py
1	0	vllm/model_executor/models/transformers.py
1	0	vllm/model_executor/models/ultravox.py
1	0	vllm/model_executor/models/utils.py
1	0	vllm/model_executor/models/vision.py
1	0	vllm/model_executor/models/whisper.py
1	0	vllm/model_executor/models/zamba2.py
1	0	vllm/model_executor/parameter.py
1	0	vllm/model_executor/pooling_metadata.py
1	0	vllm/model_executor/sampling_metadata.py
1	0	vllm/model_executor/utils.py
1	0	vllm/multimodal/__init__.py
1	0	vllm/multimodal/audio.py
1	0	vllm/multimodal/base.py
1	0	vllm/multimodal/hasher.py
1	0	vllm/multimodal/image.py
1	0	vllm/multimodal/inputs.py
1	0	vllm/multimodal/parse.py
1	0	vllm/multimodal/processing.py
1	0	vllm/multimodal/profiling.py
1	0	vllm/multimodal/registry.py
1	0	vllm/multimodal/utils.py
1	0	vllm/multimodal/video.py
1	0	vllm/outputs.py
1	0	vllm/platforms/__init__.py
1	0	vllm/platforms/cpu.py
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
1	0	vllm/platforms/interface.py
1	0	vllm/platforms/neuron.py
1	0	vllm/platforms/rocm.py
1	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py
1	0	vllm/plugins/__init__.py
1	0	vllm/plugins/lora_resolvers/filesystem_resolver.py
1	0	vllm/pooling_params.py
1	0	vllm/profiler/layerwise_profile.py
1	0	vllm/profiler/utils.py
1	0	vllm/prompt_adapter/layers.py
1	0	vllm/prompt_adapter/models.py
1	0	vllm/prompt_adapter/request.py
1	0	vllm/prompt_adapter/utils.py
1	0	vllm/prompt_adapter/worker_manager.py
1	0	vllm/reasoning/__init__.py
1	0	vllm/reasoning/abs_reasoning_parsers.py
1	0	vllm/reasoning/deepseek_r1_reasoning_parser.py
1	0	vllm/reasoning/granite_reasoning_parser.py
1	0	vllm/reasoning/qwen3_reasoning_parser.py
1	0	vllm/sampling_params.py
1	0	vllm/scalar_type.py
1	0	vllm/scripts.py
1	0	vllm/sequence.py
1	0	vllm/spec_decode/batch_expansion.py
1	0	vllm/spec_decode/draft_model_runner.py
1	0	vllm/spec_decode/interfaces.py
1	0	vllm/spec_decode/medusa_worker.py
1	0	vllm/spec_decode/metrics.py
1	0	vllm/spec_decode/mlp_speculator_worker.py
1	0	vllm/spec_decode/mqa_scorer.py
1	0	vllm/spec_decode/multi_step_worker.py
1	0	vllm/spec_decode/ngram_worker.py
1	0	vllm/spec_decode/proposer_worker_base.py
1	0	vllm/spec_decode/smaller_tp_proposer_worker.py
1	0	vllm/spec_decode/spec_decode_worker.py
1	0	vllm/spec_decode/target_model_runner.py
1	0	vllm/spec_decode/top1_proposer.py
1	0	vllm/spec_decode/util.py
1	0	vllm/test_utils.py
1	0	vllm/third_party/pynvml.py
1	0	vllm/tracing.py
1	0	vllm/transformers_utils/__init__.py
1	0	vllm/transformers_utils/chat_templates/__init__.py
1	0	vllm/transformers_utils/chat_templates/registry.py
1	0	vllm/transformers_utils/config.py
1	0	vllm/transformers_utils/configs/__init__.py
1	0	vllm/transformers_utils/configs/arctic.py
1	0	vllm/transformers_utils/configs/chatglm.py
1	0	vllm/transformers_utils/configs/cohere2.py
1	0	vllm/transformers_utils/configs/dbrx.py
1	0	vllm/transformers_utils/configs/deepseek_vl2.py
1	0	vllm/transformers_utils/configs/eagle.py
1	0	vllm/transformers_utils/configs/exaone.py
1	0	vllm/transformers_utils/configs/falcon.py
1	0	vllm/transformers_utils/configs/h2ovl.py
1	0	vllm/transformers_utils/configs/internvl.py
1	0	vllm/transformers_utils/configs/jais.py
1	0	vllm/transformers_utils/configs/kimi_vl.py
1	0	vllm/transformers_utils/configs/medusa.py
1	0	vllm/transformers_utils/configs/minimax_text_01.py
1	0	vllm/transformers_utils/configs/minimax_vl_01.py
1	0	vllm/transformers_utils/configs/mllama.py
1	0	vllm/transformers_utils/configs/mlp_speculator.py
1	0	vllm/transformers_utils/configs/moonvit.py
1	0	vllm/transformers_utils/configs/mpt.py
1	0	vllm/transformers_utils/configs/nemotron.py
1	0	vllm/transformers_utils/configs/nvlm_d.py
1	0	vllm/transformers_utils/configs/ovis.py
1	0	vllm/transformers_utils/configs/skyworkr1v.py
1	0	vllm/transformers_utils/configs/solar.py
1	0	vllm/transformers_utils/configs/telechat2.py
1	0	vllm/transformers_utils/configs/ultravox.py
1	0	vllm/transformers_utils/detokenizer.py
1	0	vllm/transformers_utils/detokenizer_utils.py
1	0	vllm/transformers_utils/processor.py
1	0	vllm/transformers_utils/processors/__init__.py
1	0	vllm/transformers_utils/processors/deepseek_vl2.py
1	0	vllm/transformers_utils/processors/ovis.py
1	0	vllm/transformers_utils/s3_utils.py
1	0	vllm/transformers_utils/tokenizer.py
1	0	vllm/transformers_utils/tokenizer_base.py
1	0	vllm/transformers_utils/tokenizer_group.py
1	0	vllm/transformers_utils/tokenizers/__init__.py
1	0	vllm/transformers_utils/tokenizers/mistral.py
1	0	vllm/transformers_utils/utils.py
1	0	vllm/triton_utils/__init__.py
1	0	vllm/triton_utils/importing.py
1	0	vllm/usage/usage_lib.py
1	0	vllm/utils.py
1	0	vllm/v1/attention/backends/flash_attn.py
1	0	vllm/v1/attention/backends/flashinfer.py
1	0	vllm/v1/attention/backends/mla/common.py
1	0	vllm/v1/attention/backends/mla/flashmla.py
1	0	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
1	0	vllm/v1/attention/backends/mla/triton_mla.py
1	0	vllm/v1/attention/backends/pallas.py
1	0	vllm/v1/attention/backends/triton_attn.py
1	0	vllm/v1/attention/backends/utils.py
1	0	vllm/v1/core/block_pool.py
1	0	vllm/v1/core/encoder_cache_manager.py
1	0	vllm/v1/core/kv_cache_manager.py
1	0	vllm/v1/core/kv_cache_utils.py
1	0	vllm/v1/core/sched/interface.py
1	0	vllm/v1/core/sched/output.py
1	0	vllm/v1/core/sched/scheduler.py
1	0	vllm/v1/core/sched/utils.py
1	0	vllm/v1/core/single_type_kv_cache_manager.py
1	0	vllm/v1/engine/__init__.py
1	0	vllm/v1/engine/async_llm.py
1	0	vllm/v1/engine/coordinator.py
1	0	vllm/v1/engine/core.py
1	0	vllm/v1/engine/core_client.py
1	0	vllm/v1/engine/detokenizer.py
1	0	vllm/v1/engine/exceptions.py
1	0	vllm/v1/engine/llm_engine.py
1	0	vllm/v1/engine/logprobs.py
1	0	vllm/v1/engine/mm_input_cache.py
1	0	vllm/v1/engine/output_processor.py
1	0	vllm/v1/engine/parallel_sampling.py
1	0	vllm/v1/engine/processor.py
1	0	vllm/v1/executor/abstract.py
1	0	vllm/v1/executor/multiproc_executor.py
1	0	vllm/v1/executor/ray_distributed_executor.py
1	0	vllm/v1/kv_cache_interface.py
1	0	vllm/v1/metrics/loggers.py
1	0	vllm/v1/metrics/prometheus.py
1	0	vllm/v1/metrics/ray_wrappers.py
1	0	vllm/v1/metrics/reader.py
1	0	vllm/v1/metrics/stats.py
1	0	vllm/v1/outputs.py
1	0	vllm/v1/request.py
1	0	vllm/v1/sample/metadata.py
1	0	vllm/v1/sample/ops/bad_words.py
1	0	vllm/v1/sample/ops/penalties.py
1	0	vllm/v1/sample/ops/topk_topp_sampler.py
1	0	vllm/v1/sample/rejection_sampler.py
1	0	vllm/v1/sample/sampler.py
1	0	vllm/v1/sample/tpu/metadata.py
1	0	vllm/v1/sample/tpu/sampler.py
1	0	vllm/v1/serial_utils.py
1	0	vllm/v1/spec_decode/eagle.py
1	0	vllm/v1/spec_decode/medusa.py
1	0	vllm/v1/spec_decode/metadata.py
1	0	vllm/v1/spec_decode/metrics.py
1	0	vllm/v1/spec_decode/ngram_proposer.py
1	0	vllm/v1/spec_decode/utils.py
1	0	vllm/v1/structured_output/__init__.py
1	0	vllm/v1/structured_output/backend_guidance.py
1	0	vllm/v1/structured_output/backend_types.py
1	0	vllm/v1/structured_output/backend_xgrammar.py
1	0	vllm/v1/structured_output/request.py
1	0	vllm/v1/structured_output/utils.py
1	0	vllm/v1/utils.py
1	0	vllm/v1/worker/block_table.py
1	0	vllm/v1/worker/gpu_input_batch.py
1	0	vllm/v1/worker/gpu_model_runner.py
1	0	vllm/v1/worker/gpu_worker.py
1	0	vllm/v1/worker/lora_model_runner_mixin.py
1	0	vllm/v1/worker/tpu_model_runner.py
1	0	vllm/v1/worker/tpu_worker.py
1	0	vllm/v1/worker/utils.py
1	0	vllm/v1/worker/worker_base.py
1	0	vllm/version.py
1	0	vllm/worker/cache_engine.py
1	0	vllm/worker/cpu_enc_dec_model_runner.py
1	0	vllm/worker/cpu_model_runner.py
1	0	vllm/worker/cpu_pooling_model_runner.py
1	0	vllm/worker/cpu_worker.py
1	0	vllm/worker/enc_dec_model_runner.py
1	0	vllm/worker/hpu_model_runner.py
1	0	vllm/worker/hpu_worker.py
1	0	vllm/worker/model_runner.py
1	0	vllm/worker/model_runner_base.py
1	0	vllm/worker/multi_step_hpu_worker.py
1	0	vllm/worker/multi_step_model_runner.py
1	0	vllm/worker/multi_step_neuron_model_runner.py
1	0	vllm/worker/multi_step_neuronx_distributed_model_runner.py
1	0	vllm/worker/multi_step_tpu_worker.py
1	0	vllm/worker/multi_step_worker.py
1	0	vllm/worker/neuron_model_runner.py
1	0	vllm/worker/neuron_worker.py
1	0	vllm/worker/neuronx_distributed_model_runner.py
1	0	vllm/worker/pooling_model_runner.py
1	0	vllm/worker/tpu_model_runner.py
1	0	vllm/worker/tpu_worker.py
1	0	vllm/worker/utils.py
1	0	vllm/worker/worker.py
1	0	vllm/worker/worker_base.py
1	0	vllm/worker/xpu_model_runner.py
1	0	vllm/worker/xpu_worker.py

[d054da199] CYJiang 2025-06-04 [Misc] fix: add miss best_of param validation (#18555)
11	0	vllm/sampling_params.py

[4b7817c11] Nicolò Lucchesi 2025-06-03 [Misc] Add missing `_Backend` enums (#19081)
2	0	vllm/platforms/interface.py

[d00dd65cd] Lu Fang 2025-06-03 [Doc] Improve the Pull Request template with key components (#19086)
11	2	.github/PULL_REQUEST_TEMPLATE.md

[d81edded6] Raushan Turganbay 2025-06-03 [Bugfix] disable processor cache  (#19068)
2	2	vllm/v1/engine/mm_input_cache.py

[476844d44] Harry Mellor 2025-06-03 Fix underscores in dict keys passed via CLI (#19030)
11	0	tests/test_utils.py
10	3	vllm/utils.py

[4e68ae5e5] Jee Jee Li 2025-06-03 [CI/Build] Remove V0 LoRA test (#19066)
2	19	tests/lora/test_add_lora.py
0	10	tests/lora/test_chatglm3_tp.py
0	8	tests/lora/test_llama_tp.py
8	26	tests/lora/test_lora_functions.py
0	8	tests/lora/test_mixtral.py
0	8	tests/lora/test_quant_model.py
0	8	tests/lora/test_qwen2vl.py
0	10	tests/lora/test_worker.py

[4e88723f3] youkaichao 2025-06-03 [doc] clarify windows support (#19088)
3	0	docs/getting_started/installation/gpu.md

[118ff9211] Cyrus Leung 2025-06-03 [Doc] Update V1 user guide for embedding and enc-dec models (#19060)
10	5	docs/usage/v1_guide.md

[ec2dcd80b] Isotr0py 2025-06-03 [Misc] Update `WeightsMapper` for qwen2-vl/qwen2.5-vl (#19054)
9	4	vllm/model_executor/models/qwen2_5_vl.py
9	4	vllm/model_executor/models/qwen2_vl.py

[42243fbda] Jee Jee Li 2025-06-03 [Doc] Add InternVL LoRA support  (#19055)
1	1	docs/models/supported_models.md

[6d18ed2a2] Michael Goin 2025-06-03 Update docker docs with ARM CUDA cross-compile (#19037)
12	1	docs/deployment/docker.md

[f32fcd944] Chen Zhang 2025-06-03 [v1][KVCacheManager] Rename BlockHashType to BlockHash (#19015)
1	1	docs/design/v1/prefix_caching.md
5	7	tests/v1/core/test_kv_cache_utils.py
2	2	tests/v1/core/test_prefix_caching.py
2	2	tests/v1/core/test_specialized_manager.py
4	4	vllm/v1/core/block_pool.py
2	2	vllm/v1/core/kv_cache_manager.py
7	7	vllm/v1/core/kv_cache_utils.py
5	5	vllm/v1/core/single_type_kv_cache_manager.py

[d32aa2e67] Lu Fang 2025-06-03 [Bugfix] Use cmake 3.26.1 instead of 3.26 to avoid build failure (#19019)
1	1	docker/Dockerfile.neuron
1	1	docs/getting_started/installation/cpu/build.inc.md
1	1	pyproject.toml
1	1	requirements/build.txt
1	1	requirements/rocm-build.txt
1	1	requirements/tpu.txt
1	1	requirements/xpu.txt

[cc977286e] Michael Goin 2025-06-03 Reduce logs in CLI scripts and plugin loader (#18970)
3	3	vllm/benchmarks/datasets.py
0	2	vllm/benchmarks/latency.py
0	1	vllm/benchmarks/throughput.py
3	3	vllm/compilation/backends.py
13	6	vllm/plugins/__init__.py

[17430e365] Reid 2025-06-03 [bugfix] small fix logic issue (#18999)
1	1	vllm/engine/arg_utils.py

[1282bd812] 汪志鹏 2025-06-03 Add tarsier model support (#18985)
1	0	docs/models/supported_models.md
20	0	examples/offline_inference/vision_language.py
21	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py
643	0	vllm/model_executor/models/tarsier.py

[bdce64f23] Rui Qiao 2025-06-02 [V1] Support DP with Ray (#18779)
1	1	requirements/test.in
50	0	requirements/test.txt
10	3	tests/v1/test_async_llm_dp.py
6	0	vllm/config.py
25	4	vllm/engine/arg_utils.py
30	5	vllm/entrypoints/cli/serve.py
9	4	vllm/v1/engine/async_llm.py
123	45	vllm/v1/engine/core.py
65	9	vllm/v1/engine/core_client.py
220	37	vllm/v1/utils.py

[9e6f61e8c] Gregory Shtrasberg 2025-06-02 [ROCm][Build] Clean up the ROCm build (#19040)
0	4	CMakeLists.txt
0	17	docker/Dockerfile.rocm
0	2	docs/getting_started/installation/gpu/rocm.inc.md
2	0	requirements/rocm.txt

[8655f47f3] Li, Jiang 2025-06-03 [CPU][CI] Re-enable the CPU CI tests (#19046)
20	22	.buildkite/scripts/hardware_ci/run-cpu-test.sh
7	3	docker/Dockerfile.cpu
2	1	vllm/distributed/parallel_state.py

[4ce42f920] Concurrensee 2025-06-02 Adding "LoRA Test %N" to AMD production tests (#18929)
4	0	.buildkite/scripts/hardware_ci/run-amd-test.sh
1	1	.buildkite/test-pipeline.yaml

[8a57872b2] Tyler Michael Smith 2025-06-02 [Bugfix][EP+DP] Use pplx-kernel internode instead of intranode (#19034)
4	0	vllm/distributed/device_communicators/all2all.py
5	1	vllm/model_executor/layers/fused_moe/layer.py

[5bc1ad6ce] Hyogeun Oh (오효근) 2025-06-03 [Doc] Remove duplicate TOCs during MkDocs migration (#19021)
0	13	docs/cli/README.md
0	10	docs/deployment/nginx.md

[9112b443a] Siyuan Liu 2025-06-02 [Hardware][TPU] Initial support of model parallelism with single worker using SPMD (#18011)
4	0	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
23	6	examples/offline_inference/tpu.py
67	0	tests/v1/tpu/test_spmd_model_weight_loading.py
89	0	tests/v1/tpu/test_tpu_qkv_linear.py
2	0	vllm/config.py
177	0	vllm/distributed/tpu_distributed_utils.py
5	0	vllm/envs.py
112	0	vllm/model_executor/model_loader/tpu.py
3	1	vllm/model_executor/utils.py
69	32	vllm/v1/worker/tpu_model_runner.py
54	33	vllm/v1/worker/tpu_worker.py

[c57d577e8] Calvin Chen 2025-06-03 add an absolute path for run.sh (#18258)
9	3	examples/offline_inference/disaggregated-prefill-v1/run.sh

[ca2f6b9c3] Gregory Shtrasberg 2025-06-02 [Bugfix][Model] Attempt to fix eagle in V0. (#18978)
2	1	vllm/config.py

[20133cfee] Frαnçois 2025-06-02 [Frontend] enable custom logging for the uvicorn server (OpenAI API server) (#18403)
19	2	vllm/entrypoints/openai/api_server.py
8	0	vllm/entrypoints/openai/cli_args.py

[ebb1ec931] jennyyyyzhen 2025-06-02 [Model] enable data parallel for Llama4 vision encoder (#18368)
4	0	vllm/config.py
8	0	vllm/engine/arg_utils.py
167	68	vllm/model_executor/models/mllama4.py
35	0	vllm/multimodal/utils.py

[5b168b6d7] Reid 2025-06-02 [doc] add pytest tips (#19010)
3	0	docs/contributing/README.md

[9760fd8f6] 22quinn 2025-06-02 [Core] Support inplace model weights loading (#18745)
0	17	tests/tensorizer_loader/test_tensorizer.py
18	0	tests/v1/worker/test_gpu_model_runner.py
20	2	vllm/model_executor/model_loader/base_loader.py
2	16	vllm/model_executor/model_loader/bitsandbytes_loader.py
18	32	vllm/model_executor/model_loader/default_loader.py
6	17	vllm/model_executor/model_loader/dummy_loader.py
8	2	vllm/model_executor/model_loader/gguf_loader.py
9	21	vllm/model_executor/model_loader/runai_streamer_loader.py
44	55	vllm/model_executor/model_loader/sharded_state_loader.py
66	99	vllm/model_executor/model_loader/tensorizer.py
29	29	vllm/model_executor/model_loader/tensorizer_loader.py
13	2	vllm/v1/worker/gpu_model_runner.py
16	5	vllm/v1/worker/tpu_model_runner.py

[b9f61e138] Robert Shaw 2025-06-01 [Bugfix][Nixl] Fix DP Metadata Handshake (#19008)
36	32	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[d6fd3a33b] zhrrr 2025-06-02 [Misc] reuse num_tokens_across_dp of get_dp_padding to avoid unnecessary dp all reduce in set_forward_context (#18935)
19	8	vllm/forward_context.py
28	10	vllm/v1/worker/gpu_model_runner.py

[432ec9926] Reid 2025-06-01 [doc] wrong output (#19000)
1	1	docs/contributing/README.md

[2b102d51a] Nick Hill 2025-05-31 [BugFix] Fix incorrect metrics shutdown error log message (#18992)
5	1	vllm/v1/metrics/prometheus.py

[aa54a7bf7] rongfu.leng 2025-06-01 [BugFix] fix data parallel construct ipv6 url addres (#18991)
3	3	vllm/distributed/parallel_state.py

[2ad6194a0] Michael Goin 2025-05-31 Let max_num_batched_tokens use human_readable_int for large numbers (#18968)
1	1	vllm/engine/arg_utils.py

[c594cbf56] Reid 2025-06-01 [doc] small fix -  mkdocs (#18996)
3	2	docs/contributing/README.md

[a35ca765a] Isotr0py 2025-06-01 [LoRA] Support dynamically initialize `packed_modules_mapping` for VLM with arbitrary components (#18987)
3	3	vllm/lora/models.py
4	4	vllm/model_executor/model_loader/bitsandbytes_loader.py
4	0	vllm/model_executor/models/intern_vit.py
0	9	vllm/model_executor/models/internvl.py
0	11	vllm/model_executor/models/qwen2_5_vl.py
0	11	vllm/model_executor/models/qwen2_vl.py
21	0	vllm/model_executor/utils.py

[6aa8f9a4e] Cyrus Leung 2025-06-01 [Core] Rework dtype resolution (#18751)
1	4	tests/basic_correctness/test_basic_correctness.py
6	1	tests/conftest.py
4	7	tests/models/language/pooling/mteb_utils.py
1	1	tests/models/language/pooling/test_classification.py
1	5	tests/models/language/pooling/test_embedding.py
1	0	tests/models/multimodal/generation/test_whisper.py
1	1	tests/models/multimodal/processing/test_common.py
1	1	tests/samplers/test_no_bad_words.py
80	22	tests/test_utils.py
135	67	vllm/config.py
1	1	vllm/platforms/cpu.py
33	7	vllm/transformers_utils/config.py
49	2	vllm/utils.py

[1bc86a3da] Benjamin Chislett 2025-05-31 [Bugfix] Fix EAGLE3 broken logits (#18909)
12	11	vllm/model_executor/models/llama_eagle3.py

[bbfa0c61d] Ekagra Ranjan 2025-05-31 [Misc][Benchmark] Add support for CustomDataset (#18511)
48	0	benchmarks/README.md
91	3	benchmarks/benchmark_dataset.py
28	2	benchmarks/benchmark_serving.py
94	3	vllm/benchmarks/datasets.py
3	0	vllm/benchmarks/serve.py

[20079c6e3] Reid 2025-06-01 [Misc] add return token strs for tokenize (#18941)
50	15	tests/entrypoints/openai/test_tokenization.py
11	0	vllm/entrypoints/openai/protocol.py
5	0	vllm/entrypoints/openai/serving_tokenization.py

[9a1b9b99d] Nick Hill 2025-05-31 [BugFix] Fix multi-node offline data-parallel (#18981)
8	4	examples/offline_inference/data_parallel.py
3	4	vllm/v1/engine/core_client.py

[8bf507d76] ptarasiewiczNV 2025-05-31 [P/D] NixlConnector use cache device index for memory registration (#18969)
2	1	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[306d60401] Charlie Fu 2025-05-31 [ROCm][Kernel] Add gfx950 support for skinny gemms (#18010)
70	43	csrc/rocm/skinny_gemms.cu
9	5	tests/kernels/quant_utils.py
2	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
2	2	vllm/model_executor/layers/utils.py
8	2	vllm/platforms/rocm.py

[f2c3f66d5] Fred Reiss 2025-05-31 [Bugfix] Fix for issue 17396 (#18773)
5	2	vllm/lora/ops/torch_ops/lora_ops.py

[0f5e0d567] vllmellm 2025-05-31 [FEAT][ROCm] Add AITER grouped topk for DeepSeekV2 (#18825)
93	0	tests/kernels/moe/test_rocm_aiter_topk.py
1	1	vllm/model_executor/layers/fused_moe/layer.py
63	15	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[c55d80467] Luka Govedič 2025-05-31 [BugFix] Pydantic part 2 (#18911)
1	0	requirements/test.in
6	2	requirements/test.txt

[749f5bdd3] Reid 2025-05-31 [doc] fix the list rendering issue - security.md (#18982)
6	6	docs/usage/security.md

[2a50ef576] Satyajith Chilappagari 2025-05-31 [Neuron] Add Multi-Modal model support for Neuron (#18921)
105	0	examples/offline_inference/neuron_multimodal.py
10	0	vllm/config.py
61	4	vllm/model_executor/model_loader/neuronx_distributed.py
13	0	vllm/worker/neuron_model_runner.py
46	42	vllm/worker/neuronx_distributed_model_runner.py

[b8b904795] Lucia Fang 2025-05-31 fix security issue of logging llm output (#18980)
2	2	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py

[ba5111f23] Chauncey 2025-05-31 [Bugfix]: Fix the incompatibility issue with Structured Outputs when Thinking is disabled (#18879)
18	12	vllm/v1/structured_output/__init__.py
1	1	vllm/v1/structured_output/request.py

[1e123529d] Yong Hoon Shin 2025-05-31 [Misc] Fix estimated max model len msg (#18966)
5	4	vllm/v1/core/kv_cache_utils.py

[dff80b0e4] Pooya Davoodi 2025-05-31 [Frontend] Add rerank support to run_batch endpoint (#16278)
11	3	tests/entrypoints/openai/test_run_batch.py
11	6	vllm/entrypoints/openai/protocol.py
26	7	vllm/entrypoints/openai/run_batch.py

[7782464a1] Yu Guo 2025-05-30 create util function for batched arange (#18937)
35	29	vllm/v1/worker/gpu_model_runner.py

[0f71e2403] Lukas Geiger 2025-05-31 [Docs] Correct multiprocessing design doc (#18964)
4	4	docs/design/multiprocessing.md

[1dab4d571] Will Eaton 2025-05-30 Tool parser regex timeout handling (#18960)
25	1	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
25	1	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
14	1	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
13	2	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
5	0	vllm/envs.py

[7f21e8052] rongfu.leng 2025-05-31 [Misc] add group_size is -1 in awq quantization (#18910)
11	3	vllm/model_executor/layers/quantization/awq.py

[5a8641638] Isotr0py 2025-05-31 [VLM] Add PP support and fix GPTQ inference for Ovis models (#18958)
1	1	docs/models/supported_models.md
1	0	tests/distributed/test_pipeline_parallel.py
120	74	vllm/model_executor/models/aimv2.py
0	5	vllm/model_executor/models/clip.py
23	11	vllm/model_executor/models/ovis.py

[f49239cb4] Michael Goin 2025-05-30 Benchmark script for fp8 vs bf16 gemm (#17126)
222	0	benchmarks/kernels/bench_fp8_gemm.py
46	0	benchmarks/kernels/weight_shapes.py

[2dbe8c077] Nick Hill 2025-05-30 [Perf] API-server scaleout with many-to-many server-engine comms  (#17546)
2	0	.buildkite/test-pipeline.yaml
268	0	tests/entrypoints/test_api_server_process_manager.py
3	2	tests/utils.py
0	1	tests/v1/core/test_kv_cache_utils.py
0	1	tests/v1/core/test_prefix_caching.py
5	4	tests/v1/core/test_scheduler.py
7	7	tests/v1/engine/test_engine_core.py
171	0	tests/v1/entrypoints/openai/test_multi_api_servers.py
2	2	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
3	3	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
0	1	tests/v1/kv_connector/unit/utils.py
170	9	vllm/entrypoints/cli/serve.py
63	39	vllm/entrypoints/openai/api_server.py
5	0	vllm/lora/worker_manager.py
5	1	vllm/utils.py
8	2	vllm/v1/core/sched/interface.py
39	14	vllm/v1/core/sched/scheduler.py
4	4	vllm/v1/engine/__init__.py
12	2	vllm/v1/engine/async_llm.py
252	0	vllm/v1/engine/coordinator.py
166	87	vllm/v1/engine/core.py
244	212	vllm/v1/engine/core_client.py
45	34	vllm/v1/metrics/loggers.py
77	0	vllm/v1/metrics/prometheus.py
3	2	vllm/v1/request.py
283	18	vllm/v1/utils.py

[84ec470fc] Richard Zou 2025-05-30 Improve "failed to get the hash of the compiled graph" error (#18956)
8	2	vllm/compilation/compiler_interface.py

[b29ca5c4d] Russell Bryant 2025-05-30 [Docs] Update SECURITY.md with link to our security guide (#18961)
2	0	SECURITY.md

[ec6833c5e] Reid 2025-05-30 [doc] show the count for fork and watch (#18950)
2	2	docs/README.md

[e1fadf119] Shawn Huang 2025-05-30 [Feature] minicpm eagle support (#18943)
5	0	tests/models/registry.py
3	2	vllm/model_executor/models/minicpm.py
390	0	vllm/model_executor/models/minicpm_eagle.py
1	0	vllm/model_executor/models/registry.py

[43ff405b9] Daniele 2025-05-30 [CI/Build] remove regex from build dependencies (#18945)
0	1	pyproject.toml
1	1	setup.py
3	0	tools/enforce_regex_import.py

[fba02e3bd] Carol Zheng 2025-05-30 [Bugfix][TPU] Fix tpu model runner testcase failure (#18810)
26	5	tests/v1/tpu/worker/test_tpu_model_runner.py
24	11	vllm/v1/worker/tpu_model_runner.py

[4577fc9ab] Always-Naive 2025-05-30 [Misc]Fix typo (#18947)
1	1	vllm/utils.py

[5f1d0c811] Rabi Mishra 2025-05-30 [Bugfix][Failing Test] Fix test_vllm_port.py (#18618)
2	1	.buildkite/test-pipeline.yaml
7	11	vllm/envs.py

[c3bb9f233] Lukas Geiger 2025-05-30 [Model] Use in-place adds in SigLIP (#18922)
6	6	vllm/model_executor/models/siglip.py

[8f8900cee] Reid 2025-05-30 [doc] add mkdocs doc (#18930)
50	3	docs/contributing/README.md

[6acb7a628] Rabi Mishra 2025-05-30 [Misc]Fix benchmarks/README.md for speculative decoding (#18897)
6	6	benchmarks/README.md

[4f4a6b844] Cyrus Leung 2025-05-30 [Deprecation] Remove mean pooling default for `Qwen2EmbeddingModel` (#18913)
1	1	docs/models/supported_models.md
3	74	vllm/model_executor/models/qwen2.py
1	1	vllm/model_executor/models/registry.py

[4d0a1541b] Michael Goin 2025-05-30 [Bugfix] Remove NVFP4 scales assertions to fix load_format=dummy (#18861)
5	3	vllm/model_executor/layers/quantization/modelopt.py
6	1	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py

[77b6e74fe] vllmellm 2025-05-30 [ROCm] Remove unnecessary assertion of max_model_len in ROCM_AITER_MLA attention backend. (#18938)
0	2	vllm/attention/backends/rocm_aiter_mla.py
0	3	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[5acf828d9] H 2025-05-29 [docs] fix: fix markdown syntax (#18927)
1	2	docs/design/arch_overview.md

[3987e2ae9] iLeGend 2025-05-30 [Model] Use AutoWeightsLoader for mamba2 (#18918)
24	19	vllm/model_executor/models/mamba2.py

[77164dad5] Chauncey 2025-05-30 [Bugfix] Consistent ascii handling in tool parsers (#18883)
2	1	vllm/entrypoints/openai/serving_chat.py

[3de3eadf5] Wenhua Cheng 2025-05-30 improve the robustness of parsing vlms config in AutoRound (#18894)
3	2	vllm/model_executor/layers/quantization/auto_round.py

[3132290a1] Carol Zheng 2025-05-30 [TPU][CI/CD] Clean up docker for TPU tests. (#18926)
35	4	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[1aa2f81b4] Cyrus Leung 2025-05-30 [Misc] Update type annotation for rotary embedding `base` (#18914)
1	1	benchmarks/kernels/benchmark_rope.py
3	3	tests/kernels/core/test_pos_encoding.py
17	17	vllm/model_executor/layers/rotary_embedding.py
2	5	vllm/model_executor/models/minimax_text_01.py

[d54af615d] Michael Goin 2025-05-29 [Bugfix] Fix PP default fallback behavior for V1 (#18915)
2	1	vllm/engine/arg_utils.py

[a1cc9f33a] Chengji Yao 2025-05-29 [TPU] remove transpose ops in moe kernel (#18923)
5	5	requirements/tpu.txt
1	1	tests/tpu/test_moe_pallas.py
2	7	vllm/model_executor/layers/fused_moe/moe_pallas.py

[a521ef06e] Richard Zou 2025-05-29 Use standalone_compile by default in torch >= 2.8.0 (#18846)
3	2	vllm/compilation/backends.py
1	1	vllm/compilation/compiler_interface.py
8	6	vllm/envs.py

[64eaf5fe0] Will Eaton 2025-05-29 [P/D] NixlConnector DP fixes (#18903)
2	1	vllm/distributed/kv_transfer/kv_connector/factory.py
7	5	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
9	0	vllm/v1/engine/core.py

[d1d61f335] Nick Hill 2025-05-29 [BugFix] Make DP work with connector-delayed new requests (#18559)
9	9	tests/v1/engine/test_engine_core.py
4	3	vllm/forward_context.py
22	28	vllm/v1/engine/core.py
2	1	vllm/v1/engine/core_client.py

[32ce3cf7c] Nicolò Lucchesi 2025-05-29 [V1] Allocate kv_cache with stride order for V1 (#18775)
58	13	tests/v1/worker/test_gpu_model_runner.py
23	3	vllm/v1/worker/gpu_model_runner.py

[d58f9c7f7] CYJiang 2025-05-30 [Misc] Remove duplicate init for self.vllm_config (#18896)
0	1	vllm/v1/engine/core.py

[c29034037] Cyrus Leung 2025-05-30 [Deprecation] Disallow pos-args other than `model` when initializing `LLM` (#18802)
0	24	tests/entrypoints/llm/test_init.py
3	17	vllm/entrypoints/llm.py

[1b7cfd5a3] Gregory Shtrasberg 2025-05-29 [ROCm][V0][Attention] Revert to the previous FA triton kernel (#18226)
3	2	vllm/attention/backends/rocm_flash_attn.py
685	1081	vllm/attention/ops/triton_flash_attention.py
6	0	vllm/platforms/rocm.py

[da4b69d0b] Gregory Shtrasberg 2025-05-29 [Attention][V1] Toggle for v1 attention backend (#18275)
2	2	vllm/attention/ops/chunked_prefill_paged_decode.py
2	2	vllm/attention/ops/prefix_prefill.py
10	2	vllm/envs.py
6	3	vllm/v1/attention/backends/triton_attn.py

[c9479b292] Isotr0py 2025-05-29 [Bugfix] Fix the failing gte embedding test (#18720)
7	6	tests/conftest.py
11	7	tests/models/language/pooling/test_embedding.py
1	0	tests/models/language/pooling/test_gte.py
1	0	tests/models/utils.py

[6f2909405] Hyogeun Oh (오효근) 2025-05-29 [Doc]  Fix codeblocks formatting in LoRA adapters documentation (#18907)
3	2	docs/features/lora.md

[b169d5f7b] Duyi-Wang 2025-05-29 [Misc][Tools][Benchmark] Add benchmark_serving supports for llama.cpp.  (#18692)
2	1	benchmarks/backend_request_func.py
4	0	benchmarks/benchmark_serving.py

[f8977c233] Chenyaaang 2025-05-29 Fix an error in dummy weight loading for quantization models (#18855)
1	1	vllm/model_executor/model_loader/weight_utils.py

[f274581f4] Luka Govedič 2025-05-29 [BugFix] Update pydantic to fix error on python 3.10 (#18852)
1	1	requirements/common.txt

[0b1447f89] Lukas Geiger 2025-05-29 [Bugfix] Ensure tensors are contiguous during serialisation (#18860)
2	2	vllm/v1/serial_utils.py

[24d0ef897] Nicolò Lucchesi 2025-05-29 [Misc] Replace TODO in serving transcription (#18895)
3	1	vllm/entrypoints/openai/serving_transcription.py

[7fcfd954f] Jee Jee Li 2025-05-29 [Bugfix] Fix misleading information in the documentation (#18845)
57	57	docs/models/supported_models.md

[e740d07f0] Reid 2025-05-29 [doc] add CLI doc (#18871)
3	0	docs/.nav.yml
179	0	docs/cli/README.md

[a652e71dd] Michael Yao 2025-05-29 [Doc] Remove redundant spaces from compatibility_matrix.md (#18891)
18	17	docs/features/compatibility_matrix.md

[34d6c447c] Jee Jee Li 2025-05-29 [LoRA] Add LoRA support for InternVL  (#18842)
23	2	vllm/model_executor/models/internvl.py

[972eddf7c] Satyajith Chilappagari 2025-05-29 [Neuron] Add multi-LoRA support for Neuron. (#18284)
98	0	tests/neuron/2_core/test_multi_lora.py
18	13	vllm/model_executor/model_loader/neuronx_distributed.py
0	3	vllm/platforms/neuron.py
31	1	vllm/worker/neuron_model_runner.py
38	4	vllm/worker/neuron_worker.py
158	5	vllm/worker/neuronx_distributed_model_runner.py

[fd7bb88d7] Brent Salisbury 2025-05-29 Fixes a dead link in nightly benchmark readme (#18856)
1	1	.buildkite/nightly-benchmarks/README.md

[3c49dbdd0] Yikun Jiang 2025-05-29 Skip device and quant Pydantic validation to make plugin device work (#18843)
2	2	vllm/config.py

[1661a9c28] aws-elaineyz 2025-05-28 [Doc][Neuron] Update documentation for Neuron (#18868)
3	0	docs/features/compatibility_matrix.md
3	3	docs/features/quantization/supported_hardware.md
93	91	docs/getting_started/installation/ai_accelerator/neuron.inc.md
1	1	vllm/config.py

[8e882ffdc] Chengji Yao 2025-05-28 [Bugfix][TPU] fix moe custom kernel import (#18853)
1	0	vllm/model_executor/layers/fused_moe/moe_pallas.py

[26b4fa45b] Richard Zou 2025-05-28 Add ability to use CUDAGraphs with use_inductor=False (#17345)
10	1	tests/compile/piecewise/test_simple.py
33	7	tests/compile/piecewise/test_toy_llama.py
4	0	vllm/compilation/compiler_interface.py
4	0	vllm/compilation/counter.py
0	5	vllm/config.py

[515b413eb] Maximilien de Bayser 2025-05-28 Prevent the cross-encoder logic from being applied to classification tasks (#18838)
20	14	vllm/model_executor/layers/pooler.py
3	3	vllm/model_executor/models/bert.py
4	3	vllm/model_executor/models/modernbert.py
4	2	vllm/model_executor/models/roberta.py
9	2	vllm/transformers_utils/config.py

[269d90173] Hongxia Yang 2025-05-28 [Bugfix][ROCm] fix the power of 2 exception from triton_unified_attention.py when running llama4 models and unit test fix (#18100)
3	1	tests/kernels/attention/test_triton_unified_attention.py
51	55	vllm/attention/ops/triton_unified_attention.py

[7951d7873] Varun Sundar Rabindranath 2025-05-28 [Core] Enable CUDA graphs for DP + All2All kernels  (#18724)
41	22	vllm/forward_context.py
39	3	vllm/model_executor/layers/fused_moe/layer.py
0	11	vllm/platforms/cuda.py
20	1	vllm/v1/worker/gpu_model_runner.py

[6dbe5b5c9] Harry Mellor 2025-05-28 Remove checks for `None` for fields which should never be `None` (#17985)
10	14	vllm/config.py

[643622ba4] Akshat Tripathi 2025-05-28 [Hardware][TPU][V1] Multi-LoRA Optimisations for the V1 TPU backend (#15655)
2	4	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
0	73	tests/tpu/lora/test_pallas_kernels.py
2	2	vllm/lora/models.py
89	51	vllm/lora/ops/xla_ops/lora_ops.py
0	133	vllm/lora/ops/xla_ops/pallas.py
118	39	vllm/lora/punica_wrapper/punica_tpu.py
42	15	vllm/v1/worker/lora_model_runner_mixin.py
70	12	vllm/v1/worker/tpu_model_runner.py
2	5	vllm/v1/worker/tpu_worker.py

[a09c7ca9f] Aaron Pham 2025-05-28 [Chore][Spec Decode] Update check NoneType instead of assigning variables (#18836)
23	27	vllm/v1/worker/gpu_model_runner.py

[0e98964e9] Mark McLoughlin 2025-05-28 [V1][Metrics] Remove metrics that were deprecated in 0.8 (#18837)
0	13	docs/usage/metrics.md
0	30	examples/online_serving/prometheus_grafana/grafana.json
1	6	tests/entrypoints/openai/test_metrics.py
0	15	vllm/engine/llm_engine.py
0	89	vllm/engine/metrics.py
0	3	vllm/engine/metrics_types.py

[c68b5c63e] rongfu.leng 2025-05-29 [Misc] fix olmoe model layer can't laod in tp gt 1 (#18828)
25	4	vllm/model_executor/models/olmoe.py

[fced75692] Aaron Pham 2025-05-28 [Chore] update ty configuration (#18839)
2	1	pyproject.toml

[321331b8a] Alex Brooks 2025-05-28 [Core] Add Lora Support to Beam Search (#18346)
34	0	tests/entrypoints/openai/test_lora_adapters.py
60	2	tests/lora/test_qwen2vl.py
4	0	vllm/beam_search.py
14	8	vllm/engine/protocol.py
36	6	vllm/entrypoints/llm.py
1	0	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/entrypoints/openai/serving_completion.py

[6e4cea1cc] daniel-salib 2025-05-28 decrement server_load on listen for disconnect (#18784)
5	0	vllm/entrypoints/utils.py

[435fa9544] Reid 2025-05-28 [Frontend] add run batch to CLI (#18804)
35	2	examples/offline_inference/openai_batch/README.md
10	16	tests/entrypoints/openai/test_run_batch.py
2	0	vllm/entrypoints/cli/main.py
55	0	vllm/entrypoints/cli/run_batch.py
8	4	vllm/entrypoints/openai/run_batch.py

[4c2b38ce9] Harry Mellor 2025-05-28 Enable Pydantic mypy checks and convert configs to Pydantic dataclasses (#17599)
1	1	.pre-commit-config.yaml
1	0	pyproject.toml
7	7	tests/lora/test_quant_model.py
1	1	tests/tracing/test_tracing.py
60	48	vllm/config.py
23	20	vllm/engine/arg_utils.py
4	1	vllm/entrypoints/llm.py
8	4	vllm/entrypoints/openai/protocol.py
8	8	vllm/entrypoints/openai/serving_engine.py
2	4	vllm/model_executor/guided_decoding/guided_fields.py
0	8	vllm/utils.py

[d781930f9] Mengqing Cao 2025-05-28 [Platform][Dist] Make torch distributed process group extendable (#18763)
52	37	vllm/distributed/utils.py
33	0	vllm/platforms/cuda.py
16	0	vllm/platforms/interface.py
33	0	vllm/platforms/rocm.py

[ce75efeec] Lucas Wilkinson 2025-05-28 [BugFix] FA2 MLA Accuracy Issue (#18807)
8	0	csrc/attention/merge_attn_states.cu
4	4	vllm/attention/backends/mla/common.py
4	4	vllm/v1/attention/backends/mla/common.py

[aa42561e4] Richard Zou 2025-05-28 Fix PiecewiseCompileInterpreter (#17338)
2	1	vllm/compilation/backends.py

[de65fc8e1] wang.yuqi 2025-05-28 [CI] improve embed testing (#18747)
2	1	tests/entrypoints/openai/correctness/test_mteb.py
2	1	tests/entrypoints/openai/test_embedding.py
3	1	tests/entrypoints/openai/test_embedding_dimensions.py
72	0	tests/models/language/pooling/embed_utils.py
8	3	tests/models/language/pooling/mteb_utils.py
71	0	tests/models/language/pooling/test_baai.py
9	24	tests/models/language/pooling/test_gte.py
42	51	tests/models/language/pooling/test_jina.py
12	24	tests/models/language/pooling/test_nomic.py
9	31	tests/models/language/pooling/test_snowflake_arctic_embed.py
1	1	tests/models/registry.py
1	23	tests/models/utils.py
12	14	vllm/config.py

[0c492b782] Cyrus Leung 2025-05-28 [Deprecation] Remove fallbacks for Embeddings API (#18795)
6	11	vllm/config.py
5	34	vllm/entrypoints/openai/api_server.py
1	15	vllm/outputs.py

[0f0926b43] Cyrus Leung 2025-05-28 [Deprecation] Remove unused sync methods in `async_timeout` (#18792)
0	19	vllm/engine/async_timeout.py

[7f2c1a87e] Cyrus Leung 2025-05-28 [Deprecation] Require overriding `get_dummy_text` and `get_dummy_mm_data` (#18796)
4	20	vllm/multimodal/profiling.py

[b78f844a6] Rabi Mishra 2025-05-28 [Bugfix][FailingTest]Fix test_model_load_with_params.py (#18758)
11	11	.buildkite/test-pipeline.yaml
0	0	tests/{ => model_executor}/test_logits_processor.py
8	11	tests/model_executor/test_model_load_with_params.py
0	0	tests/model_executor/{weight_utils.py => test_weight_utils.py}

[5e13c07d0] RonaldBXu 2025-05-27 [V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (2) (#18781)
1	1	vllm/transformers_utils/configs/eagle.py
5	1	vllm/v1/spec_decode/eagle.py

[774c5fde3] Divakar Verma 2025-05-27 [V1] fix torch profiling for V1 offline scenarios (#18445)
10	27	benchmarks/benchmark_latency.py
9	24	vllm/benchmarks/latency.py
2	0	vllm/v1/worker/gpu_worker.py
2	0	vllm/worker/worker.py

[9a21e331f] Guillaume Calmettes 2025-05-28 [Bugfix]: correctly propagate errors message caught at the chat_templating step to the client (#18769)
3	3	vllm/entrypoints/chat_utils.py

[3e9ce609b] wang.yuqi 2025-05-28 [Bugfix] Fix nomic max_model_len (#18755)
46	0	examples/offline_inference/context_extension.py
130	0	tests/models/language/pooling/test_nomic_max_model_len.py
14	0	vllm/config.py
52	3	vllm/model_executor/models/bert_with_rope.py

[794ae1f55] fxmarty-amd 2025-05-28 [rocm] Fix wrong attention log (#18764)
3	2	vllm/platforms/rocm.py

[d73a9457a] Lukas Geiger 2025-05-28 [Core] Improve Tensor serialisation (#18774)
9	8	vllm/v1/serial_utils.py

[a3896c7f0] Luka Govedič 2025-05-27 [Build] Fixes for CMake install (#18570)
5	0	CMakeLists.txt
18	2	cmake/external_projects/vllm_flash_attn.cmake
1	1	cmake/utils.cmake
1	4	setup.py

[51e98e4ff] cascade 2025-05-27 [Bugfix] Disable prefix caching by default for benchmark (#18771)
1	1	vllm/benchmarks/latency.py

[e56f44d9e] Michael Goin 2025-05-27 Support datasets in `vllm bench serve` and sync with benchmark_[serving,datasets].py (#18566)
175	10	vllm/benchmarks/datasets.py
223	3	vllm/benchmarks/endpoint_request_func.py
294	88	vllm/benchmarks/serve.py

[e0cbad4e3] Satyajith Chilappagari 2025-05-27 [Neuron] Support quantization on neuron (#18283)
11	0	tests/neuron/1_core/test_neuron_quant.py
8	1	vllm/model_executor/layers/quantization/neuron_quant.py
1	1	vllm/platforms/neuron.py

[b48d5cca1] Carol Zheng 2025-05-27 [CI/Build] [TPU] Fix TPU CI exit code (#18282)
133	84	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[587387724] Michael Goin 2025-05-27 [Bugfix] Mistral tool calling when content is list (#18729)
110	6	tests/tokenization/test_mistral_tokenizer.py
5	1	vllm/transformers_utils/tokenizers/mistral.py

[696259ca0] Cyrus Leung 2025-05-27 [Core] Automatically cast multi-modal input dtype (#18756)
1	3	vllm/model_executor/models/deepseek_vl2.py
0	5	vllm/model_executor/models/gemma3_mm.py
7	1	vllm/multimodal/inputs.py
5	2	vllm/spec_decode/draft_model_runner.py
9	3	vllm/v1/worker/gpu_model_runner.py
10	4	vllm/v1/worker/tpu_model_runner.py
5	2	vllm/worker/cpu_enc_dec_model_runner.py
4	1	vllm/worker/cpu_model_runner.py
5	2	vllm/worker/cpu_pooling_model_runner.py
7	3	vllm/worker/enc_dec_model_runner.py
5	2	vllm/worker/model_runner.py
5	2	vllm/worker/multi_step_neuron_model_runner.py
5	2	vllm/worker/multi_step_neuronx_distributed_model_runner.py
10	6	vllm/worker/neuron_model_runner.py
7	3	vllm/worker/pooling_model_runner.py
6	3	vllm/worker/xpu_model_runner.py

[6b6d49611] chunxiaozheng 2025-05-27 optimize get_kv_cache_torch_dtype (#18531)
3	4	vllm/utils.py

[aaa4ac1c9] cascade 2025-05-27 Disable prefix cache by default for benchmark (#18639)
3	0	benchmarks/benchmark_latency.py
3	0	vllm/benchmarks/latency.py

[06a033801] Mark McLoughlin 2025-05-27 [V1][Metrics] Add API for accessing in-memory Prometheus metrics (#17010)
1	0	.buildkite/test-pipeline.yaml
20	16	examples/offline_inference/eagle.py
49	0	examples/offline_inference/metrics.py
65	0	tests/v1/engine/test_llm_engine.py
112	0	tests/v1/test_metrics_reader.py
19	1	vllm/entrypoints/llm.py
25	4	vllm/v1/engine/llm_engine.py
4	4	vllm/v1/metrics/loggers.py
245	0	vllm/v1/metrics/reader.py
3	3	vllm/v1/spec_decode/metrics.py

[4318c0559] Cyrus Leung 2025-05-27 [CI/Build] Remove imports of built-in `re` (#18750)
1	0	.pre-commit-config.yaml
2	2	docs/mkdocs/hooks/generate_examples.py
1	2	docs/mkdocs/hooks/url_schemes.py
1	0	requirements/docs.txt
2	1	tools/check_triton_import.py
1	2	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py
1	1	vllm/model_executor/guided_decoding/guidance_decoding.py
1	1	vllm/model_executor/guided_decoding/outlines_decoding.py

[a68e293cb] Hyogeun Oh (오효근) 2025-05-27 [Doc]  Convert Sphinx directives ( `{class}`, `{meth}`, `{attr}`, ...) to MkDocs format for better documentation linking (#18663)
2	1	vllm/compilation/compiler_interface.py
20	16	vllm/config.py
4	1	vllm/connections.py
27	23	vllm/engine/async_llm_engine.py
4	4	vllm/engine/llm_engine.py
6	4	vllm/engine/multiprocessing/client.py
11	8	vllm/engine/multiprocessing/engine.py
5	2	vllm/engine/output_processor/multi_step.py
14	7	vllm/engine/output_processor/single_step.py
2	3	vllm/entrypoints/llm.py
4	2	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/executor/executor_base.py
3	2	vllm/inputs/__init__.py
39	29	vllm/inputs/data.py
4	4	vllm/inputs/parse.py
39	17	vllm/inputs/preprocess.py
2	2	vllm/inputs/registry.py
5	5	vllm/logger.py
11	9	vllm/model_executor/layers/sampler.py
4	2	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/molmo.py
2	2	vllm/model_executor/models/pixtral.py
2	1	vllm/model_executor/models/qwen_vl.py
1	1	vllm/model_executor/models/registry.py
1	1	vllm/model_executor/models/utils.py
4	3	vllm/multimodal/__init__.py
45	29	vllm/multimodal/inputs.py
7	6	vllm/multimodal/parse.py
48	25	vllm/multimodal/processing.py
1	1	vllm/multimodal/profiling.py
13	13	vllm/multimodal/registry.py
2	1	vllm/multimodal/utils.py
5	4	vllm/platforms/interface.py
10	8	vllm/sequence.py
3	2	vllm/utils.py
2	2	vllm/v1/worker/utils.py
6	5	vllm/worker/multi_step_model_runner.py

[688110794] Shawn Huang 2025-05-27 [BUG FIX] minicpm (#18739)
0	3	vllm/model_executor/models/minicpm.py

[e0f0ff87b] Kebe 2025-05-27 [Build] fix cpu build missing libtbbmalloc.so (#18744)
1	1	requirements/cpu.txt

[c24b1572a] maobaolong 2025-05-27 Minor fix about MooncakeStoreConnector (#18721)
5	4	vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py

[4693a3438] Calvin Chen 2025-05-27 [Doc] cleanup deprecated flag for doc (#18715)
2	4	benchmarks/README.md

[bbd9a84dc] Łukasz Durejko 2025-05-27 [Hardware][Intel-Gaudi] [CI/Build] Fix multiple containers using the same name in run-hpu-test.sh (#18752)
6	6	.buildkite/scripts/hardware_ci/run-hpu-test.sh

[a547aeb82] almersawi 2025-05-27 feat(rocm-support): support mamba2 on rocm (#18565)
2	2	CMakeLists.txt
4	6	csrc/mamba/causal_conv1d/causal_conv1d.cu
1	1	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
35	35	csrc/torch_bindings.cpp
18	5	vllm/model_executor/layers/mamba/mamba2_metadata.py

[fc6d0c290] Reid 2025-05-27 [Misc] improve docs (#18734)
43	37	examples/offline_inference/neuron_eagle.py
16	6	examples/offline_inference/qwen2_5_omni/README.md

[753944fa9] Cyrus Leung 2025-05-27 [Doc] Update reproducibility doc and example (#18741)
32	31	docs/usage/reproducibility.md
15	12	examples/offline_inference/reproducibility.py

[25a817f20] Cyrus Leung 2025-05-27 [Doc] Update OOT model docs (#18742)
15	16	docs/contributing/model/registration.md
4	2	docs/design/plugin_system.md

[d260f799a] vllmellm 2025-05-27 [FEAT] [ROCm] Upgrade AITER Fused MoE kernels. (#18271)
2	4	vllm/model_executor/layers/fused_moe/layer.py
123	278	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
6	32	vllm/model_executor/layers/quantization/fp8.py

[b50602d5f] Lukas Geiger 2025-05-27 [Model][Gemma3] Cast image pixel values already on CPU (#18732)
6	3	vllm/model_executor/models/gemma3_mm.py

[1f1b1bc03] Isotr0py 2025-05-27 [V1][Quantization] Add CUDA graph compatible v1 GGUF support (#18646)
1	3	tests/kernels/quantization/test_gguf.py
6	2	tests/models/quantization/test_gguf.py
0	8	vllm/engine/arg_utils.py
0	4	vllm/model_executor/layers/linear.py
181	42	vllm/model_executor/layers/quantization/gguf.py

[1f88dbd2b] Reid 2025-05-27 [Misc] improve web section group title display (#18684)
12	0	docs/mkdocs/stylesheets/extra.css

[0eebd7484] Lukas Geiger 2025-05-27 [Model][Gemma3] Simplify image input validation (#18710)
6	12	vllm/model_executor/models/gemma3_mm.py

[27bebcd89] Harry Mellor 2025-05-26 Convert `examples` to `ruff-format` (#18400)
1	1	.pre-commit-config.yaml
80	70	examples/offline_inference/audio_language.py
10	6	examples/offline_inference/automatic_prefix_caching.py
4	14	examples/offline_inference/basic/chat.py
8	7	examples/offline_inference/basic/classify.py
7	7	examples/offline_inference/basic/embed.py
3	3	examples/offline_inference/basic/score.py
11	11	examples/offline_inference/batch_llm_inference.py
58	52	examples/offline_inference/chat_with_tools.py
61	50	examples/offline_inference/data_parallel.py
12	11	examples/offline_inference/disaggregated-prefill-v1/decode_example.py
10	9	examples/offline_inference/disaggregated-prefill-v1/prefill_example.py
27	18	examples/offline_inference/disaggregated_prefill.py
23	27	examples/offline_inference/eagle.py
11	8	examples/offline_inference/embed_jina_embeddings_v3.py
7	8	examples/offline_inference/embed_matryoshka_fy.py
26	14	examples/offline_inference/encoder_decoder.py
32	30	examples/offline_inference/encoder_decoder_multimodal.py
19	15	examples/offline_inference/llm_engine_example.py
19	19	examples/offline_inference/load_sharded_state.py
61	55	examples/offline_inference/lora_with_quantization_inference.py
20	44	examples/offline_inference/mistral-small.py
6	5	examples/offline_inference/mlpspeculator.py
49	37	examples/offline_inference/multilora_inference.py
2	1	examples/offline_inference/neuron.py
3	3	examples/offline_inference/neuron_eagle.py
5	4	examples/offline_inference/neuron_int8_quantization.py
5	5	examples/offline_inference/neuron_speculation.py
13	8	examples/offline_inference/prefix_caching.py
82	97	examples/offline_inference/prithvi_geospatial_mae.py
133	92	examples/offline_inference/profiling.py
36	33	examples/offline_inference/profiling_tpu/profiling.py
29	36	examples/offline_inference/prompt_embed_inference.py
68	59	examples/offline_inference/qwen2_5_omni/only_thinker.py
17	13	examples/offline_inference/qwen_1m.py
10	11	examples/offline_inference/rlhf.py
13	10	examples/offline_inference/rlhf_colocate.py
16	16	examples/offline_inference/rlhf_utils.py
21	20	examples/offline_inference/save_sharded_state.py
21	22	examples/offline_inference/structured_outputs.py
1	2	examples/offline_inference/torchrun_example.py
6	4	examples/offline_inference/tpu.py
223	210	examples/offline_inference/vision_language.py
41	30	examples/offline_inference/vision_language_embedding.py
210	215	examples/offline_inference/vision_language_multi_image.py
9	13	examples/online_serving/api_client.py
9	8	examples/online_serving/cohere_rerank_client.py
93	95	examples/online_serving/disaggregated_serving/disagg_proxy_demo.py
38	44	examples/online_serving/gradio_openai_chatbot_webserver.py
14	18	examples/online_serving/gradio_webserver.py
6	6	examples/online_serving/jinaai_rerank_client.py
7	9	examples/online_serving/kv_events_subscriber.py
12	16	examples/online_serving/openai_chat_completion_client.py
114	143	examples/online_serving/openai_chat_completion_client_for_multimodal.py
74	73	examples/online_serving/openai_chat_completion_client_with_tools.py
24	37	examples/online_serving/openai_chat_completion_client_with_tools_required.py
46	34	examples/online_serving/openai_chat_completion_structured_outputs.py
21	23	examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py
35	24	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
70	88	examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py
6	6	examples/online_serving/openai_chat_completion_with_reasoning.py
1	3	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
62	65	examples/online_serving/openai_chat_embedding_client_for_multimodal.py
1	3	examples/online_serving/openai_classification_client.py
5	4	examples/online_serving/openai_completion_client.py
4	9	examples/online_serving/openai_cross_encoder_score.py
1	1	examples/online_serving/openai_embedding_client.py
9	12	examples/online_serving/openai_pooling_client.py
14	16	examples/online_serving/openai_transcription_client.py
3	6	examples/online_serving/opentelemetry/dummy_client.py
9	11	examples/online_serving/prompt_embed_inference_with_openai_client.py
1	3	examples/online_serving/ray_serve_deepseek.py
60	53	examples/online_serving/retrieval_augmented_generation_with_langchain.py
52	45	examples/online_serving/retrieval_augmented_generation_with_llamaindex.py
29	26	examples/online_serving/streamlit_openai_chatbot_webserver.py
3	3	examples/online_serving/utils.py
11	13	examples/others/lmcache/cpu_offload_lmcache.py
33	25	examples/others/lmcache/disagg_prefill_lmcache_v0.py
57	43	examples/others/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py
21	19	examples/others/lmcache/kv_cache_sharing_lmcache_v1.py
5	2	examples/others/tensorize_vllm_model.py
54	0	examples/pyproject.toml
2	0	pyproject.toml

[e7523c2e0] Lukas Geiger 2025-05-26 [V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs (#18608)
9	8	vllm/v1/sample/ops/topk_topp_sampler.py

[a869baca7] Cyrus Leung 2025-05-26 [Bugfix] Fix Llama GGUF initialization (#18717)
1	1	vllm/model_executor/models/llama.py

[82e2339b0] Cyrus Leung 2025-05-26 [Doc] Move examples and further reorganize user guide (#18666)
0	5	.buildkite/pyproject.toml
1	1	.buildkite/test-pipeline.yaml
1	1	.gitignore
0	5	benchmarks/pyproject.toml
4	5	docs/.nav.yml
7	2	docs/configuration/README.md
0	0	docs/{usage => configuration}/env_vars.md
2	2	docs/design/v1/metrics.md
1	1	docs/mkdocs/hooks/generate_examples.py
1	1	docs/models/extensions/tensorizer.md
3	3	docs/training/rlhf.md
0	0	examples/{ => others}/lmcache/README.md
0	0	examples/{ => others}/lmcache/cpu_offload_lmcache.py
0	0	examples/{ => others}/lmcache/disagg_prefill_lmcache_v0.py
0	0	examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-decoder-config.yaml
0	0	examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-prefiller-config.yaml
0	0	examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh
0	0	examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py
0	0	examples/{ => others}/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh
0	0	examples/{ => others}/lmcache/kv_cache_sharing_lmcache_v1.py
0	0	examples/{other => others}/logging_configuration.md
5	5	examples/{other => others}/tensorize_vllm_model.py
0	5	pyproject.toml
1	1	requirements/common.txt
1	1	tests/lora/test_llama_tp.py
2	2	vllm/model_executor/model_loader/tensorizer.py
2	2	vllm/model_executor/model_loader/tensorizer_loader.py

[9553fdb41] Cyrus Leung 2025-05-26 [Doc] Improve API docs (#18713)
6	5	docs/.nav.yml
1	1	docs/mkdocs/stylesheets/extra.css

[243eb9199] dylan 2025-05-26 [Bugfix]: handle hf-xet CAS error when loading Qwen3 weights in vLLM (#18701)
1	1	requirements/common.txt

[0665e2999] Reid 2025-05-26 [Misc] add AutoGen integration (#18712)
83	0	docs/deployment/frameworks/autogen.md

[e76be0655] Łukasz Durejko 2025-05-26 [Hardware][Intel-Gaudi] [CI/Build] Add tensor parallel size = 2 test to HPU CI (#18709)
2	0	.buildkite/scripts/hardware_ci/run-hpu-test.sh

[087775002] Isotr0py 2025-05-26 [CI/Build] Split pooling and generation extended language models tests in CI (#18705)
12	3	.buildkite/test-pipeline.yaml

[6d68030f1] Naveassaf 2025-05-26 [Model] Add support for YARN in NemotronNAS models (#18427)
21	14	vllm/model_executor/models/llama.py
46	2	vllm/model_executor/models/nemotron_nas.py

[5a2c76cbe] Ning Xie 2025-05-26 [CI] fix dump_input for str type (#18697)
37	1	tests/test_logger.py
3	3	vllm/logging_utils/dump_input.py

[38b13dfe7] Cyrus Leung 2025-05-26 [CI/Build] Replace `math.isclose` with `pytest.approx` (#18703)
1	2	tests/entrypoints/openai/correctness/test_mteb.py
3	5	tests/entrypoints/openai/test_score.py
1	2	tests/models/language/pooling/mteb_utils.py
4	5	tests/models/language/pooling/test_gritlm.py
3	5	tests/models/language/pooling/test_jina.py
10	12	tests/models/language/pooling/test_scoring.py

[61a45e7a7] Cyrus Leung 2025-05-26 [Bugfix] Fix Mistral-format models with sliding window (#18693)
10	4	vllm/config.py

[65523a099] Cyrus Leung 2025-05-26 [Doc] Fix issue template format (#18699)
1	1	.github/ISSUE_TEMPLATE/450-ci-failure.yml

[4b7740a10] Cyrus Leung 2025-05-26 [GH] Add issue template for reporting CI failures (#18696)
3	3	.github/ISSUE_TEMPLATE/400-bug-report.yml
69	0	.github/ISSUE_TEMPLATE/450-ci-failure.yml

[4ea62c0ea] Ning Xie 2025-05-26 [CI] add missing argument (#18694)
3	2	tests/runai_model_streamer_test/test_weight_utils.py

[561b77a0d] Maximilien de Bayser 2025-05-26 [Bugfix] Fix the lm_head in gpt_bigcode in lora mode (#6357)
5	8	vllm/model_executor/models/gpt_bigcode.py

[abd4030d9] CYJiang 2025-05-26 refactor: simplify request handler, use positive condition check for handler assignment (#18690)
6	6	vllm/entrypoints/openai/run_batch.py

[8820821b5] AlexZhao 2025-05-26 [Misc] Fixed the abnormally high TTFT issue in the PD disaggregation example (#18644)
2	2	examples/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py

[fba064270] Cyrus Leung 2025-05-26 [CI/Build][Doc] Update `gte-Qwen2-1.5B-instruct` usage (#18683)
1	4	docs/models/supported_models.md
1	5	tests/models/language/pooling/test_embedding.py
0	9	tests/models/language/pooling/test_gte.py

[6071e989d] Lukas Geiger 2025-05-25 [Core][Multimodal] Convert PIL Image to array without data copy when hashing (#18682)
2	2	vllm/multimodal/hasher.py
1	1	vllm/multimodal/video.py

[57fd13a70] Cyrus Leung 2025-05-25 [Bugfix] Fix profiling dummy data for Pixtral (#18677)
105	156	tests/models/multimodal/processing/test_common.py
1	1	tests/models/multimodal/processing/test_mllama.py
5	4	tests/models/registry.py
32	4	vllm/model_executor/models/pixtral.py
10	5	vllm/multimodal/profiling.py

[3a886bd58] Reid 2025-05-25 [Misc] small improve (#18680)
1	1	docs/features/quantization/bnb.md

[35be8fad6] Reid 2025-05-25 [CI/build] fix no regex (#18676)
5	0	.github/workflows/cleanup_pr_body.yml

[f2faac745] Yuqi Zhang 2025-05-25 [Bugfix] Fix cpu usage and cache hit stats reporting on cpu environment (#18674)
14	0	vllm/engine/llm_engine.py

[279f85451] Reid 2025-05-25 [doc] improve readability (#18675)
6	1	docs/contributing/dockerfile/dockerfile.md
4	1	docs/contributing/model/registration.md
9	6	docs/deployment/docker.md
11	3	docs/deployment/frameworks/skypilot.md
2	1	docs/deployment/frameworks/streamlit.md
31	4	docs/deployment/nginx.md
3	1	docs/features/quantization/auto_awq.md
13	2	docs/features/quantization/bitblas.md
11	3	docs/features/quantization/bnb.md
8	3	docs/features/quantization/gguf.md
2	1	docs/features/quantization/gptqmodel.md
9	2	docs/features/quantization/torchao.md
2	1	docs/features/reasoning_outputs.md
7	2	docs/features/spec_decode.md
17	2	docs/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
9	3	docs/getting_started/installation/ai_accelerator/neuron.inc.md
14	4	docs/getting_started/installation/gpu/cuda.inc.md
18	9	docs/getting_started/installation/gpu/rocm.inc.md
21	7	docs/models/extensions/runai_model_streamer.md
9	3	docs/serving/openai_compatible_server.md

[624b77a2b] Reid 2025-05-25 [doc] fix broken links (#18671)
1	1	README.md

[503f8487c] Cyrus Leung 2025-05-25 [Misc] Reduce logs on startup (#18649)
2	3	vllm/model_executor/layers/quantization/fp8.py
1	4	vllm/platforms/__init__.py
13	9	vllm/plugins/__init__.py

[44073a7ac] Ning Xie 2025-05-25 [BUGFIX] catch subclass first for try...except (#18672)
3	2	vllm/model_executor/model_loader/weight_utils.py

[63934543a] Michael Goin 2025-05-25 Speed up the `kernels/quantization/` tests (#18669)
7	7	tests/kernels/quantization/test_block_fp8.py
2	2	tests/kernels/quantization/test_gguf.py
8	16	tests/kernels/quantization/test_triton_scaled_mm.py

[75f81750f] Isotr0py 2025-05-25 [VLM] Initialize video input support for InternVL models (#18499)
4	1	docs/models/supported_models.md
11	4	examples/offline_inference/vision_language.py
11	0	tests/models/multimodal/generation/test_common.py
66	20	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	0	tests/models/multimodal/processing/test_common.py
2	1	tests/models/registry.py
2	0	vllm/entrypoints/chat_utils.py
6	5	vllm/model_executor/models/h2ovl.py
485	26	vllm/model_executor/models/internvl.py
8	5	vllm/model_executor/models/nvlm_d.py

[6ab681bcb] Mengqing Cao 2025-05-25 [Misc][ModelScope] Change to use runtime VLLM_USE_MODELSCOPE (#18655)
3	0	tests/test_regression.py
2	2	vllm/model_executor/model_loader/default_loader.py
2	2	vllm/transformers_utils/__init__.py
10	11	vllm/transformers_utils/config.py
2	2	vllm/transformers_utils/tokenizer.py

[cebc22f3b] Chenguang Li 2025-05-25 [Misc]Replace `cuda` hard code with `current_platform` in Ray (#14668)
2	4	vllm/executor/ray_utils.py

[6c6dcd861] Ning Xie 2025-05-25 [MISC] correct signature for LoaderFunction (#18670)
1	1	vllm/model_executor/model_loader/weight_utils.py

[7891fdf0c] Seiji Eicher 2025-05-24 [V1] Fix _pickle.PicklingError: Can't pickle <class 'transformers_modules.deepseek-ai.DeepSeek-V2-Lite... (#18640)
5	0	vllm/v1/engine/async_llm.py

[6825d9a99] Woosuk Kwon 2025-05-24 [BugFix][Spec Decode] Improve Prefix Caching Logic in Speculative Decoding (#18668)
2	1	vllm/v1/core/kv_cache_manager.py
5	0	vllm/v1/core/sched/scheduler.py

[b554ab736] Reid 2025-05-25 [CI/Build] fix permission denied issue (#18645)
1	1	.github/workflows/cleanup_pr_body.yml

[9ea7f1abf] Aaron Pham 2025-05-24 fix(regression): clone from reference items (#18662)
1	1	vllm/sequence.py

[2807271c8] Aaron Pham 2025-05-24 [CI] enforce import regex instead of re (#18665)
7	0	.pre-commit-config.yaml
83	0	tools/enforce_regex_import.py

[b9018a3f9] wangxiyuan 2025-05-24 [BugFix] Fix import error for fused_moe (#18642)
1	0	vllm/model_executor/layers/fused_moe/layer.py

[4ceafb629] Ning Xie 2025-05-24 [MISC] typo fix and clean import (#18664)
1	2	vllm/distributed/kv_transfer/__init__.py
1	1	vllm/distributed/kv_transfer/kv_connector/simple_connector.py

[2e6705784] Cyrus Leung 2025-05-24 [CI/Build] `chmod +x` to `cleanup_pr_body.sh` (#18650)
0	0	.github/scripts/cleanup_pr_body.sh

[1cb194a01] Cyrus Leung 2025-05-24 [Doc] Reorganize user guide (#18661)
1	1	.github/PULL_REQUEST_TEMPLATE.md
1	1	CONTRIBUTING.md
1	1	README.md
18	13	docs/.nav.yml
4	0	docs/configuration/README.md
144	0	docs/configuration/conserving_memory.md
0	0	docs/{serving => configuration}/engine_args.md
23	0	docs/configuration/model_resolution.md
1	4	docs/{performance => configuration}/optimization.md
0	0	docs/{serving => configuration}/serve_args.md
0	0	docs/contributing/{overview.md => README.md}
0	0	docs/{performance => contributing}/benchmarks.md
1	1	docs/design/multiprocessing.md
1	1	docs/design/v1/metrics.md
1	1	docs/features/tool_calling.md
0	185	docs/serving/offline_inference.md
7	0	docs/usage/README.md
0	0	docs/{serving => usage}/env_vars.md
0	0	docs/{getting_started => usage}/faq.md
0	0	docs/{serving => usage}/metrics.md
1	1	docs/{serving/seed_parameter_behavior.md => usage/reproducibility.md}
1	1	docs/{deployment => usage}/security.md
3	3	docs/{getting_started => usage}/troubleshooting.md
0	0	docs/{serving => usage}/usage_stats.md
1	1	docs/{getting_started/v1_user_guide.md => usage/v1_guide.md}
1	1	vllm/envs.py
1	1	vllm/utils.py

[2cd4d58df] ztang2370 2025-05-24 [Model] use AutoWeightsLoader for gpt2 (#18625)
43	30	vllm/model_executor/models/gpt2.py

[6d166a8d3] Cyrus Leung 2025-05-24 [Doc] Add community links (#18657)
3	1	docs/.nav.yml
1	1	docs/community/meetups.md

[ef1dd6870] Cyrus Leung 2025-05-24 [Doc] Fix indentation problems in V0 Paged Attention docs (#18659)
1	0	docs/deployment/k8s.md
371	373	docs/design/kernel/paged_attention.md

[e77dc4bad] Mengqing Cao 2025-05-24 [MISC][pre-commit] Add pre-commit check for triton import (#17716)
7	0	.pre-commit-config.yaml
75	0	tools/check_triton_import.py

[07458a51c] Cyrus Leung 2025-05-24 [Doc] Update README links, mark external links (#18635)
3	3	README.md
20	0	docs/mkdocs/stylesheets/extra.css

[c1e4a4052] qizixi 2025-05-24 [V1][Spec Decode] Support multi-layer eagle draft model (#18030)
3	0	tests/v1/spec_decode/test_eagle.py
29	4	vllm/v1/spec_decode/eagle.py
13	5	vllm/v1/worker/gpu_model_runner.py

[a85932057] Yuanhao WU 2025-05-24 [Model] Add support for Qwen2.5-Omni-7B-AWQ (Qwen2_5OmniForConditionalGeneration) (#18647)
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py

[441dc63ac] Reid 2025-05-24 [Frontend] improve vllm serve --help display (#18643)
5	2	vllm/entrypoints/cli/main.py
6	1	vllm/entrypoints/cli/serve.py
59	0	vllm/entrypoints/utils.py
4	2	vllm/utils.py

[d55e446d1] qizixi 2025-05-23 [V1][Spec Decode] Small refactors to improve eagle bookkeeping performance (#18424)
5	1	tests/v1/spec_decode/test_eagle.py
3	7	vllm/v1/spec_decode/eagle.py
13	11	vllm/v1/worker/gpu_model_runner.py

[ec82c3e38] Wenhua Cheng 2025-05-24 FIX MOE issue in AutoRound format (#18586)
1	1	README.md
29	27	vllm/model_executor/layers/quantization/auto_round.py

[45ab403a1] Mathieu Borderé 2025-05-24 config.py: Clarify that only local GGUF checkpoints are supported. (#18623)
4	1	vllm/transformers_utils/config.py

[2b10ba749] Robert Shaw 2025-05-23 [Bugfix][Nixl] Fix Preemption Bug (#18631)
81	0	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
16	15	vllm/v1/core/sched/scheduler.py

[4fc1bf813] Feng XiaoLong 2025-05-24 [Bugfix] Migrate to REGEX Library to prevent catastrophic backtracking (#18454)
1	1	.github/scripts/cleanup_pr_body.sh
2	2	benchmarks/benchmark_serving_structured_output.py
1	1	benchmarks/kernels/graph_machete_bench.py
1	1	examples/offline_inference/prithvi_geospatial_mae.py
1	0	pyproject.toml
1	0	requirements/build.txt
1	0	requirements/common.txt
1	1	requirements/nightly_torch_test.txt
1	2	setup.py
1	1	tests/entrypoints/llm/test_guided_generate.py
1	1	tests/entrypoints/openai/test_chat.py
1	2	tests/entrypoints/openai/test_completion.py
6	6	tests/entrypoints/openai/test_prompt_validation.py
1	1	tests/models/multimodal/generation/test_phi4mm.py
1	1	tests/models/multimodal/generation/vlm_utils/model_utils.py
2	2	tests/tool_use/test_tool_choice_required.py
1	1	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	1	tests/v1/entrypoints/openai/test_completion.py
2	1	tests/v1/sample/utils.py
1	1	vllm/collect_env.py
1	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py
2	1	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
1	1	vllm/lora/models.py
1	1	vllm/lora/utils.py
1	1	vllm/model_executor/guided_decoding/utils.py
1	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
1	1	vllm/model_executor/layers/quantization/modelopt.py
2	1	vllm/model_executor/layers/quantization/quark/utils.py
1	1	vllm/model_executor/layers/quantization/utils/gptq_utils.py
1	1	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/model_executor/models/mimo_mtp.py
1	1	vllm/model_executor/models/minimax_text_01.py
1	1	vllm/model_executor/models/phi3v.py
1	1	vllm/model_executor/models/qwen_vl.py
1	1	vllm/model_executor/models/transformers.py
1	1	vllm/multimodal/processing.py
1	1	vllm/reasoning/granite_reasoning_parser.py
1	1	vllm/transformers_utils/tokenizers/mistral.py
1	1	vllm/utils.py
1	1	vllm/v1/structured_output/utils.py

[f2036734f] Pavani Majety 2025-05-23 [ModelOpt] Introduce VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE env var to control blockscale tensor allocation (#18160)
9	3	vllm/_custom_ops.py
8	0	vllm/envs.py
3	15	vllm/model_executor/layers/fused_moe/cutlass_moe.py

[7d9216495] Cyrus Leung 2025-05-24 [Doc] Update references to doc files (#18637)
2	4	.github/mergify.yml
2	2	docker/Dockerfile
2	3	docs/contributing/overview.md
-	-	docs/source/assets/contributing/dockerfile-stages-dependency.png
1	1	tools/update-dockerfile-graph.sh
2	2	vllm/config.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/engine/output_processor/multi_step.py
1	1	vllm/platforms/cpu.py
1	1	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/utils.py
1	1	vllm/worker/multi_step_model_runner.py
1	1	vllm/worker/utils.py

[0ddf88e16] Michael Goin 2025-05-23 [CI] Enable test_initialization to run on V1 (#16736)
1	4	.buildkite/test-pipeline.yaml
24	8	tests/models/registry.py
13	4	tests/models/test_initialization.py
7	23	vllm/model_executor/models/grok1.py
9	6	vllm/utils.py

[1645b6019] Huy Do 2025-05-23 Use prebuilt FlashInfer x86_64 PyTorch 2.7 CUDA 12.8 wheel for CI (#18537)
8	9	docker/Dockerfile

[2628a69e3] Jiayi Yao 2025-05-23 [V1] Support Deepseek MTP (#18435)
11	2	vllm/config.py
1	1	vllm/engine/arg_utils.py
2	1	vllm/model_executor/models/deepseek_mtp.py
65	57	vllm/v1/spec_decode/eagle.py
27	0	vllm/v1/spec_decode/utils.py
14	5	vllm/v1/worker/gpu_model_runner.py

[371f7e4ca] Cyrus Leung 2025-05-24 [Doc] Fix broken links and unlinked docs, add shortcuts to home sidebar (#18627)
8	3	docs/.nav.yml
2	2	docs/contributing/model/tests.md
1	1	docs/features/spec_decode.md
3	3	docs/models/supported_models.md
1	1	docs/serving/openai_compatible_server.md
1	1	docs/{ => serving}/seed_parameter_behavior.md

[15b45ffb9] Cyrus Leung 2025-05-24 [Doc] Avoid documenting dynamic / internal modules (#18626)
4	0	mkdocs.yaml

[273cb3b4d] Cyrus Leung 2025-05-24 [Doc] Fix top-level API links/docs (#18621)
12	10	vllm/benchmarks/datasets.py
1	1	vllm/config.py
5	4	vllm/distributed/kv_transfer/kv_connector/v1/base.py
20	20	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
15	25	vllm/engine/llm_engine.py
32	31	vllm/entrypoints/llm.py
1	1	vllm/multimodal/__init__.py
2	2	vllm/multimodal/registry.py
0	9	vllm/outputs.py

[8ddd1cf26] David Xia 2025-05-23 [Doc] fix list formatting (#18624)
4	3	docs/getting_started/installation/gpu/cuda.inc.md

[6550114c9] Chen Zhang 2025-05-24 [v1] Redo "Support multiple KV cache groups in GPU model runner (#17945)" (#18593)
68	3	tests/v1/core/test_kv_cache_utils.py
18	18	tests/v1/core/test_prefix_caching.py
33	6	tests/v1/worker/test_gpu_input_batch.py
41	16	tests/v1/worker/test_gpu_model_runner.py
3	3	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	2	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
21	13	vllm/v1/core/kv_cache_manager.py
6	7	vllm/v1/core/kv_cache_utils.py
6	6	vllm/v1/core/sched/output.py
10	6	vllm/v1/core/sched/scheduler.py
42	0	vllm/v1/kv_cache_interface.py
41	0	vllm/v1/worker/block_table.py
6	6	vllm/v1/worker/gpu_input_batch.py
152	101	vllm/v1/worker/gpu_model_runner.py
20	16	vllm/v1/worker/tpu_model_runner.py

[9520a989d] Michael Goin 2025-05-23 [Docs] Change mkdocs to not use directory urls (#18622)
5	0	mkdocs.yaml

[3d28ad343] Harry Mellor 2025-05-23 Fix figures in design doc (#18612)
12	26	docs/design/kernel/paged_attention.md

[6a7988c55] youkaichao 2025-05-23 Refactor pplx init logic to make it modular (prepare for deepep) (#18200)
67	34	vllm/distributed/device_communicators/all2all.py
87	2	vllm/distributed/device_communicators/base_device_communicator.py
23	22	vllm/distributed/device_communicators/cuda_communicator.py
6	50	vllm/distributed/parallel_state.py
3	0	vllm/envs.py
97	143	vllm/model_executor/layers/fused_moe/layer.py
0	1	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
5	16	vllm/model_executor/layers/quantization/fp8.py
1	0	vllm/platforms/cuda.py
1	2	vllm/v1/worker/gpu_worker.py
1	2	vllm/v1/worker/tpu_worker.py
1	2	vllm/worker/cpu_worker.py
2	4	vllm/worker/hpu_worker.py
1	2	vllm/worker/tpu_worker.py
1	2	vllm/worker/worker.py
1	2	vllm/worker/xpu_worker.py

[022d8abe2] Cyrus Leung 2025-05-23 [Doc] Use a different color for the announcement (#18616)
4	0	docs/mkdocs/stylesheets/extra.css
3	0	mkdocs.yaml

[5221815a0] Hyogeun Oh (오효근) 2025-05-24 [Doc] Fix markdown list indentation for MkDocs rendering (#18620)
6	9	docs/design/huggingface_integration.md
1	0	pyproject.toml

[1068556b2] Simon Mo 2025-05-23 [Bugfix][Build/CI] Fixup CUDA compiler version check for CUDA_SUPPORTED_ARCHS (#18579)
9	7	CMakeLists.txt

[2cd1fa455] Reid 2025-05-23 [Misc] add Haystack integration (#18601)
60	0	docs/deployment/frameworks/haystack.md

[d4c291976] Harry Mellor 2025-05-23 Include private attributes in API documentation (#18614)
1	0	mkdocs.yaml
18	17	vllm/model_executor/layers/rejection_sampler.py
24	32	vllm/model_executor/layers/typical_acceptance_sampler.py

[6220f3c6b] Tristan Leclercq 2025-05-23 [Bugfix] Fix transformers model impl ignored for mixtral quant (#18602)
4	5	vllm/model_executor/model_loader/utils.py

[52fb23f47] Harry Mellor 2025-05-23 Fix examples with code blocks in docs (#18609)
7	4	docs/mkdocs/hooks/generate_examples.py

[6dd51c7ef] Cyrus Leung 2025-05-23 [CI/Build] Fix V1 flag being set in entrypoints tests (#18598)
3	2	.buildkite/test-pipeline.yaml

[2edb533af] Harry Mellor 2025-05-23 Replace `{func}` with mkdocs style links (#18610)
1	1	vllm/model_executor/models/llava_next.py
1	1	vllm/multimodal/processing.py
2	2	vllm/platforms/interface.py
1	1	vllm/sequence.py
1	1	vllm/v1/worker/utils.py

[38a95cb4a] Hyogeun Oh (오효근) 2025-05-23 [Doc] Fix indent of contributing to vllm (#18611)
2	2	docs/contributing/overview.md

[cd821ea5d] Ning Xie 2025-05-23 [CI] fix kv_cache_type argument (#18594)
1	1	tests/plugins_tests/test_platform_plugins.py

[7ab056c27] Kay Yan 2025-05-23 [Hardware][CPU] Update intel_extension_for_pytorch 2.7.0 and move to `requirements/cpu.txt`  (#18542)
0	3	docker/Dockerfile.cpu
4	0	requirements/cpu.txt
1	1	vllm/model_executor/layers/quantization/ipex_quant.py

[6526e0511] Harry Mellor 2025-05-23 Add myself as docs code owner (#18605)
4	0	.github/CODEOWNERS

[e493e4852] Madeesh Kannan 2025-05-23 [V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled (#17731)
22	4	vllm/model_executor/guided_decoding/guidance_logits_processors.py
12	0	vllm/model_executor/guided_decoding/outlines_logits_processors.py
5	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py
1	1	vllm/sequence.py

[4ce64e2df] Mengqing Cao 2025-05-23 [Bugfix][Model] Fix baichuan model loader for tp (#18597)
7	4	vllm/model_executor/models/baichuan.py

[fbb13a2c1] Cyrus Leung 2025-05-23 Revert "[V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (#18034)" (#18600)
2	1	vllm/transformers_utils/configs/eagle.py
1	5	vllm/v1/spec_decode/eagle.py

[a1fe24d96] Harry Mellor 2025-05-23 Migrate docs from Sphinx to MkDocs (#18145)
4	5	.buildkite/test-pipeline.yaml
1	5	.gitignore
1	0	.pre-commit-config.yaml
2	6	.readthedocs.yaml
2	0	docker/Dockerfile
51	0	docs/.nav.yml
0	25	docs/Makefile
50	43	docs/README.md
107	0	docs/api/README.md
2	0	docs/api/vllm/.meta.yml
-	-	docs/assets/contributing/dockerfile-stages-dependency.png
-	-	docs/{source => }/assets/deployment/anything-llm-chat-with-doc.png
-	-	docs/{source => }/assets/deployment/anything-llm-chat-without-doc.png
-	-	docs/{source => }/assets/deployment/anything-llm-provider.png
-	-	docs/{source => }/assets/deployment/anything-llm-upload-doc.png
-	-	docs/{source => }/assets/deployment/architecture_helm_deployment.png
-	-	docs/{source => }/assets/deployment/chatbox-chat.png
-	-	docs/{source => }/assets/deployment/chatbox-settings.png
-	-	docs/{source => }/assets/deployment/dify-chat.png
-	-	docs/{source => }/assets/deployment/dify-create-chatbot.png
-	-	docs/{source => }/assets/deployment/dify-settings.png
-	-	docs/{source => }/assets/deployment/open_webui.png
-	-	docs/{source => }/assets/deployment/streamlit-chat.png
-	-	docs/{source => }/assets/design/arch_overview/entrypoints.excalidraw.png
-	-	docs/{source => }/assets/design/arch_overview/llm_engine.excalidraw.png
-	-	docs/{source => }/assets/design/hierarchy.png
-	-	docs/{source => }/assets/design/v1/metrics/intervals-1.png
-	-	docs/{source => }/assets/design/v1/metrics/intervals-2.png
-	-	docs/{source => }/assets/design/v1/metrics/intervals-3.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/example-time-1.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/example-time-3.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/example-time-4.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/example-time-5.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/example-time-6.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/example-time-7.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/free.png
-	-	docs/{source => }/assets/design/v1/prefix_caching/overview.png
-	-	docs/{source => }/assets/features/disagg_prefill/abstraction.jpg
-	-	docs/{source => }/assets/features/disagg_prefill/overview.jpg
-	-	docs/{source => }/assets/kernel/k_vecs.png
-	-	docs/{source => }/assets/kernel/key.png
-	-	docs/{source => }/assets/kernel/logits_vec.png
-	-	docs/{source => }/assets/kernel/q_vecs.png
-	-	docs/{source => }/assets/kernel/query.png
-	-	docs/{source => }/assets/kernel/v_vec.png
-	-	docs/{source => }/assets/kernel/value.png
-	-	docs/{source => }/assets/logos/vllm-logo-only-light.ico
-	-	docs/{source => }/assets/logos/vllm-logo-only-light.png
-	-	docs/{source => }/assets/logos/vllm-logo-text-dark.png
-	-	docs/{source => }/assets/logos/vllm-logo-text-light.png
4	3	docs/{source => }/community/meetups.md
0	0	docs/{source => }/community/sponsors.md
0	0	docs/{source => }/contributing/deprecation_policy.md
4	6	docs/{source => }/contributing/dockerfile/dockerfile.md
23	0	docs/contributing/model/README.md
12	13	docs/{source => }/contributing/model/basic.md
803	0	docs/contributing/model/multimodal.md
17	20	docs/{source => }/contributing/model/registration.md
12	13	docs/{source => }/contributing/model/tests.md
28	19	docs/{source => }/contributing/overview.md
8	11	docs/{source/contributing/profiling/profiling_index.md => contributing/profiling.md}
0	0	docs/{source => }/contributing/vulnerability_management.md
126	0	docs/deployment/docker.md
8	11	docs/{source => }/deployment/frameworks/anything-llm.md
4	3	docs/{source => }/deployment/frameworks/bentoml.md
4	5	docs/{source => }/deployment/frameworks/cerebrium.md
6	7	docs/{source => }/deployment/frameworks/chatbox.md
7	9	docs/{source => }/deployment/frameworks/dify.md
6	8	docs/{source => }/deployment/frameworks/dstack.md
95	0	docs/deployment/frameworks/helm.md
4	3	docs/{source => }/deployment/frameworks/litellm.md
4	3	docs/{source => }/deployment/frameworks/lobe-chat.md
4	3	docs/{source => }/deployment/frameworks/lws.md
4	3	docs/{source => }/deployment/frameworks/modal.md
5	5	docs/{source => }/deployment/frameworks/open-webui.md
4	3	docs/{source => }/deployment/frameworks/retrieval_augmented_generation.md
4	21	docs/{source => }/deployment/frameworks/skypilot.md
5	5	docs/{source => }/deployment/frameworks/streamlit.md
4	3	docs/{source => }/deployment/frameworks/triton.md
4	3	docs/{source => }/deployment/integrations/kserve.md
4	3	docs/{source => }/deployment/integrations/kubeai.md
4	3	docs/{source => }/deployment/integrations/llamastack.md
4	3	docs/{source => }/deployment/integrations/llmaz.md
5	4	docs/{source => }/deployment/integrations/production-stack.md
6	6	docs/{source => }/deployment/k8s.md
20	20	docs/{source => }/deployment/nginx.md
0	0	docs/{source => }/deployment/security.md
47	56	docs/{source => }/design/arch_overview.md
4	3	docs/{source => }/design/automatic_prefix_caching.md
4	3	docs/{source => }/design/huggingface_integration.md
43	58	docs/{source => }/design/kernel/paged_attention.md
13	12	docs/{source => }/design/mm_processing.md
3	4	docs/{source => }/design/multiprocessing.md
5	4	docs/{source => }/design/plugin_system.md
5	11	docs/{source => }/design/v1/metrics.md
8	24	docs/{source => }/design/v1/prefix_caching.md
0	0	docs/{source => }/design/v1/torch_compile.md
6	6	docs/{source => }/features/automatic_prefix_caching.md
77	0	docs/features/compatibility_matrix.md
12	18	docs/{source => }/features/disagg_prefill.md
7	7	docs/{source => }/features/lora.md
44	54	docs/{source => }/features/multimodal_inputs.md
3	4	docs/{source => }/features/prompt_embeds.md
22	0	docs/features/quantization/README.md
4	3	docs/{source => }/features/quantization/auto_awq.md
8	8	docs/{source => }/features/quantization/bitblas.md
4	3	docs/{source => }/features/quantization/bnb.md
11	13	docs/{source => }/features/quantization/fp8.md
10	12	docs/{source => }/features/quantization/gguf.md
4	3	docs/{source => }/features/quantization/gptqmodel.md
8	9	docs/{source => }/features/quantization/int4.md
8	9	docs/{source => }/features/quantization/int8.md
0	0	docs/{source => }/features/quantization/modelopt.md
4	3	docs/{source => }/features/quantization/quantized_kvcache.md
10	10	docs/{source => }/features/quantization/quark.md
28	0	docs/features/quantization/supported_hardware.md
0	0	docs/{source => }/features/quantization/torchao.md
7	9	docs/{source => }/features/reasoning_outputs.md
14	16	docs/{source => }/features/spec_decode.md
9	16	docs/{source => }/features/structured_outputs.md
0	1	docs/{source => }/features/tool_calling.md
7	6	docs/{source => }/getting_started/faq.md
5	0	docs/getting_started/installation/.nav.yml
20	0	docs/getting_started/installation/README.md
117	0	docs/getting_started/installation/ai_accelerator.md
48	62	docs/{source => }/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
23	16	docs/{source => }/getting_started/installation/ai_accelerator/neuron.inc.md
50	64	docs/{source => }/getting_started/installation/ai_accelerator/tpu.inc.md
31	133	docs/{source => }/getting_started/installation/cpu.md
22	15	docs/{source => }/getting_started/installation/cpu/apple.inc.md
41	0	docs/getting_started/installation/cpu/arm.inc.md
2	0	docs/{source => }/getting_started/installation/cpu/build.inc.md
22	15	docs/{source => }/getting_started/installation/cpu/s390x.inc.md
46	0	docs/getting_started/installation/cpu/x86.inc.md
0	0	docs/{source => }/getting_started/installation/device.template.md
124	0	docs/getting_started/installation/gpu.md
39	36	docs/{source => }/getting_started/installation/gpu/cuda.inc.md
36	36	docs/{source => }/getting_started/installation/gpu/rocm.inc.md
21	15	docs/{source => }/getting_started/installation/gpu/xpu.inc.md
0	0	docs/{source => }/getting_started/installation/python_env_setup.inc.md
31	37	docs/{source => }/getting_started/quickstart.md
18	20	docs/{source => }/getting_started/troubleshooting.md
0	0	docs/{source => }/getting_started/v1_user_guide.md
0	35	docs/make.bat
159	0	docs/mkdocs/hooks/generate_examples.py
16	0	docs/mkdocs/hooks/remove_announcement.py
54	0	docs/mkdocs/hooks/url_schemes.py
0	19	docs/{source/_static/custom.js => mkdocs/javascript/run_llm_widget.js}
5	0	docs/mkdocs/overrides/main.html
0	0	docs/{source => }/models/extensions/fastsafetensor.md
8	9	docs/{source => }/models/extensions/runai_model_streamer.md
6	6	docs/{source => }/models/extensions/tensorizer.md
21	22	docs/{source => }/models/generative_models.md
41	66	docs/{source => }/models/pooling_models.md
690	0	docs/models/supported_models.md
8	7	docs/{source => }/performance/benchmarks.md
5	4	docs/{source => }/performance/optimization.md
17	22	docs/{source => }/serving/distributed_serving.md
18	0	docs/serving/engine_args.md
12	0	docs/serving/env_vars.md
4	3	docs/{source => }/serving/integrations/langchain.md
4	3	docs/{source => }/serving/integrations/llamaindex.md
4	6	docs/{source => }/serving/metrics.md
25	28	docs/{source => }/serving/offline_inference.md
177	219	docs/{source => }/serving/openai_compatible_server.md
38	0	docs/serving/serve_args.md
0	0	docs/{source => }/serving/usage_stats.md
0	8	docs/source/_static/custom.css
0	39	docs/source/_templates/sections/header.html
0	133	docs/source/api/summary.md
0	21	docs/source/autodoc2_docstring_parser.py
0	3	docs/source/community/blog.md
0	263	docs/source/conf.py
0	27	docs/source/contributing/model/index.md
0	834	docs/source/contributing/model/multimodal.md
0	133	docs/source/deployment/docker.md
0	250	docs/source/deployment/frameworks/helm.md
0	22	docs/source/deployment/frameworks/index.md
0	11	docs/source/deployment/integrations/index.md
0	476	docs/source/features/compatibility_matrix.md
0	24	docs/source/features/quantization/index.md
0	153	docs/source/features/quantization/supported_hardware.md
0	244	docs/source/generate_examples.py
0	28	docs/source/getting_started/installation.md
0	299	docs/source/getting_started/installation/ai_accelerator.md
0	34	docs/source/getting_started/installation/cpu/arm.inc.md
0	41	docs/source/getting_started/installation/cpu/x86.inc.md
0	301	docs/source/getting_started/installation/gpu.md
0	217	docs/source/index.md
0	9	docs/source/models/extensions/index.md
0	1406	docs/source/models/supported_models.md
0	36	docs/source/serving/engine_args.md
0	15	docs/source/serving/env_vars.md
0	8	docs/source/serving/integrations/index.md
0	47	docs/source/serving/serve_args.md
0	0	docs/{source => }/training/rlhf.md
4	5	docs/{source => }/training/trl.md
117	0	mkdocs.yaml
2	0	pyproject.toml
8	19	requirements/docs.txt
4	6	vllm/engine/llm_engine.py
2	2	vllm/engine/metrics.py
14	18	vllm/entrypoints/llm.py
32	32	vllm/entrypoints/openai/protocol.py
2	2	vllm/envs.py
3	3	vllm/executor/ray_distributed_executor.py
2	3	vllm/model_executor/models/blip2.py
2	3	vllm/model_executor/models/llava.py
2	3	vllm/model_executor/models/llava_next.py
2	3	vllm/model_executor/models/mistral3.py
2	3	vllm/multimodal/__init__.py
9	13	vllm/multimodal/inputs.py
4	6	vllm/multimodal/registry.py
2	3	vllm/utils.py
3	4	vllm/v1/worker/gpu_worker.py
3	4	vllm/worker/hpu_worker.py
3	4	vllm/worker/worker.py
3	4	vllm/worker/xpu_worker.py

[d0bc2f810] Yuqi Zhang 2025-05-23 [Bugfix] Add half type support in reshape_and_cache_cpu_impl on x86 cpu platform (#18430)
1	0	csrc/cpu/cpu_types_x86.hpp

[b046cf792] Chauncey 2025-05-23 [Feature][V1]: suupports cached_tokens in response usage (#18149)
10	1	tests/v1/core/test_scheduler_e2e.py
4	1	vllm/v1/core/sched/scheduler.py
3	0	vllm/v1/engine/__init__.py
6	3	vllm/v1/engine/output_processor.py
4	0	vllm/v1/request.py

[54af91594] Michael Goin 2025-05-23 [Doc] Update quickstart and install for cu128 using `--torch-backend=auto` (#18505)
24	16	docs/source/getting_started/installation/gpu/cuda.inc.md
1	14	docs/source/getting_started/installation/python_env_setup.inc.md
8	5	docs/source/getting_started/quickstart.md

[71ea614d4] cascade 2025-05-23 [Feature]Add async tensor parallelism using compilation pass (#17882)
1	0	.buildkite/test-pipeline.yaml
18	0	tests/compile/backend.py
248	0	tests/compile/test_async_tp.py
17	19	tests/compile/test_fusion.py
18	29	tests/compile/test_sequence_parallelism.py
126	0	vllm/compilation/collective_fusion.py
3	0	vllm/compilation/pass_manager.py
5	4	vllm/compilation/sequence_parallelism.py
2	1	vllm/compilation/vllm_inductor_pass.py
10	1	vllm/config.py
24	2	vllm/distributed/parallel_state.py

[4c611348a] RonaldBXu 2025-05-23 [V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal (#18034)
1	2	vllm/transformers_utils/configs/eagle.py
5	1	vllm/v1/spec_decode/eagle.py

[60cad94b8] Ning Xie 2025-05-23 [Hardware] correct method signatures for HPU,ROCm,XPU (#18551)
4	6	vllm/platforms/__init__.py
2	2	vllm/platforms/hpu.py
2	2	vllm/platforms/rocm.py
8	6	vllm/platforms/xpu.py

[9c1baa5bc] Shanshan Shen 2025-05-23 [Misc] Replace `cuda` hard code with `current_platform` (#16983)
3	2	vllm/distributed/parallel_state.py
4	1	vllm/forward_context.py
4	4	vllm/spec_decode/metrics.py

[4be2255c8] Teruaki Ishizaki 2025-05-23 [Bugfix][Benchmarks] Fix a benchmark of deepspeed-mii backend to use api_key (#17291)
8	1	benchmarks/backend_request_func.py

[ed5d40825] aws-elaineyz 2025-05-22 [Neuron] Remove bypass on EAGLEConfig and add a test (#18514)
8	1	.buildkite/scripts/hardware_ci/run-neuron-test.sh
82	0	tests/neuron/2_core/test_eagle.py
4	2	tests/neuron/2_core/test_mistral.py
1	2	vllm/config.py

[583507d13] Benjamin Chislett 2025-05-22 [Spec Decode] Make EAGLE3 draft token ID mapping optional (#18488)
9	1	vllm/model_executor/models/llama_eagle3.py

[e44d8ce8c] lkchen 2025-05-22 [Bugfix] Set `KVTransferConfig.engine_id` in post_init (#18576)
8	0	tests/v1/kv_connector/unit/test_multi_connector.py
4	1	vllm/config.py
1	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[93ecb8139] Nick Hill 2025-05-22 [BugFix] Increase TP execute_model timeout (#18558)
1	1	vllm/v1/executor/multiproc_executor.py

[fae453f8c] CYJiang 2025-05-23 [Misc] refactor: simplify input validation and num_requests handling in _convert_v1_inputs (#18482)
10	12	vllm/entrypoints/llm.py

[4b0da7b60] Harry Mellor 2025-05-23 Enable hybrid attention models for Transformers backend (#18494)
1	1	docs/source/contributing/model/basic.md
42	14	tests/models/test_transformers.py
11	8	vllm/config.py
51	6	vllm/model_executor/models/transformers.py

[c6b636f9f] Mark McLoughlin 2025-05-23 [V1][Spec Decoding] Use model_loader.get_model() to load models (#18273)
6	52	tests/v1/spec_decode/test_eagle.py
10	3	vllm/model_executor/model_loader/__init__.py
2	1	vllm/model_executor/model_loader/base_loader.py
2	3	vllm/model_executor/model_loader/bitsandbytes_loader.py
4	3	vllm/model_executor/model_loader/default_loader.py
2	2	vllm/model_executor/model_loader/dummy_loader.py
2	2	vllm/model_executor/model_loader/gguf_loader.py
2	3	vllm/model_executor/model_loader/runai_streamer_loader.py
2	2	vllm/model_executor/model_loader/sharded_state_loader.py
2	2	vllm/model_executor/model_loader/tensorizer_loader.py
3	1	vllm/model_executor/model_loader/utils.py
4	2	vllm/model_executor/models/llama_eagle.py
6	5	vllm/model_executor/models/llama_eagle3.py
1	4	vllm/model_executor/models/medusa.py
6	32	vllm/v1/spec_decode/eagle.py
5	18	vllm/v1/spec_decode/medusa.py

[04eb88dc8] Chenheli Hua 2025-05-22 Re-submit: Fix: Proper RGBA -> RGB conversion for PIL images. (#18569)
2	1	benchmarks/benchmark_dataset.py
3	1	examples/offline_inference/qwen2_5_omni/only_thinker.py
3	2	examples/offline_inference/vision_language.py
4	2	tests/models/multimodal/generation/test_interleaved.py
2	2	tests/models/multimodal/generation/test_phi4mm.py
2	1	tests/models/test_oot_registration.py
-	-	tests/multimodal/assets/rgba.png
36	0	tests/multimodal/test_image.py
2	1	tests/multimodal/test_utils.py
2	2	vllm/benchmarks/datasets.py
2	1	vllm/model_executor/models/internvl.py
2	1	vllm/model_executor/models/skyworkr1v.py
3	1	vllm/multimodal/hasher.py
22	3	vllm/multimodal/image.py
4	2	vllm/transformers_utils/processors/ovis.py

[46791e1b4] rasmith 2025-05-22 [AMD] [P/D] Compute num gpus for ROCm correctly in run_accuracy_test.sh (#18568)
11	2	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh

[c32e249a2] Sanger Steel 2025-05-22 [Frontend] [Core] Add Tensorizer support for V1, LoRA adapter serialization and deserialization (#17926)
1	0	.buildkite/test-pipeline.yaml
82	14	examples/other/tensorize_vllm_model.py
97	0	tests/entrypoints/openai/test_tensorizer_entrypoint.py
108	12	tests/lora/test_llama_tp.py
0	8	tests/tensorizer_loader/conftest.py
47	115	tests/tensorizer_loader/test_tensorizer.py
1	2	vllm/engine/arg_utils.py
44	29	vllm/lora/models.py
24	4	vllm/lora/peft_helper.py
1	0	vllm/lora/request.py
3	1	vllm/lora/worker_manager.py
170	12	vllm/model_executor/model_loader/tensorizer.py
4	1	vllm/model_executor/model_loader/tensorizer_loader.py
7	0	vllm/v1/engine/core.py
11	1	vllm/v1/worker/gpu_model_runner.py
8	0	vllm/v1/worker/gpu_worker.py

[c91fe7b1b] Kai Wu 2025-05-22 [Frontend][Bug Fix] Update llama4 pythonic jinja template and llama4_pythonic parser (#17917)
5	6	docs/source/features/tool_calling.md
36	64	examples/tool_chat_template_llama4_pythonic.jinja
193	0	tests/entrypoints/openai/tool_parsers/test_llama4_pythonic_tool_parser.py
1	1	tests/tool_use/utils.py
3	1	vllm/entrypoints/openai/tool_parsers/__init__.py
303	0	vllm/entrypoints/openai/tool_parsers/llama4_pythonic_tool_parser.py

[a04720bc3] Ekagra Ranjan 2025-05-22 [V1][Spec Decode][Bugfix] Load quantize weights for EAGLE (#18290)
4	2	vllm/transformers_utils/configs/eagle.py
5	1	vllm/v1/spec_decode/eagle.py

[7b9d832c8] lkchen 2025-05-22 [Tool] Add NIXL installation script (#18172)
109	0	tools/install_nixl.sh

[6e588da0f] Tyler Michael Smith 2025-05-22 [Build/CI] Fix CUDA 11.8 build (#17679)
5	1	CMakeLists.txt
3	1	csrc/moe/moe_ops.h
42	1	csrc/moe/moe_permute_unpermute_op.cu
6	4	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
3	1	csrc/moe/torch_bindings.cpp
1	1	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
11	5	docker/Dockerfile
3	1	tests/kernels/moe/test_moe_permute_unpermute.py
4	0	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py

[f8d2cc5f5] Mengqing Cao 2025-05-23 [Compile][Platform] Make PiecewiseBackend pluggable and extendable (#18076)
6	200	vllm/compilation/backends.py
71	0	vllm/compilation/base_piecewise_backend.py
213	0	vllm/compilation/cuda_piecewise_backend.py
4	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py

[721fb9b18] wangxiyuan 2025-05-23 [Platform] Move platform check to right place (#18470)
10	25	vllm/config.py
11	0	vllm/platforms/cpu.py
11	0	vllm/platforms/hpu.py
11	0	vllm/platforms/neuron.py
11	0	vllm/platforms/tpu.py
11	0	vllm/platforms/xpu.py
6	0	vllm/utils.py

[1f3a1200e] David Xia 2025-05-22 [Bugfix] make `test_openai_schema.py` pass (#18224)
1	1	.buildkite/test-pipeline.yaml
56	1	tests/entrypoints/openai/test_openai_schema.py

[54631f826] Lukas Geiger 2025-05-22 [Misc] Call `ndarray.tobytes()` directly instead of `ndarray.data.tobytes()` (#18347)
1	1	vllm/multimodal/hasher.py

[cb506ecb5] Reid 2025-05-22 [Misc] improve Automatic Prefix Caching example (#18554)
1	75	docs/source/features/automatic_prefix_caching.md
98	0	examples/offline_inference/automatic_prefix_caching.py

[93f71673c] Li, Jiang 2025-05-22 [BugFix][CPU] Fix x86 SHM distributed module initialization (#18536)
6	2	vllm/distributed/device_communicators/cpu_communicator.py

[3f505233f] Calvin Chen 2025-05-22 [Doc] Add stream flag for chat completion example (#18524)
21	3	examples/online_serving/openai_chat_completion_client.py

[4e04eceb5] Bowen Wang 2025-05-22 [Bugfix] Use random hidden states in dummy sampler run (#18543)
4	0	vllm/v1/worker/gpu_model_runner.py

[71075029f] CYJiang 2025-05-22 [Doc] Support --stream arg in openai_completion_client.py script (#18388)
5	2	examples/online_serving/openai_chat_completion_structured_outputs.py
5	2	examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py
15	5	examples/online_serving/openai_completion_client.py

[ca86a7cf6] Harry Mellor 2025-05-22 [CI/Build] Update bamba test model location (#18544)
1	1	tests/models/language/generation/test_hybrid.py
1	1	tests/models/registry.py
1	1	tests/v1/test_oracle.py

[a35a49474] lkchen 2025-05-22 [Bugfix] Add kwargs to RequestOutput __init__ to be forward compatible (#18513)
1	0	.buildkite/test-pipeline.yaml
14	0	tests/test_outputs.py
9	0	vllm/outputs.py

[f6037d190] 燃 2025-05-22 [Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text (#18526)
4	7	vllm/worker/model_runner.py

[fa72f9a81] aws-elaineyz 2025-05-22 Order sequence ids + config update to support specifying custom quantization layers (#18279)
35	5	tests/neuron/2_core/test_mistral.py
38	5	vllm/model_executor/model_loader/neuronx_distributed.py

[ebed81fbf] aws-elaineyz 2025-05-22 Update default neuron config for speculation (#18274)
3	1	vllm/model_executor/model_loader/neuronx_distributed.py

[e2d7d3124] Satyajith Chilappagari 2025-05-22 [Neuron] Update Dockerfile.neuron to use latest neuron release (2.23) (#18512)
4	3	docker/Dockerfile.neuron

[23b67b37b] Cyrus Leung 2025-05-22 [Doc] Fix invalid JSON in example args (#18527)
7	3	docs/source/design/v1/torch_compile.md

[db5a29ba1] Jee Jee Li 2025-05-22 [Bugfix] Fix LoRA test (#18518)
1	1	tests/lora/test_lora_functions.py
72	64	tests/v1/sample/test_topk_topp_sampler.py

[51797775c] Shane A 2025-05-21 [Bugfix][Model] Make Olmo2Model weight loading return loaded weights (#18504)
5	1	vllm/model_executor/models/olmo2.py

[cf5984b2f] Nick Hill 2025-05-21 [BugFix][DP] Send DP wave completion only from `dp_rank==0` (#18502)
2	2	vllm/v1/engine/core.py

[d022115cc] youngrok cha 2025-05-22 [Bugfix] Inconsistent token calculation compared to HF in llava family (#18479)
4	2	vllm/model_executor/models/llava_next.py
4	2	vllm/model_executor/models/llava_onevision.py

[acb54ca8e] Rabi Mishra 2025-05-22 Intialize io_thread_pool attribute in the beginning. (#18331)
1	1	vllm/v1/executor/multiproc_executor.py

[6e0fd34d3] Russell Bryant 2025-05-21 [CI] Fix race condition with StatelessProcessGroup.barrier (#18506)
5	5	tests/distributed/test_shm_broadcast.py
1	17	vllm/distributed/device_communicators/shm_broadcast.py
151	3	vllm/distributed/utils.py

[176d62e4e] Ning Xie 2025-05-22 [MISC] update project urls in pyproject.toml (#18519)
2	2	pyproject.toml

[20bd6f4d2] Dhia Eddine Rhaiem 2025-05-22 [FalconH1] Fix output dtype in RMSNorm fallback path for Falcon-H1 (e.g. 0.5B) (#18500)
5	3	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	1	vllm/model_executor/models/falcon_h1.py

[1f079540d] Sebastian Schoennenbeck 2025-05-21 [Bugfix] Consistent ascii handling in tool parsers (#17704)
8	4	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
8	4	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
8	4	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
10	6	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
8	4	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
5	4	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
6	3	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py

[94d8ec8d2] vllmellm 2025-05-22 [FEAT][ROCm] Upgrade AITER MLA v1 backend (#18338)
1	1	docker/Dockerfile.rocm_base
30	6	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[bb0a31121] Mark McLoughlin 2025-05-21 Revert "[v1] Support multiple KV cache groups in GPU model runner (#17945) (#18459)
3	68	tests/v1/core/test_kv_cache_utils.py
18	18	tests/v1/core/test_prefix_caching.py
6	33	tests/v1/worker/test_gpu_input_batch.py
16	41	tests/v1/worker/test_gpu_model_runner.py
3	3	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	2	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
13	21	vllm/v1/core/kv_cache_manager.py
7	6	vllm/v1/core/kv_cache_utils.py
6	6	vllm/v1/core/sched/output.py
6	10	vllm/v1/core/sched/scheduler.py
0	42	vllm/v1/kv_cache_interface.py
0	47	vllm/v1/worker/block_table.py
6	7	vllm/v1/worker/gpu_input_batch.py
112	158	vllm/v1/worker/gpu_model_runner.py
16	19	vllm/v1/worker/tpu_model_runner.py

[dd5fa7e04] Hosang 2025-05-21 [ROCm][Kernel][V1] Enable AMD Radeon GPU Custom Paged Attention on v1 (#17004)
5	1	benchmarks/kernels/benchmark_paged_attention.py
1880	171	csrc/rocm/attention.cu
7	1	tests/kernels/attention/test_attention.py
2	1	vllm/attention/backends/rocm_flash_attn.py
2	1	vllm/attention/ops/chunked_prefill_paged_decode.py
34	14	vllm/platforms/rocm.py

[2b1610455] Hyogeun Oh (오효근) 2025-05-21 [Misc] Update deprecation message for `--enable-reasoning` (#18404)
1	1	vllm/engine/arg_utils.py

[371376f99] Kebe 2025-05-21 [Build] fix Dockerfile shell (#18402)
2	0	docker/Dockerfile

[c6c10ca92] bnellnm 2025-05-21 [Bugfix] Reduce moe_sum test size to avoid OOM (#18484)
1	1	tests/kernels/moe/test_moe.py

[c154d8930] GiantCroc 2025-05-21 [Doc] fix arg docstring in linear layers (#18410)
9	0	vllm/model_executor/layers/linear.py

[eca18691d] Dhia Eddine Rhaiem 2025-05-21 [MODEL] FalconH1 (#18406)
5	0	docs/source/models/supported_models.md
3	0	tests/models/registry.py
104	59	vllm/model_executor/layers/mamba/mamba_mixer2.py
685	0	vllm/model_executor/models/falcon_h1.py
1	0	vllm/model_executor/models/registry.py

[61acfc45b] Rabi Mishra 2025-05-21 [Bugfix][Failing Test] Fix test_events.py (#18460)
2	0	.buildkite/test-pipeline.yaml
4	5	tests/distributed/test_events.py

[107f5fc4c] Reid 2025-05-21 [Misc] refactor disaggregated-prefill-v1 example (#18474)
1	0	examples/offline_inference/disaggregated-prefill-v1/README.md
44	32	examples/offline_inference/disaggregated-prefill-v1/decode_example.py
51	39	examples/offline_inference/disaggregated-prefill-v1/prefill_example.py

[907f935de] Yong Hoon Shin 2025-05-21 [V1] Fix general plugins not loaded in engine for multiproc (#18326)
4	0	vllm/v1/engine/core.py

[5d7f54520] Kebe 2025-05-21 [Frontend] deprecate `--device` arg (#18399)
0	1	docs/source/getting_started/installation/gpu/xpu.inc.md
5	1	vllm/config.py
4	2	vllm/engine/arg_utils.py

[cd8dfc6df] Nicolò Lucchesi 2025-05-21 [Misc] MultiConnector._connectors type (#18423)
1	1	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[d06dd72ba] wwl2755 2025-05-21 [Bugfix][Failing Test] Fix nixl connector test when promt size < block size (#18429)
9	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[ad0012a0a] Cyrus Leung 2025-05-21 Revert "[Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text (#18407)" (#18456)
8	0	vllm/worker/model_runner.py

[92247c522] bnellnm 2025-05-21 [Bug] Fix moe_sum signature (#18440)
1	1	csrc/moe/torch_bindings.cpp
18	0	tests/kernels/moe/test_moe.py

[0c15c2e48] Gregory Shtrasberg 2025-05-21 [Bugfix] config.head_dim is now explicitly set to None (#18432)
3	2	vllm/distributed/kv_transfer/kv_connector/utils.py
3	2	vllm/model_executor/models/exaone.py
3	2	vllm/model_executor/models/granite.py
6	4	vllm/model_executor/models/minimax_text_01.py
3	2	vllm/model_executor/models/mixtral.py
3	2	vllm/model_executor/models/mixtral_quant.py
3	2	vllm/model_executor/models/nemotron.py
3	2	vllm/model_executor/models/solar.py

[3b17ea26e] Michael Goin 2025-05-20 [TPU] Re-enable the Pallas MoE kernel (#18025)
5	5	requirements/tpu.txt
1	2	vllm/model_executor/layers/fused_moe/layer.py
18	2	vllm/model_executor/layers/fused_moe/moe_pallas.py

[23baa2180] Dilip Gowda Bhagavan 2025-05-21 fix:Build torch wheel inline rather than picking from nightly (#18351)
29	3	docker/Dockerfile.s390x
0	1	requirements/cpu.txt

[980a17247] Percy 2025-05-20 [Kernel] update comment for KV shape in unified triton attn (#18099)
2	2	vllm/attention/ops/triton_unified_attention.py

[e1f5a71ed] Calvin Chen 2025-05-21 [Model] use AutoWeightsLoader for bloom (#18300)
46	33	vllm/model_executor/models/bloom.py

[f4a8a3746] Michael Goin 2025-05-20 [Minor] Rename quantization nvfp4 to modelopt_fp4 (#18356)
3	3	tests/models/quantization/test_nvfp4.py
1	1	vllm/config.py
2	2	vllm/model_executor/layers/quantization/__init__.py
1	1	vllm/model_executor/layers/quantization/modelopt.py

[8f55962a7] Reid 2025-05-20 [Misc] refactor prompt embedding examples (#18405)
2	102	docs/source/features/prompt_embeds.md
103	0	examples/offline_inference/prompt_embed_inference.py
86	0	examples/online_serving/prompt_embed_inference_with_openai_client.py

[be48360c1] 燃 2025-05-20 [Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text (#18407)
0	8	vllm/worker/model_runner.py

[86847700d] wang.yuqi 2025-05-20 [CI] Add mteb testing to test the accuracy of the embedding model (#17175)
1	0	requirements/test.in
21	1	requirements/test.txt
42	0	tests/entrypoints/openai/correctness/test_mteb.py
0	2	tests/models/language/pooling/test_gte.py
0	1	tests/models/language/pooling/test_nomic.py
0	1	tests/models/language/pooling/test_snowflake_arctic_embed.py

[d6c86d09a] 汪志鹏 2025-05-20 Update cpu.txt (#18398)
2	0	requirements/cpu.txt

[6b35cb10a] Jee Jee Li 2025-05-20 [Misc] Add LoRA code owner (#18387)
2	0	.github/CODEOWNERS
0	6	tests/quantization/test_bitsandbytes.py

[1b1e8e05f] Reid 2025-05-20 [doc] update env variable export (#18391)
5	0	docs/source/getting_started/quickstart.md

[bca55b556] Random Fly 2025-05-20 [Bugfix] fix adding bias twice in ipex GPTQ quantization (#18363)
0	2	vllm/model_executor/layers/quantization/ipex_quant.py

[d98139677] Kevin H. Luu 2025-05-19 [release] Change dockerhub username for TPU release (#18389)
1	1	.buildkite/release-pipeline.yaml

[9609327fa] Nan Qin 2025-05-19 [Core] [Bugfix]: tensor parallel with prompt embeds (#18171)
65	10	tests/basic_correctness/test_basic_correctness.py
9	0	tests/conftest.py
7	7	vllm/sequence.py
55	45	vllm/worker/model_runner.py

[f07a673eb] Isotr0py 2025-05-20 [Misc] Allow `AutoWeightsLoader` to skip loading weights with specific substr in name (#18358)
70	0	tests/models/test_utils.py
6	10	vllm/model_executor/models/granite.py
6	4	vllm/model_executor/models/grok1.py
1	1	vllm/model_executor/models/mixtral.py
1	4	vllm/model_executor/models/mixtral_quant.py
1	10	vllm/model_executor/models/nemotron.py
2	14	vllm/model_executor/models/olmo.py
2	14	vllm/model_executor/models/olmo2.py
1	4	vllm/model_executor/models/olmoe.py
1	10	vllm/model_executor/models/orion.py
1	3	vllm/model_executor/models/phi4mm.py
1	4	vllm/model_executor/models/phimoe.py
1	4	vllm/model_executor/models/qwen2_moe.py
1	4	vllm/model_executor/models/qwen3_moe.py
1	10	vllm/model_executor/models/solar.py
1	9	vllm/model_executor/models/stablelm.py
2	3	vllm/model_executor/models/starcoder2.py
17	1	vllm/model_executor/models/utils.py

[d565e0976] Liangfu Chen 2025-05-19 [neuron] fix authorization issue (#18364)
3	1	.buildkite/scripts/hardware_ci/run-neuron-test.sh

[258bf621d] Lucia Fang 2025-05-19 fix CUDA_check redefinition in #17918 (#18287)
0	9	csrc/cutlass_extensions/common.hpp
5	3	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh

[dc1440cf9] Satyajith Chilappagari 2025-05-19 Neuron up mistral (#18222)
32	0	tests/neuron/2_core/test_mistral.py
3	0	vllm/model_executor/model_loader/neuronx_distributed.py
1	2	vllm/platforms/neuron.py

[817122183] Gong Shufan 2025-05-20 [Misc] Fix typo (#18330)
1	1	examples/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh

[7937c2fd5] sunyicode0012 2025-05-20 Add files via uploadAdd fused MoE kernel tuning configs (fp8_w8a8) for DeepSeek V3/R1 on a single-node 8x NVIDIA H20 96GB setup (#18337)
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json

[e2ee1e8e9] Wenhua Cheng 2025-05-20 [Feature]Add support for models quantized with AutoRound (#17850)
30	0	tests/quantization/test_auto_round.py
3	0	vllm/model_executor/layers/quantization/__init__.py
306	0	vllm/model_executor/layers/quantization/auto_round.py

[20d8ce81e] Reid 2025-05-20 [Frontend] add --quick option for vllm chat/complete (#18297)
29	0	vllm/entrypoints/cli/openai.py

[84ab4feb7] Elad Segal 2025-05-19 [Doc] Fix typo (#18355)
1	1	docs/source/models/supported_models.md

[6781af560] Jee Jee Li 2025-05-20 [Quantization] Pool model support bitsandbytes (#18087)
64	2	tests/quantization/test_bitsandbytes.py
15	1	vllm/model_executor/model_loader/bitsandbytes_loader.py

[1b15df254] Nick Hill 2025-05-19 [BugFix] Fix handling of num_computed_tokens with connector (#18232)
11	5	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
18	11	vllm/v1/core/sched/scheduler.py

[43b5f61dc] Cyrus Leung 2025-05-19 [Doc] Move input-related docs to Features (#18353)
0	0	docs/source/{serving => features}/multimodal_inputs.md
0	0	docs/source/{serving => features}/prompt_embeds.md
2	2	docs/source/index.md

[c5bb0ebdc] Li Wang 2025-05-19 [Doc] Fix prompt embedding examples (#18350)
2	0	docs/source/serving/prompt_embeds.md

[d637b9609] Shaoyu Yang 2025-05-19 [BugFix] [Vul] Add missing `usedforsecurity=False` in MD5 hashing to enable FIPS (#18319)
6	6	vllm/model_executor/model_loader/neuronx_distributed.py

[275c5daeb] CYJiang 2025-05-19 fix: Add type specifications for CLI arguments in tensorizer options (#18314)
5	0	vllm/model_executor/model_loader/tensorizer.py

[47fda6d08] Simon Mo 2025-05-18 [Build] Supports CUDA 12.6 and 11.8 after Blackwell Update (#18316)
2	2	.buildkite/release-pipeline.yaml
7	2	docker/Dockerfile

[27d095260] Reid 2025-05-19 [Misc] extract parser.parse_args() (#18323)
6	2	examples/offline_inference/qwen2_5_omni/only_thinker.py
6	2	examples/online_serving/disaggregated_serving/disagg_proxy_demo.py

[221cfc2fe] Nan Qin 2025-05-18 Feature/vllm/input embedding completion api (#17590)
1	0	docs/source/index.md
142	0	docs/source/serving/prompt_embeds.md
257	0	tests/entrypoints/openai/test_completion_with_prompt_embeds.py
7	2	vllm/entrypoints/logger.py
3	0	vllm/entrypoints/openai/cli_args.py
10	1	vllm/entrypoints/openai/protocol.py
39	6	vllm/entrypoints/openai/serving_completion.py
163	28	vllm/entrypoints/openai/serving_engine.py
3	2	vllm/entrypoints/openai/serving_tokenization.py
12	1	vllm/inputs/data.py

[9da1095da] wwl2755 2025-05-18 [Spec Decode][V0] Fix spec decode correctness test in V0 eagle/medusa (#18175)
0	2	tests/spec_decode/e2e/test_eagle_correctness.py
11	0	vllm/model_executor/models/eagle.py
8	1	vllm/model_executor/models/medusa.py
2	0	vllm/sequence.py

[d1211f879] Robin 2025-05-19 [Doc] Add doc to explain the usage of Qwen3 thinking (#18291)
5	0	docs/source/features/reasoning_outputs.md

[b6a6e7a52] Reid 2025-05-18 [Misc] add litellm integration (#18320)
1	0	docs/source/deployment/frameworks/index.md
75	0	docs/source/deployment/frameworks/litellm.md

[4fb349f66] Lifu Huang 2025-05-18 Fix copy-paste error in phi4mm image processing (#18315)
7	19	vllm/model_executor/models/phi4mm.py

[908733aca] 22quinn 2025-05-18 [Model] Use sigmoid for single-label classification (#18313)
9	2	vllm/model_executor/layers/pooler.py

[1a8f68bb9] Reid 2025-05-18 [doc] update reasoning doc (#18306)
3	4	docs/source/features/reasoning_outputs.md

[9ab2c02ff] cascade 2025-05-17 Support sequence parallelism combined with pipeline parallelism (#18243)
35	2	tests/distributed/test_sequence_parallel.py
0	12	vllm/config.py
39	13	vllm/v1/worker/gpu_model_runner.py

[66e63e86e] Ning Xie 2025-05-18 [MISC] fix typo (#18305)
2	2	vllm/usage/usage_lib.py

[9214e6063] rongfu.leng 2025-05-17 [Model] use AutoWeightsLoader for solar (#18113)
53	47	vllm/model_executor/models/mixtral_quant.py
68	60	vllm/model_executor/models/nemotron.py
56	47	vllm/model_executor/models/olmo.py
52	44	vllm/model_executor/models/olmo2.py
70	62	vllm/model_executor/models/solar.py

[f880d4258] Nishidha 2025-05-17 Fixed build on ppc64le due to openssl conflicts (#18262)
2	6	docker/Dockerfile.ppc64le

[dcfe95234] Michael Goin 2025-05-17 Update Dockerfile to build for Blackwell (#18095)
3	3	docker/Dockerfile

[48ac2bed5] Siyuan Liu 2025-05-17 [Hardware][TPU] Optionally import for TPU backend (#18269)
9	0	vllm/distributed/device_communicators/tpu_communicator.py
8	0	vllm/platforms/tpu.py
8	0	vllm/v1/worker/tpu_worker.py

[3e0d43502] David Ben-David 2025-05-17 [P/D][V1] Support dynamic loading of external KV connector implementations (#18142)
4	0	vllm/config.py
11	2	vllm/distributed/kv_transfer/kv_connector/factory.py

[4ee4826ed] 汪志鹏 2025-05-16 [BugFix] Correct max_model_len derivation from config.json for Mistral format  (#17937)
18	3	vllm/transformers_utils/config.py

[60017dc84] Reid 2025-05-17 [Misc] reformat the collect-env output (#18285)
54	31	vllm/collect_env.py

[55f1a468d] Trevor Royer 2025-05-16 Move cli args docs to its own page (#18228) (#18264)
1	0	docs/source/index.md
2	0	docs/source/serving/engine_args.md
1	49	docs/source/serving/openai_compatible_server.md
47	0	docs/source/serving/serve_args.md

[fd195b194] Michael Goin 2025-05-16 [V1][P/D] Local attention optimization for NIXL (#18170)
90	11	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[fabe89bbc] Woosuk Kwon 2025-05-16 [Spec Decode] Don't fall back to V0 when spec decoding is enabled (#18265)
1	9	vllm/engine/arg_utils.py

[e73b7dfd6] Jinzhen Lin 2025-05-17 [Bugfix] fix `an illegal memory access was encountered` of marlin kernel + act_order  (#18245)
14	11	csrc/moe/marlin_moe_wna16/marlin_template.h
14	10	csrc/quantization/gptq_marlin/marlin_template.h
1	1	tests/weight_loading/models.txt

[7fdfa0153] Bowen Wang 2025-05-16 [Sampler] Adapt to FlashInfer 0.2.3 sampler API (#15777)
2	1	docker/Dockerfile
12	2	tests/samplers/test_rejection_sampler.py
2	0	tests/samplers/test_sampler.py
71	1	tests/v1/sample/test_topk_topp_sampler.py
7	6	vllm/model_executor/layers/rejection_sampler.py
13	39	vllm/model_executor/layers/sampler.py
16	40	vllm/v1/sample/ops/topk_topp_sampler.py

[aef94c6d0] Sanger Steel 2025-05-16 [CI] Assign reviewer to mergify with changes to Tensorizer files (#18278)
11	0	.github/mergify.yml

[0ceaebf87] Nick Hill 2025-05-16 [BugFix] Fix ordering of KVConnector finished send/rcv sets (#18211)
2	1	vllm/distributed/kv_transfer/kv_connector/v1/base.py
3	3	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[1db4f47f8] Nick Hill 2025-05-16 [BugFix] Fix multi async save in MultiConnector (#18246)
17	6	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[d3d91b6f7] Reid 2025-05-16 [Misc][MacOS] fix bfloat16 error (#18249)
1	1	vllm/platforms/cpu.py

[87d871470] learner0810 2025-05-16 [Model] Use autoweightloader for dbrx (#18251)
53	47	vllm/model_executor/models/dbrx.py

[a5f8c111c] fxmarty-amd 2025-05-16 [Fix] Fix typo in `resolve_hf_chat_template` (#18259)
2	2	vllm/entrypoints/chat_utils.py

[e23564cb7] Lain 2025-05-16 use ceil_div in cutlass block scaling shape check (#17918)
10	2	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
39	21	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_helper.hpp
13	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[390ec8890] Isotr0py 2025-05-16 [Misc] Consolidate Audio tests into multimodal common generation tests (#18214)
60	4	tests/models/multimodal/generation/test_common.py
3	109	tests/models/multimodal/generation/test_ultravox.py
96	37	tests/models/multimodal/generation/vlm_utils/builders.py
4	4	tests/models/multimodal/generation/vlm_utils/case_filtering.py
18	11	tests/models/multimodal/generation/vlm_utils/core.py
42	34	tests/models/multimodal/generation/vlm_utils/custom_inputs.py
12	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
27	10	tests/models/multimodal/generation/vlm_utils/runners.py
20	6	tests/models/multimodal/generation/vlm_utils/types.py

[541817670] Seiji Eicher 2025-05-16 [Misc] Add Ray Prometheus logger to V1 (#17925)
57	0	tests/v1/metrics/test_ray_metrics.py
29	25	vllm/v1/metrics/loggers.py
120	0	vllm/v1/metrics/ray_wrappers.py
17	10	vllm/v1/spec_decode/metrics.py

[67da5720d] Vadim Gimpelson 2025-05-16 [PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding (#17973)
121	83	vllm/model_executor/models/qwen2_5_vl.py

[5c04bb8b8] David Xia 2025-05-16 [doc] fix multimodal example script (#18089)
15	11	examples/online_serving/openai_chat_completion_client_for_multimodal.py
25	0	examples/online_serving/utils.py

[3d2779c29] Lucia Fang 2025-05-15 [Feature] Support Pipeline Parallism in torchrun SPMD offline inference for V1 (#17827)
2	0	.buildkite/test-pipeline.yaml
14	9	examples/offline_inference/torchrun_example.py
2	1	tests/distributed/test_torchrun_example.py
0	1	vllm/config.py
4	2	vllm/distributed/device_communicators/custom_all_reduce.py
3	2	vllm/engine/arg_utils.py
0	3	vllm/executor/uniproc_executor.py
27	6	vllm/v1/worker/gpu_model_runner.py
3	3	vllm/v1/worker/gpu_worker.py

[6b31c84af] Will Eaton 2025-05-16 Throw better error for when running into k8s service discovery issue (#18209)
35	0	tests/test_vllm_port.py
34	3	vllm/envs.py

[b18201fe0] Harry Mellor 2025-05-16 Allow users to pass arbitrary JSON keys from CLI (#18208)
2	2	tests/engine/test_arg_utils.py
25	0	tests/test_utils.py
5	1	vllm/engine/arg_utils.py
46	0	vllm/utils.py

[f4937a51c] Sky Lee 2025-05-16 [Model] vLLM v1 supports Medusa (#17956)
4	1	vllm/engine/arg_utils.py
4	1	vllm/model_executor/models/medusa.py
74	0	vllm/v1/spec_decode/medusa.py
26	0	vllm/v1/worker/gpu_model_runner.py

[ee659e3b6] kliuae 2025-05-16 [Bugfix][ROCm] Use `chunked_prefill_paged_decode` as fallback for V1 attention on ROCm (#18093)
77	32	vllm/v1/attention/backends/triton_attn.py

[4e1c6a026] Lucas Wilkinson 2025-05-15 [Bugfix] fix rotary embedding test for _get_padded_tensor_shape (#18229)
4	0	tests/kernels/core/test_pos_encoding.py

[c7852a6d9] Lucas Wilkinson 2025-05-15 [Build] Allow shipping PTX on a per-file basis (#18155)
6	3	CMakeLists.txt
69	20	cmake/utils.cmake

[8795eb997] Lucia Fang 2025-05-15 [Bugfix] Fix test_eagle test (#18223)
8	2	tests/v1/spec_decode/test_eagle.py

[0b3459301] Alexei-V-Ivanov-AMD 2025-05-15 Adding "AMD: Tensorizer Test" to amdproduction. (#18216)
1	1	.buildkite/test-pipeline.yaml

[e3f3aee6f] Nicolò Lucchesi 2025-05-15 [Misc] Avoid cuda graph log when sizes still match (#18202)
6	5	vllm/config.py

[92540529c] TJian 2025-05-16 [Bugfix] [ROCm]: Remove assertion logic when using AITER fused moe in unquantizedMethod to reenable LLama4 BF16 (#18205)
0	1	vllm/model_executor/layers/fused_moe/layer.py

[fadb8d5c2] Zhonghua Deng 2025-05-16 [Bugfix]Change the exception thrown by call_hf_processor from RuntimeError to ValueError (#18181)
1	1	vllm/inputs/registry.py

[2aa5470ac] Sebastian Schoennenbeck 2025-05-15 [Frontend] Fix chat template content format detection (#18190)
3	4	vllm/entrypoints/chat_utils.py

[51ff15463] Harry Mellor 2025-05-15 Improve examples rendering in docs and GitHub (#18203)
9	0	examples/offline_inference/disaggregated-prefill-v1/README.md
9	9	examples/offline_inference/{openai/openai_batch.md => openai_batch/README.md}
0	0	examples/offline_inference/{openai => openai_batch}/openai_example_batch.jsonl
8	0	examples/online_serving/disaggregated_serving/README.md
1	1	examples/online_serving/{disagg_examples => disaggregated_serving}/disagg_proxy_demo.py
0	0	examples/online_serving/{ => disaggregated_serving}/kv_events.sh
0	0	examples/online_serving/opentelemetry/{Otel.md => README.md}

[566ec04c3] Alexei-V-Ivanov-AMD 2025-05-15 Adding "Basic Models Test" and "Multi-Modal Models Test (Extended) 3" in AMD Pipeline (#18106)
8	0	.buildkite/scripts/hardware_ci/run-amd-test.sh
3	3	.buildkite/test-pipeline.yaml
6	0	requirements/rocm-test.txt
8	0	tests/models/test_transformers.py

[01c22335b] Thomas Parnell 2025-05-15 [Kernel] [V1] Fix performance regression for triton unified attention (#18161)
2	2	vllm/attention/ops/triton_unified_attention.py
16	3	vllm/v1/attention/backends/triton_attn.py

[451da4bcb] hustxiayang 2025-05-15 add tools into TokenizeChatRequest (#18187)
77	0	tests/entrypoints/openai/test_tokenization.py
4	0	vllm/entrypoints/openai/protocol.py
3	0	vllm/entrypoints/openai/serving_tokenization.py

[07ad27121] Harry Mellor 2025-05-15 Update deprecated type hinting in `model_loader` (#18130)
2	2	pyproject.toml
13	12	vllm/model_executor/model_loader/bitsandbytes_loader.py
6	5	vllm/model_executor/model_loader/default_loader.py
3	3	vllm/model_executor/model_loader/gguf_loader.py
5	5	vllm/model_executor/model_loader/neuron.py
3	3	vllm/model_executor/model_loader/neuronx_distributed.py
4	3	vllm/model_executor/model_loader/runai_streamer_loader.py
7	6	vllm/model_executor/model_loader/sharded_state_loader.py
4	3	vllm/model_executor/model_loader/tensorizer.py
2	2	vllm/model_executor/model_loader/tensorizer_loader.py
7	7	vllm/model_executor/model_loader/utils.py
24	23	vllm/model_executor/model_loader/weight_utils.py

[a9944aabf] omahs 2025-05-15 fix: typos (#18151)
2	2	csrc/attention/attention_kernels.cuh
2	2	examples/offline_inference/chat_with_tools.py
1	1	tests/lora/test_lora_huggingface.py
3	3	tests/model_executor/weight_utils.py
1	1	vllm/config.py
1	1	vllm/lora/ops/triton_ops/lora_expand_op.py
1	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
2	2	vllm/model_executor/models/granite_speech.py
4	4	vllm/model_executor/models/phi4mm_audio.py
1	1	vllm/v1/request.py

[a8f5aec20] Russell Bryant 2025-05-15 [V1] Update zmq socket creation in nixl connector (#18148)
6	1	tests/test_utils.py
10	14	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
18	0	vllm/utils.py

[de71fec81] David Xia 2025-05-15 [CI] don't skip fixed `test_kv_cache_events()` (#18183)
0	2	tests/v1/engine/test_engine_core_client.py

[70f8b9672] Mengqing Cao 2025-05-15 [Bugfix] Fix FusedMoEPrepareAndFinalize for cuda-disalike backends (#18178)
1	0	vllm/model_executor/layers/fused_moe/layer.py

[dd2a94596] inkcherry 2025-05-15 [Model] Allow the use of sliding window in Qwen2  (#17772)
8	8	vllm/model_executor/models/qwen2.py

[420caf755] Ning Xie 2025-05-15 [UT] Add ut for none hash (#17892)
31	10	tests/v1/core/test_kv_cache_utils.py

[4f07a6407] Chenheli Hua 2025-05-14 Support custom implementations of VideoLoader backends. (#18091)
41	0	tests/multimodal/test_video.py
11	0	vllm/envs.py
31	2	vllm/multimodal/video.py

[e6b8e65d2] Thomas Parnell 2025-05-15 [Bugfix] Fix fp8 tests for triton_unified_attention for Triton 3.3 (#18013)
3	0	tests/kernels/{ => attention}/test_triton_unified_attention.py
4	0	vllm/attention/ops/triton_unified_attention.py

[26d041930] Harry Mellor 2025-05-15 Update deprecated type hinting in `models` (#18132)
0	1	pyproject.toml
7	6	vllm/model_executor/models/arctic.py
10	10	vllm/model_executor/models/aria.py
4	4	vllm/model_executor/models/aya_vision.py
8	7	vllm/model_executor/models/baichuan.py
8	7	vllm/model_executor/models/bamba.py
4	3	vllm/model_executor/models/bart.py
7	6	vllm/model_executor/models/bert.py
13	12	vllm/model_executor/models/bert_with_rope.py
5	4	vllm/model_executor/models/blip.py
4	4	vllm/model_executor/models/blip2.py
5	4	vllm/model_executor/models/bloom.py
10	10	vllm/model_executor/models/chameleon.py
6	5	vllm/model_executor/models/chatglm.py
5	4	vllm/model_executor/models/clip.py
6	5	vllm/model_executor/models/commandr.py
6	6	vllm/model_executor/models/constant_size_cache.py
5	4	vllm/model_executor/models/dbrx.py
8	7	vllm/model_executor/models/deepseek.py
5	4	vllm/model_executor/models/deepseek_mtp.py
7	6	vllm/model_executor/models/deepseek_v2.py
9	9	vllm/model_executor/models/deepseek_vl2.py
3	2	vllm/model_executor/models/eagle.py
10	9	vllm/model_executor/models/exaone.py
4	4	vllm/model_executor/models/fairseq2_llama.py
7	6	vllm/model_executor/models/falcon.py
12	12	vllm/model_executor/models/florence2.py
3	3	vllm/model_executor/models/fuyu.py
8	7	vllm/model_executor/models/gemma.py
8	7	vllm/model_executor/models/gemma2.py
8	7	vllm/model_executor/models/gemma3.py
3	3	vllm/model_executor/models/gemma3_mm.py
6	5	vllm/model_executor/models/glm4.py
5	4	vllm/model_executor/models/gpt2.py
7	6	vllm/model_executor/models/gpt_bigcode.py
7	6	vllm/model_executor/models/gpt_j.py
7	6	vllm/model_executor/models/gpt_neox.py
9	8	vllm/model_executor/models/granite.py
4	3	vllm/model_executor/models/granite_speech.py
6	5	vllm/model_executor/models/granitemoe.py
8	7	vllm/model_executor/models/granitemoehybrid.py
6	5	vllm/model_executor/models/granitemoeshared.py
10	9	vllm/model_executor/models/grok1.py
5	4	vllm/model_executor/models/idefics2_vision_model.py
4	4	vllm/model_executor/models/idefics3.py
44	44	vllm/model_executor/models/interfaces.py
13	13	vllm/model_executor/models/interfaces_base.py
5	4	vllm/model_executor/models/intern_vit.py
10	9	vllm/model_executor/models/internlm2.py
2	2	vllm/model_executor/models/internlm2_ve.py
3	3	vllm/model_executor/models/internvl.py
5	4	vllm/model_executor/models/jais.py
7	6	vllm/model_executor/models/jamba.py
4	5	vllm/model_executor/models/kimi_vl.py
10	9	vllm/model_executor/models/llama.py
14	13	vllm/model_executor/models/llama4.py
5	5	vllm/model_executor/models/llama_eagle.py
7	6	vllm/model_executor/models/llama_eagle3.py
4	4	vllm/model_executor/models/llava.py
8	7	vllm/model_executor/models/llava_next.py
7	7	vllm/model_executor/models/llava_next_video.py
10	11	vllm/model_executor/models/llava_onevision.py
9	8	vllm/model_executor/models/mamba.py
7	6	vllm/model_executor/models/mamba2.py
2	3	vllm/model_executor/models/mamba_cache.py
13	12	vllm/model_executor/models/medusa.py
5	4	vllm/model_executor/models/mimo.py
5	4	vllm/model_executor/models/mimo_mtp.py
9	8	vllm/model_executor/models/minicpm.py
2	2	vllm/model_executor/models/minicpm3.py
3	4	vllm/model_executor/models/minicpmo.py
6	7	vllm/model_executor/models/minicpmv.py
9	8	vllm/model_executor/models/minimax_text_01.py
3	3	vllm/model_executor/models/minimax_vl_01.py
4	4	vllm/model_executor/models/mistral3.py
7	6	vllm/model_executor/models/mixtral.py
5	4	vllm/model_executor/models/mixtral_quant.py
40	40	vllm/model_executor/models/mllama.py
9	9	vllm/model_executor/models/mllama4.py
5	5	vllm/model_executor/models/mlp_speculator.py
6	5	vllm/model_executor/models/modernbert.py
9	9	vllm/model_executor/models/module_mapping.py
16	16	vllm/model_executor/models/molmo.py
4	3	vllm/model_executor/models/moonvit.py
7	6	vllm/model_executor/models/mpt.py
8	7	vllm/model_executor/models/nemotron.py
9	8	vllm/model_executor/models/nemotron_nas.py
6	5	vllm/model_executor/models/olmo.py
4	3	vllm/model_executor/models/olmo2.py
8	7	vllm/model_executor/models/olmoe.py
7	6	vllm/model_executor/models/opt.py
9	8	vllm/model_executor/models/orion.py
5	5	vllm/model_executor/models/ovis.py
3	3	vllm/model_executor/models/paligemma.py
7	6	vllm/model_executor/models/persimmon.py
7	6	vllm/model_executor/models/phi.py
9	8	vllm/model_executor/models/phi3_small.py
7	7	vllm/model_executor/models/phi3v.py
6	6	vllm/model_executor/models/phi4mm.py
2	2	vllm/model_executor/models/phi4mm_audio.py
2	2	vllm/model_executor/models/phi4mm_utils.py
7	6	vllm/model_executor/models/phimoe.py
13	13	vllm/model_executor/models/pixtral.py
4	3	vllm/model_executor/models/plamo2.py
4	4	vllm/model_executor/models/prithvi_geospatial_mae.py
7	6	vllm/model_executor/models/qwen.py
10	9	vllm/model_executor/models/qwen2.py
10	10	vllm/model_executor/models/qwen2_5_omni_thinker.py
10	10	vllm/model_executor/models/qwen2_5_vl.py
3	3	vllm/model_executor/models/qwen2_audio.py
9	8	vllm/model_executor/models/qwen2_moe.py
4	3	vllm/model_executor/models/qwen2_rm.py
8	9	vllm/model_executor/models/qwen2_vl.py
6	5	vllm/model_executor/models/qwen3.py
8	7	vllm/model_executor/models/qwen3_moe.py
4	5	vllm/model_executor/models/qwen_vl.py
31	31	vllm/model_executor/models/registry.py
6	5	vllm/model_executor/models/roberta.py
6	5	vllm/model_executor/models/siglip.py
3	3	vllm/model_executor/models/skyworkr1v.py
2	2	vllm/model_executor/models/smolvlm.py
7	6	vllm/model_executor/models/solar.py
8	7	vllm/model_executor/models/stablelm.py
7	6	vllm/model_executor/models/starcoder2.py
6	6	vllm/model_executor/models/telechat2.py
2	1	vllm/model_executor/models/transformers.py
3	3	vllm/model_executor/models/ultravox.py
25	25	vllm/model_executor/models/utils.py
11	11	vllm/model_executor/models/whisper.py
13	12	vllm/model_executor/models/zamba2.py

[83f74c698] Luka Govedič 2025-05-15 [Fix][ROCm] Enforce eager for all encoder-decoder models on ROCm (#18154)
8	3	vllm/config.py

[2dff09357] Reid 2025-05-15 [Misc] add lobe-chat support (#18177)
1	0	docs/source/deployment/frameworks/index.md
13	0	docs/source/deployment/frameworks/lobe-chat.md

[afe3236e9] Aaron Pham 2025-05-15 [Chore] astral's ty (#18116)
2	2	docs/source/getting_started/quickstart.md
6	0	pyproject.toml

[65334ef3b] Mark McLoughlin 2025-05-15 [V1][Metrics] Remove unused code (#18158)
0	1	.buildkite/test-pipeline.yaml
0	302	tests/v1/test_stats.py
0	0	vllm/v1/stats/__init__.py
0	453	vllm/v1/stats/common.py

[e60f550b3] Chen Zhang 2025-05-15 [v1] Support multiple KV cache groups in GPU model runner (#17945)
68	3	tests/v1/core/test_kv_cache_utils.py
18	18	tests/v1/core/test_prefix_caching.py
33	6	tests/v1/worker/test_gpu_input_batch.py
41	16	tests/v1/worker/test_gpu_model_runner.py
1	1	tests/weight_loading/models.txt
3	3	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	2	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
21	13	vllm/v1/core/kv_cache_manager.py
6	7	vllm/v1/core/kv_cache_utils.py
6	6	vllm/v1/core/sched/output.py
10	6	vllm/v1/core/sched/scheduler.py
42	0	vllm/v1/kv_cache_interface.py
47	0	vllm/v1/worker/block_table.py
7	6	vllm/v1/worker/gpu_input_batch.py
158	112	vllm/v1/worker/gpu_model_runner.py
19	16	vllm/v1/worker/tpu_model_runner.py

[f25e0d112] David Xia 2025-05-14 [Bugfix]: make most of `test_openai_schema.py` pass (#17664)
238	33	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_tokenization.py

[09f106a91] Andrey Talman 2025-05-14 Upload vllm index for the rc builds (#18173)
1	0	.buildkite/scripts/upload-wheels.sh

[2142035b5] Michael Goin 2025-05-14 [V1] Support multiple kv connectors (#17564)
241	0	tests/v1/kv_connector/unit/test_multi_connector.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
0	2	vllm/distributed/kv_transfer/kv_connector/v1/base.py
178	0	vllm/distributed/kv_transfer/kv_connector/v1/multi_connector.py

[78aa341d1] Russell Bryant 2025-05-14 [CI] Fix race condition in test_kv_cache_events test (#18169)
0	1	tests/v1/engine/test_engine_core_client.py
1	1	vllm/distributed/kv_events.py

[797473674] Jerry Zhang 2025-05-14 Add support for loading torchao models with `AOPerModuleConfig` (#17826)
15	3	tests/quantization/test_torchao.py
22	7	vllm/model_executor/layers/quantization/torchao.py

[2fc9075b8] Aaron Pham 2025-05-14 [V1] Structured Outputs + Thinking compatibility (#16577)
2	2	docs/source/features/reasoning_outputs.md
92	4	tests/v1/entrypoints/llm/test_struct_output_generate.py
2	2	vllm/config.py
4	2	vllm/reasoning/abs_reasoning_parsers.py
4	4	vllm/v1/core/sched/scheduler.py
85	16	vllm/v1/structured_output/__init__.py
7	15	vllm/v1/structured_output/backend_guidance.py
15	2	vllm/v1/structured_output/backend_types.py
21	28	vllm/v1/structured_output/backend_xgrammar.py
1	0	vllm/v1/structured_output/request.py

[d93c976a0] Lucas Wilkinson 2025-05-14 [Kernel] Have rotary embeddings support tensors (#18046)
28	12	csrc/pos_encoding_kernels.cu
13	1	tests/kernels/core/test_pos_encoding.py
15	2	tests/kernels/core/test_rotary_embedding.py
3	16	vllm/_custom_ops.py

[749f79255] David Xia 2025-05-14 [Frontend] decrease import time of vllm.multimodal (#18031)
24	21	vllm/multimodal/inputs.py
13	8	vllm/multimodal/parse.py
8	5	vllm/multimodal/processing.py

[856865008] Robert Shaw 2025-05-14 [CI] Disable Failing Tests (#18165)
2	0	tests/spec_decode/e2e/test_eagle_correctness.py
2	0	tests/v1/engine/test_engine_core_client.py

[f9c069c85] bnellnm 2025-05-14 Modularize fused experts and integrate PPLX kernels (#15956)
3	0	csrc/activation_kernels.cu
14	0	csrc/dispatch_utils.h
4	4	csrc/moe/moe_align_sum_kernels.cu
45	18	csrc/moe/topk_softmax_kernels.cu
16	6	examples/offline_inference/data_parallel.py
114	0	tests/kernels/moe/test_batched_moe.py
21	25	tests/kernels/moe/test_cutlass_moe.py
51	42	tests/kernels/moe/test_moe.py
691	0	tests/kernels/moe/test_pplx_moe.py
20	14	tests/kernels/moe/test_triton_moe_ptpc_fp8.py
10	10	tests/kernels/quantization/test_block_fp8.py
4	1	tests/kernels/quantization/test_block_int8.py
51	2	vllm/distributed/parallel_state.py
6	7	vllm/distributed/utils.py
4	1	vllm/forward_context.py
3	2	vllm/model_executor/layers/fused_moe/__init__.py
195	108	vllm/model_executor/layers/fused_moe/cutlass_moe.py
131	198	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
755	0	vllm/model_executor/layers/fused_moe/fused_batched_moe.py
287	101	vllm/model_executor/layers/fused_moe/fused_moe.py
499	52	vllm/model_executor/layers/fused_moe/layer.py
364	0	vllm/model_executor/layers/fused_moe/modular_kernel.py
79	11	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
147	0	vllm/model_executor/layers/fused_moe/pplx_prepare_finalize.py
60	0	vllm/model_executor/layers/fused_moe/prepare_finalize.py
112	0	vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py
54	5	vllm/model_executor/layers/fused_moe/utils.py
56	28	vllm/model_executor/layers/quantization/fp8.py
0	1	vllm/model_executor/models/dbrx.py
8	6	vllm/model_executor/models/deepseek_v2.py
4	4	vllm/model_executor/models/llama4.py
4	5	vllm/model_executor/models/qwen2_moe.py
2	4	vllm/model_executor/models/qwen3_moe.py
1	0	vllm/platforms/cuda.py
4	2	vllm/v1/attention/backends/mla/common.py
2	1	vllm/v1/worker/gpu_worker.py
2	1	vllm/v1/worker/tpu_worker.py
2	1	vllm/worker/cpu_worker.py
4	2	vllm/worker/hpu_worker.py
2	1	vllm/worker/tpu_worker.py
2	1	vllm/worker/worker.py
2	1	vllm/worker/xpu_worker.py

[418d2f8bf] Ekagra Ranjan 2025-05-14 [V1][Spec Decode] Share input embedding of target model with EAGLE draft model to free ~1GB for llama 3 model (#17326)
7	0	examples/offline_inference/eagle.py
18	9	vllm/model_executor/models/llama_eagle.py
10	5	vllm/model_executor/models/llama_eagle3.py
24	5	vllm/v1/spec_decode/eagle.py

[964472b96] Chen Zhang 2025-05-14 [Doc] Update prefix cache metrics to counting tokens (#18138)
2	2	docs/source/design/v1/metrics.md

[59dd311cf] Nick Hill 2025-05-14 [KVConnector] Keep KVTransferParams as a dict (#18033)
12	14	tests/v1/kv_connector/unit/utils.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
0	25	vllm/distributed/kv_transfer/kv_connector/v1/base.py
34	95	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
8	4	vllm/v1/core/sched/scheduler.py
4	8	vllm/v1/engine/core.py
4	9	vllm/v1/request.py

[d066e5201] Cyrus Leung 2025-05-14 [Bugfix] Fix chat utils tests (#18139)
3	3	tests/entrypoints/test_chat_utils.py

[c8ea982d9] Harry Mellor 2025-05-14 Update deprecated type hinting in `platform`, `plugins`, `triton_utils`, `vllm_flash_attn` (#18129)
0	5	pyproject.toml
6	7	vllm/platforms/cuda.py
3	3	vllm/platforms/interface.py
5	5	vllm/platforms/rocm.py
2	2	vllm/platforms/tpu.py
2	2	vllm/plugins/__init__.py

[dc372b9c8] Harry Mellor 2025-05-14 Update deprecated type hinting in `vllm/device_allocator` and `vllm/distributed` (#18126)
0	2	pyproject.toml
5	5	vllm/device_allocator/cumem.py
2	2	vllm/distributed/communication_op.py
2	2	vllm/distributed/device_communicators/base_device_communicator.py
2	2	vllm/distributed/device_communicators/cpu_communicator.py
2	2	vllm/distributed/device_communicators/cuda_communicator.py
4	4	vllm/distributed/device_communicators/cuda_wrapper.py
4	4	vllm/distributed/device_communicators/custom_all_reduce.py
5	4	vllm/distributed/device_communicators/custom_all_reduce_utils.py
5	5	vllm/distributed/device_communicators/pynccl_wrapper.py
4	4	vllm/distributed/device_communicators/shm_broadcast.py
6	6	vllm/distributed/kv_transfer/kv_connector/base.py
3	3	vllm/distributed/kv_transfer/kv_connector/factory.py
4	4	vllm/distributed/kv_transfer/kv_connector/lmcache_connector.py
4	4	vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py
5	5	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
2	2	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
4	4	vllm/distributed/kv_transfer/kv_connector_agent.py
3	3	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
7	7	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
3	3	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
22	22	vllm/distributed/parallel_state.py
7	6	vllm/distributed/utils.py

[9b5b39b65] Harry Mellor 2025-05-14 Update deprecated type hinting in `vllm/lora` (#18128)
0	1	pyproject.toml
11	11	vllm/lora/fully_sharded_layers.py
31	31	vllm/lora/layers.py
7	7	vllm/lora/lora.py
28	28	vllm/lora/models.py
3	5	vllm/lora/ops/triton_ops/lora_expand_op.py
2	2	vllm/lora/ops/triton_ops/lora_kernel_metadata.py
3	5	vllm/lora/ops/triton_ops/lora_shrink_op.py
4	6	vllm/lora/ops/triton_ops/utils.py
2	2	vllm/lora/peft_helper.py
41	41	vllm/lora/punica_wrapper/punica_base.py
23	23	vllm/lora/punica_wrapper/punica_cpu.py
18	18	vllm/lora/punica_wrapper/punica_gpu.py
15	15	vllm/lora/punica_wrapper/punica_hpu.py
25	25	vllm/lora/punica_wrapper/punica_tpu.py
8	8	vllm/lora/punica_wrapper/utils.py
4	3	vllm/lora/resolver.py
9	9	vllm/lora/utils.py
11	11	vllm/lora/worker_manager.py

[9ccc6ded4] Reid 2025-05-14 [doc] add missing import (#18133)
2	0	docs/source/serving/offline_inference.md

[d62a076e8] Cyrus Leung 2025-05-14 [Model] GritLM supports other attention backends (#18109)
31	46	tests/models/language/pooling/test_gritlm.py
12	34	vllm/model_executor/models/gritlm.py
28	14	vllm/model_executor/models/llama.py
13	13	vllm/model_executor/models/qwen2.py

[259127f8b] Jee Jee Li 2025-05-14 [Bugfix] Fix LoRA test (#18123)
11	5	tests/lora/test_worker.py

[612c2edb4] TJian 2025-05-14 [FEAT] [ROCm]: Add AITER CK 2 Stages MoE support (#17110)
7	0	tests/kernels/moe/test_moe.py
2	22	tests/model_executor/test_enabled_custom_ops.py
0	3	vllm/model_executor/layers/fused_moe/fused_moe.py
27	5	vllm/model_executor/layers/fused_moe/layer.py
106	67	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
25	7	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
34	8	vllm/model_executor/layers/quantization/fp8.py

[38fe728d6] Andrzej Kotłowski 2025-05-14 [Bugfix] Fix QKVCrossParallelLinear::sync_weight_attrs for PyTorch compile (#17844)
2	2	vllm/model_executor/layers/linear.py

[82e7f9bb0] rongfu.leng 2025-05-14 [Misc] replace does not exist model (#18119)
1	1	docs/source/models/supported_models.md
1	1	tests/distributed/test_pipeline_parallel.py
1	1	tests/models/registry.py
1	1	vllm/test_utils.py

[63dc3426e] Jee Jee Li 2025-05-14 [Model] Add packed_modules_mapping for Qwen3-MOE (#18118)
11	0	vllm/model_executor/models/qwen3_moe.py

[8f5dc4148] Cyrus Leung 2025-05-14 [Bugfix] Fix entrypoints audio test failure (#18111)
4	4	tests/entrypoints/openai/test_audio.py

[63ad62223] wang.yuqi 2025-05-14 [New Model]: support GTE NewModel (#17986)
16	2	docs/source/models/supported_models.md
12	5	tests/models/language/pooling/mteb_utils.py
104	0	tests/models/language/pooling/test_gte.py
4	0	tests/models/language/pooling/test_nomic.py
4	0	tests/models/language/pooling/test_snowflake_arctic_embed.py
6	0	tests/models/registry.py
1	1	vllm/model_executor/layers/activation.py
42	0	vllm/model_executor/layers/rotary_embedding.py
84	23	vllm/model_executor/models/bert_with_rope.py
3	0	vllm/model_executor/models/modernbert.py
3	1	vllm/model_executor/models/registry.py

[e7ef61c1f] majianpeng 2025-05-14 [Bugfix][Example] make lmcache v0 work. (#18051)
4	2	examples/lmcache/cpu_offload_lmcache.py

[d4154c35a] Jinzhen Lin 2025-05-14 [Bugfix] fix moe marlin `topk_weight` loading (#18080)
7	7	csrc/moe/marlin_moe_wna16/marlin_template.h

[6685890d1] lkchen 2025-05-13 [Fix] Move "model_config" as keyword args in chat_utils.py (#18098)
2	2	tests/entrypoints/openai/test_chat_template.py
5	5	tests/entrypoints/test_chat_utils.py
27	8	vllm/entrypoints/chat_utils.py
3	3	vllm/entrypoints/llm.py
2	2	vllm/entrypoints/openai/api_server.py
3	3	vllm/entrypoints/openai/serving_engine.py

[33011318c] Ecthlion_zyy 2025-05-14 Fix broken example: examples/offline_inference/profiling at scheduler_config  (#18117)
1	1	examples/offline_inference/profiling.py

[4f8b37322] qli88 2025-05-14 [BugFix][AMD] Compatible patch for AITER lib after 04/20 (#17912)
37	12	vllm/attention/backends/rocm_aiter_mla.py
12	1	vllm/attention/ops/rocm_aiter_mla.py
5	4	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[7b2f28deb] Charlie Fu 2025-05-14 [AMD][torch.compile] Enable silu+fp8_quant fusion for rocm (#18082)
1	0	.buildkite/test-pipeline.yaml
2	1	csrc/quantization/activation_kernels.cu
3	3	tests/compile/test_silu_mul_quant_fusion.py
3	2	tests/kernels/quantization/test_rocm_skinny_gemms.py
3	2	tests/kernels/test_fused_quant_activation.py
2	1	vllm/compilation/activation_quant_fusion.py

[2d912fb66] vllmellm 2025-05-14 [FEAT] [ROCm] [V1]: Add AITER biased group topk for DeepSeekV3 (#17955)
122	0	tests/kernels/moe/test_rocm_aiter_topk.py
8	2	vllm/model_executor/layers/fused_moe/layer.py
71	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[12e6c0b41] Michael Goin 2025-05-13 [Bugfix][V1] Fix FlashInfer V1 backend using the wrong VllmConfig (#18086)
2	3	vllm/v1/attention/backends/flashinfer.py

[9a2a6357d] Michael Goin 2025-05-13 [Bugfix] Fix FP8 Marlin MoE and enable for compressed-tensors models (#18026)
48	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
4	0	vllm/model_executor/layers/quantization/fp8.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[6266c57ba] youkaichao 2025-05-14 [core][distributed] add ep group and all2all interface (#18077)
93	0	vllm/distributed/device_communicators/all2all.py
25	1	vllm/distributed/device_communicators/base_device_communicator.py
37	2	vllm/distributed/device_communicators/cuda_communicator.py
59	2	vllm/distributed/parallel_state.py
7	2	vllm/distributed/utils.py
5	0	vllm/envs.py
5	33	vllm/model_executor/layers/fused_moe/layer.py
3	1	vllm/v1/worker/gpu_model_runner.py

[754b699cb] Jon Gill 2025-05-13 [Bug]: Fix S3 model/tokenizer path resolution (#18083)
24	17	vllm/config.py

[6e27c6d86] Roger Wang 2025-05-13 [Misc] Remove unused numpy tensor (#18084)
0	1	vllm/v1/worker/gpu_model_runner.py
0	1	vllm/v1/worker/tpu_model_runner.py

[d5af47a14] Nick Hill 2025-05-13 [P/D] Add some more debug logs to `NixlConnector` (#18102)
16	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py

[65f0f74b6] Pavani Majety 2025-05-13 [Hardware/NVIDIA/Modelopt] Fix modelopt forward method for v1 torch.compile (#18101)
5	2	vllm/model_executor/layers/fused_moe/cutlass_moe.py
9	8	vllm/model_executor/layers/quantization/modelopt.py

[176a95c67] Luka Govedič 2025-05-13 [Fix] Support CUDAGraph capture for encoder-decoder on ROCm (#18104)
8	8	vllm/attention/backends/utils.py

[f2ae883b6] Chen Zhang 2025-05-14 [v1][KVCacheManager] pass num_new_computed_tokens to kv cache manager (#18001)
93	33	tests/v1/core/test_prefix_caching.py
6	10	vllm/v1/core/kv_cache_manager.py
20	10	vllm/v1/core/sched/scheduler.py

[40de1ef45] vllmellm 2025-05-14 [FEAT] [ROCm]: Add AITER Block-Scaled GEMM Feature (#14968)
31	0	tests/model_executor/test_enabled_custom_ops.py
8	0	vllm/model_executor/layers/quantization/fp8.py
98	32	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[0189a65a2] Russell Bryant 2025-05-13 [Docs] Expand security doc with firewall info (#18081)
39	0	docs/source/deployment/security.md

[55aa7af99] Nick Hill 2025-05-13 [V1] DP scale-out (2/N): Decouple engine process management and comms (#15977)
1	1	tests/async_engine/test_async_llm_engine.py
8	7	tests/v1/engine/test_engine_core_client.py
19	22	vllm/config.py
2	1	vllm/distributed/utils.py
38	0	vllm/engine/arg_utils.py
79	2	vllm/entrypoints/cli/serve.py
4	0	vllm/utils.py
122	67	vllm/v1/engine/core.py
174	119	vllm/v1/engine/core_client.py
78	33	vllm/v1/utils.py

[0b217da64] Harry Mellor 2025-05-13 Update deprecated type hinting in `vllm/adapter_commons` (#18073)
0	1	pyproject.toml
2	3	vllm/adapter_commons/layers.py
4	4	vllm/adapter_commons/models.py
9	9	vllm/adapter_commons/utils.py
3	3	vllm/adapter_commons/worker_manager.py

[19324d660] Harry Mellor 2025-05-13 Update deprecated type hinting in `vllm/compilation` (#18072)
0	1	pyproject.toml
17	16	vllm/compilation/backends.py
19	19	vllm/compilation/compiler_interface.py
4	4	vllm/compilation/decorators.py
8	7	vllm/compilation/fix_functionalization.py
5	5	vllm/compilation/fusion.py
2	1	vllm/compilation/fx_utils.py
2	2	vllm/compilation/inductor_pass.py
3	3	vllm/compilation/multi_output_match.py
2	1	vllm/compilation/noop_elimination.py
1	3	vllm/compilation/pass_manager.py
5	5	vllm/compilation/sequence_parallelism.py
2	2	vllm/compilation/wrapper.py

[fc407a142] Harry Mellor 2025-05-13 Give auto-merge label workflow permission to add labels to issues (#18078)
1	1	.github/workflows/add_label_automerge.yml
1	1	.github/workflows/reminder_comment.yml

[009d9e759] Harry Mellor 2025-05-13 Convert `benchmarks` to `ruff format` (#18068)
0	4	.buildkite/pyproject.toml
1	3	.pre-commit-config.yaml
94	95	benchmarks/backend_request_func.py
186	169	benchmarks/benchmark_dataset.py
41	34	benchmarks/benchmark_latency.py
51	40	benchmarks/benchmark_long_document_qa_throughput.py
64	52	benchmarks/benchmark_prefix_caching.py
70	52	benchmarks/benchmark_prioritization.py
337	248	benchmarks/benchmark_serving.py
263	226	benchmarks/benchmark_serving_structured_output.py
257	180	benchmarks/benchmark_throughput.py
7	8	benchmarks/benchmark_utils.py
250	122	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
17	16	benchmarks/cutlass_benchmarks/utils.py
140	134	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
1	1	benchmarks/cutlass_benchmarks/weight_shapes.py
13	14	benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
10	10	benchmarks/disagg_benchmarks/round_robin_proxy.py
21	23	benchmarks/disagg_benchmarks/visualize_benchmark_results.py
96	45	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
118	78	benchmarks/kernels/benchmark_aqlm.py
33	28	benchmarks/kernels/benchmark_bitblas.py
249	168	benchmarks/kernels/benchmark_cutlass_fp4_moe.py
219	144	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
32	28	benchmarks/kernels/benchmark_layernorm.py
378	275	benchmarks/kernels/benchmark_lora.py
212	157	benchmarks/kernels/benchmark_machete.py
117	73	benchmarks/kernels/benchmark_marlin.py
237	165	benchmarks/kernels/benchmark_moe.py
160	93	benchmarks/kernels/benchmark_moe_permute_unpermute.py
45	52	benchmarks/kernels/benchmark_paged_attention.py
38	33	benchmarks/kernels/benchmark_quant.py
24	33	benchmarks/kernels/benchmark_rmsnorm.py
46	37	benchmarks/kernels/benchmark_rope.py
53	60	benchmarks/kernels/benchmark_w8a8_block_fp8.py
3	1	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
16	17	benchmarks/kernels/graph_machete_bench.py
26	26	benchmarks/kernels/utils.py
19	17	benchmarks/overheads/benchmark_hashing.py
54	0	benchmarks/pyproject.toml
5	0	pyproject.toml

[b922c2ebd] Cyrus Leung 2025-05-13 [Bugfix] Fix entrypoints metrics tests (#18063)
4	4	vllm/entrypoints/openai/api_server.py
1	1	vllm/v1/engine/core.py

[00b14e0f1] Russell Bryant 2025-05-13 [CI] set token permissions for pre-commit CI job (#17729)
3	0	.github/workflows/pre-commit.yml

[54e467e6f] Russell Bryant 2025-05-13 [CI] Add token permissions for add-ready-label CI job (#17730)
2	0	.github/workflows/add_label_automerge.yml

[79a1d25bb] Russell Bryant 2025-05-13 [CI] Add workflow permissions for helm CI job (#17727)
3	0	.github/workflows/lint-and-deploy.yaml

[9944011b3] Russell Bryant 2025-05-13 [CI] Set token permissions for reminder comment CI job (#17728)
2	0	.github/workflows/reminder_comment.yml

[8c946cecc] Harry Mellor 2025-05-13 Update deprecated type hinting in `vllm/transformers_utils` (#18058)
5	5	vllm/transformers_utils/config.py
3	3	vllm/transformers_utils/configs/arctic.py
3	3	vllm/transformers_utils/configs/cohere2.py
2	3	vllm/transformers_utils/configs/deepseek_vl2.py
1	3	vllm/transformers_utils/configs/exaone.py
2	2	vllm/transformers_utils/configs/jais.py
3	3	vllm/transformers_utils/configs/mlp_speculator.py
9	9	vllm/transformers_utils/configs/mpt.py
1	1	vllm/transformers_utils/configs/solar.py
5	5	vllm/transformers_utils/configs/ultravox.py
3	3	vllm/transformers_utils/detokenizer.py
12	12	vllm/transformers_utils/detokenizer_utils.py
14	15	vllm/transformers_utils/processors/deepseek_vl2.py
6	6	vllm/transformers_utils/processors/ovis.py
4	4	vllm/transformers_utils/tokenizer_group.py
23	23	vllm/transformers_utils/tokenizers/mistral.py
2	2	vllm/transformers_utils/utils.py

[ff334ca1c] Harry Mellor 2025-05-13 Update deprecated type hinting in `vllm/profiler` (#18057)
0	1	pyproject.toml
19	19	vllm/profiler/layerwise_profile.py
4	4	vllm/profiler/utils.py

[6223dd811] Harry Mellor 2025-05-13 Update deprecated type hinting in `model_executor/layers` (#18056)
0	1	pyproject.toml
3	3	vllm/model_executor/layers/fused_moe/__init__.py
2	2	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
23	23	vllm/model_executor/layers/fused_moe/fused_moe.py
4	4	vllm/model_executor/layers/fused_moe/layer.py
2	2	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
2	2	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
7	7	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
4	4	vllm/model_executor/layers/fused_moe/utils.py
10	10	vllm/model_executor/layers/layernorm.py
3	3	vllm/model_executor/layers/mamba/mamba_mixer2.py
4	4	vllm/model_executor/layers/pooler.py
3	3	vllm/model_executor/layers/quantization/__init__.py
7	7	vllm/model_executor/layers/quantization/aqlm.py
7	7	vllm/model_executor/layers/quantization/awq.py
8	8	vllm/model_executor/layers/quantization/awq_marlin.py
8	8	vllm/model_executor/layers/quantization/base_config.py
8	8	vllm/model_executor/layers/quantization/bitblas.py
7	7	vllm/model_executor/layers/quantization/bitsandbytes.py
19	19	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
6	6	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
6	5	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
5	5	vllm/model_executor/layers/quantization/deepspeedfp.py
4	4	vllm/model_executor/layers/quantization/experts_int8.py
6	6	vllm/model_executor/layers/quantization/fbgemm_fp8.py
7	7	vllm/model_executor/layers/quantization/fp8.py
6	6	vllm/model_executor/layers/quantization/gguf.py
7	7	vllm/model_executor/layers/quantization/gptq.py
7	7	vllm/model_executor/layers/quantization/gptq_bitblas.py
10	10	vllm/model_executor/layers/quantization/gptq_marlin.py
5	5	vllm/model_executor/layers/quantization/gptq_marlin_24.py
6	6	vllm/model_executor/layers/quantization/hqq_marlin.py
5	5	vllm/model_executor/layers/quantization/ipex_quant.py
5	5	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
4	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
2	2	vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark.py
4	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
2	2	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py
2	2	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
2	2	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
3	3	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
4	4	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
2	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py
2	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
2	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
2	2	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
5	5	vllm/model_executor/layers/quantization/marlin.py
11	11	vllm/model_executor/layers/quantization/modelopt.py
8	8	vllm/model_executor/layers/quantization/moe_wna16.py
4	4	vllm/model_executor/layers/quantization/neuron_quant.py
3	3	vllm/model_executor/layers/quantization/ptpc_fp8.py
5	5	vllm/model_executor/layers/quantization/qqq.py
27	27	vllm/model_executor/layers/quantization/quark/quark.py
2	2	vllm/model_executor/layers/quantization/quark/quark_moe.py
4	4	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
2	2	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
3	3	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
3	2	vllm/model_executor/layers/quantization/quark/utils.py
2	2	vllm/model_executor/layers/quantization/schema.py
6	6	vllm/model_executor/layers/quantization/torchao.py
6	6	vllm/model_executor/layers/quantization/tpu_int8.py
4	4	vllm/model_executor/layers/quantization/utils/bitblas_utils.py
9	9	vllm/model_executor/layers/quantization/utils/fp8_utils.py
2	2	vllm/model_executor/layers/quantization/utils/gptq_utils.py
8	8	vllm/model_executor/layers/quantization/utils/int8_utils.py
4	4	vllm/model_executor/layers/quantization/utils/machete_utils.py
6	6	vllm/model_executor/layers/quantization/utils/marlin_utils.py
3	3	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
4	5	vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py
4	6	vllm/model_executor/layers/quantization/utils/marlin_utils_test_qqq.py
1	2	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
9	8	vllm/model_executor/layers/quantization/utils/quant_utils.py
9	9	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
8	8	vllm/model_executor/layers/rejection_sampler.py
5	5	vllm/model_executor/layers/resampler.py
53	53	vllm/model_executor/layers/rotary_embedding.py
28	27	vllm/model_executor/layers/sampler.py
2	2	vllm/model_executor/layers/spec_decode_base_sampler.py
2	2	vllm/model_executor/layers/utils.py
8	7	vllm/model_executor/layers/vocab_parallel_embedding.py

[906f0598f] Reid 2025-05-13 [doc] add download/list/delete HF model CLI usage (#17940)
60	0	docs/source/models/supported_models.md

[cb528d058] Aaron Pham 2025-05-13 [Fix] check to make sure processor has chat templates (#18047)
1	1	tests/compile/test_pass_manager.py
1	1	vllm/compilation/inductor_pass.py
2	2	vllm/entrypoints/chat_utils.py

[98fcba157] Harry Mellor 2025-05-13 Convert `.buildkite` to `ruff format` (#17656)
12	8	.buildkite/check-wheel-size.py
2	2	.buildkite/generate_index.py
10	6	.buildkite/lm-eval-harness/conftest.py
15	11	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
46	43	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
6	9	.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
20	21	.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
10	14	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
55	0	.buildkite/pyproject.toml
4	0	.pre-commit-config.yaml
2	0	pyproject.toml

[23b3134eb] Russell Bryant 2025-05-13 [Benchmarks] Refactor run_structured_output_benchmarks.sh (#17722)
85	16	benchmarks/run_structured_output_benchmark.sh

[ea6ae8cb4] Michael Goin 2025-05-13 [Bugfix] Fix marlin moe fallback logic for llama4 (#18042)
2	1	tests/weight_loading/models-large.txt
1	1	vllm/model_executor/layers/fused_moe/layer.py
9	3	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[2ff297dce] Woosuk Kwon 2025-05-13 [BugFix] Set default random seed to 0 for V1 (#17929)
20	1	vllm/config.py

[8dd0671ba] Jin Huang 2025-05-13 [Bugfix][V1] Only get input embeddings w/ multi-modal models if first PP (#17916)
1	1	vllm/v1/worker/gpu_model_runner.py

[f0d610a8a] Chen Zhang 2025-05-13 [v1][KVCacheManager] Avoid full cache hit by controlling max_length (#17999)
5	2	tests/v1/core/test_specialized_manager.py
10	21	vllm/v1/core/kv_cache_manager.py
21	16	vllm/v1/core/single_type_kv_cache_manager.py

[e57e4d6e9] Driss Guessous 2025-05-12 Fix Broken macro for cutlass moe (#18049)
2	1	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[ee5be834e] Nick Hill 2025-05-12 [BugFix] Fix 4-GPU RLHF tests (#18007)
2	2	.buildkite/test-pipeline.yaml

[48545728d] Calvin Chen 2025-05-13 cleanup invalid prints (#18050)
0	2	vllm/worker/hpu_model_runner.py

[dc1a82176] Chauncey 2025-05-13 [Feature][V1]  Support `tool_choice: required` when using Xgrammar as the `StructuredOutputBackend`. (#17845)
1	1	requirements/common.txt
154	0	tests/entrypoints/openai/test_completion_with_function_calling.py
3	1	tests/v1/entrypoints/conftest.py
0	8	tests/v1/structured_output/test_utils.py
2	3	vllm/v1/structured_output/backend_xgrammar.py

[61e0a506a] Cyrus Leung 2025-05-13 [Bugfix] Avoid repeatedly creating dummy data during engine startup (#17935)
3	0	vllm/engine/async_llm_engine.py
7	0	vllm/engine/llm_engine.py
5	0	vllm/engine/multiprocessing/__init__.py
8	0	vllm/engine/multiprocessing/client.py
6	0	vllm/engine/multiprocessing/engine.py
5	0	vllm/engine/protocol.py
4	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/multimodal/processing.py
10	4	vllm/multimodal/registry.py
5	0	vllm/v1/engine/async_llm.py
9	0	vllm/v1/engine/core.py
15	0	vllm/v1/engine/core_client.py
8	0	vllm/v1/engine/llm_engine.py
5	0	vllm/v1/engine/mm_input_cache.py
4	0	vllm/v1/engine/processor.py

[1df491c52] Michael Goin 2025-05-12 [Bugfix] Fixes for new marlin moe usage (#18017)
3	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	2	vllm/model_executor/layers/quantization/gptq_marlin.py

[d8487ef55] Arjun Kathuria 2025-05-13 [ROCm]: Fix build from source failure with gcc14 and ROCm 6.3 (#13779)
14	2	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
7	1	csrc/quantization/fused_kernels/quant_conversions.cuh

[c06af9a95] Jee Jee Li 2025-05-13 [Misc] Slight spelling modification (#18039)
1	1	README.md
1	1	vllm/model_executor/layers/logits_processor.py

[60f762433] Tao He 2025-05-13 Implements dual-chunk-flash-attn backend for dual chunk attention with sparse attention support (#11844)
1	0	CMakeLists.txt
401	0	csrc/attention/vertical_slash_index.cu
25	0	csrc/ops.h
23	0	csrc/torch_bindings.cpp
66	0	examples/offline_inference/qwen_1m.py
95	0	vllm/_custom_ops.py
1494	0	vllm/attention/backends/dual_chunk_flash_attn.py
19	0	vllm/config.py
13	2	vllm/engine/arg_utils.py
202	2	vllm/model_executor/layers/rotary_embedding.py
33	0	vllm/model_executor/model_loader/weight_utils.py
35	21	vllm/model_executor/models/qwen2.py
19	7	vllm/model_executor/models/qwen2_moe.py
4	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
1	0	vllm/utils.py
12	0	vllm/worker/model_runner.py

[f6518b2b4] hissu-hyvarinen 2025-05-13 [ROCm] Skip tests for quantizations incompatible with ROCm (#17905)
4	1	tests/models/quantization/test_aqlm.py
8	0	tests/models/quantization/test_fp8.py
4	1	tests/models/quantization/test_gptq_marlin.py
4	1	tests/models/quantization/test_gptq_marlin_24.py

[d67085c2c] Harry Mellor 2025-05-13 Remove noisy warnings from `SchedulerConfig` (#17995)
0	6	vllm/config.py

[307939f29] Michael Goin 2025-05-12 Use NVFP4 Marlin for CompressedTensorsW4A16Fp4 (#18000)
26	41	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py

[9d7ea9dbb] Harry Mellor 2025-05-13 Update some more deprecated type hinting (#17998)
3	1	pyproject.toml
1	3	vllm/model_executor/custom_op.py
2	2	vllm/model_executor/guided_decoding/guidance_logits_processors.py
5	5	vllm/model_executor/guided_decoding/guided_fields.py
2	2	vllm/model_executor/guided_decoding/outlines_decoding.py
8	8	vllm/model_executor/guided_decoding/outlines_logits_processors.py
2	2	vllm/model_executor/guided_decoding/xgrammar_decoding.py
4	4	vllm/model_executor/pooling_metadata.py
44	44	vllm/model_executor/sampling_metadata.py
2	2	vllm/model_executor/utils.py

[acee8f48a] bwshen-mi 2025-05-13 [Model] Support MiMo-7B inference with MTP (#17433)
5	0	docs/source/models/supported_models.md
5	0	tests/models/registry.py
17	3	vllm/config.py
190	0	vllm/model_executor/models/mimo.py
283	0	vllm/model_executor/models/mimo_mtp.py
2	0	vllm/model_executor/models/registry.py
5	1	vllm/worker/worker.py

[f065de4e8] Michael Goin 2025-05-12 Fix FBGEMM integration (#18002)
3	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py
8	12	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[dc9905368] wwl2755 2025-05-12 [V1][Spec Decode] Eagle unit tests (#17350)
340	0	tests/v1/spec_decode/test_eagle.py
4	0	vllm/v1/spec_decode/eagle.py

[ebab1ac37] Russell Bryant 2025-05-12 [CI] Make JSON output tests less likely to fail (#17859)
8	4	tests/v1/entrypoints/conftest.py
19	7	tests/v1/entrypoints/llm/test_struct_output_generate.py

[2b0db9b0e] Yang Wang 2025-05-12 Enable standard language model for torhc nightly (#18004)
4	0	.buildkite/test-pipeline.yaml

[195adb47c] Robert Shaw 2025-05-12 [Chore] Remove unused method (#18024)
0	5	vllm/v1/core/kv_cache_manager.py

[302f3aca7] Chen Zhang 2025-05-13 [v1][KVCacheManager] Change prefix caching metric from counting blocks to counting tokens (#18003)
6	6	vllm/v1/core/kv_cache_manager.py
2	2	vllm/v1/metrics/loggers.py
1	1	vllm/v1/metrics/stats.py

[e9c730c9b] Alexei-V-Ivanov-AMD 2025-05-12 Enabling "Weight Loading Multiple GPU Test - Large Models" (#18020)
1	0	.buildkite/test-pipeline.yaml

[289199feb] Jade Zheng 2025-05-13 [Core] Use platform-agnostic device control for DP engine core (#17245)
4	22	vllm/platforms/cuda.py
19	0	vllm/platforms/interface.py
1	10	vllm/platforms/rocm.py
6	7	vllm/v1/engine/core.py

[b9fd0d7a6] Carol Zheng 2025-05-12 [CI/Build] Fix TPU V1 Test mixed use of & and && across tests (#17968)
21	21	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[72a3f6b89] Harry Mellor 2025-05-12 Construct `KVTransferConfig` properly from Python instead of using JSON blobs without CLI (#17994)
8	6	examples/lmcache/disagg_prefill_lmcache_v0.py
4	4	examples/lmcache/kv_cache_sharing_lmcache_v1.py
11	10	examples/offline_inference/disaggregated-prefill-v1/decode_example.py
6	5	examples/offline_inference/disaggregated-prefill-v1/prefill_example.py
8	6	examples/offline_inference/disaggregated_prefill.py

[98ea35601] Jonathan Berkhahn 2025-05-12 [Lora][Frontend]Add default local directory LoRA resolver plugin. (#16855)
2	1	.buildkite/test-pipeline.yaml
5	2	docs/source/features/lora.md
3	0	pyproject.toml
0	0	tests/plugins/lora_resolvers/__init__.py
65	0	tests/plugins/lora_resolvers/test_filesystem_resolver.py
7	0	vllm/envs.py
15	0	vllm/plugins/lora_resolvers/README.md
0	0	vllm/plugins/lora_resolvers/__init__.py
49	0	vllm/plugins/lora_resolvers/filesystem_resolver.py

[d19110204] Robert Shaw 2025-05-12 [P/D] NIXL Integration (#17751)
1	0	.buildkite/test-pipeline.yaml
3	3	tests/v1/core/test_scheduler.py
171	0	tests/v1/kv_connector/nixl_integration/run_accuracy_test.sh
123	0	tests/v1/kv_connector/nixl_integration/run_edge_case_test.sh
60	0	tests/v1/kv_connector/nixl_integration/test_accuracy.py
77	0	tests/v1/kv_connector/nixl_integration/test_edge_cases.py
260	0	tests/v1/kv_connector/nixl_integration/toy_proxy_server.py
0	0	tests/v1/kv_connector/unit/__init__.py
73	0	tests/v1/kv_connector/unit/test_nixl_connector.py
181	0	tests/v1/kv_connector/unit/test_remote_decode_lifecycle.py
342	0	tests/v1/kv_connector/unit/test_remote_prefill_lifecycle.py
190	0	tests/v1/kv_connector/unit/utils.py
5	1	vllm/config.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
2	5	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
82	7	vllm/distributed/kv_transfer/kv_connector/v1/base.py
4	2	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py
805	0	vllm/distributed/kv_transfer/kv_connector/v1/nixl_connector.py
6	6	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
17	2	vllm/entrypoints/openai/protocol.py
1	0	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py
10	0	vllm/envs.py
0	21	vllm/forward_context.py
5	1	vllm/outputs.py
32	1	vllm/v1/core/kv_cache_manager.py
4	0	vllm/v1/core/sched/interface.py
152	36	vllm/v1/core/sched/scheduler.py
1	0	vllm/v1/engine/__init__.py
9	0	vllm/v1/engine/core.py
9	3	vllm/v1/engine/output_processor.py
13	9	vllm/v1/outputs.py
12	1	vllm/v1/request.py
68	10	vllm/v1/worker/gpu_model_runner.py

[05a4324f8] Maximilien de Bayser 2025-05-12 Initialize the delta tool call fields explicitly (#17340)
1	1	tests/entrypoints/openai/tool_parsers/utils.py
4	0	vllm/entrypoints/chat_utils.py
5	4	vllm/entrypoints/openai/protocol.py
25	14	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py
2	1	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py

[7ea6cb28b] Jee Jee Li 2025-05-12 [Misc] Improve modelscope  import error  (#17983)
16	12	vllm/transformers_utils/__init__.py

[9fbf2bfbd] Aaruni Aggarwal 2025-05-12 Correcting testcases in builkite job for IBM Power (#17675)
6	3	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh

[3a5ea7512] Xu Wenqing 2025-05-12 [Feature] Support DeepSeekV3 Function Call (#17784)
7	0	docs/source/features/tool_calling.md
96	0	examples/tool_chat_template_deepseekv3.jinja
2	1	vllm/entrypoints/openai/tool_parsers/__init__.py
368	0	vllm/entrypoints/openai/tool_parsers/deepseekv3_tool_parser.py

[891b9d33d] Brayden Zhong 2025-05-12 [Fix] Benchmark `"EngineClient" has no attribute "model_config"` (#17976)
4	3	benchmarks/benchmark_throughput.py
3	2	vllm/benchmarks/throughput.py

[430783018] Siyuan Liu 2025-05-11 [Bugfix][TPU] Use np array when updating cache slot_mapping (#17971)
1	1	vllm/v1/worker/tpu_model_runner.py

[19a3c78d1] Li Wang 2025-05-12 [Bugfix] Fix pydantic.errors.PydanticUserError (#17962)
7	1	vllm/entrypoints/openai/serving_engine.py

[ada50aa29] Reid 2025-05-12 [bugfix] fix the wrong parser (#17958)
3	4	vllm/entrypoints/cli/collect_env.py

[08bf78407] Cheng Kuan Yong Jason 2025-05-12 [Bugfix] validate grammar and throw 400 error instead of crashing the engine when xgrammar validation fails (#17623)
137	0	tests/v1/entrypoints/openai/test_chat_completion.py
94	0	tests/v1/entrypoints/openai/test_completion.py
3	1	vllm/v1/engine/processor.py
6	0	vllm/v1/structured_output/backend_xgrammar.py

[d45fe333f] youkaichao 2025-05-12 [misc] add instructions on how to install nvshmem/pplx/deepep (#17964)
27	0	tools/ep_kernels/README.md
77	0	tools/ep_kernels/install_python_libraries.sh
24	0	tools/ep_kernels/install_system_drivers.sh
18	0	tools/ep_kernels/install_system_libraries.sh

[021c16c7c] Isotr0py 2025-05-12 [Model] Broadcast Ovis2 implementation to fit Ovis1.6 (#17861)
3	3	docs/source/models/supported_models.md
12	9	examples/offline_inference/vision_language.py
12	10	examples/offline_inference/vision_language_multi_image.py
7	1	tests/conftest.py
26	1	tests/models/multimodal/generation/test_common.py
11	6	tests/models/multimodal/generation/vlm_utils/model_utils.py
4	1	tests/models/multimodal/processing/test_common.py
3	3	tests/models/registry.py
1	1	vllm/entrypoints/chat_utils.py
2	125	vllm/model_executor/models/aimv2.py
203	37	vllm/model_executor/models/{ovis2.py => ovis.py}
1	1	vllm/model_executor/models/registry.py
1	1	vllm/transformers_utils/configs/__init__.py
13	0	vllm/transformers_utils/configs/{ovis2.py => ovis.py}
1	1	vllm/transformers_utils/processors/__init__.py
30	12	vllm/transformers_utils/processors/{ovis2.py => ovis.py}

[7de18d541] TJian 2025-05-12 [BUG] [ROCm] [MLA] Fix variable name bug due to change in variable name in PR #17483 (#17961)
3	3	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[a810b5b08] TJian 2025-05-11 [BugFix] [ROCm]: Bugfix and handle addition case of input for `rocm_aiter_rms_norm` (#17857)
4	0	tests/models/language/generation/test_common.py
11	4	vllm/model_executor/layers/layernorm.py

[009b3d538] Reid 2025-05-11 [Misc] not show --model in vllm serve --help (#16691)
3	1	vllm/engine/arg_utils.py

[e4b871338] wang.yuqi 2025-05-11 [New Model]: nomic-embed-text-v2-moe (#17785)
17	3	docs/source/models/supported_models.md
111	0	tests/models/language/pooling/mteb_utils.py
47	0	tests/models/language/pooling/test_nomic.py
22	43	tests/models/language/pooling/test_snowflake_arctic_embed.py
2	1	tests/models/utils.py
29	238	vllm/model_executor/models/bert.py
652	0	vllm/model_executor/models/bert_with_rope.py
2	2	vllm/model_executor/models/registry.py
17	77	vllm/model_executor/models/roberta.py

[06c0922a6] Gregory Shtrasberg 2025-05-11 [FP8][ROCm][Attention] Enable FP8 KV cache on ROCm for V1 (#17870)
2	1	vllm/attention/ops/chunked_prefill_paged_decode.py
3	1	vllm/engine/arg_utils.py
12	6	vllm/v1/attention/backends/triton_attn.py

[cd3edfc90] Dipika Sikka 2025-05-11 [Misc] Add compressed-tensors NVFP4A16 emulation support (#17914)
23	3	tests/quantization/test_compressed_tensors.py
22	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
107	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_nvfp4.py
61	0	vllm/model_executor/layers/quantization/utils/nvfp4_emulation_utils.py

[9cea90eab] Frieda Huang 2025-05-11 [Frontend] Add /classify endpoint (#17032)
1	0	docs/source/models/pooling_models.md
126	0	docs/source/serving/openai_compatible_server.md
49	0	examples/online_serving/openai_classification_client.py
156	0	tests/entrypoints/openai/test_classification.py
36	0	vllm/entrypoints/openai/api_server.py
41	0	vllm/entrypoints/openai/protocol.py
159	0	vllm/entrypoints/openai/serving_classification.py
110	149	vllm/entrypoints/openai/serving_embedding.py
283	13	vllm/entrypoints/openai/serving_engine.py

[d1110f5b5] Reid 2025-05-11 [doc] update lora doc (#17936)
4	2	docs/source/features/lora.md

[8132365b7] Ben Browning 2025-05-11 [Bugfix]: v1 engine - consider lora adapters in allowed_token_ids (#17855)
14	2	tests/lora/conftest.py
134	0	tests/lora/test_lora_allowed_token_ids.py
6	3	vllm/v1/engine/processor.py

[eea22a56a] Shiyan Deng 2025-05-11 fix amd triton mla path (#17871)
1	1	vllm/attention/backends/mla/common.py

[911215528] Kuntai Du 2025-05-11 [Perf] Use small max_num_batched_tokens for A100 (#17885)
5	1	vllm/engine/arg_utils.py

[90d0a74b6] xinli-centml 2025-05-11 [Bugfix] Add revision to `transformers.Auto*.from_pretrained` processors (#17948)
10	1	vllm/transformers_utils/processor.py

[d74e5f37b] Jinzhen Lin 2025-05-11 [Kernel] fp4 marlin kernel (#17687)
3	0	csrc/core/scalar_type.hpp
11	2	csrc/moe/marlin_moe_wna16/generate_kernels.py
12	11	csrc/moe/marlin_moe_wna16/kernel.h
99	40	csrc/moe/marlin_moe_wna16/marlin_template.h
66	18	csrc/moe/marlin_moe_wna16/ops.cu
2	1	csrc/moe/torch_bindings.cpp
286	70	csrc/quantization/gptq_marlin/dequant.h
11	2	csrc/quantization/gptq_marlin/generate_kernels.py
65	16	csrc/quantization/gptq_marlin/gptq_marlin.cu
8	7	csrc/quantization/gptq_marlin/kernel.h
85	36	csrc/quantization/gptq_marlin/marlin_template.h
2	2	csrc/torch_bindings.cpp
103	32	tests/kernels/moe/test_moe.py
41	73	tests/kernels/quantization/test_marlin_gemm.py
12	8	vllm/_custom_ops.py
11	3	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
3	1	vllm/model_executor/layers/quantization/hqq_marlin.py
80	4	vllm/model_executor/layers/quantization/modelopt.py
14	2	vllm/model_executor/layers/quantization/utils/marlin_utils.py
277	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
24	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[ca66a1674] Chen Zhang 2025-05-11 [v1] Rename specialized_manager.py to single_type_kv_cache_manager.py (#17946)
1	1	tests/v1/core/test_specialized_manager.py
2	1	vllm/v1/core/kv_cache_manager.py
0	0	vllm/v1/core/{specialized_manager.py => single_type_kv_cache_manager.py}

[950751a98] Chen Zhang 2025-05-11 [v1] Pass BlockTable and KVCacheSpec to AttentionMetadataBuilders (#17483)
3	0	tests/v1/worker/test_gpu_input_batch.py
20	1	tests/v1/worker/test_gpu_model_runner.py
30	17	vllm/v1/attention/backends/flash_attn.py
20	15	vllm/v1/attention/backends/flashinfer.py
15	8	vllm/v1/attention/backends/mla/common.py
7	4	vllm/v1/attention/backends/mla/flashmla.py
5	2	vllm/v1/attention/backends/mla/rocm_aiter_mla.py
11	0	vllm/v1/worker/block_table.py
3	0	vllm/v1/worker/gpu_input_batch.py
9	12	vllm/v1/worker/gpu_model_runner.py
9	9	vllm/v1/worker/tpu_model_runner.py

[4c31218f8] Reid 2025-05-10 [Misc] remove --model from vllm serve usage (#17944)
2	2	examples/online_serving/openai_chat_completion_client_with_tools.py

[68311891f] Harry Mellor 2025-05-10 Don't default construct `ModelConfig` when default constructing `VllmConfig` (#17943)
3	1	vllm/config.py

[fc4441a4e] Ximo Guanter 2025-05-10 Add missing content type headers to /ping and /health (#17036) (#17786)
2	2	tests/entrypoints/openai/test_openai_schema.py
3	3	vllm/entrypoints/openai/api_server.py

[246e3e0a3] tracelogfb 2025-05-09 fix broken test vllm:test_kernels - test_attention_selector.py::test_flash_attn (#17873)
3	2	tests/kernels/attention/test_attention_selector.py

[7042cc96b] Mark McLoughlin 2025-05-10 [V1][Spec Decoding] Log accumulated metrics after system goes idle (#17913)
1	3	vllm/v1/metrics/loggers.py
2	0	vllm/v1/spec_decode/metrics.py

[0c0fdae84] Pavani Majety 2025-05-09 [Hardware/NVIDIA/Kernel] Enable nvidia/DeepSeek-R1-FP4 Model (#16362)
5	2	CMakeLists.txt
408	0	benchmarks/kernels/benchmark_cutlass_fp4_moe.py
12	0	csrc/ops.h
11	7	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
402	0	csrc/quantization/fp4/nvfp4_blockwise_moe_kernel.cu
404	0	csrc/quantization/fp4/nvfp4_experts_quant.cu
23	1	csrc/quantization/fp4/nvfp4_quant_entry.cu
15	0	csrc/torch_bindings.cpp
144	0	tests/kernels/moe/test_nvfp4_moe.py
66	0	tests/kernels/quantization/nvfp4_utils.py
14	84	tests/kernels/quantization/test_nvfp4_scaled_mm.py
91	7	vllm/_custom_ops.py
2	1	vllm/model_executor/layers/fused_moe/__init__.py
125	1	vllm/model_executor/layers/fused_moe/cutlass_moe.py
20	3	vllm/model_executor/layers/fused_moe/layer.py
252	6	vllm/model_executor/layers/quantization/modelopt.py

[3b602cdea] Alexei-V-Ivanov-AMD 2025-05-09 AMD conditional all test execution // new test groups (#17556)
16	0	.buildkite/scripts/hardware_ci/run-amd-test.sh
51	25	.buildkite/test-pipeline.yaml
2	0	requirements/rocm-test.txt

[4b2ed7926] Harry Mellor 2025-05-09 Improve configs - the rest! (#17562)
1	4	tests/compile/test_full_graph.py
3	4	tests/compile/test_functionalization.py
3	3	tests/compile/test_fusion.py
3	4	tests/compile/test_sequence_parallelism.py
2	3	tests/compile/test_silu_mul_quant_fusion.py
2	2	tests/distributed/test_sequence_parallel.py
61	15	tests/engine/test_arg_utils.py
2	5	vllm/compilation/vllm_inductor_pass.py
303	220	vllm/config.py
2	1	vllm/distributed/kv_events.py
56	75	vllm/engine/arg_utils.py
9	4	vllm/entrypoints/llm.py
6	5	vllm/platforms/tpu.py
8	0	vllm/utils.py

[7e3571134] Mark McLoughlin 2025-05-09 [V1][Spec Decoding] Include bonus tokens in mean acceptance length (#17908)
2	2	examples/offline_inference/eagle.py
7	3	vllm/v1/spec_decode/metrics.py

[ea2236bf9] Richard Zou 2025-05-09 Add option to use torch._inductor.standalone_compile (#17057)
27	6	vllm/compilation/backends.py
118	23	vllm/compilation/compiler_interface.py
5	0	vllm/envs.py

[7d4aedae7] Harry Mellor 2025-05-09 Handle error when `str` passed to `/v1/audio/transcriptions` (#17909)
9	2	vllm/entrypoints/openai/protocol.py

[22481fbfa] Michael Goin 2025-05-09 Update CT WNA16MarlinMoE integration (#16666)
38	81	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[5c4c08f6f] Isotr0py 2025-05-10 [Misc] Auto fallback to float16 for pre-Ampere GPUs when detected bfloat16 config (#17265)
21	25	vllm/config.py
15	1	vllm/platforms/cpu.py
13	0	vllm/platforms/cuda.py
8	0	vllm/platforms/interface.py

[c44c384b1] Rui Qiao 2025-05-09 [Misc] Add references in ray_serve_deepseek example (#17907)
3	1	examples/online_serving/ray_serve_deepseek.py

[85b72cb7b] Michael Goin 2025-05-09 Revert "[BugFix][AMD] Compatible patch for latest AITER(05/07/2025)" (#17910)
6	6	vllm/attention/backends/mla/common.py
12	37	vllm/attention/backends/rocm_aiter_mla.py
1	6	vllm/attention/ops/rocm_aiter_mla.py
4	5	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[6e5595ca3] Cyrus Leung 2025-05-09 [CI/Build] Automatically retry flaky tests (#17856)
1	0	tests/kernels/moe/test_moe.py
0	33	tests/tensorizer_loader/conftest.py
1	2	tests/tensorizer_loader/test_tensorizer.py

[200da9a51] Chen Zhang 2025-05-09 [v1] Move block management logic from KVCacheManager to SpecializedManager (#17474)
3	3	tests/v1/core/test_kv_cache_utils.py
8	5	tests/v1/core/test_prefix_caching.py
8	5	tests/v1/core/test_scheduler.py
18	18	tests/v1/core/test_specialized_manager.py
35	100	vllm/v1/core/kv_cache_manager.py
197	24	vllm/v1/core/specialized_manager.py

[9f64e9341] qli88 2025-05-09 [BugFix][AMD] Compatible patch for latest AITER(05/07/2025) (#17864)
6	6	vllm/attention/backends/mla/common.py
37	12	vllm/attention/backends/rocm_aiter_mla.py
6	1	vllm/attention/ops/rocm_aiter_mla.py
5	4	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[ec61ea20a] Reid 2025-05-09 [Misc] add dify integration (#17895)
-	-	docs/source/assets/deployment/dify-chat.png
-	-	docs/source/assets/deployment/dify-create-chatbot.png
-	-	docs/source/assets/deployment/dify-settings.png
56	0	docs/source/deployment/frameworks/dify.md
1	0	docs/source/deployment/frameworks/index.md

[c6798baa9] Harry Mellor 2025-05-09 Change `top_k` to be disabled with `0` (still accept `-1` for now) (#17773)
1	1	tests/samplers/test_sampler.py
3	3	vllm/entrypoints/openai/protocol.py
1	1	vllm/model_executor/sampling_metadata.py
7	6	vllm/sampling_params.py
1	1	vllm/worker/neuron_model_runner.py
1	1	vllm/worker/tpu_model_runner.py

[5b2dcbf0b] inkcherry 2025-05-09 Fix Whisper crash caused by invalid``` max_num_batched_tokens``` config (#17853)
14	0	vllm/config.py

[6e4a93e3f] Isotr0py 2025-05-09 [Bugfix][CPU] Fix broken AVX2 CPU TP support (#17252)
2	1	vllm/distributed/device_communicators/cpu_communicator.py

[217db4baa] vllmellm 2025-05-09 [Bugfix][ROCm] Fix AITER MLA V1 (#17880)
1	3	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[ff8c40050] Yan Ma 2025-05-09 [Doc] remove visible token in doc (#17884)
1	1	docs/source/design/v1/prefix_caching.md

[89a0315f4] Michael Yao 2025-05-09 [Doc] Update several links in reasoning_outputs.md (#17846)
8	6	docs/source/features/reasoning_outputs.md

[3d1e38765] Simon Mo 2025-05-08 [Docs] Add Slides from NYC Meetup (#17879)
7	5	README.md
1	0	docs/source/community/meetups.md

[d310e6de9] Ning Xie 2025-05-09 [BUGFIX]: return fast when request requires prompt logprobs (#17251)
2	2	tests/v1/core/test_prefix_caching.py
5	5	vllm/v1/core/kv_cache_manager.py

[5e6f93948] Lucas Wilkinson 2025-05-08 [Attention] MLA move rotary embedding to cuda-graph region (#17668)
11	60	vllm/attention/backends/mla/common.py
2	4	vllm/attention/backends/rocm_aiter_mla.py
3	2	vllm/model_executor/layers/rotary_embedding.py
7	1	vllm/model_executor/models/deepseek_v2.py
11	51	vllm/v1/attention/backends/mla/common.py
1	3	vllm/v1/attention/backends/mla/flashmla.py

[760e3ecc8] Shanshan Shen 2025-05-09 [V1][Structured Output] Update llguidance (`>= 0.7.11`) to avoid AttributeError (no `StructTag`)  (#17839)
1	1	requirements/common.txt

[3c9396a64] vllmellm 2025-05-09 [FEAT][ROCm]: Support AITER MLA on V1 Engine (#17523)
1	1	docker/Dockerfile.rocm_base
4	1	tests/kernels/attention/test_attention_selector.py
4	2	tests/kernels/attention/test_rocm_attention_selector.py
46	0	vllm/attention/ops/rocm_aiter_mla.py
1	0	vllm/engine/arg_utils.py
1	1	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
2	1	vllm/platforms/interface.py
8	3	vllm/platforms/rocm.py
6	5	vllm/v1/attention/backends/mla/common.py
196	0	vllm/v1/attention/backends/mla/rocm_aiter_mla.py

[376786fac] Shu Wang 2025-05-08 Add cutlass support for blackwell fp8 blockwise gemm (#14383)
1	0	CMakeLists.txt
10	0	csrc/cutlass_extensions/common.hpp
27	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8.cu
205	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm100_fp8_dispatch.cuh
57	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_helper.hpp
5	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp
5	17	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm100.cu
5	46	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90.cu
2	0	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
3	1	tests/kernels/quantization/test_cutlass_scaled_mm.py
12	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[4f605a6de] Michael Goin 2025-05-08 Fix noisy warning for uncalibrated q_scale/p_scale (#17414)
5	4	vllm/model_executor/layers/quantization/kv_cache.py

[8342e3abd] Michael Goin 2025-05-08 [CI] Prune down lm-eval small tests (#17012)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-FP8-compressed-tensors.yaml
11	0	.buildkite/lm-eval-harness/configs/Qwen2.5-1.5B-Instruct.yaml
11	0	.buildkite/lm-eval-harness/configs/Qwen2.5-VL-3B-Instruct-FP8-dynamic.yaml
1	0	.buildkite/lm-eval-harness/configs/models-large.txt
2	6	.buildkite/lm-eval-harness/configs/models-small.txt

[a83a0f92b] yarongmu-google 2025-05-08 [Test] Attempt all TPU V1 tests, even if some of them fail. (#17334)
74	30	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[226a4272c] Russell Bryant 2025-05-08 [V1] Improve VLLM_ALLOW_INSECURE_SERIALIZATION logging (#17860)
10	7	vllm/v1/serial_utils.py

[ec54d73c3] Russell Bryant 2025-05-08 [CI] Fix test_collective_rpc (#17858)
2	1	tests/entrypoints/llm/test_collective_rpc.py

[a944f8ede] Jee Jee Li 2025-05-08 [Misc] Delete LoRA-related redundancy code (#17841)
1	1	vllm/lora/models.py
1	3	vllm/model_executor/models/grok1.py
0	8	vllm/model_executor/models/nemotron_nas.py
1	5	vllm/model_executor/models/phi4mm.py

[015815fe0] Cyrus Leung 2025-05-08 [Bugfix] `use_fast` failing to be propagated to Qwen2-VL image processor (#17838)
5	3	vllm/model_executor/models/qwen2_5_omni_thinker.py
5	3	vllm/model_executor/models/qwen2_5_vl.py
5	3	vllm/model_executor/models/qwen2_vl.py

[e4ca6e3a9] Harry Mellor 2025-05-08 Fix transient dependency error in docs build (#17848)
1	0	requirements/docs.txt

[53d0cb742] Reid 2025-05-08 [Misc] add chatbox integration (#17828)
-	-	docs/source/assets/deployment/chatbox-chat.png
-	-	docs/source/assets/deployment/chatbox-settings.png
36	0	docs/source/deployment/frameworks/chatbox.md
1	0	docs/source/deployment/frameworks/index.md

[f50dcb7c2] Lu Fang 2025-05-08 [Easy] Eliminate c10::optional usage in vllm/csrc (#17819)
2	2	csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
2	2	csrc/quantization/gptq_allspark/allspark_repack.cu
2	2	csrc/rocm/attention.cu
1	1	csrc/rocm/ops.h

[a1e19b635] Cyrus Leung 2025-05-08 [Doc] Fix a typo in the file name (#17836)
0	0	examples/offline_inference/{reproduciblity.py => reproducibility.py}

[bb239a730] fxmarty-amd 2025-05-08 [Bugfix] Fix quark fp8 format loading on AMD GPUs (#12612)
26	0	tests/quantization/test_quark.py
12	9	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py

[a463555de] Jevin Jiang 2025-05-08 [TPU] Fix the test_sampler (#17820)
1	1	tests/v1/tpu/test_sampler.py
1	1	vllm/v1/attention/backends/pallas.py

[ca04b97c9] Rick Yuan 2025-05-08 [Bugfix] Fix tool call template validation for Mistral models (#17644)
9	2	examples/tool_chat_template_mistral3.jinja

[0a9bbaa10] xsank 2025-05-08 [Misc] support model prefix & add deepseek vl2 tiny fused moe config (#17763)
15	9	benchmarks/kernels/benchmark_moe.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=896,device_name=NVIDIA_H20.json

[39956efb3] Qiong Zhou Huang 2025-05-07 [Bugfix] Fix bad words for Mistral models (#17753)
7	10	vllm/logits_process.py
2	8	vllm/sampling_params.py

[597051e56] Ximingwang-09 2025-05-08 [Qwen3]add qwen3-235b-bf16 fused moe config on A100 (#17715)
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json

[96722aa81] Cyrus Leung 2025-05-08 [Frontend] Chat template fallbacks for multimodal models (#17805)
6	3	docs/source/serving/multimodal_inputs.md
0	3	examples/template_florence2.jinja
0	3	examples/template_paligemma.jinja
0	3	examples/template_qwen_vl.jinja
18	2	tests/entrypoints/openai/test_chat_template.py
96	19	tests/entrypoints/test_chat_utils.py
4	2	tests/models/registry.py
26	12	vllm/entrypoints/chat_utils.py
2	2	vllm/entrypoints/llm.py
2	1	vllm/entrypoints/openai/api_server.py
2	2	vllm/entrypoints/openai/serving_engine.py
4	0	vllm/transformers_utils/chat_templates/__init__.py
59	0	vllm/transformers_utils/chat_templates/registry.py
0	0	examples/template_chameleon.jinja => vllm/transformers_utils/chat_templates/template_basic.jinja
0	0	{examples => vllm/transformers_utils/chat_templates}/template_blip2.jinja
0	0	examples/template_qwen_vl_chat.jinja => vllm/transformers_utils/chat_templates/template_chatml.jinja
0	0	{examples => vllm/transformers_utils/chat_templates}/template_deepseek_vl2.jinja
0	0	{examples => vllm/transformers_utils/chat_templates}/template_fuyu.jinja

[843b22272] Agata Dobrzyniewicz 2025-05-08 [Hardware][Intel-Gaudi] Support Automatic Prefix Caching on HPU (#17648)
20	12	vllm/attention/backends/hpu_attn.py
9	27	vllm/attention/ops/hpu_paged_attn.py
117	17	vllm/worker/hpu_model_runner.py

[e515668ed] Akash kaothalkar 2025-05-08 [Hardware][Power] Enable compressed tensor W8A8 INT8 quantization for POWER (#17153)
32	1	cmake/cpu_extension.cmake
265	0	csrc/cpu/cpu_types_vsx.hpp
337	4	csrc/cpu/quant.cpp
35	0	csrc/cpu/torch_bindings.cpp

[5a499e70d] Hashem Hashemi 2025-05-07 [Kernel][Hardware][AMD] Bf16 mfma opt for ROCm skinny GEMMs (#17071)
313	226	csrc/rocm/skinny_gemms.cu
1	1	tests/kernels/quantization/test_rocm_skinny_gemms.py
1	1	vllm/model_executor/layers/utils.py
1	0	vllm/platforms/rocm.py

[6930a4111] Russell Bryant 2025-05-08 [V1] Add VLLM_ALLOW_INSECURE_SERIALIZATION env var (#17490)
50	34	tests/v1/entrypoints/llm/test_struct_output_generate.py
78	73	tests/v1/test_serial_utils.py
7	0	vllm/envs.py
35	8	vllm/v1/serial_utils.py

[998eea4a0] Harry Mellor 2025-05-08 Only log non-default CLI args for online serving (#17803)
3	2	vllm/entrypoints/openai/api_server.py
12	0	vllm/entrypoints/openai/cli_args.py

[c747d8457] Mikhail Podvitskii 2025-05-08 [Installation] OpenTelemetry version update (#17771)
4	4	requirements/common.txt

[b2da14a05] Vadim Markovtsev 2025-05-08 Improve exception reporting in MP engine (#17800)
2	2	vllm/engine/multiprocessing/engine.py

[7ea2adb80] Chanh Nguyen 2025-05-07 [Core] Support full cuda graph in v1 (#16072)
6	0	docs/source/design/v1/torch_compile.md
97	0	tests/compile/piecewise/test_full_cudagraph.py
17	2	vllm/config.py
10	3	vllm/v1/attention/backends/flash_attn.py
60	8	vllm/v1/worker/gpu_model_runner.py

[3d13ca0e2] Nick Hill 2025-05-07 [BugFix] Fix `--disable-log-stats` in V1 server mode (#17600)
3	2	vllm/v1/engine/async_llm.py
4	3	vllm/v1/engine/core_client.py

[66ab3b13c] Harry Mellor 2025-05-08 Don't call the venv `vllm` (#17810)
2	2	docs/source/getting_started/installation/python_env_setup.inc.md

[a8238bbdb] Aaron Pham 2025-05-07 [Chore][Doc] uses model id determined from OpenAI client (#17815)
1	1	examples/online_serving/openai_chat_completion_structured_outputs.py
1	1	examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py
1	1	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py

[d43f914d4] Wallas Henrique 2025-05-07 [Core][Feature] Input metadata dump on crash (#13407)
1	1	.github/ISSUE_TEMPLATE/400-bug-report.yml
43	6	tests/basic_correctness/test_basic_correctness.py
84	0	vllm/logging_utils/dump_input.py
27	0	vllm/v1/core/sched/output.py
14	2	vllm/v1/engine/core.py

[ed5272cf2] Nick Hill 2025-05-07 [BugFix] Avoid secondary missing `MultiprocExecutor.workers` error (#17811)
4	3	vllm/v1/executor/multiproc_executor.py

[c20ef40fd] Akshat Tripathi 2025-05-07 [Hardware][TPU][V1] Multi-LoRA implementation for the V1 TPU backend (#14238)
3	0	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
1	1	tests/lora/conftest.py
0	0	tests/tpu/lora/__init__.py
124	0	tests/tpu/lora/test_lora.py
73	0	tests/tpu/lora/test_pallas_kernels.py
3	2	vllm/config.py
29	10	vllm/lora/fully_sharded_layers.py
37	16	vllm/lora/layers.py
6	0	vllm/lora/ops/xla_ops/__init__.py
106	0	vllm/lora/ops/xla_ops/lora_ops.py
133	0	vllm/lora/ops/xla_ops/pallas.py
13	12	vllm/lora/punica_wrapper/punica_base.py
325	0	vllm/lora/punica_wrapper/punica_tpu.py
4	2	vllm/lora/punica_wrapper/utils.py
21	0	vllm/platforms/interface.py
17	1	vllm/platforms/tpu.py
22	2	vllm/v1/worker/tpu_model_runner.py
8	0	vllm/v1/worker/tpu_worker.py
4	0	vllm/worker/tpu_worker.py

[db593aa67] Bowen Bao 2025-05-07 [Quantization] Quark MXFP4 format loading  (#16943)
40	0	tests/models/quantization/test_mxfp4.py
9	0	vllm/envs.py
55	1	vllm/model_executor/layers/quantization/quark/quark.py
2	1	vllm/model_executor/layers/quantization/quark/schemes/__init__.py
125	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w4a4_mxfp4.py
45	0	vllm/model_executor/layers/quantization/utils/mxfp4_utils.py
1	1	vllm/model_executor/model_loader/utils.py
7	0	vllm/platforms/interface.py
5	0	vllm/platforms/rocm.py

[f98e30758] Isotr0py 2025-05-08 [Bugfix] Fix missing lora name mapping for lora without prefix (#17793)
57	12	tests/lora/test_utils.py
4	2	vllm/lora/utils.py

[646a31e51] Harry Mellor 2025-05-07 Fix and simplify `deprecated=True` CLI `kwarg` (#17781)
2	3	examples/offline_inference/basic/chat.py
2	3	examples/offline_inference/basic/generate.py
1	0	vllm/engine/arg_utils.py
22	56	vllm/utils.py

[be8ff88e6] Isotr0py 2025-05-07 [Bugfix] Fix Video IO error for short video (#17791)
17	0	tests/multimodal/test_utils.py
4	2	vllm/multimodal/video.py

[1a6af1453] Christian Heimes 2025-05-07 Only depend on importlib-metadata for Python < 3.10 (#17776)
1	1	requirements/common.txt

[32aa74c09] Gregory Shtrasberg 2025-05-07 [ROCm][FP8][Kernel] FP8 quantization fused into Custom Paged Attention (#17139)
61	31	csrc/rocm/attention.cu
9	11	csrc/rocm/ops.h
2	1	csrc/rocm/torch_bindings.cpp
2	1	vllm/_custom_ops.py

[7377dd030] Reid 2025-05-07 [doc] update the issue link (#17782)
1	1	docs/source/features/quantization/fp8.md
1	1	docs/source/features/quantization/int4.md
1	1	docs/source/features/quantization/int8.md

[98c89e16f] Yong Hoon Shin 2025-05-07 Make key optional for rotary embedding (#17566)
24	15	csrc/cpu/pos_encoding.cpp
1	1	csrc/cpu/torch_bindings.cpp
4	4	csrc/ops.h
55	43	csrc/pos_encoding_kernels.cu
2	2	csrc/torch_bindings.cpp
34	15	tests/kernels/core/test_pos_encoding.py
4	3	tests/kernels/core/test_rotary_embedding.py
21	12	tests/neuron/1_core/test_rotary_embedding.py
8	6	vllm/_custom_ops.py
68	50	vllm/model_executor/layers/rotary_embedding.py

[324a3119b] Yong Hoon Shin 2025-05-07 Fix test_memory_usage_no_spec (#17754)
6	6	tests/spec_decode/test_memory_usage.py

[8a15c2603] Cyrus Leung 2025-05-07 [Frontend] Add missing chat templates for various MLLMs (#17758)
1	1	docs/source/serving/multimodal_inputs.md
1	1	examples/online_serving/openai_chat_completion_client_for_multimodal.py
3	0	examples/template_chameleon.jinja
1	5	examples/template_florence2.jinja
3	0	examples/template_fuyu.jinja
0	23	examples/template_llava.jinja
3	0	examples/template_paligemma.jinja
3	0	examples/template_qwen_vl.jinja
10	0	examples/template_qwen_vl_chat.jinja
5	1	tests/entrypoints/test_chat_utils.py
0	2	tests/v1/tpu/test_multimodal.py

[043e4c495] Satyajith Chilappagari 2025-05-07 Add NeuronxDistributedInference support, Speculative Decoding, Dynamic on-device sampling (#16357)
54	0	examples/offline_inference/neuron_eagle.py
64	0	examples/offline_inference/neuron_speculation.py
2	1	requirements/neuron.txt
126	0	tests/neuron/1_core/test_neuron_model_runner.py
5	1	vllm/config.py
2	4	vllm/engine/llm_engine.py
237	4	vllm/model_executor/model_loader/neuron.py
584	0	vllm/model_executor/model_loader/neuronx_distributed.py
13	4	vllm/platforms/__init__.py
76	3	vllm/platforms/neuron.py
81	0	vllm/worker/multi_step_neuron_model_runner.py
60	0	vllm/worker/multi_step_neuronx_distributed_model_runner.py
130	52	vllm/worker/neuron_model_runner.py
53	33	vllm/worker/neuron_worker.py
136	0	vllm/worker/neuronx_distributed_model_runner.py

[ba7703e65] Jee Jee Li 2025-05-07 [Misc] Remove  qlora_adapter_name_or_path (#17699)
26	31	examples/offline_inference/lora_with_quantization_inference.py
17	17	vllm/engine/arg_utils.py
7	15	vllm/model_executor/model_loader/weight_utils.py

[f80ae5bdc] Wanrui Dai 2025-05-07 [Kernel] Use fused rmsnorm for some models like qwen3 series (#17735)
4	0	csrc/layernorm_kernels.cu
3	1	vllm/_custom_ops.py
4	6	vllm/model_executor/models/intern_vit.py
2	2	vllm/model_executor/models/molmo.py
2	2	vllm/model_executor/models/olmo2.py
2	2	vllm/model_executor/models/qwen3.py
2	2	vllm/model_executor/models/qwen3_moe.py

[1a45a6138] Szymon Ożóg 2025-05-07 [Kernel] GGUF MoeVec kernel (#16780)
4	0	csrc/ops.h
137	0	csrc/quantization/gguf/gguf_kernel.cu
338	0	csrc/quantization/gguf/moe_vec.cuh
6	0	csrc/torch_bindings.cpp
6	0	tests/kernels/quantization/test_ggml.py
5	15	tests/kernels/quantization/test_gguf.py
31	0	vllm/_custom_ops.py
17	1	vllm/model_executor/layers/quantization/gguf.py

[c3e9d5060] Isotr0py 2025-05-07 [Misc] Use `apply_rotary_emb` from vllm_flash_attn for Qwen2-VL vision RoPE (#17726)
2	7	vllm/model_executor/models/qwen2_5_vl.py
4	5	vllm/model_executor/models/qwen2_vl.py

[822de7fb9] Jee Jee Li 2025-05-07 [Misc] Split model loader (#17712)
2	3	tests/runai_model_streamer_test/test_runai_model_streamer_loader.py
1	1	tests/test_sharded_state_loader.py
1	1	tests/utils.py
1	1	vllm/config.py
53	5	vllm/model_executor/model_loader/__init__.py
23	0	vllm/model_executor/model_loader/base_loader.py
568	0	vllm/model_executor/model_loader/bitsandbytes_loader.py
293	0	vllm/model_executor/model_loader/default_loader.py
37	0	vllm/model_executor/model_loader/dummy_loader.py
113	0	vllm/model_executor/model_loader/gguf_loader.py
0	1544	vllm/model_executor/model_loader/loader.py
120	0	vllm/model_executor/model_loader/runai_streamer_loader.py
210	0	vllm/model_executor/model_loader/sharded_state_loader.py
119	0	vllm/model_executor/model_loader/tensorizer_loader.py
131	2	vllm/model_executor/model_loader/utils.py
2	2	vllm/model_executor/models/mllama4.py
1	1	vllm/model_executor/models/ultravox.py
2	2	vllm/model_executor/models/utils.py
1	1	vllm/v1/spec_decode/eagle.py
1	1	vllm/v1/worker/gpu_worker.py
2	2	vllm/worker/model_runner.py

[8d84d836d] Woosuk Kwon 2025-05-06 [BugFix][Spec Decode] Fix hidden size mismatch between target and eagle head (#17740)
13	14	vllm/v1/spec_decode/eagle.py

[950b71186] Michael Goin 2025-05-06 Replace lm-eval bash script with pytest and use enforce_eager for faster CI (#17717)
39	0	.buildkite/lm-eval-harness/conftest.py
0	59	.buildkite/lm-eval-harness/run-tests.sh
11	30	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
2	2	.buildkite/test-pipeline.yaml

[e50a1f1a9] Michael Goin 2025-05-06 [TPU] Add kernel test for moe_pallas (#17496)
3	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
87	0	tests/tpu/test_moe_pallas.py
2	1	vllm/attention/backends/pallas.py
4	1	vllm/model_executor/layers/fused_moe/moe_pallas.py

[a17cef70e] Michael Goin 2025-05-06 Removed unused marlin cuda code (#17684)
0	1616	csrc/moe/marlin_kernels/marlin_moe_kernel.h
0	31	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.cu
0	20	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.h
0	31	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.cu
0	20	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.h
0	31	csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.cu
0	18	csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.h
0	588	csrc/moe/marlin_moe_ops.cu
0	1311	csrc/quantization/fp8/fp8_marlin.cu

[18dd5e01f] Chih-Chieh Yang 2025-05-06 [Model] Mamba2 causal conv1d Refactor to Split Prefill and Decode Requests for Corresponding Kernels (#17146)
4	3	tests/kernels/mamba/test_mamba_ssm_ssd.py
45	43	vllm/model_executor/layers/mamba/mamba2_metadata.py
104	74	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	1	vllm/model_executor/layers/mamba/ops/ssd_combined.py
0	1	vllm/model_executor/models/bamba.py
0	1	vllm/model_executor/models/granitemoehybrid.py
0	1	vllm/model_executor/models/mamba2.py
0	1	vllm/model_executor/models/zamba2.py

[6de3e1341] Yang Wang 2025-05-06 Add logging for torch nightly version (#17669)
3	1	docker/Dockerfile.nightly_torch
9	1	requirements/nightly_torch_test.txt

[ed3a1d210] Hongxia Yang 2025-05-06 [ROCm] fix num_stages for default moe config to avoid triton OutOfResource error (#17744)
3	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[022afbeb4] Harry Mellor 2025-05-07 Fix doc build performance (#17748)
0	3	docs/source/conf.py
4	1	requirements/docs.txt

[2f925e577] Thomas Parnell 2025-05-06 [Kernel] Unified Triton kernel that doesn't distinguish between prefill + decode (#16828)
189	0	tests/kernels/test_triton_unified_attention.py
333	0	vllm/attention/ops/triton_unified_attention.py
44	27	vllm/v1/attention/backends/triton_attn.py

[de906b95f] Gregory Shtrasberg 2025-05-06 [Bugfix] Fix for the condition to accept empty encoder inputs for mllama (#17732)
1	1	vllm/engine/llm_engine.py

[d456aea71] d.transposed 2025-05-06 [Misc] Add Next Edit Prediction (NEP) datasets support in `benchmark_serving.py` (#16839)
88	0	benchmarks/benchmark_dataset.py
6	2	benchmarks/benchmark_serving.py
88	0	vllm/benchmarks/datasets.py

[621ca2c0a] Jevin Jiang 2025-05-06 [TPU] Increase block size and reset block shapes (#16458)
2	1	examples/offline_inference/tpu.py
5	5	requirements/tpu.txt
6	4	vllm/platforms/tpu.py
7	0	vllm/utils.py
15	1	vllm/v1/attention/backends/pallas.py

[6115b1158] Harry Mellor 2025-05-06 Make right sidebar more readable in "Supported Models" (#17723)
24	8	docs/source/models/supported_models.md

[5b8c39074] Cyrus Leung 2025-05-07 [Bugfix] Fix modality limits in vision language example (#17721)
36	36	examples/offline_inference/vision_language.py

[7525d5f3d] Reid 2025-05-07 [doc] Add RAG Integration example (#17692)
1	0	docs/source/deployment/frameworks/index.md
84	0	docs/source/deployment/frameworks/retrieval_augmented_generation.md
249	0	examples/online_serving/retrieval_augmented_generation_with_langchain.py
217	0	examples/online_serving/retrieval_augmented_generation_with_llamaindex.py

[aabcd2cae] Chen Zhang 2025-05-06 [v1] Introduce KVCacheBlocks as interface between Scheduler and KVCacheManager (#17479)
3	3	tests/v1/core/test_kv_cache_utils.py
70	69	tests/v1/core/test_prefix_caching.py
44	21	vllm/v1/core/kv_cache_manager.py
4	6	vllm/v1/core/sched/scheduler.py

[0d115460a] Michael Yao 2025-05-06 [Docs] Use gh-file to add links to tool_calling.md (#17709)
14	14	docs/source/features/tool_calling.md

[175bda67a] Aaron Pham 2025-05-06 [Feat] Add deprecated=True to CLI args (#17426)
1	0	vllm/engine/arg_utils.py
76	2	vllm/utils.py

[cba31c47c] Chen Zhang 2025-05-06 [v1] AttentionMetadata for each layer (#17394)
12	3	vllm/attention/layer.py
8	3	vllm/forward_context.py
5	6	vllm/v1/attention/backends/flash_attn.py
5	5	vllm/v1/attention/backends/flashinfer.py
5	5	vllm/v1/attention/backends/mla/common.py
18	0	vllm/v1/attention/backends/utils.py
10	1	vllm/v1/spec_decode/eagle.py
47	21	vllm/v1/worker/gpu_model_runner.py
16	2	vllm/v1/worker/tpu_model_runner.py

[a6fed0206] Li, Jiang 2025-05-06 [V1][PP] Support PP for MultiprocExecutor (#14219)
7	3	tests/distributed/test_pipeline_parallel.py
3	4	vllm/engine/arg_utils.py
69	18	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/worker/gpu_model_runner.py
18	2	vllm/v1/worker/gpu_worker.py

[d419aa5dc] Michael Goin 2025-05-06 [V1] Enable TPU V1 backend by default (#17673)
4	3	vllm/engine/arg_utils.py

[f9bc5a069] Mengqing Cao 2025-05-06 [Bugfix] Fix triton import with local TritonPlaceholder (#17446)
1	1	benchmarks/kernels/benchmark_moe.py
1	1	benchmarks/kernels/benchmark_rmsnorm.py
1	1	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py
1	1	tests/kernels/attention/test_flashmla.py
92	0	tests/test_triton_utils.py
2	2	vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
2	1	vllm/attention/ops/blocksparse_attention/utils.py
1	2	vllm/attention/ops/chunked_prefill_paged_decode.py
1	2	vllm/attention/ops/prefix_prefill.py
1	3	vllm/attention/ops/triton_decode_attention.py
1	2	vllm/attention/ops/triton_flash_attention.py
2	2	vllm/attention/ops/triton_merge_attn_states.py
1	2	vllm/lora/ops/triton_ops/kernel_utils.py
1	2	vllm/model_executor/layers/fused_moe/fused_moe.py
1	2	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
2	2	vllm/model_executor/layers/lightning_attn.py
1	3	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
2	2	vllm/model_executor/layers/mamba/ops/ssd_bmm.py
2	2	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
2	2	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
2	1	vllm/model_executor/layers/mamba/ops/ssd_combined.py
2	2	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
2	2	vllm/model_executor/layers/quantization/awq_triton.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
1	2	vllm/model_executor/layers/quantization/utils/fp8_utils.py
1	2	vllm/model_executor/layers/quantization/utils/int8_utils.py
10	2	vllm/triton_utils/__init__.py
31	29	vllm/triton_utils/importing.py
1	2	vllm/v1/sample/rejection_sampler.py
1	2	vllm/v1/spec_decode/eagle.py

[05e1f9641] Harry Mellor 2025-05-06 Fix `dockerfilegraph` pre-commit hook (#17698)
0	2	.pre-commit-config.yaml
-	-	docs/source/assets/contributing/dockerfile-stages-dependency.png
6	3	tools/update-dockerfile-graph.sh

[6eae34533] Lucas Wilkinson 2025-05-06 [Misc] Fix ScalarType float4 naming  (#17690)
1	1	tests/kernels/quantization/test_nvfp4_quant.py
1	1	tests/test_scalartype.py
1	1	vllm/scalar_type.py

[63ced7b43] Cyrus Leung 2025-05-06 [Doc] Update notes for H2O-VL and Gemma3 (#17219)
1	6	docs/source/models/supported_models.md

[dc47ba32f] Mikhail Podvitskii 2025-05-06 [Bugfix] Fixed prompt length for random dataset (#17408)
15	2	benchmarks/benchmark_dataset.py

[edbf2d609] Richard Zou 2025-05-06 [easy] Fix logspam on PiecewiseBackend errors (#17138)
2	0	vllm/compilation/backends.py

[999328be0] Stan Wozniak 2025-05-06 [Model] Add GraniteMoeHybrid 4.0 model (#17497)
5	0	docs/source/models/supported_models.md
41	0	tests/models/language/generation/test_granitemoehybrid.py
3	0	tests/models/language/generation/test_hybrid.py
2	0	tests/models/registry.py
585	0	vllm/model_executor/models/granitemoehybrid.py
1	0	vllm/model_executor/models/registry.py

[98834fefa] Michael Goin 2025-05-05 Update nm to rht in doc links + refine fp8 doc (#17678)
15	71	docs/source/features/quantization/fp8.md
1	1	docs/source/serving/offline_inference.md

[90bd2ae17] Varun Sundar Rabindranath 2025-05-06 [Bugfix] LoRA - Retire unused maxnreg LoRA kernel argument (#17677)
0	2	vllm/lora/ops/triton_ops/lora_expand_op.py
0	2	vllm/lora/ops/triton_ops/lora_shrink_op.py

[5941e0b7e] Nicolò Lucchesi 2025-05-05 [TPU][V1] Add support for top-logprobs (#17072)
48	0	tests/v1/tpu/test_sampler.py
9	4	vllm/v1/sample/tpu/metadata.py
2	12	vllm/v1/sample/tpu/sampler.py
46	1	vllm/v1/worker/tpu_model_runner.py

[976594082] XiongfeiWei 2025-05-05 [TPU] Enable gemma3-27b with TP>1 on multi-chips. (#17335)
43	0	tests/v1/tpu/test_basic.py
1	0	vllm/platforms/tpu.py

[5ea5c514d] Nick Hill 2025-05-05 [BugFix] Increase timeout for startup failure test (#17642)
21	14	tests/v1/engine/test_engine_core_client.py

[d3efde817] Russell Bryant 2025-05-05 [Benchmarks] Remove invalid option under V1 engine (#17651)
0	12	benchmarks/benchmark_serving_structured_output.py
3	8	benchmarks/run_structured_output_benchmark.sh

[aea302be6] Thomas J. Fan 2025-05-05 Use git-path commit in hook (#17616)
2	2	.pre-commit-config.yaml

[cc05b90d8] Isotr0py 2025-05-06 [Doc] Fix broken cuda installation doc rendering (#17654)
2	2	requirements/docs.txt

[1d0c9d6b2] Jinzhen Lin 2025-05-06 [Kernel] some optimizations for dense marlin and moe marlin (#16850)
46	2	CMakeLists.txt
1	0	csrc/moe/marlin_moe_wna16/.gitignore
12	8	csrc/moe/marlin_moe_wna16/generate_kernels.py
4	6	csrc/moe/marlin_moe_wna16/kernel.h
208	260	csrc/moe/marlin_moe_wna16/marlin_template.h
155	190	csrc/moe/marlin_moe_wna16/ops.cu
1	0	csrc/quantization/gptq_marlin/.gitignore
291	0	csrc/quantization/gptq_marlin/dequant.h
116	0	csrc/quantization/gptq_marlin/generate_kernels.py
442	2063	csrc/quantization/gptq_marlin/gptq_marlin.cu
37	0	csrc/quantization/gptq_marlin/kernel.h
1678	0	csrc/quantization/gptq_marlin/marlin_template.h
4	13	csrc/torch_bindings.cpp
44	121	tests/kernels/moe/test_moe.py
0	164	tests/kernels/quantization/test_awq_marlin.py
38	103	tests/kernels/quantization/test_marlin_gemm.py
14	31	vllm/_custom_ops.py
19	170	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
11	13	vllm/model_executor/layers/quantization/awq_marlin.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
57	42	vllm/model_executor/layers/quantization/fp8.py
11	8	vllm/model_executor/layers/quantization/gptq_marlin.py
2	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
60	16	vllm/model_executor/layers/quantization/utils/marlin_utils.py
237	42	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
11	0	vllm/scalar_type.py

[f62cad643] Tyler Michael Smith 2025-05-04 [Build/CI] Upgrade CUTLASS to 3.9.2 (#17641)
1	1	CMakeLists.txt

[5394ad738] Chauncey 2025-05-05 [Bugfix] fix KeyError on top logprobs are special tokens (#17637)
2	1	vllm/entrypoints/openai/serving_chat.py

[68e1ee007] Tyler Michael Smith 2025-05-04 [Bugfix][Easy] Fix whitespace in shm_broadcast.py logging (#17635)
1	1	vllm/distributed/device_communicators/shm_broadcast.py

[2858830c3] Cyrus Leung 2025-05-04 [Bugfix] Prioritize dtype in root config before checking text config (#17629)
4	2	vllm/config.py

[d6484ef3c] Harry Mellor 2025-05-04 Add full API docs and improve the UX of navigating them (#17485)
1	1	.buildkite/test-pipeline.yaml
1	0	.gitignore
1	0	docs/Makefile
0	7	docs/source/api/engine/async_llm_engine.md
0	17	docs/source/api/engine/index.md
0	7	docs/source/api/engine/llm_engine.md
0	21	docs/source/api/inference_params.md
0	9	docs/source/api/model/adapters.md
0	11	docs/source/api/model/index.md
0	9	docs/source/api/model/interfaces.md
0	9	docs/source/api/model/interfaces_base.md
0	28	docs/source/api/multimodal/index.md
0	49	docs/source/api/multimodal/inputs.md
0	9	docs/source/api/multimodal/parse.md
0	9	docs/source/api/multimodal/processing.md
0	9	docs/source/api/multimodal/profiling.md
0	9	docs/source/api/multimodal/registry.md
0	9	docs/source/api/offline_inference/index.md
0	7	docs/source/api/offline_inference/llm.md
0	19	docs/source/api/offline_inference/llm_inputs.md
133	0	docs/source/api/summary.md
21	0	docs/source/autodoc2_docstring_parser.py
66	70	docs/source/conf.py
2	2	docs/source/design/arch_overview.md
3	3	docs/source/features/compatibility_matrix.md
2	5	docs/source/index.md
1	1	docs/source/models/generative_models.md
1	1	docs/source/models/pooling_models.md
2	2	docs/source/serving/offline_inference.md
1	1	examples/offline_inference/profiling.py
2	14	requirements/docs.txt
4	4	tests/conftest.py
1	1	tests/tokenization/test_get_eos.py
1	1	tests/utils.py
1	1	tests/v1/core/test_scheduler.py
2	0	vllm/attention/backends/mla/common.py
1	1	vllm/attention/backends/utils.py
1	1	vllm/compilation/compiler_interface.py
5	5	vllm/config.py
1	1	vllm/connections.py
1	0	vllm/distributed/kv_transfer/__init__.py
50	48	vllm/engine/async_llm_engine.py
54	51	vllm/engine/llm_engine.py
2	2	vllm/engine/multiprocessing/client.py
6	6	vllm/engine/multiprocessing/engine.py
2	2	vllm/engine/output_processor/multi_step.py
6	6	vllm/engine/output_processor/single_step.py
33	29	vllm/entrypoints/llm.py
1	18	vllm/entrypoints/openai/protocol.py
2	2	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/executor/executor_base.py
1	1	vllm/inputs/__init__.py
21	21	vllm/inputs/data.py
11	11	vllm/inputs/preprocess.py
4	4	vllm/inputs/registry.py
3	3	vllm/logger.py
2	2	vllm/lora/ops/triton_ops/__init__.py
0	0	vllm/lora/ops/triton_ops/{lora_expand.py => lora_expand_op.py}
0	0	vllm/lora/ops/triton_ops/{lora_shrink.py => lora_shrink_op.py}
17	14	vllm/model_executor/layers/rejection_sampler.py
1	1	vllm/model_executor/layers/sampler.py
8	7	vllm/model_executor/layers/typical_acceptance_sampler.py
3	2	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/interfaces.py
3	2	vllm/model_executor/models/llava.py
4	3	vllm/model_executor/models/llava_next.py
3	2	vllm/model_executor/models/mistral3.py
1	1	vllm/model_executor/models/molmo.py
2	2	vllm/model_executor/models/phi4mm_utils.py
2	2	vllm/model_executor/models/pixtral.py
1	1	vllm/model_executor/models/qwen_vl.py
5	7	vllm/model_executor/models/registry.py
1	1	vllm/model_executor/models/utils.py
4	3	vllm/multimodal/__init__.py
22	22	vllm/multimodal/base.py
129	126	vllm/multimodal/inputs.py
5	5	vllm/multimodal/parse.py
122	122	vllm/multimodal/processing.py
2	2	vllm/multimodal/profiling.py
11	9	vllm/multimodal/registry.py
23	18	vllm/multimodal/utils.py
0	2	vllm/platforms/cpu.py
1	7	vllm/platforms/cuda.py
3	3	vllm/platforms/interface.py
0	7	vllm/profiler/__init__.py
7	7	vllm/sequence.py
2	1	vllm/spec_decode/smaller_tp_proposer_worker.py
1	2	vllm/transformers_utils/configs/dbrx.py
22	22	vllm/transformers_utils/configs/exaone.py
4	4	vllm/transformers_utils/tokenizer.py
7	17	vllm/utils.py
2	0	vllm/v1/attention/backends/mla/common.py
2	0	vllm/v1/core/kv_cache_manager.py
1	3	vllm/v1/engine/output_processor.py
1	1	vllm/v1/sample/rejection_sampler.py
4	3	vllm/v1/worker/gpu_worker.py
3	3	vllm/v1/worker/utils.py
4	3	vllm/worker/hpu_worker.py
2	2	vllm/worker/multi_step_model_runner.py
4	3	vllm/worker/worker.py
5	4	vllm/worker/xpu_worker.py

[46fae69cf] Cyrus Leung 2025-05-04 [Misc] V0 fallback for `--enable-prompt-embeds` (#17615)
6	0	vllm/engine/arg_utils.py
0	3	vllm/inputs/preprocess.py

[f66f1e0fa] Isotr0py 2025-05-04 [Bugfix] Fix broken Qwen2.5-omni tests (#17613)
5	3	tests/models/multimodal/generation/test_common.py
8	0	tests/models/multimodal/generation/vlm_utils/model_utils.py
1	1	tests/models/multimodal/processing/test_common.py
7	4	tests/models/registry.py

[887d7af88] Cyrus Leung 2025-05-04 [Core] Gate `prompt_embeds` behind a feature flag (#17607)
60	0	tests/engine/test_options.py
0	29	tests/engine/test_skip_tokenizer_init.py
6	2	tests/models/language/generation/test_common.py
3	0	tests/worker/test_model_runner.py
4	0	vllm/config.py
4	0	vllm/engine/arg_utils.py
4	1	vllm/inputs/preprocess.py
3	1	vllm/worker/model_runner.py

[a92842454] Gregory Shtrasberg 2025-05-03 [Bugfix][ROCm] Using device_type because on ROCm the API is still torch.cuda (#17601)
2	2	vllm/platforms/interface.py

[c8386fa61] Tyler Michael Smith 2025-05-03 [Build/CI] Upgrade CUTLASS to 3.9.1 (#17602)
3	4	CMakeLists.txt

[87baebebd] Chenyaaang 2025-05-02 [Frontend][TPU] Add TPU default max-num-batched-tokens based on device name  (#17508)
8	0	vllm/config.py
29	3	vllm/engine/arg_utils.py
3	1	vllm/platforms/tpu.py

[e3d0a1d19] rasmith 2025-05-02 [Quantizaton] [AMD] Add support for running DeepSeek int8 w8a8 MoE on ROCm (#17558)
2	2	vllm/_custom_ops.py
27	1	vllm/model_executor/layers/quantization/utils/int8_utils.py

[d47b605ec] 22quinn 2025-05-02 Update test requirements to CUDA 12.8 (#17576)
1	1	.pre-commit-config.yaml
16	16	requirements/test.txt

[22c6f6397] Liangfu Chen 2025-05-02 [Neuron][Build] Require setuptools >= 77.0.3 for PEP 639 (#17603)
2	0	requirements/neuron.txt

[3ec97e2cc] Kevin H. Luu 2025-05-02 [release] Add command to clean up Docker containers/images in TPU release machine (#17606)
1	0	.buildkite/release-pipeline.yaml

[9b103a1d7] Eric Hartford 2025-05-02 fix typo in logging (#17605)
1	1	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[b90b0852e] Richard Zou 2025-05-02 [easy] Print number of needed GPUs in skip message (#17594)
2	1	tests/compile/test_basic_correctness.py

[9352cdb56] Xiaodong Wang 2025-05-02 [Hardware][AMD] Improve OAM device ID + llama4 Maverick MOE tuning (#16263)
17	2	benchmarks/kernels/benchmark_moe.py
200	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=AMD_Instinct_MI300X.json
14	1	vllm/platforms/rocm.py

[182f40ea8] Zhiyu 2025-05-02 Add NVIDIA TensorRT Model Optimizer in vLLM documentation (#17561)
1	0	docs/source/features/quantization/index.md
78	0	docs/source/features/quantization/modelopt.md
11	1	docs/source/features/quantization/supported_hardware.md

[3e887d2e0] Caleb_Du 2025-05-03 permute/unpermute kernel for moe optimization (#14568)
13	1	CMakeLists.txt
2	1	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
2	2	benchmarks/kernels/benchmark_moe.py
349	0	benchmarks/kernels/benchmark_moe_permute_unpermute.py
133	0	csrc/moe/moe_permute_unpermute_op.cu
53	0	csrc/moe/permute_unpermute_kernels/dispatch.h
229	0	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.cu
95	0	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.h
211	0	csrc/moe/permute_unpermute_kernels/moe_permute_unpermute_kernel.inl
22	0	csrc/moe/torch_bindings.cpp
2	1	tests/kernels/moe/test_moe.py
223	0	tests/kernels/moe/test_moe_permute_unpermute.py
2	1	tests/kernels/quantization/test_awq_marlin.py
4	2	tests/kernels/quantization/test_block_fp8.py
2	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
9	10	vllm/model_executor/layers/fused_moe/fused_moe.py
5	4	vllm/model_executor/layers/fused_moe/layer.py
116	0	vllm/model_executor/layers/fused_moe/moe_permute_unpermute.py
2	4	vllm/model_executor/models/arctic.py

[0f87d8f7b] Lucas Wilkinson 2025-05-02 [BugFix][Attention] Fix sliding window attention in V1 giving incorrect results (#17574)
35	1	vllm/v1/attention/backends/flash_attn.py

[4c33d6732] Hui Liu 2025-05-02 [Bugfix] fix tmp_out and exp_sums dimensions (#17438)
1	1	vllm/attention/ops/chunked_prefill_paged_decode.py

[cb234955d] Cyrus Leung 2025-05-02 [Misc] Clean up input processing (#17582)
8	6	tests/models/multimodal/pooling/test_intern_vit.py
0	4	vllm/engine/async_llm_engine.py
7	27	vllm/engine/llm_engine.py
3	0	vllm/engine/protocol.py
4	2	vllm/entrypoints/llm.py
18	5	vllm/inputs/data.py
8	19	vllm/inputs/parse.py
302	215	vllm/inputs/preprocess.py
7	5	vllm/multimodal/processing.py

[3a500cd0b] Reid 2025-05-02 [doc] miss result (#17589)
1	1	docs/source/features/quantization/fp8.md

[868c546da] Michael Goin 2025-05-02 Support W8A8 INT8 MoE for compressed-tensors (#16745)
135	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py

[99404f53c] Cyrus Leung 2025-05-02 [Security] Fix image hash collision (#17378)
-	-	tests/multimodal/assets/image1.png
-	-	tests/multimodal/assets/image2.png
61	0	tests/multimodal/test_hasher.py
22	10	vllm/multimodal/hasher.py

[785d75a03] Harry Mellor 2025-05-02 Automatically tell users that dict args must be valid JSON in CLI (#17577)
5	0	tests/engine/test_arg_utils.py
7	12	vllm/config.py
3	1	vllm/engine/arg_utils.py

[6d1479ca4] Reid 2025-05-02 [doc] add the print result (#17584)
3	0	docs/source/features/quantization/fp8.md

[b8b0859b5] Yang Wang 2025-05-02 add more pytorch related tests for torch nightly (#17422)
4	0	.buildkite/test-pipeline.yaml
7	1	requirements/nightly_torch_test.txt
3	3	vllm/sampling_params.py

[d7543862b] Cyrus Leung 2025-05-02 [Misc] Rename assets for testing (#17575)
2	2	examples/offline_inference/qwen2_5_omni/only_thinker.py
1	1	examples/offline_inference/vision_language.py
21	35	tests/conftest.py
11	10	tests/models/multimodal/generation/test_common.py
2	2	tests/models/multimodal/generation/test_florence2.py
5	4	tests/models/multimodal/generation/test_granite_speech.py
1	1	tests/models/multimodal/generation/test_interleaved.py
8	8	tests/models/multimodal/generation/test_mllama.py
1	1	tests/models/multimodal/generation/test_qwen2_vl.py
25	19	tests/models/multimodal/generation/test_ultravox.py
5	5	tests/models/multimodal/generation/vlm_utils/builders.py
4	4	tests/models/multimodal/generation/vlm_utils/model_utils.py
6	5	tests/models/multimodal/generation/vlm_utils/runners.py
3	3	tests/models/multimodal/generation/vlm_utils/types.py
2	2	tests/models/multimodal/pooling/test_intern_vit.py
2	2	tests/models/multimodal/processing/test_h2ovl.py
2	2	tests/models/multimodal/processing/test_idefics3.py
2	2	tests/models/multimodal/processing/test_internvl.py
2	2	tests/models/multimodal/processing/test_llama4.py
2	2	tests/models/multimodal/processing/test_minimax_vl_01.py
2	2	tests/models/multimodal/processing/test_phi3v.py
2	2	tests/models/multimodal/processing/test_phi4mm.py
2	2	tests/models/multimodal/processing/test_qwen2_vl.py
2	2	tests/models/multimodal/processing/test_smolvlm.py
2	2	tests/models/quantization/test_awq.py
9	3	vllm/assets/audio.py
3	1	vllm/assets/image.py
16	5	vllm/assets/video.py

[c777df79f] Robert Shaw 2025-05-02 [BugFix] Fix Memory Leak (#17567)
77	0	tests/v1/core/test_scheduler.py
4	1	vllm/v1/core/sched/scheduler.py

[cc2a77d7f] Andrew Sansom 2025-05-02 [Core] [Bugfix] Add Input Embeddings (#15428)
10	8	tests/conftest.py
73	1	tests/core/test_scheduler.py
9	2	tests/core/utils.py
26	0	tests/models/language/generation/test_common.py
109	34	tests/worker/test_model_runner.py
11	3	vllm/attention/backends/flashinfer.py
32	0	vllm/core/scheduler.py
8	0	vllm/engine/async_llm_engine.py
13	3	vllm/engine/llm_engine.py
4	2	vllm/engine/output_processor/multi_step.py
2	1	vllm/engine/output_processor/single_step.py
4	2	vllm/inputs/__init__.py
33	3	vllm/inputs/data.py
48	8	vllm/inputs/parse.py
63	8	vllm/inputs/preprocess.py
6	1	vllm/model_executor/layers/sampler.py
84	7	vllm/sequence.py
14	3	vllm/spec_decode/draft_model_runner.py
2	1	vllm/spec_decode/multi_step_worker.py
13	4	vllm/worker/enc_dec_model_runner.py
116	18	vllm/worker/model_runner.py
11	4	vllm/worker/pooling_model_runner.py

[9e2de9b9e] Isotr0py 2025-05-02 [Bugifx] Remove TritonPlaceholder from sys.modules (#17317)
1	7	vllm/triton_utils/importing.py

[109e15a33] Jerry Zhang 2025-05-01 Add `pt_load_map_location` to allow loading to cuda (#16869)
26	0	tests/quantization/test_torchao.py
15	1	tests/test_config.py
10	0	vllm/config.py
17	1	vllm/engine/arg_utils.py
2	0	vllm/model_executor/model_loader/loader.py
4	1	vllm/model_executor/model_loader/weight_utils.py

[f192ca90e] Michael Goin 2025-05-01 Fix PixtralHF missing spatial_merge_size (#17571)
2	3	vllm/model_executor/models/llava.py
2	6	vllm/model_executor/models/mistral3.py
3	2	vllm/model_executor/models/pixtral.py
11	14	vllm/model_executor/models/vision.py

[f89d0e11b] Cyrus Leung 2025-05-02 [Misc] Continue refactoring model tests (#17573)
2	3	examples/offline_inference/qwen2_5_omni/only_thinker.py
1	1	examples/offline_inference/vision_language.py
9	1	tests/conftest.py
1	1	tests/models/multimodal/generation/test_interleaved.py
3	6	tests/models/multimodal/{generation => pooling}/test_intern_vit.py
4	4	vllm/assets/video.py

[b4003d11f] Michael Goin 2025-05-01 Check if bitblas is installed during support check (#17572)
9	0	vllm/model_executor/layers/quantization/utils/bitblas_utils.py

[292fc59d6] Michael Goin 2025-05-01 [CI] Actually run tests/kv_transfer/test_disagg.py in CI (#17555)
2	2	tests/kv_transfer/test_disagg.py

[afcb3f886] Lucas Wilkinson 2025-05-01 [Attention] MLA move o_proj q_proj into cuda-graph region (#17484)
2	3	vllm/attention/backends/cpu_mla.py
1	1	vllm/attention/backends/flashmla.py
19	35	vllm/attention/backends/mla/common.py
1	1	vllm/attention/backends/rocm_aiter_mla.py
1	1	vllm/attention/backends/triton_mla.py
12	9	vllm/model_executor/models/deepseek_v2.py
17	33	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/flashmla.py
1	1	vllm/v1/attention/backends/mla/triton_mla.py

[afb12e429] David Xia 2025-05-01 [Doc] note that not all unit tests pass on CPU platforms (#17554)
6	0	docs/source/contributing/overview.md

[24aebae17] Michael Goin 2025-05-01 [Bugfix] Disable gptq_bitblas for <SM80 to fix GPTQ on V100/T4 (#17541)
1	1	vllm/model_executor/layers/quantization/gptq_bitblas.py

[39c0813a7] qizixi 2025-05-01 [V1][Spec Decode] Apply torch.compile & cudagraph to EAGLE3 (#17504)
17	8	vllm/model_executor/models/llama_eagle3.py
19	23	vllm/v1/spec_decode/eagle.py

[9b70e2b4c] Chenyaaang 2025-05-01 [Misc][Tools][Benchmark] Publish script to auto tune server parameters (#17207)
212	0	benchmarks/auto_tune.sh

[173daac19] Chen Xia 2025-05-01 [Bug]change the position of cuda_graph_sizes in dataclasses (#17548)
7	7	vllm/config.py

[04f2cfc89] sstamenk 2025-05-01 Remove duplicate code from dbrx.py (#17550)
1	5	vllm/model_executor/models/dbrx.py

[811a6c097] Juan Villamizar 2025-05-01 [ROCM] Add gfx950 to the custom attention archs (#16034)
6	5	csrc/rocm/attention.cu
6	3	vllm/platforms/rocm.py

[9b1769dd9] Cyrus Leung 2025-05-02 [Bugfix] Fix lint error (#17547)
1	2	vllm/config.py

[61c299f81] Chen Xia 2025-05-01 [Misc]add configurable cuda graph size (#17201)
17	3	vllm/config.py
5	0	vllm/engine/arg_utils.py

[4acfa3354] Hongxia Yang 2025-05-01 [ROCm] update installation guide to include build aiter from source instructions (#17542)
16	1	docs/source/getting_started/installation/gpu/rocm.inc.md

[88c830410] Isotr0py 2025-05-02 [Model] Refactor Ovis2 to support original tokenizer (#17537)
0	2	examples/offline_inference/vision_language.py
0	2	examples/offline_inference/vision_language_multi_image.py
0	1	tests/models/registry.py
66	9	vllm/model_executor/models/ovis2.py
36	34	vllm/transformers_utils/processors/ovis2.py

[6768ff4a2] Harry Mellor 2025-05-01 Move the last arguments in `arg_utils.py` to be in their final groups (#17531)
1	1	vllm/config.py
134	137	vllm/engine/arg_utils.py

[f2e7af9b8] Cyrus Leung 2025-05-02 [CI/Build] Remove `awscli` dependency (#17532)
0	1	requirements/nightly_torch_test.txt
0	1	requirements/rocm.txt
0	1	requirements/test.in
1	14	requirements/test.txt

[7423cf0a9] Reid 2025-05-01 [Misc] refactor example - cpu_offload_lmcache (#17460)
2	2	examples/lmcache/README.md
62	11	examples/lmcache/{cpu_offload_lmcache_v0.py => cpu_offload_lmcache.py}
0	57	examples/lmcache/cpu_offload_lmcache_v1.py

[460a2b110] Sage Moore 2025-05-01 [torch.compile] Add torch inductor pass for fusing silu_and_mul with subsequent scaled_fp8_quant operations (#10867)
1	0	CMakeLists.txt
19	0	csrc/core/math.hpp
3	0	csrc/ops.h
120	0	csrc/quantization/activation_kernels.cu
5	1	csrc/torch_bindings.cpp
14	3	tests/compile/test_functionalization.py
74	0	tests/compile/test_silu_mul_quant_fusion.py
69	0	tests/kernels/test_fused_quant_activation.py
87	0	vllm/compilation/activation_quant_fusion.py
12	5	vllm/compilation/fix_functionalization.py
2	0	vllm/compilation/pass_manager.py

[28566d73b] Hongxia Yang 2025-05-01 [ROCm] remove unsupported archs from rocm triton flash-attention supported list (#17536)
1	1	vllm/attention/ops/triton_flash_attention.py

[98060b001] Chauncey 2025-05-01 [Feature][Frontend]: Deprecate --enable-reasoning (#17452)
5	8	docs/source/features/reasoning_outputs.md
1	1	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
1	1	examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py
1	1	examples/online_serving/openai_chat_completion_with_reasoning.py
1	1	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
3	3	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
3	11	tests/entrypoints/openai/test_cli_args.py
2	3	vllm/config.py
12	5	vllm/engine/arg_utils.py
1	1	vllm/engine/llm_engine.py
1	2	vllm/entrypoints/openai/api_server.py
0	5	vllm/entrypoints/openai/cli_args.py
14	45	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/model_executor/guided_decoding/__init__.py
1	1	vllm/model_executor/guided_decoding/outlines_logits_processors.py
1	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[f5a3c655b] TJian 2025-05-01 [FEAT] [ROCm]: Add Qwen/Qwen3-235B-A22B-FP8 TP4 triton fused moe config (#17535)
164	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json

[7169f87ad] Reid 2025-05-01 [doc] add streamlit integration (#17522)
-	-	docs/source/assets/deployment/streamlit-chat.png
1	0	docs/source/deployment/frameworks/index.md
42	0	docs/source/deployment/frameworks/streamlit.md
185	0	examples/online_serving/streamlit_openai_chatbot_webserver.py

[b74d888c6] Huy Do 2025-05-01 Fix more broken speculative decode tests (#17450)
1	1	tests/spec_decode/e2e/test_medusa_correctness.py
2	2	tests/spec_decode/e2e/test_mlp_correctness.py
1	1	tests/spec_decode/e2e/test_ngram_correctness.py
5	0	vllm/spec_decode/multi_step_worker.py

[2007d4d54] TJian 2025-05-01 [FEAT] [ROCm]: Add Qwen/Qwen3-30B-A3B-FP8 fused moe config for MI300X (#17530)
164	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json

[48e925fab] Cyrus Leung 2025-05-01 [Misc] Clean up test docstrings and names (#17521)
8	2	.buildkite/test-pipeline.yaml
0	5	tests/models/language/generation/{test_models.py => test_common.py}
0	4	tests/models/language/generation/test_granite.py
0	4	tests/models/language/generation/test_mistral.py
0	4	tests/models/language/generation/test_phimoe.py
1	5	tests/models/language/pooling/{test_cls_models.py => test_classification.py}
0	4	tests/models/language/pooling/test_embedding.py
3	8	tests/models/language/pooling/test_jina.py
22	39	tests/models/language/pooling/test_scoring.py
0	4	tests/models/language/pooling/test_snowflake_arctic_embed.py
12	12	tests/models/language/pooling/test_truncation_control.py
0	4	tests/models/multimodal/generation/test_pixtral.py
2	2	tests/models/multimodal/generation/test_whisper.py
0	5	tests/models/quantization/test_aqlm.py
0	2	tests/models/quantization/test_bitblas.py
0	2	tests/models/quantization/test_gptq_bitblas.py
2	3	tests/models/quantization/test_gptq_marlin.py
0	2	tests/models/quantization/test_gptq_marlin_24.py
1	4	tests/models/test_transformers.py

[1903c0b8a] Cyrus Leung 2025-05-01 [Frontend] Show progress bar for adding requests (#17525)
12	2	vllm/entrypoints/llm.py

[86a1f67a3] Teruaki Ishizaki 2025-05-01 [Bugfix][Benchmarks] Allow benchmark of deepspeed-mii backend to select a model (#17285)
1	0	benchmarks/backend_request_func.py

[a257d9bcc] Harry Mellor 2025-05-01 Improve configs - `ObservabilityConfig` (#17453)
57	9	vllm/config.py
39	57	vllm/engine/arg_utils.py

[015069b01] Chauncey 2025-05-01 [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content (#17515)
27	26	vllm/reasoning/qwen3_reasoning_parser.py

[fbefc8a78] Russell Bryant 2025-05-01 [Core] Enable IPv6 with vllm.utils.make_zmq_socket() (#16506)
53	1	tests/test_utils.py
28	0	vllm/utils.py

[26bc4bbcd] Keyun Tong 2025-05-01 Avoid overwriting vllm_compile_cache.py (#17418)
5	3	vllm/compilation/backends.py

[3c3d76720] Lucas Wilkinson 2025-05-01 [BugFix] Fix mla cpu - missing 3 required positional arguments (#17494)
3	1	vllm/_ipex_ops.py
3	0	vllm/attention/backends/cpu_mla.py

[13cf6b623] Noah Yoshida 2025-04-30 [BugFix] fix speculative decoding memory leak when speculation is disabled (#15506)
90	0	tests/spec_decode/test_memory_usage.py
1	0	vllm/spec_decode/spec_decode_worker.py

[90d0a54c4] Hongxia Yang 2025-05-01 [ROCm] Effort to reduce the number of environment variables in command line (#17229)
9	0	docker/Dockerfile.rocm

[7a0a146c5] Russell Bryant 2025-05-01 [Build] Require setuptools >= 77.0.3 for PEP 639 (#17389)
2	2	pyproject.toml
2	2	requirements/build.txt
1	1	requirements/common.txt
1	1	requirements/hpu.txt
2	2	requirements/rocm-build.txt
2	2	requirements/test.txt
1	1	requirements/tpu.txt
2	2	requirements/xpu.txt

[7ab643e42] Alexei-V-Ivanov-AMD 2025-05-01 FIxing the AMD test failures caused by PR#16457 (#17511)
0	1	docker/Dockerfile.rocm
1	1	vllm/model_executor/layers/rotary_embedding.py

[afb4429b4] Cyrus Leung 2025-05-01 [CI/Build] Reorganize models tests (#17459)
38	48	.buildkite/test-pipeline.yaml
0	1	pyproject.toml
6	6	tests/entrypoints/openai/test_embedding.py
3	2	tests/entrypoints/openai/test_embedding_dimensions.py
0	66	tests/models/embedding/utils.py
0	0	tests/models/embedding/vision_language/__init__.py
0	0	tests/models/encoder_decoder/__init__.py
0	0	tests/models/encoder_decoder/audio_language/__init__.py
0	0	tests/models/encoder_decoder/language/__init__.py
0	0	tests/models/encoder_decoder/vision_language/__init__.py
0	37	tests/models/encoder_decoder/vision_language/test_broadcast.py
0	0	tests/models/{decoder_only => language}/__init__.py
0	0	tests/models/{decoder_only/audio_language => language/generation}/__init__.py
0	4	tests/models/{encoder_decoder/language => language/generation}/test_bart.py
0	0	tests/models/{decoder_only/language => language/generation}/test_granite.py
10	8	tests/models/{decoder_only/language => language/generation}/test_hybrid.py
0	0	tests/models/{decoder_only/language => language/generation}/test_mistral.py
0	0	tests/models/{decoder_only/language => language/generation}/test_models.py
0	0	tests/models/{decoder_only/language => language/generation}/test_phimoe.py
0	0	tests/models/{decoder_only/language => language/pooling}/__init__.py
0	0	tests/models/{embedding/language => language/pooling}/test_cls_models.py
1	1	tests/models/{embedding/language => language/pooling}/test_embedding.py
95	94	tests/models/{embedding/language => language/pooling}/test_gritlm.py
2	1	tests/models/{embedding/language => language/pooling}/test_jina.py
0	0	tests/models/{embedding/language => language/pooling}/test_scoring.py
1	3	tests/models/{embedding/language => language/pooling}/test_snowflake_arctic_embed.py
0	0	tests/models/{embedding/language => language/pooling}/test_truncation_control.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/__init__.py
7	0	tests/models/{decoder_only/vision_language/test_models.py => multimodal/generation/test_common.py}
0	0	tests/models/{encoder_decoder/vision_language => multimodal/generation}/test_florence2.py
0	0	tests/models/{decoder_only/audio_language => multimodal/generation}/test_granite_speech.py
1	0	tests/models/{decoder_only/vision_language => multimodal/generation}/test_interleaved.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/test_intern_vit.py
33	1	tests/models/{encoder_decoder/vision_language => multimodal/generation}/test_mllama.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/test_phi4mm.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/test_pixtral.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/test_qwen2_vl.py
0	0	tests/models/{decoder_only/audio_language => multimodal/generation}/test_ultravox.py
35	23	tests/models/{encoder_decoder/audio_language => multimodal/generation}/test_whisper.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/__init__.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/builders.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/case_filtering.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/core.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/custom_inputs.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/model_utils.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/runners.py
0	0	tests/models/{decoder_only/vision_language => multimodal/generation}/vlm_utils/types.py
0	0	tests/models/{embedding => multimodal/pooling}/__init__.py
1	1	tests/models/{embedding/vision_language => multimodal/pooling}/test_dse_qwen2_vl.py
1	1	tests/models/{embedding/vision_language => multimodal/pooling}/test_llava_next.py
1	1	tests/models/{embedding/vision_language => multimodal/pooling}/test_phi3v.py
0	0	tests/models/{embedding/language => quantization}/__init__.py
0	1	tests/models/{decoder_only/language => quantization}/test_aqlm.py
2	3	tests/models/{decoder_only/vision_language => quantization}/test_awq.py
1	1	tests/models/{ => quantization}/test_bitblas.py
1	6	tests/models/{decoder_only/language => quantization}/test_fp8.py
3	4	tests/models/{decoder_only/language => quantization}/test_gguf.py
1	1	tests/models/{ => quantization}/test_gptq_bitblas.py
1	2	tests/models/{decoder_only/language => quantization}/test_gptq_marlin.py
1	2	tests/models/{decoder_only/language => quantization}/test_gptq_marlin_24.py
0	1	tests/models/{decoder_only/language => quantization}/test_modelopt.py
0	1	tests/models/{decoder_only/language => quantization}/test_nvfp4.py
65	1	tests/models/utils.py
3	1	vllm/config.py
4	2	vllm/model_executor/models/llama.py

[aa4502e7f] Michael Goin 2025-04-30 [CI][Bugfix] Fix failing V1 Test due to missing 'cache_salt' arg (#17500)
1	0	tests/v1/engine/test_engine_core_client.py

[17b4d85f6] Michael Goin 2025-04-30 [CI][TPU] Skip structured outputs+spec decode tests on TPU (#17510)
3	0	tests/v1/entrypoints/llm/test_struct_output_generate.py

[1144a8efe] NaLan ZeYu 2025-05-01 [Bugfix] Temporarily disable gptq_bitblas on ROCm (#17411)
1	1	docs/source/features/quantization/supported_hardware.md
5	0	vllm/model_executor/layers/quantization/gptq_bitblas.py

[08fb5587b] Gregory Shtrasberg 2025-04-30 [Bugfix][ROCm] Fix import error on ROCm (#17495)
1	1	vllm/model_executor/layers/rotary_embedding.py

[dbc18e781] Siyuan Liu 2025-04-30 [CI][TPU] Skip Multimodal test (#17488)
2	0	tests/v1/tpu/test_multimodal.py

[02bd65484] Alex Brooks 2025-04-30 [Misc] Rename Audios -> Audio in Qwen2audio Processing (#17507)
8	1	vllm/model_executor/models/qwen2_audio.py

[200bbf92e] Rahul Tuli 2025-04-30 Bump Compressed Tensors version to 0.9.4 (#17478)
1	1	requirements/common.txt

[81ecf425f] Chen Zhang 2025-05-01 [v1][Spec Decode] Make sliding window compatible with eagle prefix caching (#17398)
60	4	tests/v1/core/test_prefix_caching.py
6	2	tests/v1/core/test_specialized_manager.py
1	7	vllm/v1/core/kv_cache_manager.py
29	10	vllm/v1/core/specialized_manager.py

[42d9a2c4c] David Xia 2025-04-30 doc: fix bug report Github template formatting (#17486)
2	2	.github/ISSUE_TEMPLATE/400-bug-report.yml

[2ac74d098] Reid 2025-05-01 [doc] add install tips (#17373)
7	7	docs/source/features/quantization/fp8.md
7	1	docs/source/features/quantization/int4.md
7	1	docs/source/features/quantization/int8.md
1	1	docs/source/features/quantization/quantized_kvcache.md
7	0	docs/source/features/quantization/quark.md

[584f5fb4c] Gregory Shtrasberg 2025-04-30 [Bugfix][ROCm] Restrict ray version due to a breaking release (#17480)
1	1	requirements/rocm.txt

[d586ddc69] zh Wang 2025-05-01 [BugFix] Fix authorization of openai_transcription_client.py (#17321)
6	2	examples/online_serving/openai_transcription_client.py

[0b7e701dd] Michael Goin 2025-04-30 [Docs] Update optimization.md doc (#17482)
155	32	docs/source/performance/optimization.md

[947f2f537] Russell Bryant 2025-04-30 [V1] Allow turning off pickle fallback in vllm.v1.serial_utils (#17427)
98	0	tests/v1/test_serial_utils.py
15	6	vllm/v1/serial_utils.py

[739e03b34] Pete Savage 2025-04-30 [Bugfix] Fixed mistral tokenizer path when pointing to file (#17457)
1	0	vllm/transformers_utils/tokenizers/mistral.py

[da4e7687b] Aaron Pham 2025-04-30 [Fix] Support passing args to logger (#17425)
6	6	vllm/config.py
9	8	vllm/logger.py
1	2	vllm/lora/punica_wrapper/punica_selector.py
3	3	vllm/model_executor/custom_op.py
3	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py
3	2	vllm/model_executor/layers/quantization/awq_marlin.py
11	9	vllm/model_executor/model_loader/weight_utils.py
4	4	vllm/model_executor/models/chameleon.py
4	5	vllm/model_executor/models/olmoe.py
4	5	vllm/model_executor/models/qwen2_moe.py
4	5	vllm/model_executor/models/qwen3_moe.py
16	22	vllm/multimodal/profiling.py
7	5	vllm/multimodal/registry.py

[39317cf42] Russell Bryant 2025-04-30 [Docs] Add command for running mypy tests from CI (#17475)
4	0	docs/source/contributing/overview.md

[2990cee95] Chauncey 2025-04-30 [Feature] The Qwen3 reasoning parser supports  guided decoding (#17466)
12	1	vllm/reasoning/qwen3_reasoning_parser.py

[0be6d05b5] Alec 2025-04-30 [V1][Metrics] add support for kv event publishing (#16750)
86	0	examples/online_serving/kv_events.sh
114	0	examples/online_serving/kv_events_subscriber.py
145	0	tests/distributed/conftest.py
193	0	tests/distributed/test_events.py
59	3	tests/v1/core/test_prefix_caching.py
2	0	tests/v1/engine/conftest.py
127	37	tests/v1/engine/test_engine_core_client.py
60	0	vllm/config.py
295	0	vllm/distributed/kv_events.py
14	8	vllm/engine/arg_utils.py
48	1	vllm/v1/core/block_pool.py
13	1	vllm/v1/core/kv_cache_manager.py
5	0	vllm/v1/core/sched/interface.py
20	1	vllm/v1/core/sched/scheduler.py
2	0	vllm/v1/engine/core.py

[77073c77b] Marko Rosenmueller 2025-04-30 [Core] Prevent side-channel attacks via cache salting (#17045)
19	1	docs/source/design/v1/prefix_caching.md
40	0	tests/entrypoints/openai/test_serving_chat.py
10	2	tests/tokenization/test_detokenize.py
42	1	tests/v1/core/test_kv_cache_utils.py
63	1	tests/v1/core/test_prefix_caching.py
1	0	tests/v1/engine/test_engine_core.py
1	0	tests/v1/engine/test_engine_core_client.py
6	1	tests/v1/engine/test_output_processor.py
28	4	vllm/entrypoints/openai/protocol.py
3	0	vllm/entrypoints/openai/serving_engine.py
19	1	vllm/inputs/data.py
72	106	vllm/inputs/preprocess.py
5	0	vllm/multimodal/inputs.py
1	1	vllm/multimodal/processing.py
9	4	vllm/v1/core/kv_cache_utils.py
1	0	vllm/v1/engine/__init__.py
1	0	vllm/v1/engine/processor.py
3	0	vllm/v1/request.py

[a7d5b016b] Nicolò Lucchesi 2025-04-30 [TPU][V1][CI] Update regression test baseline for v6 CI (#17064)
7	8	tests/v1/tpu/test_perf.py

[d80378673] rongfu.leng 2025-04-30 [V1][Bugfix]: vllm v1 verison metric num_gpu_blocks is None (#15755)
2	1	vllm/v1/engine/async_llm.py
8	1	vllm/v1/engine/core.py
10	3	vllm/v1/engine/core_client.py
17	6	vllm/v1/metrics/loggers.py

[1534d389a] Chauncey 2025-04-30 [Misc] Remove deprecated files (#17447)
0	35	vllm/model_executor/guided_decoding/reasoner/__init__.py

[ece5a8b0b] Lu Fang 2025-04-30 Make the _apply_rotary_emb compatible with dynamo (#17435)
3	1	vllm/model_executor/layers/rotary_embedding.py

[54072f315] Marco 2025-04-30 [MODEL ADDITION] Ovis2 Model Addition (#15826)
7	0	docs/source/models/supported_models.md
31	0	examples/offline_inference/vision_language.py
31	0	examples/offline_inference/vision_language_multi_image.py
12	0	tests/models/decoder_only/vision_language/test_models.py
1	1	tests/models/decoder_only/vision_language/vlm_utils/core.py
30	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
1	0	tests/models/multimodal/processing/test_common.py
4	0	tests/models/registry.py
3	2	vllm/entrypoints/chat_utils.py
322	0	vllm/model_executor/models/aimv2.py
331	0	vllm/model_executor/models/ovis2.py
1	0	vllm/model_executor/models/registry.py
4	3	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
170	0	vllm/transformers_utils/configs/ovis2.py
2	1	vllm/transformers_utils/processors/__init__.py
397	0	vllm/transformers_utils/processors/ovis2.py

[be633fba0] Chauncey 2025-04-30 [Bugfix] Fix AttributeError: 'State' object has no attribute 'engine_client' (#17434)
1	1	vllm/entrypoints/api_server.py

[ed6cfb90c] Kunshang Ji 2025-04-30 [Hardware][Intel GPU] Upgrade to torch 2.7 (#17444)
0	6	docker/Dockerfile.xpu
0	9	docs/source/getting_started/installation/gpu/xpu.inc.md
3	3	requirements/xpu.txt
9	9	vllm/_ipex_ops.py
6	8	vllm/attention/backends/ipex_attn.py

[6ed9f6047] Kunshang Ji 2025-04-30 [Intel GPU] [CI]Fix XPU ci, setuptools >=80.0 have build issue (#17298)
1	1	requirements/xpu.txt

[a44c4f1d2] Michael Goin 2025-04-29 Support LoRA for Mistral3 (#17428)
1	1	docs/source/models/supported_models.md
14	3	vllm/model_executor/models/mistral3.py

[88fcf00dd] Huy Do 2025-04-29 Fix some speculative decode tests with tl.dot (#17371)
3	6	tests/spec_decode/e2e/test_multistep_correctness.py

[d1f569b1b] Harry Mellor 2025-04-30 Fix call to `logger.info_once` (#17416)
2	2	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[13698db63] Harry Mellor 2025-04-30 Improve configs - `ModelConfig` (#17130)
1	1	tests/conftest.py
1	12	tests/engine/test_arg_utils.py
2	2	tests/quantization/test_register_quantization_config.py
5	4	tests/test_config.py
258	255	vllm/config.py
137	314	vllm/engine/arg_utils.py
7	11	vllm/entrypoints/llm.py
2	1	vllm/model_executor/layers/quantization/aqlm.py
2	1	vllm/model_executor/layers/quantization/awq.py
4	3	vllm/model_executor/layers/quantization/awq_marlin.py
9	4	vllm/model_executor/layers/quantization/base_config.py
4	3	vllm/model_executor/layers/quantization/bitblas.py
2	1	vllm/model_executor/layers/quantization/bitsandbytes.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
3	2	vllm/model_executor/layers/quantization/deepspeedfp.py
2	1	vllm/model_executor/layers/quantization/experts_int8.py
2	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py
2	1	vllm/model_executor/layers/quantization/fp8.py
2	1	vllm/model_executor/layers/quantization/gguf.py
2	1	vllm/model_executor/layers/quantization/gptq.py
4	3	vllm/model_executor/layers/quantization/gptq_bitblas.py
4	3	vllm/model_executor/layers/quantization/gptq_marlin.py
4	3	vllm/model_executor/layers/quantization/gptq_marlin_24.py
2	1	vllm/model_executor/layers/quantization/hqq_marlin.py
4	3	vllm/model_executor/layers/quantization/ipex_quant.py
4	3	vllm/model_executor/layers/quantization/marlin.py
4	3	vllm/model_executor/layers/quantization/modelopt.py
4	3	vllm/model_executor/layers/quantization/moe_wna16.py
2	1	vllm/model_executor/layers/quantization/neuron_quant.py
2	1	vllm/model_executor/layers/quantization/ptpc_fp8.py
2	1	vllm/model_executor/layers/quantization/qqq.py
2	1	vllm/model_executor/layers/quantization/quark/quark.py
2	1	vllm/model_executor/layers/quantization/torchao.py
2	1	vllm/model_executor/layers/quantization/tpu_int8.py
1	1	vllm/model_executor/layers/rotary_embedding.py
0	1	vllm/model_executor/model_loader/neuron.py

[2c4f59afc] Huy Do 2025-04-29 Update PyTorch to 2.7.0 (#16859)
5	5	.buildkite/release-pipeline.yaml
9	9	.buildkite/scripts/upload-wheels.sh
1	1	.buildkite/test-pipeline.yaml
2	2	.github/workflows/lint-and-deploy.yaml
1	1	.pre-commit-config.yaml
2	2	CMakeLists.txt
32	14	docker/Dockerfile
3	3	docs/source/getting_started/installation/gpu/cuda.inc.md
1	1	examples/online_serving/chart-helm/values.yaml
1	1	pyproject.toml
1	1	requirements/build.txt
6	5	requirements/cpu.txt
5	4	requirements/cuda.txt
3	3	requirements/rocm-build.txt
3	3	requirements/test.in
24	20	requirements/test.txt
1	1	setup.py
2	1	vllm/attention/ops/ipex_attn.py

[1c2bc7ead] Gabriel Marinho 2025-04-29 Truncation control for embedding models (#14776)
103	0	tests/entrypoints/openai/test_truncation.py
69	0	tests/models/embedding/language/test_truncation_control.py
3	0	vllm/engine/llm_engine.py
2	2	vllm/engine/protocol.py
22	3	vllm/entrypoints/llm.py
4	4	vllm/entrypoints/openai/protocol.py
4	10	vllm/entrypoints/openai/serving_embedding.py
5	5	vllm/entrypoints/openai/serving_engine.py
4	10	vllm/entrypoints/openai/serving_pooling.py
4	11	vllm/entrypoints/openai/serving_score.py
1	1	vllm/entrypoints/score_utils.py
24	0	vllm/entrypoints/utils.py
48	15	vllm/inputs/preprocess.py
4	3	vllm/sampling_params.py
12	2	vllm/transformers_utils/tokenizer.py
2	0	vllm/transformers_utils/tokenizer_base.py
9	0	vllm/transformers_utils/tokenizer_group.py
2	0	vllm/transformers_utils/tokenizers/mistral.py
4	2	vllm/v1/engine/async_llm.py
3	1	vllm/v1/engine/llm_engine.py
3	1	vllm/v1/engine/processor.py

[4055130a8] Kevin H. Luu 2025-04-29 [release] Always git fetch all to get latest tag on TPU release (#17322)
1	0	.buildkite/release-pipeline.yaml

[34120f5ac] Benjamin Chislett 2025-04-29 [V1][Feature] Enable Speculative Decoding with Structured Outputs (#14702)
1	0	benchmarks/backend_request_func.py
6	3	benchmarks/benchmark_serving_structured_output.py
28	7	tests/v1/entrypoints/llm/test_struct_output_generate.py
12	5	vllm/v1/core/sched/scheduler.py
46	13	vllm/v1/structured_output/__init__.py
21	0	vllm/v1/structured_output/backend_guidance.py
24	0	vllm/v1/structured_output/backend_types.py
30	2	vllm/v1/structured_output/backend_xgrammar.py
41	29	vllm/v1/worker/gpu_model_runner.py

[7489ec0ba] Harry Mellor 2025-04-29 Remove Bamba 9B from CI (#17407)
1	1	tests/models/decoder_only/language/test_hybrid.py
2	1	tests/models/registry.py
1	1	tests/v1/test_oracle.py

[70788bdbd] Bryan Lu 2025-04-29 [V1][Spec Decode] Apply torch.compile & cudagraph to EAGLE (#17211)
12	2	examples/offline_inference/eagle.py
12	3	vllm/compilation/backends.py
14	11	vllm/model_executor/models/llama_eagle.py
3	2	vllm/model_executor/models/llama_eagle3.py
100	22	vllm/v1/spec_decode/eagle.py
11	13	vllm/v1/worker/gpu_model_runner.py

[c9c1b59e5] Dilip Gowda Bhagavan 2025-04-30 Fix: Python package installation for opentelmetry (#17049)
2	1	docker/Dockerfile.s390x

[0350809f3] Harry Mellor 2025-04-29 Remove Falcon3 2x7B from CI (#17404)
3	4	tests/models/decoder_only/language/test_models.py
1	1	tests/models/registry.py

[a6977dbd1] Harry Mellor 2025-04-29 Simplify (and fix) passing of guided decoding backend options (#17008)
4	3	examples/online_serving/openai_chat_completion_structured_outputs.py
125	81	tests/entrypoints/llm/test_guided_generate.py
9	6	tests/model_executor/test_guided_processors.py
16	15	tests/v1/entrypoints/llm/test_struct_output_generate.py
2	1	tests/v1/test_oracle.py
60	10	vllm/config.py
26	10	vllm/engine/arg_utils.py
1	1	vllm/engine/llm_engine.py
2	2	vllm/engine/multiprocessing/client.py
12	12	vllm/model_executor/guided_decoding/__init__.py
2	3	vllm/model_executor/guided_decoding/guidance_decoding.py
5	7	vllm/model_executor/guided_decoding/xgrammar_decoding.py
26	30	vllm/sampling_params.py
4	4	vllm/v1/engine/processor.py
4	4	vllm/v1/structured_output/__init__.py
8	17	vllm/v1/structured_output/backend_guidance.py
3	11	vllm/v1/structured_output/backend_xgrammar.py

[2fa2a50bf] Isotr0py 2025-04-30 [Bugfix] Fix Minicpm-O-int4 GPTQ model inference (#17397)
35	1	vllm/model_executor/models/minicpmo.py
1	1	vllm/model_executor/models/minicpmv.py

[08e15defa] Reid 2025-04-30 [CI/Build] Add retry mechanism for add-apt-repository (#17107)
8	2	docker/Dockerfile
8	2	docker/Dockerfile.nightly_torch
4	1	docker/Dockerfile.rocm_base

[b37685afb] Aaron Pham 2025-04-29 [CI] Uses Python 3.11 for TPU (#17359)
1	0	requirements/tpu.txt

[792595b59] Nicolò Lucchesi 2025-04-29 [TPU][V1][CI] Replace `python3 setup.py develop` with standard `pip install --e` on TPU (#17374)
1	1	docker/Dockerfile.tpu
1	1	docs/source/getting_started/installation/ai_accelerator/tpu.inc.md

[0c1c78831] casinca 2025-04-29 [Doc][Typo] Fixing label in new model requests link in overview.md (#17400)
1	1	docs/source/contributing/overview.md

[56d64fbe3] Russell Bryant 2025-04-29 [Docs] Propose a deprecation policy for the project (#17063)
87	0	docs/source/contributing/deprecation_policy.md
3	0	docs/source/design/v1/metrics.md
1	0	docs/source/index.md

[608968b7c] Alexei-V-Ivanov-AMD 2025-04-29  Enabling multi-group kernel tests. (#17115)
44	30	.buildkite/scripts/hardware_ci/run-amd-test.sh
5	0	.buildkite/test-pipeline.yaml

[06ffc7e1d] TY-AMD 2025-04-30 [Misc][ROCm] Exclude `cutlass_mla_decode` for ROCm build (#17289)
7	7	csrc/torch_bindings.cpp

[d3cf61b89] Qiming Zhang 2025-04-29 fix gemma3 results all zero (#17364)
4	1	vllm/model_executor/layers/layernorm.py

[a39203f99] mofanke 2025-04-30 [Bugfix] add qwen3 reasoning-parser fix content is None when disable … (#17369)
1	0	docs/source/features/reasoning_outputs.md
141	0	tests/reasoning/test_qwen3_reasoning_parser.py
2	0	vllm/reasoning/__init__.py
138	0	vllm/reasoning/qwen3_reasoning_parser.py

[24e6ad3f1] Chen Zhang 2025-04-30 [V1] Remove num_input_tokens from attn_metadata (#17193)
7	9	vllm/forward_context.py
0	3	vllm/v1/attention/backends/flash_attn.py
0	3	vllm/v1/attention/backends/flashinfer.py
0	3	vllm/v1/attention/backends/mla/common.py
3	2	vllm/v1/worker/gpu_model_runner.py
4	1	vllm/v1/worker/tpu_model_runner.py

[2ef5d106b] Harry Mellor 2025-04-29 Improve literal dataclass field conversion to argparse argument (#17391)
30	4	tests/engine/test_arg_utils.py
34	1	tests/test_config.py
15	4	vllm/config.py
18	9	vllm/engine/arg_utils.py

[0ed27ef66] a2q1p 2025-04-30 Fix: Spelling of inference (#17387)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[900edfa8d] Harry Mellor 2025-04-29 Transformers backend tweaks (#17365)
9	13	vllm/model_executor/models/transformers.py

[88ad9ec6b] Cyrus Leung 2025-04-29 [Frontend] Support `chat_template_kwargs` in `LLM.chat` (#17356)
93	16	tests/entrypoints/llm/test_chat.py
13	8	vllm/entrypoints/llm.py

[40896bdf3] Harry Mellor 2025-04-29 `pre-commit autoupdate` (#17380)
6	6	.pre-commit-config.yaml
4	4	csrc/moe/marlin_kernels/marlin_moe_kernel.h
4	4	csrc/moe/marlin_moe_wna16/marlin_template.h
8	8	csrc/moe/moe_wna16_utils.h
1	1	csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
8	8	csrc/quantization/gptq_marlin/gptq_marlin.cu
2	2	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
2	2	csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu
2	2	csrc/quantization/marlin/sparse/common/mma.h

[00ee37efa] Cyrus Leung 2025-04-29 [Bugfix] Clean up MiniMax-VL and fix processing (#17354)
7	0	docs/source/models/supported_models.md
1	0	tests/models/multimodal/processing/test_common.py
0	1	tests/models/multimodal/processing/test_minimax_vl_01.py
30	282	vllm/model_executor/models/minimax_vl_01.py

[890f104cd] Jee Jee Li 2025-04-29 [Doc] Fix QWen3MOE info (#17381)
2	2	docs/source/models/supported_models.md
2	10	tests/models/registry.py

[4a5e13149] Harry Mellor 2025-04-29 Update docs requirements (#17379)
4	4	requirements/docs.txt

[97cc8729f] Ekagra Ranjan 2025-04-29 [Model] Ignore rotary embed load for Cohere model (#17319)
4	0	vllm/model_executor/models/commandr.py

[446410921] Gregory Shtrasberg 2025-04-29 [Build][Bugfix] Restrict setuptools version to <80 (#17320)
1	1	requirements/common.txt

[193e78e35] Hyogeun Oh (오효근) 2025-04-29 [Fix] Documentation spacing in compilation config help text (#17342)
1	1	vllm/engine/arg_utils.py

[bdb2cddaf] ponix-j 2025-04-29 [Misc]Use a platform independent interface to obtain the device attributes (#17100)
2	1	tests/conftest.py
3	1	tests/v1/sample/test_sampler.py
3	2	vllm/worker/multi_step_model_runner.py

[ebb3930d2] Cyrus Leung 2025-04-29 [Misc] Move config fields to MultiModalConfig (#17343)
42	15	vllm/config.py
4	12	vllm/engine/arg_utils.py
4	2	vllm/inputs/registry.py
3	2	vllm/model_executor/models/qwen2_vl.py
2	1	vllm/multimodal/registry.py
2	1	vllm/transformers_utils/processor.py
4	1	vllm/v1/engine/mm_input_cache.py
1	2	vllm/v1/engine/processor.py

[cde384cd9] qscqesze 2025-04-29 [Model] support MiniMax-VL-01 model (#16328)
13	0	tests/models/decoder_only/vision_language/test_models.py
19	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
99	0	tests/models/multimodal/processing/test_minimax_vl_01.py
2	0	tests/models/registry.py
53	14	vllm/model_executor/models/minimax_text_01.py
615	0	vllm/model_executor/models/minimax_vl_01.py
1	0	vllm/model_executor/models/registry.py
9	5	vllm/transformers_utils/config.py
4	0	vllm/transformers_utils/configs/__init__.py
69	0	vllm/transformers_utils/configs/minimax_text_01.py
70	0	vllm/transformers_utils/configs/minimax_vl_01.py

[96e06e3cb] Chauncey 2025-04-29 [Misc] Add a Jinja template to support Mistral3 function calling (#17195)
119	0	examples/tool_chat_template_mistral3.jinja

[17eb306fc] Zhengyuan Su (苏政渊) 2025-04-29 [Bugfix] Add contiguous call inside rope kernel wrapper (#17091)
14	3	vllm/_custom_ops.py
3	4	vllm/v1/attention/backends/mla/common.py

[165cb5632] Richard Zou 2025-04-28 Ignore `'<string>'` filepath (#17330)
4	0	vllm/compilation/backends.py

[d6da8a8ff] Richard Barnes 2025-04-28 [Bugfix] Fix `numel()` downcast in fused_layernorm_dynamic_per_token_quant.cu (#17316)
1	1	csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu

[b4ac4fa04] Lucia Fang 2025-04-28 [model] make llama4 compatible with pure dense layers (#17315)
2	2	vllm/model_executor/models/llama4.py

[e13600059] Ekagra Ranjan 2025-04-28 [V1][Spec Decode] Make Eagle model arch config driven (#17323)
2	1	vllm/config.py
18	1	vllm/transformers_utils/configs/eagle.py
6	11	vllm/v1/spec_decode/eagle.py

[86d9fc29c] Michał Moskal 2025-04-28 implement Structural Tag with Guidance backend (#17333)
4	7	tests/v1/entrypoints/llm/test_struct_output_generate.py
28	3	vllm/v1/structured_output/backend_guidance.py

[506475de5] Cyrus Leung 2025-04-29 [Optim] Compute multimodal hash only once per item (#17314)
9	7	vllm/model_executor/models/deepseek_vl2.py
9	7	vllm/model_executor/models/h2ovl.py
0	3	vllm/model_executor/models/llava.py
0	2	vllm/model_executor/models/mistral3.py
10	5	vllm/model_executor/models/pixtral.py
205	104	vllm/multimodal/processing.py

[cfe453209] Ekagra Ranjan 2025-04-28 [Benchmark] Add single turn MTBench to Serving Bench (#17202)
54	0	benchmarks/benchmark_dataset.py
6	3	benchmarks/benchmark_serving.py

[8fc88d63f] Michael Goin 2025-04-28 [Model] Add tuned triton fused_moe configs for Qwen3Moe (#17328)
5	3	benchmarks/kernels/benchmark_moe.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H20.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H20.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=512,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H20.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=96,device_name=NVIDIA_H20.json
1	2	vllm/model_executor/layers/fused_moe/configs/README

[6e74fd494] Alex Wu 2025-04-28 Support loading transformers models with named parameters (#16868)
23	0	vllm/model_executor/models/transformers.py

[dcbac4cb4] Simon Mo 2025-04-28 [Model] Qwen3 Dense FP8 Compat Fixes (#17318)
9	0	vllm/model_executor/layers/linear.py

[ed2462030] Charlie Fu 2025-04-28 [Bugfix] Fix moe weight losing all extra attrs after `process_weights_after_loading`. (#16854)
5	10	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[cc5befbce] Lucas Wilkinson 2025-04-28 [BugFix] Fix cascade attention - RuntimeError: scheduler_metadata must have shape (metadata_size) (#17283)
1	1	vllm/v1/attention/backends/flash_attn.py

[2c89cd96a] Aaron Pham 2025-04-28 [Chore] cleanup license indicators in light of SPDX (#17259)
2	2	pyproject.toml

[a0304dc50] Russell Bryant 2025-04-28 [Security] Don't bind tcp zmq socket to all interfaces (#17197)
4	0	docs/source/serving/distributed_serving.md
1	1	vllm/distributed/device_communicators/shm_broadcast.py

[c7941cca1] Harry Mellor 2025-04-28 Explicitly explain quant method override ordering and ensure all overrides are ordered (#17256)
34	2	vllm/config.py
5	7	vllm/model_executor/layers/quantization/__init__.py

[b6dd32aa0] Harry Mellor 2025-04-28 Make name of `compressed-tensors` quant method consistent across vLLM (#17255)
1	5	tests/compile/test_full_graph.py
5	3	vllm/config.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	2	vllm/platforms/rocm.py
1	3	vllm/platforms/tpu.py

[f94886946] Harry Mellor 2025-04-28 Improve conversion from dataclass configs to argparse arguments (#17303)
106	2	tests/engine/test_arg_utils.py
125	138	vllm/engine/arg_utils.py
12	12	vllm/entrypoints/openai/cli_args.py
2	2	vllm/entrypoints/openai/run_batch.py

[72dfe4c74] Russell Bryant 2025-04-28 [Docs] Add a security guide (#17230)
58	0	docs/source/deployment/security.md
1	0	docs/source/index.md

[8b464d966] Cyrus Leung 2025-04-28 [Misc] Clean up Qwen2.5-Omni code (#17301)
8	51	vllm/model_executor/models/qwen2_5_omni_thinker.py
67	43	vllm/multimodal/processing.py

[889ebb263] Nicolò Lucchesi 2025-04-28 [Misc] Minor typo/grammar in `platforms/interface.py` (#17307)
2	2	vllm/platforms/interface.py

[3ad986c28] Reid 2025-04-28 [doc] update wrong model id (#17287)
5	3	docs/source/features/quantization/gptqmodel.md

[344e193b7] Cyrus Leung 2025-04-28 [Bugfix] Add missing `get_language_model` to new MLLMs (#17300)
3	0	vllm/model_executor/models/kimi_vl.py
3	0	vllm/model_executor/models/qwen2_5_omni_thinker.py

[fb1c933ad] Harry Mellor 2025-04-28 Add missing class docstring for `PromptAdapterConfig` (#17302)
2	0	vllm/config.py

[72c5b9723] idouba 2025-04-28 Update tpu_worker.py 's typo (#17288)
2	2	vllm/worker/tpu_worker.py

[fa93cd9f6] Alex Brooks 2025-04-28 [Model] Add Granite Speech Support (#16246)
7	0	docs/source/models/supported_models.md
32	0	examples/offline_inference/audio_language.py
30	4	tests/conftest.py
143	0	tests/models/decoder_only/audio_language/test_granite_speech.py
6	11	tests/models/decoder_only/audio_language/test_ultravox.py
1	0	tests/models/multimodal/processing/test_common.py
3	1	tests/models/registry.py
1	1	vllm/entrypoints/chat_utils.py
24	11	vllm/model_executor/models/blip2.py
777	0	vllm/model_executor/models/granite_speech.py
1	0	vllm/model_executor/models/registry.py

[aec9674db] Cyrus Leung 2025-04-28 [Core] Remove legacy input mapper/processor from V0 (#15686)
0	1	vllm/core/scheduler.py
1	2	vllm/engine/async_llm_engine.py
3	10	vllm/engine/llm_engine.py
3	5	vllm/inputs/__init__.py
2	159	vllm/inputs/data.py
11	49	vllm/inputs/preprocess.py
23	298	vllm/inputs/registry.py
1	3	vllm/multimodal/__init__.py
1	22	vllm/multimodal/audio.py
17	267	vllm/multimodal/base.py
1	79	vllm/multimodal/image.py
42	233	vllm/multimodal/registry.py
2	69	vllm/multimodal/video.py
17	30	vllm/sequence.py
0	35	vllm/transformers_utils/processor.py
5	20	vllm/worker/cpu_model_runner.py
2	0	vllm/worker/enc_dec_model_runner.py
3	9	vllm/worker/hpu_model_runner.py
5	17	vllm/worker/model_runner.py
3	17	vllm/worker/neuron_model_runner.py
2	14	vllm/worker/xpu_model_runner.py

[7fcc4223d] Wanrui Dai 2025-04-28 [Minor][Models] Pass partial_rotary_factor parameter to rope (#17266)
4	3	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/persimmon.py
4	4	vllm/model_executor/models/stablelm.py

[8262a3e23] Nick Hill 2025-04-27 [Misc] Validate `stop_token_ids` contents (#17268)
4	0	vllm/sampling_params.py

[f211331c4] Reid 2025-04-28 [Doc] small fix (#17277)
1	1	docs/source/models/generative_models.md
2	0	docs/source/models/supported_models.md

[9053d0b13] Kuntai Du 2025-04-27 [Doc] Fix wrong github link in LMCache examples (#17274)
2	2	examples/lmcache/README.md

[cb3f2d8d1] Michael Goin 2025-04-27 [Bugfix] Fix Mistral3 spatial merge error (#17270)
3	0	vllm/model_executor/models/mistral3.py
2	3	vllm/model_executor/models/pixtral.py

[c12df53b6] TherLF 2025-04-28 [Bugfix] Fix cutlass dispatch for fp8/int8 to properly invoke M<=16 c… (#16751)
1	1	csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_fp8_dispatch.cuh
1	1	csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_int8_dispatch.cuh

[d1aeea755] Lennart K. M. Schulz 2025-04-28 [Bugfix] Fix missing ARG in Dockerfile for arm64 platforms (#17261)
1	0	docker/Dockerfile

[d8bccde68] Lucas Wilkinson 2025-04-27 [BugFix] Fix vllm_flash_attn install issues (#17267)
1	0	.github/CODEOWNERS
0	2	.gitignore
19	7	setup.py
3	3	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/mla/common.py
0	0	vllm/{vllm_flash_attn => attention/utils}/fa_utils.py
1	1	vllm/engine/arg_utils.py
2	2	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/mla/common.py
0	22	vllm/vllm_flash_attn/__init__.py
0	245	vllm/vllm_flash_attn/flash_attn_interface.pyi

[20e489eaa] Lily Liu 2025-04-27 [V1][Spec Decode] Make eagle compatible with prefix caching. (#17137)
57	0	tests/v1/core/test_prefix_caching.py
0	1	tests/v1/e2e/test_spec_decode.py
10	0	vllm/v1/core/kv_cache_manager.py
14	8	vllm/v1/core/sched/scheduler.py

[4213475ec] Cyrus Leung 2025-04-28 [Metrics] Fix minor inconsistencies in bucket progression (#17262)
1	1	vllm/engine/metrics.py
2	2	vllm/v1/metrics/loggers.py

[d92879baf] Reid 2025-04-27 [doc] Add feature status legend (#17257)
10	0	docs/source/models/supported_models.md

[690fe019f] cascade 2025-04-27 [Feature] support sequence parallelism using compilation pass (#16155)
3	0	.buildkite/test-pipeline.yaml
8	6	tests/compile/test_functionalization.py
5	4	tests/compile/test_fusion.py
5	4	tests/compile/test_pass_manager.py
190	0	tests/compile/test_sequence_parallelism.py
30	1	tests/distributed/test_comm_ops.py
296	0	tests/distributed/test_sequence_parallel.py
1	1	vllm/compilation/backends.py
8	5	vllm/compilation/compiler_interface.py
4	4	vllm/compilation/fusion.py
16	0	vllm/compilation/fx_utils.py
32	0	vllm/compilation/inductor_pass.py
18	11	vllm/compilation/pass_manager.py
266	0	vllm/compilation/sequence_parallelism.py
8	5	vllm/compilation/vllm_inductor_pass.py
50	2	vllm/config.py
6	0	vllm/distributed/communication_op.py
34	0	vllm/distributed/device_communicators/base_device_communicator.py
25	0	vllm/distributed/device_communicators/cuda_communicator.py
58	0	vllm/distributed/parallel_state.py
9	1	vllm/v1/worker/gpu_model_runner.py

[ed7a29d9f] Kaixi Hou 2025-04-27 [NVIDIA] Support Cutlass MLA for Blackwell GPUs (#16032)
24	4	CMakeLists.txt
38	0	csrc/attention/mla/cutlass_mla_entry.cu
225	0	csrc/attention/mla/cutlass_mla_kernels.cu
6	0	csrc/ops.h
1	1	csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu
7	0	csrc/torch_bindings.cpp
93	0	tests/kernels/test_cutlass_mla_decode.py
9	0	vllm/_custom_ops.py

[756848e79] Alex Brooks 2025-04-27 [Bugfix] Fix Lora Name Parsing (#17196)
12	0	tests/lora/test_utils.py
9	4	vllm/lora/utils.py

[18445edd0] Flex Wang 2025-04-27 [Misc] Change buckets of histogram_iteration_tokens to [1, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8096] to represent number of tokens (#17033)
3	6	vllm/engine/metrics.py
4	11	vllm/v1/metrics/loggers.py

[30215ca61] Jade Zheng 2025-04-27 [MISC] Use string annotation types for class definitions (#17244)
2	5	vllm/platforms/cuda.py
2	5	vllm/platforms/rocm.py

[838cedade] Chen Zhang 2025-04-27 [Bugfix] Get a specific type of layer from forward context (#17222)
2	4	vllm/attention/backends/flashinfer.py
15	1	vllm/config.py
3	4	vllm/v1/attention/backends/flashinfer.py
5	10	vllm/v1/worker/gpu_model_runner.py
3	4	vllm/v1/worker/tpu_model_runner.py

[4283a28c2] Jee Jee Li 2025-04-27 [Bugfix] Fix QWen2 VL multimodal mapping (#17240)
3	2	vllm/model_executor/models/qwen2_5_vl.py
3	2	vllm/model_executor/models/qwen2_vl.py

[93a126fbc] Cyrus Leung 2025-04-27 [Misc] Make cached tokenizer pickle-compatible (#17048)
8	6	benchmarks/benchmark_prefix_caching.py
31	12	tests/tokenization/test_cached_tokenizer.py
19	16	vllm/transformers_utils/tokenizer.py
17	17	vllm/transformers_utils/tokenizer_base.py
5	5	vllm/transformers_utils/tokenizers/mistral.py

[8e4b351a0] rasmith 2025-04-26 [Kernel][Triton][FP8] Adding fp8 and variable length sequence support to Triton FAv2 kernel (#12591)
499	0	tests/kernels/test_triton_flash_attention.py
1158	604	vllm/attention/ops/triton_flash_attention.py

[9869453c4] Happy 2025-04-27 Update test_flash_attn.py (#17102)
1	1	tests/kernels/attention/test_flash_attn.py

[3642c59aa] Reid 2025-04-27 [CI/Build] remove -t for run-lm-eval-gsm-hf-baseline.sh (#16271)
1	0	.buildkite/lm-eval-harness/configs/DeepSeek-V2-Lite-Chat.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct-FBGEMM-nonuniform.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FBGEMM-nonuniform.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-INT8-compressed-tensors-asym.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-INT8-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-nonuniform-compressed-tensors.yaml
2	1	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-QQQ.yaml
1	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-INT8-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Minitron-4B-Base-FP8.yaml
1	0	.buildkite/lm-eval-harness/configs/Mixtral-8x22B-Instruct-v0.1-FP8-Dynamic.yaml
1	0	.buildkite/lm-eval-harness/configs/Mixtral-8x7B-Instruct-v0.1-FP8.yaml
2	1	.buildkite/lm-eval-harness/configs/Mixtral-8x7B-Instruct-v0.1.yaml
1	0	.buildkite/lm-eval-harness/configs/Qwen1.5-MoE-W4A16-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-FP8W8.yaml
1	0	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-INT8-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-W8A16-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/Qwen2-57B-A14-Instruct.yaml
1	0	.buildkite/lm-eval-harness/configs/SparseLlama3.1_2of4_fp8_compressed.yaml

[43eea2953] Woosuk Kwon 2025-04-26 [Minor] Fix lint error in main branch (#17233)
2	2	vllm/model_executor/layers/rotary_embedding.py

[de7eb10ce] Kero Liang 2025-04-27 [Bugfix] Fix Qwen2.5-Omni M-RoPE position ids generation (#16878)
13	15	vllm/model_executor/layers/rotary_embedding.py

[fd11a325b] Ning Xie 2025-04-27 [MISC] rename interval to max_recent_requests (#14285)
1	1	tests/v1/core/test_kv_cache_utils.py
6	6	vllm/v1/core/kv_cache_utils.py

[4d17e2031] Lu Fang 2025-04-26 Disable the torch.compile cache checks when VLLM_DISABLE_COMPILE_CACHE=1 (#16573)
9	4	vllm/compilation/compiler_interface.py

[10fd1d738] changjun.lee 2025-04-27 [Bugfix] fix error due to an uninitialized tokenizer when using `skip_tokenizer_init` with `num_scheduler_steps` (#9276)
1	1	vllm/engine/output_processor/multi_step.py

[52b4f4a8d] Russell Bryant 2025-04-26 [Docs] Update structured output doc for V1 (#17135)
28	13	docs/source/features/structured_outputs.md

[e782e0a17] Aaron Pham 2025-04-26 [Chore] added stubs for `vllm_flash_attn` during development mode (#17228)
2	1	pyproject.toml
0	1	setup.py
22	0	vllm/vllm_flash_attn/__init__.py
245	0	vllm/vllm_flash_attn/flash_attn_interface.pyi

[dc2ceca5c] Ning Xie 2025-04-26 [BUGFIX] use random for NONE_HASH only when PYTHONHASHSEED not set (#17088)
1	1	vllm/v1/core/kv_cache_utils.py

[f8acd01ff] Russell Bryant 2025-04-26 [V1] Add `structural_tag` support using xgrammar (#17085)
85	0	examples/online_serving/openai_chat_completion_structured_outputs_structural_tag.py
101	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
3	1	vllm/entrypoints/llm.py
39	7	vllm/entrypoints/openai/protocol.py
6	5	vllm/model_executor/guided_decoding/guided_fields.py
5	2	vllm/sampling_params.py
3	0	vllm/v1/structured_output/backend_guidance.py
1	0	vllm/v1/structured_output/backend_types.py
25	0	vllm/v1/structured_output/backend_xgrammar.py
2	0	vllm/v1/structured_output/request.py

[c48334d40] Agata Dobrzyniewicz 2025-04-26 [Hardware][Intel-Gaudi] Update hpu-extension and update bucketing system for HPU device (#17186)
1	1	requirements/hpu.txt
54	52	vllm/attention/backends/hpu_attn.py
0	1	vllm/attention/ops/hpu_paged_attn.py
2	1	vllm/model_executor/layers/layernorm.py
70	280	vllm/worker/hpu_model_runner.py
1	0	vllm/worker/hpu_worker.py

[909fdaf15] Cyrus Leung 2025-04-26 [Bugfix] Fix standard models tests (#17217)
1	1	docs/source/models/supported_models.md
3	3	tests/distributed/test_pipeline_parallel.py
41	31	tests/models/decoder_only/language/test_models.py
24	25	tests/models/registry.py

[8c1c926d0] Isotr0py 2025-04-26 [Bugfix] Fix missing int type for `-n` in multi-image example (#17223)
3	1	examples/offline_inference/vision_language_multi_image.py

[df6f3ce88] Nick Hill 2025-04-25 [Core] Remove prompt string from engine core data structures (#17214)
2	2	tests/tokenization/test_detokenize.py
0	1	tests/v1/core/test_kv_cache_utils.py
0	1	tests/v1/core/test_prefix_caching.py
0	1	tests/v1/core/test_scheduler.py
1	2	tests/v1/engine/test_engine_core.py
0	1	tests/v1/engine/test_engine_core_client.py
16	26	tests/v1/engine/test_output_processor.py
0	1	tests/v1/tpu/worker/test_tpu_model_runner.py
0	1	tests/v1/worker/test_gpu_input_batch.py
0	1	tests/v1/worker/test_gpu_model_runner.py
0	2	vllm/v1/core/sched/output.py
0	3	vllm/v1/engine/__init__.py
9	8	vllm/v1/engine/async_llm.py
0	9	vllm/v1/engine/core_client.py
6	7	vllm/v1/engine/llm_engine.py
4	1	vllm/v1/engine/output_processor.py
2	3	vllm/v1/engine/processor.py
0	3	vllm/v1/request.py
0	1	vllm/v1/worker/gpu_input_batch.py
0	1	vllm/v1/worker/gpu_model_runner.py
0	1	vllm/v1/worker/tpu_model_runner.py

[513f07476] Woosuk Kwon 2025-04-25 [CI/test] Fix Eagle Correctness Test (#17209)
2	2	tests/v1/e2e/test_spec_decode.py
3	2	vllm/config.py

[b07bf83c7] Nick Hill 2025-04-25 [BugFix] Avoid race conditions in zero-copy tensor transmission (#17203)
3	0	tests/v1/test_serial_utils.py
22	3	vllm/v1/engine/core.py
52	9	vllm/v1/engine/core_client.py

[53e8cf53a] Zijing Liu 2025-04-25 [V1][Metrics] Allow V1 AsyncLLM to use custom logger (#14661)
33	0	tests/v1/engine/test_async_llm.py
33	19	vllm/v1/engine/async_llm.py
9	9	vllm/v1/engine/llm_engine.py
43	2	vllm/v1/metrics/loggers.py

[54271bb76] Charlie Fu 2025-04-26 [ROCm][Misc] Follow-ups for Skinny Gemms on ROCm. (#17011)
4	3	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
4	3	vllm/model_executor/layers/utils.py
2	1	vllm/model_executor/layers/vocab_parallel_embedding.py
8	8	vllm/platforms/rocm.py

[9e96f56ef] Shu Wang 2025-04-26 Allocate kv_cache with stride order (#16605)
21	18	csrc/cache_kernels.cu
41	19	tests/kernels/attention/test_cache.py
4	0	vllm/attention/backends/abstract.py
25	5	vllm/attention/backends/flashinfer.py
10	3	vllm/utils.py
18	5	vllm/worker/cache_engine.py

[b27891122] Woosuk Kwon 2025-04-25 [Minor][Models] Fix Return Types of Llama & Eagle (#17220)
2	1	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/llama_eagle.py
2	2	vllm/model_executor/models/llama_eagle3.py

[7bd0c7745] yarongmu-google 2025-04-25 [Doc] Minor fix for the vLLM TPU setup page (#17206)
4	4	docs/source/getting_started/installation/ai_accelerator/tpu.inc.md

[1cf0719eb] Woosuk Kwon 2025-04-25 [Minor][Spec Decode] Add use_eagle to SpeculativeConfig (#17213)
3	0	vllm/config.py
1	1	vllm/v1/core/sched/scheduler.py
2	4	vllm/v1/worker/gpu_model_runner.py

[537d5ee02] Reid 2025-04-26 [doc] add Anything LLM integration (#17216)
-	-	docs/source/assets/deployment/anything-llm-chat-with-doc.png
-	-	docs/source/assets/deployment/anything-llm-chat-without-doc.png
-	-	docs/source/assets/deployment/anything-llm-provider.png
-	-	docs/source/assets/deployment/anything-llm-upload-doc.png
47	0	docs/source/deployment/frameworks/anything-llm.md
1	0	docs/source/deployment/frameworks/index.md

[c8e5be35f] Lu Fang 2025-04-25 [MISC][AMD] Add unused annotation to rocm kernel file (#17097)
2	2	csrc/rocm/skinny_gemms.cu

[a6e72e1e4] James Wu 2025-04-25 [Bugfix] [pytorch] Patch AOTAutogradCache._get_shape_env (#17142)
18	1	vllm/compilation/compiler_interface.py

[5e83a7277] Yihua Cheng 2025-04-25 [v1] [P/D] Adding LMCache KV connector for v1 (#16625)
56	0	examples/lmcache/README.md
0	0	examples/{offline_inference/cpu_offload_lmcache.py => lmcache/cpu_offload_lmcache_v0.py}
57	0	examples/lmcache/cpu_offload_lmcache_v1.py
0	0	examples/{offline_inference/disaggregated_prefill_lmcache.py => lmcache/disagg_prefill_lmcache_v0.py}
13	0	examples/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-decoder-config.yaml
13	0	examples/lmcache/disagg_prefill_lmcache_v1/configs/lmcache-prefiller-config.yaml
136	0	examples/lmcache/disagg_prefill_lmcache_v1/disagg_example_nixl.sh
193	0	examples/lmcache/disagg_prefill_lmcache_v1/disagg_proxy_server.py
59	0	examples/lmcache/disagg_prefill_lmcache_v1/disagg_vllm_launcher.sh
130	0	examples/lmcache/kv_cache_sharing_lmcache_v1.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
131	0	vllm/distributed/kv_transfer/kv_connector/v1/lmcache_connector.py

[68af5f6c5] rasmith 2025-04-25 [AMD][FP8][BugFix] Remove V1 check in arg_utils.py for FP8 since it is not necessary (#17215)
0	17	vllm/engine/arg_utils.py
0	12	vllm/model_executor/layers/quantization/quark/quark.py

[8de2901fe] Chen Zhang 2025-04-26 [Bugfix] gemma[2,3] interleaved attention when sliding window is disabled (#17180)
2	2	vllm/model_executor/models/gemma2.py
3	1	vllm/model_executor/models/gemma3.py
10	8	vllm/model_executor/models/gemma3_mm.py

[c53e0730c] Rui Qiao 2025-04-25 [Misc] Refine ray_serve_deepseek example (#17204)
29	25	examples/online_serving/ray_serve_deepseek.py

[a0e619e62] Benjamin Chislett 2025-04-25 [V1][Spec Decode] EAGLE-3 Support (#16937)
11	3	examples/offline_inference/eagle.py
4	0	tests/models/registry.py
16	8	tests/v1/e2e/test_spec_decode.py
10	3	vllm/config.py
1	1	vllm/engine/arg_utils.py
17	1	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/llama_eagle.py
232	0	vllm/model_executor/models/llama_eagle3.py
1	0	vllm/model_executor/models/registry.py
1	1	vllm/v1/core/sched/scheduler.py
28	11	vllm/v1/spec_decode/eagle.py
36	6	vllm/v1/worker/gpu_model_runner.py

[70116459c] Nick Hill 2025-04-25 [BugFix][Frontend] Fix `LLM.chat()` tokenization (#16081)
28	0	tests/entrypoints/llm/test_chat.py
15	14	vllm/entrypoints/llm.py

[65e262b93] Christian Heimes 2025-04-26 Fix Python packaging edge cases (#17159)
1	0	.gitignore
1	2	pyproject.toml
0	0	vllm/benchmarks/__init__.py
0	0	vllm/vllm_flash_attn/__init__.py

[43faa0461] Cyrus Leung 2025-04-26 [Bugfix] Fix hybrid model tests (#17182)
4	1	tests/conftest.py
154	196	tests/models/decoder_only/language/test_hybrid.py
0	337	tests/models/decoder_only/language/test_mamba.py

[48cb2109b] Daniel Li 2025-04-25 [V1] Move usage stats to worker and start logging TPU hardware (#16211)
9	0	vllm/usage/usage_lib.py
0	4	vllm/v1/engine/async_llm.py
0	4	vllm/v1/engine/llm_engine.py
3	1	vllm/v1/utils.py
5	0	vllm/v1/worker/gpu_worker.py
5	1	vllm/v1/worker/tpu_worker.py

[a5450f11c] Russell Bryant 2025-04-25 [Security] Use safe serialization and fix zmq setup for mooncake pipe (#17192)
13	8	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py

[9d98ab5ec] Cyrus Leung 2025-04-26 [Misc] Inline Molmo requirements (#17190)
27	1	docs/source/models/supported_models.md
0	20	requirements/molmo.txt

[df5c87952] Reid 2025-04-26 [doc] update wrong hf model links (#17184)
1	1	docs/source/features/quantization/auto_awq.md
2	2	docs/source/features/quantization/bitblas.md
1	1	docs/source/features/quantization/bnb.md
1	1	docs/source/features/quantization/gptqmodel.md
1	2	docs/source/features/quantization/torchao.md

[423e9f1cb] Harry Mellor 2025-04-25 Use Transformers helper `get_text_config()` instead of checking for `text_config` (#17105)
2	3	benchmarks/kernels/benchmark_moe.py
1	4	tests/models/test_initialization.py
2	4	vllm/config.py
12	9	vllm/transformers_utils/config.py
3	8	vllm/worker/cpu_model_runner.py
5	9	vllm/worker/hpu_model_runner.py
5	9	vllm/worker/model_runner.py

[0bd7f8fca] Harry Mellor 2025-04-25 Bump Transformers to 4.51.3 (#17116)
1	1	requirements/test.in
1	1	requirements/test.txt
21	22	tests/models/decoder_only/language/test_models.py
2	1	tests/models/registry.py

[d5615af9a] Jasmond L 2025-04-25 [Bugfix] Fix Mistral ChatCompletionRequest Body Exception (#16769)
32	8	vllm/entrypoints/chat_utils.py

[19dcc02a7] Cyrus Leung 2025-04-25 [Bugfix] Fix mistral model tests (#17181)
36	28	tests/models/decoder_only/language/test_mistral.py
4	0	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[7feae92c1] Alex Brooks 2025-04-25 [Doc] Move todo out of beam search docstring (#17183)
2	4	vllm/entrypoints/llm.py

[f851b8426] Michael Yao 2025-04-25 [Doc] Add two links to disagg_prefill.md (#17168)
2	2	docs/source/features/disagg_prefill.md

[fc966e9cc] Lu Fang 2025-04-25 Only turn on FastIncrementalDetokenizer when tokenizers >= 0.21.1 (#17158)
5	1	vllm/v1/engine/detokenizer.py

[ef19e67d2] Michael Yao 2025-04-25 [Doc] Add headings to improve gptqmodel.md (#17164)
11	0	docs/source/features/quantization/gptqmodel.md

[a41351f36] rasmith 2025-04-25 [Quantization][FP8] Add support for FP8 models with input_scale for output projection and QK quantization (#15734)
1	0	vllm/attention/backends/abstract.py
7	0	vllm/attention/backends/rocm_flash_attn.py
1	0	vllm/attention/layer.py
11	0	vllm/config.py
17	0	vllm/engine/arg_utils.py
5	0	vllm/model_executor/layers/quantization/fp8.py
36	0	vllm/model_executor/layers/quantization/kv_cache.py
27	20	vllm/model_executor/layers/quantization/quark/quark.py

[6aae216b4] Sangyeon Cho 2025-04-25 [Bugfix] remove fallback in guided_json (int range, patterns) (#16725)
57	1	tests/entrypoints/llm/test_guided_generate.py
18	16	tests/v1/entrypoints/conftest.py
16	37	tests/v1/structured_output/test_utils.py
1	1	vllm/model_executor/guided_decoding/__init__.py
1	9	vllm/model_executor/guided_decoding/utils.py
1	8	vllm/v1/structured_output/backend_xgrammar.py

[b22980a1d] yexin(叶鑫) 2025-04-25 [Perf]Optimize rotary_emb implementation to use Triton operator for improved inference performance (#16457)
1	1	cmake/external_projects/vllm_flash_attn.cmake
23	11	vllm/model_executor/layers/rotary_embedding.py

[881f73582] Lucas Wilkinson 2025-04-25 [Misc] Benchmark Serving Script Support Appending Results (#17028)
20	11	benchmarks/benchmark_serving.py

[2f5404550] Mengqing Cao 2025-04-25 [Bugfix][Misc] Use TritonPlaceholderModule to defensively import triton (#15099)
8	2	benchmarks/kernels/benchmark_lora.py
3	1	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
1	1	vllm/triton_utils/__init__.py
38	2	vllm/triton_utils/importing.py
3	0	vllm/utils.py

[5aa6efb9a] Lifu Huang 2025-04-24 [Misc] Clean up redundant code in uniproc_executor.py (#16762)
2	2	vllm/executor/uniproc_executor.py

[6ca023447] Harry Mellor 2025-04-25 Move missed `SchedulerConfig` args into scheduler config group in `EngineArgs` (#17131)
2	1	vllm/config.py
7	16	vllm/engine/arg_utils.py

[649818995] Michael Goin 2025-04-24 [Docs] Fix True->true in supported_models.md (#17141)
3	3	docs/source/models/supported_models.md

[7a0a9da72] Varun Sundar Rabindranath 2025-04-24 [Doc] V1 : Update LoRA status (#17133)
1	6	docs/source/getting_started/v1_user_guide.md

[69bff9bc8] Zaida Zhou 2025-04-25 fix float16 support for kimi-vl (#17156)
1	2	vllm/model_executor/models/kimi_vl.py

[41ca7eb49] Lucas Wilkinson 2025-04-24 [Attention] FA3 decode perf improvement - single mma warp group support for head dim 128 (#16864)
1	1	cmake/external_projects/vllm_flash_attn.cmake

[eef364723] vllmellm 2025-04-25 [FEAT] [ROCm]: AITER Fused MOE V1 Support (#16752)
4	3	tests/model_executor/test_enabled_custom_ops.py
299	128	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[0d6e187e8] jglaser 2025-04-24 Use custom address for listening socket (#15988)
21	1	vllm/distributed/utils.py

[9420a1fc3] Michael Goin 2025-04-24 Better error message for missing mistral params.json (#17132)
5	0	vllm/transformers_utils/config.py

[583e90099] Rui Qiao 2025-04-24 [Misc] Add example to run DeepSeek with Ray Serve LLM (#17134)
44	0	examples/online_serving/ray_serve_deepseek.py

[05e1fbfc5] Maximilien de Bayser 2025-04-24 Add chat template for Llama 4 models (#16428)
8	1	docs/source/features/tool_calling.md
116	0	examples/tool_chat_template_llama4_json.jinja
14	0	tests/tool_use/utils.py
1	0	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py

[fe9217632] Yinghai Lu 2025-04-24 Add collective_rpc to llm engine (#16999)
18	0	vllm/engine/async_llm_engine.py
11	0	vllm/v1/engine/async_llm.py

[6d0df0ebe] Russell Bryant 2025-04-24 [Docs] Generate correct github links for decorated functions (#17125)
5	0	docs/source/conf.py

[0fa939e2d] Harry Mellor 2025-04-24 Improve configs - `LoRAConfig` + `PromptAdapterConfig` (#16980)
22	8	tests/lora/test_lora_manager.py
38	8	vllm/config.py
70	75	vllm/engine/arg_utils.py

[0422ce109] Harry Mellor 2025-04-24 Add `:markdownhelp:` to `EngineArgs` docs so markdown docstrings render properly (#17124)
2	0	docs/source/serving/engine_args.md
1	0	requirements/docs.txt

[47bdee409] Eyshika Agarwal 2025-04-24 Molmo Requirements (#17026)
4	0	docs/source/models/supported_models.md
20	0	requirements/molmo.txt

[49f189439] Atilla 2025-04-24 existing torch installation pip command fix for docs (#17059)
1	1	docs/source/getting_started/installation/gpu/cuda.inc.md

[5adf6f6b7] Aaruni Aggarwal 2025-04-24 Updating builkite job for IBM Power  (#17111)
11	4	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh

[4115f1995] Russell Bryant 2025-04-24 [CI] Add automation for the `tool-calling` github label (#17118)
22	0	.github/mergify.yml

[340d7b1b2] Mark McLoughlin 2025-04-24 [V1][Spec Decoding] Add num_drafts and num_accepted_tokens_per_position metrics (#16665)
25	14	tests/v1/core/test_scheduler.py
9	7	vllm/v1/core/sched/scheduler.py
9	26	vllm/v1/metrics/loggers.py
115	13	vllm/v1/spec_decode/metrics.py

[1bcbcbf57] Reid 2025-04-24 [Misc] refactor example series - structured outputs (#17040)
117	77	examples/online_serving/openai_chat_completion_structured_outputs.py
92	66	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py

[82e43b2d7] Michael Goin 2025-04-24 Add missing rocm_skinny_gemms kernel test to CI (#17060)
60	0	tests/kernels/quant_utils.py
1	1	tests/kernels/quantization/test_block_fp8.py
1	1	tests/kernels/quantization/test_block_int8.py
0	0	tests/kernels/{ => quantization}/test_rocm_skinny_gemms.py
0	63	tests/kernels/utils_block.py

[67309a1cb] wang.yuqi 2025-04-24 [Frontend] Using matryoshka_dimensions control the allowed output dimensions. (#16970)
5	5	docs/source/models/pooling_models.md
2	2	examples/online_serving/openai_embedding_matryoshka_fy.py
24	18	tests/entrypoints/openai/test_embedding.py
91	43	tests/entrypoints/openai/test_embedding_dimensions.py
21	11	tests/models/embedding/language/test_jina.py
20	1	tests/models/embedding/utils.py
4	0	vllm/config.py
10	1	vllm/pooling_params.py

[b724afe34] Shanshan Shen 2025-04-24 [V1][Structured Output] Clear xgrammar compiler object when engine core shut down to avoid nanobind leaked warning (#16954)
1	0	vllm/v1/engine/core.py
4	0	vllm/v1/structured_output/__init__.py
3	0	vllm/v1/structured_output/backend_guidance.py
6	0	vllm/v1/structured_output/backend_types.py
3	0	vllm/v1/structured_output/backend_xgrammar.py

[21f4f1c9a] Harry Mellor 2025-04-24 Improve static type checking in `LoRAModelRunnerMixin` (#17104)
6	10	vllm/v1/worker/lora_model_runner_mixin.py

[b0c1f6202] Isotr0py 2025-04-24 [Misc] Remove OLMo2 config copy (#17066)
1	1	vllm/model_executor/models/olmo2.py
3	5	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	168	vllm/transformers_utils/configs/olmo2.py

[c0dfd9751] Rui Qiao 2025-04-24 [V1][PP] Optimization: continue scheduling prefill chunks (#17080)
0	4	tests/v1/core/test_scheduler.py
84	22	tests/v1/engine/test_engine_core.py
0	5	vllm/v1/core/sched/interface.py
32	35	vllm/v1/core/sched/scheduler.py
12	8	vllm/v1/engine/core.py

[a9138e85b] Harry Mellor 2025-04-24 Fix OOT registration test (#17099)
2	3	tests/models/test_oot_registration.py

[0a05ed57e] Harry Mellor 2025-04-24 Simplify `TokenizerGroup` (#16790)
1	15	tests/conftest.py
3	7	tests/lora/test_tokenizer_group.py
2	7	tests/tokenization/test_detokenize.py
3	184	tests/tokenization/test_tokenizer_group.py
1	1	tests/v1/engine/conftest.py
2	3	tests/v1/engine/utils.py
22	75	vllm/config.py
8	18	vllm/engine/arg_utils.py
0	2	vllm/engine/async_llm_engine.py
6	24	vllm/engine/llm_engine.py
0	1	vllm/engine/multiprocessing/client.py
2	3	vllm/entrypoints/llm.py
3	3	vllm/inputs/preprocess.py
2	2	vllm/transformers_utils/detokenizer.py
17	13	vllm/transformers_utils/{tokenizer_group => }/tokenizer_group.py
0	56	vllm/transformers_utils/tokenizer_group/__init__.py
0	68	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
0	244	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
0	2	vllm/v1/engine/async_llm.py
4	16	vllm/v1/engine/llm_engine.py
2	2	vllm/v1/engine/output_processor.py
2	2	vllm/v1/engine/processor.py
0	2	vllm/v1/structured_output/backend_guidance.py
0	2	vllm/v1/structured_output/backend_xgrammar.py

[14288d133] Michael Goin 2025-04-24 Disable enforce_eager for V1 TPU sampler and structured output tests (#17016)
1	0	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
5	1	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	1	tests/v1/tpu/test_sampler.py

[b411418ff] Woosuk Kwon 2025-04-24 [Chore] Remove Sampler from Model Code (#17084)
2	3	tests/spec_decode/test_scorer.py
0	10	vllm/model_executor/models/arctic.py
1	11	vllm/model_executor/models/aria.py
0	16	vllm/model_executor/models/aya_vision.py
0	10	vllm/model_executor/models/baichuan.py
0	10	vllm/model_executor/models/bamba.py
0	10	vllm/model_executor/models/bart.py
1	17	vllm/model_executor/models/blip2.py
0	10	vllm/model_executor/models/bloom.py
0	10	vllm/model_executor/models/chameleon.py
0	10	vllm/model_executor/models/chatglm.py
0	10	vllm/model_executor/models/commandr.py
0	10	vllm/model_executor/models/dbrx.py
0	10	vllm/model_executor/models/deepseek.py
0	11	vllm/model_executor/models/deepseek_mtp.py
0	10	vllm/model_executor/models/deepseek_v2.py
0	16	vllm/model_executor/models/deepseek_vl2.py
0	13	vllm/model_executor/models/eagle.py
0	11	vllm/model_executor/models/exaone.py
0	10	vllm/model_executor/models/falcon.py
0	21	vllm/model_executor/models/florence2.py
0	13	vllm/model_executor/models/fuyu.py
0	10	vllm/model_executor/models/gemma.py
0	10	vllm/model_executor/models/gemma2.py
0	10	vllm/model_executor/models/gemma3.py
2	14	vllm/model_executor/models/gemma3_mm.py
0	10	vllm/model_executor/models/glm4.py
0	10	vllm/model_executor/models/gpt2.py
0	10	vllm/model_executor/models/gpt_bigcode.py
0	10	vllm/model_executor/models/gpt_j.py
0	10	vllm/model_executor/models/gpt_neox.py
0	11	vllm/model_executor/models/granite.py
0	11	vllm/model_executor/models/granitemoe.py
0	11	vllm/model_executor/models/granitemoeshared.py
0	10	vllm/model_executor/models/grok1.py
0	10	vllm/model_executor/models/idefics3.py
0	9	vllm/model_executor/models/interfaces_base.py
1	11	vllm/model_executor/models/internlm2.py
1	17	vllm/model_executor/models/internvl.py
0	10	vllm/model_executor/models/jais.py
0	10	vllm/model_executor/models/jamba.py
1	11	vllm/model_executor/models/kimi_vl.py
0	8	vllm/model_executor/models/llama.py
0	16	vllm/model_executor/models/llava.py
0	16	vllm/model_executor/models/llava_next.py
0	16	vllm/model_executor/models/llava_next_video.py
0	16	vllm/model_executor/models/llava_onevision.py
0	10	vllm/model_executor/models/mamba.py
0	10	vllm/model_executor/models/mamba2.py
0	10	vllm/model_executor/models/minicpm.py
1	17	vllm/model_executor/models/minicpmv.py
0	12	vllm/model_executor/models/minimax_text_01.py
0	16	vllm/model_executor/models/mistral3.py
0	10	vllm/model_executor/models/mixtral.py
0	10	vllm/model_executor/models/mixtral_quant.py
0	10	vllm/model_executor/models/mllama.py
0	13	vllm/model_executor/models/mllama4.py
1	11	vllm/model_executor/models/molmo.py
0	10	vllm/model_executor/models/mpt.py
0	11	vllm/model_executor/models/nemotron.py
0	8	vllm/model_executor/models/nemotron_nas.py
0	10	vllm/model_executor/models/olmo.py
0	10	vllm/model_executor/models/olmo2.py
0	10	vllm/model_executor/models/olmoe.py
0	10	vllm/model_executor/models/opt.py
0	10	vllm/model_executor/models/orion.py
1	13	vllm/model_executor/models/paligemma.py
0	10	vllm/model_executor/models/persimmon.py
0	10	vllm/model_executor/models/phi.py
1	12	vllm/model_executor/models/phi3_small.py
0	16	vllm/model_executor/models/phi3v.py
0	10	vllm/model_executor/models/phi4mm.py
0	10	vllm/model_executor/models/phimoe.py
0	15	vllm/model_executor/models/pixtral.py
0	10	vllm/model_executor/models/plamo2.py
0	10	vllm/model_executor/models/qwen.py
0	10	vllm/model_executor/models/qwen2.py
1	16	vllm/model_executor/models/qwen2_5_omni_thinker.py
1	16	vllm/model_executor/models/qwen2_5_vl.py
0	16	vllm/model_executor/models/qwen2_audio.py
0	10	vllm/model_executor/models/qwen2_moe.py
1	16	vllm/model_executor/models/qwen2_vl.py
0	10	vllm/model_executor/models/qwen3.py
0	10	vllm/model_executor/models/qwen3_moe.py
1	17	vllm/model_executor/models/skyworkr1v.py
0	11	vllm/model_executor/models/solar.py
0	10	vllm/model_executor/models/stablelm.py
0	10	vllm/model_executor/models/starcoder2.py
0	9	vllm/model_executor/models/transformers.py
0	16	vllm/model_executor/models/ultravox.py
0	10	vllm/model_executor/models/whisper.py
0	19	vllm/model_executor/models/zamba2.py
1	1	vllm/spec_decode/draft_model_runner.py
2	3	vllm/spec_decode/multi_step_worker.py
2	2	vllm/spec_decode/spec_decode_worker.py
10	6	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/worker/cpu_enc_dec_model_runner.py
3	7	vllm/worker/cpu_model_runner.py
1	1	vllm/worker/enc_dec_model_runner.py
4	3	vllm/worker/hpu_model_runner.py
3	2	vllm/worker/model_runner.py
1	2	vllm/worker/multi_step_model_runner.py
3	2	vllm/worker/xpu_model_runner.py

[2bc0f72ae] omer-dayan 2025-04-24 Add docs for runai_streamer_sharded (#17093)
26	0	docs/source/models/extensions/runai_model_streamer.md

[9c1244de5] Reid 2025-04-24 [doc] update to hyperlink (#17096)
1	1	docs/source/features/quantization/auto_awq.md

[db2f8d915] Reid 2025-04-24 [V1] Update structured output (#16812)
6	6	benchmarks/benchmark_serving_structured_output.py
7	7	docs/source/features/structured_outputs.md
8	8	examples/online_serving/openai_chat_completion_structured_outputs.py
6	6	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py

[6167c0e5d] 张宇 2025-04-24 [Bugfix][Core] add seq_id_to_seq_group clearing to avoid memory leak when s… (#16472)
7	0	vllm/outputs.py

[ed2e46465] Areeb Syed 2025-04-24 Addendum Fix to support FIPS enabled machines with MD5 hashing (#17043)
2	1	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
2	1	vllm/envs.py

[2c8ed8ee4] Harry Mellor 2025-04-24 More informative error when using Transformers backend (#16988)
26	22	docs/source/models/supported_models.md
12	12	vllm/model_executor/model_loader/utils.py

[ed50f4664] Michael Goin 2025-04-23 [Bugfix] Enable V1 usage stats (#16986)
4	4	vllm/usage/usage_lib.py
19	1	vllm/utils.py
4	0	vllm/v1/engine/async_llm.py
4	0	vllm/v1/engine/llm_engine.py
44	0	vllm/v1/utils.py

[46e678bcf] Woosuk Kwon 2025-04-23 [Minor] Use larger batch sizes for A100/B100/B200/MI300x (#17073)
5	5	vllm/engine/arg_utils.py

[6b2427f99] Chen Xia 2025-04-23 [Quantization]add prefix for commandA quantized model (#17017)
8	1	vllm/model_executor/models/commandr.py

[b07d74166] Sangyeon Cho 2025-04-24 [CI/Build] workaround for CI build failure (#17070)
6	0	docker/Dockerfile

[41fb013d2] Woosuk Kwon 2025-04-23 [V1][Spec Decode] Always use argmax for sampling draft tokens  (#16899)
7	7	vllm/v1/sample/rejection_sampler.py
10	12	vllm/v1/spec_decode/eagle.py
1	4	vllm/v1/worker/gpu_model_runner.py

[32d4b669d] Yong Hoon Shin 2025-04-23 [BugFix][V1] Fix int32 token index overflow when preparing input ids (#16806)
2	1	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/v1/worker/tpu_model_runner.py

[3cde34a4a] Travis Johnson 2025-04-23 [Frontend] Support guidance:no-additional-properties for compatibility with xgrammar (#15949)
59	1	tests/entrypoints/llm/test_guided_generate.py
56	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	1	vllm/config.py
9	9	vllm/engine/arg_utils.py
13	2	vllm/model_executor/guided_decoding/guidance_decoding.py
0	8	vllm/v1/engine/processor.py
51	9	vllm/v1/structured_output/backend_guidance.py
12	4	vllm/v1/structured_output/backend_xgrammar.py

[bdb366031] Harry Mellor 2025-04-23 Use `@property` and private field for `data_parallel_rank_local` (#17053)
15	2	vllm/config.py
3	3	vllm/v1/engine/core_client.py

[f3a21e9c6] Harry Mellor 2025-04-23 `CacheConfig.block_size` should always be `int` when used (#17052)
5	2	vllm/config.py

[8e630d680] Harry Mellor 2025-04-23 Improve Transformers backend model loading QoL (#17039)
8	5	vllm/model_executor/model_loader/utils.py

[af869f6df] Russell Bryant 2025-04-23 [CI] Update structured-output label automation (#17055)
10	2	.github/mergify.yml

[53c0fa1e2] Harry Mellor 2025-04-23 Ensure that `pid` passed to `kill_process_tree` is `int` for `mypy` (#17051)
2	2	vllm/v1/utils.py

[f7912cba3] Michael Yao 2025-04-23 [Doc] Add top anchor and a note to quantization/bitblas.md (#17042)
8	0	docs/source/features/quantization/bitblas.md

[6317a5174] Michael Goin 2025-04-23 Categorize `tests/kernels/` based on kernel type (#16799)
1	1	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
36	5	.buildkite/test-pipeline.yaml
0	0	tests/kernels/{ => attention}/conftest.py
1	2	tests/kernels/{ => attention}/test_attention.py
9	0	tests/kernels/{ => attention}/test_attention_selector.py
1	2	tests/kernels/{ => attention}/test_blocksparse_attention.py
0	0	tests/kernels/{ => attention}/test_cache.py
0	0	tests/kernels/{ => attention}/test_cascade_flash_attn.py
0	0	tests/kernels/{ => attention}/test_encoder_decoder_attn.py
0	0	tests/kernels/{ => attention}/test_flash_attn.py
0	0	tests/kernels/{ => attention}/test_flashinfer.py
0	0	tests/kernels/{ => attention}/test_flashmla.py
0	0	tests/kernels/{ => attention}/test_lightning_attn.py
0	0	tests/kernels/{ => attention}/test_merge_attn_states.py
0	0	tests/kernels/{ => attention}/test_mha_attn.py
0	0	tests/kernels/{ => attention}/test_mla_decode_cpu.py
0	0	tests/kernels/{ => attention}/test_prefix_prefill.py
0	0	tests/kernels/{ => attention}/test_rocm_attention_selector.py
0	0	tests/kernels/{ => attention}/test_triton_decode_attention.py
1	2	tests/kernels/{ => core}/test_activation.py
0	0	tests/kernels/{ => core}/test_fused_quant_layernorm.py
0	0	tests/kernels/{ => core}/test_layernorm.py
25	0	tests/kernels/core/test_opcheck.py
0	0	tests/kernels/{ => core}/test_permute_cols.py
1	2	tests/kernels/{ => core}/test_pos_encoding.py
0	0	tests/kernels/{ => core}/test_rotary_embedding.py
0	0	tests/kernels/{ => core}/test_uva.py
0	0	tests/kernels/{ => mamba}/test_causal_conv1d.py
0	0	tests/kernels/{ => mamba}/test_mamba_mixer2.py
0	0	tests/kernels/{ => mamba}/test_mamba_ssm.py
0	0	tests/kernels/{ => mamba}/test_mamba_ssm_ssd.py
0	0	tests/kernels/{ => moe}/test_cutlass_moe.py
0	0	tests/kernels/{ => moe}/test_moe.py
0	0	tests/kernels/{ => moe}/test_triton_moe_ptpc_fp8.py
0	0	tests/kernels/{ => quantization}/test_allspark_gemm.py
0	0	tests/kernels/{ => quantization}/test_aqlm.py
0	0	tests/kernels/{ => quantization}/test_awq.py
0	0	tests/kernels/{ => quantization}/test_awq_marlin.py
0	0	tests/kernels/{ => quantization}/test_awq_triton.py
1	2	tests/kernels/{ => quantization}/test_block_fp8.py
1	2	tests/kernels/{ => quantization}/test_block_int8.py
1	2	tests/kernels/{ => quantization}/test_cutlass_2of4_sparse.py
1	3	tests/kernels/{test_cutlass.py => quantization/test_cutlass_scaled_mm.py}
0	0	tests/kernels/{ => quantization}/test_fp8_quant.py
0	0	tests/kernels/{ => quantization}/test_ggml.py
0	0	tests/kernels/{ => quantization}/test_gguf.py
0	0	tests/kernels/{ => quantization}/test_gptq.py
0	0	tests/kernels/{ => quantization}/test_int8_kernel.py
0	0	tests/kernels/{ => quantization}/test_int8_quant.py
0	0	tests/kernels/{ => quantization}/test_machete_mm.py
0	0	tests/kernels/{ => quantization}/test_marlin_gemm.py
0	0	tests/kernels/{ => quantization}/test_nvfp4_quant.py
0	0	tests/kernels/{ => quantization}/test_nvfp4_scaled_mm.py
0	0	tests/kernels/{ => quantization}/test_triton_scaled_mm.py
0	25	tests/kernels/test_utils.py

[aa72d9a4e] Michael Goin 2025-04-23 Mistral-format support for compressed-tensors (#16803)
6	0	vllm/transformers_utils/config.py

[ce17db808] Russell Bryant 2025-04-23 [CI] Run v1/test_serial_utils.py in CI (#16996)
1	0	.buildkite/test-pipeline.yaml

[8c87a9ad4] Chauncey 2025-04-23 [Bugfix] Fix AssertionError: skip_special_tokens=False is not supported for Mistral tokenizers (#16964)
8	4	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[ec69124eb] huafeng 2025-04-23 [Misc] Improve readability of get_open_port function. (#17024)
5	5	vllm/utils.py

[d0da99fb7] Lucas Wilkinson 2025-04-23 [BugFix] llama4 fa3 fix - RuntimeError: scheduler_metadata must have shape (metadata_size) (#16998)
48	28	vllm/v1/attention/backends/flash_attn.py

[b2f195c42] Nick Hill 2025-04-22 [V1] Avoid socket errors during shutdown when requests are in in-flight (#16807)
1	1	vllm/v1/engine/core.py
6	4	vllm/v1/engine/core_client.py

[047797ef9] vllmellm 2025-04-23 [Bugfix] Triton FA function takes no keyword arguments (#16902)
8	1	vllm/attention/backends/mla/common.py

[eb8ef4224] Reid 2025-04-23 [doc] add download path tips (#17013)
1	1	docs/source/models/supported_models.md

[56a735261] Chendi.Xue 2025-04-22 [INTEL-HPU][v0] Port delayed sampling to upstream (#16949)
7	0	vllm/envs.py
133	7	vllm/worker/hpu_model_runner.py

[e1cf90e09] youkaichao 2025-04-23 [misc] tune some env vars for GB200 (#16992)
15	2	vllm/env_override.py

[6bc1e30ef] Chauncey 2025-04-23 Revert "[Misc] Add S3 environment variables for better support of MinIO." (#17021)
2	12	vllm/transformers_utils/s3_utils.py

[7e081ba7c] vllmellm 2025-04-23 [BugFix] Revert ROCm Custom Paged Attention Env Flag Check (#17022)
1	0	vllm/platforms/rocm.py

[1e013fa38] Nick Hill 2025-04-22 [V1][DP] More robust DP/EP dummy request coordination (#16277)
2	2	tests/v1/test_async_llm_dp.py
12	3	vllm/v1/engine/__init__.py
42	21	vllm/v1/engine/core.py
38	31	vllm/v1/engine/core_client.py

[bc7c4d206] Aleksandr Malyshev 2025-04-22 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 (#13305)
3	3	tests/core/block/e2e/test_correctness.py
821	813	vllm/attention/ops/prefix_prefill.py

[f67e9e9f2] Yang Wang 2025-04-22 add Dockerfile build vllm against torch nightly (#16936)
3	0	.buildkite/test-pipeline.yaml
307	0	docker/Dockerfile.nightly_torch
28	0	requirements/nightly_torch_test.txt

[36fe78769] Guillaume Calmettes 2025-04-23 [Bugfix] validate urls object for multimodal content parts (#16990)
29	0	tests/entrypoints/openai/test_audio.py
29	0	tests/entrypoints/openai/test_video.py
30	0	tests/entrypoints/openai/test_vision.py
6	4	vllm/entrypoints/chat_utils.py

[83d933718] Chenyaaang 2025-04-22 [Core][V1][TPU] Enable structured decoding on TPU V1 (#16499)
3	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
1	1	benchmarks/benchmark_serving_structured_output.py
5	2	tests/v1/tpu/test_sampler.py
2	2	vllm/platforms/tpu.py
147	25	vllm/v1/worker/tpu_model_runner.py

[5175b884f] Nick Hill 2025-04-22 [BugFix] Remove default multiproc executor `collective_rpc` timeout (#17000)
7	7	vllm/v1/executor/multiproc_executor.py

[5536b30a4] Alexei-V-Ivanov-AMD 2025-04-22 Fencing Kernels Tests for enabling on AMD (#16929)
7	0	.buildkite/scripts/hardware_ci/run-amd-test.sh
1	1	.buildkite/test-pipeline.yaml

[7f58fb971] Richard Zou 2025-04-22 Add assertion for no objects while hashing hf_config (#16930)
30	0	vllm/config.py

[30bc3e0f6] vllmellm 2025-04-23 [FEAT][ROCm]: Support AITER MLA (#15893)
128	21	tests/kernels/test_attention_selector.py
28	1	tests/kernels/test_rocm_attention_selector.py
18	3	vllm/attention/backends/mla/common.py
412	0	vllm/attention/backends/rocm_aiter_mla.py
42	0	vllm/attention/ops/rocm_aiter_mla.py
1	1	vllm/config.py
6	0	vllm/envs.py
1	0	vllm/platforms/interface.py
31	3	vllm/platforms/rocm.py

[f34410715] Reid 2025-04-22 [frontend] enhance tool_calls type check (#16882)
5	1	vllm/entrypoints/chat_utils.py

[68d4c3320] Chauncey 2025-04-22 [Misc] Add S3 environment variables for better support of MinIO. (#16977)
12	2	vllm/transformers_utils/s3_utils.py

[f961d7f6e] Zhengyuan Su (苏政渊) 2025-04-22 [BugFix] Pass in correct VLLM config in FlashInfer backend (#13207) (#16973)
3	3	vllm/attention/backends/flashinfer.py

[d05911049] Harry Mellor 2025-04-22 Improve configs - `SpeculativeConfig` (#16971)
73	99	vllm/config.py
12	5	vllm/engine/arg_utils.py

[571e8dd65] Yang Fan 2025-04-22 [Bugfix] Fix distributed bug again in Qwen2.5-VL & Qwen2.5-Omni (#16974)
4	1	vllm/model_executor/models/qwen2_5_vl.py

[4b91c927f] Reid 2025-04-22 [Misc] refactor example series (#16972)
84	74	examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py

[0e237f003] vllmellm 2025-04-22 [FEAT][ROCm] Integrate Paged Attention Kernel from AITER (#15001)
1	1	docker/Dockerfile.rocm_base
84	25	vllm/attention/backends/rocm_flash_attn.py
101	0	vllm/attention/ops/rocm_aiter_paged_attn.py
7	0	vllm/envs.py
2	1	vllm/platforms/rocm.py

[8f7bace7c] Cyrus Leung 2025-04-22 [Doc] Improve documentation for multimodal CLI args (#16960)
9	4	vllm/config.py
8	6	vllm/engine/arg_utils.py

[e4d614423] Nick Hill 2025-04-22 [BugFix] Fix incremental detokenization perf issue (#16963)
1	1	vllm/v1/engine/detokenizer.py

[8d32dc603] Lei Wang 2025-04-22 [Kernel] Support Microsoft Runtime Kernel Lib for our Low Precision Computation - BitBLAS (#6036)
236	0	benchmarks/kernels/benchmark_bitblas.py
40	0	docs/source/features/quantization/bitblas.md
1	0	docs/source/features/quantization/index.md
11	0	docs/source/features/quantization/supported_hardware.md
63	0	tests/models/test_bitblas.py
61	0	tests/models/test_gptq_bitblas.py
2	1	vllm/config.py
16	0	vllm/model_executor/layers/linear.py
7	1	vllm/model_executor/layers/quantization/__init__.py
459	0	vllm/model_executor/layers/quantization/bitblas.py
438	0	vllm/model_executor/layers/quantization/gptq_bitblas.py
4	1	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
299	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/bitblas.py
198	0	vllm/model_executor/layers/quantization/utils/bitblas_utils.py
29	4	vllm/model_executor/parameter.py

[c4ab9f3e7] Woosuk Kwon 2025-04-22 [V1] Remove pre-allocation for KV cache (#16941)
4	11	tests/v1/core/test_kv_cache_utils.py
44	97	tests/v1/core/test_prefix_caching.py
4	8	tests/v1/core/test_scheduler.py
2	21	vllm/v1/core/kv_cache_manager.py
5	2	vllm/v1/core/sched/scheduler.py

[2689d5c02] Flora Feng 2025-04-22 [Model] Use autoweightloader for mamba (#16950)
23	18	vllm/model_executor/models/mamba.py

[acba33a0f] Chauncey 2025-04-22 [Bugfix] Fix the issue where llm.generate cannot be called repeatedly after setting GuidedDecodingParams (#16767)
11	3	tests/v1/entrypoints/llm/test_struct_output_generate.py
11	0	vllm/sampling_params.py
10	1	vllm/v1/engine/processor.py

[a114bf20a] SnowCharm 2025-04-22 [Perf] Optimize `_update_states` for GPU model runner (#16910)
1	1	vllm/v1/worker/gpu_model_runner.py

[3097ce3a3] Michael Yao 2025-04-22 [Doc] Update ai_accelerator/hpu-gaudi.inc.md (#16956)
16	18	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md

[d6da9322c] Cyrus Leung 2025-04-22 [Bugfix] Fix f-string for Python 3.9-3.11 (#16962)
1	1	vllm/model_executor/model_loader/loader.py

[71ce44047] omer-dayan 2025-04-22 Support S3 Sharded loading with RunAI Model Streamer (#16317)
1	0	vllm/config.py
52	28	vllm/model_executor/model_loader/loader.py

[188b7f9b8] Charlie Fu 2025-04-21 [Performance][ROCm] Add skinny gemms for unquantized linear on ROCm (#15830)
1	0	CMakeLists.txt
9	0	csrc/rocm/ops.h
1600	0	csrc/rocm/skinny_gemms.cu
18	0	csrc/rocm/torch_bindings.cpp
80	0	tests/kernels/test_rocm_skinny_gemms.py
20	0	vllm/_custom_ops.py
6	0	vllm/envs.py
2	2	vllm/model_executor/layers/linear.py
173	92	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
36	1	vllm/model_executor/layers/utils.py
7	0	vllm/platforms/interface.py
5	0	vllm/platforms/rocm.py

[b9b474695] wangxiyuan 2025-04-22 [V1] Remove additional_config check (#16710)
0	5	vllm/engine/arg_utils.py

[7b8a2ab76] Varun Sundar Rabindranath 2025-04-21 [Kernel] Add expert_map support to Cutlass FP8 MOE (#16861)
15	2	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
280	160	tests/kernels/test_cutlass_moe.py
1	0	vllm/config.py
35	8	vllm/model_executor/layers/fused_moe/cutlass_moe.py
2	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[c9acbf114] Jee Jee Li 2025-04-22 [Misc] Remove the chunked prefill warning for LoRA  (#16925)
0	9	vllm/config.py

[5b794cae8] kliuae 2025-04-22 [ROCm] Add aiter tkw1 kernel for Llama4 fp8 (#16727)
1	1	docker/Dockerfile.rocm_base
0	8	vllm/envs.py
3	3	vllm/model_executor/layers/fused_moe/fused_moe.py
107	33	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
23	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	3	vllm/model_executor/layers/quantization/fp8.py

[0e4254492] Jeffrey Li 2025-04-21 [Bugfix]: fix issue with n>1 sampling on v1 requests overriding each other (#16863)
81	0	tests/v1/engine/test_output_processor.py
20	14	vllm/outputs.py
3	6	vllm/v1/engine/output_processor.py

[1311913f5] Woosuk Kwon 2025-04-21 [BugFix][Spec Decode] No in-place update to draft probs (#16952)
3	1	vllm/v1/spec_decode/eagle.py

[29f395c97] Cyrus Leung 2025-04-22 [Doc] Remove unnecessary V1 flag (#16924)
2	2	docs/source/design/v1/torch_compile.md

[fa3bba2a5] Nicolò Lucchesi 2025-04-22 [TPU][V1] Enable Top-P (#16843)
4	3	tests/v1/tpu/test_sampler.py
5	7	vllm/v1/sample/tpu/metadata.py

[986537f1c] Michael Goin 2025-04-21 [V1] V1 FlashInfer Attention (#16684)
9	1	tests/v1/e2e/test_cascade_attention.py
10	3	vllm/engine/arg_utils.py
3	0	vllm/platforms/cuda.py
3	4	vllm/v1/attention/backends/flash_attn.py
639	0	vllm/v1/attention/backends/flashinfer.py
3	4	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/worker/gpu_model_runner.py

[210207525] Nicolò Lucchesi 2025-04-22 [TPU][V1] Capture multimodal encoder during model compilation (#15051)
3	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh
91	0	tests/v1/tpu/test_multimodal.py
232	46	vllm/v1/worker/tpu_model_runner.py
1	1	vllm/v1/worker/tpu_worker.py

[71eda0bb7] Michael Goin 2025-04-21 Update Qwen1.5-MoE-W4A16-compressed-tensors.yaml (#16946)
2	2	.buildkite/lm-eval-harness/configs/Qwen1.5-MoE-W4A16-compressed-tensors.yaml

[471fe6563] Chengji Yao 2025-04-21 [TPU][V1] Implicitly adjust page size when there's SMEM OOM (#16871)
5	2	tests/v1/tpu/test_basic.py
14	0	vllm/platforms/tpu.py
15	0	vllm/v1/attention/backends/pallas.py

[3a0fba5cf] Woosuk Kwon 2025-04-21 [V1][Spec Decode] Handle draft tokens beyond max_model_len (#16087)
6	1	tests/v1/core/test_scheduler.py
57	0	tests/v1/spec_decode/test_max_len.py
19	9	tests/v1/spec_decode/test_ngram.py
7	0	vllm/v1/core/sched/scheduler.py
32	3	vllm/v1/spec_decode/eagle.py
9	1	vllm/v1/spec_decode/ngram_proposer.py
7	1	vllm/v1/worker/gpu_model_runner.py

[299ebb62b] Chanh Nguyen 2025-04-21 [Core] Speed up decode by remove synchronizing operation in sampler (#16436)
9	4	vllm/model_executor/layers/utils.py

[f728ab8e3] David Xia 2025-04-21 [Doc] mention how to install in CPU editable mode (#16923)
6	0	docs/source/getting_started/installation/cpu/build.inc.md

[63e26fff7] David Xia 2025-04-21 [doc] install required python3-dev apt package (#16888)
1	1	docs/source/getting_started/installation/cpu/build.inc.md

[fe3462c77] Yan Ma 2025-04-22 [XPU][Bugfix] minor fix for XPU (#15591)
2	0	docs/source/getting_started/installation/gpu/xpu.inc.md
6	6	vllm/attention/backends/ipex_attn.py

[3b34fd527] Kartik Ramesh 2025-04-21 Raise error for data-parallel with benchmark_throughput (#16737)
7	0	benchmarks/benchmark_throughput.py

[55d6d3fdb] Isotr0py 2025-04-21 [Bugfix] Fix GLM rotary_dim issue and support v1 (#16912)
2	3	vllm/model_executor/models/glm.py

[7272bfae7] Shanshan Shen 2025-04-21 [Misc] Refactor platform to get device specific stream and event (#14411)
9	0	vllm/platforms/interface.py
6	5	vllm/spec_decode/metrics.py

[d9ac9e3dc] wangxiyuan 2025-04-21 [Misc] fix collect_env version parse (#15267)
12	5	vllm/collect_env.py

[d41faaf9d] Han Zhang 2025-04-21 Restore buffers when wake up from level 2 sleep (#16564) (#16889)
20	0	vllm/v1/worker/gpu_worker.py
20	0	vllm/worker/worker.py

[b34f33438] Alex Brooks 2025-04-21 [Doc] Split dummy_processor_inputs() in Multimodal Docs (#16915)
34	28	docs/source/contributing/model/multimodal.md
1	1	docs/source/design/mm_processing.md

[26c040655] Yang Fan 2025-04-21 [Bugfix] Fix distributed bug in Qwen2.5-VL & Qwen2.5-Omni (#16907)
1	2	vllm/model_executor/models/qwen2_5_vl.py

[4c41278b7] Woosuk Kwon 2025-04-20 [CI/CD][V1] Add spec decode tests to CI (#16900)
1	0	.buildkite/test-pipeline.yaml

[bb3605db8] qizixi 2025-04-20 [Bugfix] Fix v1/spec_decode/test_ngram.py (#16895)
22	31	tests/v1/spec_decode/test_ngram.py
8	7	vllm/config.py

[fe742aef5] Richard Zou 2025-04-20 [easy] Pass compile_fx only the config patches (#16845)
1	2	vllm/compilation/compiler_interface.py

[4b07d3689] Harry Mellor 2025-04-20 Improve configs - `CacheConfig` (#16835)
69	51	vllm/config.py
53	105	vllm/engine/arg_utils.py
1	1	vllm/platforms/neuron.py

[87aaadef7] Staszek Paśko 2025-04-19 Serialize tensors using int8 views (#16866)
10	6	tests/v1/test_serial_utils.py
38	7	vllm/v1/serial_utils.py

[682e0b6d2] Richard Zou 2025-04-19 Log how much time loading a compiled artifact takes (#16848)
8	4	vllm/compilation/backends.py

[d6195a748] Reid 2025-04-20 [doc] update hyperlink (#16877)
1	1	docs/source/models/extensions/fastsafetensor.md

[205d84aaa] Cyrus Leung 2025-04-19 [VLM] Clean up models (#16873)
1	0	examples/offline_inference/mistral-small.py
1	1	examples/offline_inference/vision_language.py
0	23	vllm/model_executor/models/phi4mm.py
0	18	vllm/model_executor/models/qwen2_5_omni_thinker.py

[5124f5bf5] Roger Wang 2025-04-19 [Model] Qwen2.5-Omni Cleanup  (#16872)
2	2	docs/source/models/supported_models.md
0	3	vllm/model_executor/models/qwen2_5_omni_thinker.py

[83f3c3bd9] Isotr0py 2025-04-19 [Model] Refactor Phi-4-multimodal to use merged processor and support V1 (#15477)
1	1	docs/source/models/supported_models.md
1	1	examples/offline_inference/audio_language.py
4	1	examples/offline_inference/vision_language.py
3	1	examples/offline_inference/vision_language_multi_image.py
1	0	requirements/docs.txt
18	9	tests/models/decoder_only/audio_language/test_ultravox.py
3	3	tests/models/decoder_only/vision_language/test_phi4mm.py
1	0	tests/models/multimodal/processing/test_common.py
59	0	tests/models/multimodal/processing/test_phi4mm.py
2	5	vllm/entrypoints/chat_utils.py
1	1	vllm/model_executor/models/phi3v.py
614	1125	vllm/model_executor/models/phi4mm.py
18	57	vllm/model_executor/models/phi4mm_audio.py
51	2	vllm/multimodal/audio.py
15	14	vllm/multimodal/parse.py

[d9737ca1c] vie-serendipity 2025-04-19 [V1][Misc] stop update prefix cache stats when logs_stats is disabled (#16460)
22	0	tests/v1/core/test_prefix_caching.py
19	11	vllm/v1/core/kv_cache_manager.py
3	1	vllm/v1/core/sched/scheduler.py

[9d4ca19d5] Nicolò Lucchesi 2025-04-19 [Misc] Benchmarks for audio models (#16505)
107	0	benchmarks/backend_request_func.py
80	0	benchmarks/benchmark_dataset.py
11	5	benchmarks/benchmark_serving.py
1	0	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py

[2ef0dc53b] Nicolò Lucchesi 2025-04-19 [Frontend] Add sampling params to `v1/audio/transcriptions` endpoint (#16591)
18	1	docs/source/serving/openai_compatible_server.md
6	1	examples/online_serving/openai_transcription_client.py
33	0	tests/entrypoints/openai/test_transcription_validation.py
65	9	vllm/entrypoints/openai/protocol.py

[1d4680fad] Divakar Verma 2025-04-19 [rocm][MI300] llama4 maverick fp8 moe config tp8 (#16847)
164	0	vllm/model_executor/layers/fused_moe/configs/E=128,N=1024,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json

[2c1bd848a] Yang Fan 2025-04-19 [Model][VLM] Add Qwen2.5-Omni model support (thinker only) (#15130)
15	0	docs/source/models/supported_models.md
31	0	examples/offline_inference/audio_language.py
32	0	examples/offline_inference/qwen2_5_omni/README.md
160	0	examples/offline_inference/qwen2_5_omni/only_thinker.py
37	0	examples/offline_inference/vision_language.py
17	0	tests/models/decoder_only/vision_language/test_models.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
17	1	vllm/assets/video.py
5	1	vllm/entrypoints/chat_utils.py
1	1	vllm/envs.py
318	8	vllm/model_executor/layers/rotary_embedding.py
19	19	vllm/model_executor/models/llava_onevision.py
977	0	vllm/model_executor/models/qwen2_5_omni_thinker.py
51	28	vllm/model_executor/models/qwen2_5_vl.py
1	0	vllm/model_executor/models/registry.py
1	1	vllm/model_executor/models/transformers.py
63	15	vllm/multimodal/inputs.py
24	2	vllm/transformers_utils/config.py
49	0	vllm/transformers_utils/processor.py
9	0	vllm/v1/worker/gpu_model_runner.py
11	3	vllm/worker/cpu_model_runner.py
11	3	vllm/worker/model_runner.py

[5c9121203] omrishiv 2025-04-19 [release] Publish neuron docker image (#16733)
15	0	.buildkite/release-pipeline.yaml

[490b1698a] Justin Ho 2025-04-18 [Doc] Updated Llama section in tool calling docs to have llama 3.2 config info (#16857)
13	8	docs/source/features/tool_calling.md

[5a5e29de8] Reid 2025-04-19 [Misc] refactor examples series - Chat Completion Client With Tools (#16829)
112	83	examples/online_serving/openai_chat_completion_client_with_tools.py

[3d3ab3689] wang.yuqi 2025-04-18 [New Model]: Snowflake Arctic Embed (Family)  (#16649)
4	11	tests/entrypoints/openai/test_embedding_dimensions.py
101	0	tests/models/embedding/language/test_snowflake_arctic_embed.py
8	0	tests/models/embedding/utils.py
4	0	tests/models/registry.py
1	0	vllm/model_executor/layers/activation.py
189	12	vllm/model_executor/models/bert.py
5	3	vllm/model_executor/models/registry.py

[686623c5e] Harry Mellor 2025-04-18 Fix `nullable_kvs` fallback (#16837)
1	1	tests/engine/test_arg_utils.py
3	1	tests/entrypoints/openai/test_audio.py
3	1	tests/entrypoints/openai/test_video.py
3	1	tests/entrypoints/openai/test_vision.py
3	1	tests/entrypoints/openai/test_vision_embedding.py
2	1	tests/models/decoder_only/audio_language/test_ultravox.py
3	4	vllm/config.py
9	9	vllm/engine/arg_utils.py

[aadb65656] Cyrus Leung 2025-04-18 [Misc] Clean up Kimi-VL (#16833)
2	2	examples/offline_inference/vision_language.py
1	2	examples/offline_inference/vision_language_multi_image.py
17	40	vllm/model_executor/models/kimi_vl.py

[87e067de4] Jonghyun Choe 2025-04-18 [Model] use AutoWeightsLoader for BigCode, GPT-J (#16823)
30	24	vllm/model_executor/models/gpt_bigcode.py
61	55	vllm/model_executor/models/gpt_j.py

[26507f897] Michael Yao 2025-04-18 [Docs] Fix a link and grammar issue in production-stack.md (#16809)
1	1	docs/source/deployment/integrations/production-stack.md

[9c1d5b456] Nathan Weinberg 2025-04-18 [Doc] add podman setup instructions for official image (#16796)
12	0	docs/source/deployment/docker.md

[e31045f95] Lucia Fang 2025-04-17 [Bugfix] fix pp for llama4 (#16746)
3	3	vllm/model_executor/models/mllama4.py

[aaec845f8] Luka Govedič 2025-04-18 [ROCm] [Attention] Cleanup ROCm output passing (#16431)
18	23	vllm/attention/backends/rocm_flash_attn.py

[7bdfd29a3] rongfu.leng 2025-04-18 [Misc] add collect_env to cli and docker image (#16759)
1	1	.github/ISSUE_TEMPLATE/200-installation.yml
1	1	.github/ISSUE_TEMPLATE/300-usage.yml
1	1	.github/ISSUE_TEMPLATE/400-bug-report.yml
1	1	.github/ISSUE_TEMPLATE/700-performance-discussion.yml
1	0	docker/Dockerfile
1	0	docker/Dockerfile.cpu
11	8	collect_env.py => vllm/collect_env.py
35	0	vllm/entrypoints/cli/collect_env.py
2	0	vllm/entrypoints/cli/main.py

[e78587a64] Harry Mellor 2025-04-18 Improve-mm-and-pooler-and-decoding-configs (#16789)
1	1	docs/source/models/supported_models.md
1	1	docs/source/serving/multimodal_inputs.md
2	2	examples/offline_inference/mistral-small.py
1	1	examples/online_serving/openai_chat_completion_client_for_multimodal.py
4	0	tests/engine/test_arg_utils.py
1	1	tests/entrypoints/openai/test_audio.py
1	1	tests/entrypoints/openai/test_video.py
1	1	tests/entrypoints/openai/test_vision.py
1	1	tests/entrypoints/openai/test_vision_embedding.py
3	3	tests/models/decoder_only/audio_language/test_ultravox.py
28	21	vllm/config.py
32	40	vllm/engine/arg_utils.py
4	2	vllm/multimodal/processing.py
4	3	vllm/multimodal/registry.py

[7eb425562] Lucas Wilkinson 2025-04-18 [BugFix] Accuracy fix for llama4 int4 - improperly casted scales (#16801)
3	7	csrc/moe/moe_wna16.cu
1	0	vllm/model_executor/layers/fused_moe/layer.py
2	2	vllm/model_executor/models/llama4.py

[6a0f54756] Michael Goin 2025-04-17 Add hardware print to TPU V1 test (#16792)
3	1	.buildkite/scripts/hardware_ci/run-tpu-v1-test.sh

[30ed81b7c] Shanshan Shen 2025-04-18 [V1][Structured Output] Minor modification to `_validate_structured_output()` (#16748)
10	10	vllm/v1/engine/processor.py

[7a4a5de72] Chauncey 2025-04-18 [Misc] Update outdated note: LMCache now supports chunked prefill (#16697)
2	2	examples/offline_inference/cpu_offload_lmcache.py

[c16fb5dae] Cyrus Leung 2025-04-18 [Doc] Improve help examples for `--compilation-config` (#16729)
1	1	docs/source/design/v1/torch_compile.md
12	4	tests/engine/test_arg_utils.py
4	3	vllm/engine/arg_utils.py

[e37073efd] Tarun Kumar 2025-04-18 Add property-based testing for vLLM endpoints using an API defined by an OpenAPI 3.1 schema (#16721)
1	1	.buildkite/test-pipeline.yaml
1	0	requirements/test.in
83	5	requirements/test.txt
49	0	tests/entrypoints/openai/test_openai_schema.py

[183dad7a8] Lucas Wilkinson 2025-04-17 [Attention] Update to lastest FA3 code (#13111)
1	1	cmake/external_projects/vllm_flash_attn.cmake
92	90	vllm/attention/backends/mla/common.py
25	1	vllm/attention/backends/utils.py
58	1	vllm/v1/attention/backends/flash_attn.py
65	25	vllm/v1/attention/backends/mla/common.py

[3408e4715] Yihua Cheng 2025-04-17 [P/D][V1] KV Connector API V1 (#15960)
36	0	examples/offline_inference/disaggregated-prefill-v1/decode_example.py
43	0	examples/offline_inference/disaggregated-prefill-v1/prefill_example.py
5	0	examples/offline_inference/disaggregated-prefill-v1/run.sh
406	9	tests/v1/core/test_scheduler.py
44	1	vllm/attention/layer.py
11	0	vllm/distributed/kv_transfer/__init__.py
4	0	vllm/distributed/kv_transfer/kv_connector/base.py
47	5	vllm/distributed/kv_transfer/kv_connector/factory.py
8	0	vllm/distributed/kv_transfer/kv_connector/v1/__init__.py
209	0	vllm/distributed/kv_transfer/kv_connector/v1/base.py
382	0	vllm/distributed/kv_transfer/kv_connector/v1/shared_storage_connector.py
1	1	vllm/distributed/kv_transfer/{kv_transfer_agent.py => kv_connector_agent.py}
70	0	vllm/distributed/kv_transfer/kv_transfer_state.py
1	34	vllm/distributed/parallel_state.py
0	6	vllm/engine/arg_utils.py
23	0	vllm/forward_context.py
3	2	vllm/v1/core/kv_cache_manager.py
5	0	vllm/v1/core/sched/output.py
56	14	vllm/v1/core/sched/scheduler.py
1	5	vllm/v1/engine/core.py
11	0	vllm/v1/worker/gpu_model_runner.py
7	3	vllm/v1/worker/gpu_worker.py
2	1	vllm/worker/model_runner.py
2	2	vllm/worker/worker.py

[0377b8310] Nick Hill 2025-04-17 [MLA] Simplification to batch P/D reordering (#16673)
5	7	vllm/v1/attention/backends/mla/common.py
7	9	vllm/v1/worker/gpu_model_runner.py

[e4755f7fa] Mark McLoughlin 2025-04-17 [V1][Metrics] Fix http metrics middleware (#15894)
11	0	docs/source/design/v1/metrics.md
18	18	vllm/entrypoints/openai/api_server.py

[92edf3582] Sijia(Jackson) Chen 2025-04-17 [ROCM] enable aiter fused moe kernel for llama4 bf16 checkpoints (#16674)
13	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[eb5819b2d] Nicolò Lucchesi 2025-04-17 [V1][TPU] Enable Top K (#15489)
18	0	tests/v1/tpu/test_sampler.py
21	1	tests/v1/tpu/test_topk_topp_sampler.py
0	6	vllm/envs.py
15	12	vllm/v1/sample/ops/topk_topp_sampler.py
8	5	vllm/v1/sample/tpu/metadata.py
7	2	vllm/v1/worker/tpu_model_runner.py

[5989f4684] Nicolò Lucchesi 2025-04-17 [TPU][V1] Fix padding recompilation when `max-num-batched-tokens` is not even (#16726)
8	0	tests/v1/tpu/worker/test_tpu_model_runner.py
7	5	vllm/v1/worker/tpu_model_runner.py
2	2	vllm/v1/worker/tpu_worker.py

[5125d72f0] rongfu.leng 2025-04-18 [Model] use AutoWeightsLoader for olmoe,opt,orion,persimmon,phi3_small (#16548)
58	54	vllm/model_executor/models/olmoe.py
48	40	vllm/model_executor/models/opt.py
49	42	vllm/model_executor/models/orion.py
36	38	vllm/model_executor/models/persimmon.py
25	19	vllm/model_executor/models/phi3_small.py

[a018e555f] Ximingwang-09 2025-04-18 [Kernel] Add fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 on NVIDIA H20 (#16753)
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json

[6211b9227] Robin 2025-04-18 [Bugfix]Fix index out of range error in api server log (#16787)
2	1	vllm/entrypoints/openai/api_server.py

[05fcd1b43] Nick Hill 2025-04-17 [V1][Perf] Faster incremental detokenization (#15137)
1	1	requirements/common.txt
1	0	requirements/test.in
4	2	requirements/test.txt
1	0	tests/lora/test_llama_tp.py
137	55	tests/tokenization/test_detokenize.py
9	0	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
163	86	vllm/v1/engine/detokenizer.py

[7c02d6a13] Insu Kim 2025-04-17 [Doc] Changed explanation of generation_tokens_total and prompt_tokens_total counter type metrics to avoid confusion (#16784)
2	2	docs/source/design/v1/metrics.md

[11c3b9849] wang.yuqi 2025-04-17 [Doc] Document Matryoshka Representation Learning support (#16770)
74	0	docs/source/models/pooling_models.md
36	0	examples/online_serving/openai_embedding_matryoshka_fy.py

[dbe7f0700] Cyrus Leung 2025-04-17 [Doc] Make sure to update vLLM when installing latest code (#16781)
4	2	docs/source/getting_started/installation/gpu/cuda.inc.md

[c69bf4ee0] Reid 2025-04-17 fix: hyperlink (#16778)
2	2	docs/source/deployment/frameworks/open-webui.md

[d27ea9403] Harry Mellor 2025-04-17 Improve configs - `TokenizerPoolConfig` + `DeviceConfig` (#16603)
24	2	tests/test_config.py
51	22	vllm/config.py
61	57	vllm/engine/arg_utils.py

[99ed52610] Reid 2025-04-17 [Misc] refactor examples series - lmcache (#16758)
85	50	examples/offline_inference/cpu_offload_lmcache.py

[207da2818] Michael Yao 2025-04-17 [Doc] Fix a 404 link in installation/cpu.md (#16773)
1	1	docs/source/getting_started/installation/cpu.md

[5b1aca2ae] intervitens 2025-04-17 [Bugfix] Fix GLM4 model (#16618)
1	1	docs/source/models/supported_models.md
1	1	tests/models/registry.py
4	4	vllm/model_executor/models/glm4.py

[d8e557b5e] Reid 2025-04-17 [doc] add open-webui example (#16747)
-	-	docs/source/assets/deployment/open_webui.png
1	0	docs/source/deployment/frameworks/index.md
29	0	docs/source/deployment/frameworks/open-webui.md

[61a44a0b2] Cyrus Leung 2025-04-17 [Doc] Add more tips to avoid OOM (#16765)
25	0	docs/source/serving/offline_inference.md
8	0	docs/source/serving/openai_compatible_server.md

[a6481525b] DefTruth 2025-04-17 [misc] ignore marlin_moe_wna16 local gen codes (#16760)
3	0	.gitignore

[8cac35ba4] Richard Liaw 2025-04-16 [Ray] Improve documentation on batch inference (#16609)
90	0	examples/offline_inference/batch_llm_inference.py
0	109	examples/offline_inference/distributed.py

[9dbf7a2dc] Russell Bryant 2025-04-17 [V1] Remove log noise when idle (#16735)
13	2	vllm/v1/metrics/loggers.py
2	2	vllm/v1/spec_decode/metrics.py

[607029e51] David Heineman 2025-04-16 [Bugfix] Revert max_prompt_len validation for decoder-only models. (#16741)
1	1	vllm/engine/llm_engine.py
1	1	vllm/v1/engine/processor.py

[cb072ce93] Isotr0py 2025-04-17 [Bugfix] Update Florence-2 tokenizer to make grounding tasks work (#16734)
2	1	examples/offline_inference/encoder_decoder_multimodal.py
1	1	examples/offline_inference/vision_language.py
2	0	tests/conftest.py
10	7	tests/models/encoder_decoder/vision_language/test_florence2.py
1	1	tests/models/registry.py

[95aca283b] Divakar Verma 2025-04-16 [rocm][V0] fix selection logic for custom PA in V0 (#16426)
4	1	vllm/platforms/rocm.py

[2b05b8ce6] Robert Shaw 2025-04-16 [V1][Frontend] Improve Shutdown And Logs (#11737)
1	0	.buildkite/test-pipeline.yaml
97	0	tests/v1/shutdown/test_delete.py
129	0	tests/v1/shutdown/test_forward_error.py
69	0	tests/v1/shutdown/test_processor_error.py
97	0	tests/v1/shutdown/test_startup_error.py
5	0	tests/v1/shutdown/utils.py
23	8	vllm/distributed/device_communicators/shm_broadcast.py
62	36	vllm/entrypoints/launcher.py
2	0	vllm/v1/engine/__init__.py
115	73	vllm/v1/engine/async_llm.py
60	27	vllm/v1/engine/core.py
126	90	vllm/v1/engine/core_client.py
16	0	vllm/v1/engine/exceptions.py
24	9	vllm/v1/engine/output_processor.py
10	1	vllm/v1/executor/abstract.py
211	119	vllm/v1/executor/multiproc_executor.py

[3c776dcef] Aaruni Aggarwal 2025-04-17 Adding vllm buildkite job for IBM Power (#16679)
26	2	.buildkite/scripts/hardware_ci/run-cpu-test-ppc64le.sh

[2cbd4d299] Bryan Lu 2025-04-16 [V1][Spec Dec Bug Fix] Respect Spec Dec Method Specification (#16636)
3	1	vllm/config.py

[3092375e2] Staszek Paśko 2025-04-17 [V1][Performance] Implement custom serializaton for MultiModalKwargs [Rebased] (#16432)
98	1	tests/v1/test_serial_utils.py
11	0	vllm/envs.py
103	5	vllm/v1/serial_utils.py

[3cd91dc95] Harry Mellor 2025-04-17 Help user create custom model for Transformers backend remote code models (#16719)
4	0	docs/source/models/supported_models.md

[8a7368e06] Jade Zheng 2025-04-17 [Misc] Remove redundant comment (#16703)
0	3	vllm/v1/worker/gpu_model_runner.py

[93e561ec4] Harry Mellor 2025-04-17 Improve error for structured output backend selection (#16717)
7	4	vllm/v1/engine/processor.py

[e1b004839] Joe Runde 2025-04-16 [Hardware] Add processor inputs to platform validation (#16680)
2	1	vllm/platforms/interface.py
2	1	vllm/platforms/tpu.py
6	6	vllm/v1/engine/processor.py

[ee378f3d4] xsank 2025-04-16 [Model] support modernbert  (#16648)
5	0	docs/source/models/supported_models.md
3	0	tests/models/registry.py
325	0	vllm/model_executor/models/modernbert.py
2	0	vllm/model_executor/models/registry.py

[e82ee40de] DefTruth 2025-04-16 [Bugfix][Kernel] fix potential cuda graph broken for merge_attn_states kernel (#16693)
15	10	csrc/attention/merge_attn_states.cu

[facbe2a11] Cyrus Leung 2025-04-16 [Doc] Improve OOM troubleshooting (#16704)
1	1	docs/source/getting_started/troubleshooting.md
53	3	docs/source/serving/offline_inference.md

[716892049] Reid 2025-04-16 [Misc] refactor examples series (#16708)
8	4	examples/offline_inference/llm_engine_example.py
0	4	examples/online_serving/gradio_openai_chatbot_webserver.py
11	7	examples/online_serving/openai_chat_completion_client_for_multimodal.py
32	26	examples/online_serving/openai_chat_completion_client_with_tools_required.py
36	27	examples/online_serving/openai_chat_completion_with_reasoning.py
45	37	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
9	2	examples/online_serving/openai_chat_embedding_client_for_multimodal.py
33	25	examples/online_serving/openai_completion_client.py
15	8	examples/online_serving/openai_cross_encoder_score.py
26	19	examples/online_serving/openai_embedding_client.py
13	2	examples/online_serving/openai_pooling_client.py

[21378a232] Kay Yan 2025-04-16 [CI] Cleanup `additional_dependencies: [toml]` for pre-commit yapf hook (#16405)
0	1	.pre-commit-config.yaml

[976711d9d] Shanshan Shen 2025-04-16 [V1][Structured Output] Move xgrammar related utils to `backend_xgrammar.py` (#16578)
1	1	tests/v1/structured_output/test_utils.py
4	4	vllm/v1/engine/processor.py
115	1	vllm/v1/structured_output/backend_xgrammar.py
0	120	vllm/v1/structured_output/utils.py

[44fa4d556] Sage Moore 2025-04-15 [ROCM] Bind triton version to 3.2 in requirements-built.txt  (#16664)
1	0	requirements/rocm-build.txt

[3ac98edcb] billishyahao 2025-04-16 [Feature] add model aware kv ops helper (#16020)
12	27	vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py
21	74	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
90	0	vllm/distributed/kv_transfer/kv_connector/utils.py

[966c742ed] Richard Zou 2025-04-16 Disable remote caching when calling compile_fx (#16611)
13	0	vllm/compilation/compiler_interface.py

[0d7d05f4b] Jee Jee Li 2025-04-16 [Misc] Modify LRUCache touch (#16689)
4	1	vllm/utils.py

[96bb8aa68] rongfu.leng 2025-04-16 [Bugfix] fix gpu docker image mis benchmarks dir (#16628)
1	0	docker/Dockerfile

[3badb0213] Shinichi Hemmi 2025-04-16 [Model] Add PLaMo2 (#14323)
6	1	.buildkite/test-pipeline.yaml
5	0	docs/source/models/supported_models.md
1	0	requirements/test.in
9	0	requirements/test.txt
18	23	tests/models/decoder_only/language/test_hybrid.py
2	0	tests/models/registry.py
12	0	vllm/config.py
746	0	vllm/model_executor/models/plamo2.py
1	0	vllm/model_executor/models/registry.py

[fdcb850f1] Angky William 2025-04-15 [Misc] Enable vLLM to Dynamically Load LoRA from a Remote Server (#10546)
56	5	docs/source/features/lora.md
209	0	tests/entrypoints/openai/test_lora_resolvers.py
74	0	tests/lora/test_resolver.py
13	1	vllm/entrypoints/openai/serving_engine.py
70	0	vllm/entrypoints/openai/serving_models.py
83	0	vllm/lora/resolver.py

[54a66e5fe] Dipika Sikka 2025-04-15 [Misc] Update `compressed-tensors` WNA16 to support zero-points (#14211)
15	6	tests/quantization/test_compressed_tensors.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
39	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
2	5	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
24	27	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
2	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[280d62b8a] DefTruth 2025-04-15 [Kernel] Remove redundant Exp calculations (#16123)
6	3	vllm/attention/ops/triton_merge_attn_states.py

[1666e6644] Xihui Cang 2025-04-15 Add "/server_info" endpoint in api_server to retrieve the vllm_config.  (#16572)
4	0	vllm/engine/async_llm_engine.py
4	0	vllm/engine/llm_engine.py
4	0	vllm/engine/multiprocessing/client.py
6	1	vllm/engine/protocol.py
12	4	vllm/entrypoints/openai/api_server.py
4	1	vllm/v1/engine/async_llm.py
3	0	vllm/v1/engine/llm_engine.py

[1575c1701] Jee Jee Li 2025-04-15 [CI/Build] Fix LoRA OOM (#16624)
13	4	tests/lora/test_minicpmv_tp.py

[6ae996a87] Reid 2025-04-15 [Misc] refactor argument parsing in examples (#16635)
28	24	examples/offline_inference/audio_language.py
20	13	examples/offline_inference/basic/basic.py
19	13	examples/offline_inference/basic/chat.py
11	7	examples/offline_inference/basic/classify.py
11	7	examples/offline_inference/basic/embed.py
20	12	examples/offline_inference/basic/generate.py
11	7	examples/offline_inference/basic/score.py
36	31	examples/offline_inference/data_parallel.py
7	2	examples/offline_inference/eagle.py
11	7	examples/offline_inference/embed_jina_embeddings_v3.py
11	7	examples/offline_inference/embed_matryoshka_fy.py
104	86	examples/offline_inference/encoder_decoder.py
18	15	examples/offline_inference/encoder_decoder_multimodal.py
5	2	examples/offline_inference/mistral-small.py
5	2	examples/offline_inference/mlpspeculator.py
33	29	examples/offline_inference/prithvi_geospatial_mae.py
10	2	examples/offline_inference/profiling.py
18	15	examples/offline_inference/save_sharded_state.py
5	1	examples/offline_inference/simple_profiling.py
54	51	examples/offline_inference/vision_language.py
10	6	examples/offline_inference/vision_language_embedding.py
20	17	examples/offline_inference/vision_language_multi_image.py
11	7	examples/online_serving/api_client.py
102	57	examples/online_serving/gradio_openai_chatbot_webserver.py
26	2	examples/online_serving/gradio_webserver.py

[b590adfdc] Richard Zou 2025-04-15 Fix vLLM x torch.compile config caching (#16491)
9	3	vllm/config.py

[b4fe16c75] Michael Goin 2025-04-15 Add `vllm bench [latency, throughput]` CLI commands (#16508)
7	0	.buildkite/test-pipeline.yaml
0	0	tests/benchmarks/__init__.py
19	0	tests/benchmarks/test_latency_cli.py
44	0	tests/benchmarks/test_serve_cli.py
19	0	tests/benchmarks/test_throughput_cli.py
831	0	vllm/benchmarks/datasets.py
181	0	vllm/benchmarks/latency.py
608	0	vllm/benchmarks/throughput.py
29	0	vllm/entrypoints/cli/benchmark/latency.py
4	2	vllm/entrypoints/cli/benchmark/main.py
29	0	vllm/entrypoints/cli/benchmark/throughput.py

[bc5dd4f66] Pooya Davoodi 2025-04-14 [Bugfix] Fix broken GritLM model and tests (missing pooling_metadata) (#16631)
11	10	tests/models/embedding/language/test_gritlm.py
2	1	vllm/model_executor/models/gritlm.py

[dbb036cf6] Tyler Michael Smith 2025-04-15 [Bugfix] Fix tests/kernels/test_mamba_ssm_ssd.py (#16623)
12	5	tests/kernels/test_mamba_ssm_ssd.py

[70e7ed841] Taneem Ibrahim 2025-04-14 [BugFix]: Update minimum `pyzmq` version (#16549)
1	1	requirements/common.txt

[d06ba4ed3] Jinzhen Lin 2025-04-15 [Kernel] moe wna16 marlin kernel (#14447)
41	11	CMakeLists.txt
103	0	csrc/moe/marlin_moe_wna16/generate_kernels.py
44	0	csrc/moe/marlin_moe_wna16/kernel.h
1917	0	csrc/moe/marlin_moe_wna16/marlin_template.h
927	0	csrc/moe/marlin_moe_wna16/ops.cu
11	8	csrc/moe/torch_bindings.cpp
7	2	csrc/quantization/gptq_marlin/marlin.cuh
7	3	csrc/quantization/gptq_marlin/marlin_dtypes.cuh
137	117	tests/kernels/test_moe.py
46	0	vllm/_custom_ops.py
173	160	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
12	8	vllm/model_executor/layers/fused_moe/fused_moe.py
1	0	vllm/model_executor/layers/fused_moe/layer.py
22	13	vllm/model_executor/layers/quantization/awq_marlin.py
23	14	vllm/model_executor/layers/quantization/gptq_marlin.py
13	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[6b40996ae] Alex Brooks 2025-04-14 [Core][Bugfix] Fix Offline MM Beam Search (#16390)
20	12	tests/conftest.py
82	3	tests/samplers/test_beam_search.py
11	2	vllm/beam_search.py
27	13	vllm/entrypoints/llm.py

[d2020acac] Shuqiao Li 2025-04-15 config check sleep mode support oot platforms (#16562)
4	2	vllm/config.py
3	0	vllm/platforms/interface.py

[1eb3c2ed4] Chengji Yao 2025-04-14 [DOC][TPU] Add core idea about avoiding recompilation after warmup (#16614)
35	0	vllm/v1/worker/tpu_model_runner.py

[c64ee8726] Siyuan Liu 2025-04-14 [Hardware][TPU] Add torchvision to tpu dependency file (#16616)
2	3	requirements/tpu.txt

[b1308b84a] courage17340 2025-04-15 [Model][VLM] Add Kimi-VL model support (#16387)
7	0	docs/source/models/supported_models.md
24	0	examples/offline_inference/vision_language.py
40	0	examples/offline_inference/vision_language_multi_image.py
1	0	requirements/test.in
9	1	requirements/test.txt
12	0	tests/models/decoder_only/vision_language/test_models.py
11	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
1	0	tests/models/multimodal/processing/test_common.py
3	0	tests/models/registry.py
2	0	vllm/entrypoints/chat_utils.py
608	0	vllm/model_executor/models/kimi_vl.py
628	0	vllm/model_executor/models/moonvit.py
1	0	vllm/model_executor/models/registry.py
8	6	vllm/transformers_utils/config.py
4	0	vllm/transformers_utils/configs/__init__.py
36	0	vllm/transformers_utils/configs/kimi_vl.py
32	0	vllm/transformers_utils/configs/moonvit.py
9	7	vllm/v1/worker/gpu_model_runner.py

[7b5ecf79b] Nishan Acharya 2025-04-14 s390x: Fix PyArrow build and add CPU test script for Buildkite CI (#16036)
13	0	.buildkite/scripts/hardware_ci/run-cpu-test-s390x.sh
22	2	docker/Dockerfile.s390x

[9883a1885] Harry Mellor 2025-04-14 Fix triton install condition on CPU (#16600)
2	2	requirements/cpu.txt

[b3f2fddd1] Nicolò Lucchesi 2025-04-14 [TPU][V1] Fix exponential padding when `max-num-batched-tokens` is not a power of 2 (#16596)
12	0	tests/v1/tpu/worker/test_tpu_model_runner.py
3	1	vllm/v1/worker/tpu_model_runner.py

[aa29841ed] Cyrus Leung 2025-04-15 [Bugfix] Multi-modal caches not acting like LRU caches (#16593)
0	109	tests/lora/test_utils.py
128	5	tests/test_utils.py
58	11	vllm/utils.py
1	1	vllm/v1/engine/mm_input_cache.py

[6bf27affb] Md. Shafi Hussain 2025-04-14 [fix]: Dockerfile.ppc64le fixes for opencv-python and hf-xet (#16048)
14	4	docker/Dockerfile.ppc64le

[1dd23386e] shangmingc 2025-04-14 [Misc] Update usage with mooncake lib for kv transfer (#16523)
1	1	vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py
10	10	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py

[7cbfc1094] Reid 2025-04-14 [Misc] refactor examples (#16563)
5	1	examples/offline_inference/disaggregated_prefill.py
9	6	examples/offline_inference/disaggregated_prefill_lmcache.py
38	24	examples/online_serving/cohere_rerank_client.py
16	9	examples/online_serving/jinaai_rerank_client.py
42	31	examples/online_serving/openai_chat_completion_client.py

[ce4ddd2d1] DefTruth 2025-04-14 [Misc] remove warning if triton>=3.2.0 (#16553)
6	5	vllm/attention/ops/triton_decode_attention.py

[e51929ebc] Harry Mellor 2025-04-14 Improve configs - `SchedulerConfig` (#16533)
102	55	vllm/config.py
167	153	vllm/engine/arg_utils.py
12	12	vllm/entrypoints/openai/cli_args.py
2	2	vllm/entrypoints/openai/run_batch.py

[dc1b4a6f1] Russell Bryant 2025-04-13 [Core][V0] Enable regex support with xgrammar (#13228)
13	2	tests/entrypoints/llm/test_guided_generate.py
2	7	vllm/model_executor/guided_decoding/__init__.py
10	0	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[63d2705ed] Jennifer Zhao 2025-04-13 [Benchmark][Bugfix] Fix SonnetDataset default values in benchmark_throughput.py (#16556)
16	8	benchmarks/benchmark_throughput.py

[d085a4408] Michael Goin 2025-04-13 Enable PTPC FP8 for CompressedTensorsW8A8Fp8MoEMethod (triton fused_moe) (#16537)
68	39	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[f49e5aff1] Lily Liu 2025-04-12 [V1][Spec Decode] KV cache slots for eagle heads (#16370)
74	12	tests/v1/core/test_kv_cache_utils.py
12	4	vllm/v1/core/kv_cache_manager.py
11	2	vllm/v1/core/sched/scheduler.py
1	0	vllm/v1/engine/core.py

[6c11ecf8d] Ryan McConville 2025-04-12 [Bugfix] Validate logit biases to prevent out of vocab ids crashing engine (#16529)
88	0	tests/entrypoints/openai/test_chat_logit_bias_validation.py
21	0	vllm/v1/engine/processor.py
10	0	vllm/v1/sample/sampler.py

[93e5f3c5f] SnowCharm 2025-04-12 [Perf] Optimize Preparing Inputs for GPU Model Runner (#16484)
4	8	vllm/v1/worker/gpu_model_runner.py

[70363bccf] Jie Fu (傅杰) 2025-04-12 Fix syntaxWarning: invalid escape sequence '\s' (#16532)
2	2	vllm/utils.py

[3cdc57669] Jee Jee Li 2025-04-12 [Misc] Delete redundant code (#16530)
0	7	examples/offline_inference/vision_language.py
0	7	examples/offline_inference/vision_language_multi_image.py

[68bb122eb] Huazhong Ji 2025-04-12 [MISC] Make GroupCoordinator compatible with out-of-tree devices (#16464)
3	1	vllm/distributed/parallel_state.py

[d9fc8cd9d] Cyrus Leung 2025-04-12 [V1] Enable multi-input by default (#15799)
3	1	docs/source/models/supported_models.md
24	0	docs/source/serving/offline_inference.md
5	0	examples/offline_inference/audio_language.py
5	0	examples/offline_inference/encoder_decoder_multimodal.py
45	34	examples/offline_inference/vision_language.py
7	0	examples/offline_inference/vision_language_embedding.py
5	0	examples/offline_inference/vision_language_multi_image.py
34	27	tests/entrypoints/openai/test_audio.py
4	0	tests/models/decoder_only/vision_language/vlm_utils/core.py
1	0	tests/models/test_oot_registration.py
5	2	tests/multimodal/test_processing.py
9	3	vllm/config.py
3	3	vllm/engine/arg_utils.py
24	5	vllm/entrypoints/chat_utils.py
1	1	vllm/model_executor/models/minicpmo.py
2	2	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/qwen2_vl.py
29	5	vllm/multimodal/processing.py
1	17	vllm/multimodal/profiling.py
4	2	vllm/multimodal/registry.py
1	1	vllm/multimodal/utils.py

[f069f3ea7] Nicolò Lucchesi 2025-04-12 [Misc] Openai transcription client example use same Whisper model (#16487)
1	1	examples/online_serving/openai_transcription_client.py

[c5bc0e7fc] Cyrus Leung 2025-04-12 [Misc] Update chat utils tests (#16520)
7	0	tests/entrypoints/test_chat_utils.py

[4a3a51872] Tianer Zhou 2025-04-12 fix: spelling (#16466)
1	1	csrc/custom_all_reduce.cuh

[fbf722c6e] wang.yuqi 2025-04-12 [Frontend] support matryoshka representation / support embedding API dimensions (#16331)
48	0	examples/offline_inference/embed_matryoshka_fy.py
8	8	tests/conftest.py
82	0	tests/entrypoints/openai/test_embedding_dimensions.py
39	1	tests/models/embedding/language/test_jina.py
7	0	tests/models/embedding/utils.py
9	0	vllm/config.py
10	0	vllm/entrypoints/llm.py
4	2	vllm/entrypoints/openai/protocol.py
7	5	vllm/entrypoints/openai/serving_embedding.py
18	4	vllm/model_executor/layers/pooler.py
21	2	vllm/pooling_params.py

[e92d7085b] leon-seidel 2025-04-12 [Feature][V1] Add xgrammar to support minLength, maxLength with test (#16516)
39	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
8	8	tests/v1/structured_output/test_utils.py
1	2	vllm/v1/structured_output/utils.py

[bd6028d6b] Michael Goin 2025-04-12 Optimized topk for topk=1 (Llama-4) (#16512)
2	2	vllm/model_executor/models/llama4.py
9	0	vllm/model_executor/models/utils.py

[802329dee] Ye (Charlotte) Qi 2025-04-11 [Doc] Update Llama4 Model Names in Supported Models (#16509)
1	1	docs/source/models/supported_models.md

[41cc883c2] Nick Hill 2025-04-11 [BugFix] Handle non-contiguous tensors properly when serializing (#16492)
18	4	tests/v1/test_serial_utils.py
12	7	vllm/v1/serial_utils.py

[57504a4bc] Michael Goin 2025-04-11 [CI][Bugfix] Add mistral_tool_use to Ci (#16517)
2	0	.buildkite/test-pipeline.yaml

[ed4792c99] Yuan Tang 2025-04-11 [Doc] Fix link to vLLM blog (#16519)
1	1	README.md

[87b836ba7] Michael Goin 2025-04-11 Bugfix for PixtralHF models without spatial_merge_size (#16513)
3	2	vllm/model_executor/models/pixtral.py

[56c76c2e0] rongfu.leng 2025-04-12 [Bugfix] clean up duplicated code (#16485)
0	1	vllm/model_executor/models/opt.py

[c09632a66] Christian Sears 2025-04-11 Update openai_compatible_server.md (#16507)
3	3	docs/source/serving/openai_compatible_server.md

[a3bf8d4a2] Yong Hoon Shin 2025-04-11 [Kernel] Add tuned FusedMoE kernel config for Llama4 Scout, TP=8 on H100  (#16488)
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=NVIDIA_H100.json

[16eda8c43] Ye (Charlotte) Qi 2025-04-11 [Frontend] Added chat templates for LLaMa4 pythonic tool calling (#16463)
2	0	docs/source/features/tool_calling.md
139	0	examples/tool_chat_template_llama4_pythonic.jinja
24	1	tests/tool_use/conftest.py
16	0	tests/tool_use/utils.py
1	1	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py

[cd77382ac] Harry Mellor 2025-04-11 Improve configs - `LoadConfig` (#16422)
42	35	vllm/config.py
33	55	vllm/engine/arg_utils.py
21	7	vllm/utils.py

[71b9cde01] Travis Johnson 2025-04-11 [Bugfix] handle alignment of encoder_seq_lens in mllama.py (#14784)
50	9	tests/models/encoder_decoder/vision_language/test_mllama.py
32	13	vllm/model_executor/models/mllama.py

[5285589f3] Isotr0py 2025-04-12 [Doc] Document InternVL3 support (#16495)
2	2	docs/source/models/supported_models.md

[f41647ee6] Michael Goin 2025-04-11 [Kernel] Support W8A8 channel-wise weights and per-token activations in triton fused_moe_kernel (#16366)
19	73	tests/kernels/test_block_fp8.py
199	0	tests/kernels/test_block_int8.py
149	0	tests/kernels/test_int8_kernel.py
159	0	tests/kernels/test_triton_moe_ptpc_fp8.py
63	0	tests/kernels/utils_block.py
181	85	vllm/model_executor/layers/fused_moe/fused_moe.py
459	0	vllm/model_executor/layers/quantization/utils/int8_utils.py

[4d022cbc7] Nicolò Lucchesi 2025-04-11 [TPU][V1] Make `--disable_chunked_mm_input` mandatory for serving MM models (#16483)
7	0	vllm/platforms/tpu.py

[70de35a88] Richard Zou 2025-04-11 Fix erroneous "model doesn't support compile" warning (#16486)
4	1	vllm/config.py

[34b2cf3b3] Tomasz Zielinski 2025-04-11 [Hardware][Intel-Gaudi] Multi-step scheduling implementation for HPU (#12779)
3	3	vllm/platforms/hpu.py
314	113	vllm/worker/hpu_model_runner.py
122	0	vllm/worker/multi_step_hpu_worker.py

[9e90c9f73] chaow-amd 2025-04-11 [Bugfix] Fix bugs of running Quark quantized models (#16236)
37	8	tests/quantization/test_quark.py
6	4	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
24	10	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py

[e9528f6dc] DefTruth 2025-04-11 [Kernel] support merge_attn_states CUDA kernel, 3x speedup (#16173)
1	0	CMakeLists.txt
173	0	csrc/attention/merge_attn_states.cu
9	0	csrc/ops.h
15	0	csrc/torch_bindings.cpp
265	0	tests/kernels/test_merge_attn_states.py
11	0	vllm/_custom_ops.py
1	2	vllm/attention/backends/mla/common.py
42	0	vllm/attention/ops/merge_attn_states.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/mla/common.py

[51baa9c33] Harry Mellor 2025-04-11 Don't install triton on `ppc64le` platform (#16470)
1	1	requirements/cpu.txt

[35e076b3a] Reid 2025-04-11 [Misc] update api_client example (#16459)
16	9	examples/online_serving/api_client.py

[a26f59ccb] Jee Jee Li 2025-04-11 [Misc] Raise error for V1 not supporting Long LoRA. (#16415)
6	0	vllm/config.py
1	1	vllm/lora/models.py

[aa3b3d76e] Michael Goin 2025-04-11 Enforce valid max_num_batched_tokens when disable_chunked_mm_input=True (#16447)
9	0	tests/v1/core/test_scheduler.py
1	1	vllm/engine/arg_utils.py
8	0	vllm/v1/core/encoder_cache_manager.py

[f7030df3b] Jee Jee Li 2025-04-11 [Core][LoRA][1/N] Add LoRA for EncoderDecoderModelRunner (#15990)
5	0	vllm/lora/layers.py
11	0	vllm/model_executor/models/mllama.py
25	1	vllm/worker/enc_dec_model_runner.py

[905e91e9a] DefTruth 2025-04-11 Revert "[Model] use AutoWeightsLoader for deepseek_v2, internlm2" (#16453)
71	73	vllm/model_executor/models/deepseek_v2.py
36	39	vllm/model_executor/models/internlm2.py

[f8f9c0ba6] Alex Brooks 2025-04-11 [Bugfix] Don't set an upper bound on repetition penalty (#16403)
4	3	vllm/sampling_params.py

[dda811021] Li, Jiang 2025-04-11 [CPU][Bugfix] Fix CPU docker issues (#16454)
3	0	docker/Dockerfile.cpu

[93195146e] Isotr0py 2025-04-11 [Bugfix][VLM] Fix failing Phi-4-MM multi-images tests and add vision-speech test (#16424)
9	9	examples/offline_inference/audio_language.py
24	10	examples/offline_inference/vision_language.py
4	8	examples/offline_inference/vision_language_multi_image.py
2	2	tests/models/decoder_only/vision_language/test_models.py
80	17	tests/models/decoder_only/vision_language/test_phi4mm.py

[ed3759954] Michael Goin 2025-04-10 Update supported_hardware.md for TPU INT8 (#16437)
1	1	docs/source/features/quantization/supported_hardware.md

[99ef59cf7] Yong Hoon Shin 2025-04-10 [Llama4] Enable attention temperature tuning by default for long context (>32k) (#16439)
6	2	vllm/model_executor/models/llama4.py

[d544d141e] Chenyaaang 2025-04-10 update benchmark_serving_structured_output to include auto backend (#16438)
9	7	benchmarks/benchmark_serving_structured_output.py

[3e397a948] Alexey Belyakov 2025-04-11 check input length of sonnet samples (#16423)
9	8	benchmarks/benchmark_dataset.py

[268c32507] WWW 2025-04-11 Fix range_ratio Bug in RandomDataset (#16126)
18	5	benchmarks/benchmark_dataset.py
12	7	benchmarks/benchmark_serving.py
12	8	benchmarks/benchmark_throughput.py

[3cc9af88f] Nicolò Lucchesi 2025-04-10 [TPU][V1] Disable per-request seed/Generator (#16172)
5	0	tests/v1/tpu/test_sampler.py
8	5	vllm/platforms/tpu.py
10	6	vllm/v1/sample/tpu/metadata.py
1	7	vllm/v1/worker/tpu_model_runner.py

[7cd0bd721] look 2025-04-11 [Bugfix] Fix output token length check logic (#16419)
1	1	benchmarks/benchmark_serving.py

[56d4aefa3] Cyrus Leung 2025-04-11 [VLM] Avoid unnecessary dummy multimodal data during processing (#16416)
14	13	vllm/model_executor/models/aria.py
13	12	vllm/model_executor/models/aya_vision.py
9	10	vllm/model_executor/models/blip2.py
14	10	vllm/model_executor/models/chameleon.py
14	13	vllm/model_executor/models/deepseek_vl2.py
10	11	vllm/model_executor/models/florence2.py
9	10	vllm/model_executor/models/fuyu.py
15	14	vllm/model_executor/models/gemma3_mm.py
16	16	vllm/model_executor/models/glm4v.py
17	15	vllm/model_executor/models/idefics3.py
11	11	vllm/model_executor/models/internvl.py
12	11	vllm/model_executor/models/llava.py
14	13	vllm/model_executor/models/llava_next_video.py
14	13	vllm/model_executor/models/llava_onevision.py
18	16	vllm/model_executor/models/minicpmo.py
15	12	vllm/model_executor/models/minicpmv.py
14	12	vllm/model_executor/models/mistral3.py
14	14	vllm/model_executor/models/mllama.py
14	12	vllm/model_executor/models/mllama4.py
9	10	vllm/model_executor/models/molmo.py
11	12	vllm/model_executor/models/nvlm_d.py
7	9	vllm/model_executor/models/paligemma.py
14	13	vllm/model_executor/models/phi3v.py
9	10	vllm/model_executor/models/pixtral.py
13	12	vllm/model_executor/models/prithvi_geospatial_mae.py
14	10	vllm/model_executor/models/qwen2_audio.py
15	14	vllm/model_executor/models/qwen2_vl.py
16	15	vllm/model_executor/models/qwen_vl.py
11	11	vllm/model_executor/models/skyworkr1v.py
11	11	vllm/model_executor/models/ultravox.py
12	12	vllm/model_executor/models/whisper.py
1	12	vllm/multimodal/processing.py
34	3	vllm/multimodal/profiling.py

[dd143ef54] Nick Hill 2025-04-10 [V1] Zero-copy tensor/ndarray serialization/transmission (#13790)
80	0	tests/v1/test_serial_utils.py
4	4	vllm/v1/engine/core.py
13	13	vllm/v1/engine/core_client.py
120	41	vllm/v1/serial_utils.py

[daefed052] Chih-Chieh Yang 2025-04-10 [Model] Reduce redundant computations in mamba2 blocks for Bamba-9B (#15423)
109	0	vllm/model_executor/layers/mamba/mamba2_metadata.py
20	32	vllm/model_executor/layers/mamba/mamba_mixer2.py
10	41	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
9	1	vllm/model_executor/layers/mamba/ops/ssd_combined.py
0	2	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
13	19	vllm/model_executor/models/bamba.py
11	17	vllm/model_executor/models/mamba2.py
14	20	vllm/model_executor/models/zamba2.py

[5fbab20e0] Chenyaaang 2025-04-10 [Bugfix] Fix bug when dataset is json (#15899)
3	2	benchmarks/benchmark_serving_structured_output.py

[e8224f3dc] Lily Liu 2025-04-10 [V1][Spec Decode] Eagle Model loading (#16035)
1	0	examples/offline_inference/eagle.py
4	0	tests/models/registry.py
49	0	tests/v1/e2e/{test_ngram_spec_decode.py => test_spec_decode.py}
2	2	vllm/model_executor/model_loader/loader.py
151	0	vllm/model_executor/models/llama_eagle.py
1	0	vllm/model_executor/models/registry.py
4	1	vllm/transformers_utils/configs/eagle.py
36	25	vllm/v1/spec_decode/eagle.py
5	2	vllm/v1/worker/gpu_model_runner.py

[9665313c3] Russell Bryant 2025-04-10 [V1] Set structured output backend to `auto` by default (#15724)
11	63	tests/entrypoints/openai/test_chat.py
2	2	vllm/config.py
3	3	vllm/engine/arg_utils.py
6	0	vllm/model_executor/guided_decoding/__init__.py

[0c54fc727] Harry Mellor 2025-04-10 Improve configs - `ParallelConfig` (#16332)
118	28	vllm/config.py
64	57	vllm/engine/arg_utils.py

[c1b57855e] Nicolò Lucchesi 2025-04-10 [TPU][V1] Use `language_model` interface for getting text backbone in MM (#16410)
1	2	vllm/v1/worker/tpu_model_runner.py

[83b824c8b] Cyrus Leung 2025-04-11 [VLM] Remove `BaseProcessingInfo.get_mm_max_tokens_per_item` (#16408)
37	203	docs/source/contributing/model/multimodal.md
0	5	tests/models/multimodal/processing/test_llama4.py
0	7	vllm/model_executor/models/aria.py
0	25	vllm/model_executor/models/aya_vision.py
0	7	vllm/model_executor/models/blip2.py
0	7	vllm/model_executor/models/chameleon.py
0	3	vllm/model_executor/models/clip.py
0	14	vllm/model_executor/models/deepseek_vl2.py
2	9	vllm/model_executor/models/florence2.py
0	15	vllm/model_executor/models/fuyu.py
0	16	vllm/model_executor/models/gemma3_mm.py
0	7	vllm/model_executor/models/glm4v.py
0	23	vllm/model_executor/models/h2ovl.py
0	16	vllm/model_executor/models/idefics3.py
0	16	vllm/model_executor/models/internvl.py
0	7	vllm/model_executor/models/llava.py
0	16	vllm/model_executor/models/llava_next_video.py
0	10	vllm/model_executor/models/llava_onevision.py
0	11	vllm/model_executor/models/minicpmo.py
0	12	vllm/model_executor/models/minicpmv.py
0	15	vllm/model_executor/models/mistral3.py
0	10	vllm/model_executor/models/mllama.py
0	19	vllm/model_executor/models/mllama4.py
0	16	vllm/model_executor/models/molmo.py
33	15	vllm/model_executor/models/paligemma.py
0	15	vllm/model_executor/models/phi3v.py
0	23	vllm/model_executor/models/pixtral.py
0	3	vllm/model_executor/models/prithvi_geospatial_mae.py
0	11	vllm/model_executor/models/qwen2_audio.py
0	10	vllm/model_executor/models/qwen2_vl.py
0	7	vllm/model_executor/models/qwen_vl.py
0	3	vllm/model_executor/models/siglip.py
0	16	vllm/model_executor/models/skyworkr1v.py
0	12	vllm/model_executor/models/ultravox.py
0	4	vllm/model_executor/models/vision.py
2	9	vllm/model_executor/models/whisper.py
0	15	vllm/multimodal/processing.py
22	43	vllm/multimodal/profiling.py
8	2	vllm/multimodal/registry.py

[7678fcd5b] Lu Fang 2025-04-10 Fix the torch version parsing logic (#15857)
2	3	vllm/compilation/compiler_interface.py
3	3	vllm/compilation/inductor_pass.py
3	5	vllm/config.py
18	0	vllm/utils.py

[8661c0241] wineandchord 2025-04-10 [CI] Add auto update workflow for Dockerfile graph (#11879)
6	0	.pre-commit-config.yaml
78	0	tools/update-dockerfile-graph.sh

[ce8d6b75f] Reid 2025-04-10 [doc] update the wrong link (#16401)
1	1	.github/ISSUE_TEMPLATE/600-new-model.yml
1	1	docs/source/models/extensions/tensorizer.md

[61de3ef74] Ye (Charlotte) Qi 2025-04-10 [Model] Remove image mm limit for LLaMa4  (#16365)
23	6	examples/offline_inference/vision_language_multi_image.py
3	1	vllm/model_executor/models/mllama4.py

[ec1f9c8c9] cyyever 2025-04-10 Update Numba to 0.61.2 (#16376)
1	1	requirements/cuda.txt
1	1	requirements/rocm.txt
1	1	requirements/test.in
1	1	requirements/test.txt

[65e09094c] Reid 2025-04-10 [doc] add download model tips (#16389)
29	0	docs/source/models/supported_models.md

[c70cf0fe0] Michael Goin 2025-04-10 [Kernel] Use moe_wna16 kernel for compressed tensors wna16 moe models (#16038)
11	0	.buildkite/lm-eval-harness/configs/Qwen1.5-MoE-W4A16-compressed-tensors.yaml
1	1	.buildkite/lm-eval-harness/configs/models-small.txt
7	4	vllm/model_executor/layers/fused_moe/layer.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
234	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[a5d11a54d] Cyrus Leung 2025-04-10 [Bugfix] Fix validation error for text-only Mllama 3.2 (#16377)
17	13	vllm/engine/llm_engine.py
4	1	vllm/model_executor/models/mllama.py
18	16	vllm/v1/engine/processor.py

[3d4c87758] Cyrus Leung 2025-04-10 [Misc] Update transformers version limits of multi-modal tests (#16381)
2	5	.buildkite/test-pipeline.yaml
14	17	tests/models/decoder_only/vision_language/test_models.py
0	245	tests/models/decoder_only/vision_language/test_phi3v.py
4	3	tests/models/registry.py

[a9bd832fc] Aaron Ang 2025-04-10 [Model] use AutoWeightsLoader for deepseek_v2, internlm2 (#16383)
73	71	vllm/model_executor/models/deepseek_v2.py
39	36	vllm/model_executor/models/internlm2.py

[417bcefba] Chenyaaang 2025-04-09 fix sonnet dataset sample when prefix len is very small (#16379)
1	1	benchmarks/benchmark_dataset.py

[baada0e73] Michael Goin 2025-04-09 [Bugfix][TPU] Fix TPU validate_request (#16369)
1	4	vllm/platforms/tpu.py

[82eb61dd4] Benjamin Kitor 2025-04-09 [misc] use tqdm.auto where appropriate (#16290)
1	1	vllm/entrypoints/llm.py
1	1	vllm/worker/model_runner.py

[0d4d06fe2] Roger Wang 2025-04-09 [CI][Bugfix] Pin triton version for CPU (#16384)
3	0	requirements/cpu.txt

[4aed0ca6a] Jintao 2025-04-10 [bugfix] Avoid the time consumption caused by creating dummy videos. (#16371)
6	0	vllm/multimodal/profiling.py

[1621b2528] Chengji Yao 2025-04-09 [TPU] Fix dummy loading OOM (#16372)
15	2	vllm/model_executor/model_loader/weight_utils.py

[a56479715] Aaron Ang 2025-04-09 [Model] use AutoWeightsLoader for granite, granitemoe, granitemoeshared, grok1, mixtral (#16325)
76	69	vllm/model_executor/models/granite.py
43	36	vllm/model_executor/models/granitemoe.py
43	36	vllm/model_executor/models/granitemoeshared.py
101	96	vllm/model_executor/models/grok1.py
90	86	vllm/model_executor/models/mixtral.py

[1da6a0927] Guillaume Calmettes 2025-04-10 [Bugfix]: do not shutdown server if `skip_special_use=False` for MistralTokenizer (#14094)
3	1	vllm/entrypoints/openai/serving_chat.py
3	2	vllm/transformers_utils/tokenizers/__init__.py
7	0	vllm/transformers_utils/tokenizers/mistral.py

[1e44ffc3f] Yuxuan Zhang 2025-04-10 Add GLM-4-0414 support (#16338)
5	0	docs/source/models/supported_models.md
5	0	tests/models/registry.py
313	0	vllm/model_executor/models/glm4.py
1	0	vllm/model_executor/models/registry.py

[a45474854] Chengji Yao 2025-04-09 [TPU][V1] Refine tpu_model_runner to mitigate future recompilation issues (#16275)
10	4	tests/tpu/test_compilation.py
19	6	tests/v1/tpu/worker/test_tpu_model_runner.py
35	42	vllm/v1/sample/tpu/metadata.py
101	72	vllm/v1/worker/tpu_model_runner.py

[1bff42c4b] Reid 2025-04-10 [Misc] refactor Structured Outputs example (#16322)
60	36	examples/offline_inference/structured_outputs.py

[cb391d85d] Joe Runde 2025-04-09 [Hardware] add platform-specific request validation api (#16291)
0	4	vllm/platforms/cpu.py
0	4	vllm/platforms/cuda.py
0	4	vllm/platforms/hpu.py
15	8	vllm/platforms/interface.py
0	4	vllm/platforms/neuron.py
0	4	vllm/platforms/rocm.py
18	4	vllm/platforms/tpu.py
0	4	vllm/platforms/xpu.py
5	5	vllm/v1/engine/processor.py

[fee5b8d37] Russell Bryant 2025-04-09 [Build/CI] Add tracing deps to vllm container image (#15224)
0	5	.buildkite/test-pipeline.yaml
4	0	requirements/common.txt

[b2ce859bd] Michael Goin 2025-04-09 Fix `benchmark_throughput.py --backend=hf` (#16352)
5	2	benchmarks/benchmark_throughput.py

[566f10a92] Chendi.Xue 2025-04-09 [CI]Fix hpu docker and numpy version for CI (#16355)
1	1	docker/Dockerfile.hpu
1	0	requirements/hpu.txt

[c3b518913] Guillaume Calmettes 2025-04-09 [Bugfix] catch AssertionError in MistralTokenizer as ValueError (#16344)
12	5	vllm/entrypoints/chat_utils.py

[a25866ac8] zh Wang 2025-04-10 [Bugfix] Fix profiling.py (#16202)
2	3	examples/offline_inference/profiling.py

[098900d7c] Michael Goin 2025-04-09 Revert "Update label-tpu mergify and remove removal bot" (#16350)
17	1	.github/mergify.yml

[98d01d3ce] Guillaume Calmettes 2025-04-09 [Bugfix][Frontend] respect provided default guided decoding backend (#15476)
78	3	tests/test_sampling_params.py
0	2	vllm/entrypoints/openai/protocol.py

[d55244df3] Nicolò Lucchesi 2025-04-09 [Model] Add `SupportsMultiModal.get_language_model` interface (#16007)
11	0	docs/source/contributing/model/multimodal.md
3	0	vllm/model_executor/models/aria.py
3	0	vllm/model_executor/models/aya_vision.py
3	0	vllm/model_executor/models/blip2.py
3	0	vllm/model_executor/models/chameleon.py
3	0	vllm/model_executor/models/deepseek_vl2.py
3	0	vllm/model_executor/models/florence2.py
3	0	vllm/model_executor/models/fuyu.py
3	0	vllm/model_executor/models/gemma3_mm.py
3	0	vllm/model_executor/models/glm4v.py
3	0	vllm/model_executor/models/idefics3.py
12	0	vllm/model_executor/models/interfaces.py
3	0	vllm/model_executor/models/internvl.py
3	0	vllm/model_executor/models/llava.py
3	0	vllm/model_executor/models/llava_next.py
3	0	vllm/model_executor/models/llava_next_video.py
3	0	vllm/model_executor/models/llava_onevision.py
3	0	vllm/model_executor/models/minicpmv.py
3	0	vllm/model_executor/models/mistral3.py
3	0	vllm/model_executor/models/mllama.py
3	0	vllm/model_executor/models/mllama4.py
3	0	vllm/model_executor/models/molmo.py
3	0	vllm/model_executor/models/paligemma.py
3	0	vllm/model_executor/models/phi3v.py
3	0	vllm/model_executor/models/phi4mm.py
3	0	vllm/model_executor/models/pixtral.py
3	0	vllm/model_executor/models/qwen2_5_vl.py
3	0	vllm/model_executor/models/qwen2_audio.py
3	0	vllm/model_executor/models/qwen2_vl.py
3	0	vllm/model_executor/models/qwen_vl.py
3	0	vllm/model_executor/models/skyworkr1v.py
3	0	vllm/model_executor/models/ultravox.py
3	0	vllm/model_executor/models/whisper.py

[04149cce2] yihong 2025-04-09 [BugFix] fix some typos found by typos. (#16314)
2	2	benchmarks/benchmark_serving.py
2	2	benchmarks/benchmark_serving_structured_output.py
1	1	csrc/mamba/causal_conv1d/causal_conv1d.cu
1	1	vllm/attention/backends/flash_attn.py
3	3	vllm/attention/backends/hpu_attn.py
3	3	vllm/attention/backends/mla/common.py
3	3	vllm/attention/backends/xformers.py
1	1	vllm/attention/ops/nki_flash_attn.py
2	2	vllm/benchmarks/serve.py
1	1	vllm/engine/output_processor/multi_step.py
1	1	vllm/entrypoints/openai/tool_parsers/utils.py
1	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
3	3	vllm/platforms/cpu.py
1	1	vllm/platforms/interface.py
1	1	vllm/reasoning/granite_reasoning_parser.py
1	1	vllm/sampling_params.py
1	1	vllm/third_party/pynvml.py
2	2	vllm/v1/attention/backends/mla/common.py
1	1	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/tpu_model_runner.py

[24834f489] ajayvohra2005 2025-04-09 update neuron config (#16289)
32	1	vllm/model_executor/model_loader/neuron.py

[ec7da6fcf] Lucia Fang 2025-04-09 [BugFix] llama4 qknorm should be not shared across head (#16311)
7	12	vllm/model_executor/models/llama4.py

[819d548e8] yihong 2025-04-09 [BugFix] logger is not callable (#16312)
2	2	vllm/attention/backends/hpu_attn.py

[477d2a8aa] Michael Goin 2025-04-09 Update label-tpu mergify and remove removal bot (#16298)
1	17	.github/mergify.yml

[e484e0285] Cyrus Leung 2025-04-09 [Bugfix] Avoid transferring cached multi-modal items from P0 to P1 (#16273)
2	1	vllm/v1/engine/__init__.py
3	3	vllm/v1/engine/core.py
34	7	vllm/v1/engine/mm_input_cache.py
17	6	vllm/v1/engine/processor.py
9	5	vllm/v1/request.py

[24f6b9a71] Accelerator1996 2025-04-09 [Misc] Fix test_sharded_state_loader.py(#16004) (#16005)
10	9	tests/test_sharded_state_loader.py

[9cdde4728] Luka Govedič 2025-04-09 [BugFix] Fix fusion test and add them to CI (#16287)
8	1	.buildkite/test-pipeline.yaml
58	45	tests/compile/test_full_graph.py
8	3	tests/compile/test_fusion.py

[b1eb4ca15] Chengji Yao 2025-04-08 [TPU] Update PyTorch/XLA (#16288)
6	6	requirements/tpu.txt
4	8	tests/v1/tpu/test_pallas.py

[87b4ac56c] Michael Goin 2025-04-08 [CI][Bugfix] Fix bad tolerance for test_batch_base64_embedding (#16221)
17	11	tests/entrypoints/openai/test_embedding.py

[cb84e45ac] Russell Bryant 2025-04-08 [Core] Upgrade to xgrammar 0.1.18, add cache size limit (#16283)
1	1	requirements/common.txt
7	0	vllm/envs.py
7	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py
7	1	vllm/v1/structured_output/backend_xgrammar.py

[4716377fb] rongfu.leng 2025-04-09 [Feature] Estimate max-model-len use available KV cache memory (#16168)
45	1	tests/v1/core/test_kv_cache_utils.py
61	4	vllm/v1/core/kv_cache_utils.py

[4e9cf8c1d] rongfu.leng 2025-04-09 [Bugfix] fix gettid method is not define (#16084)
5	0	csrc/cpu/utils.cpp

[2976dc27e] TJian 2025-04-09 [Bug] [ROCm] Fix Llama 4 Enablement Bug on ROCm: V0 ROCmFlashAttentionImpl and Triton Fused MoE bugs (#16198)
4	1	vllm/attention/backends/rocm_flash_attn.py
2	0	vllm/model_executor/layers/fused_moe/fused_moe.py
9	8	vllm/utils.py

[102bf967f] Chauncey 2025-04-09 [Model] Add smolvlm support (#16017)
7	0	docs/source/models/supported_models.md
29	0	examples/offline_inference/vision_language.py
28	0	examples/offline_inference/vision_language_multi_image.py
1	0	requirements/test.in
4	0	requirements/test.txt
10	0	tests/models/decoder_only/vision_language/test_models.py
6	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
1	0	tests/models/multimodal/processing/test_common.py
65	0	tests/models/multimodal/processing/test_smolvlm.py
1	0	tests/models/registry.py
1	1	vllm/entrypoints/chat_utils.py
14	5	vllm/model_executor/models/idefics3.py
1	0	vllm/model_executor/models/registry.py
51	0	vllm/model_executor/models/smolvlm.py

[1f4b09b52] yueshen2016 2025-04-08 Add support to modelopt quantization of Mixtral model (#15961)
7	1	vllm/model_executor/models/mixtral_quant.py

[86c3369eb] Jee Jee Li 2025-04-09 [CI/Build] Fix CI LoRA failure (#16270)
12	0	tests/lora/conftest.py
0	1	tests/lora/test_baichuan.py
0	1	tests/lora/test_chatglm3_tp.py
1	1	tests/lora/test_layers.py
0	1	tests/lora/test_llama_tp.py
5	0	tests/lora/test_punica_ops.py
1	8	tests/lora/test_quant_model.py
0	1	tests/lora/test_transfomers_model.py

[2755c34a8] Russell Bryant 2025-04-08 [V1] Update structured output offline inference example (#15721)
7	12	examples/offline_inference/structured_outputs.py

[db1042218] Jinzhen Lin 2025-04-09 [Bugfix] fix deepseek fp16 scale bug (#14809)
24	10	vllm/model_executor/models/deepseek_v2.py

[e1a2c699d] Lucas Wilkinson 2025-04-08 [BugFix] Fix Llama4 - Index Error When Single Request Near Max Context (#16209)
1	1	vllm/v1/attention/backends/flash_attn.py

[0115ccd5c] Harry Mellor 2025-04-08 Add warning that content below line in template will be removed (#16276)
1	1	.github/PULL_REQUEST_TEMPLATE.md

[40b4284fe] Isotr0py 2025-04-09 [Bugfix] Handle `process_weights_after_loading` for `QKVCrossParallelLinear` (#15328)
21	6	vllm/model_executor/layers/linear.py
3	0	vllm/model_executor/layers/quantization/fp8.py
9	0	vllm/model_executor/model_loader/loader.py

[4ebc0b964] Cyrus Leung 2025-04-09 [Bugfix] Proper input validation for multi-modal encoder-decoder models (#16156)
1	1	examples/offline_inference/encoder_decoder_multimodal.py
1	1	examples/offline_inference/vision_language.py
2	2	examples/offline_inference/vision_language_multi_image.py
2	1	tests/engine/test_short_mm_context.py
1	1	tests/entrypoints/llm/test_prompt_validation.py
1	1	tests/entrypoints/openai/test_prompt_validation.py
4	4	tests/models/encoder_decoder/vision_language/test_mllama.py
46	17	vllm/engine/llm_engine.py
6	7	vllm/multimodal/profiling.py
49	27	vllm/v1/engine/processor.py

[dc96fd54c] Kero Liang 2025-04-09 [Misc] Avoid stripping meaningful whitespace from `nvidia-smi topo -m` output in collect_env.py (#16272)
7	1	collect_env.py

[1f5d13ab9] wang.yuqi 2025-04-08 [New Model]: jinaai/jina-embeddings-v3 (#16120)
50	0	examples/offline_inference/embed_jina_embeddings_v3.py
3	2	tests/conftest.py
61	3	tests/models/embedding/language/{test_jina_reranker_v2.py => test_jina.py}
5	0	vllm/config.py
49	17	vllm/model_executor/models/bert.py
129	64	vllm/model_executor/models/roberta.py

[90cb44eb0] Harry Mellor 2025-04-08 Update to transformers==4.51.1 (#16257)
1	1	requirements/common.txt
1	1	requirements/test.in
1	1	requirements/test.txt

[e11880dee] Kebe 2025-04-08 [Bugfix] Remove triton do_bench fast_flush arg (#16256)
1	1	tests/kernels/test_flashmla.py

[9351f91be] TY-AMD 2025-04-08 [BugFix][ROCm] Fix GGUF MoE Dispatch Block_Dim for ROCm (#16247)
10	10	csrc/quantization/gguf/moe.cuh

[5a1e1c835] rongfu.leng 2025-04-08 [Model] use AutoWeightsLoader for phimoe,qwen2_moe,qwen3_moe (#16203)
90	83	vllm/model_executor/models/phimoe.py
65	57	vllm/model_executor/models/qwen2_moe.py
65	58	vllm/model_executor/models/qwen3_moe.py

[69ecaa7c7] Alex Brooks 2025-04-08 [Misc] Add warning for multimodal data in LLM.beam_search (#16241)
10	0	vllm/entrypoints/llm.py

[7f00899ff] Reid 2025-04-08 [Misc] format and refactor some examples (#16252)
4	1	examples/offline_inference/mistral-small.py
2	0	examples/offline_inference/multilora_inference.py
33	24	examples/offline_inference/neuron.py
37	28	examples/offline_inference/neuron_int8_quantization.py
59	52	examples/offline_inference/prefix_caching.py
14	7	examples/offline_inference/reproduciblity.py
6	2	examples/offline_inference/rlhf.py
3	1	examples/offline_inference/simple_profiling.py
3	1	examples/offline_inference/torchrun_example.py
19	11	examples/offline_inference/tpu.py
4	0	examples/offline_inference/vision_language.py
2	0	examples/offline_inference/vision_language_embedding.py
4	0	examples/offline_inference/vision_language_multi_image.py

[995e3d1f4] Simon Mo 2025-04-08 [Docs] Add Slides from Singapore Meetup (#16213)
1	4	README.md
1	0	docs/source/community/meetups.md

[b4ac449a8] Kebe 2025-04-08 [Misc] Merge the logs of pp layers partitions (#16225)
5	4	vllm/distributed/utils.py

[8e5314a46] Michael Goin 2025-04-08 [V1] Add `disable_chunked_mm_input` arg to disable partial mm input prefill (#15837)
45	0	tests/v1/core/test_scheduler.py
8	0	vllm/config.py
16	0	vllm/engine/arg_utils.py
11	0	vllm/v1/core/sched/scheduler.py

[87918e40c] Siyuan Liu 2025-04-07 [torch.compile][TPU] Make @support_torch_compile work for XLA backend (#15782)
38	73	vllm/v1/worker/tpu_model_runner.py
9	3	vllm/v1/worker/tpu_worker.py

[f6b32efb7] Isotr0py 2025-04-08 [Bugfix] Fix and reorganize broken GGUF tests and bump gguf version (#16194)
1	1	requirements/common.txt
60	20	tests/models/decoder_only/language/test_gguf.py

[b99733d09] Michael Goin 2025-04-07 [Bugfix] Do not skip "empty" parts of chats that are parsable (#16219)
87	2	tests/entrypoints/test_chat_utils.py
11	11	vllm/entrypoints/chat_utils.py

[05a015d6a] Yong Hoon Shin 2025-04-07 Add warning for Attention backends that do not support irope yet (#16212)
8	0	vllm/attention/backends/flashinfer.py
5	0	vllm/attention/backends/hpu_attn.py
8	0	vllm/attention/backends/ipex_attn.py
8	0	vllm/attention/backends/pallas.py
5	0	vllm/attention/backends/rocm_flash_attn.py
5	0	vllm/attention/backends/torch_sdpa.py
5	0	vllm/attention/backends/xformers.py
8	0	vllm/v1/attention/backends/pallas.py

[ad971af8c] zxfan-cpu 2025-04-08 [Bugfix] fix use-ep bug to enable ep by dp/tp size > 1 (#16161)
1	1	vllm/model_executor/layers/fused_moe/layer.py

[f2ebb6f54] Roger Wang 2025-04-07 [V1] Scatter and gather placeholders in the model runner (#16076)
8	8	docs/source/contributing/model/multimodal.md
0	3	docs/source/models/supported_models.md
2	3	tests/models/decoder_only/vision_language/test_models.py
8	16	tests/models/decoder_only/vision_language/test_pixtral.py
1	11	tests/models/multimodal/processing/test_llama4.py
2	2	tests/models/multimodal/processing/test_llava_next.py
2	2	tests/models/multimodal/processing/test_llava_onevision.py
9	0	tests/multimodal/test_processing.py
17	29	tests/v1/core/test_kv_cache_utils.py
21	50	vllm/model_executor/models/aya_vision.py
3	3	vllm/model_executor/models/chameleon.py
33	52	vllm/model_executor/models/fuyu.py
20	63	vllm/model_executor/models/gemma3_mm.py
1	1	vllm/model_executor/models/h2ovl.py
15	67	vllm/model_executor/models/idefics3.py
3	40	vllm/model_executor/models/internvl.py
6	49	vllm/model_executor/models/llava.py
9	65	vllm/model_executor/models/minicpmo.py
41	101	vllm/model_executor/models/minicpmv.py
6	44	vllm/model_executor/models/mistral3.py
33	66	vllm/model_executor/models/mllama4.py
13	61	vllm/model_executor/models/molmo.py
2	30	vllm/model_executor/models/nvlm_d.py
3	3	vllm/model_executor/models/paligemma.py
3	8	vllm/model_executor/models/phi3v.py
8	40	vllm/model_executor/models/pixtral.py
3	3	vllm/model_executor/models/qwen2_audio.py
3	3	vllm/model_executor/models/qwen_vl.py
3	39	vllm/model_executor/models/skyworkr1v.py
1	76	vllm/model_executor/models/vision.py
2	2	vllm/multimodal/base.py
29	3	vllm/multimodal/inputs.py
52	25	vllm/multimodal/processing.py
1	1	vllm/multimodal/profiling.py
1	1	vllm/multimodal/utils.py
3	4	vllm/v1/core/kv_cache_utils.py
4	4	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/request.py
40	21	vllm/v1/worker/gpu_model_runner.py
65	21	vllm/v1/worker/tpu_model_runner.py
45	0	vllm/v1/worker/utils.py

[1d0121126] Satyajith Chilappagari 2025-04-07 Update BASE_IMAGE to 2.22 release of Neuron (#16218)
4	4	docker/Dockerfile.neuron

[f94ab12f7] Miles Williams 2025-04-08 [Misc] Update compressed-tensors to version 0.9.3 (#16196)
1	1	requirements/common.txt

[a865bc1ca] youkaichao 2025-04-08 [core] do not send error across process (#16174)
7	6	vllm/v1/executor/multiproc_executor.py

[21802c4b6] Michael Goin 2025-04-07 [ROCm][Bugfix][FP8] Make fp8 quant respect fused modules mapping (#16031)
3	1	vllm/model_executor/layers/quantization/fp8.py

[652907b35] Driss Guessous 2025-04-07 Torchao (#14231)
1	0	docs/source/features/quantization/index.md
34	0	docs/source/features/quantization/torchao.md
25	0	tests/quantization/test_torchao.py
4	1	vllm/model_executor/layers/quantization/__init__.py
127	0	vllm/model_executor/layers/quantization/torchao.py

[24f1c01e0] leon-seidel 2025-04-08 [Bugfix][V0] XGrammar structured output supports Enum (#15878)
43	0	tests/entrypoints/llm/test_guided_generate.py
0	4	vllm/model_executor/guided_decoding/utils.py

[fad6e2538] Reid 2025-04-08 [Misc] add description attribute in CLI (#15921)
1	0	vllm/entrypoints/cli/benchmark/base.py
1	0	vllm/entrypoints/cli/benchmark/main.py
5	2	vllm/entrypoints/cli/openai.py
2	1	vllm/entrypoints/cli/serve.py

[7f6d47c1a] Nick Hill 2025-04-07 [V1][BugFix] Exit properly if engine core fails during startup (#16137)
1	0	requirements/test.in
3	0	requirements/test.txt
41	0	tests/v1/engine/test_engine_core_client.py
14	9	vllm/v1/engine/core_client.py
8	5	vllm/v1/utils.py

[3147586eb] Benjamin Chislett 2025-04-07 [Bugfix] Fix guidance backend for Qwen models (#16210)
2	1	vllm/v1/structured_output/backend_guidance.py

[ed636d99c] Roger Wang 2025-04-07 [Misc] Move Llama 4 projector call into encoder execution (#16201)
4	3	vllm/model_executor/models/mllama4.py

[090c856d7] Nicolò Lucchesi 2025-04-07 [Misc] Human-readable `max-model-len` cli arg (#16181)
37	1	tests/engine/test_arg_utils.py
48	2	vllm/engine/arg_utils.py

[ad434d4cf] Gregory Shtrasberg 2025-04-07 Print the warning only once (#16193)
12	10	vllm/multimodal/profiling.py

[66d433b94] Cyrus Leung 2025-04-08 [V1] Revert the default `max_num_seqs` to V0 values for most hardware (#16158)
0	7	docs/source/getting_started/v1_user_guide.md
4	2	tests/v1/engine/test_engine_args.py
2	1	vllm/engine/arg_utils.py

[027b204ff] Cyrus Leung 2025-04-07 [Bugfix] Re-enable support for `ChatGLMForConditionalGeneration` (#16187)
2	2	docs/source/models/supported_models.md
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py

[55dcce91d] Lu Fang 2025-04-07 Upstream Llama4 Support to Main (#16113)
2	1	.buildkite/test-pipeline.yaml
3	0	benchmarks/kernels/benchmark_moe.py
9	2	docs/source/models/supported_models.md
1	1	examples/offline_inference/audio_language.py
37	0	examples/offline_inference/vision_language.py
38	0	examples/offline_inference/vision_language_multi_image.py
1	1	requirements/common.txt
1	1	requirements/test.in
1	1	requirements/test.txt
13	1	tests/models/decoder_only/audio_language/test_ultravox.py
34	3	tests/models/decoder_only/vision_language/test_models.py
9	0	tests/models/decoder_only/vision_language/test_phi3v.py
2	0	tests/models/decoder_only/vision_language/test_pixtral.py
1	0	tests/models/multimodal/processing/test_common.py
99	0	tests/models/multimodal/processing/test_llama4.py
5	1	tests/models/registry.py
13	4	tests/models/test_initialization.py
2	0	vllm/config.py
1	1	vllm/entrypoints/chat_utils.py
200	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1024,device_name=AMD_Instinct_MI300X.json
12	3	vllm/model_executor/layers/fused_moe/cutlass_moe.py
18	10	vllm/model_executor/layers/fused_moe/fused_moe.py
41	24	vllm/model_executor/layers/fused_moe/layer.py
5	2	vllm/model_executor/layers/layernorm.py
5	0	vllm/model_executor/layers/quantization/awq_marlin.py
24	14	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
15	12	vllm/model_executor/layers/quantization/experts_int8.py
2	0	vllm/model_executor/layers/quantization/fp8.py
6	0	vllm/model_executor/layers/quantization/gguf.py
5	0	vllm/model_executor/layers/quantization/gptq_marlin.py
18	15	vllm/model_executor/layers/quantization/moe_wna16.py
17	13	vllm/model_executor/layers/quantization/quark/quark_moe.py
68	0	vllm/model_executor/layers/rotary_embedding.py
3	1	vllm/model_executor/model_loader/loader.py
19	7	vllm/model_executor/models/llama.py
531	0	vllm/model_executor/models/llama4.py
895	0	vllm/model_executor/models/mllama4.py
1	0	vllm/model_executor/models/registry.py
3	2	vllm/model_executor/models/telechat2.py
2	3	vllm/model_executor/models/teleflm.py
236	14	vllm/v1/attention/backends/flash_attn.py
37	18	vllm/v1/attention/backends/triton_attn.py
1	0	vllm/v1/worker/gpu_model_runner.py

[8017c8db7] Robin 2025-04-07 [Doc]Update image to latest version (#16186)
3	3	docs/source/deployment/docker.md

[dc3529dbf] Reid 2025-04-07 [Misc] improve example mlpspeculator and llm_engine_example (#16175)
6	1	examples/offline_inference/llm_engine_example.py
15	6	examples/offline_inference/mlpspeculator.py

[7699258ef] YamPengLi 2025-04-07 [Model] Add Qwen3 and Qwen3MoE (#15289)
10	0	docs/source/models/supported_models.md
10	0	tests/models/registry.py
11	5	vllm/model_executor/models/qwen2.py
329	0	vllm/model_executor/models/qwen3.py
531	0	vllm/model_executor/models/qwen3_moe.py
2	0	vllm/model_executor/models/registry.py

[e9ba99f29] Shanshan Shen 2025-04-07 [V1][Structured Output] Add `supports_structured_output()` method to Platform (#16148)
4	0	vllm/platforms/cpu.py
4	0	vllm/platforms/cuda.py
4	0	vllm/platforms/hpu.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/neuron.py
4	0	vllm/platforms/rocm.py
5	0	vllm/platforms/tpu.py
4	0	vllm/platforms/xpu.py
5	3	vllm/v1/engine/processor.py

[7c8036871] Isotr0py 2025-04-07 [VLM] Florence-2 supports online serving (#16164)
7	0	examples/template_florence2.jinja
2	2	vllm/entrypoints/chat_utils.py
13	1	vllm/model_executor/models/florence2.py

[95d63f38c] yihong 2025-04-07 doc: fix some typos in doc (#16154)
1	1	docs/source/design/mm_processing.md
1	1	docs/source/design/v1/torch_compile.md
1	1	docs/source/features/quantization/gguf.md

[bb8dab821] Roger Wang 2025-04-06 [CI] Set max transformers version for Ultravox model test  (#16149)
2	1	tests/models/registry.py

[fc0f87768] Isotr0py 2025-04-07 [Bugfix] Make dummy encoder prompt padding alternative and add missing warnings (#16129)
71	0	tests/models/multimodal/processing/test_mllama.py
3	0	tests/models/utils.py
4	0	vllm/model_executor/models/whisper.py
4	0	vllm/multimodal/processing.py
26	4	vllm/multimodal/profiling.py

[0a5738672] Cyrus Leung 2025-04-07 [Misc] Update Mistral-3.1 example (#16147)
22	8	examples/offline_inference/mistral-small.py

[3749e2877] Woosuk Kwon 2025-04-06 [V1][Minor] Minor simplification for get_computed_blocks  (#16139)
31	30	vllm/v1/core/kv_cache_manager.py

[86fc2321f] Kay Yan 2025-04-07 [Metrics] Add bucket for `request_latency`, `time_to_first_token` and `time_per_output_token` (#15202)
4	3	vllm/engine/metrics.py
4	3	vllm/v1/metrics/loggers.py

[2549c0dfe] Martin Hoyer 2025-04-07 Fix requires-python (#16132)
1	1	pyproject.toml

[b10e51989] Woosuk Kwon 2025-04-06 [V1][Minor] Optimize get_cached_block (#16135)
5	5	vllm/v1/core/block_pool.py

[9bde5ba12] Chengji Yao 2025-04-06 [TPU] Update PyTorch/XLA (#16130)
6	6	requirements/tpu.txt

[72c8f1ad0] Reid 2025-04-06 [Misc] update requires-python in pyproject.toml (#16116)
1	1	pyproject.toml

[da224daaa] paolovic 2025-04-06 [Bugfix] add hf_token to EngineArgs (#16093)
5	1	vllm/config.py
12	0	vllm/engine/arg_utils.py
5	0	vllm/entrypoints/llm.py
5	1	vllm/transformers_utils/config.py

[3a100b927] Varun Sundar Rabindranath 2025-04-06 [Bugfix] LoRA : Fix the order in which the kernels process LoRAs  (#16040)
1	1	vllm/lora/ops/triton_ops/lora_kernel_metadata.py

[242a637ae] rongfu.leng 2025-04-06 [Model] use AutoWeightsLoader for stablelm,starcoder2,zamba2 (#16103)
50	44	vllm/model_executor/models/stablelm.py
45	39	vllm/model_executor/models/starcoder2.py
40	38	vllm/model_executor/models/zamba2.py

[c2a967151] Isotr0py 2025-04-06 [Misc] Improve model redirect to accept json dictionary (#16119)
5	0	vllm/envs.py
26	11	vllm/transformers_utils/utils.py

[d5ae4f7f4] Paul Schweigert 2025-04-06 [Doc][Bugfix] Add missing EOF in k8s deploy doc (#16025)
1	0	docs/source/deployment/k8s.md

[b6c502a15] Reid 2025-04-06 [Misc] refactor example eagle (#16100)
99	86	examples/offline_inference/eagle.py

[9ca710e52] Roger Wang 2025-04-06 [CI][V1] Fix passing `tokenizer` as kwarg to `validate_guidance_grammar` (#16117)
1	2	vllm/v1/structured_output/backend_guidance.py

[eb07c8cb5] Ben Jackson 2025-04-06 [Frontend] Fix typo in tool chat templates for llama3.2 and toolace (#14501)
1	1	examples/tool_chat_template_llama3.2_pythonic.jinja
1	1	examples/tool_chat_template_toolace.jinja

[ba1080196] Hyesoo Yang 2025-04-05 [Benchmark] Add sampling parameters to benchmark_serving. (#16022)
18	0	benchmarks/README.md
6	0	benchmarks/backend_request_func.py
56	3	benchmarks/benchmark_serving.py

[620fc2d09] Lucia Fang 2025-04-05 [Model] fix model testing for TeleChat2ForCausalLM and V0 llama4 (#16112)
5	0	vllm/attention/backends/flash_attn.py
6	2	vllm/model_executor/models/telechat2.py

[29283eaa7] Jonghyun Choe 2025-04-06 [Model] use AutoWeightsLoader for phi, gemma, deepseek (#16088)
54	48	vllm/model_executor/models/deepseek.py
47	42	vllm/model_executor/models/gemma.py
46	41	vllm/model_executor/models/phi.py

[2fa66ef71] Jinzhen Lin 2025-04-06 [Bugfix] fix use_atomic_add support of marlin kernel when using v1 engine (#15946)
5	1	csrc/quantization/gptq_marlin/gptq_marlin.cu
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[13affc432] Chauncey 2025-04-06 [Misc] Remove redundant code (#16098)
1	3	vllm/entrypoints/chat_utils.py

[d8f094a92] Reid 2025-04-06 [Misc] format output for encoder_decoder.py (#16095)
6	5	examples/offline_inference/encoder_decoder.py

[97ae6d777] Harry Mellor 2025-04-05 Fix some capitalisations in generated examples doc titles (#16094)
2	0	docs/source/generate_examples.py

[6baeee70d] yihong 2025-04-05 Revert "doc: add info for macos clang errors (#16049)" (#16091)
1	9	docs/source/getting_started/installation/cpu/apple.inc.md

[d2517a493] Reid 2025-04-05 [doc] fix 404 (#16082)
1	1	docs/source/models/supported_models.md

[6342adc43] yihong 2025-04-05 fix: support clang17 for macos and fix the real libomp (#16086)
0	2	cmake/cpu_extension.cmake

[0adba9154] Kevin H. Luu 2025-04-05 [CI] Fix benchmark script level (#16089)
2	2	.buildkite/scripts/run-benchmarks.sh

[4285e423a] Tristan Leclercq 2025-04-05 [Misc] Auto detect bitsandbytes pre-quantized models (#16027)
6	3	docs/source/features/quantization/bnb.md
6	4	tests/quantization/test_bitsandbytes.py
4	0	vllm/engine/arg_utils.py

[63375f0cd] Woosuk Kwon 2025-04-04 [V1][Spec Decode] Update N-gram Proposer Interface (#15750)
15	13	vllm/v1/spec_decode/ngram_proposer.py
1	5	vllm/v1/worker/gpu_model_runner.py

[70ad3f9e9] Michael Goin 2025-04-04 [Bugfix][TPU] Fix V1 TPU worker for sliding window (#16059)
4	3	vllm/v1/worker/tpu_worker.py

[d6fc629f4] bnellnm 2025-04-04 [Kernel][Minor] Re-fuse triton moe weight application (#16071)
18	24	vllm/model_executor/layers/fused_moe/fused_moe.py

[af51d80fa] Roger Wang 2025-04-04 Revert "[V1] Scatter and gather placeholders in the model runner" (#16075)
8	8	docs/source/contributing/model/multimodal.md
3	0	docs/source/models/supported_models.md
1	1	examples/offline_inference/audio_language.py
1	4	tests/models/decoder_only/audio_language/test_ultravox.py
1	1	tests/models/decoder_only/vision_language/test_models.py
16	10	tests/models/decoder_only/vision_language/test_pixtral.py
2	2	tests/models/multimodal/processing/test_llava_next.py
2	2	tests/models/multimodal/processing/test_llava_onevision.py
1	3	tests/models/registry.py
0	9	tests/multimodal/test_processing.py
29	17	tests/v1/core/test_kv_cache_utils.py
50	21	vllm/model_executor/models/aya_vision.py
3	3	vllm/model_executor/models/chameleon.py
52	33	vllm/model_executor/models/fuyu.py
63	20	vllm/model_executor/models/gemma3_mm.py
1	1	vllm/model_executor/models/h2ovl.py
67	15	vllm/model_executor/models/idefics3.py
40	3	vllm/model_executor/models/internvl.py
49	6	vllm/model_executor/models/llava.py
65	9	vllm/model_executor/models/minicpmo.py
101	41	vllm/model_executor/models/minicpmv.py
44	6	vllm/model_executor/models/mistral3.py
61	13	vllm/model_executor/models/molmo.py
30	2	vllm/model_executor/models/nvlm_d.py
3	3	vllm/model_executor/models/paligemma.py
8	3	vllm/model_executor/models/phi3v.py
40	8	vllm/model_executor/models/pixtral.py
3	3	vllm/model_executor/models/qwen2_audio.py
3	3	vllm/model_executor/models/qwen_vl.py
39	3	vllm/model_executor/models/skyworkr1v.py
76	1	vllm/model_executor/models/vision.py
2	2	vllm/multimodal/base.py
3	29	vllm/multimodal/inputs.py
25	52	vllm/multimodal/processing.py
1	1	vllm/multimodal/profiling.py
1	1	vllm/multimodal/utils.py
4	3	vllm/v1/core/kv_cache_utils.py
4	4	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/request.py
20	40	vllm/v1/worker/gpu_model_runner.py
20	65	vllm/v1/worker/tpu_model_runner.py
0	45	vllm/v1/worker/utils.py

[f5722a505] Cyrus Leung 2025-04-05 [V1] Scatter and gather placeholders in the model runner (#15712)
8	8	docs/source/contributing/model/multimodal.md
0	3	docs/source/models/supported_models.md
1	1	examples/offline_inference/audio_language.py
4	1	tests/models/decoder_only/audio_language/test_ultravox.py
1	1	tests/models/decoder_only/vision_language/test_models.py
10	16	tests/models/decoder_only/vision_language/test_pixtral.py
2	2	tests/models/multimodal/processing/test_llava_next.py
2	2	tests/models/multimodal/processing/test_llava_onevision.py
3	1	tests/models/registry.py
9	0	tests/multimodal/test_processing.py
17	29	tests/v1/core/test_kv_cache_utils.py
21	50	vllm/model_executor/models/aya_vision.py
3	3	vllm/model_executor/models/chameleon.py
33	52	vllm/model_executor/models/fuyu.py
20	63	vllm/model_executor/models/gemma3_mm.py
1	1	vllm/model_executor/models/h2ovl.py
15	67	vllm/model_executor/models/idefics3.py
3	40	vllm/model_executor/models/internvl.py
6	49	vllm/model_executor/models/llava.py
9	65	vllm/model_executor/models/minicpmo.py
41	101	vllm/model_executor/models/minicpmv.py
6	44	vllm/model_executor/models/mistral3.py
13	61	vllm/model_executor/models/molmo.py
2	30	vllm/model_executor/models/nvlm_d.py
3	3	vllm/model_executor/models/paligemma.py
3	8	vllm/model_executor/models/phi3v.py
8	40	vllm/model_executor/models/pixtral.py
3	3	vllm/model_executor/models/qwen2_audio.py
3	3	vllm/model_executor/models/qwen_vl.py
3	39	vllm/model_executor/models/skyworkr1v.py
1	76	vllm/model_executor/models/vision.py
2	2	vllm/multimodal/base.py
29	3	vllm/multimodal/inputs.py
52	25	vllm/multimodal/processing.py
1	1	vllm/multimodal/profiling.py
1	1	vllm/multimodal/utils.py
3	4	vllm/v1/core/kv_cache_utils.py
4	4	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/request.py
40	20	vllm/v1/worker/gpu_model_runner.py
65	20	vllm/v1/worker/tpu_model_runner.py
45	0	vllm/v1/worker/utils.py

[651cf0fec] Nick Hill 2025-04-04 [V1] DP scale-out (1/N): Use zmq ROUTER/DEALER sockets for input queue (#15906)
25	10	vllm/utils.py
16	12	vllm/v1/engine/core.py
71	37	vllm/v1/engine/core_client.py
1	10	vllm/v1/utils.py

[4dc52e1c5] Kevin H. Luu 2025-04-04 [CI] Reorganize .buildkite directory (#16001)
3	3	.buildkite/release-pipeline.yaml
0	0	.buildkite/{ => scripts/hardware_ci}/run-amd-test.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-cpu-test-ppc64le.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-cpu-test.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-gh200-test.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-hpu-test.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-neuron-test.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-tpu-v1-test.sh
0	0	.buildkite/{ => scripts/hardware_ci}/run-xpu-test.sh
0	0	.buildkite/{ => scripts}/run-benchmarks.sh
1	1	.buildkite/{ => scripts}/run-multi-node-test.sh
0	0	.buildkite/{ => scripts}/upload-wheels.sh
1	1	.buildkite/test-pipeline.yaml
2	2	tools/shellcheck.sh

[4708f13a9] Michael Goin 2025-04-04 [Bugfix] Fix default behavior/fallback for pp in v1 (#16057)
3	2	vllm/engine/arg_utils.py

[a6d042df0] Gregory Shtrasberg 2025-04-04 [ROCm][Bugfix] Bring back fallback to eager mode removed in #14917, but for ROCm only (#15413)
8	1	vllm/config.py

[40a36ccfe] Gregory Shtrasberg 2025-04-04 [ROCm][Bugfix] Use platform specific FP8 dtype (#15717)
1	1	vllm/attention/ops/prefix_prefill.py

[ef608c37a] Ilya Markov 2025-04-04 [Distributed] [ROCM] Fix custom allreduce enable checks (#16010)
3	4	vllm/config.py
4	0	vllm/platforms/cuda.py
7	0	vllm/platforms/interface.py
7	0	vllm/platforms/rocm.py

[2386803f2] Li, Jiang 2025-04-05 [CPU] Change default block_size for CPU backend (#16002)
9	1	vllm/platforms/cpu.py

[95862f7b4] Ziji Shi (Steven) 2025-04-04 [Benchmark][Doc] Update throughput benchmark and README (#15998)
29	0	benchmarks/README.md
15	11	benchmarks/benchmark_throughput.py

[230b131b5] Isotr0py 2025-04-05 [Bugfix][kernels] Fix half2float conversion in gguf kernels (#15995)
5	0	csrc/quantization/gguf/ggml-common.h

[0812d8dd4] liuzhenwei 2025-04-05 [Hardware][Gaudi][BugFix] fix arguments of hpu fused moe (#15945)
5	2	vllm/model_executor/layers/fused_moe/layer.py

[bf7e3c51a] Jonghyun Choe 2025-04-05 [Model] use AutoWeightsLoader for baichuan, gpt-neox, mpt (#15939)
57	48	vllm/model_executor/models/baichuan.py
42	37	vllm/model_executor/models/gpt_neox.py
20	15	vllm/model_executor/models/mpt.py

[a35a8a839] Mark McLoughlin 2025-04-04 [V1][Spec Decode] Avoid logging useless nan metrics (#16023)
9	8	tests/v1/core/test_scheduler.py
19	6	vllm/v1/core/sched/scheduler.py

[4ef0bb1fc] yihong 2025-04-04 doc: add info for macos clang errors (#16049)
9	1	docs/source/getting_started/installation/cpu/apple.inc.md

[fadc59c0e] Chengji Yao 2025-04-04 [TPU][V1] Remove ragged attention kernel parameter hard coding (#16041)
6	14	vllm/v1/attention/backends/pallas.py
2	6	vllm/v1/worker/tpu_model_runner.py

[86cbd2eee] Reid 2025-04-04 [Misc] improve gguf check (#15974)
8	3	vllm/transformers_utils/utils.py

[092475f73] Huy Do 2025-04-03 [ROCm] Tweak the benchmark script to run on ROCm (#14252)
21	6	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh

[dcc56d62d] bnellnm 2025-04-03 [Bugfix] Fix function names in test_block_fp8.py (#16033)
5	5	tests/kernels/test_block_fp8.py

[f15e70d90] Robert Shaw 2025-04-03 [TPU] Switch Test to Non-Sliding Window (#15981)
1	1	tests/tpu/test_compilation.py

[b6be6f8d1] iefgnoix 2025-04-03 [TPU] Support sliding window and logit soft capping in the paged attention kernel for TPU. (#15732)
4	2	.buildkite/run-tpu-v1-test.sh
20	10	tests/entrypoints/llm/test_accuracy.py
98	0	tests/v1/tpu/test_pallas.py
6	6	vllm/v1/attention/backends/pallas.py

[03a70eaca] Alexei-V-Ivanov-AMD 2025-04-03 Re-enable the AMD Testing for the passing tests. (#15586)
17	3	.buildkite/run-amd-test.sh
9	6	.buildkite/test-pipeline.yaml

[45b1ff7a2] yarongmu-google 2025-04-03 [Misc][Performance] Advance tpu.txt to the most recent nightly torch … (#16024)
7	6	requirements/tpu.txt

[15ba07ef2] bnellnm 2025-04-03 [Minor] Fused experts refactor (#15914)
6	3	tests/kernels/test_block_fp8.py
8	8	tests/kernels/test_cutlass_moe.py
4	2	vllm/model_executor/layers/fused_moe/__init__.py
144	0	vllm/model_executor/layers/fused_moe/cutlass_moe.py
294	0	vllm/model_executor/layers/fused_moe/deep_gemm_moe.py
43	724	vllm/model_executor/layers/fused_moe/fused_moe.py
243	0	vllm/model_executor/layers/fused_moe/moe_align_block_size.py
48	0	vllm/model_executor/layers/fused_moe/utils.py

[d2b58ca20] Liangfu Chen 2025-04-03 [Neuron][kernel] Fuse kv cache into a single tensor (#15911)
3	1	tests/neuron/1_core/test_cache.py
5	8	tests/neuron/1_core/test_prefix_prefill.py
38	47	vllm/attention/ops/nki_flash_attn.py

[82e7e19a6] Kyle Sayers 2025-04-03 [SupportsQuant] Chameleon, Chatglm, Commandr (#15952)
7	2	vllm/model_executor/models/chameleon.py
8	4	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/commandr.py

[421c46294] Kyle Sayers 2025-04-03 [SupportsQuant] Bert, Blip, Blip2, Bloom (#15573)
6	4	vllm/model_executor/models/bert.py
4	1	vllm/model_executor/models/blip.py
4	2	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/bloom.py

[84884cd9a] yihong 2025-04-03 fix: tiny fix make format.sh excutable (#16015)
0	0	format.sh

[a43aa183d] Reid 2025-04-03 [doc] update contribution link (#15922)
1	1	README.md

[463bbb183] wwl2755 2025-04-03 [Bugfix][V1] Fix bug from putting llm_engine.model_executor in a background process (#15367)
93	0	examples/offline_inference/load_sharded_state.py
19	4	examples/offline_inference/save_sharded_state.py
10	0	vllm/v1/engine/core.py
31	0	vllm/v1/engine/core_client.py
14	0	vllm/v1/worker/gpu_worker.py

[5e125e74d] youkaichao 2025-04-03 [misc] improve error message for "Failed to infer device type" (#15994)
4	1	vllm/config.py

[06f21ce7a] Ziji Shi (Steven) 2025-04-02 [Benchmark] Add AIMO Dataset to Benchmark (#15955)
49	0	benchmarks/benchmark_dataset.py
9	5	benchmarks/benchmark_serving.py

[57a810db9] Aleksandr Malyshev 2025-04-02 [ROCM][V0] PA kennel selection when no sliding window provided (#15982)
2	1	vllm/platforms/rocm.py

[8b664706a] youkaichao 2025-04-03 [bugfix] add seed in torchrun_example.py (#15980)
4	0	examples/offline_inference/torchrun_example.py
6	0	vllm/config.py

[37bfee92b] yihong 2025-04-03 fix: better error message for get_config close #13889 (#15943)
13	1	vllm/transformers_utils/config.py

[e73ff24e3] Aleksandr Malyshev 2025-04-02 [ROCM][KERNEL] Paged attention for V1 (#15720)
73	32	csrc/rocm/attention.cu
3	2	csrc/rocm/ops.h
3	1	csrc/rocm/torch_bindings.cpp
4	0	tests/kernels/test_prefix_prefill.py
4	2	vllm/_custom_ops.py
6	18	vllm/attention/backends/rocm_flash_attn.py
103	54	vllm/attention/ops/chunked_prefill_paged_decode.py
2	0	vllm/attention/ops/paged_attn.py
1	0	vllm/attention/ops/prefix_prefill.py
20	1	vllm/platforms/rocm.py
1	0	vllm/v1/attention/backends/triton_attn.py

[bd7599d34] Nicolò Lucchesi 2025-04-03 [V1][TPU] Do not compile sampling more than needed (#15883)
3	1	vllm/v1/worker/tpu_model_runner.py

[01b611365] Chengji Yao 2025-04-02 [TPU] optimize the all-reduce performance (#15903)
6	1	vllm/distributed/device_communicators/tpu_communicator.py
4	1	vllm/distributed/parallel_state.py
6	0	vllm/v1/worker/tpu_worker.py

[1b84eff03] Hyesoo Yang 2025-04-02 [V1][TPU] TPU-optimized top-p implementation (avoids scattering). (#15736)
3	1	.buildkite/run-tpu-v1-test.sh
132	0	tests/v1/tpu/test_topk_topp_sampler.py
39	14	vllm/v1/sample/ops/topk_topp_sampler.py

[55acf86bf] Harry Mellor 2025-04-03 Fix `huggingface-cli[hf-xet]` -> `huggingface-cli[hf_xet]` (#15969)
1	1	requirements/common.txt
1	1	requirements/test.in

[f021b9799] Michael Goin 2025-04-02 [V1] Support Mistral3 in V1 (#15950)
1	1	docs/source/models/supported_models.md
9	6	vllm/model_executor/models/mistral3.py

[1cab43c2d] youkaichao 2025-04-03 [misc] instruct pytorch to use nvml-based cuda check (#15951)
4	16	vllm/__init__.py
21	0	vllm/env_override.py

[8bd651b31] Nishidha 2025-04-02 Restricted cmake to be less than version 4 as 4.x breaks the build of… (#15859)
2	2	docker/Dockerfile.ppc64le

[58e234a75] Jee Jee Li 2025-04-02 [Misc] V1 LoRA support CPU offload (#15843)
3	3	vllm/config.py

[e86c414d6] rongfu.leng 2025-04-02 [Model] use AutoWeightsLoader in model load_weights (#15770)
5	0	docs/source/models/supported_models.md
55	50	vllm/model_executor/models/bamba.py
74	66	vllm/model_executor/models/exaone.py
55	49	vllm/model_executor/models/falcon.py

[550b2801a] Li, Jiang 2025-04-02 [CPU][Bugfix] Using custom allreduce for CPU backend (#15934)
1	0	cmake/cpu_extension.cmake
58	0	csrc/cpu/cpu_types_x86.hpp
781	0	csrc/cpu/shm.cpp
43	0	csrc/cpu/torch_bindings.cpp
1	1	csrc/cpu/utils.cpp
5	3	docs/source/getting_started/installation/cpu.md
117	12	vllm/distributed/device_communicators/cpu_communicator.py
7	0	vllm/worker/cpu_worker.py

[cefb9e5a2] Matthias Matt 2025-04-02 [Frontend] Implement Tool Calling with `tool_choice='required'` (#13483)
7	1	docs/source/features/tool_calling.md
136	0	examples/online_serving/openai_chat_completion_client_with_tools_required.py
121	42	tests/entrypoints/openai/test_chat.py
4	3	tests/tool_use/test_chat_completion_request_validations.py
336	0	tests/tool_use/test_tool_choice_required.py
99	35	vllm/entrypoints/openai/protocol.py
165	12	vllm/entrypoints/openai/serving_chat.py

[98d7367b6] Mark McLoughlin 2025-04-02 [Metrics] Hide deprecated metrics (#15458)
17	4	tests/entrypoints/openai/test_metrics.py
3	0	tests/utils.py
85	78	vllm/engine/metrics.py
9	0	vllm/version.py

[594a8b903] Chauncey 2025-04-02 [Bugfix] Fix the issue where the model name is empty string, causing no response with the model name. (#15938)
28	3	tests/entrypoints/openai/test_chat.py
1	1	vllm/entrypoints/openai/serving_engine.py

[44f990515] Kay Yan 2025-04-02 [CI] Remove duplicate entrypoints-test (#15940)
0	1	.buildkite/test-pipeline.yaml

[252937806] Brayden Zhong 2025-04-02 [Bugfix][Benchmarks] Ensure `async_request_deepspeed_mii` uses the OpenAI choices key (#15926)
9	1	benchmarks/backend_request_func.py

[51826d51f] Harry Mellor 2025-04-02 Add minimum version for `huggingface_hub` to enable Xet downloads (#15873)
1	0	requirements/common.txt
1	0	requirements/test.in
4	1	requirements/test.txt

[14e53ed11] Russell Bryant 2025-04-02 [V1] Fix json_object support with xgrammar (#15488)
1	1	requirements/common.txt
2	10	tests/v1/entrypoints/llm/test_struct_output_generate.py
0	6	vllm/model_executor/guided_decoding/__init__.py
4	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py
3	1	vllm/v1/structured_output/backend_xgrammar.py

[ddb94c260] Eric Tang 2025-04-02 [core] Add tags parameter to wake_up() (#15500)
19	1	tests/basic_correctness/test_cumem.py
25	4	tests/entrypoints/openai/test_sleep.py
19	13	vllm/device_allocator/cumem.py
2	2	vllm/engine/async_llm_engine.py
2	2	vllm/engine/llm_engine.py
3	2	vllm/engine/multiprocessing/__init__.py
2	2	vllm/engine/multiprocessing/client.py
3	3	vllm/engine/multiprocessing/engine.py
1	1	vllm/engine/protocol.py
23	14	vllm/entrypoints/llm.py
6	3	vllm/entrypoints/openai/api_server.py
20	5	vllm/executor/executor_base.py
2	2	vllm/v1/engine/async_llm.py
2	2	vllm/v1/engine/core.py
8	8	vllm/v1/engine/core_client.py
2	2	vllm/v1/engine/llm_engine.py
2	2	vllm/v1/worker/gpu_worker.py
2	2	vllm/worker/worker.py

[90969fb39] LukasBluebaum 2025-04-02 [Kernel] Add more dtype support for GGUF dequantization (#15879)
2	1	csrc/ops.h
34	31	csrc/quantization/gguf/dequantize.cuh
16	1	csrc/quantization/gguf/ggml-common.h
10	5	csrc/quantization/gguf/gguf_kernel.cu
3	1	csrc/torch_bindings.cpp
2	1	tests/kernels/test_ggml.py
2	2	tests/kernels/test_gguf.py
9	6	vllm/_custom_ops.py
2	2	vllm/model_executor/layers/quantization/gguf.py

[101f1481f] Chris Thi 2025-04-02 [Build/CI] Update lm-eval to 0.4.8 (#15912)
1	1	requirements/test.in
1	1	requirements/test.txt

[2edc87b16] Thien Tran 2025-04-02 [Bugfix] Fix cache block size calculation for CPU MLA (#15848)
1	1	vllm/worker/cpu_worker.py

[4203926f1] Jee Jee Li 2025-04-02 [CI/Build] Further clean up LoRA tests (#15920)
1	3	.buildkite/test-pipeline.yaml
0	23	tests/lora/conftest.py
1	1	tests/lora/test_layers.py
0	17	tests/lora/test_llama_tp.py
0	1	tests/lora/test_minicpmv_tp.py
7	1	tests/lora/test_transfomers_model.py

[cdb57015a] Chauncey 2025-04-02 [Misc] Replace print with logger (#15923)
7	5	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py

[aa557e642] Li Wang 2025-04-02 [Benchmark]Fix error message (#15866)
0	9	benchmarks/benchmark_dataset.py
14	3	benchmarks/benchmark_serving.py

[0e00d40e4] Roger Wang 2025-04-01 [V1][Bugfix] Fix typo in MoE TPU checking (#15927)
1	1	vllm/model_executor/layers/fused_moe/layer.py

[c920e0124] chun 2025-04-02 [Doc] Update rocm.inc.md (#15917)
1	1	docs/source/getting_started/installation/gpu/rocm.inc.md

[274d8e881] Woosuk Kwon 2025-04-01 [V1][Minor] Enhance SpecDecoding Metrics Log in V1 (#15902)
10	7	vllm/v1/spec_decode/metrics.py

[2039c6305] Thien Tran 2025-04-02 [Bugfix] Fix imports for MoE on CPU (#15841)
3	2	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py

[6efb195a6] Brayden Zhong 2025-04-01 [V1] Fix: make sure `k_index` is int64 for `apply_top_k_only` (#15907)
1	1	vllm/v1/sample/ops/topk_topp_sampler.py

[24b7fb455] Ekagra Ranjan 2025-04-01 [Spec Decode] Fix input triton kernel for eagle (#15909)
1	2	vllm/v1/spec_decode/eagle.py

[58f5a5976] Simon Mo 2025-04-01 [Docs] Add Intel as Sponsor (#15913)
1	0	README.md
1	0	docs/source/community/sponsors.md

[db9dfcfa6] Simon Mo 2025-04-01 [Docs] Add Ollama meetup slides (#15905)
1	3	README.md
2	0	docs/source/community/meetups.md

[9ef98d527] Gerald 2025-04-02 [Model][MiniMaxText01] Support MiniMaxText01 model inference (#13454)
5	0	docs/source/models/supported_models.md
286	0	tests/kernels/test_lightning_attn.py
2	0	tests/models/registry.py
25	17	vllm/config.py
5	2	vllm/engine/async_llm_engine.py
651	0	vllm/model_executor/layers/lightning_attn.py
136	0	vllm/model_executor/models/constant_size_cache.py
21	111	vllm/model_executor/models/mamba_cache.py
35	0	vllm/model_executor/models/minimax_cache.py
1273	0	vllm/model_executor/models/minimax_text_01.py
1	0	vllm/model_executor/models/registry.py

[93491aefc] yihong 2025-04-02 [BugFix] make sure socket close (#15875)
4	3	vllm/entrypoints/openai/api_server.py

[7acd539cd] Simon Mo 2025-04-01 [Docs] update usage stats language (#15898)
3	1	docs/source/serving/usage_stats.md

[e75a6301b] Woosuk Kwon 2025-04-01 [V1][Spec Decode] Implement Eagle Proposer [1/N] (#15729)
3	2	vllm/config.py
16	6	vllm/engine/arg_utils.py
262	0	vllm/v1/spec_decode/eagle.py
9	0	vllm/v1/spec_decode/ngram_proposer.py
10	1	vllm/v1/worker/gpu_input_batch.py
78	12	vllm/v1/worker/gpu_model_runner.py

[a79cc68b3] Mark McLoughlin 2025-04-01 [V1][Metrics] Initial speculative decoding metrics (#15151)
95	0	tests/v1/core/test_scheduler.py
13	2	vllm/v1/core/sched/scheduler.py
33	0	vllm/v1/metrics/loggers.py
4	0	vllm/v1/metrics/stats.py
59	0	vllm/v1/spec_decode/metrics.py

[7e3f7a4ee] Roger Wang 2025-04-01 [CI] Disable flaky structure decoding test temporarily. (#15892)
2	1	tests/v1/entrypoints/llm/test_struct_output_generate.py

[9ec825791] cloud11665 2025-04-02 [Model] Add module name prefixes to gemma3 (#15889)
17	6	vllm/model_executor/models/gemma3.py

[38327cf45] Jennifer Zhao 2025-04-01 [Model] Aya Vision (#15441)
7	0	docs/source/models/supported_models.md
23	0	examples/offline_inference/vision_language.py
36	0	examples/offline_inference/vision_language_multi_image.py
14	0	tests/models/decoder_only/vision_language/test_models.py
1	0	tests/models/multimodal/processing/test_common.py
1	0	tests/models/registry.py
4	0	vllm/config.py
3	2	vllm/entrypoints/chat_utils.py
527	0	vllm/model_executor/models/aya_vision.py
1	0	vllm/model_executor/models/registry.py

[dfa82e2a3] Jee Jee Li 2025-04-02 [CI/Build] Clean up LoRA tests (#15867)
0	8	tests/lora/test_baichuan.py
0	65	tests/lora/test_gemma.py
23	117	tests/lora/test_layers.py
4	0	tests/lora/test_minicpmv_tp.py
0	10	tests/lora/test_transfomers_model.py

[e59ca942f] bnellnm 2025-04-01 Add option to use DeepGemm contiguous grouped gemm kernel for fused MoE operations. (#13932)
61	36	benchmarks/kernels/benchmark_moe.py
256	25	tests/kernels/test_block_fp8.py
1	1	vllm/_custom_ops.py
5	0	vllm/envs.py
415	53	vllm/model_executor/layers/fused_moe/fused_moe.py
36	0	vllm/model_executor/layers/quantization/fp8.py

[a57a3044a] Gregory Shtrasberg 2025-04-01 [ROCm][Build][Bugfix] Bring the base dockerfile in sync with the ROCm fork (#15820)
30	22	docker/Dockerfile.rocm_base
1	1	requirements/rocm-build.txt

[4e5a0f6ae] Isotr0py 2025-04-01 [Misc] Allow using OpenCV as video IO fallback (#15055)
1	0	requirements/common.txt
1	1	requirements/test.in
4	5	requirements/test.txt
1	1	setup.py
11	8	vllm/assets/video.py
66	20	vllm/multimodal/video.py

[b63bd1499] Harry Mellor 2025-04-01 Reinstate `format.sh` and make `pre-commit` installation simpler (#15890)
3	0	.pre-commit-config.yaml
6	0	format.sh

[2041c0e36] chaow-amd 2025-04-01 [Doc] Quark quantization documentation (#15861)
1	0	docs/source/features/quantization/index.md
217	0	docs/source/features/quantization/quark.md

[085cbc4f9] wang.yuqi 2025-04-01 [New Model]: jinaai/jina-reranker-v2-base-multilingual  (#15876)
1	1	docs/source/models/supported_models.md
70	0	tests/models/embedding/language/test_jina_reranker_v2.py
15	2	vllm/model_executor/models/roberta.py

[2b93162fb] Harry Mellor 2025-04-01 Remove `format.sh` as it's been unsupported >70 days (#15884)
0	6	format.sh

[2e45bd29f] Reid 2025-04-01 [Misc] remove unused script (#15746)
0	16	python_only_dev.py

[51d7c6a2b] Michael Goin 2025-04-01 [Model] Support Mistral3 in the HF Transformers format (#15505)
7	0	docs/source/models/supported_models.md
24	0	examples/offline_inference/vision_language.py
23	0	examples/offline_inference/vision_language_multi_image.py
3	0	tests/models/registry.py
2	1	vllm/entrypoints/chat_utils.py
656	0	vllm/model_executor/models/mistral3.py
4	3	vllm/model_executor/models/pixtral.py
1	0	vllm/model_executor/models/registry.py
3	0	vllm/model_executor/models/vision.py

[f3aca1ee3] Yang Chen 2025-04-01 setup correct nvcc version with CUDA_HOME (#15725)
5	3	setup.py

[8dd41d6bc] Rui Qiao 2025-04-01 [Misc] Use envs.VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE (#15831)
13	7	vllm/envs.py
17	10	vllm/executor/ray_distributed_executor.py

[0a298ea41] Isotr0py 2025-04-01 [Bugfix] Fix no video/image profiling edge case for `MultiModalDataParser` (#15828)
27	6	vllm/multimodal/parse.py

[d330558ba] Harry Mellor 2025-04-01 [Docs] Fix small error in link text (#15868)
1	1	docs/source/models/supported_models.md

[656fd7297] shangmingc 2025-04-01 [Misc] Fix speculative config repr string (#15860)
3	5	vllm/config.py

[79455cf42] Varun Sundar Rabindranath 2025-04-01 [Misc] Enable V1 LoRA by default (#15320)
57	3	tests/entrypoints/openai/test_chat.py
8	8	tests/lora/test_baichuan.py
8	8	tests/lora/test_chatglm3_tp.py
8	8	tests/lora/test_gemma.py
0	5	tests/lora/test_layers.py
8	12	tests/lora/test_llama_tp.py
11	3	tests/lora/test_lora_manager.py
8	8	tests/lora/test_phi.py
8	8	tests/lora/test_quant_model.py
8	11	tests/lora/test_transfomers_model.py
1	9	tests/v1/test_oracle.py
0	4	vllm/engine/arg_utils.py

[30d6a015e] Wei Zeng 2025-04-01 [Feature] specify model in config.yaml (#15798)
3	1	docs/source/serving/openai_compatible_server.md
0	0	tests/{data => config}/test_config.yaml
7	0	tests/config/test_config_with_model.yaml
12	0	tests/conftest.py
53	11	tests/test_utils.py
7	11	vllm/entrypoints/cli/serve.py
27	9	vllm/utils.py

[8af5a5c4e] yihong 2025-04-01 fix: can not use uv run collect_env close #13888 (#15792)
18	6	collect_env.py

[3a5f0afcd] Chen Zhang 2025-04-01 [V1] Implement sliding window attention in kv_cache_manager (#14097)
11	4	tests/core/block/e2e/test_correctness_sliding_window.py
63	70	tests/v1/core/test_prefix_caching.py
12	0	tests/v1/core/test_scheduler.py
138	0	tests/v1/core/test_specialized_manager.py
84	0	tests/v1/e2e/test_correctness_sliding_window.py
1	2	vllm/config.py
11	4	vllm/v1/core/block_pool.py
50	21	vllm/v1/core/kv_cache_manager.py
32	3	vllm/v1/core/kv_cache_utils.py
5	17	vllm/v1/core/sched/scheduler.py
161	0	vllm/v1/core/specialized_manager.py
11	7	vllm/v1/engine/core.py
46	11	vllm/v1/kv_cache_interface.py
19	9	vllm/v1/worker/gpu_model_runner.py
18	10	vllm/v1/worker/tpu_model_runner.py

[c7e63aa4d] Gregory Shtrasberg 2025-04-01 [ROCm] Use device name in the warning (#15838)
1	1	vllm/engine/arg_utils.py

[4a9ce1784] Lionel Villard 2025-04-01 [sleep mode] clear pytorch cache after sleep (#15248)
7	3	vllm/device_allocator/cumem.py

[7e4e709b4] Alexander Matveev 2025-04-01 [V1] TPU - Fix fused MOE (#15834)
1	1	vllm/model_executor/layers/fused_moe/layer.py

[63d8eabed] Alexey Kiryushin 2025-04-01 [Bugfix]: Fix is_embedding_layer condition in VocabParallelEmbedding  (#15824)
1	1	vllm/model_executor/layers/vocab_parallel_embedding.py

[e830b0138] Percy 2025-04-01 [Bugfix] Fix extra comma (#15851)
1	1	vllm/model_executor/sampling_metadata.py

[ff6473980] Yan Ma 2025-04-01 [Bugfix][Model] fix mllama multi-image (#14883)
1	1	tests/models/encoder_decoder/vision_language/test_mllama.py
28	6	vllm/model_executor/models/mllama.py

[a164aea35] Kinfey 2025-04-01 [Frontend] Add Phi-4-mini function calling support (#14886)
60	0	examples/tool_chat_template_phi4_mini.jinja
2	1	vllm/entrypoints/openai/tool_parsers/__init__.py
108	0	vllm/entrypoints/openai/tool_parsers/phi4mini_tool_parser.py

[a76f547e1] Harry Mellor 2025-04-01 Rename fallback model and refactor supported models section (#15829)
1	1	docs/source/index.md
70	49	docs/source/models/supported_models.md
2	2	tests/models/registry.py
3	4	vllm/model_executor/model_loader/utils.py
3	3	vllm/model_executor/models/registry.py
1	1	vllm/model_executor/models/transformers.py

[b7b7676d6] Ilya Markov 2025-04-01 [Distributed] Add custom allreduce support for ROCM (#14125)
1	1	CMakeLists.txt
47	2	csrc/custom_all_reduce.cu
162	77	csrc/custom_all_reduce.cuh
45	13	csrc/custom_all_reduce_test.cu
6	3	csrc/ops.h
8	3	csrc/torch_bindings.cpp
1	1	tests/distributed/test_custom_all_reduce.py
10	1	tests/utils.py
14	2	vllm/_custom_ops.py
4	2	vllm/config.py
42	49	vllm/distributed/device_communicators/custom_all_reduce.py
3	3	vllm/platforms/cuda.py
30	3	vllm/platforms/rocm.py

[e6e3c55ef] Harry Mellor 2025-03-31 Move dockerfiles into their own directory (#14549)
6	6	.buildkite/release-pipeline.yaml
1	1	.buildkite/run-cpu-test-ppc64le.sh
2	2	.buildkite/run-cpu-test.sh
1	0	.buildkite/run-gh200-test.sh
1	1	.buildkite/run-hpu-test.sh
1	1	.buildkite/run-neuron-test.sh
1	1	.buildkite/run-tpu-v1-test.sh
1	1	.buildkite/run-xpu-test.sh
1	1	.github/mergify.yml
1	1	.github/workflows/lint-and-deploy.yaml
1	1	CMakeLists.txt
0	0	Dockerfile => docker/Dockerfile
0	0	Dockerfile.arm => docker/Dockerfile.arm
0	0	Dockerfile.cpu => docker/Dockerfile.cpu
0	0	Dockerfile.hpu => docker/Dockerfile.hpu
0	0	Dockerfile.neuron => docker/Dockerfile.neuron
0	0	Dockerfile.ppc64le => docker/Dockerfile.ppc64le
0	0	Dockerfile.rocm => docker/Dockerfile.rocm
0	0	Dockerfile.rocm_base => docker/Dockerfile.rocm_base
0	0	Dockerfile.s390x => docker/Dockerfile.s390x
0	0	Dockerfile.tpu => docker/Dockerfile.tpu
0	0	Dockerfile.xpu => docker/Dockerfile.xpu
3	3	docs/source/contributing/dockerfile/dockerfile.md
1	1	docs/source/contributing/overview.md
3	2	docs/source/deployment/docker.md
2	2	docs/source/deployment/nginx.md
1	1	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
1	1	docs/source/getting_started/installation/ai_accelerator/neuron.inc.md
2	2	docs/source/getting_started/installation/ai_accelerator/tpu.inc.md
3	3	docs/source/getting_started/installation/cpu.md
7	7	docs/source/getting_started/installation/gpu/rocm.inc.md
1	1	docs/source/getting_started/installation/gpu/xpu.inc.md
1	1	docs/source/getting_started/quickstart.md
2	2	vllm/config.py

[f98a4920f] Mark McLoughlin 2025-03-31 [V1][Core] Remove unused speculative config from scheduler (#15818)
0	1	tests/v1/core/test_scheduler.py
1	4	vllm/v1/core/sched/scheduler.py
0	1	vllm/v1/engine/core.py

[d4bfc23ef] Harry Mellor 2025-03-31 Fix Transformers backend compatibility check (#15290)
1	4	vllm/model_executor/model_loader/utils.py

[9a2160fa5] Alexander Matveev 2025-03-31 [V1] TPU CI - Add basic perf regression test (#15414)
2	0	.buildkite/run-tpu-v1-test.sh
1	1	tests/entrypoints/llm/test_accuracy.py
3	2	tests/v1/tpu/test_basic.py
146	0	tests/v1/tpu/test_perf.py
40	17	vllm/v1/worker/tpu_model_runner.py

[2de411824] yihong 2025-04-01 fix: change GB to GiB in logging close #14979 (#15807)
2	2	vllm/v1/core/kv_cache_utils.py
4	4	vllm/v1/worker/gpu_model_runner.py
2	2	vllm/worker/model_runner.py
3	3	vllm/worker/xpu_model_runner.py

[239b7befd] shangmingc 2025-04-01 [V1][Spec Decode] Remove deprecated spec decode config params (#15466)
6	4	.buildkite/nightly-benchmarks/tests/serving-tests.json
1	1	docs/source/features/spec_decode.md
6	4	examples/offline_inference/eagle.py
8	4	tests/metrics/test_metrics.py
4	2	tests/models/test_initialization.py
70	10	tests/spec_decode/e2e/test_integration_dist_tp2.py
4	2	tests/spec_decode/e2e/test_integration_dist_tp4.py
3	1	tests/v1/test_oracle.py
9	10	vllm/config.py
14	182	vllm/engine/arg_utils.py

[09e974d48] Cyrus Leung 2025-04-01 [Bugfix] Check dimensions of multimodal embeddings in V1 (#15816)
5	4	examples/offline_inference/vision_language.py
1	1	tests/distributed/test_pipeline_parallel.py
4	4	tests/models/decoder_only/vision_language/test_models.py
2	1	tests/models/registry.py
2	1	vllm/model_executor/models/florence2.py
14	9	vllm/model_executor/models/fuyu.py
4	2	vllm/model_executor/models/gemma3_mm.py
7	2	vllm/model_executor/models/idefics3.py
5	4	vllm/model_executor/models/llava_next_video.py
5	2	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/vision.py
12	6	vllm/v1/worker/gpu_model_runner.py
7	0	vllm/v1/worker/tpu_model_runner.py
29	0	vllm/v1/worker/utils.py

[e5ef4fa99] Harry Mellor 2025-03-31 Upgrade `transformers` to `v4.50.3` (#13905)
1	1	docs/source/models/supported_models.md
1	1	requirements/common.txt
1	1	requirements/test.in
1	1	requirements/test.txt
1	1	tests/distributed/test_pipeline_parallel.py
19	33	tests/models/decoder_only/vision_language/test_models.py
7	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
37	11	tests/models/registry.py

[037bcd942] Mrm 2025-03-31 [Bugfix] Fix missing return value in load_weights method of adapters.py (#15542)
5	4	vllm/model_executor/models/adapters.py

[c2e7507ad] Alex Brooks 2025-03-31 [Bugfix] Fix Crashing When Loading Modules With Batchnorm Stats (#15813)
79	0	tests/models/test_utils.py
24	0	vllm/model_executor/models/utils.py

[3aa2b6a63] Naveassaf 2025-03-31 [Model] Update support for NemotronNAS models (#15008)
1	1	docs/source/models/supported_models.md
1	1	tests/models/registry.py
20	1	vllm/config.py
0	124	vllm/model_executor/models/decilm.py
29	0	vllm/model_executor/models/interfaces.py
454	0	vllm/model_executor/models/nemotron_nas.py
15	5	vllm/model_executor/models/registry.py
4	1	vllm/model_executor/models/utils.py

[555aa2190] youkaichao 2025-03-31 [V1] Fully Transparent Implementation of CPU Offloading (#15354)
1	0	CMakeLists.txt
39	0	csrc/cuda_view.cu
2	0	csrc/ops.h
4	0	csrc/torch_bindings.cpp
0	7	tests/basic_correctness/test_cpu_offload.py
61	0	tests/kernels/test_uva.py
0	7	tests/quantization/test_cpu_offload.py
3	2	vllm/config.py
0	6	vllm/engine/arg_utils.py
18	3	vllm/model_executor/models/utils.py
16	0	vllm/utils.py
4	0	vllm/v1/worker/gpu_model_runner.py

[e7ae3bf3d] yihong 2025-03-31 fix: better install requirement for install in setup.py (#15796)
2	3	setup.py

[b932c048a] Harry Mellor 2025-03-31 Recommend developing with Python 3.12 in developer guide (#15811)
6	0	docs/source/contributing/overview.md

[e85829450] Charlie Fu 2025-03-31 [Feature][ROCm]Enable fusion pass for torch.compile on ROCm (#15050)
2	5	csrc/quantization/fp8/common.cu
5	36	csrc/quantization/fp8/common.cuh
10	18	csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
7	6	csrc/quantization/fused_kernels/layernorm_utils.cuh
2	2	csrc/quantization/fused_kernels/quant_conversions.cuh
59	0	csrc/quantization/utils.cuh
5	3	tests/compile/test_fusion.py
2	2	vllm/compilation/fusion.py

[effc5d24f] Jennifer Zhao 2025-03-31 [Benchmark] Update Vision Arena Dataset and HuggingFaceDataset Setup (#15748)
134	106	benchmarks/README.md
69	86	benchmarks/benchmark_dataset.py
9	8	benchmarks/benchmark_serving.py
15	15	benchmarks/benchmark_throughput.py

[18ed3132d] Chengyang LIU 2025-03-30 [Misc] update the comments (#15780)
1	1	vllm/v1/worker/gpu_model_runner.py

[9b459eca8] Woosuk Kwon 2025-03-30 [V1][Scheduler] Avoid calling `_try_schedule_encoder_inputs` for every request (#15778)
28	23	vllm/v1/core/sched/scheduler.py
2	7	vllm/v1/request.py

[70fedd0f7] yihong 2025-03-31 fix: Comments to English for better dev experience (#15768)
1	1	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/transformers_utils/processors/deepseek_vl2.py

[bb103b29b] kYLe 2025-03-30 [Bugfix] Added `embed_is_patch` mask for fuyu model (#15731)
36	5	vllm/model_executor/models/fuyu.py

[248e76c4d] yihong 2025-03-30 fix: lint fix a ruff checkout syntax error (#15767)
1	1	tests/models/registry.py

[803d5c35f] Cyrus Leung 2025-03-30 [V1] Override `mm_counts` for dummy data creation (#15703)
2	26	tests/models/decoder_only/vision_language/test_models.py
9	5	vllm/model_executor/models/llava_next_video.py
17	8	vllm/model_executor/models/llava_onevision.py
15	11	vllm/model_executor/models/minicpmo.py
24	12	vllm/model_executor/models/minicpmv.py
17	9	vllm/model_executor/models/qwen2_vl.py
21	9	vllm/multimodal/profiling.py
4	2	vllm/multimodal/registry.py
5	11	vllm/v1/worker/gpu_model_runner.py

[7fd8c0f85] pansicheng 2025-03-30 fix test_phi3v (#15321)
40	6	tests/entrypoints/openai/test_vision.py
21	3	tests/entrypoints/openai/test_vision_embedding.py
13	0	tests/models/embedding/vision_language/test_phi3v.py
36	5	vllm/model_executor/models/phi3v.py

[44c3a5abc] Reid 2025-03-30 [doc] update conda to usage link in installation (#15761)
1	1	docs/source/getting_started/installation/python_env_setup.inc.md

[6909a7620] Julien Denize 2025-03-30 [Bugfix] Fix Mistral guided generation using xgrammar (#15704)
23	10	tests/v1/entrypoints/llm/test_struct_output_generate.py
11	7	vllm/v1/structured_output/backend_xgrammar.py

[045533716] Chauncey 2025-03-30 [CI] xgrammar structured output supports Enum. (#15757)
4	4	tests/v1/structured_output/test_utils.py

[3c0ff914a] Isotr0py 2025-03-30 [Bugfix] Fix Mllama interleaved images input support (#15564)
2	2	examples/offline_inference/vision_language_multi_image.py
50	15	vllm/model_executor/models/mllama.py

[2bc4be4e3] Woosuk Kwon 2025-03-29 [V1][Minor] Simplify rejection sampler's parse_output (#15741)
0	7	vllm/v1/sample/rejection_sampler.py
3	4	vllm/v1/worker/gpu_model_runner.py

[c67abd614] Roger Wang 2025-03-29 [V1] Support interleaved modality items (#15605)
1	0	.buildkite/test-pipeline.yaml
19	22	tests/conftest.py
77	0	tests/models/decoder_only/vision_language/test_interleaved.py
64	16	tests/multimodal/test_utils.py
30	52	vllm/multimodal/utils.py
20	31	vllm/v1/engine/processor.py

[6fa7cd3db] shangmingc 2025-03-29 [Feature][Disaggregated] Support XpYd disaggregated prefill with MooncakeStore (#12957)
450	0	examples/online_serving/disagg_examples/disagg_proxy_demo.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
216	0	vllm/distributed/kv_transfer/kv_connector/mooncake_store_connector.py
75	10	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
160	0	vllm/distributed/kv_transfer/kv_lookup_buffer/mooncake_store.py

[94744ba41] wwl2755 2025-03-29 [V1] [Feature] Collective RPC (#15444)
3	3	.buildkite/test-pipeline.yaml
11	2	vllm/engine/llm_engine.py
2	2	vllm/entrypoints/llm.py
11	1	vllm/v1/engine/core.py
42	1	vllm/v1/engine/core_client.py
9	1	vllm/v1/engine/llm_engine.py
8	0	vllm/v1/serial_utils.py

[4965ec42d] TJian 2025-03-29 [FEAT] [ROCm] Add AITER int8 scaled gemm kernel (#15433)
72	4	tests/quantization/test_compressed_tensors.py
8	0	vllm/envs.py
3	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
119	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/aiter.py

[73aa7041b] Reid 2025-03-29 [doc] update doc (#15740)
26	3	docs/README.md

[7c1f76002] yarongmu-google 2025-03-28 [Kernel][TPU][ragged-paged-attn] vLLM code change for PR#8896 (#15659)
6	6	requirements/tpu.txt
22	21	vllm/v1/attention/backends/pallas.py
5	6	vllm/v1/worker/tpu_model_runner.py
4	4	vllm/v1/worker/tpu_worker.py

[da461f3cb] Nicolò Lucchesi 2025-03-29 [TPU][V1][Bugfix] Fix w8a8 recompiilation with GSM8K (#15714)
4	6	.buildkite/run-tpu-v1-test.sh
2	1	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
8	6	vllm/v1/worker/tpu_model_runner.py
2	2	vllm/v1/worker/tpu_worker.py

[5b800f093] Jinzhen Lin 2025-03-29 [Bugfix] set VLLM_WORKER_MULTIPROC_METHOD=spawn for vllm.entrypoionts.openai.api_server (#15700)
2	26	vllm/entrypoints/cli/main.py
3	1	vllm/entrypoints/openai/api_server.py
26	0	vllm/entrypoints/utils.py

[8427f7049] cyyever 2025-03-29 Use numba 0.61 for python 3.10+ to support numpy>=2 (#15692)
1	1	requirements/common.txt
2	1	requirements/cuda.txt
2	1	requirements/rocm.txt
3	1	requirements/test.in
5	3	requirements/test.txt

[7a7992085] Russell Bryant 2025-03-29 [CI] Speed up V1 structured output tests (#15718)
89	133	tests/v1/entrypoints/llm/test_struct_output_generate.py

[1286211f5] Varun Sundar Rabindranath 2025-03-28 [Bugfix] LoRA V1: add and fix entrypoints tests (#15715)
13	1	tests/entrypoints/llm/test_generate_multiple_loras.py
14	1	tests/entrypoints/openai/test_lora_adapters.py
1	1	vllm/entrypoints/openai/serving_models.py

[6d531ad7b] Nick Hill 2025-03-28 [Misc][V1] Misc code streamlining (#15723)
1	4	vllm/distributed/utils.py
25	30	vllm/v1/core/sched/scheduler.py
1	1	vllm/v1/engine/core_client.py
1	1	vllm/v1/engine/output_processor.py
5	3	vllm/v1/request.py

[762b424a5] Ce Gao 2025-03-29 [Docs] Document v0 engine support in reasoning outputs (#15739)
8	1	docs/source/features/reasoning_outputs.md

[de1cb3876] pengyuange 2025-03-29 [Model] Support Skywork-R1V (#15397)
7	0	docs/source/models/supported_models.md
36	0	examples/offline_inference/vision_language.py
14	0	tests/models/decoder_only/vision_language/test_models.py
57	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
5	4	tests/models/multimodal/processing/test_common.py
1	0	tests/models/registry.py
1	1	vllm/entrypoints/chat_utils.py
1	0	vllm/model_executor/models/registry.py
1014	0	vllm/model_executor/models/skyworkr1v.py
3	2	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
53	0	vllm/transformers_utils/configs/skyworkr1v.py

[c802f5430] Gregory Shtrasberg 2025-03-28 [ROCm][AMD][Build] Update AMD supported arch list (#15632)
1	1	CMakeLists.txt
1	1	docs/source/getting_started/installation/gpu/rocm.inc.md

[cff8991a5] simpx 2025-03-29 [Docs][V1] Optimize diagrams in prefix caching design (#15716)
-	-	docs/source/assets/design/v1/prefix_caching/example-time-1.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-3.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-4.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-5.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-6.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-7.png

[f3f8d8fff] daniel-salib 2025-03-28 implement prometheus fast-api-instrumentor for http service metrics (#15657)
11	0	vllm/entrypoints/openai/api_server.py

[26df46ee5] Reid 2025-03-29 [Misc] cli auto show default value (#15582)
1	3	vllm/benchmarks/serve.py
8	17	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/openai/cli_args.py
1	1	vllm/utils.py

[c3f687ac2] Alexander Matveev 2025-03-28 [V1] TPU - Fix the chunked prompt bug (#15713)
4	1	tests/v1/tpu/test_basic.py
13	0	vllm/v1/worker/tpu_model_runner.py

[04437e313] Luka Govedič 2025-03-28 [Bugfix] [torch.compile] Add Dynamo metrics context during compilation (#15639)
54	27	tests/compile/test_full_graph.py
36	2	vllm/compilation/compiler_interface.py

[038bededb] Robert Shaw 2025-03-28 [TPU] [Perf] Improve Memory Usage Estimation (#15671)
7	1	vllm/v1/worker/tpu_worker.py

[d03308be0] shangmingc 2025-03-29 [Misc] Remove stale func in KVTransferConfig (#14746)
0	6	vllm/config.py

[c6bc0034d] Cyrus Leung 2025-03-29 [Misc] Remove unused utils and clean up imports (#15708)
1	68	tests/multimodal/test_utils.py
0	119	vllm/multimodal/utils.py
1	2	vllm/v1/core/sched/output.py
3	6	vllm/v1/worker/gpu_input_batch.py

[70e132244] Woosuk Kwon 2025-03-28 [Minor] Remove TGI launching script  (#15646)
0	3	benchmarks/benchmark_serving.py
0	3	benchmarks/benchmark_serving_structured_output.py
0	16	benchmarks/launch_tgi_server.sh

[47e9038d2] Michael Goin 2025-03-28 Fix cpu offload testing for gptq/awq/ct (#15648)
9	3	tests/quantization/test_cpu_offload.py
33	0	tests/utils.py

[432cf22a6] Kebe 2025-03-28 [Bugfix] Fix regex compile display format (#15368)
8	6	vllm/transformers_utils/tokenizers/mistral.py

[2914006fe] Reid 2025-03-28 [doc] add missing imports (#15699)
6	0	docs/source/models/generative_models.md
8	0	docs/source/models/pooling_models.md
2	0	docs/source/performance/optimization.md
8	0	docs/source/serving/multimodal_inputs.md
6	0	docs/source/serving/offline_inference.md

[7329ff546] Russell Bryant 2025-03-28 [V1] Support disable_any_whtespace for guidance backend (#15584)
7	55	tests/entrypoints/llm/test_guided_generate.py
7	47	tests/v1/entrypoints/llm/test_struct_output_generate.py
2	1	vllm/engine/arg_utils.py
10	2	vllm/model_executor/guided_decoding/guidance_decoding.py
6	5	vllm/v1/engine/processor.py
12	7	vllm/v1/structured_output/backend_guidance.py

[541d1df48] Cyrus Leung 2025-03-28 [Bugfix] `embed_is_patch` for Idefics3 (#15696)
0	1	vllm/model_executor/models/commandr.py
318	183	vllm/model_executor/models/idefics3.py
0	1	vllm/model_executor/models/mllama.py
1	1	vllm/model_executor/models/qwen2_audio.py
1	2	vllm/model_executor/models/ultravox.py

[3b00ff913] Chauncey 2025-03-28 [Bugfix][v1] xgrammar structured output supports Enum. (#15594)
53	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
0	4	vllm/v1/structured_output/utils.py

[91276c572] Jee Jee Li 2025-03-28 [Model] Adding torch compile annotations to chatglm (#15624)
2	0	vllm/model_executor/models/chatglm.py

[0b4167526] Harry Mellor 2025-03-28 [Docs] Add "Generation quality changed" section to troubleshooting (#15701)
8	0	docs/source/getting_started/troubleshooting.md

[fd5fd2690] Reid 2025-03-28 [Frontend] update priority for --api-key and VLLM_API_KEY (#15588)
2	1	vllm/entrypoints/openai/api_server.py

[3bbaacbe1] Ce Gao 2025-03-28 [Bugfix][Frontend] Eliminate regex based check in reasoning full generator (#14821)
64	0	tests/reasoning/test_deepseekr1_reasoning_parser.py
25	18	vllm/reasoning/deepseek_r1_reasoning_parser.py

[a10314c6b] Lize Cai 2025-03-28 [Misc] Fix test_sleep to use query parameters (#14373)
2	1	tests/entrypoints/openai/test_sleep.py

[70f2c2a70] Jee Jee Li 2025-03-28 [Bugfix] Fix 'InductorAdaptor object has no attribute 'cache_dir' (#15674)
1	1	vllm/compilation/compiler_interface.py

[280d07410] Li, Jiang 2025-03-28 [CPU][CI] Improve CPU Dockerfile (#15690)
1	1	.buildkite/release-pipeline.yaml
9	7	.buildkite/run-cpu-test.sh
107	38	Dockerfile.cpu
27	8	docs/source/getting_started/installation/cpu.md
2	0	docs/source/getting_started/installation/cpu/x86.inc.md

[32b14baf8] Ce Gao 2025-03-28 [Refactor][Frontend] Keep all logic about reasoning into one class (#14428)
0	0	tests/{entrypoints/openai/reasoning_parsers => reasoning}/__init__.py
41	11	tests/{entrypoints/openai/reasoning_parsers => reasoning}/test_deepseekr1_reasoning_parser.py
2	4	tests/{entrypoints/openai/reasoning_parsers => reasoning}/test_granite_reasoning_parser.py
1	1	tests/{entrypoints/openai/reasoning_parsers => reasoning}/utils.py
2	1	vllm/engine/arg_utils.py
3	2	vllm/engine/llm_engine.py
1	1	vllm/entrypoints/openai/api_server.py
1	2	vllm/entrypoints/openai/serving_chat.py
11	4	vllm/model_executor/guided_decoding/__init__.py
4	4	vllm/model_executor/guided_decoding/outlines_decoding.py
7	7	vllm/model_executor/guided_decoding/outlines_logits_processors.py
0	38	vllm/model_executor/guided_decoding/reasoner/deepseek_reasoner.py
0	23	vllm/model_executor/guided_decoding/reasoner/reasoner.py
3	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py
0	0	vllm/{entrypoints/openai/reasoning_parsers => reasoning}/__init__.py
47	54	vllm/{entrypoints/openai/reasoning_parsers => reasoning}/abs_reasoning_parsers.py
47	43	vllm/{entrypoints/openai/reasoning_parsers => reasoning}/deepseek_r1_reasoning_parser.py
1	2	vllm/{entrypoints/openai/reasoning_parsers => reasoning}/granite_reasoning_parser.py

[2d9045fce] Robert Shaw 2025-03-28 [TPU][CI] Fix TPUModelRunner Test (#15667)
1	1	.buildkite/run-tpu-v1-test.sh
1	17	tests/v1/tpu/worker/test_tpu_model_runner.py

[355f66348] Cyrus Leung 2025-03-28 [V1] Remove legacy input registry (#15673)
1	6	tests/models/multimodal/processing/test_h2ovl.py
1	6	tests/models/multimodal/processing/test_idefics3.py
1	6	tests/models/multimodal/processing/test_internvl.py
3	13	tests/models/multimodal/processing/test_llava_next.py
3	13	tests/models/multimodal/processing/test_llava_onevision.py
1	6	tests/models/multimodal/processing/test_phi3v.py
2	6	tests/models/multimodal/processing/test_qwen2_vl.py
4	14	tests/multimodal/test_processing.py
6	6	vllm/inputs/preprocess.py
17	8	vllm/inputs/registry.py
24	31	vllm/multimodal/profiling.py
53	10	vllm/multimodal/registry.py
4	3	vllm/v1/engine/async_llm.py
1	3	vllm/v1/engine/llm_engine.py
10	12	vllm/v1/engine/processor.py
1	8	vllm/v1/worker/gpu_model_runner.py
0	2	vllm/v1/worker/tpu_model_runner.py

[8693e47e6] Cyrus Leung 2025-03-28 [Bugfix] Fix `mm_hashes` forgetting to be passed (#15668)
2	0	vllm/inputs/preprocess.py
2	0	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/mllama.py
8	8	vllm/model_executor/models/phi4mm.py
1	0	vllm/model_executor/models/prithvi_geospatial_mae.py
1	1	vllm/multimodal/inputs.py

[cec8c7d7f] Jason (Siyu) Zhu 2025-03-27 Refactor error handling for multiple exceptions in preprocessing (#15650)
2	10	vllm/entrypoints/openai/serving_chat.py
1	4	vllm/entrypoints/openai/serving_embedding.py
1	7	vllm/entrypoints/openai/serving_pooling.py
1	7	vllm/entrypoints/openai/serving_tokenization.py

[4d0ec3726] Gregory Shtrasberg 2025-03-27 [Quantization][FP8] Adding support for fp8 gemm layer input in fp8 (#14578)
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
2	0	vllm/model_executor/layers/quantization/fbgemm_fp8.py
17	0	vllm/model_executor/layers/quantization/fp8.py
2	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
18	9	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[e7f720ea5] Chen Xia 2025-03-27 [Misc]add coding benchmark for speculative decoding (#15303)
63	0	benchmarks/benchmark_dataset.py
11	5	benchmarks/benchmark_serving.py
27	16	benchmarks/benchmark_throughput.py

[4ae17bf1e] Wes 2025-03-27 Revert "Use Cache Hinting for fused_moe kernel (#15511)" (#15645)
4	12	vllm/model_executor/layers/fused_moe/fused_moe.py

[8a49eea74] Robert Shaw 2025-03-27 [CI][TPU] Temporarily Disable Quant Test on TPU (#15649)
5	4	.buildkite/run-tpu-v1-test.sh
0	3	tests/v1/tpu/test_basic.py

[b4245a48d] wwl2755 2025-03-27 [Doc] Fix dead links in Job Board (#15637)
1	1	docs/source/conf.py

[4e0f6076b] Kebe 2025-03-28 [Bugfix] Fix failure to launch in Tensor Parallel TP mode on macOS. (#14948)
2	2	docs/source/design/multiprocessing.md
7	2	vllm/distributed/device_communicators/shm_broadcast.py
8	0	vllm/platforms/cpu.py

[726efc6a3] Jee Jee Li 2025-03-28 [Quantization][V1]  BitsAndBytes support V1 (#15611)
0	1	tests/models/encoder_decoder/vision_language/test_mllama.py
0	1	tests/models/test_transformers.py
0	3	tests/quantization/test_bitsandbytes.py
4	2	vllm/config.py
1	1	vllm/engine/arg_utils.py
45	16	vllm/model_executor/layers/quantization/bitsandbytes.py
2	0	vllm/model_executor/model_loader/loader.py

[bd45912b9] Robert Shaw 2025-03-27 [TPU] Lazy Import (#15656)
2	1	vllm/distributed/utils.py

[15dac210f] Nick Hill 2025-03-27 [V1] AsyncLLM data parallel (#13923)
5	0	.buildkite/test-pipeline.yaml
15	7	examples/offline_inference/data_parallel.py
4	4	tests/v1/engine/test_engine_core_client.py
109	0	tests/v1/test_async_llm_dp.py
16	5	vllm/config.py
12	0	vllm/distributed/utils.py
10	0	vllm/engine/arg_utils.py
8	0	vllm/envs.py
9	5	vllm/utils.py
15	2	vllm/v1/core/sched/scheduler.py
8	1	vllm/v1/engine/__init__.py
16	7	vllm/v1/engine/async_llm.py
183	31	vllm/v1/engine/core.py
266	68	vllm/v1/engine/core_client.py
12	5	vllm/v1/engine/llm_engine.py
21	17	vllm/v1/executor/multiproc_executor.py
10	4	vllm/v1/metrics/loggers.py
7	4	vllm/v1/utils.py

[112b3e5b3] Russell Bryant 2025-03-27 [CI] Update rules for applying `tpu` label. (#15634)
20	1	.github/mergify.yml

[32d669275] cnorman 2025-03-27 Correct PowerPC to modern IBM Power (#15635)
1	1	docs/source/index.md

[4098b7221] Nicolò Lucchesi 2025-03-27 [Bugfix][TPU][V1] Fix recompilation (#15553)
3	1	.buildkite/run-tpu-v1-test.sh
5	64	tests/v1/tpu/test_sampler.py
1	7	vllm/v1/sample/tpu/metadata.py
6	2	vllm/v1/worker/tpu_model_runner.py

[46450b8d3] Harry Mellor 2025-03-27 Use absolute placement for Ask AI button (#15628)
2	2	docs/source/_static/custom.js

[13ac9cab2] Cyrus Leung 2025-03-28 [Misc] Avoid direct access of global `mm_registry` in `compute_encoder_budget` (#15621)
12	4	vllm/v1/core/encoder_cache_manager.py
3	0	vllm/v1/core/sched/scheduler.py
3	3	vllm/v1/worker/gpu_model_runner.py
1	0	vllm/v1/worker/tpu_model_runner.py

[66aa4c0bf] Yuan Tang 2025-03-27 [Feature] Add middleware to log API Server responses (#15593)
16	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/envs.py

[247181536] Cyrus Leung 2025-03-28 [Misc] Replace `is_encoder_decoder_inputs` with `split_enc_dec_inputs` (#15620)
1	1	tests/models/multimodal/processing/test_idefics3.py
1	1	tests/models/multimodal/processing/test_phi3v.py
1	1	vllm/engine/arg_utils.py
11	15	vllm/engine/llm_engine.py
14	8	vllm/inputs/parse.py
6	8	vllm/inputs/registry.py
2	2	vllm/model_executor/models/idefics3.py
11	16	vllm/v1/engine/processor.py

[07bf813fb] Cyrus Leung 2025-03-28 [Doc] Link to onboarding tasks (#15629)
5	0	docs/source/conf.py
9	0	docs/source/contributing/overview.md

[8958217ad] Hiroaki Sugiyama 2025-03-27 [Bugfix] Fix use_cascade_attention handling for Alibi-based models on vllm/v1 (#15211)
13	1	vllm/utils.py
5	2	vllm/v1/worker/gpu_model_runner.py

[ac5bc615b] Cyrus Leung 2025-03-27 [Model] MiniCPM-V/O supports V1 (#15487)
2	2	docs/source/models/supported_models.md
214	211	vllm/model_executor/models/minicpmo.py
343	361	vllm/model_executor/models/minicpmv.py
18	24	vllm/model_executor/models/molmo.py

[8063dfc61] Reid 2025-03-27 [Doc] update --system for transformers installation in docker doc (#15616)
3	3	docs/source/deployment/docker.md

[6278bc829] Richard Zou 2025-03-27 Fix incorrect filenames in vllm_compile_cache.py (#15494)
14	1	vllm/compilation/compiler_interface.py

[3f532cb6a] wang.yuqi 2025-03-27 [Misc] Use model_redirect to redirect the model name to a local folder. (#14116)
7	3	vllm/config.py
5	0	vllm/envs.py
38	0	vllm/transformers_utils/utils.py

[e6c9053f9] Cyrus Leung 2025-03-27 [Misc] Clean up `scatter_patch_features` (#15559)
6	11	vllm/model_executor/models/gemma3_mm.py
9	12	vllm/model_executor/models/internvl.py
10	12	vllm/model_executor/models/llava.py
32	73	vllm/model_executor/models/molmo.py
7	11	vllm/model_executor/models/pixtral.py
20	19	vllm/model_executor/models/vision.py

[43ed4143c] Robert Shaw 2025-03-27 [Quantization] Fp8 Channelwise Dynamic Per Token GroupedGEMM (#15587)
0	26	vllm/model_executor/layers/fused_moe/layer.py
67	40	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[f4c98b4d4] Bella kira 2025-03-27 [Misc] Consolidate LRUCache implementations (#15481)
1	2	vllm/multimodal/processing.py
103	54	vllm/utils.py

[e1e0fd754] Robert Shaw 2025-03-27 [TPU] Avoid Triton Import (#15589)
3	3	vllm/model_executor/layers/fused_moe/layer.py
5	3	vllm/model_executor/layers/quantization/fp8.py

[df8d3d128] Rui Qiao 2025-03-26 [Misc] Restrict ray version dependency and update PP feature warning in V1 (#15556)
1	1	requirements/cuda.txt
1	1	requirements/test.in
1	1	vllm/config.py
5	2	vllm/engine/arg_utils.py

[619d3de8b] Chengji Yao 2025-03-26 [TPU] [V1] fix cases when max_num_reqs is set smaller than MIN_NUM_SEQS (#15583)
1	4	examples/offline_inference/tpu.py
1	1	vllm/v1/worker/tpu_model_runner.py

[ecff8309a] Gregory Shtrasberg 2025-03-27 [ROCm] Env variable to trigger custom PA (#15557)
2	1	vllm/attention/backends/rocm_flash_attn.py
6	0	vllm/envs.py

[dcf2a590f] Jerry Zhang 2025-03-26 Allow torchao quantization in SiglipMLP (#15575)
4	2	vllm/model_executor/models/siglip.py

[54aa61945] Cody Yu 2025-03-26 [V1] Refactor num_computed_tokens logic (#15307)
13	3	tests/v1/core/test_scheduler.py
11	7	tests/v1/engine/test_engine_core.py
48	43	vllm/v1/core/sched/scheduler.py
19	0	vllm/v1/sample/rejection_sampler.py
15	4	vllm/v1/worker/gpu_model_runner.py

[fb22be581] Mengqing Cao 2025-03-27 [moe][quant] add weight name case for offset (#15515)
3	2	vllm/model_executor/layers/fused_moe/layer.py

[7f301dd8e] Wei Zeng 2025-03-26 [Doc] Update V1 user guide for fp8 kv cache support (#15585)
1	3	docs/source/getting_started/v1_user_guide.md

[8095341a0] Varun Sundar Rabindranath 2025-03-26 [misc] LoRA: Remove unused long context test data (#15558)
0	33	tests/lora/conftest.py
0	0	tests/lora/data/__init__.py
0	121	tests/lora/data/long_context_test_data.py

[69db16a46] Chenyaaang 2025-03-26 add platform check back (#15578)
3	0	vllm/v1/engine/processor.py

[ce78f9af4] Michael Goin 2025-03-26 Add automatic tpu label to mergify.yml (#15560)
11	0	.github/mergify.yml

[9239bf718] ElizaWszola 2025-03-27 [Kernel] CUTLASS grouped gemm fp8 MoE kernel (#13972)
27	0	CMakeLists.txt
340	0	benchmarks/kernels/benchmark_grouped_gemm_cutlass.py
16	0	benchmarks/kernels/benchmark_shapes.py
11	1	csrc/cutlass_extensions/common.hpp
457	0	csrc/cutlass_extensions/epilogue/broadcast_load_epilogue_array_c3x.hpp
66	0	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
14	0	csrc/ops.h
80	0	csrc/quantization/cutlass_w8a8/moe/get_group_starts.cuh
160	0	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cu
149	0	csrc/quantization/cutlass_w8a8/moe/grouped_mm_c3x.cuh
90	0	csrc/quantization/cutlass_w8a8/moe/moe_data.cu
67	0	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
29	0	csrc/torch_bindings.cpp
134	0	tests/kernels/test_cutlass.py
244	0	tests/kernels/test_cutlass_moe.py
53	0	vllm/_custom_ops.py
3	2	vllm/model_executor/layers/fused_moe/__init__.py
137	0	vllm/model_executor/layers/fused_moe/fused_moe.py
23	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
201	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
10	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
6	3	vllm/utils.py

[7a6d45bc8] Matthew Vine 2025-03-26 Support FIPS enabled machines with MD5 hashing (#15299)
2	1	tests/compile/piecewise/test_toy_llama.py
4	3	vllm/compilation/backends.py
2	1	vllm/compilation/compiler_interface.py
28	14	vllm/config.py

[e74ff409e] Chengji Yao 2025-03-26 [TPU] support disabling xla compilation cache (#15567)
10	3	vllm/v1/worker/tpu_worker.py
10	3	vllm/worker/tpu_worker.py

[7a888271f] Wes 2025-03-26 Use Cache Hinting for fused_moe kernel (#15511)
12	4	vllm/model_executor/layers/fused_moe/fused_moe.py

[9d119a86a] Alexander Matveev 2025-03-26 [V1] TPU CI - Fix test_compilation.py (#15570)
1	1	.buildkite/run-tpu-v1-test.sh
17	40	tests/tpu/test_compilation.py

[b2e85e26f] Alexander Matveev 2025-03-26 [V1] TPU - Revert to exponential padding by default (#15565)
2	2	vllm/envs.py
26	9	vllm/v1/worker/tpu_model_runner.py

[dd8a29da9] Alexei-V-Ivanov-AMD 2025-03-26 Applying some fixes for K8s agents in CI (#15493)
6	4	.buildkite/run-amd-test.sh
2	1	Dockerfile.rocm

[27df5199d] marko 2025-03-26 Support SHA256 as hash function in prefix caching (#15297)
4	3	docs/source/design/v1/prefix_caching.md
22	1	tests/test_utils.py
29	13	tests/v1/core/test_kv_cache_utils.py
17	5	tests/v1/core/test_prefix_caching.py
20	0	tests/v1/engine/test_engine_args.py
9	0	vllm/config.py
32	6	vllm/engine/arg_utils.py
20	0	vllm/utils.py
11	9	vllm/v1/core/block_pool.py
6	2	vllm/v1/core/kv_cache_manager.py
43	32	vllm/v1/core/kv_cache_utils.py
1	0	vllm/v1/core/sched/scheduler.py

[35fad35a4] Nick Hill 2025-03-26 [V1][Sampler] Faster top-k only implementation (#15478)
37	0	tests/v1/sample/test_topk_topp_sampler.py
48	5	vllm/v1/sample/ops/topk_topp_sampler.py
6	0	vllm/v1/sample/sampler.py

[733e7c9e9] Aaron Pham 2025-03-26 [Refactor] Remove unnecessary backend parameter in structured output interface (#15317)
5	5	vllm/v1/structured_output/__init__.py

[0af4d764d] Harry Mellor 2025-03-26 Fix weight loading for some models in Transformers backend (#15544)
5	3	vllm/model_executor/models/transformers.py

[e64afa455] youkaichao 2025-03-26 multi-node offline DP+EP example (#15484)
97	23	examples/offline_inference/data_parallel.py

[1711b929b] Alex Brooks 2025-03-26 [Model] Add Reasoning Parser for Granite Models (#14202)
6	1	docs/source/features/reasoning_outputs.md
1	0	examples/online_serving/openai_chat_completion_with_reasoning.py
1	0	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
349	0	tests/entrypoints/openai/reasoning_parsers/test_granite_reasoning_parser.py
1	1	vllm/engine/arg_utils.py
5	1	vllm/entrypoints/openai/reasoning_parsers/__init__.py
363	0	vllm/entrypoints/openai/reasoning_parsers/granite_reasoning_parser.py
4	0	vllm/model_executor/guided_decoding/reasoner/__init__.py

[c091c0a58] Harry Mellor 2025-03-26 Improve validation of TP in Transformers backend (#15540)
4	1	vllm/model_executor/models/transformers.py

[1aa162e03] cyyever 2025-03-26 Apply torchfix (#15532)
2	3	vllm/attention/backends/rocm_flash_attn.py
3	1	vllm/lora/models.py
3	3	vllm/model_executor/models/nemotron.py
6	3	vllm/model_executor/models/phi4mm_utils.py
1	1	vllm/multimodal/image.py

[cf5c8f168] Harry Mellor 2025-03-26 Separate base model from `TransformersModel` (#15467)
3	3	docs/source/models/supported_models.md
1	1	tests/distributed/test_pipeline_parallel.py
1	1	tests/models/registry.py
3	3	vllm/model_executor/model_loader/utils.py
2	2	vllm/model_executor/models/registry.py
100	49	vllm/model_executor/models/transformers.py

[4ec2cee00] Reid 2025-03-26 [Misc] improve example script output (#15528)
4	1	examples/offline_inference/basic/basic.py
3	2	examples/offline_inference/basic/chat.py
3	1	examples/offline_inference/basic/classify.py
3	1	examples/offline_inference/basic/embed.py
3	1	examples/offline_inference/basic/score.py

[99f536f83] wwl2755 2025-03-26 [Misc] Enhance warning information to user-defined chat template (#15408)
5	5	tests/entrypoints/test_chat_utils.py
26	14	vllm/entrypoints/chat_utils.py
24	3	vllm/entrypoints/openai/api_server.py

[5ebf66748] vllmellm 2025-03-26 [FEAT][ROCm] Integrate Fused MoE Kernels from AITER (#14967)
19	6	tests/kernels/test_moe.py
36	0	tests/model_executor/test_enabled_custom_ops.py
9	32	tests/models/decoder_only/language/test_mistral.py
20	3	tests/quantization/test_fp8.py
15	0	vllm/envs.py
69	23	vllm/model_executor/layers/fused_moe/fused_moe.py
13	1	vllm/model_executor/layers/fused_moe/layer.py
157	0	vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py
52	0	vllm/model_executor/layers/quantization/fp8.py

[781d05628] Bryan Lu 2025-03-26 [Feature] Enhance EAGLE Architecture with Proper RMS Norms (#14990)
12	4	vllm/config.py
45	6	vllm/model_executor/models/eagle.py
13	2	vllm/transformers_utils/configs/eagle.py

[5aefd6ac3] daniel-salib 2025-03-25 Fix raw_request extraction in load_aware_call decorator (#15382)
10	3	vllm/entrypoints/utils.py

[6c663dfd5] Varun Sundar Rabindranath 2025-03-25 [misc] LoRA - Skip LoRA kernels when not required (#15152)
12	1	vllm/lora/ops/triton_ops/lora_expand.py
36	6	vllm/lora/ops/triton_ops/lora_kernel_metadata.py
12	1	vllm/lora/ops/triton_ops/lora_shrink.py
53	25	vllm/worker/model_runner.py

[33437bc6e] Lucas Wilkinson 2025-03-25 [BugFix] Fix nightly MLA failure (FA2 + MLA chunked prefill, i.e. V1, producing bad results) (#15492)
9	0	vllm/attention/ops/triton_merge_attn_states.py

[23114d336] Tyler Michael Smith 2025-03-25 [Misc] Warn about v0 in benchmark_paged_attn.py (#15495)
6	0	benchmarks/kernels/benchmark_paged_attention.py

[997c8811d] Cyrus Leung 2025-03-26 [Model] Support multi-image for Molmo (#15438)
1	1	docs/source/models/supported_models.md
1	1	tests/models/decoder_only/vision_language/test_models.py
28	29	vllm/model_executor/models/molmo.py
9	4	vllm/model_executor/models/vision.py

[e42389f9d] Harry Mellor 2025-03-26 Transformers backend already supports V1 (#15463)
5	17	tests/models/test_transformers.py
0	8	vllm/engine/arg_utils.py
2	0	vllm/model_executor/models/transformers.py

[ff38f0a32] Varun Sundar Rabindranath 2025-03-25 [CI/Build] LoRA: Delete long context tests (#15503)
1	3	.buildkite/test-pipeline.yaml
0	301	tests/lora/test_long_context.py

[a5cfbab3c] Varun Sundar Rabindranath 2025-03-25 [Core] LoRA: V1 Scheduler optimization (#15422)
28	29	vllm/v1/core/sched/scheduler.py

[ac3cd6e83] Chenyaaang 2025-03-25 [core] add bucket padding to tpu_model_runner (#14995)
21	1	tests/v1/tpu/worker/test_tpu_model_runner.py
7	0	vllm/envs.py
35	18	vllm/v1/worker/tpu_model_runner.py

[082ab86f5] Lu Fang 2025-03-25 [V1] Support long_prefill_token_threshold in v1 scheduler (#15419)
75	1	tests/v1/core/test_scheduler.py
29	0	tests/v1/core/test_scheduler_e2e.py
1	3	vllm/engine/arg_utils.py
8	0	vllm/v1/core/sched/scheduler.py

[6aa196c8d] Nick Hill 2025-03-25 [V1][Minor] Use `SchedulerInterface` type for engine scheduler field (#15499)
2	1	vllm/v1/engine/core.py

[a0dd7dcd4] Nicolò Lucchesi 2025-03-25 [TPU][V1] Fix Sampler recompilation (#15309)
71	104	vllm/v1/sample/tpu/metadata.py
13	23	vllm/v1/worker/tpu_model_runner.py

[e977c1111] Maximilien de Bayser 2025-03-25 Add workaround for shared field_names in pydantic model class (#13925)
3	0	vllm/entrypoints/openai/protocol.py

[5f063a80b] Joe Runde 2025-03-25 [bugfix] add supports_v1 platform interface (#15417)
2	3	vllm/engine/arg_utils.py
6	1	vllm/platforms/cuda.py
9	1	vllm/platforms/interface.py
7	1	vllm/platforms/rocm.py
7	1	vllm/platforms/tpu.py

[5d8e1c927] Antonio Gómez 2025-03-25 [Bugfix] Support triton==3.3.0+git95326d9f for RTX 5090 (Unsloth + vLLM compatibility) (#15471)
1	1	vllm/lora/ops/triton_ops/kernel_utils.py

[0a049c7d8] yarongmu-google 2025-03-25 [CI/Build] Add tests for the V1 tpu_model_runner. (#14843)
3	1	.buildkite/run-tpu-v1-test.sh
0	0	tests/v1/tpu/worker/__init__.py
307	0	tests/v1/tpu/worker/test_tpu_model_runner.py

[d0cfec7ab] youkaichao 2025-03-25 [bugfix] fix inductor cache on max_position_embeddings (#15436)
3	0	vllm/config.py

[a60816002] Szymon Ożóg 2025-03-25 [Kernel] Fix conflicting macro names for gguf kernels (#15456)
10	10	csrc/quantization/gguf/gguf_kernel.cu
80	80	csrc/quantization/gguf/moe.cuh

[3f04a7fbf] Cyrus Leung 2025-03-25 [Doc] Update V1 user guide for multi-modality (#15460)
3	0	docs/source/getting_started/v1_user_guide.md

[5994430b8] Cyrus Leung 2025-03-25 [Misc] Remove redundant `num_embeds` (#15443)
0	16	vllm/model_executor/models/gemma3_mm.py
0	14	vllm/model_executor/models/internvl.py
0	16	vllm/model_executor/models/llava.py
0	14	vllm/model_executor/models/pixtral.py
25	4	vllm/model_executor/models/vision.py

[a9e879b31] Cyrus Leung 2025-03-25 [Misc] Clean up MiniCPM-V/O code (#15337)
1	0	examples/offline_inference/vision_language.py
45	20	tests/models/decoder_only/vision_language/test_models.py
41	49	tests/models/multimodal/processing/test_common.py
0	2	vllm/model_executor/models/gemma3_mm.py
149	191	vllm/model_executor/models/minicpmo.py
288	399	vllm/model_executor/models/minicpmv.py
7	0	vllm/multimodal/inputs.py

[3e2f37a69] Md. Shafi Hussain 2025-03-25 Dockerfile.ppc64le changes to move to UBI (#15402)
251	21	Dockerfile.ppc64le
3	3	requirements/cpu.txt

[4f044b1d6] Thien Tran 2025-03-25 [Kernel][CPU] CPU MLA (#14744)
2	0	.buildkite/run-cpu-test.sh
1	0	cmake/cpu_extension.cmake
74	0	csrc/cpu/cache.cpp
2	0	csrc/cpu/cpu_types_x86.hpp
393	0	csrc/cpu/mla_decode.cpp
20	0	csrc/cpu/torch_bindings.cpp
69	0	tests/kernels/test_cache.py
94	0	tests/kernels/test_mla_decode_cpu.py
12	0	vllm/_custom_ops.py
22	9	vllm/_ipex_ops.py
303	0	vllm/attention/backends/cpu_mla.py
13	5	vllm/attention/backends/mla/common.py
3	3	vllm/platforms/cpu.py
1	0	vllm/worker/cpu_model_runner.py
1	0	vllm/worker/cpu_worker.py

[4157f563b] Siyuan Liu 2025-03-25 [Hardware][TPU][Bugfix] Fix v1 mp profiler (#15409)
8	2	vllm/v1/worker/tpu_worker.py

[051da7efe] Lu Fang 2025-03-25 Fix CUDA kernel index data type in vllm/csrc/quantization/gptq_marlin/awq_marlin_repack.cu +10 (#15160)
6	6	csrc/quantization/gptq_marlin/awq_marlin_repack.cu
14	14	csrc/quantization/gptq_marlin/gptq_marlin.cu
8	8	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
5	5	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
7	7	csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu
7	7	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
26	26	csrc/rocm/attention.cu

[25f560a62] Woosuk Kwon 2025-03-24 [V1][Spec Decode] Update target_logits in place for rejection sampling (#15427)
5	2	vllm/v1/sample/rejection_sampler.py
7	2	vllm/v1/worker/gpu_model_runner.py

[a09ad90a7] Russell Bryant 2025-03-25 [V1] guidance backend for structured output + `auto` fallback mode (#14779)
1	1	requirements/common.txt
102	67	tests/v1/entrypoints/llm/test_struct_output_generate.py
7	2	vllm/config.py
9	12	vllm/engine/arg_utils.py
32	7	vllm/v1/engine/processor.py
3	0	vllm/v1/structured_output/__init__.py
164	0	vllm/v1/structured_output/backend_guidance.py
26	21	vllm/v1/structured_output/request.py
1	1	vllm/v1/structured_output/utils.py

[10b34e36b] Chauncey 2025-03-25 [Bugfix] Fixed the issue of not being able to input video and image simultaneously (#15387)
6	6	vllm/entrypoints/chat_utils.py

[b5269db95] Tyler Michael Smith 2025-03-24 Revert "Fix non-contiguous input passed to Marlin kernel (#15319)" (#15398)
0	4	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py

[6db94571d] Jee Jee Li 2025-03-25 [Misc] Remove LoRA log (#15388)
0	6	vllm/config.py
5	9	vllm/lora/punica_wrapper/punica_gpu.py

[97cfa65df] Harry Mellor 2025-03-25 Add pipeline parallel support to `TransformersModel` (#12832)
1	1	docs/source/models/supported_models.md
3	0	tests/distributed/test_pipeline_parallel.py
230	86	vllm/model_executor/models/transformers.py
11	1	vllm/model_executor/models/utils.py

[911c8eb00] Woosuk Kwon 2025-03-24 [Minor][Spec Decode] Remove compiled_softmax (#15416)
0	30	vllm/v1/sample/ops/utils.py
1	3	vllm/v1/sample/rejection_sampler.py

[ebcebeeb6] Woosuk Kwon 2025-03-24 [V1][Spec Decode] Enable spec decode for top-p & top-k sampling (#15063)
148	2	tests/v1/sample/test_rejection_sampler.py
70	13	vllm/v1/sample/rejection_sampler.py
1	4	vllm/v1/spec_decode/utils.py

[f533b5837] Gregory Shtrasberg 2025-03-24 [ROCm][Kernel] MoE weights padding (#14454)
35	10	tests/kernels/test_moe.py
5	0	vllm/envs.py
3	3	vllm/model_executor/layers/fused_moe/fused_moe.py
19	0	vllm/model_executor/layers/fused_moe/layer.py
3	3	vllm/model_executor/layers/quantization/fp8.py

[8279201ce] Gregory Shtrasberg 2025-03-24 [Build] Cython compilation support fix (#14296)
1	1	Dockerfile.rocm
1	0	pyproject.toml
38	0	tests/build_cython.py
1	1	vllm/engine/llm_engine.py
2	1	vllm/model_executor/layers/sampler.py
3	3	vllm/utils.py

[23fdab00a] Siyuan Liu 2025-03-24 [Hardware][TPU] Skip failed compilation test (#15421)
1	1	.buildkite/run-tpu-v1-test.sh
90	86	tests/tpu/test_compilation.py

[623e2ed29] Nick Hill 2025-03-24 [BugFix][V1] Quick fix for min_tokens with multiple EOS (#15407)
2	1	vllm/sampling_params.py

[9d72daf4c] Nick Hill 2025-03-24 [V1][Perf] Simpler request output queues (#15156)
88	1	tests/v1/engine/test_output_processor.py
14	20	vllm/v1/engine/async_llm.py
44	4	vllm/v1/engine/output_processor.py

[6dd55af6c] Cyrus Leung 2025-03-25 [Doc] Update docs on handling OOM (#15357)
1	1	docs/source/getting_started/installation/cpu.md
5	2	docs/source/getting_started/v1_user_guide.md
7	2	docs/source/serving/engine_args.md
7	0	docs/source/serving/offline_inference.md
3	3	vllm/envs.py
1	1	vllm/platforms/cpu.py

[3eb08ed9b] Yuan Tang 2025-03-24 [DOC] Add Kubernetes deployment guide with CPUs (#14865)
1	0	docs/source/conf.py
102	3	docs/source/deployment/k8s.md

[5eeadc264] liuzhenwei 2025-03-25 [Hardware][Gaudi][Feature] Enable Dynamic MoE for Mixtral (#12303)
31	0	vllm/model_executor/layers/fused_moe/layer.py
10	0	vllm/model_executor/model_loader/loader.py
16	2	vllm/worker/hpu_model_runner.py

[3aee6573d] Nick Hill 2025-03-24 [V1] Aggregate chunked prompt logprobs in model runner (#14875)
4	2	vllm/v1/core/sched/scheduler.py
0	1	vllm/v1/engine/logprobs.py
3	18	vllm/v1/engine/output_processor.py
4	15	vllm/v1/metrics/stats.py
19	0	vllm/v1/outputs.py
5	0	vllm/v1/worker/gpu_input_batch.py
33	8	vllm/v1/worker/gpu_model_runner.py

[9cc645141] Yi Liu 2025-03-25 [MISC] Refine no available block debug msg (#15076)
10	4	vllm/distributed/device_communicators/shm_broadcast.py

[0893567db] Chen1022 2025-03-24 [V1][Minor]   fix comments (#15392)
1	1	vllm/v1/sample/sampler.py

[8abe69b49] Russell Bryant 2025-03-24 [Core] Don't force uppercase for VLLM_LOGGING_LEVEL (#15306)
1	1	vllm/envs.py

[761702fd1] Manish Sethi 2025-03-24 [Core] Integrate `fastsafetensors` loader for loading model weights (#10647)
5	0	docs/source/models/extensions/fastsafetensor.md
1	0	docs/source/models/extensions/index.md
1	0	requirements/test.in
12	1	requirements/test.txt
1	0	setup.py
0	0	tests/fastsafetensors_loader/__init__.py
22	0	tests/fastsafetensors_loader/test_fastsafetensors_loader.py
46	0	tests/fastsafetensors_loader/test_weight_utils.py
1	0	vllm/config.py
16	8	vllm/model_executor/model_loader/loader.py
47	0	vllm/model_executor/model_loader/weight_utils.py

[9606d572e] youkaichao 2025-03-24 [distributed] fix dp group (#15355)
13	26	vllm/distributed/parallel_state.py

[cbcdf2c60] Cyrus Leung 2025-03-24 [Bugfix] Fix chat template loading (#15143)
2	0	tests/entrypoints/openai/test_chat_template.py
2	2	tests/entrypoints/openai/test_video.py
62	2	tests/entrypoints/test_chat_utils.py
4	1	tests/tool_use/utils.py
106	39	vllm/entrypoints/chat_utils.py
5	2	vllm/entrypoints/llm.py
6	1	vllm/entrypoints/openai/serving_engine.py

[038de04d7] Russell Bryant 2025-03-24 Fix zmq IPv6 URL format error (#15341)
1	0	vllm/distributed/device_communicators/shm_broadcast.py

[6b3cc75be] Jinzhen Lin 2025-03-24 [Kernel] allow non-contiguous input for marlin kernel (#14658)
28	20	csrc/quantization/gptq_marlin/gptq_marlin.cu
45	0	tests/kernels/test_marlin_gemm.py

[7ffcccfa5] Simon Mo 2025-03-24 Revert "[CI/Build] Use uv python for docker rather than ppa:deadsnakess/ppa (#13569)" (#15377)
51	38	Dockerfile

[cc8accfd5] sfbemerk 2025-03-24 [Misc] Update guided decoding logs to debug (#15310)
1	1	vllm/engine/async_llm_engine.py

[948ab03e7] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2025-03-24 [Bugfix][V1] Avoid importing PreTrainedModel (#15366)
1	1	vllm/model_executor/model_loader/utils.py

[5797fb97e] Rui Qiao 2025-03-24 [Misc] Remove ignore_reinit_error for ray.init() (#15373)
3	5	vllm/executor/ray_utils.py

[3892e58ad] Jee Jee Li 2025-03-24 [Misc] Upgrade BNB version (#15183)
1	1	Dockerfile
1	1	docs/source/features/quantization/bnb.md
4	4	vllm/model_executor/layers/quantization/bitsandbytes.py
4	4	vllm/model_executor/model_loader/loader.py

[d20e26119] Qubitium-ModelCloud 2025-03-24 Fix non-contiguous input passed to Marlin kernel (#15319)
4	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py

[f622dbcf3] Luka Govedič 2025-03-23 [Fix] [torch.compile] Improve UUID system for custom passes (#15249)
46	16	tests/compile/test_pass_manager.py
25	28	vllm/compilation/inductor_pass.py
9	35	vllm/compilation/pass_manager.py
41	0	vllm/compilation/torch25_custom_graph_pass.py
4	5	vllm/config.py

[dccf535f8] Lucas Wilkinson 2025-03-23 [V1] Enable V1 Fp8 cache for FA3 in the oracle (#15191)
2	1	.gitignore
12	4	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/mla/common.py
0	4	vllm/config.py
14	3	vllm/engine/arg_utils.py
3	5	vllm/platforms/cuda.py
6	4	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/mla/common.py
6	0	vllm/{ => vllm_flash_attn}/fa_utils.py

[9c5c81b0d] Roger Wang 2025-03-23 [Misc][Doc] Add note regarding loading `generation_config` by default (#15281)
11	1	docs/source/getting_started/quickstart.md
5	0	docs/source/models/generative_models.md
4	0	docs/source/serving/openai_compatible_server.md
7	0	vllm/config.py

[d6cd59f12] Robin 2025-03-24 [Frontend] Support tool calling and reasoning parser (#14511)
1	1	.buildkite/test-pipeline.yaml
46	5	docs/source/features/reasoning_outputs.md
177	0	examples/online_serving/openai_chat_completion_tool_calls_with_reasoning.py
145	0	tests/entrypoints/openai/test_chat_with_tool_reasoning.py
0	7	vllm/entrypoints/openai/cli_args.py
35	0	vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py
13	0	vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
138	50	vllm/entrypoints/openai/serving_chat.py

[bc8ed3c4b] Woosuk Kwon 2025-03-23 [V1][Spec Decode] Use better defaults for N-gram (#15358)
22	10	vllm/config.py

[b9bd76ca1] Woosuk Kwon 2025-03-23 [V1][Spec Decode] Respect prompt_lookup_max (#15348)
52	1	tests/v1/spec_decode/test_ngram.py
13	4	vllm/v1/spec_decode/ngram_proposer.py
2	0	vllm/v1/worker/gpu_model_runner.py

[6ebaf9ac7] DefTruth 2025-03-23 [Bugfix] consider related env vars for torch.compiled cache hash (#14953)
5	0	vllm/compilation/backends.py
41	0	vllm/envs.py

[f90d34b49] DefTruth 2025-03-23 [Misc] Add tuned R1 w8a8 and MoE configs for NVIDIA L20 (#15322)
420	0	benchmarks/kernels/benchmark_w8a8_block_fp8.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
18	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_L20,dtype=fp8_w8a8,block_shape=[128,128].json

[f68cce8e6] youkaichao 2025-03-23 [ci/build] fix broken tests in LLM.collective_rpc (#15350)
1	1	.buildkite/test-pipeline.yaml
2	11	tests/entrypoints/llm/test_collective_rpc.py

[09b6a9555] youkaichao 2025-03-23 [ci/build] update torch nightly version for GH200 (#15135)
2	1	.buildkite/run-gh200-test.sh
4	2	Dockerfile

[50c9636d8] shangmingc 2025-03-23 [V1][Usage] Refactor speculative decoding configuration and tests (#14434)
25	14	docs/source/features/spec_decode.md
3	1	examples/offline_inference/mlpspeculator.py
1	1	tests/spec_decode/e2e/conftest.py
20	9	tests/spec_decode/e2e/test_compatibility.py
57	40	tests/spec_decode/e2e/test_eagle_correctness.py
28	18	tests/spec_decode/e2e/test_integration.py
40	37	tests/spec_decode/e2e/test_integration_dist_tp2.py
13	15	tests/spec_decode/e2e/test_integration_dist_tp4.py
102	88	tests/spec_decode/e2e/test_logprobs.py
55	40	tests/spec_decode/e2e/test_medusa_correctness.py
54	35	tests/spec_decode/e2e/test_mlp_correctness.py
39	25	tests/spec_decode/e2e/test_mtp_correctness.py
112	65	tests/spec_decode/e2e/test_multistep_correctness.py
88	69	tests/spec_decode/e2e/test_ngram_correctness.py
5	5	tests/spec_decode/e2e/test_seed.py
10	6	tests/v1/e2e/test_ngram_spec_decode.py
307	301	vllm/config.py
92	26	vllm/engine/arg_utils.py
7	9	vllm/spec_decode/spec_decode_worker.py
3	4	vllm/v1/worker/gpu_model_runner.py

[0661cfef7] hijkzzz 2025-03-23 Fix v1 supported oracle for worker-cls and worker-extension-cls (#15324)
2	2	.buildkite/test-pipeline.yaml
0	10	vllm/engine/arg_utils.py
5	0	vllm/utils.py

[a827aa815] Chen Zhang 2025-03-23 [doc] Add back previous news (#15331)
19	2	README.md

[b877031d8] Russell Bryant 2025-03-22 Remove openvino support in favor of external plugin (#15339)
0	16	.buildkite/run-openvino-test.sh
0	29	Dockerfile.openvino
0	1	docs/source/getting_started/installation.md
0	77	docs/source/getting_started/installation/ai_accelerator.md
0	110	docs/source/getting_started/installation/ai_accelerator/openvino.inc.md
0	8	requirements/openvino.txt
1	9	setup.py
1	2	tests/conftest.py
3	11	tests/kernels/test_attention_selector.py
1	1	tests/utils.py
0	146	vllm/attention/backends/openvino.py
1	1	vllm/config.py
0	1	vllm/engine/arg_utils.py
1	27	vllm/envs.py
0	204	vllm/model_executor/model_loader/openvino.py
0	17	vllm/platforms/__init__.py
0	5	vllm/platforms/interface.py
0	152	vllm/platforms/openvino.py
0	372	vllm/worker/openvino_model_runner.py
0	600	vllm/worker/openvino_worker.py

[dd861b992] Wang Ran (汪然) 2025-03-23 [BugFix][Typing] Fix Imprecise Type Annotations (#15208)
3	3	vllm/v1/engine/core_client.py

[eb63ea1e1] Russell Bryant 2025-03-22 [V1] Add `disable-any-whitespace` option support for xgrammar (#15316)
44	1	tests/v1/entrypoints/llm/test_struct_output_generate.py
3	1	vllm/engine/arg_utils.py
1	1	vllm/v1/engine/processor.py
5	2	vllm/v1/structured_output/backend_xgrammar.py

[2f4bd358f] Naitong Yu 2025-03-22 [Model] Support Tele-FLM Model (#15023)
5	0	docs/source/models/supported_models.md
12	0	examples/template_teleflm.jinja
2	0	tests/models/registry.py
1	0	vllm/model_executor/models/registry.py
79	0	vllm/model_executor/models/teleflm.py

[8a8b30eac] Varun Sundar Rabindranath 2025-03-22 [Bugfix] LoRA V0 - Fix case where `max_num_seqs` is between cudagraph capture sizes (#15308)
8	6	tests/lora/test_llama_tp.py
9	1	vllm/lora/punica_wrapper/punica_gpu.py

[2fa0e1396] Jee Jee Li 2025-03-22 [Bugfix] Fix torch.compile raise FileNotFoundError (#15278)
1	0	vllm/compilation/backends.py

[1c2bec0f8] wwl2755 2025-03-22 [Doc] add load_format items in docs (#14804)
6	0	vllm/config.py
8	2	vllm/engine/arg_utils.py

[ec870fba9] TJian 2025-03-22 [FEAT] [ROCm]:  Add AITER RMS Norm (Layer Norm) Feature (#14959)
15	1	Dockerfile.rocm_base
28	1	tests/model_executor/test_enabled_custom_ops.py
40	10	tests/models/decoder_only/language/test_models.py
13	0	vllm/envs.py
77	17	vllm/model_executor/layers/layernorm.py

[df1430265] Andy Lo 2025-03-22 [Bugfix][V0] Multi-sequence logprobs streaming edge case (#15259)
6	1	vllm/outputs.py

[4c69e228b] Rui Qiao 2025-03-21 [Misc] Increase RayDistributedExecutor RAY_CGRAPH_get_timeout (#15301)
9	0	vllm/executor/ray_distributed_executor.py

[790b79750] Russell Bryant 2025-03-21 [Build/CI] Fix env var typo (#15305)
1	1	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh

[cfbb8c930] Nicolò Lucchesi 2025-03-21 [TPU][V1] MHA Pallas backend (#15288)
109	0	tests/v1/tpu/test_mha_attn.py
8	2	vllm/attention/layer.py

[baec0d4de] Cyrus Leung 2025-03-21 Revert "[Feature] specify model in config.yaml  (#14855)" (#15293)
1	3	docs/source/serving/openai_compatible_server.md
0	7	tests/config/test_config_with_model.yaml
0	12	tests/conftest.py
0	0	tests/{config => data}/test_config.yaml
11	42	tests/test_utils.py
9	13	vllm/entrypoints/cli/serve.py
9	25	vllm/utils.py

[c21b99b91] Mengqing Cao 2025-03-21 [Bugfix][VLM] fix llava processor (#15285)
7	1	vllm/model_executor/models/llava.py

[93a00d7dd] Chen Zhang 2025-03-21 [v1] Refactor KVCacheConfig (#14079)
109	1	tests/v1/core/test_kv_cache_utils.py
95	35	vllm/v1/core/kv_cache_utils.py
22	9	vllm/v1/engine/core.py
5	8	vllm/v1/executor/abstract.py
34	18	vllm/v1/kv_cache_interface.py
29	17	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py
23	21	vllm/v1/worker/tpu_model_runner.py
1	1	vllm/v1/worker/tpu_worker.py
1	1	vllm/v1/worker/worker_base.py

[61e8c1835] Russell Bryant 2025-03-21 [Misc] Add cProfile helpers (#15074)
49	0	docs/source/contributing/profiling/profiling_index.md
48	0	vllm/utils.py

[8afcd0f63] Isotr0py 2025-03-21 [Bugfix] Fix broken kernel test due to missing rename for v1 Triton backend (#15282)
1	1	tests/kernels/test_attention_selector.py
1	1	tests/kernels/test_rocm_attention_selector.py

[91ca929dc] Lehua Ding 2025-03-21 [V1] Fix wrong import path of get_flash_attn_version (#15280)
1	1	vllm/v1/attention/backends/mla/common.py

[84e00adc8] Isotr0py 2025-03-21 [Bugfix] Fix incorrect resolving order for transformers fallback (#15279)
7	5	vllm/model_executor/models/registry.py

[47c712621] Isotr0py 2025-03-21 [Misc] Add attention mask pre-computation optimization back to Qwen2.5-VL (#15273)
23	10	vllm/model_executor/models/qwen2_5_vl.py
12	6	vllm/model_executor/models/qwen2_vl.py

[a989ca2bf] Shanshan Shen 2025-03-21 [Bugfix] Add int8 torch dtype for KVCache (#15260)
1	0	vllm/utils.py

[0fa3970de] Wei Zeng 2025-03-21 [Feature] specify model in config.yaml  (#14855)
3	1	docs/source/serving/openai_compatible_server.md
0	0	tests/{data => config}/test_config.yaml
7	0	tests/config/test_config_with_model.yaml
12	0	tests/conftest.py
42	11	tests/test_utils.py
13	9	vllm/entrypoints/cli/serve.py
25	9	vllm/utils.py

[da6ea29f7] Nick Hill 2025-03-20 [V1] Avoid redundant input processing in n>1 case (#14985)
2	4	tests/lora/test_tokenizer_group.py
9	18	tests/tokenization/test_tokenizer_group.py
0	1	vllm/engine/async_llm_engine.py
0	1	vllm/engine/llm_engine.py
1	4	vllm/engine/protocol.py
9	54	vllm/inputs/preprocess.py
0	2	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
2	8	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
0	2	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
32	19	vllm/v1/engine/async_llm.py
26	16	vllm/v1/engine/llm_engine.py
1	12	vllm/v1/engine/parallel_sampling.py
0	1	vllm/v1/engine/processor.py

[7297941b3] Edwin Hernandez 2025-03-20 [Doc] Update LWS docs (#15163)
189	2	docs/source/deployment/frameworks/lws.md

[f8a08cb90] Isotr0py 2025-03-21 [V1] Enable Triton(ROCm) Attention backend for Nvidia GPUs (#14071)
1	1	vllm/engine/arg_utils.py
8	3	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
3	2	vllm/platforms/rocm.py
10	10	vllm/v1/attention/backends/{rocm_attn.py => triton_attn.py}

[b15fd2be2] Siyuan Liu 2025-03-20 [Hardware][TPU] Add check for no additional graph compilation during runtime (#14710)
8	6	.buildkite/run-tpu-v1-test.sh
5	0	vllm/envs.py
19	0	vllm/v1/worker/tpu_model_runner.py

[e588ac237] Woosuk Kwon 2025-03-20 Add an example for reproducibility (#15262)
36	0	examples/offline_inference/reproduciblity.py

[5df2da5b9] Cody Yu 2025-03-20 [Misc] Better RayExecutor and multiprocessing compatibility (#14705)
14	1	vllm/engine/arg_utils.py
2	2	vllm/executor/multiproc_worker_utils.py
13	8	vllm/executor/ray_utils.py
38	10	vllm/utils.py

[11b986b3f] Woosuk Kwon 2025-03-20 [Docs] Trim the latest news in README (#15261)
1	13	README.md

[296f927f2] Chih-Chieh Yang 2025-03-20 [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14857)
7	6	vllm/model_executor/layers/mamba/mamba_mixer2.py

[0032903a5] Travis Johnson 2025-03-20 [Bugfix] detect alibi and revert to FA2 (#15231)
2	1	vllm/attention/backends/flash_attn.py
9	3	vllm/fa_utils.py

[47195057e] Hyesoo Yang 2025-03-20 [V1][TPU] Speed up top-k on TPU by using torch.topk (#15242)
2	1	tests/v1/tpu/test_sampler.py
6	0	vllm/envs.py
21	3	vllm/v1/sample/ops/topk_topp_sampler.py

[6edbfa924] Harry Mellor 2025-03-21 Mention `extra_body` as a way top pass vLLM only parameters using the OpenAI client (#15240)
5	0	docs/source/serving/openai_compatible_server.md

[1e508343e] Isotr0py 2025-03-21 [Bugfix] Fix incorrect qwen2.5-vl attention mask pre-computation (#15200)
13	0	tests/models/decoder_only/vision_language/test_models.py
18	0	tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
6	4	vllm/model_executor/models/qwen2_5_vl.py

[2e0b4cfde] Sage Moore 2025-03-20 [ROCM] Upgrade torch to 2.6 (#15244)
4	4	requirements/rocm-build.txt

[10f55fe6c] Jee Jee Li 2025-03-21 [Misc] Clean up the BitsAndBytes arguments (#15140)
3	3	docs/source/features/quantization/bnb.md
0	1	examples/offline_inference/lora_with_quantization_inference.py
4	11	vllm/engine/arg_utils.py

[d3ccbd635] Lu Fang 2025-03-20 Fix CUDA kernel index data type in vllm/csrc/quantization/fused_kernels/layernorm_utils.cuh +10 (#15159)
6	6	csrc/quantization/fused_kernels/layernorm_utils.cuh
25	25	csrc/quantization/gguf/dequantize.cuh
2	2	csrc/quantization/gguf/gguf_kernel.cu
6	6	csrc/quantization/gguf/mmq.cuh
2	2	csrc/quantization/gguf/mmvq.cuh
5	5	csrc/quantization/gguf/moe.cuh
53	53	csrc/quantization/gptq/q_gemm.cu
19	19	csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
4	4	csrc/quantization/gptq_allspark/allspark_repack.cu
2	2	csrc/quantization/gptq_allspark/allspark_utils.cuh

[0cfe7d386] Varun Sundar Rabindranath 2025-03-20 [CI/Build] LoRA : make add_lora_test safer (#15181)
8	40	tests/lora/test_add_lora.py

[0c6f5023c] Woosuk Kwon 2025-03-20 [V1] Scheduler Refactoring [1/N] - Add Scheduler Interface (#15250)
1	1	tests/plugins_tests/test_scheduler_plugins.py
2	1	tests/v1/core/test_scheduler.py
2	2	tests/v1/worker/test_gpu_model_runner.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/executor/ray_utils.py
1	1	vllm/v1/attention/backends/flash_attn.py
1	1	vllm/v1/attention/backends/mla/common.py
0	0	vllm/v1/core/sched/__init__.py
139	0	vllm/v1/core/sched/interface.py
0	0	vllm/v1/core/{scheduler_output.py => sched/output.py}
6	31	vllm/v1/core/{ => sched}/scheduler.py
22	0	vllm/v1/core/sched/utils.py
2	2	vllm/v1/engine/core.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py
1	1	vllm/v1/worker/tpu_model_runner.py
1	1	vllm/v1/worker/tpu_worker.py

[06dd08256] Yu Chin Fabian Lim 2025-03-21 Enforce that TP > 1 is not supported for Mamba2 if Quantization is Enabled. (#14617)
21	16	vllm/model_executor/layers/mamba/mamba_mixer2.py

[2b22290ce] Woosuk Kwon 2025-03-20 [V1] Add flag to disable cascade attention (#15243)
2	0	vllm/config.py
12	0	vllm/engine/arg_utils.py
9	5	vllm/v1/worker/gpu_model_runner.py

[d8e82bc06] Jason 2025-03-21 [Bugfix] fix V1 Engine crash while handling requests with duplicate request id (#15043)
16	0	tests/v1/engine/test_engine_core.py
0	10	vllm/v1/engine/core.py

[086b56824] Chi Zhang 2025-03-21 [ci] feat: make the test_torchrun_example run with tp=2, external_dp=2 (#15172)
4	2	.buildkite/test-pipeline.yaml
2	0	tests/distributed/test_torchrun_example.py

[5a0905ba2] Harry Mellor 2025-03-20 Replace `misc` issues with link to forum (#15226)
0	28	.github/ISSUE_TEMPLATE/800-misc-discussion.yml
4	0	.github/ISSUE_TEMPLATE/config.yml

[a8f12a63f] Richard Liu 2025-03-20 Fix env vars for running Ray distributed backend on GKE (#15166)
2	0	vllm/executor/ray_distributed_executor.py
2	0	vllm/platforms/interface.py
4	0	vllm/platforms/tpu.py

[69ae2380c] Harry Mellor 2025-03-20 Add user forum to README (#15220)
6	5	README.md

[27261e40a] Cyrus Leung 2025-03-20 [Bugfix] Multi-video inference on LLaVA-Onevision (#15082)
38	45	vllm/model_executor/models/llava_onevision.py

[e3f813c33] Quang-Linh LE 2025-03-20 [macOS] Ugrade pytorch to 2.6.0 (#15129)
2	1	requirements/cpu.txt

[c607a2652] Wang Ran (汪然) 2025-03-20 Fixing Imprecise Type Annotations (#15192)
1	1	vllm/transformers_utils/tokenizer_group/__init__.py

[3d45e3d74] Kevin H. Luu 2025-03-20 [release] Tag vllm-cpu with latest upon new version released (#15193)
1	1	.buildkite/release-pipeline.yaml

[742369d35] billishyahao 2025-03-20 [Frontend][Bugfix] support prefill decode disaggregation on deepseek (#14824)
9	4	examples/online_serving/disaggregated_prefill.sh
60	17	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
1	0	vllm/model_executor/models/deepseek_v2.py

[bfe2fe0af] Wang Ran (汪然) 2025-03-20 typo: Update config.py (#15189)
1	1	vllm/config.py

[a8652f4f0] Matt Ritter 2025-03-19 Enable CUDA graph support for llama 3.2 vision (#14917)
0	4	tests/models/encoder_decoder/vision_language/test_mllama.py
0	8	vllm/config.py
1	1	vllm/model_executor/models/mllama.py

[2f726b241] Cyrus Leung 2025-03-20 [Doc] Update README.md (#15187)
1	1	README.md

[a597a5759] Mickaël Seznec 2025-03-20 [Attention] Flash Attention 3 - fp8 (#14570)
1	1	cmake/external_projects/vllm_flash_attn.cmake
68	8	tests/kernels/test_flash_attn.py
8	4	vllm/attention/__init__.py
1	0	vllm/attention/backends/abstract.py
68	11	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/mla/common.py
0	34	vllm/attention/backends/utils.py
7	2	vllm/attention/layer.py
6	1	vllm/envs.py
42	0	vllm/fa_utils.py
13	3	vllm/model_executor/layers/quantization/kv_cache.py
12	9	vllm/platforms/cuda.py
40	1	vllm/v1/attention/backends/flash_attn.py
4	0	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/worker/gpu_model_runner.py

[ae65f3e23] Chauncey 2025-03-20 [Misc]fixed disable these http request logs (#14754)
3	0	vllm/entrypoints/openai/api_server.py
3	0	vllm/entrypoints/openai/cli_args.py

[34868b106] Roger Wang 2025-03-19 [Doc] Update Mistral Small 3.1/Pixtral example (#15184)
8	2	examples/offline_inference/{pixtral.py => mistral-small.py}

[1f16b7fe7] Russell Bryant 2025-03-20 [Core][V0] Add guidance backend for structured output (#14589)
6	5	benchmarks/benchmark_serving_structured_output.py
1	0	requirements/common.txt
3	1	tests/entrypoints/llm/test_guided_generate.py
3	1	tests/model_executor/test_guided_processors.py
3	1	vllm/config.py
22	5	vllm/model_executor/guided_decoding/__init__.py
44	0	vllm/model_executor/guided_decoding/guidance_decoding.py
85	0	vllm/model_executor/guided_decoding/guidance_logits_processors.py

[b88be2216] Jennifer Zhao 2025-03-19 [Benchmark] Allow oversample request in benchmark dataset (#15170)
54	3	benchmarks/README.md
85	56	benchmarks/benchmark_dataset.py

[d8c6d7d6b] Nicolò Lucchesi 2025-03-20 [V1][TPU] Support V1 Sampler for ragged attention (#14227)
94	0	tests/v1/tpu/test_sampler.py
15	1	vllm/v1/sample/ops/topk_topp_sampler.py
0	0	vllm/v1/sample/tpu/__init__.py
159	0	vllm/v1/sample/tpu/metadata.py
154	0	vllm/v1/sample/tpu/sampler.py
113	54	vllm/v1/worker/tpu_model_runner.py

[40828ce5f] Wang, Yi 2025-03-20 fix "Total generated tokens:" is 0 if using --backend tgi and --endpo… (#14673)
5	1	benchmarks/backend_request_func.py

[ffa443afe] Cyrus Leung 2025-03-20 [Bugfix] Fix embedding assignment for InternVL-based models (#15086)
0	1	examples/offline_inference/vision_language.py
0	2	examples/offline_inference/vision_language_multi_image.py
1	1	vllm/model_executor/models/gemma3_mm.py
6	16	vllm/model_executor/models/h2ovl.py
104	69	vllm/model_executor/models/internvl.py
8	13	vllm/model_executor/models/nvlm_d.py
4	4	vllm/multimodal/processing.py

[70e500cad] Jovan Sardinha 2025-03-19 Fix broken tests (#14713)
1	0	.buildkite/test-pipeline.yaml
1	1	requirements/test.in
17	13	tests/compile/test_pass_manager.py

[4cb1c05c9] Rui Qiao 2025-03-19 [Doc] Clarify run vllm only on one node in distributed inference (#15148)
1	1	docs/source/serving/distributed_serving.md

[c47aafa37] Nick Hill 2025-03-19 [BugFix] Lazily import XgrammarBackend to avoid early cuda init (#15171)
3	1	vllm/v1/structured_output/__init__.py

[cfbca8a2f] Alexander Matveev 2025-03-19 [V1] TPU - Tensor parallel MP support (#15059)
1	1	vllm/config.py
36	14	vllm/distributed/device_communicators/tpu_communicator.py

[0fe560987] Simon Mo 2025-03-19 [Docs] Annouce Ollama and Singapore Meetups (#15161)
8	0	README.md

[22d33baca] Nick Hill 2025-03-19 [FrontEnd][Perf] `merge_async_iterators` fast-path for single-prompt requests (#15150)
5	0	vllm/utils.py

[b0e96aaeb] iefgnoix 2025-03-19 [V1][TPU] Change kv cache shape. (#15145)
6	6	requirements/tpu.txt
7	10	vllm/v1/attention/backends/pallas.py

[8310e0b59] Wang Ran (汪然) 2025-03-20 simple bugfix: Update stats.py (#15139)
1	1	vllm/v1/metrics/stats.py

[26dd972ad] maobaolong 2025-03-20 [FEAT]Support reset prefix cache by specified device (#15003)
4	2	vllm/core/block/cpu_gpu_block_allocator.py
1	1	vllm/core/block/interfaces.py
2	2	vllm/core/block_manager.py
3	3	vllm/core/interfaces.py
2	2	vllm/core/placeholder_block_space_manager.py
2	2	vllm/core/scheduler.py
4	3	vllm/engine/async_llm_engine.py
2	2	vllm/engine/llm_engine.py
4	3	vllm/engine/multiprocessing/__init__.py
4	3	vllm/engine/multiprocessing/client.py
3	2	vllm/engine/protocol.py
4	3	vllm/entrypoints/llm.py
7	3	vllm/entrypoints/openai/api_server.py
5	2	vllm/v1/engine/async_llm.py
2	1	vllm/v1/engine/llm_engine.py

[61c7a1b85] Murali Andoorveedu 2025-03-19 [V1] Minor V1 async engine test refactor (#15075)
9	16	tests/v1/engine/test_async_llm.py

[374ee287d] Alessandro Sangiorgi 2025-03-19 [Frontend] Remove custom_cache_manager (#13791)
0	8	vllm/executor/multiproc_worker_utils.py
1	8	vllm/triton_utils/__init__.py
0	55	vllm/triton_utils/custom_cache_manager.py

[a4d83661d] Kero Liang 2025-03-19 [Misc] Update the "the first vLLM China Meetup" slides link to point to the first page (#15134)
1	1	README.md

[8363cd093] Jan Kaniecki 2025-03-19  [Bugfix] Adjust mllama to regional compilation (#15112)
4	7	vllm/model_executor/models/mllama.py

[6c5a3195d] Aaron Pham 2025-03-19 [Misc][Benchmark] Add support for different `tokenizer_mode` (#15040)
12	2	benchmarks/benchmark_serving_structured_output.py
1	0	benchmarks/run_structured_output_benchmark.sh

[073d1ed35] Marc-Alexandre Côté 2025-03-19 [Doc] Update tip info on using latest transformers when creating a custom Dockerfile  (#15070)
3	3	docs/source/deployment/docker.md

[3d446433e] Cyrus Leung 2025-03-19 [Bugfix] Fix size calculation of processing cache (#15114)
46	2	tests/multimodal/test_processing.py
46	14	vllm/multimodal/processing.py

[1fe0fd12d] Cyrus Leung 2025-03-19 [Misc] Avoid unnecessary HF `do_rescale` warning when passing dummy data (#15107)
2	2	vllm/multimodal/profiling.py

[dafb4e504] Roger Wang 2025-03-19 [V1][Bugfix] Fix oracle for device checking (#15104)
7	0	vllm/engine/arg_utils.py

[68cf1601d] Kunshang Ji 2025-03-19 [CI][Intel GPU] update XPU dockerfile and CI script (#15109)
5	4	.buildkite/run-xpu-test.sh
3	9	Dockerfile.xpu

[61f412187] Cyrus Leung 2025-03-19 [Bugfix] Re-enable Gemma3 for V1 (#14980)
2	5	docs/source/models/supported_models.md
54	1	tests/multimodal/test_processing.py
211	36	vllm/model_executor/models/gemma3_mm.py
18	51	vllm/model_executor/models/llava.py
4	11	vllm/model_executor/models/molmo.py
16	48	vllm/model_executor/models/pixtral.py
49	1	vllm/model_executor/models/vision.py
65	22	vllm/multimodal/processing.py

[05ccd0aa3] Woosuk Kwon 2025-03-18 [V1] Ensure using int64 for sampled token ids (#15065)
8	1	vllm/v1/sample/sampler.py

[f690372b6] Cyrus Leung 2025-03-19 [Core] Update dtype detection and defaults (#14858)
1	1	tests/compile/test_basic_correctness.py
63	53	tests/conftest.py
0	1	tests/entrypoints/llm/test_chat.py
0	2	tests/entrypoints/openai/test_audio.py
0	2	tests/entrypoints/openai/test_video.py
0	2	tests/entrypoints/openai/test_vision.py
0	2	tests/entrypoints/openai/test_vision_embedding.py
3	3	tests/entrypoints/test_chat_utils.py
2	13	tests/models/decoder_only/audio_language/test_ultravox.py
3	36	tests/models/decoder_only/vision_language/test_models.py
0	3	tests/models/decoder_only/vision_language/vlm_utils/core.py
49	42	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
2	9	tests/models/decoder_only/vision_language/vlm_utils/types.py
28	24	tests/models/embedding/vision_language/test_dse_qwen2_vl.py
1	2	tests/models/embedding/vision_language/test_llava_next.py
1	2	tests/models/embedding/vision_language/test_phi3v.py
1	6	tests/models/encoder_decoder/vision_language/test_mllama.py
4	7	tests/models/utils.py
3	3	tests/multimodal/test_processing.py
2	2	tests/tensorizer_loader/test_tensorizer.py
1	1	tests/v1/engine/test_llm_engine.py
11	11	vllm/config.py

[8b3e94a35] Brayden Zhong 2025-03-19 [Model] Remove duplicated message check in Mistral chat completion request (#15069)
0	4	vllm/transformers_utils/tokenizers/mistral.py

[437f9162d] Julien Denize 2025-03-19 [Model] Pixtral: Remove layer instantiation duplication (#15053)
0	9	vllm/model_executor/models/pixtral.py

[4f065f12f] Cody Yu 2025-03-18 [Misc][V1] Skip device checking if not available (#15061)
4	0	vllm/engine/arg_utils.py

[228b768db] Jennifer Zhao 2025-03-18 [Doc] Minor v1_user_guide update (#15064)
2	0	docs/source/getting_started/v1_user_guide.md

[027827cc1] Chujie Zheng 2025-03-19 fix long dtype in topk sampling (#15049)
1	1	vllm/v1/sample/sampler.py

[72a8639b6] Alexander Matveev 2025-03-18 [V1] TPU - CI/CD use smaller model (#15054)
7	7	.buildkite/run-tpu-v1-test.sh
3	2	tests/v1/tpu/test_basic.py

[99abb8b65] Woosuk Kwon 2025-03-18 [V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels (#14930)
166	65	tests/v1/sample/test_rejection_sampler.py
0	1	vllm/envs.py
1	1	vllm/v1/outputs.py
30	0	vllm/v1/sample/ops/utils.py
506	292	vllm/v1/sample/rejection_sampler.py
61	0	vllm/v1/spec_decode/metadata.py
0	1	vllm/v1/spec_decode/utils.py
134	71	vllm/v1/worker/gpu_model_runner.py

[3a1e64815] Russell Bryant 2025-03-18 [V1] Refactor Structured Output for multiple backends (#14694)
15	10	vllm/v1/engine/processor.py
31	92	vllm/v1/structured_output/__init__.py
89	0	vllm/v1/structured_output/backend_types.py
143	0	vllm/v1/structured_output/backend_xgrammar.py
0	77	vllm/v1/structured_output/grammar.py
12	6	vllm/v1/structured_output/request.py

[46c759c16] Jee Jee Li 2025-03-19 [Bugfix] Fix LoRA extra vocab size (#15047)
0	1	examples/offline_inference/audio_language.py
0	1	examples/offline_inference/vision_language.py
0	1	examples/offline_inference/vision_language_multi_image.py
0	1	tests/models/decoder_only/vision_language/test_phi4mm.py
1	1	vllm/config.py

[179a619c2] Isotr0py 2025-03-18 [Bugfix] Fix broken CPU quantization due to triton import (#15038)
4	1	vllm/model_executor/layers/quantization/gguf.py

[452e8fd96] yury-tokpanov 2025-03-18 [MODEL] Add support for Zamba2 models (#13185)
5	0	docs/source/models/supported_models.md
29	22	tests/models/decoder_only/language/test_hybrid.py
2	0	tests/models/registry.py
14	0	vllm/config.py
0	1	vllm/model_executor/layers/mamba/mamba_mixer2.py
0	2	vllm/model_executor/models/bamba.py
0	2	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/registry.py
1031	0	vllm/model_executor/models/zamba2.py

[8b793f7ec] ekuznetsov139 2025-03-18 MI325 configs, fused_moe_kernel bugfix (#14987)
200	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=1024,device_name=AMD_Instinct_MI325X,block_shape=[128,128].json
200	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=1024,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
200	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
200	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
200	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
6	1	vllm/model_executor/layers/fused_moe/fused_moe.py
164	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=8192,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=8192,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=8192,K=1536,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=8192,K=1536,device_name=AMD_Instinct_MI325_OAM,dtype=fp8_w8a8,block_shape=[128,128].json

[af35d3a3c] Nicolò Lucchesi 2025-03-18 [TPU][V1][Bugfix] Fix chunked prefill with padding (#15037)
3	0	vllm/v1/worker/tpu_model_runner.py

[3b457143d] Simon Mo 2025-03-18 [Bugfix] Register serializers for V0 MQ Engine (#15009)
8	3	vllm/engine/multiprocessing/engine.py
5	0	vllm/entrypoints/openai/api_server.py

[ab656f2c2] Cyrus Leung 2025-03-18 [Bugfix] Loosen type check to avoid errors in V1 (#15021)
5	7	vllm/model_executor/models/blip2.py
3	4	vllm/model_executor/models/chameleon.py
1	1	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/glm4v.py
4	2	vllm/model_executor/models/internvl.py
6	9	vllm/model_executor/models/llava_next_video.py
1	4	vllm/model_executor/models/llava_onevision.py
4	6	vllm/model_executor/models/paligemma.py
3	3	vllm/model_executor/models/qwen_vl.py

[64fc2193d] Serena 2025-03-18 [Misc][Docs] fix the comments of KV_T and CACHE_T in CALL_RESHAPE_AND_CACHE_XX macros (#14347)
6	6	csrc/cache_kernels.cu

[dd732028f] Sebastian Schoennenbeck 2025-03-18 [Bugfix][Frontend] Fix validation of `logprobs` in `ChatCompletionRequest` (#14352)
1	1	vllm/entrypoints/openai/protocol.py

[414919138] hoshi-hiyouga 2025-03-18 [Bugfix] torchrun compatibility (#14899)
3	1	vllm/config.py
20	1	vllm/distributed/parallel_state.py

[db7c8ca91] Jee Jee Li 2025-03-18 [Misc] Embedding model support LoRA (#14935)
30	2	vllm/lora/models.py

[f863ffc96] Patrick von Platen 2025-03-18 [Mistral-Small 3.1] Update docs and tests (#14977)
1	1	docs/source/models/supported_models.md
5	5	examples/offline_inference/pixtral.py
27	53	tests/models/decoder_only/vision_language/test_pixtral.py
1	0	tests/models/fixtures/mistral_small_3_chat.json
0	1	tests/models/fixtures/pixtral_chat_engine.json

[400d483e8] Varun Sundar Rabindranath 2025-03-18 [Kernels] LoRA - Retire SGMV and BGMV Kernels (#14685)
63	373	benchmarks/kernels/benchmark_lora.py
84	359	tests/lora/test_punica_ops.py
6	10	vllm/lora/ops/triton_ops/__init__.py
0	188	vllm/lora/ops/triton_ops/bgmv_expand.py
0	207	vllm/lora/ops/triton_ops/bgmv_expand_slice.py
0	168	vllm/lora/ops/triton_ops/bgmv_shrink.py
9	9	vllm/lora/ops/triton_ops/{v1/v1_expand.py => lora_expand.py}
5	5	vllm/lora/ops/triton_ops/{v1/v1_kernel_metadata.py => lora_kernel_metadata.py}
17	17	vllm/lora/ops/triton_ops/{v1/v1_shrink.py => lora_shrink.py}
0	249	vllm/lora/ops/triton_ops/sgmv_expand.py
0	224	vllm/lora/ops/triton_ops/sgmv_shrink.py
0	46	vllm/lora/ops/triton_ops/utils.py
0	11	vllm/lora/ops/triton_ops/v1/__init__.py
59	225	vllm/lora/punica_wrapper/punica_gpu.py
4	3	vllm/v1/worker/lora_model_runner_mixin.py

[d1695758b] Shanshan Shen 2025-03-18 [Doc][V1] Fix V1 APC doc (#14920)
2	2	docs/source/design/v1/prefix_caching.md

[53a0cf8b9] Liangfu Chen 2025-03-18 [Neuron] trim attention kernel tests to fit trn1.2x instance (#14988)
1	1	tests/neuron/1_core/test_prefix_prefill.py

[5eeabc2a4] Tristan Leclercq 2025-03-18 [Bugfix] Fix bnb quantization for models with both HF-format and Mistral-format weights (#14950)
2	0	tests/quantization/test_bitsandbytes.py
25	6	vllm/model_executor/model_loader/loader.py

[18551e820] Alexander Matveev 2025-03-17 [V1] TPU - Fix CI/CD runner (#14974)
0	25	.buildkite/run-tpu-test.sh
16	7	.buildkite/run-tpu-v1-test.sh
39	24	tests/tpu/test_compilation.py
14	9	tests/tpu/test_custom_dispatcher.py

[e41e16026] Robert Shaw 2025-03-17 [V1] Guard Against Main Thread Usage (#14972)
6	0	vllm/engine/arg_utils.py

[b89fb2a4a] Cyrus Leung 2025-03-18 [CI/Build] Use `AutoModelForImageTextToText` to load VLMs in tests (#14945)
14	14	tests/models/decoder_only/vision_language/test_models.py
2	2	tests/models/embedding/vision_language/test_llava_next.py
3	3	tests/models/encoder_decoder/vision_language/test_mllama.py

[5340b0e22] Roger Wang 2025-03-17 [Bugfix] Fix interface for Olmo2 on V1 (#14976)
9	5	vllm/model_executor/models/olmo2.py

[37e380613] Roger Wang 2025-03-17 [Bugfix] Make Gemma3 MM V0 only for now (#14971)
4	1	docs/source/models/supported_models.md
2	2	vllm/model_executor/models/gemma3_mm.py

[c0efdd655] Aaron Pham 2025-03-17 [Fix][Structured Output] using vocab_size to construct matcher (#14868)
1	0	.buildkite/test-pipeline.yaml
1	1	requirements/common.txt
8	4	tests/model_executor/test_guided_processors.py
0	3	tests/v1/entrypoints/llm/test_struct_output_generate.py
1	9	vllm/model_executor/guided_decoding/__init__.py
58	67	vllm/model_executor/guided_decoding/xgrammar_decoding.py
1	1	vllm/v1/structured_output/__init__.py

[aaaec52ad] Quentin 2025-03-17 [Bugfix][Model] Mixtral: use unused head_dim config argument (#14961)
5	1	vllm/model_executor/models/mixtral.py
5	1	vllm/model_executor/models/mixtral_quant.py

[e1eb45d39] Tyler Michael Smith 2025-03-17 [Bugfix] Fix precommit - line too long in pixtral.py (#14960)
1	1	requirements/test.txt
5	4	vllm/model_executor/models/pixtral.py

[89fca671f] Simon Mo 2025-03-17 [V1] Default MLA to V1 (#14921)
1	5	vllm/engine/arg_utils.py

[d20b0c139] Patrick von Platen 2025-03-17 Add patch merger (#14957)
1	1	requirements/common.txt
1	1	requirements/docs.txt
2	2	requirements/test.in
162	4	vllm/model_executor/models/pixtral.py

[166a168b0] Cyrus Leung 2025-03-17 [Doc] Fix misleading log during multi-modal profiling (#14955)
3	1	vllm/multimodal/profiling.py

[2bb0e1a79] vllmellm 2025-03-17 [Bugfix][ROCm] running new process using spawn method for rocm in tests. (#14810)
5	5	tests/basic_correctness/test_cumem.py
2	2	tests/compile/test_full_graph.py
2	2	tests/distributed/test_expert_parallel.py
4	4	tests/distributed/test_pipeline_parallel.py
2	2	tests/distributed/test_pp_cudagraph.py
2	2	tests/entrypoints/llm/test_collective_rpc.py
4	5	tests/lora/test_chatglm3_tp.py
6	7	tests/lora/test_llama_tp.py
5	4	tests/lora/test_minicpmv_tp.py
4	5	tests/lora/test_transfomers_model.py
15	15	tests/models/decoder_only/vision_language/test_models.py
6	6	tests/models/decoder_only/vision_language/vlm_utils/case_filtering.py
2	2	tests/models/encoder_decoder/audio_language/test_whisper.py
5	5	tests/models/test_oot_registration.py
3	3	tests/models/test_registry.py
7	6	tests/quantization/test_bitsandbytes.py
13	13	tests/spec_decode/e2e/test_multistep_correctness.py
2	2	tests/test_utils.py
77	3	tests/utils.py
5	4	tests/v1/engine/test_engine_core.py
3	2	tests/v1/engine/test_engine_core_client.py

[6eaf1e5c5] Cyrus Leung 2025-03-17 [Misc] Add `--seed` option to offline multi-modal examples (#14934)
5	2	.buildkite/test-pipeline.yaml
88	44	examples/offline_inference/audio_language.py
37	11	examples/offline_inference/encoder_decoder_multimodal.py
298	157	examples/offline_inference/vision_language.py
20	11	examples/offline_inference/vision_language_embedding.py
89	90	examples/offline_inference/vision_language_multi_image.py

[868a8c5b2] Cyrus Leung 2025-03-17 [Bugfix] Fix Ultravox on V1 (#14929)
25	17	vllm/model_executor/models/ultravox.py

[b4ad56c1b] iefgnoix 2025-03-17 [V1][TPU] Apply the ragged paged attention kernel fix and remove the padding. (#14846)
6	6	requirements/tpu.txt
2	5	vllm/v1/worker/tpu_model_runner.py

[69698f257] kushanam 2025-03-17 fix minor miscalled method (#14327)
[cd0cd8510] Lu Fang 2025-03-17 [MISC] More AMD unused var clean up (#14926)
4	4	csrc/rocm/attention.cu

[0a74bfce9] Russell Bryant 2025-03-17 setup.py: drop assumption about local `main` branch (#14692)
16	14	setup.py

[dd3b86585] Chen Zhang 2025-03-17 [Doc] Add vLLM Beijing meetup slide (#14938)
1	10	README.md

[9b87a579a] Yan Ma 2025-03-17 [Misc][XPU] Use None as device capacity for XPU (#14932)
5	4	vllm/platforms/xpu.py

[b539222d4] Cyrus Leung 2025-03-17 [V1] Remove input cache client (#14864)
6	0	vllm/inputs/preprocess.py
1	1	vllm/v1/engine/__init__.py
10	112	vllm/v1/engine/mm_input_cache.py
23	57	vllm/v1/engine/processor.py
8	31	vllm/v1/worker/gpu_model_runner.py

[8d6cf8952] Lily Liu 2025-03-16 [V1] [Spec Decode] Support random sampling for spec decode (#13933)
251	56	tests/v1/sample/test_rejection_sampler.py
274	128	vllm/v1/sample/rejection_sampler.py
0	8	vllm/v1/sample/sampler.py
22	0	vllm/v1/spec_decode/utils.py
25	6	vllm/v1/worker/gpu_model_runner.py

[583a9778e] Simon Mo 2025-03-16 [Benchmark] Do not save detailed info to json by default (#14879)
4	1	benchmarks/backend_request_func.py
15	0	benchmarks/benchmark_serving.py

[a73e183e3] Sibi 2025-03-17 [Misc] Replace os environ to monkeypatch in test suite (#14516)
1	1	.buildkite/test-pipeline.yaml
63	52	tests/basic_correctness/test_basic_correctness.py
95	79	tests/basic_correctness/test_chunked_prefill.py
35	37	tests/basic_correctness/test_cumem.py
106	101	tests/compile/test_basic_correctness.py
103	10	tests/compile/test_full_graph.py
0	93	tests/compile/utils.py
1	1	tests/conftest.py
63	22	tests/distributed/test_comm_ops.py
101	82	tests/distributed/test_custom_all_reduce.py
35	29	tests/distributed/test_pipeline_partition.py
26	16	tests/distributed/test_pp_cudagraph.py
2	2	tests/entrypoints/llm/test_accuracy.py
27	22	tests/entrypoints/offline_mode/test_offline_mode.py
3	2	tests/entrypoints/openai/correctness/test_lmeval.py
77	54	tests/kernels/test_attention_selector.py
30	30	tests/kernels/test_awq.py
10	8	tests/kernels/test_rocm_attention_selector.py
32	32	tests/kernels/utils.py
0	0	tests/kv_transfer/{disagg_test.py => test_disagg.py}
0	0	tests/kv_transfer/{module_test.py => test_module.py}
70	66	tests/models/decoder_only/language/test_fp8.py
53	43	tests/models/embedding/language/test_gritlm.py
75	61	tests/models/test_oot_registration.py
17	14	tests/mq_llm_engine/test_error_handling.py
119	115	tests/multi_step/test_correctness_async_llm.py
164	161	tests/multi_step/test_correctness_llm.py
42	42	tests/neuron/1_core/test_block_table.py
163	159	tests/neuron/1_core/test_prefix_prefill.py
7	6	tests/plugins_tests/test_platform_plugins.py
33	29	tests/plugins_tests/test_scheduler_plugins.py
67	58	tests/prefix_caching/test_prefix_caching.py
4	12	tests/test_regression.py
32	31	tests/test_utils.py
13	12	tests/tpu/test_custom_dispatcher.py
159	138	tests/tracing/test_tracing.py
9	2	tests/utils.py
9	2	tests/v1/e2e/test_ngram_spec_decode.py
7	4	tests/v1/engine/test_async_llm.py
5	5	tests/v1/engine/test_engine_core.py
3	2	tests/v1/engine/test_engine_core_client.py
125	115	tests/v1/sample/test_logprobs.py
11	5	tests/v1/tpu/test_basic.py

[1e799b7ec] Lucas Wilkinson 2025-03-16 [BugFix] Fix MLA + V1 + TP==1 causing reinitialization of cuda context (#14910)
1	1	vllm/platforms/cuda.py

[7f6c5ee06] Woosuk Kwon 2025-03-16 [V1][Minor] Add __repr__ to ConstantList (#14907)
3	0	vllm/v1/utils.py

[faa027573] Woosuk Kwon 2025-03-16 [V1] Optimize the overhead of rewinding (#14905)
5	6	vllm/v1/worker/gpu_model_runner.py

[8a5a9b70d] Cyrus Leung 2025-03-17 [CI/Build] Update defaults for test reproducibility (#14893)
16	2	tests/conftest.py

[bb3aeddfa] Robert Shaw 2025-03-16 [CI] Nightly Tests (#14898)
1	0	tests/models/decoder_only/language/test_mistral.py
13	2	tests/tool_use/utils.py

[aecc780db] Robert Shaw 2025-03-16 [V1] Enable Entrypoints Tests (#14903)
1	0	.buildkite/test-pipeline.yaml
3	0	tests/v1/entrypoints/llm/test_struct_output_generate.py

[90df7f23a] Vadim Gimpelson 2025-03-17 [Doc] Add guidance for using `ccache` with `pip install -e .` in doc (#14901)
2	0	docs/source/getting_started/installation/gpu/cuda.inc.md

[b9b5bdfc7] Rui Qiao 2025-03-16 [Misc] Catching Ray Compiled Graph PP test failures for V1 (#14847)
15	5	tests/distributed/test_pipeline_parallel.py

[31060b275] Woosuk Kwon 2025-03-16 [V1][BugFix] Detect interleaved sliding window attention (#14896)
9	2	vllm/v1/worker/gpu_model_runner.py

[fc1f67715] Nick Hill 2025-03-16 [BugFix][V1] Fix overhead related to bad_words sampling when not in use (#14894)
3	2	tests/v1/worker/test_gpu_input_batch.py
4	3	vllm/sampling_params.py
3	2	vllm/v1/worker/gpu_input_batch.py

[f6137adbc] Cyrus Leung 2025-03-17 Revert "[Bugfix] Limit profiling run sequence length by max_model_len (#14785) (#14892)
0	5	vllm/inputs/registry.py
0	1	vllm/worker/enc_dec_model_runner.py
0	1	vllm/worker/model_runner.py
0	1	vllm/worker/openvino_model_runner.py
0	1	vllm/worker/xpu_model_runner.py

[e53b1350f] Cyrus Leung 2025-03-17 [Bugfix] Explicitly disable Phi-4-multimodal in V1 (#14889)
3	2	vllm/model_executor/models/phi4mm.py

[d30aa7e9e] Kyle Sayers 2025-03-16 [Bugfix] Limit profiling run sequence length by max_model_len (#14785)
5	0	vllm/inputs/registry.py
1	0	vllm/worker/enc_dec_model_runner.py
1	0	vllm/worker/model_runner.py
1	0	vllm/worker/openvino_model_runner.py
1	0	vllm/worker/xpu_model_runner.py

[d1ad2a57a] Lily Liu 2025-03-16 [V1] [Spec Decode] Fix ngram tests (#14878)
29	24	tests/v1/spec_decode/test_ngram.py

[b82662d95] Nick Hill 2025-03-15 [BugFix] Fix torch distributed stateless PG backend init (#14870)
5	0	examples/offline_inference/data_parallel.py
3	3	vllm/distributed/utils.py

[71c1e0710] Simon Mo 2025-03-15 [Kernel] Add more tuned configs (#14877)
146	0	vllm/model_executor/layers/fused_moe/configs/E=160,N=192,device_name=NVIDIA_A800-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=64,device_name=NVIDIA_A800-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_A800-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=2560,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=2560,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=2560,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=320,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_A800-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_GeForce_RTX_4090,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H200.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=NVIDIA_H200,dtype=fp8_w8a8.json
3	0	vllm/model_executor/layers/fused_moe/configs/README
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_A800-SXM4-80GB,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H20,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
26	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_L20Y,dtype=fp8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_H20,dtype=int8_w8a8,block_shape=[128,128].json

[b30c75dda] Roger Wang 2025-03-15 [V1] Remove V0 fallback for mistral-tokenizer (#14873)
0	7	vllm/engine/arg_utils.py

[def232e12] Isotr0py 2025-03-16 [VLM] Clean up Phi-4-MM ViT implementation (#14812)
1	0	requirements/test.in
2	0	requirements/test.txt
229	0	tests/models/decoder_only/vision_language/test_phi4mm.py
2	2	vllm/model_executor/models/aria.py
49	8	vllm/model_executor/models/idefics2_vision_model.py
33	12	vllm/model_executor/models/phi4mm.py
0	1966	vllm/model_executor/models/vision_siglip_navit.py

[3453b964a] Roger Wang 2025-03-15 [Misc][Doc] Minor benchmark README update (#14874)
2	2	benchmarks/README.md

[61c6a5a79] Rémi Delacourt 2025-03-15 [VLM] Merged multi-modal processor for Pixtral (#12211)
18	6	examples/offline_inference/pixtral.py
149	51	tests/models/multimodal/processing/test_common.py
50	76	vllm/model_executor/models/llava.py
2	4	vllm/model_executor/models/molmo.py
4	5	vllm/model_executor/models/paligemma.py
374	208	vllm/model_executor/models/pixtral.py
9	5	vllm/multimodal/processing.py
15	4	vllm/transformers_utils/tokenizer.py
1	1	vllm/utils.py

[74bc397b0] Jun Duan 2025-03-15 [Core] Expose API endpoint `/is_sleeping` (#14312)
7	0	tests/entrypoints/openai/test_sleep.py
3	0	vllm/engine/async_llm_engine.py
3	0	vllm/engine/llm_engine.py
14	2	vllm/engine/multiprocessing/__init__.py
25	2	vllm/engine/multiprocessing/client.py
13	0	vllm/engine/multiprocessing/engine.py
5	0	vllm/engine/protocol.py
6	0	vllm/entrypoints/openai/api_server.py
3	0	vllm/v1/engine/async_llm.py
3	0	vllm/v1/engine/core.py
15	0	vllm/v1/engine/core_client.py
3	0	vllm/v1/engine/llm_engine.py

[f58aea002] Kunshang Ji 2025-03-15 [CI][Intel GPU] refine intel GPU ci docker build (#14860)
14	3	.buildkite/run-xpu-test.sh

[3556a4143] Cyrus Leung 2025-03-15 [VLM] Limit multimodal input cache by memory (#14805)
1	1	.pre-commit-config.yaml
1	0	requirements/common.txt
1	0	requirements/docs.txt
1	1	tests/models/multimodal/processing/test_common.py
5	6	vllm/envs.py
79	0	vllm/jsontree.py
2	1	vllm/model_executor/models/llava.py
2	1	vllm/model_executor/models/molmo.py
2	1	vllm/multimodal/inputs.py
43	8	vllm/multimodal/processing.py
2	2	vllm/multimodal/registry.py
0	16	vllm/utils.py
20	18	vllm/v1/engine/mm_input_cache.py

[9ed6ee92d] Bryan Lu 2025-03-14 [Bugfix] EAGLE output norm bug (#14464)
1	1	docs/source/features/spec_decode.md
93	0	examples/offline_inference/eagle.py
6	1	vllm/engine/llm_engine.py
5	0	vllm/engine/output_processor/multi_step.py
1	1	vllm/model_executor/models/eagle.py
29	15	vllm/sequence.py
1	1	vllm/spec_decode/spec_decode_worker.py
16	16	vllm/spec_decode/util.py

[ee3778d5f] Russell Bryant 2025-03-15 [Build/CI] Upgrade jinja2 to get 3 moderate CVE fixes (#14839)
1	1	requirements/build.txt
1	1	requirements/rocm-build.txt
1	1	requirements/test.txt
1	1	requirements/tpu.txt
1	1	requirements/xpu.txt

[aaacf1732] Jennifer Zhao 2025-03-14 [Doc] V1 user guide (#13991)
159	0	docs/source/getting_started/v1_user_guide.md
2	0	docs/source/index.md

[4c7629cae] Aaron Pham 2025-03-15 [V1][Structured Output] calculate vocab_size eagerly (#14851)
1	1	vllm/v1/structured_output/__init__.py

[e0fdfa160] Jee Jee Li 2025-03-15 [CI/Build] Delete LoRA bias test (#14849)
0	5	tests/lora/conftest.py
0	63	tests/lora/test_lora_bias_e2e.py

[5952d8ab6] Lucas Wilkinson 2025-03-15 [Attention] Get rid of mla cache alignment (#14842)
11	28	tests/kernels/test_cache.py
0	10	vllm/envs.py
0	6	vllm/utils.py
3	39	vllm/worker/cache_engine.py

[a2ae49658] Li, Jiang 2025-03-15 [CPU] Support FP8 KV cache (#14741)
21	17	csrc/cpu/cache.cpp
9	0	csrc/cpu/cpu_types_x86.hpp
1	1	docs/source/getting_started/installation/cpu.md
2	2	tests/basic_correctness/test_chunked_prefill.py
61	0	tests/models/decoder_only/language/test_fp8.py
5	4	vllm/attention/backends/torch_sdpa.py
19	11	vllm/platforms/cpu.py
4	1	vllm/worker/cpu_worker.py

[877e35226] Simon Mo 2025-03-14 [Docs] Add new East Coast vLLM Meetup slides to README and meetups.md (#14852)
1	0	README.md
1	0	docs/source/community/meetups.md

[d4d93db2c] Robert Shaw 2025-03-15 [V1] V1 Enablement Oracle  (#13726)
2	2	.buildkite/lm-eval-harness/configs/Minitron-4B-Base-FP8.yaml
5	0	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
18	16	.buildkite/test-pipeline.yaml
11	0	tests/async_engine/conftest.py
5	1	tests/async_engine/test_api_server.py
9	0	tests/async_engine/test_async_llm_engine.py
9	0	tests/basic_correctness/test_chunked_prefill.py
7	0	tests/basic_correctness/test_cpu_offload.py
9	0	tests/basic_correctness/test_preemption.py
14	0	tests/compile/conftest.py
20	0	tests/conftest.py
11	0	tests/core/conftest.py
0	0	tests/{engine/output_processor => detokenizer}/__init__.py
10	0	tests/detokenizer/conftest.py
1	0	tests/{engine/test_detokenization.py => detokenizer/test_disable_detokenization.py}
0	0	tests/{engine/output_processor => detokenizer}/test_stop_checker.py
0	0	tests/{engine => detokenizer}/test_stop_reason.py
141	0	tests/detokenizer/test_stop_strings.py
12	0	tests/distributed/test_pipeline_parallel.py
9	0	tests/encoder_decoder/test_e2e_correctness.py
11	0	tests/engine/conftest.py
1	1	tests/engine/{output_processor/test_multi_step.py => test_multi_step_output_processor.py}
0	165	tests/engine/test_stop_strings.py
9	0	tests/entrypoints/llm/test_lazy_outlines.py
0	3	tests/entrypoints/openai/test_chat_echo.py
0	3	tests/entrypoints/openai/test_root_path.py
21	7	tests/kernels/test_attention_selector.py
10	0	tests/kernels/test_encoder_decoder_attn.py
2	1	tests/kernels/test_rocm_attention_selector.py
4	0	tests/lora/test_llama_tp.py
2	2	tests/lora/test_lora_functions.py
3	0	tests/lora/test_lora_manager.py
9	0	tests/metrics/test_metrics.py
10	10	tests/models/decoder_only/language/test_gguf.py
6	8	tests/models/decoder_only/language/test_hybrid.py
0	7	tests/models/decoder_only/language/test_mamba.py
11	10	tests/models/decoder_only/language/test_mistral.py
9	7	tests/models/decoder_only/language/test_models.py
6	1	tests/models/decoder_only/vision_language/test_awq.py
60	29	tests/models/decoder_only/vision_language/test_models.py
11	1	tests/models/decoder_only/vision_language/test_qwen2_vl.py
0	7	tests/models/embedding/language/test_cls_models.py
0	7	tests/models/embedding/language/test_embedding.py
5	3	tests/models/registry.py
11	3	tests/models/test_initialization.py
5	3	tests/models/test_oot_registration.py
11	0	tests/mq_llm_engine/conftest.py
11	0	tests/plugins_tests/conftest.py
4	1	tests/prefix_caching/test_disable_sliding_window.py
9	0	tests/prefix_caching/test_prefix_caching.py
8	0	tests/quantization/test_compressed_tensors.py
7	0	tests/quantization/test_cpu_offload.py
6	1	tests/quantization/test_fp8.py
4	2	tests/quantization/test_gptq_dynamic.py
3	0	tests/quantization/test_lm_head.py
3	1	tests/quantization/test_quark.py
3	1	tests/quantization/test_register_quantization_config.py
8	0	tests/samplers/test_beam_search.py
7	0	tests/samplers/test_ignore_eos.py
8	0	tests/samplers/test_logits_processor.py
9	0	tests/samplers/test_logprobs.py
7	0	tests/samplers/test_no_bad_words.py
6	0	tests/samplers/test_ranks.py
9	0	tests/samplers/test_rejection_sampler.py
8	0	tests/samplers/test_sampler.py
3	1	tests/samplers/test_seeded_generate.py
8	0	tests/samplers/test_typical_acceptance_sampler.py
11	0	tests/spec_decode/conftest.py
8	0	tests/tensorizer_loader/conftest.py
2	0	tests/test_regression.py
4	1	tests/test_utils.py
5	0	tests/tokenization/test_detokenize.py
16	14	tests/tool_use/utils.py
10	0	tests/tracing/test_tracing.py
18	8	tests/v1/engine/test_engine_args.py
1	3	tests/v1/sample/test_logprobs.py
169	0	tests/v1/test_oracle.py
8	1	tests/weight_loading/test_weight_loading.py
10	0	tests/worker/conftest.py
17	10	vllm/config.py
346	80	vllm/engine/arg_utils.py
42	15	vllm/engine/async_llm_engine.py
35	12	vllm/engine/llm_engine.py
2	3	vllm/engine/multiprocessing/client.py
37	18	vllm/engine/multiprocessing/engine.py
5	14	vllm/entrypoints/llm.py
39	13	vllm/entrypoints/openai/api_server.py
18	2	vllm/envs.py
2	1	vllm/model_executor/model_loader/utils.py
2	2	vllm/model_executor/models/bloom.py
2	1	vllm/model_executor/models/glm.py
3	2	vllm/model_executor/models/ultravox.py
2	1	vllm/v1/attention/backends/flash_attn.py
41	6	vllm/v1/engine/async_llm.py
27	0	vllm/v1/engine/llm_engine.py
16	3	vllm/v1/engine/processor.py

[8c0d15d5c] Lu Fang 2025-03-14 [Misc][Easy] Annotate unused vars in the csrc files (#14798)
1	1	csrc/prepare_inputs/advance_step.cu
1	1	csrc/quantization/fp8/amd/quant_utils.cuh
8	8	csrc/quantization/gptq/q_gemm.cu
4	3	csrc/rocm/attention.cu

[97ac781c6] Isotr0py 2025-03-15 [Misc] Remove misleading message in gemma2 and gemma3 (#14850)
1	5	vllm/model_executor/models/gemma.py
0	5	vllm/model_executor/models/gemma2.py
0	5	vllm/model_executor/models/gemma3.py

[776dcec8f] Russell Bryant 2025-03-14 Disable outlines cache by default (#14837)
7	0	vllm/envs.py
9	1	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[ccf02fcba] Tyler Michael Smith 2025-03-14 Revert "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U… (#14848)
8	22	vllm/model_executor/layers/mamba/mamba_mixer2.py

[acaea3bb0] DefTruth 2025-03-15 [Bugfix][V1] Fix flashinfer sampling (#14815)
18	1	vllm/v1/sample/ops/topk_topp_sampler.py

[9f3742277] Liangfu Chen 2025-03-14 [Neuron][CI] update docker run command (#14829)
1	1	.buildkite/run-neuron-test.sh

[dd344e034] yarongmu-google 2025-03-14 [Bugfix] Fix torch_xla in V0 which can't handle None seed introduced … (#14844)
3	0	vllm/worker/tpu_worker.py

[54a880445] Yuan Tang 2025-03-14 [Doc] More neutral K8s deployment guide (#14084)
10	8	docs/source/deployment/k8s.md

[bbd94a19f] Russell Bryant 2025-03-14 [Build/CI] Upgrade aiohttp to incldue CVE fix (#14840)
1	1	requirements/test.txt

[233ffce1e] Russell Bryant 2025-03-14 [Build/CI] Move ninja to common deps (#14835)
1	0	requirements/common.txt
0	1	requirements/rocm-build.txt
0	1	requirements/tpu.txt
1	2	requirements/xpu.txt

[40677783a] Richard Liu 2025-03-14 [CI] Add TPU v1 test (#14834)
27	0	.buildkite/run-tpu-v1-test.sh

[14f301b54] Michael Goin 2025-03-14 Update to torch==2.6.0 (#12721)
2	2	CMakeLists.txt
1	1	Dockerfile
1	1	pyproject.toml
1	1	requirements/build.txt
5	5	requirements/cuda.txt
4	3	requirements/test.in
10	8	requirements/test.txt
4	2	tests/compile/backend.py
15	0	vllm/config.py

[46f98893d] Russell Bryant 2025-03-14 [V1] Fix model parameterization for structured output tests (#14833)
11	7	tests/v1/entrypoints/llm/test_struct_output_generate.py

[fe66b3472] Chih-Chieh Yang 2025-03-14 [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14778)
22	8	vllm/model_executor/layers/mamba/mamba_mixer2.py

[270a5da49] Alexei-V-Ivanov-AMD 2025-03-14 Re-enable the AMD Entrypoints Test (#14711)
19	5	.buildkite/run-amd-test.sh
1	0	Dockerfile.rocm
23	0	requirements/rocm-test.txt

[7097b4cc1] Kevin H. Luu 2025-03-14 [release] Remove log cleanup commands from TPU job (#14838)
0	2	.buildkite/release-pipeline.yaml

[977a16772] Yajie Wang 2025-03-15 [Bugfix][Kernel]: Fix AllSpark kernel compilation errors and enable for CUDA < 12.0 (#14430)
2	2	CMakeLists.txt
8	5	csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
5	3	csrc/quantization/gptq_allspark/allspark_utils.cuh

[73deea2fd] daniel-salib 2025-03-14 [Frontend] track server_load (#13950)
48	0	tests/entrypoints/openai/test_basic.py
30	2	vllm/entrypoints/openai/api_server.py
7	0	vllm/entrypoints/openai/cli_args.py
46	2	vllm/entrypoints/utils.py

[9d2b4a70f] Mark McLoughlin 2025-03-14 [V1][Metrics] Updated list of deprecated metrics in v0.8 (#14695)
10	1	docs/source/serving/metrics.md

[0b0d6421b] Russell Bryant 2025-03-14 [Frontend] Fix log message to use http vs https (#14774)
4	2	vllm/entrypoints/openai/api_server.py

[1140991a7] Russell Bryant 2025-03-14 [V1] Fix vocab size calculation for structured output (#14826)
1	1	vllm/v1/structured_output/__init__.py

[613c5bb94] Cyrus Leung 2025-03-15 [Bugfix] Fix Aria test loading (#14823)
3	1	tests/models/decoder_only/vision_language/test_models.py

[fd8e055ff] Guillaume Calmettes 2025-03-14 [BugFix]: properly catch templating error when preprocess input (#13976)
10	0	vllm/entrypoints/openai/serving_chat.py
10	0	vllm/entrypoints/openai/serving_completion.py
3	0	vllm/entrypoints/openai/serving_embedding.py
7	0	vllm/entrypoints/openai/serving_pooling.py
7	0	vllm/entrypoints/openai/serving_tokenization.py

[ab93f1360] Cyrus Leung 2025-03-14 [VLM] Various cleanup and fixes (#14806)
14	1	vllm/entrypoints/chat_utils.py
8	7	vllm/model_executor/models/fuyu.py
2	2	vllm/model_executor/models/interfaces.py
64	72	vllm/model_executor/models/llava.py
4	3	vllm/model_executor/models/llava_next.py
14	15	vllm/model_executor/models/llava_onevision.py
46	38	vllm/model_executor/models/minicpmo.py
74	46	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/molmo.py
42	82	vllm/model_executor/models/pixtral.py
4	4	vllm/model_executor/models/qwen2_audio.py
4	0	vllm/multimodal/inputs.py
4	0	vllm/multimodal/parse.py
2	2	vllm/multimodal/processing.py

[40253bab4] DefTruth 2025-03-14 [Bugfix][W8A8] fixed cutlass block fp8 binding (#14796)
1	1	csrc/torch_bindings.cpp

[c77620d22] Woosuk Kwon 2025-03-14 [V1][Minor] Minor code cleanup for scheduling metrics (#14800)
10	25	vllm/v1/core/scheduler.py
13	15	vllm/v1/request.py

[989ecd200] Jee Jee Li 2025-03-14 [Misc] Gemma3ForConditionalGeneration supports LoRA (#14797)
14	3	vllm/model_executor/models/gemma3_mm.py

[54cc46f3e] WeiCheng 2025-03-14 [Bugfix] Fix small typo in the example of Streaming delimiter (#14793)
1	1	examples/online_serving/api_client.py
1	1	examples/online_serving/gradio_webserver.py

[601bd3268] Cyrus Leung 2025-03-14 [Misc] Clean up type annotation for `SupportsMultiModal` (#14794)
3	2	docs/source/contributing/model/multimodal.md
2	2	tests/distributed/test_pipeline_parallel.py
4	6	vllm/model_executor/models/aria.py
4	6	vllm/model_executor/models/blip2.py
4	6	vllm/model_executor/models/chameleon.py
3	4	vllm/model_executor/models/deepseek_vl2.py
5	5	vllm/model_executor/models/florence2.py
4	5	vllm/model_executor/models/fuyu.py
5	5	vllm/model_executor/models/gemma3_mm.py
5	5	vllm/model_executor/models/glm4v.py
3	4	vllm/model_executor/models/idefics3.py
13	11	vllm/model_executor/models/interfaces.py
3	4	vllm/model_executor/models/internvl.py
4	3	vllm/model_executor/models/llava.py
4	5	vllm/model_executor/models/llava_next.py
4	6	vllm/model_executor/models/llava_next_video.py
6	8	vllm/model_executor/models/llava_onevision.py
4	5	vllm/model_executor/models/molmo.py
4	6	vllm/model_executor/models/paligemma.py
5	6	vllm/model_executor/models/phi3v.py
4	5	vllm/model_executor/models/pixtral.py
6	6	vllm/model_executor/models/qwen2_5_vl.py
4	6	vllm/model_executor/models/qwen2_audio.py
6	6	vllm/model_executor/models/qwen2_vl.py
5	6	vllm/model_executor/models/qwen_vl.py
4	4	vllm/model_executor/models/ultravox.py
3	4	vllm/model_executor/models/whisper.py

[09269b312] Li Wang 2025-03-14 [BugFix]Fix performance serving benchmark when enable profiling (#14737)
1	1	benchmarks/backend_request_func.py

[27b50f1fe] Thien Tran 2025-03-14 [Bugfix][Kernel][CPU] Fix num_tokens in CPU rotary embedding kernel (#14667)
1	1	csrc/cpu/pos_encoding.cpp

[9532c4983] Lucas Wilkinson 2025-03-14 [Attention] MLA get rid of materialization (#14770)
57	210	vllm/attention/backends/mla/common.py
0	19	vllm/envs.py
2	57	vllm/model_executor/layers/quantization/utils/fp8_utils.py
58	213	vllm/v1/attention/backends/mla/common.py

[0c2af17c7] Roger Wang 2025-03-13 [CI] Fix missing example model id in processor test (#14787)
1	1	tests/models/multimodal/processing/test_common.py
1	1	tests/models/registry.py

[a6e0d096d] Jennifer Zhao 2025-03-13 [Feature] Add visionarena offline support for benchmark_throughput (#14654)
47	11	benchmarks/README.md
43	22	benchmarks/benchmark_dataset.py
201	66	benchmarks/benchmark_throughput.py

[d3d495626] Liangfu Chen 2025-03-13 [Neuron] flatten test parameterization for neuron attention kernels (#14712)
1	1	.buildkite/run-neuron-test.sh
0	0	tests/neuron/{ => 1_core}/test_activation.py
0	0	tests/neuron/{ => 1_core}/test_block_table.py
0	0	tests/neuron/{ => 1_core}/test_cache.py
0	0	tests/neuron/{ => 1_core}/test_layernorm.py
0	0	tests/neuron/{ => 1_core}/test_logits_processor.py
25	21	tests/neuron/{ => 1_core}/test_prefix_prefill.py
0	0	tests/neuron/{ => 1_core}/test_rotary_embedding.py
0	0	tests/neuron/{ => 2_core}/test_comm_ops.py

[4059adc31] Nick Hill 2025-03-13 [Misc][Minor] Simplify `SamplingParams.__post_init__()` (#14772)
5	11	vllm/sampling_params.py

[f1f632d9e] Kevin H. Luu 2025-03-13 [ci] Reduce number of tests in fastcheck (#14782)
0	5	.buildkite/test-pipeline.yaml

[95d680b86] Thien Tran 2025-03-14 [Bugfix][IPEX] Add `VLLM_CPU_MOE_PREPACK` to allow disabling MoE prepack when CPU does not support it (#14681)
1	0	docs/source/getting_started/installation/cpu.md
7	0	vllm/envs.py
2	1	vllm/model_executor/layers/fused_moe/layer.py

[fb4c7f8ef] Thomas Parnell 2025-03-14 [Kernel] [V1] Further optimizations to ROCm (Triton) Backend to better handle GQA. (#14431)
63	40	vllm/attention/ops/chunked_prefill_paged_decode.py

[0b1cfa618] Varun Sundar Rabindranath 2025-03-13 [Kernel] LoRA - Enable CUDAGraphs for V1 (#14626)
1	0	tests/lora/test_worker.py
19	6	vllm/config.py
9	6	vllm/lora/layers.py
6	2	vllm/lora/punica_wrapper/punica_gpu.py

[32ef4983c] Woosuk Kwon 2025-03-13 [V1] Temporarily disable FlashInfer Rejection Sampler (#14788)
1	1	vllm/v1/sample/ops/topk_topp_sampler.py
12	3	vllm/v1/sample/rejection_sampler.py

[ad19c8a00] Roger Wang 2025-03-13 [V1] Move OOM check into sampler run (#14728)
12	3	vllm/v1/worker/gpu_model_runner.py
5	15	vllm/v1/worker/gpu_worker.py

[2a602b055] Jeff Daily 2025-03-13 forward fix PR 14245, restore build on ROCm 6.2 (#14709)
12	0	csrc/quantization/fp8/amd/quant_utils.cuh

[7888e1d0a] Alexander Matveev 2025-03-13 [V1] TPU - Enable prefix caching by default (#14773)
0	6	vllm/platforms/tpu.py

[60c872d4b] Chen Zhang 2025-03-14 [Doc] Fix small typo in Transformers fallback (#14791)
1	1	docs/source/models/supported_models.md

[3fb17d26c] yasu52 2025-03-13 [Doc] Fix typo in documentation (#14783)
2	2	docs/source/deployment/frameworks/helm.md
1	1	docs/source/deployment/k8s.md
1	1	docs/source/design/kernel/paged_attention.md
2	2	docs/source/design/v1/metrics.md
1	1	docs/source/features/lora.md
1	1	docs/source/getting_started/faq.md
1	1	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
1	1	docs/source/getting_started/installation/ai_accelerator/openvino.inc.md
4	4	docs/source/getting_started/installation/gpu/xpu.inc.md
2	2	docs/source/serving/distributed_serving.md
1	1	docs/source/training/rlhf.md
1	1	examples/other/logging_configuration.md
1	1	vllm/distributed/kv_transfer/README.md

[d47807ba0] Lucas Wilkinson 2025-03-13 [Attention] Remove slow setattr in MLA (#14769)
7	2	vllm/model_executor/layers/rotary_embedding.py

[02fcaa3d0] afeldman-nm 2025-03-13 [V1] Detokenizer: Respect Stop Tokens + not include_stop_str_in_output (#14624)
170	9	tests/v1/engine/test_output_processor.py
20	3	tests/v1/engine/utils.py
22	3	vllm/v1/engine/detokenizer.py
3	3	vllm/v1/engine/output_processor.py

[8a4a2efc6] Aaron Pham 2025-03-13 [V1][Core] using cached vocab_size for Structured Outputs (#14630)
1	1	vllm/v1/structured_output/__init__.py

[8e9ffd37d] Cyrus Leung 2025-03-14 [Misc] Clean up processor tests (#14771)
3	5	tests/models/multimodal/processing/test_h2ovl.py
3	7	tests/models/multimodal/processing/test_idefics3.py
3	5	tests/models/multimodal/processing/test_internvl.py
3	6	tests/models/multimodal/processing/test_llava_next.py
3	6	tests/models/multimodal/processing/test_llava_onevision.py
1	3	tests/models/multimodal/processing/test_phi3v.py
1	2	tests/models/multimodal/processing/test_qwen2_vl.py
13	12	tests/models/utils.py

[01b3fd0af] Woosuk Kwon 2025-03-13 [V1][Minor] Minor enhancements on scheduler (#14732)
3	5	vllm/v1/core/scheduler.py

[f53a0586b] Cyrus Leung 2025-03-13 [Bugfix] Fix prompt format of GLM4V (#14539)
11	3	tests/models/decoder_only/vision_language/test_models.py
3	1	tests/models/decoder_only/vision_language/vlm_utils/core.py
13	5	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
8	4	vllm/config.py
4	3	vllm/entrypoints/chat_utils.py
2	1	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/qwen.py

[b1cc4dfef] Isotr0py 2025-03-13 [VLM] Support loading InternVideo2.5 models as original InternVLChatModel (#14738)
2	2	docs/source/models/supported_models.md
8	1	vllm/model_executor/models/internvl.py

[382403921] Cyrus Leung 2025-03-13 [VLM] Support pan-and-scan for Gemma3 multi-modal processor (#14672)
25	30	docs/source/models/supported_models.md
8	4	examples/offline_inference/vision_language.py
8	4	examples/offline_inference/vision_language_multi_image.py
18	1	tests/models/decoder_only/vision_language/test_models.py
12	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
8	1	vllm/inputs/registry.py
229	38	vllm/model_executor/models/gemma3_mm.py
5	1	vllm/multimodal/base.py
2	2	vllm/utils.py

[a73122de9] Jee Jee Li 2025-03-13 [Bugfix] fix benchmark moe (#14653)
21	5	benchmarks/kernels/benchmark_moe.py

[bd44b812c] Jee Jee Li 2025-03-13 [CI/Build]  Delete ultravox LoRA test (#14730)
0	131	tests/lora/test_ultravox.py

[55211b01e] Szymon Ożóg 2025-03-13 [Bugfix] Fix chunked prefill for GGUF (#14666)
7	0	vllm/model_executor/layers/quantization/gguf.py

[5d043c168] Kyle Sayers 2025-03-13 [Quant] Bamba SupportsQuant (#14698)
2	2	vllm/model_executor/models/bamba.py

[36d1ccb28] Kyle Sayers 2025-03-13 [Quant] BartModel SupportsQuant (#14699)
4	3	vllm/model_executor/models/bart.py

[1bc3b739c] Siyuan Liu 2025-03-12 [V1][TPU] Add assertion on multi-step-scheduler (#14707)
11	5	vllm/platforms/tpu.py

[1bd32bc8d] Mathis Felardos 2025-03-13 [Config][Disaggregated] Add timeout configuration for the torch.store and add KVTransferConfig.kv_connector_extra_config (#14367)
6	0	vllm/config.py
1	1	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
12	10	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
3	0	vllm/distributed/utils.py

[128bf7528] TY-AMD 2025-03-13 [BugFix][TritonMLA] Process weights after model loading for GGUF (#14555)
4	1	vllm/model_executor/model_loader/loader.py

[a94a699c3] Gregory Shtrasberg 2025-03-12 [ROCm][FP8] Fix for adjustments needed only for fnuz (#14689)
2	2	vllm/model_executor/layers/quantization/kv_cache.py

[ab426ec9c] Richard Liu 2025-03-12 Add ray[data] as tpu dependency (#14691)
1	0	requirements/tpu.txt

[165290d35] Joe Runde 2025-03-12 [bugfix] fixup warning message for plugged schedulers for v1 (#14700)
11	4	vllm/v1/engine/core.py

[ce2012467] Kevin H. Luu 2025-03-12 [release] Add force remove for TPU logs (#14697)
2	2	.buildkite/release-pipeline.yaml

[53be4a863] Woosuk Kwon 2025-03-12 [V1] Allow sliding window + prefix caching (#13069)
1	1	vllm/config.py

[f5d3acd47] Nick Hill 2025-03-12 [BugFix][V1] Fix parallel sampling finishing/aborts (#14512)
50	6	tests/v1/engine/test_async_llm.py
15	6	tests/v1/entrypoints/openai/test_completion.py
18	46	vllm/outputs.py
1	2	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/llm_engine.py
30	19	vllm/v1/engine/output_processor.py
22	33	vllm/v1/engine/parallel_sampling.py

[916836bbf] TJian 2025-03-13 [FEAT] [ROCm] [Embedding] Add encoder-only model support into ROCm Flash Attention to enable embedding models. (#14664)
4	0	CMakeLists.txt
1	0	csrc/moe/torch_bindings.cpp
15	2	tests/models/embedding/language/test_cls_models.py
11	2	tests/models/embedding/language/test_embedding.py
2	2	tests/models/embedding/language/test_gritlm.py
17	0	tests/models/embedding/vision_language/test_llava_next.py
68	44	vllm/attention/backends/rocm_flash_attn.py

[d9f83d620] Sage Moore 2025-03-12 [ROCm] Enable chunked prefill/paged attention in MLA on ROCm (#14316)
2	16	vllm/attention/backends/mla/common.py
2	2	vllm/config.py

[4a754fcf1] ameyanjarlekar 2025-03-12 [Bugfix] Missing thumbnail from NVLM-D processor (#14633)
1	1	vllm/model_executor/models/nvlm_d.py

[c0c25e25f] Woosuk Kwon 2025-03-12 [Model] Add support for Gemma 3 (#14660)
39	2	docs/source/models/supported_models.md
19	1	examples/offline_inference/vision_language.py
37	0	examples/offline_inference/vision_language_multi_image.py
1	0	tests/models/multimodal/processing/test_common.py
4	0	tests/models/registry.py
9	6	vllm/config.py
2	0	vllm/entrypoints/chat_utils.py
533	0	vllm/model_executor/models/gemma3.py
425	0	vllm/model_executor/models/gemma3_mm.py
2	0	vllm/model_executor/models/registry.py

[45f3f3f59] Sage Moore 2025-03-12 [ROCm][Bugfix] Ensure that the moe_wna16_gemm kernel is not built on ROCm platforms. (#14629)
1	1	CMakeLists.txt
2	1	csrc/moe/moe_ops.h
1	1	csrc/moe/torch_bindings.cpp
4	0	vllm/_custom_ops.py

[ff47aab05] Li, Jiang 2025-03-12 [CPU] Upgrade CPU backend to torch-2.6 (#13381)
5	3	.buildkite/run-cpu-test.sh
1	1	Dockerfile.cpu
1	1	cmake/cpu_extension.cmake
1	1	requirements/cpu.txt
1	1	tests/lora/test_qwen2vl.py
1	1	vllm/attention/ops/ipex_attn.py
5	4	vllm/executor/multiproc_worker_utils.py
5	1	vllm/model_executor/layers/fused_moe/layer.py
3	0	vllm/platforms/cpu.py

[debd6bbf0] Pavani Majety 2025-03-11 [Kernel] Add ModelOpt FP4 Checkpoint Support (#12520)
5	3	csrc/ops.h
6	0	csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu
4	3	csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu
4	0	csrc/torch_bindings.cpp
82	0	tests/models/decoder_only/language/test_nvfp4.py
4	0	vllm/_custom_ops.py
1	1	vllm/config.py
17	6	vllm/model_executor/layers/linear.py
3	1	vllm/model_executor/layers/quantization/__init__.py
262	16	vllm/model_executor/layers/quantization/modelopt.py

[5c538c37b] Benjamin Chislett 2025-03-12 [V1][Bugfix][Spec Decode] Fix incorrect outputs in V1 speculative decoding due to batch indexing (#14645)
49	12	tests/v1/e2e/test_ngram_spec_decode.py
1	3	vllm/v1/worker/gpu_model_runner.py

[e22ee1e7a] Szymon Ożóg 2025-03-12 [Kernel] GGUF MoE kernel (#14613)
8	0	csrc/ops.h
138	4	csrc/quantization/gguf/gguf_kernel.cu
739	0	csrc/quantization/gguf/moe.cuh
10	0	csrc/torch_bindings.cpp
13	0	tests/kernels/test_ggml.py
64	0	tests/kernels/test_gguf.py
37	0	vllm/_custom_ops.py
61	21	vllm/model_executor/layers/quantization/gguf.py

[e392d8583] Isotr0py 2025-03-12 [Core] Refactor `QKVCrossParallelLinear` implementation to support BNB 4-bit quantization (#14545)
45	0	tests/models/encoder_decoder/vision_language/test_mllama.py
178	35	vllm/model_executor/layers/linear.py
11	30	vllm/model_executor/models/mllama.py

[77a318bd0] Aaron Pham 2025-03-11 [V1][Core] Support MistralTokenizer for Structured Output (#14625)
64	23	tests/v1/entrypoints/llm/test_struct_output_generate.py
38	3	vllm/v1/structured_output/__init__.py

[80e78d02a] Farzad Abdolhosseini 2025-03-11 [Model] Extend Ultravox to accept audio longer than 30s (#13631)
1	1	tests/models/decoder_only/audio_language/test_ultravox.py
50	7	tests/models/multimodal/processing/test_common.py
1	2	tests/models/registry.py
181	71	vllm/model_executor/models/ultravox.py

[4a42b9f5d] Jennifer Zhao 2025-03-11 [Doc] Update benchmarks README (#14646)
165	13	benchmarks/README.md

[47532cd9f] Joe Runde 2025-03-11 [core][V1] pluggable scheduler (#14466)
40	9	tests/plugins_tests/test_scheduler_plugins.py
5	0	vllm/engine/arg_utils.py
13	2	vllm/v1/engine/core.py

[36e0c8f7d] Randy Chen 2025-03-11 [Feature] Add `vllm bench` CLI (#13993)
160	0	vllm/benchmarks/endpoint_request_func.py
927	0	vllm/benchmarks/serve.py
69	0	vllm/benchmarks/utils.py
0	0	vllm/entrypoints/cli/benchmark/__init__.py
37	0	vllm/entrypoints/cli/benchmark/base.py
50	0	vllm/entrypoints/cli/benchmark/main.py
29	0	vllm/entrypoints/cli/benchmark/serve.py
2	0	vllm/entrypoints/cli/main.py

[9f583e360] Kevin H. Luu 2025-03-11 [release] Add commands to clean up logs on TPU release node (#14642)
2	0	.buildkite/release-pipeline.yaml

[b706d898a] Cody Yu 2025-03-11 [Bugfix][V1][PP] Only warmup sampler at last PP rank (#14643)
17	14	vllm/v1/worker/gpu_worker.py

[863d315c8] iefgnoix 2025-03-11 [V1][TPU] Pad the block_table.shape[1] so the ragged paged attention can handle correctly (#14597)
5	2	vllm/v1/worker/tpu_model_runner.py

[d374f04a3] Richard Liu 2025-03-11 Fix run_tpu_test (#14641)
0	1	.buildkite/run-tpu-test.sh

[61a01b27a] Russell Bryant 2025-03-11 [V1] Delay all xgrammar usage until needed (#14616)
20	13	vllm/v1/structured_output/__init__.py

[53056731f] Yang.Tao 2025-03-12 fix some typos : supported_head_sizes (#14627)
3	3	vllm/attention/backends/blocksparse_attn.py

[4cbf28679] Russell Bryant 2025-03-11 [V1] Remove cache from StructuredOutputManager (#14622)
1	1	vllm/v1/engine/core.py
11	47	vllm/v1/structured_output/__init__.py

[c6e14a61a] Kunshang Ji 2025-03-11 [Hardware][Intel GPU] upgrade IPEX dependency to 2.6.10.  (#14564)
10	12	Dockerfile.xpu
13	5	docs/source/getting_started/installation/gpu/xpu.inc.md
12	5	requirements/xpu.txt

[07b4b7a37] Lucas Wilkinson 2025-03-11 [BugFix/Build] Fix sparse kernels not getting built on hopper (#14572)
2	1	CMakeLists.txt
6	2	csrc/sparse/cutlass/sparse_scaled_mm_entry.cu

[07964e2f3] Dilip Gowda Bhagavan 2025-03-11 docs: Add documentation for s390x cpu implementation (#14198)
1	0	docs/source/getting_started/installation.md
34	0	docs/source/getting_started/installation/cpu.md
62	0	docs/source/getting_started/installation/cpu/s390x.inc.md

[4bf82d4b9] Russell Bryant 2025-03-11 [V1] Add regex structured output support with xgrammar (#14590)
2	2	requirements/common.txt
16	16	tests/v1/entrypoints/llm/test_struct_output_generate.py
2	0	vllm/v1/structured_output/__init__.py
5	1	vllm/v1/structured_output/utils.py

[9ab326713] Richard Liu 2025-03-11 Uninstall dependencies before installing requirements/tpu.txt (#14586)
3	0	Dockerfile.tpu

[af295e9b0] Cyrus Leung 2025-03-11 [Bugfix] Update `--hf-overrides` for `Alibaba-NLP/gte-Qwen2` (#14609)
4	7	docs/source/models/supported_models.md
2	2	tests/models/embedding/language/test_embedding.py

[a1c8f3796] Jeff Daily 2025-03-11 dynamic distpatch of fp8 kernels (#14245)
1	2	benchmarks/kernels/benchmark_moe.py
26	6	csrc/dispatch_utils.h
33	25	csrc/layernorm_quant_kernels.cu
22	0	csrc/quantization/fp8/amd/quant_utils.cuh
43	25	csrc/quantization/fp8/common.cu
59	27	csrc/quantization/fp8/common.cuh
3	0	csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
11	8	csrc/quantization/fused_kernels/quant_conversions.cuh
0	1	csrc/quantization/vectorization.cuh
1	2	tests/kernels/quant_utils.py
2	5	tests/kernels/test_triton_scaled_mm.py
3	6	tests/quantization/test_fp8.py
17	18	vllm/_custom_ops.py
3	3	vllm/attention/backends/mla/common.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py
5	9	vllm/model_executor/layers/quantization/fp8.py
1	2	vllm/model_executor/layers/quantization/quark/quark_moe.py
2	2	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
2	10	vllm/model_executor/layers/quantization/utils/fp8_utils.py
4	0	vllm/platforms/cuda.py
30	0	vllm/platforms/interface.py
17	0	vllm/platforms/rocm.py
3	3	vllm/v1/attention/backends/mla/common.py

[08a1a1121] Russell Bryant 2025-03-11 benchmarks: simplify test jsonschema (#14567)
10	16	benchmarks/structured_schemas/structured_schema_1.json

[1477ffc38] Isotr0py 2025-03-11 [VLM] Cleanup siglip legacy code and fix broken paligemma multimodal processor (#14602)
10	5	vllm/model_executor/models/paligemma.py
4	71	vllm/model_executor/models/siglip.py

[70b808fe1] yexin(叶鑫) 2025-03-11 [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)
33	12	vllm/model_executor/models/qwen2_5_vl.py
37	12	vllm/model_executor/models/qwen2_vl.py

[63d635d17] Isotr0py 2025-03-11 [Misc] Correct deepseek-vl2 chat template (#14558)
3	3	examples/template_deepseek_vl2.jinja

[1fc973c0b] Roger Wang 2025-03-10 [V1][Core] Fix memory issue with logits & sampling (#14508)
10	1	tests/basic_correctness/test_cumem.py
5	0	vllm/config.py
98	87	vllm/v1/worker/gpu_model_runner.py
23	0	vllm/v1/worker/gpu_worker.py
3	3	vllm/v1/worker/lora_model_runner_mixin.py

[c982ac572] Concurrensee 2025-03-10 [Bugfix] Fix FP16 overflow for DeepSeek V2 (#13232)
24	4	vllm/model_executor/models/deepseek_v2.py

[4290b704f] Cody Yu 2025-03-10 [V1][PP] Do not block engine core when no requests to schedule (#14585)
11	16	vllm/v1/engine/core.py

[c91b64f74] Liangfu Chen 2025-03-10 [neuron] add reshape_and_cache (#14391)
83	0	tests/neuron/test_cache.py
43	0	vllm/attention/ops/nki_flash_attn.py

[d6123170d] gnovack 2025-03-10 [Neuron] Add Neuron device communicator for vLLM v1 (#14085)
100	0	tests/neuron/test_comm_ops.py
19	0	vllm/distributed/device_communicators/neuron_communicator.py
8	0	vllm/platforms/neuron.py

[485afdd3c] Cody Yu 2025-03-10 [MISC][V1] Handle exception of current_platform.get_device_name() in arg_utils (#14379)
13	2	vllm/engine/arg_utils.py

[90e88ab75] Jinzhen Lin 2025-03-11 [Kernel] moe wna16 cuda kernel (#13321)
8	0	CMakeLists.txt
10	0	csrc/moe/moe_ops.h
346	0	csrc/moe/moe_wna16.cu
200	0	csrc/moe/moe_wna16_utils.h
10	0	csrc/moe/torch_bindings.cpp
15	0	vllm/_custom_ops.py
109	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[04421dff8] Russell Bryant 2025-03-10 [V1] Prevent xgrammar from breaking TPU support (#14575)
4	0	vllm/v1/engine/processor.py
7	2	vllm/v1/structured_output/__init__.py

[432d6dad1] Russell Bryant 2025-03-10 Fix typo in benchmark_serving_structured_output.py (#14566)
5	6	benchmarks/benchmark_serving_structured_output.py

[5ff0d3258] Varun Sundar Rabindranath 2025-03-10 [V1] LoRA - Add triton kernels for V1 (#13096)
141	24	benchmarks/kernels/benchmark_lora.py
50	18	tests/lora/test_layers.py
125	80	tests/lora/test_punica_ops.py
5	3	vllm/lora/models.py
2	2	vllm/lora/ops/triton_ops/utils.py
11	0	vllm/lora/ops/triton_ops/v1/__init__.py
282	0	vllm/lora/ops/triton_ops/v1/v1_expand.py
117	0	vllm/lora/ops/triton_ops/v1/v1_kernel_metadata.py
236	0	vllm/lora/ops/triton_ops/v1/v1_shrink.py
190	58	vllm/lora/punica_wrapper/punica_gpu.py
3	3	vllm/v1/worker/lora_model_runner_mixin.py

[0967110e4] Woosuk Kwon 2025-03-10 [Minor] Update the tqdm bar for parallel sampling (#14571)
3	2	vllm/entrypoints/llm.py

[fb0acb6c7] Simon Mo 2025-03-10 [Perf] Improve MLA on V1 (#14540)
41	27	vllm/v1/attention/backends/mla/common.py

[92b0ce2ac] Chauncey 2025-03-11 [Bugfix][v1] fixed llava-hf/llava-1.5-7b-hf is broken on V1 (#14554)
12	8	vllm/model_executor/models/llava.py

[bc2d4473b] Harry Mellor 2025-03-10 [Docs] Make installation URLs nicer (#14556)
1	1	README.md
6	6	docs/source/getting_started/{installation/index.md => installation.md}
32	32	docs/source/getting_started/installation/{ai_accelerator/index.md => ai_accelerator.md}
10	10	docs/source/getting_started/installation/{cpu/index.md => cpu.md}
1	1	docs/source/getting_started/installation/cpu/arm.inc.md
1	1	docs/source/getting_started/installation/cpu/x86.inc.md
23	23	docs/source/getting_started/installation/{gpu/index.md => gpu.md}
1	1	docs/source/index.md

[3b352a2f9] Harry Mellor 2025-03-10 Correct capitalisation: `VLLM` -> `vLLM` (#14562)
1	1	benchmarks/kernels/benchmark_rmsnorm.py
1	1	docs/source/contributing/vulnerability_management.md
1	1	docs/source/design/v1/metrics.md
1	1	examples/offline_inference/disaggregated_prefill_lmcache.py
1	1	tests/tpu/test_quantization_accuracy.py
1	1	vllm/attention/selector.py
1	1	vllm/compilation/backends.py
1	1	vllm/compilation/compiler_interface.py
4	4	vllm/config.py
1	1	vllm/entrypoints/openai/protocol.py
3	3	vllm/envs.py
1	1	vllm/model_executor/models/phi4mm.py
1	1	vllm/platforms/cuda.py
2	2	vllm/platforms/rocm.py
1	1	vllm/transformers_utils/tokenizers/mistral.py
1	1	vllm/v1/engine/core_client.py
1	1	vllm/v1/engine/output_processor.py
2	2	vllm/v1/engine/processor.py

[dea985aef] Roger Wang 2025-03-10 [V1][Bugfix] Fix handing of `second_per_grid_ts` for Qwen2-VL & Qwen2.5-VL (#14548)
10	5	vllm/model_executor/layers/rotary_embedding.py

[39be30351] Harry Mellor 2025-03-10 Correct capitalisation: `Github` -> `GitHub` (#14561)
1	1	.github/workflows/publish.yml
1	1	.github/workflows/scripts/create_release.js
2	2	README.md
1	1	vllm/engine/async_llm_engine.py

[001a9c7b0] Cyrus Leung 2025-03-10 [Doc] Update PaliGemma note to a warning (#14565)
7	5	docs/source/models/supported_models.md

[89cdaa83e] Szymon Ożóg 2025-03-10 [Kernel] Add more dtype support for GGUF kernels (#14043)
168	152	csrc/quantization/gguf/gguf_kernel.cu
74	64	csrc/quantization/gguf/mmq.cuh
60	41	csrc/quantization/gguf/mmvq.cuh
12	6	tests/kernels/test_gguf.py
2	2	vllm/_custom_ops.py
3	2	vllm/model_executor/layers/quantization/gguf.py

[b0746fae3] Chauncey 2025-03-10 [Frontend] support image embeds (#13955)
66	1	docs/source/serving/multimodal_inputs.md
103	10	vllm/entrypoints/chat_utils.py
19	0	vllm/multimodal/image.py
13	1	vllm/multimodal/utils.py

[60a98b2de] Harry Mellor 2025-03-10 [Docs] Mention `model_impl` arg when explaining Transformers fallback (#14552)
4	0	docs/source/models/supported_models.md

[460f553a6] Chauncey 2025-03-10 [Misc] Add log information for handle_process_request. (#14130)
2	0	vllm/engine/multiprocessing/engine.py

[1253b1577] Jennifer Zhao 2025-03-10 [Feature] Consolidate performance benchmark datasets (#14036)
667	0	benchmarks/benchmark_dataset.py
77	382	benchmarks/benchmark_serving.py
81	197	benchmarks/benchmark_throughput.py

[dc74613fa] Martin Hoyer 2025-03-10 [Bugfix] Wrong requirements path - rocm (#14527)
1	1	Dockerfile.rocm

[a21076ed3] Yanyi Liu 2025-03-09 [Misc] Ensure out-of-tree quantization method recognize by cli args (#14328)
4	4	vllm/engine/arg_utils.py

[212007b16] Chengji Yao 2025-03-09 [Hardware][TPU] Fix the recompiling issue in logits processor after warmup (#14510)
3	1	examples/offline_inference/tpu.py
38	9	vllm/v1/worker/tpu_model_runner.py

[fb16eea48] Isotr0py 2025-03-09 [Bugfix] Revert QKVCrossParallelLinear usage in Mllama to keep BNB quantization work (#14498)
29	10	vllm/model_executor/models/mllama.py

[73ae0b44e] Yuchen Yan 2025-03-09 [Bugfix] Fix tqdm progress bar when SamplingParams.n > 1 (#12428)
3	1	vllm/entrypoints/llm.py

[6d7f03774] Jiayi Yao 2025-03-08 [Feat] Support chunked prefill for LMCache connector (#14505)
9	19	vllm/distributed/kv_transfer/kv_connector/lmcache_connector.py

[10f755278] iefgnoix 2025-03-08 [V1][TPU] Remove unnecessary padding for running on TPU. (#14467)
2	2	vllm/v1/attention/backends/pallas.py
4	16	vllm/v1/worker/tpu_model_runner.py

[b0d541947] Lucas Wilkinson 2025-03-08 [Attention] Default to FlashMLA backend for MLA (#14451)
24	16	vllm/platforms/cuda.py

[5f0b53c6e] Robert Shaw 2025-03-08 Revert "[V1][Core] Fix memory issue with logits & sampling" (#14504)
1	10	tests/basic_correctness/test_cumem.py
29	39	vllm/v1/worker/gpu_model_runner.py
0	21	vllm/v1/worker/gpu_worker.py

[eb8b5eb18] 22quinn 2025-03-08 [V1] Support bad_words in sampler (#13376)
24	1	tests/test_utils.py
1	0	tests/v1/sample/test_rejection_sampler.py
76	0	tests/v1/sample/test_sampler.py
16	2	tests/v1/sample/test_sampling_params_e2e.py
6	0	tests/v1/worker/test_gpu_input_batch.py
51	1	vllm/sampling_params.py
16	0	vllm/utils.py
2	3	vllm/v1/engine/processor.py
3	0	vllm/v1/sample/metadata.py
38	0	vllm/v1/sample/ops/bad_words.py
16	0	vllm/v1/sample/sampler.py
16	21	vllm/v1/worker/gpu_input_batch.py
1	0	vllm/v1/worker/gpu_model_runner.py

[951329003] Cyrus Leung 2025-03-09 [Misc] Upgrade to Python 3.9 typing for additional directories (#14492)
0	5	pyproject.toml
3	3	vllm/assets/video.py
27	27	vllm/inputs/data.py
9	8	vllm/inputs/parse.py
12	11	vllm/inputs/preprocess.py
3	2	vllm/inputs/registry.py
4	3	vllm/multimodal/base.py
2	1	vllm/multimodal/hasher.py
2	2	vllm/multimodal/image.py
7	7	vllm/multimodal/registry.py
2	2	vllm/multimodal/video.py
7	7	vllm/usage/usage_lib.py

[0d5e73d30] Russell Bryant 2025-03-08 Update CODEOWNERS for structured output (#14496)
15	10	.github/CODEOWNERS

[609ef61fe] Isotr0py 2025-03-09 [Bugfix] Fix profiling OOM and decouple encoder multimodal profiling (#14361)
1	1	tests/multimodal/test_processing.py
4	2	vllm/inputs/registry.py
54	30	vllm/multimodal/profiling.py

[db84f5eb3] Lucas Wilkinson 2025-03-08 [Bugfix] DeepSeek Accuracy (#14476)
7	5	vllm/v1/attention/backends/mla/common.py

[206e2577f] Harry Mellor 2025-03-08 Move requirements into their own directory (#12547)
1	1	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
1	1	.buildkite/run-cpu-test.sh
1	1	.buildkite/test-pipeline.yaml
1	1	.github/workflows/publish.yml
1	1	.github/workflows/scripts/build.sh
2	2	.pre-commit-config.yaml
1	1	.readthedocs.yaml
12	12	Dockerfile
5	5	Dockerfile.arm
5	5	Dockerfile.cpu
1	1	Dockerfile.hpu
1	1	Dockerfile.neuron
1	1	Dockerfile.openvino
2	2	Dockerfile.ppc64le
3	3	Dockerfile.rocm
4	4	Dockerfile.s390x
1	1	Dockerfile.tpu
3	3	Dockerfile.xpu
5	5	MANIFEST.in
1	1	docs/README.md
1	1	docs/source/contributing/overview.md
2	2	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
1	1	docs/source/getting_started/installation/ai_accelerator/neuron.inc.md
1	1	docs/source/getting_started/installation/ai_accelerator/openvino.inc.md
1	1	docs/source/getting_started/installation/ai_accelerator/tpu.inc.md
1	1	docs/source/getting_started/installation/cpu/apple.inc.md
1	1	docs/source/getting_started/installation/cpu/build.inc.md
1	1	docs/source/getting_started/installation/gpu/cuda.inc.md
1	1	docs/source/getting_started/installation/gpu/rocm.inc.md
1	1	docs/source/getting_started/installation/gpu/xpu.inc.md
1	1	format.sh
1	1	pyproject.toml
0	0	requirements-build.txt => requirements/build.txt
0	0	requirements-common.txt => requirements/common.txt
1	1	requirements-cpu.txt => requirements/cpu.txt
1	1	requirements-cuda.txt => requirements/cuda.txt
2	2	requirements-dev.txt => requirements/dev.txt
0	0	docs/requirements-docs.txt => requirements/docs.txt
1	1	requirements-hpu.txt => requirements/hpu.txt
0	0	requirements-lint.txt => requirements/lint.txt
1	1	requirements-neuron.txt => requirements/neuron.txt
1	1	requirements-openvino.txt => requirements/openvino.txt
1	1	requirements-rocm-build.txt => requirements/rocm-build.txt
1	1	requirements-rocm.txt => requirements/rocm.txt
0	0	requirements-test.in => requirements/test.in
36	36	requirements-test.txt => requirements/test.txt
1	1	requirements-tpu.txt => requirements/tpu.txt
1	1	requirements-xpu.txt => requirements/xpu.txt
12	15	setup.py
1	1	use_existing_torch.py

[e02883c40] Cyrus Leung 2025-03-08 [Misc] Don't run ruff at all on 3rd party libs (#14493)
1	1	pyproject.toml

[9085aabd6] Russell Bryant 2025-03-08 [benchmarks] Add option to use unique jsonschema for each request (#14457)
41	17	benchmarks/benchmark_serving_structured_output.py

[8d5aa466f] Roger Wang 2025-03-08 [V1][Core] Fix memory issue with logits & sampling (#13776)
10	1	tests/basic_correctness/test_cumem.py
38	28	vllm/v1/worker/gpu_model_runner.py
21	0	vllm/v1/worker/gpu_worker.py

[0b7f06b44] Aaron Pham 2025-03-08 [Misc] add `use_tqdm_on_load` to reduce logs (#14407)
3	0	vllm/config.py
10	0	vllm/engine/arg_utils.py
21	5	vllm/model_executor/model_loader/loader.py
20	17	vllm/model_executor/model_loader/weight_utils.py

[03fe18ae0] Isotr0py 2025-03-08 [VLM] Add TP support for Phi-4-MM (#14453)
1	0	examples/offline_inference/audio_language.py
24	49	vllm/model_executor/models/phi4mm.py
18	150	vllm/model_executor/models/phi4mm_audio.py
7	96	vllm/model_executor/models/phi4mm_utils.py

[cb8bdfade] Alexander Matveev 2025-03-08 [V1] TPU - Add tensor parallel support via Ray (#13618)
8	0	tests/entrypoints/llm/test_accuracy.py
0	0	tests/v1/tpu/__init__.py
54	0	tests/v1/tpu/test_basic.py
6	1	vllm/executor/ray_distributed_executor.py
8	2	vllm/executor/ray_utils.py
2	0	vllm/v1/worker/tpu_model_runner.py
2	1	vllm/v1/worker/tpu_worker.py

[33f227e16] Cyrus Leung 2025-03-08 [CI/Build] Use a fixed seed to avoid flaky tests (#14480)
0	2	tests/entrypoints/openai/test_chat_echo.py
0	2	tests/entrypoints/openai/test_metrics.py
0	2	tests/entrypoints/openai/test_root_path.py
7	0	tests/utils.py

[cfd0ae823] Harry Mellor 2025-03-08 Add RLHF document (#14482)
2	1	docs/source/generate_examples.py
1	0	docs/source/index.md
11	0	docs/source/training/rlhf.md

[7caff01a7] Lucas Wilkinson 2025-03-08 [Build/BugFix] Fix hopper 12.8 build (#14354)
45	33	CMakeLists.txt
34	0	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm100.cu
3	24	csrc/quantization/cutlass_w8a8/{scaled_mm_c3x.cu => scaled_mm_c3x_sm90.cu}
14	16	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[be0b399d7] Harry Mellor 2025-03-08 Add training doc signposting to TRL (#14439)
8	0	docs/source/index.md
13	0	docs/source/training/trl.md

[b8b0ccbd2] Jee Jee Li 2025-03-08 [Bugfix] Make the deviceprofiler include LoRA memory. (#14469)
29	29	vllm/worker/model_runner.py

[c908a07f5] Robin 2025-03-08 [Doc] Added QwQ-32B to the supported models list in the reasoning out… (#14479)
1	0	docs/source/features/reasoning_outputs.md

[7b6fd6e48] Robin 2025-03-08 [Doc]add doc for Qwen models tool calling (#14478)
9	0	docs/source/features/tool_calling.md

[47512b320] Harry Mellor 2025-03-08 Default to `generation_config` from model (#12622)
1	1	tests/entrypoints/openai/correctness/test_lmeval.py
1	0	tests/entrypoints/openai/test_serving_chat.py
4	4	tests/test_config.py
6	9	vllm/config.py
7	7	vllm/engine/arg_utils.py
4	2	vllm/entrypoints/openai/serving_chat.py
4	3	vllm/entrypoints/openai/serving_completion.py

[3b9c6c694] Roger Meier 2025-03-08 [CI/Build] refactor: set timezone of container to UTC (#12888)
2	6	Dockerfile

[4aae66766] Aviv Keshet 2025-03-07 [core] add `extra_args` to `SamplingParams` (#13300)
8	1	vllm/sampling_params.py

[9f3bc0f58] Cody Yu 2025-03-07 [MISC][V1] Register process killing handler only in the main thread (#14380)
9	1	vllm/v1/engine/core_client.py

[980385f8c] Mathis Felardos 2025-03-08 [Bugfix][Disaggregated] Add a check in send_kv_caches_and_hidden_states and fix the reshape of the KVCache (#14369)
14	3	vllm/distributed/kv_transfer/kv_connector/simple_connector.py

[ca7a2d5f2] Tyler Michael Smith 2025-03-08 Revert "[Perf] Reduce MLA CPU overheads in V1 (#14384)" (#14471)
2	7	vllm/model_executor/layers/rotary_embedding.py
4	11	vllm/v1/attention/backends/mla/common.py

[333681408] Tyler Michael Smith 2025-03-08 [Bugfix][V1] Handle MLA in kv_cache_interface (#14462)
8	5	vllm/v1/kv_cache_interface.py
3	2	vllm/v1/worker/gpu_model_runner.py
4	3	vllm/v1/worker/tpu_model_runner.py

[ef6404407] afeldman-nm 2025-03-07 [V1] Prompt logprobs + APC compatibility; prompt logprobs reqs cannot fill APC (#13949)
110	2	tests/v1/core/test_prefix_caching.py
49	12	tests/v1/core/test_scheduler.py
0	36	tests/v1/engine/test_async_llm.py
0	15	tests/v1/engine/test_llm_engine.py
0	3	tests/v1/engine/utils.py
85	60	tests/v1/sample/test_logprobs.py
24	9	tests/v1/sample/utils.py
24	19	vllm/v1/core/kv_cache_manager.py
0	6	vllm/v1/engine/processor.py

[66e16a038] yarongmu-google 2025-03-07 [Bugfix] Fix torch_xla which can't handle None seed introduced in #14274 (#14459)
3	0	vllm/v1/worker/tpu_worker.py

[e1f0835ae] Mark McLoughlin 2025-03-07 [V1][Metrics] Fix traceback with preemptions+LoRA (#14220)
8	0	vllm/v1/metrics/stats.py

[8ed5421aa] Nick Hill 2025-03-07 [V1] Eagerly remove finished requests from the batch (#14388)
10	0	tests/v1/engine/test_engine_core.py
2	2	tests/v1/engine/test_engine_core_client.py
10	1	vllm/v1/core/scheduler.py
3	3	vllm/v1/engine/async_llm.py
4	2	vllm/v1/engine/core.py
8	4	vllm/v1/metrics/loggers.py
10	0	vllm/v1/outputs.py
6	3	vllm/v1/worker/gpu_model_runner.py
5	1	vllm/v1/worker/tpu_model_runner.py

[c6359e8ca] youkaichao 2025-03-08 [v1] torch.compile integration explanation (#14437)
139	0	docs/source/design/v1/torch_compile.md
1	0	docs/source/index.md
14	1	vllm/compilation/compiler_interface.py

[952a07498] Jee Jee Li 2025-03-08 [Misc] Add Phi4-MM example (#14343)
38	0	examples/offline_inference/audio_language.py
38	0	examples/offline_inference/vision_language.py
44	0	examples/offline_inference/vision_language_multi_image.py
11	7	vllm/model_executor/models/phi4mm.py

[d0feea31c] Jinzhen Lin 2025-03-08 [Kernel] optimize performance of gptq marlin kernel when n is small (#14138)
46	16	csrc/quantization/gptq_marlin/gptq_marlin.cu
2	1	csrc/torch_bindings.cpp
10	6	tests/kernels/test_marlin_gemm.py
4	1	vllm/_custom_ops.py
5	0	vllm/envs.py
32	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[58abe3545] Jeremy Arnold 2025-03-07 [Benchmarks] Make detokenization optional in benchmark scripts (#11697)
7	0	benchmarks/benchmark_latency.py
9	1	benchmarks/benchmark_prefix_caching.py
11	2	benchmarks/benchmark_prioritization.py
18	4	benchmarks/benchmark_throughput.py

[f7ebad230] York-RDWang 2025-03-07 [Doc] Update prefix_caching.md to match the example image (#14420)
1	1	docs/source/design/v1/prefix_caching.md

[80e9afb5b] Aaron Pham 2025-03-07 [V1][Core] Support for Structured Outputs (#12388)
1	0	.buildkite/test-pipeline.yaml
1	1	.gitignore
0	507	benchmarks/benchmark_guided.py
47	32	benchmarks/{benchmark_serving_guided.py => benchmark_serving_structured_output.py}
64	0	benchmarks/run_structured_output_benchmark.sh
22	110	benchmarks/structured_schemas/structured_schema_1.json
28	11	tests/v1/core/test_scheduler.py
17	5	tests/v1/entrypoints/conftest.py
0	0	tests/v1/entrypoints/llm/__init__.py
269	0	tests/v1/entrypoints/llm/test_struct_output_generate.py
0	0	tests/v1/structured_output/__init__.py
196	0	tests/v1/structured_output/test_utils.py
12	0	tests/v1/worker/test_gpu_model_runner.py
57	2	vllm/utils.py
61	4	vllm/v1/core/scheduler.py
19	8	vllm/v1/core/scheduler_output.py
3	5	vllm/v1/engine/async_llm.py
21	1	vllm/v1/engine/core.py
1	3	vllm/v1/engine/llm_engine.py
33	14	vllm/v1/engine/processor.py
24	11	vllm/v1/request.py
152	0	vllm/v1/structured_output/__init__.py
77	0	vllm/v1/structured_output/grammar.py
71	0	vllm/v1/structured_output/request.py
295	0	vllm/v1/structured_output/utils.py
57	1	vllm/v1/worker/gpu_model_runner.py

[1e3598ede] iefgnoix 2025-03-07 Use the optimized block sizes after tuning the kernel. (#14329)
2	2	vllm/v1/attention/backends/pallas.py

[f7a6bd0fa] Harry Mellor 2025-03-07 Fix missing `kv_caches` and `attn_metadata` in `OpenVINOCausalLM` (#14271)
9	10	vllm/model_executor/model_loader/openvino.py
2	0	vllm/worker/openvino_model_runner.py

[0ca3b8e01] Aleksandr Malyshev 2025-03-07 [BUGFIX] Skip tokenization support for throughput benchmark (#12712)
33	22	benchmarks/benchmark_throughput.py
3	1	vllm/engine/arg_utils.py

[cc1028149] மனோஜ்குமார் பழனிச்சாமி 2025-03-07 [Misc] Set default value of seed to None (#14274)
2	1	tests/distributed/test_torchrun_example.py
2	1	tests/entrypoints/llm/test_encode.py
1	1	tests/entrypoints/llm/test_guided_generate.py
2	0	tests/entrypoints/openai/test_chat_echo.py
2	0	tests/entrypoints/openai/test_metrics.py
2	0	tests/entrypoints/openai/test_root_path.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/entrypoints/llm.py
2	2	vllm/utils.py

[05fb6718f] Cyrus Leung 2025-03-07 [Bugfix] Clean up multi-modal processors (#14417)
9	0	vllm/config.py
27	29	vllm/model_executor/models/deepseek_vl2.py
28	30	vllm/model_executor/models/h2ovl.py
1	1	vllm/model_executor/models/llava_next_video.py
2	2	vllm/model_executor/models/llava_onevision.py
3	3	vllm/model_executor/models/minicpmo.py
2	2	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/pixtral.py
2	2	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/multimodal/processing.py
1	3	vllm/multimodal/profiling.py
1	1	vllm/multimodal/registry.py

[12c29a881] Jee Jee Li 2025-03-07 [Bugfix] Further clean up LoRA test (#14422)
0	5	tests/lora/conftest.py
0	42	tests/lora/test_mixtral.py
2	1	tests/lora/test_quant_model.py

[70da0c074] Peng Li 2025-03-07 correct wrong markdown syntax (#14414)
1	1	docs/source/design/huggingface_integration.md

[c1588a2c9] Cyrus Leung 2025-03-07 [GH] Auto-apply multi-modality label to relevant PRs (#14402)
15	0	.github/mergify.yml

[8ca7a71df] Ilya Lavrenov 2025-03-07 OpenVINO: added CPU-like conditions (#14338)
2	1	tests/conftest.py
1	1	tests/utils.py

[63137cd92] Isotr0py 2025-03-07 [Build] Add nightly wheel fallback when latest commit wheel unavailable (#14358)
16	0	setup.py

[ddd1ef66e] Jee Jee Li 2025-03-07 [Bugfix] Fix JambaForCausalLM LoRA  (#14370)
0	24	tests/lora/conftest.py
0	54	tests/lora/test_jamba.py
3	0	tests/lora/test_layers.py
32	5	vllm/lora/layers.py

[e5e03c2c1] Lucas Wilkinson 2025-03-07 [BugFix] Illegal Memory Access in the blockwise cutlass fp8 GEMMs (#14396)
1	1	csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp

[e1744502c] Luka Govedič 2025-03-07 [FP8] Refactor apply_fp8_linear and apply_fp8_linear_generic into an object (#14390)
7	13	tests/compile/test_fusion.py
4	3	vllm/attention/backends/mla/common.py
8	11	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
9	13	vllm/model_executor/layers/quantization/fbgemm_fp8.py
10	11	vllm/model_executor/layers/quantization/fp8.py
7	9	vllm/model_executor/layers/quantization/modelopt.py
9	9	vllm/model_executor/layers/quantization/ptpc_fp8.py
7	11	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
54	38	vllm/model_executor/layers/quantization/utils/fp8_utils.py
149	121	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
4	3	vllm/v1/attention/backends/mla/common.py

[dae689697] Lucas Wilkinson 2025-03-06 [Perf] Reduce MLA CPU overheads in V1 (#14384)
7	2	vllm/model_executor/layers/rotary_embedding.py
11	4	vllm/v1/attention/backends/mla/common.py

[c34eeec58] Brayden Zhong 2025-03-06 [Bugfix] Correctly call `cudaProfilerStop` in benchmarks script (#14183)
1	1	benchmarks/kernels/benchmark_layernorm.py
0	1	benchmarks/kernels/benchmark_lora.py
1	2	benchmarks/kernels/benchmark_machete.py
1	0	benchmarks/kernels/benchmark_moe.py
1	1	benchmarks/kernels/benchmark_paged_attention.py
1	1	benchmarks/kernels/benchmark_quant.py

[ad60bbb2b] Daniel Li 2025-03-06 [Doc] Fix a typo (#14385)
1	1	benchmarks/benchmark_serving.py

[0578e5a46] Chengji Yao 2025-03-06 [Hardware][TPU]Enable ragged paged attention kernel and resolve recompilation issue (#14310)
6	6	requirements-tpu.txt
23	16	vllm/v1/attention/backends/pallas.py
29	44	vllm/v1/worker/tpu_model_runner.py

[04222984f] Michael Goin 2025-03-06 [Docs] Add nsight guide to profiling docs (#14298)
88	3	docs/source/contributing/profiling/profiling_index.md

[6832707e9] Michael Goin 2025-03-06 [V1][Bugfix] Standardize quantized kv cache rejection for attention backends (#14221)
4	0	vllm/attention/backends/abstract.py
8	1	vllm/attention/backends/flash_attn.py
6	3	vllm/attention/backends/flashmla.py
6	1	vllm/attention/backends/hpu_attn.py
3	2	vllm/attention/backends/ipex_attn.py
3	2	vllm/attention/backends/pallas.py
6	2	vllm/attention/backends/torch_sdpa.py
6	3	vllm/attention/backends/triton_mla.py
5	1	vllm/v1/attention/backends/flash_attn.py
6	4	vllm/v1/attention/backends/mla/flashmla.py
6	1	vllm/v1/attention/backends/mla/triton_mla.py

[6b2ef5cd1] Michael Goin 2025-03-06 [Bug] Fix Attention when ignored in by quant_method (#14313)
3	1	vllm/attention/layer.py

[958adce47] Tyler Michael Smith 2025-03-06 [Bugfix] Fix use_direct_call condition in FusedMoE layer for  (#14382)
1	1	vllm/model_executor/layers/fused_moe/layer.py

[99b0915d3] Tyler Michael Smith 2025-03-06 [Kernel] Add needs_fixed_stride_order tag to most GEMMs (#14306)
40	15	csrc/torch_bindings.cpp

[8ca2b21c9] Thomas Parnell 2025-03-06 [CI] Disable spawn when running V1 Test (#14345)
0	1	.buildkite/test-pipeline.yaml

[d9292786e] Michael Goin 2025-03-06 [CI/Build] Use uv python for docker rather than ppa:deadsnakes/ppa (#13569)
38	44	Dockerfile

[cc2f9b32c] Tyler Michael Smith 2025-03-06 [Distributed] Add enable_expert_parallel arg (#14305)
3	3	examples/offline_inference/data_parallel.py
2	1	vllm/config.py
7	0	vllm/engine/arg_utils.py
0	7	vllm/envs.py
15	10	vllm/model_executor/layers/fused_moe/layer.py

[cd579352b] Himanshu Jaju 2025-03-06 [V1] Do not detokenize if sampling param detokenize is False (#14224)
28	1	tests/v1/sample/test_sampling_params_e2e.py
24	16	vllm/v1/engine/detokenizer.py
15	10	vllm/v1/engine/logprobs.py
2	0	vllm/v1/engine/output_processor.py

[9f1710f1a] Ying Zhong 2025-03-07 Fix mla prefill context performance (#13897)
1	1	vllm/attention/backends/mla/common.py
1	1	vllm/v1/attention/backends/mla/common.py

[e642ec962] Thomas Parnell 2025-03-06 Add authors to license header. (#14371)
5	0	vllm/attention/ops/chunked_prefill_paged_decode.py

[ada19210a] Dilip Gowda Bhagavan 2025-03-06 Adding cpu inference with VXE ISA for s390x architecture (#12613)
152	0	Dockerfile.s390x
10	1	cmake/cpu_extension.cmake
2	2	csrc/cpu/attention.cpp
3	0	csrc/cpu/cpu_types.hpp
480	0	csrc/cpu/cpu_types_vxe.hpp
1	1	csrc/cpu/quant.cpp
5	4	requirements-cpu.txt

[bf0560bda] Harry Mellor 2025-03-06 Reinstate `best_of` for V0 (#14356)
8	0	tests/v1/sample/test_sampling_params_e2e.py
5	1	vllm/entrypoints/llm.py
4	0	vllm/entrypoints/openai/protocol.py
6	2	vllm/entrypoints/openai/serving_completion.py
24	0	vllm/sampling_params.py
3	0	vllm/v1/engine/processor.py

[151b08e0f] youkaichao 2025-03-07 [RLHF] use worker_extension_cls for compatibility with V0 and V1 (#14185)
4	2	.buildkite/test-pipeline.yaml
3	63	examples/offline_inference/rlhf.py
1	35	examples/offline_inference/rlhf_colocate.py
105	0	examples/offline_inference/rlhf_utils.py
4	0	vllm/config.py
9	0	vllm/engine/arg_utils.py
27	0	vllm/worker/worker_base.py

[81b2f4a45] Jitse Klomp 2025-03-06 [Doc] Fix date typo in README.md (#14366)
1	1	README.md

[82551ad61] Cyrus Leung 2025-03-07 [Core] Don't use cache during multi-modal profiling (#14336)
3	1	vllm/inputs/registry.py
12	4	vllm/multimodal/registry.py

[caac5c2e5] courage17340 2025-03-06 [Bugfix][Core] fix abort_seq_group and memory leak when n>1 (#14326)
24	9	vllm/core/scheduler.py
7	1	vllm/engine/llm_engine.py

[6bd1dd9d2] Thomas Parnell 2025-03-06 [Kernel] [V1] Improved performance for V1 Triton (ROCm) backend  (#14152)
76	59	tests/kernels/test_prefix_prefill.py
289	0	vllm/attention/ops/chunked_prefill_paged_decode.py
13	1	vllm/attention/ops/prefix_prefill.py
20	17	vllm/v1/attention/backends/rocm_attn.py

[4f27044aa] Irina Yuryeva 2025-03-06 [Doc] Correct beam_search using in generative_models.md (#14363)
6	4	docs/source/models/generative_models.md

[0ddc991f5] Yanyi Liu 2025-03-06 [Doc] Update reasoning with stream example to use OpenAI library (#14077)
49	1	docs/source/features/reasoning_outputs.md
33	56	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py

[fa82b9385] Nicolò Lucchesi 2025-03-06 [Frontend][Docs] Transcription API streaming (#13301)
4	0	docs/source/serving/openai_compatible_server.md
51	8	examples/online_serving/openai_transcription_client.py
72	0	tests/entrypoints/openai/test_transcription_validation.py
39	1	vllm/entrypoints/openai/protocol.py
131	17	vllm/entrypoints/openai/serving_transcription.py

[69ff99fdc] Nicolò Lucchesi 2025-03-06 [Core] Optimizing cross-attention `QKVParallelLinear` computation (#12325)
95	0	vllm/model_executor/layers/linear.py
13	26	vllm/model_executor/models/bart.py
12	17	vllm/model_executor/models/mllama.py
1	1	vllm/model_executor/models/utils.py

[5d802522a] lkchen 2025-03-06 [V1][VLM][Pixtral-HF] Support Pixtral-HF on V1 (#14275)
1	5	docs/source/models/supported_models.md
164	7	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/molmo.py
8	2	vllm/model_executor/models/pixtral.py

[176992807] kYLe 2025-03-06 [Model] Update Paligemma multimodal processing with PromptUpdate  (#14015)
3	3	docs/source/models/supported_models.md
2	3	tests/models/decoder_only/vision_language/test_models.py
2	0	tests/models/multimodal/processing/test_common.py
139	80	vllm/model_executor/models/paligemma.py

[ed6ea0657] Pavani Majety 2025-03-05 [Hardware] Update the flash attn tag to support Blackwell (#14244)
2	2	cmake/external_projects/vllm_flash_attn.cmake
6	1	vllm/attention/backends/utils.py

[5ee10e990] Nicolò Lucchesi 2025-03-06 [Bugfix][CI] ALiBi test case in xformers multi_query_kv_attention (#11301)
78	17	tests/kernels/test_attention.py
5	3	tests/kernels/test_prefix_prefill.py
0	2	vllm/attention/backends/xformers.py

[3dbd2d813] Varun Sundar Rabindranath 2025-03-05 [V1] LoRA - Enable more V1 tests (#14315)
0	3	tests/lora/test_chatglm3_tp.py
8	0	tests/lora/test_mixtral.py
8	0	tests/lora/test_qwen2vl.py
9	0	tests/lora/test_ultravox.py
32	5	tests/lora/test_worker.py

[f5f7f00cd] Ce Gao 2025-03-06 [Bugfix][Structured Output] Support outlines engine with reasoning outputs for DeepSeek R1 (#14114)
46	3	docs/source/features/reasoning_outputs.md
67	2	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
1	0	vllm/model_executor/guided_decoding/__init__.py
9	5	vllm/model_executor/guided_decoding/outlines_logits_processors.py
9	1	vllm/model_executor/guided_decoding/reasoner/__init__.py
10	0	vllm/model_executor/guided_decoding/reasoner/deepseek_reasoner.py
4	0	vllm/model_executor/guided_decoding/reasoner/reasoner.py
1	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[abcc61e0a] Rui Qiao 2025-03-05 [misc] Mention `ray list nodes` command to troubleshoot ray issues (#14318)
2	2	docs/source/serving/distributed_serving.md
7	6	vllm/executor/ray_utils.py

[f6bb18fd9] Lucas Wilkinson 2025-03-05 [BugFix] MLA + V1, illegal memory access and accuracy issues (#14253)
89	1	tests/v1/worker/test_gpu_input_batch.py
2	2	vllm/v1/attention/backends/flash_attn.py
178	127	vllm/v1/attention/backends/mla/common.py
33	25	vllm/v1/attention/backends/mla/flashmla.py
5	2	vllm/v1/attention/backends/mla/triton_mla.py
23	2	vllm/v1/worker/gpu_input_batch.py
4	2	vllm/v1/worker/gpu_model_runner.py

[71eaf8969] Yuan Tang 2025-03-05 [Build] Add UV_HTTP_TIMEOUT to avoid timeout during installation (#13850)
24	0	Dockerfile

[ca100c90f] Michael Goin 2025-03-05 Add benchmark for DeepGEMM and vLLM Block FP8 Dense GEMM (#13917)
129	0	benchmarks/kernels/deepgemm/README.md
464	0	benchmarks/kernels/deepgemm/benchmark_fp8_block_dense_gemm.py

[ffad94397] Russell Bryant 2025-03-05 [CI/Build] Use spawn multiprocessing mode for V1 test pipeline (#14243)
3	0	.buildkite/test-pipeline.yaml

[4dacaa4a8] Lucas Wilkinson 2025-03-05 [BugFix] Fix prefix caching V0 MLA (#14255)
24	20	vllm/attention/backends/mla/common.py

[a7ea35aa6] Tyler Michael Smith 2025-03-05 [Bugfix] Remove num_tokens_across_dp (#14302)
1	2	vllm/forward_context.py

[1e3e76b6c] pyc96 2025-03-05 [Bugfix] Fix DeepSeek MTP crash when using TP1ModelRunner with CUDA graph due to shape mismatch (#14237)
9	4	vllm/spec_decode/draft_model_runner.py

[53ea6ad83] Lu Fang 2025-03-05 [V1][Easy] Add empty allowed_token_ids in the v1 sampler test (#14308)
4	0	tests/v1/sample/test_sampling_params_e2e.py

[1b7624bf5] Serena 2025-03-06 [misc] Add FlashMLA as a new option of VLLM_ATTENTION_BACKEND env (#14267)
1	0	vllm/envs.py

[ac60dc7fe] Nick Hill 2025-03-05 [V1][BugFix] Fix for mixed top_k batch (#14301)
5	2	vllm/v1/worker/gpu_input_batch.py

[a4f1ee35d] Vincent 2025-03-05 Deprecate `best_of` Sampling Parameter in anticipation for vLLM V1 (#13997)
0	5	benchmarks/backend_request_func.py
0	14	benchmarks/benchmark_serving.py
0	1	examples/offline_inference/llm_engine_example.py
0	1	examples/online_serving/opentelemetry/dummy_client.py
0	4	tests/core/test_scheduler.py
13	14	tests/core/utils.py
0	8	tests/v1/sample/test_sampling_params_e2e.py
1	4	vllm/entrypoints/llm.py
0	4	vllm/entrypoints/openai/protocol.py
2	6	vllm/entrypoints/openai/serving_completion.py
0	24	vllm/sampling_params.py
0	3	vllm/v1/engine/processor.py

[a32c8669c] Nick Hill 2025-03-05 [V1][Minor] Remove obsolete FIXME comment (#14304)
0	5	vllm/v1/worker/gpu_input_batch.py

[ca2ca8de5] Simon Mo 2025-03-05 [Docs] Add Meta Slides (#14297)
1	0	README.md
1	0	docs/source/community/meetups.md

[f71b00a19] Isotr0py 2025-03-05 [Bugfix] Fix broken vision language example (#14292)
18	22	examples/offline_inference/vision_language.py

[8f808cf86] DaividFrank 2025-03-05 prefix_caching.md: Fixed typo (#14293)
1	1	docs/source/design/v1/prefix_caching.md

[7bab4bb04] Jee Jee Li 2025-03-05 [Misc] Add Qwen2MoeForCausalLM moe tuning support  (#14276)
5	0	benchmarks/kernels/benchmark_moe.py

[e17e4488b] Isotr0py 2025-03-05 [LoRA] Remove linear hack outside transformers backend (#14177)
30	21	vllm/lora/layers.py
0	10	vllm/lora/utils.py
110	61	vllm/model_executor/layers/linear.py
2	13	vllm/model_executor/models/transformers.py

[257e200a2] Robert Shaw 2025-03-05 [V1][Frontend] Add Testing For V1 Runtime Parameters (#14159)
150	0	tests/v1/sample/test_sampling_params_e2e.py
46	17	vllm/v1/engine/processor.py
5	0	vllm/v1/worker/gpu_input_batch.py

[47d4a7e00] Zhe Zhang 2025-03-05 Small update for external_launcher backend docs (#14288)
1	0	vllm/executor/uniproc_executor.py

[7f89a594d] Cyrus Leung 2025-03-05 [Doc] [3/N] Refer code examples for common cases in dev multimodal processor (#14278)
33	1	docs/source/contributing/model/multimodal.md

[961644e6a] Iacopo Poli 2025-03-05 [Doc] Update nginx guide: remove privileged from vllm container run and add target GPU ID (#14217)
3	3	docs/source/deployment/nginx.md

[8d6cd32b7] Lu Fang 2025-03-05 [Bugfix][V1] Fix allowed_token_ids for v1 Sampler (#14169)
5	3	vllm/v1/engine/processor.py
7	1	vllm/v1/worker/gpu_input_batch.py

[ec79b67c7] Roger Wang 2025-03-04 [Misc][V1] Avoid using `envs.VLLM_USE_V1` in mm processing (#14256)
22	2	vllm/inputs/preprocess.py
3	1	vllm/model_executor/models/llava.py
3	1	vllm/model_executor/models/minicpmv.py
3	1	vllm/model_executor/models/mllama.py
1	0	vllm/model_executor/models/prithvi_geospatial_mae.py
5	3	vllm/multimodal/processing.py
1	0	vllm/v1/engine/processor.py

[32985bed7] Benjamin Chislett 2025-03-05 [Frontend] Allow return_tokens_as_token_ids to be passed as a request param (#14066)
30	14	tests/entrypoints/openai/test_return_tokens_as_ids.py
12	0	vllm/entrypoints/openai/protocol.py
13	8	vllm/entrypoints/openai/serving_chat.py
9	3	vllm/entrypoints/openai/serving_completion.py

[dae9ec464] Michael Goin 2025-03-05 Temporarily disable test_awq_gemm_opcheck (#14251)
1	0	tests/kernels/test_awq.py

[6eaf93020] youkaichao 2025-03-05 [platforms] improve rocm debugging info (#14257)
3	0	vllm/platforms/__init__.py

[72c62eae5] Tyler Michael Smith 2025-03-05 [V1] EP/TP MoE + DP Attention (#13931)
10	7	examples/offline_inference/data_parallel.py
1	0	tests/kernels/test_moe.py
2	2	vllm/attention/layer.py
3	2	vllm/compilation/backends.py
15	7	vllm/forward_context.py
155	32	vllm/model_executor/layers/fused_moe/layer.py
11	7	vllm/model_executor/models/aria.py
6	2	vllm/model_executor/models/dbrx.py
15	6	vllm/model_executor/models/jamba.py
2	0	vllm/model_executor/models/mixtral.py
3	1	vllm/model_executor/models/olmoe.py
4	1	vllm/model_executor/models/phimoe.py
5	2	vllm/model_executor/models/qwen2_moe.py
9	0	vllm/platforms/cuda.py
2	2	vllm/utils.py
0	1	vllm/v1/engine/core.py
7	3	vllm/v1/worker/gpu_model_runner.py

[0a995d543] Congcong Chen 2025-03-04 [Model] New model support for Phi-4-multimodal-instruct (#14119)
8	1	docs/source/models/supported_models.md
1	0	requirements-common.txt
2	0	tests/models/registry.py
2	2	vllm/config.py
4	0	vllm/entrypoints/chat_utils.py
1803	0	vllm/model_executor/models/phi4mm.py
1403	0	vllm/model_executor/models/phi4mm_audio.py
1969	0	vllm/model_executor/models/phi4mm_utils.py
1	0	vllm/model_executor/models/registry.py
1966	0	vllm/model_executor/models/vision_siglip_navit.py

[ade3f7d98] Cody Yu 2025-03-04 [V1][Bugfix] Do not reset prefix caching metrics (#14235)
3	3	vllm/v1/metrics/loggers.py

[0df25101d] rainkert 2025-03-05 [Bugfix] Fix gptq_marlin for deepseek-v3 (#13750)
3	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[e123aafdf] Michael Goin 2025-03-04 Disable GPTQ AllSpark kernels for CUDA Compiler < 12.0 (#14157)
2	2	CMakeLists.txt

[5b143d33b] Nishidha 2025-03-05 Moved numba from common requirements to cuda/rocm specific requirements (#14199)
0	1	requirements-common.txt
2	0	requirements-cuda.txt
3	1	requirements-rocm.txt

[eb59b5a6c] youkaichao 2025-03-05 [misc] announce china meetup (#14248)
5	1	README.md

[fbfc3ee37] Michael Goin 2025-03-04 [V1][TPU] TPU multimodal model support for ragged attention (#14158)
193	29	vllm/v1/worker/tpu_model_runner.py
1	1	vllm/v1/worker/tpu_worker.py

[3e1d22362] Sage Moore 2025-03-04 [ROCm] Disable a few more kernel tests that are broken on ROCm (#14145)
6	1	.buildkite/run-amd-test.sh

[4f5b059f1] Tyler Michael Smith 2025-03-04 Clean up unused padding_idx variables across many model definitions (#13240)
0	1	vllm/model_executor/models/arctic.py
0	1	vllm/model_executor/models/baichuan.py
0	1	vllm/model_executor/models/bart.py
0	1	vllm/model_executor/models/chameleon.py
0	1	vllm/model_executor/models/deepseek.py
0	1	vllm/model_executor/models/deepseek_v2.py
0	1	vllm/model_executor/models/exaone.py
0	1	vllm/model_executor/models/florence2.py
0	1	vllm/model_executor/models/fuyu.py
0	1	vllm/model_executor/models/granite.py
0	1	vllm/model_executor/models/granitemoe.py
0	1	vllm/model_executor/models/idefics3.py
0	1	vllm/model_executor/models/internlm2.py
0	1	vllm/model_executor/models/jamba.py
0	1	vllm/model_executor/models/llama.py
0	1	vllm/model_executor/models/mamba.py
0	1	vllm/model_executor/models/minicpm.py
0	1	vllm/model_executor/models/mixtral.py
0	1	vllm/model_executor/models/mixtral_quant.py
0	1	vllm/model_executor/models/mllama.py
0	1	vllm/model_executor/models/nemotron.py
0	1	vllm/model_executor/models/olmoe.py
0	1	vllm/model_executor/models/opt.py
0	1	vllm/model_executor/models/orion.py
0	1	vllm/model_executor/models/phimoe.py
0	1	vllm/model_executor/models/qwen2.py
0	1	vllm/model_executor/models/qwen2_moe.py
0	1	vllm/model_executor/models/solar.py
0	2	vllm/model_executor/models/starcoder2.py
1	5	vllm/model_executor/models/whisper.py

[288ca110f] Kuntai Du 2025-03-04 [Security] Serialize using safetensors instead of pickle in Mooncake Pipe (#14228)
6	6	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py

[c2bd2196f] Mark McLoughlin 2025-03-04 [v1][Metrics] Add design doc (#12745)
-	-	docs/source/assets/design/v1/metrics/intervals-1.png
-	-	docs/source/assets/design/v1/metrics/intervals-2.png
-	-	docs/source/assets/design/v1/metrics/intervals-3.png
712	0	docs/source/design/v1/metrics.md
1	0	docs/source/index.md

[550c7ba3d] Michael Goin 2025-03-04 [Docs] Update Dockerfile dependency image (#14215)
-	-	docs/source/assets/contributing/dockerfile-stages-dependency.png

[e5b2f1601] Harry Mellor 2025-03-04 [Frontend] Do `prompt_logprobs` clamping for chat as well as completions (#14225)
3	2	vllm/entrypoints/openai/serving_chat.py
3	8	vllm/entrypoints/openai/serving_completion.py
16	1	vllm/entrypoints/openai/serving_engine.py

[9badee53d] Harry Mellor 2025-03-04 Fix performance when `--generation-config` is not `None` (#14223)
6	4	vllm/entrypoints/llm.py
6	8	vllm/entrypoints/openai/serving_chat.py
6	8	vllm/entrypoints/openai/serving_completion.py
5	5	vllm/entrypoints/openai/serving_transcription.py

[beebf4742] Siyuan Liu 2025-03-04 [TPU][Profiler] Support start_profile/stop_profile in TPU worker (#13988)
6	5	requirements-tpu.txt
19	0	vllm/v1/worker/tpu_worker.py
22	0	vllm/worker/tpu_worker.py

[f89978ad7] kushanam 2025-03-04 add cutlass support for blackwell fp8 gemm (#13798)
24	8	CMakeLists.txt
23	25	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
20	12	csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh
62	6	csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh
6	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp
24	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8.cu
67	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm100_fp8_dispatch.cuh
25	0	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
19	2	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
1	6	csrc/quantization/machete/machete_mm_kernel.cuh
1	6	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh

[b3cf368d7] lkchen 2025-03-04 [V1][Molmo] Fix get_multimodal_embeddings() in molmo.py (#14161)
176	118	examples/offline_inference/vision_language.py
3	1	vllm/model_executor/models/aria.py
3	1	vllm/model_executor/models/blip2.py
3	1	vllm/model_executor/models/chameleon.py
3	1	vllm/model_executor/models/deepseek_vl2.py
3	1	vllm/model_executor/models/florence2.py
4	2	vllm/model_executor/models/fuyu.py
3	1	vllm/model_executor/models/glm4v.py
3	1	vllm/model_executor/models/idefics3.py
9	9	vllm/model_executor/models/interfaces.py
3	1	vllm/model_executor/models/internvl.py
3	1	vllm/model_executor/models/llava.py
3	1	vllm/model_executor/models/llava_next.py
3	1	vllm/model_executor/models/llava_next_video.py
6	3	vllm/model_executor/models/molmo.py
3	1	vllm/model_executor/models/paligemma.py
3	1	vllm/model_executor/models/phi3v.py
3	1	vllm/model_executor/models/pixtral.py
3	1	vllm/model_executor/models/qwen2_audio.py
3	1	vllm/model_executor/models/qwen_vl.py
3	1	vllm/model_executor/models/ultravox.py
3	1	vllm/model_executor/models/whisper.py

[c8525f06f] Mark McLoughlin 2025-03-04 [V0][Metrics] Deprecate some questionable request time metrics (#14135)
17	6	vllm/engine/metrics.py

[5db6b2c96] Nick Hill 2025-03-04 [V1][BugFix] Fix remaining sync engine client shutdown errors/hangs (#13869)
0	2	tests/v1/engine/test_llm_engine.py
12	10	vllm/utils.py
56	28	vllm/v1/engine/core_client.py

[6247bae6c] Michael Goin 2025-03-04 [Bugfix] Restrict MacOS CPU detection (#14210)
3	3	vllm/platforms/__init__.py

[3610fb493] youkaichao 2025-03-04 [doc] add "Failed to infer device type" to faq (#14200)
4	0	docs/source/getting_started/troubleshooting.md

[71c4b4056] youkaichao 2025-03-04 [sleep mode] error out with expandable_segments (#14189)
7	0	vllm/device_allocator/cumem.py

[ac65bc92d] youkaichao 2025-03-04 [platform] add debug logging during inferring the device type (#14195)
55	9	vllm/platforms/__init__.py

[f78c0be80] Michael Goin 2025-03-04 Fix benchmark_moe.py tuning for CUDA devices (#14164)
3	1	benchmarks/kernels/benchmark_moe.py

[66233af7b] Zhanwen Chen 2025-03-04 Use math.prod instead of np.prod for trivial ops (#14142)
2	2	vllm/worker/cache_engine.py

[bf13d4097] Rui Qiao 2025-03-03 [core] Pass all driver env vars to ray workers unless excluded (#14099)
27	3	vllm/executor/ray_distributed_executor.py

[989f4f430] Cody Yu 2025-03-03 [Misc] Remove lru_cache in NvmlCudaPlatform (#14156)
1	6	vllm/platforms/cuda.py

[bb5b64035] Divakar Verma 2025-03-03 [core] moe fp8 block quant tuning support (#14068)
67	31	benchmarks/kernels/benchmark_moe.py
62	26	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json

[c060b7140] Travis Johnson 2025-03-03 [Model] Add support for GraniteMoeShared models (#13313)
5	0	docs/source/models/supported_models.md
2	0	tests/models/registry.py
343	0	vllm/model_executor/models/granitemoeshared.py
1	0	vllm/model_executor/models/registry.py

[79e4937c6] iefgnoix 2025-03-03 [v1] Add comments to the new ragged paged attention Pallas kernel (#14155)
5	1	vllm/v1/attention/backends/pallas.py

[cd1d3c3df] Qubitium-ModelCloud 2025-03-04 [Docs] Add GPTQModel (#14056)
1	1	docs/source/features/quantization/auto_awq.md
83	0	docs/source/features/quantization/gptqmodel.md
1	0	docs/source/features/quantization/index.md

[19d98e0c7] Michael Goin 2025-03-03 [Kernel] Optimize moe intermediate_cache usage (#13625)
11	6	vllm/model_executor/layers/fused_moe/fused_moe.py

[2b04c209e] Michael Goin 2025-03-03 [Bugfix] Allow shared_experts skip quantization for DeepSeekV2/V3 (#14100)
1	0	vllm/model_executor/models/deepseek_v2.py

[ae122b1cb] Mark McLoughlin 2025-03-03 [WIP][[V1][Metrics] Implement max_num_generation_tokens,  request_params_n, and request_params_max_tokens metrics (#14055)
6	0	tests/entrypoints/openai/test_metrics.py
13	0	vllm/v1/engine/output_processor.py
37	2	vllm/v1/engine/parallel_sampling.py
50	0	vllm/v1/metrics/loggers.py
5	0	vllm/v1/metrics/stats.py

[872db2be0] Nick Hill 2025-03-03 [V1] Simplify stats logging (#14082)
11	10	vllm/v1/engine/async_llm.py
4	13	vllm/v1/engine/core.py
15	14	vllm/v1/metrics/loggers.py

[2dfdfed8a] Mark McLoughlin 2025-03-03 [V0][Metrics] Deprecate some KV/prefix cache metrics (#14136)
25	5	vllm/engine/metrics.py

[c41d27156] Mark McLoughlin 2025-03-03 [V0][Metrics] Remove unimplemented `vllm:tokens_total` (#14134)
0	4	vllm/engine/metrics.py

[91373a0d1] Harry Mellor 2025-03-03 Fix `head_dim` not existing in all model configs (Transformers backend) (#14141)
15	12	vllm/model_executor/models/transformers.py

[848a6438a] TJian 2025-03-04 [ROCm] Faster Custom Paged Attention kernels (#12348)
0	1	.buildkite/run-amd-test.sh
51	20	benchmarks/kernels/benchmark_paged_attention.py
1084	422	csrc/rocm/attention.cu
1	1	requirements-rocm.txt
7	1	tests/kernels/test_attention.py
2	2	vllm/attention/backends/rocm_flash_attn.py

[98175b281] Harry Mellor 2025-03-03 Improve the docs for `TransformersModel` (#14147)
49	19	docs/source/models/supported_models.md

[4167252ea] Mark McLoughlin 2025-03-03 [V1] Refactor parallel sampling support (#13774)
21	40	vllm/v1/engine/async_llm.py
22	52	vllm/v1/engine/llm_engine.py
112	69	vllm/v1/engine/output_processor.py
44	300	vllm/v1/engine/parallel_sampling.py
2	3	vllm/v1/metrics/stats.py

[f35f8e224] Cody Yu 2025-03-03 [Build] Make sure local main branch is synced when VLLM_USE_PRECOMPILED=1 (#13921)
27	1	setup.py
1	1	tests/standalone_tests/python_only_compile.sh
7	1	vllm/envs.py

[b87c21fc8] Mengqing Cao 2025-03-03 [Misc][Platform] Move use allgather to platform (#14010)
3	7	vllm/model_executor/layers/logits_processor.py
13	0	vllm/platforms/interface.py
4	0	vllm/platforms/neuron.py
4	0	vllm/platforms/tpu.py

[e584b85af] wang.yuqi 2025-03-03 [Misc] duplicate code in deepseek_v2  (#14106)
0	1	vllm/model_executor/models/deepseek_v2.py

[09e56f926] Sheng Yao 2025-03-03 [Bugfix] Explicitly include "omp.h" for MacOS to avoid installation failure (#14051)
4	0	csrc/cpu/cpu_types_arm.hpp

[cf069aa8a] Harry Mellor 2025-03-03 Update deprecated Python 3.8 typing (#13971)
3	3	benchmarks/backend_request_func.py
8	9	benchmarks/benchmark_guided.py
3	3	benchmarks/benchmark_latency.py
8	8	benchmarks/benchmark_prefix_caching.py
4	4	benchmarks/benchmark_prioritization.py
39	38	benchmarks/benchmark_serving.py
29	28	benchmarks/benchmark_serving_guided.py
19	19	benchmarks/benchmark_throughput.py
4	4	benchmarks/benchmark_utils.py
5	4	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
4	4	benchmarks/cutlass_benchmarks/utils.py
9	8	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
3	2	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
30	30	benchmarks/kernels/benchmark_lora.py
13	12	benchmarks/kernels/benchmark_machete.py
2	4	benchmarks/kernels/benchmark_marlin.py
9	9	benchmarks/kernels/benchmark_moe.py
2	2	benchmarks/kernels/benchmark_paged_attention.py
2	2	benchmarks/kernels/benchmark_rmsnorm.py
2	2	benchmarks/kernels/benchmark_rope.py
1	2	benchmarks/kernels/graph_machete_bench.py
2	1	benchmarks/kernels/utils.py
7	7	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
10	10	csrc/quantization/machete/generate.py
1	2	docs/source/conf.py
2	2	docs/source/features/reasoning_outputs.md
1	1	docs/source/features/structured_outputs.md
1	1	docs/source/generate_examples.py
5	5	examples/offline_inference/distributed.py
3	4	examples/offline_inference/llm_engine_example.py
4	4	examples/offline_inference/lora_with_quantization_inference.py
1	2	examples/offline_inference/mlpspeculator.py
4	4	examples/offline_inference/multilora_inference.py
4	4	examples/offline_inference/prithvi_geospatial_mae.py
8	7	examples/offline_inference/profiling.py
1	2	examples/offline_inference/profiling_tpu/profiling.py
17	17	examples/offline_inference/vision_language_multi_image.py
3	3	examples/online_serving/api_client.py
1	1	examples/online_serving/openai_embedding_client.py
26	2	pyproject.toml
3	4	setup.py
3	2	tests/async_engine/api_server_async_engine.py
2	2	tests/async_engine/test_async_llm_engine.py
3	3	tests/compile/piecewise/test_toy_llama.py
4	4	tests/compile/test_basic_correctness.py
79	80	tests/conftest.py
2	1	tests/core/block/e2e/conftest.py
5	6	tests/core/block/e2e/test_correctness_sliding_window.py
3	5	tests/core/block/test_block_table.py
2	2	tests/core/block/test_naive_block.py
8	8	tests/core/block/test_prefix_caching_block.py
12	13	tests/core/test_chunked_prefill_scheduler.py
9	10	tests/core/test_scheduler.py
1	3	tests/core/test_scheduler_encoder_decoder.py
10	11	tests/core/utils.py
3	3	tests/distributed/test_expert_parallel.py
4	4	tests/distributed/test_pipeline_parallel.py
2	3	tests/distributed/test_pynccl.py
1	2	tests/distributed/test_shm_broadcast.py
2	2	tests/encoder_decoder/test_e2e_correctness.py
3	3	tests/engine/test_executor.py
3	3	tests/engine/test_multiproc_workers.py
3	3	tests/engine/test_stop_strings.py
1	3	tests/entrypoints/llm/test_chat.py
2	3	tests/entrypoints/llm/test_encode.py
1	2	tests/entrypoints/llm/test_generate.py
1	2	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py
1	3	tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
7	7	tests/entrypoints/openai/reasoning_parsers/utils.py
7	9	tests/entrypoints/openai/test_audio.py
1	2	tests/entrypoints/openai/test_basic.py
4	4	tests/entrypoints/openai/test_chat.py
4	4	tests/entrypoints/openai/test_completion.py
2	2	tests/entrypoints/openai/test_embedding.py
2	2	tests/entrypoints/openai/test_pooling.py
2	2	tests/entrypoints/openai/test_root_path.py
5	7	tests/entrypoints/openai/test_video.py
5	7	tests/entrypoints/openai/test_vision.py
1	3	tests/entrypoints/openai/test_vision_embedding.py
1	2	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
5	4	tests/entrypoints/openai/tool_parsers/utils.py
3	3	tests/kernels/quant_utils.py
1	2	tests/kernels/test_activation.py
8	8	tests/kernels/test_attention.py
6	6	tests/kernels/test_blocksparse_attention.py
2	3	tests/kernels/test_cache.py
4	4	tests/kernels/test_cascade_flash_attn.py
5	6	tests/kernels/test_cutlass.py
2	3	tests/kernels/test_cutlass_2of4_sparse.py
2	2	tests/kernels/test_encoder_decoder_attn.py
8	8	tests/kernels/test_flash_attn.py
11	11	tests/kernels/test_flashinfer.py
6	6	tests/kernels/test_fused_quant_layernorm.py
1	2	tests/kernels/test_gguf.py
7	7	tests/kernels/test_machete_mm.py
1	2	tests/kernels/test_mamba_mixer2.py
3	5	tests/kernels/test_mamba_ssm_ssd.py
3	3	tests/kernels/test_pos_encoding.py
2	2	tests/kernels/test_triton_scaled_mm.py
34	34	tests/kernels/utils.py
1	2	tests/kv_transfer/test_send_recv.py
3	3	tests/lora/conftest.py
2	2	tests/lora/data/long_context_test_data.py
4	5	tests/lora/test_add_lora.py
2	4	tests/lora/test_baichuan.py
2	4	tests/lora/test_chatglm3_tp.py
2	4	tests/lora/test_gemma.py
2	4	tests/lora/test_jamba.py
24	24	tests/lora/test_layers.py
2	4	tests/lora/test_llama_tp.py
8	8	tests/lora/test_long_context.py
2	4	tests/lora/test_lora_bias_e2e.py
2	4	tests/lora/test_lora_checkpoints.py
2	3	tests/lora/test_lora_functions.py
1	3	tests/lora/test_lora_huggingface.py
3	4	tests/lora/test_lora_manager.py
2	4	tests/lora/test_minicpmv_tp.py
2	4	tests/lora/test_mixtral.py
2	4	tests/lora/test_phi.py
2	3	tests/lora/test_punica_ops.py
3	4	tests/lora/test_quant_model.py
5	5	tests/lora/test_qwen2vl.py
2	4	tests/lora/test_transfomers_model.py
3	4	tests/lora/test_ultravox.py
7	7	tests/lora/utils.py
1	2	tests/metrics/test_metrics.py
4	4	tests/mistral_tool_use/utils.py
1	3	tests/model_executor/test_enabled_custom_ops.py
8	8	tests/models/decoder_only/audio_language/test_ultravox.py
3	3	tests/models/decoder_only/language/test_gguf.py
1	2	tests/models/decoder_only/language/test_modelopt.py
3	3	tests/models/decoder_only/vision_language/test_awq.py
19	20	tests/models/decoder_only/vision_language/test_models.py
5	5	tests/models/decoder_only/vision_language/test_phi3v.py
6	6	tests/models/decoder_only/vision_language/test_pixtral.py
23	23	tests/models/decoder_only/vision_language/test_qwen2_vl.py
4	3	tests/models/decoder_only/vision_language/vlm_utils/builders.py
5	5	tests/models/decoder_only/vision_language/vlm_utils/case_filtering.py
11	11	tests/models/decoder_only/vision_language/vlm_utils/core.py
8	8	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
10	11	tests/models/decoder_only/vision_language/vlm_utils/runners.py
18	18	tests/models/decoder_only/vision_language/vlm_utils/types.py
5	6	tests/models/embedding/language/test_gritlm.py
3	3	tests/models/embedding/utils.py
6	6	tests/models/embedding/vision_language/test_dse_qwen2_vl.py
3	5	tests/models/embedding/vision_language/test_llava_next.py
3	5	tests/models/embedding/vision_language/test_phi3v.py
5	5	tests/models/encoder_decoder/language/test_bart.py
4	4	tests/models/encoder_decoder/vision_language/test_florence2.py
18	18	tests/models/encoder_decoder/vision_language/test_mllama.py
2	1	tests/models/multimodal/processing/test_h2ovl.py
2	1	tests/models/multimodal/processing/test_internvl.py
3	2	tests/models/registry.py
7	8	tests/models/test_transformers.py
11	10	tests/models/utils.py
2	2	tests/mq_llm_engine/utils.py
2	2	tests/multi_step/test_correctness_async_llm.py
4	4	tests/multimodal/test_utils.py
1	2	tests/neuron/test_logits_processor.py
3	2	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
1	2	tests/quantization/test_configs.py
4	4	tests/quantization/test_register_quantization_config.py
1	3	tests/samplers/test_logprobs.py
8	8	tests/samplers/test_no_bad_words.py
5	6	tests/samplers/test_rejection_sampler.py
22	22	tests/samplers/test_sampler.py
5	4	tests/spec_decode/e2e/conftest.py
1	3	tests/spec_decode/test_batch_expansion.py
7	8	tests/spec_decode/test_multi_step_worker.py
1	2	tests/spec_decode/test_scorer.py
5	6	tests/spec_decode/test_spec_decode_worker.py
16	17	tests/spec_decode/utils.py
3	3	tests/test_cache_block_hashing.py
1	3	tests/test_inputs.py
1	1	tests/test_logger.py
1	2	tests/test_logits_processor.py
2	2	tests/test_utils.py
12	11	tests/tokenization/test_detokenize.py
2	2	tests/tokenization/test_tokenizer_group.py
16	16	tests/tokenization/test_tokenizer_registry.py
2	4	tests/tool_use/test_chat_completions.py
7	6	tests/tool_use/test_jamba_tool_parser.py
5	5	tests/tool_use/test_parallel_tool_calls.py
5	5	tests/tool_use/test_tool_calls.py
13	13	tests/tool_use/utils.py
3	2	tests/tracing/test_tracing.py
21	21	tests/utils.py
1	2	tests/v1/core/test_prefix_caching.py
3	3	tests/v1/core/test_scheduler.py
2	4	tests/v1/engine/conftest.py
5	5	tests/v1/engine/test_async_llm.py
1	2	tests/v1/engine/test_engine_core.py
5	5	tests/v1/engine/test_engine_core_client.py
4	4	tests/v1/engine/test_llm_engine.py
6	6	tests/v1/engine/test_output_processor.py
25	25	tests/v1/engine/utils.py
6	6	tests/v1/entrypoints/openai/test_completion.py
4	5	tests/v1/sample/test_logprobs.py
3	4	tests/v1/sample/test_rejection_sampler.py
13	13	tests/v1/sample/test_sampler.py
2	3	tests/v1/sample/utils.py
2	4	tests/v1/test_utils.py
11	11	tests/v1/worker/test_gpu_input_batch.py
2	1	tests/vllm_test_utils/vllm_test_utils/blame.py
2	1	tests/vllm_test_utils/vllm_test_utils/monitor.py
10	11	tests/worker/test_encoder_decoder_model_runner.py
5	6	tests/worker/test_model_input.py
9	11	tests/worker/test_model_runner.py
1	2	tools/profiler/print_layerwise_table.py
7	7	tools/profiler/visualize_layerwise_profile.py
28	28	vllm/_custom_ops.py
4	4	vllm/_ipex_ops.py
9	9	vllm/beam_search.py
71	70	vllm/config.py
2	1	vllm/connections.py
2	1	vllm/entrypoints/api_server.py
25	24	vllm/entrypoints/chat_utils.py
5	5	vllm/entrypoints/cli/openai.py
1	2	vllm/entrypoints/cli/serve.py
105	105	vllm/entrypoints/llm.py
2	2	vllm/entrypoints/logger.py
5	4	vllm/entrypoints/openai/api_server.py
4	3	vllm/entrypoints/openai/cli_args.py
12	11	vllm/entrypoints/openai/logits_processors.py
64	64	vllm/entrypoints/openai/protocol.py
11	10	vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py
3	2	vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
5	4	vllm/entrypoints/openai/run_batch.py
15	16	vllm/entrypoints/openai/serving_chat.py
16	16	vllm/entrypoints/openai/serving_completion.py
8	7	vllm/entrypoints/openai/serving_embedding.py
21	22	vllm/entrypoints/openai/serving_engine.py
5	5	vllm/entrypoints/openai/serving_models.py
8	7	vllm/entrypoints/openai/serving_pooling.py
25	24	vllm/entrypoints/openai/serving_score.py
2	2	vllm/entrypoints/openai/serving_tokenization.py
2	1	vllm/entrypoints/openai/serving_transcription.py
11	10	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
3	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
3	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
4	3	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
3	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
6	5	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
6	5	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
7	6	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
3	2	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
3	3	vllm/entrypoints/openai/tool_parsers/utils.py
7	7	vllm/entrypoints/score_utils.py
4	4	vllm/envs.py
3	3	vllm/forward_context.py
1	1	vllm/logger.py
8	8	vllm/logits_process.py
12	12	vllm/outputs.py
26	27	vllm/sampling_params.py
66	66	vllm/sequence.py
2	1	vllm/tracing.py
38	38	vllm/utils.py
9	9	vllm/v1/attention/backends/flash_attn.py
10	11	vllm/v1/attention/backends/mla/common.py
7	7	vllm/v1/attention/backends/mla/flashmla.py
4	4	vllm/v1/attention/backends/mla/triton_mla.py
8	8	vllm/v1/attention/backends/pallas.py
8	8	vllm/v1/attention/backends/rocm_attn.py
9	8	vllm/v1/core/block_pool.py
8	8	vllm/v1/core/encoder_cache_manager.py
10	9	vllm/v1/core/kv_cache_manager.py
16	16	vllm/v1/core/kv_cache_utils.py
23	22	vllm/v1/core/scheduler.py
21	21	vllm/v1/core/scheduler_output.py
8	8	vllm/v1/engine/__init__.py
5	4	vllm/v1/engine/async_llm.py
8	8	vllm/v1/engine/core.py
17	17	vllm/v1/engine/core_client.py
6	6	vllm/v1/engine/detokenizer.py
9	8	vllm/v1/engine/llm_engine.py
6	6	vllm/v1/engine/logprobs.py
9	9	vllm/v1/engine/mm_input_cache.py
10	10	vllm/v1/engine/output_processor.py
8	8	vllm/v1/engine/parallel_sampling.py
2	1	vllm/v1/engine/processor.py
5	5	vllm/v1/executor/abstract.py
5	5	vllm/v1/executor/multiproc_executor.py
3	4	vllm/v1/kv_cache_interface.py
9	9	vllm/v1/metrics/loggers.py
10	10	vllm/v1/metrics/stats.py
10	10	vllm/v1/outputs.py
12	12	vllm/v1/request.py
5	5	vllm/v1/sample/metadata.py
5	7	vllm/v1/sample/ops/penalties.py
5	5	vllm/v1/sample/ops/topk_topp_sampler.py
3	4	vllm/v1/sample/rejection_sampler.py
9	9	vllm/v1/stats/common.py
10	10	vllm/v1/utils.py
2	4	vllm/v1/worker/block_table.py
31	31	vllm/v1/worker/gpu_input_batch.py
17	17	vllm/v1/worker/gpu_model_runner.py
2	2	vllm/v1/worker/gpu_worker.py
8	9	vllm/v1/worker/lora_model_runner_mixin.py
12	12	vllm/v1/worker/tpu_model_runner.py
3	3	vllm/v1/worker/tpu_worker.py

[bf33700ec] Ce Gao 2025-03-03 [v0][structured output] Support reasoning output (#12955)
35	8	docs/source/features/reasoning_outputs.md
64	0	examples/online_serving/openai_chat_completion_structured_outputs_with_reasoning.py
102	14	tests/model_executor/test_guided_processors.py
2	0	vllm/config.py
25	1	vllm/engine/arg_utils.py
8	3	vllm/engine/async_llm_engine.py
6	1	vllm/engine/llm_engine.py
2	1	vllm/engine/multiprocessing/client.py
0	18	vllm/entrypoints/openai/cli_args.py
21	9	vllm/model_executor/guided_decoding/__init__.py
20	10	vllm/model_executor/guided_decoding/outlines_decoding.py
29	8	vllm/model_executor/guided_decoding/outlines_logits_processors.py
23	0	vllm/model_executor/guided_decoding/reasoner/__init__.py
28	0	vllm/model_executor/guided_decoding/reasoner/deepseek_reasoner.py
19	0	vllm/model_executor/guided_decoding/reasoner/reasoner.py
16	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[bc6ccb987] qux-bbb 2025-03-02 [Doc] Source building add clone step (#14086)
8	1	docs/source/getting_started/installation/cpu/build.inc.md

[82fbeae92] Jun Duan 2025-03-01 [Misc] Accurately capture the time of loading weights (#14063)
11	0	vllm/model_executor/model_loader/loader.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/worker/model_runner.py

[cc5e8f6db] Jee Jee Li 2025-03-02 [Model] Add LoRA support for TransformersModel (#13770)
2	1	.buildkite/test-pipeline.yaml
1	14	docs/source/models/supported_models.md
5	0	tests/lora/conftest.py
120	0	tests/lora/test_transfomers_model.py
18	7	vllm/lora/layers.py
14	11	vllm/lora/utils.py
6	37	vllm/model_executor/models/transformers.py

[d54990da4] Chen Zhang 2025-03-02 [v1] Add `__repr__` to KVCacheBlock to avoid recursive print (#14081)
13	0	vllm/v1/core/kv_cache_utils.py

[b9f1d4294] Chen Zhang 2025-03-01 [v1][Bugfix] Only cache blocks that are not in the prefix cache (#14073)
4	18	vllm/v1/core/block_pool.py
5	4	vllm/v1/core/kv_cache_manager.py

[b28246f6f] Sage Moore 2025-02-28 [ROCm][V1][Bugfix] Add get_builder_cls method to the ROCmAttentionBackend class (#14065)
6	1	vllm/v1/attention/backends/rocm_attn.py

[3b5567a20] Woosuk Kwon 2025-02-28 [V1][Minor] Do not print attn backend twice (#13985)
4	3	vllm/platforms/cuda.py

[fdcc40534] Isotr0py 2025-03-01 [Doc] Consolidate `whisper` and `florence2` examples (#14050)
50	32	examples/offline_inference/audio_language.py
158	0	examples/offline_inference/encoder_decoder_multimodal.py
0	53	examples/offline_inference/florence2_inference.py
0	61	examples/offline_inference/whisper.py
2	2	vllm/model_executor/models/whisper.py

[8994dabc2] Kuntai Du 2025-02-28 [Documentation] Add more deployment guide for Kubernetes deployment (#13841)
1	0	docs/source/deployment/integrations/index.md
154	0	docs/source/deployment/integrations/production-stack.md
11	7	docs/source/deployment/k8s.md

[02296f420] Li, Jiang 2025-03-01 [Bugfix][V1][Minor] Fix shutting_down flag checking in V1 MultiprocExecutor (#14053)
1	1	vllm/v1/executor/multiproc_executor.py

[6a92ff93e] YajieWang 2025-03-01 [Misc][Kernel]: Add GPTQAllSpark Quantization (#12931)
16	0	CMakeLists.txt
45	2	benchmarks/kernels/benchmark_marlin.py
1008	0	csrc/quantization/gptq_allspark/allspark_qgemm_w8a16.cu
163	0	csrc/quantization/gptq_allspark/allspark_repack.cu
408	0	csrc/quantization/gptq_allspark/allspark_utils.cuh
19	0	csrc/torch_bindings.cpp
100	0	tests/kernels/test_allspark_gemm.py
0	2	tests/quantization/test_compressed_tensors.py
77	0	vllm/_custom_ops.py
3	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
115	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/allspark.py
51	0	vllm/model_executor/layers/quantization/utils/allspark_utils.py

[6a84164ad] Jee Jee Li 2025-03-01 [Bugfix] Add file lock for ModelScope download (#14060)
10	5	benchmarks/backend_request_func.py
12	8	vllm/model_executor/model_loader/loader.py
4	1	vllm/model_executor/model_loader/weight_utils.py
14	8	vllm/transformers_utils/tokenizer.py

[f64ffa8c2] Brayden Zhong 2025-03-01 [Docs] Add `pipeline_parallel_size` to optimization docs (#14059)
1	0	docs/source/performance/optimization.md

[bd56c983d] Luka Govedič 2025-02-28 [torch.compile] Fix RMSNorm + quant fusion in the non-cutlass-fp8 case, rename RedundantReshapesPass to NoopEliminationPass (#10902)
9	4	tests/compile/backend.py
4	4	tests/compile/test_functionalization.py
69	58	tests/compile/test_fusion.py
135	0	vllm/compilation/noop_elimination.py
4	4	vllm/compilation/pass_manager.py
0	90	vllm/compilation/reshapes.py
16	2	vllm/compilation/vllm_inductor_pass.py
6	7	vllm/config.py
6	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[084bbac8c] Rui Qiao 2025-02-28 [core] Bump ray to 2.43 (#13994)
1	1	.github/dependabot.yml
1	1	requirements-cuda.txt
1	1	requirements-test.in
1	1	requirements-test.txt
4	6	vllm/executor/ray_distributed_executor.py

[28943d36c] Chen Zhang 2025-03-01 [v1] Move block pool operations to a separate class (#13973)
49	40	tests/v1/core/test_prefix_caching.py
285	0	vllm/v1/core/block_pool.py
26	237	vllm/v1/core/kv_cache_manager.py

[b526ca672] Andrey Talman 2025-02-28 Add RELEASE.md (#13926)
54	0	RELEASE.md

[e7bd944e0] Chen Zhang 2025-03-01 [v1] Cleanup the BlockTable in InputBatch (#13977)
14	0	tests/v1/worker/test_gpu_model_runner.py
6	7	vllm/v1/worker/block_table.py
1	2	vllm/v1/worker/gpu_input_batch.py
2	4	vllm/v1/worker/gpu_model_runner.py
2	4	vllm/v1/worker/tpu_model_runner.py

[c3b6559a1] iefgnoix 2025-02-28 [V1][TPU] Integrate the new ragged paged attention kernel with vLLM v1 on TPU (#13379)
5	6	requirements-tpu.txt
62	218	vllm/v1/attention/backends/pallas.py
1	1	vllm/v1/outputs.py
2	2	vllm/v1/worker/gpu_model_runner.py
281	674	vllm/v1/worker/tpu_model_runner.py
2	4	vllm/v1/worker/tpu_worker.py

[4be4b26cb] Harry Mellor 2025-02-28 Fix entrypoint tests for embedding models (#14052)
11	11	tests/entrypoints/openai/test_embedding.py

[2aed2c9fa] Brayden Zhong 2025-02-28 [Doc] Fix ROCm documentation (#14041)
2	2	docs/source/getting_started/installation/gpu/rocm.inc.md

[9b61dd41e] Yang Liu 2025-02-28 [Bugfix] Initialize attention bias on the same device as Query/Key/Value for QwenVL Series (#14031)
2	1	vllm/model_executor/models/qwen2_5_vl.py
2	1	vllm/model_executor/models/qwen2_vl.py

[f7bee5c81] Cyrus Leung 2025-02-28 [VLM][Bugfix] Enable specifying prompt target via index (#14038)
256	2	tests/multimodal/test_processing.py
3	3	vllm/model_executor/models/blip2.py
3	2	vllm/model_executor/models/florence2.py
3	3	vllm/model_executor/models/molmo.py
167	49	vllm/multimodal/processing.py

[e0734387f] Jee Jee Li 2025-02-28 [Bugfix] Fix MoeWNA16Method activation (#14024)
2	1	vllm/model_executor/layers/quantization/moe_wna16.py

[f58f8b5c9] Harry Mellor 2025-02-28 Update AutoAWQ docs (#14042)
2	2	docs/source/features/quantization/auto_awq.md

[b3f7aaccd] Thibault Schueller 2025-02-28 [V1][Minor] Restore V1 compatibility with LLMEngine class (#13090)
5	0	vllm/engine/llm_engine.py

[b91660ddb] Kacper Pietkun 2025-02-28 [Hardware][Intel-Gaudi] Regional compilation support (#13213)
37	6	vllm/worker/hpu_model_runner.py

[76c89fcad] Harry Mellor 2025-02-28 Use smaller embedding model when not testing model specifically (#13891)
1	1	tests/entrypoints/llm/test_encode.py
1	1	tests/entrypoints/openai/test_embedding.py
2	2	tests/entrypoints/openai/test_metrics.py
5	5	tests/entrypoints/openai/test_run_batch.py
2	2	tests/model_executor/test_model_load_with_params.py
1	1	tests/models/embedding/language/test_embedding.py
1	1	tests/models/registry.py
1	1	tests/test_config.py
1	1	vllm/test_utils.py

[b9e41734c] Mathis Felardos 2025-02-28 [Bugfix][Disaggregated] patch the inflight batching on the decode node in SimpleConnector to avoid hangs in SimpleBuffer (nccl based) (#13987)
15	2	vllm/distributed/kv_transfer/kv_connector/simple_connector.py

[1088f0624] Cyrus Leung 2025-02-28 [Doc] Move multimodal Embedding API example to Online Serving page (#14017)
9	80	docs/source/serving/multimodal_inputs.md
77	3	docs/source/serving/openai_compatible_server.md
3	1	vllm/model_executor/models/registry.py

[73e0225ee] Travis Johnson 2025-02-27 [Bugfix] Check that number of images matches number of <|image|> tokens with mllama (#13911)
3	2	tests/models/encoder_decoder/vision_language/test_mllama.py
23	1	vllm/model_executor/models/mllama.py

[6c85da3a1] Roger Wang 2025-02-27 [V1]`SupportsV0Only` protocol for model definitions (#13959)
5	0	vllm/config.py
5	2	vllm/model_executor/models/__init__.py
3	2	vllm/model_executor/models/bamba.py
2	1	vllm/model_executor/models/bart.py
2	2	vllm/model_executor/models/bert.py
2	2	vllm/model_executor/models/florence2.py
3	1	vllm/model_executor/models/gritlm.py
26	0	vllm/model_executor/models/interfaces.py
3	2	vllm/model_executor/models/jamba.py
4	2	vllm/model_executor/models/mamba.py
4	2	vllm/model_executor/models/mamba2.py
4	2	vllm/model_executor/models/minicpmv.py
3	2	vllm/model_executor/models/mllama.py
2	2	vllm/model_executor/models/paligemma.py
4	2	vllm/model_executor/models/prithvi_geospatial_mae.py
3	2	vllm/model_executor/models/qwen2_rm.py
12	2	vllm/model_executor/models/registry.py
3	2	vllm/model_executor/models/roberta.py
3	2	vllm/model_executor/models/whisper.py

[67fc42684] Jee Jee Li 2025-02-28 [Misc] Print FusedMoE detail info (#13974)
20	0	vllm/model_executor/layers/fused_moe/layer.py

[9804145ca] Benjamin Chislett 2025-02-27 [Model][Speculative Decoding] Expand DeepSeek MTP code to support k > n_predict (#13626)
5	6	vllm/config.py
9	5	vllm/model_executor/models/deepseek_mtp.py
4	7	vllm/spec_decode/draft_model_runner.py
17	0	vllm/spec_decode/multi_step_worker.py
3	3	vllm/spec_decode/spec_decode_worker.py
11	1	vllm/worker/model_runner.py

[2e94b9cfb] Lucas Wilkinson 2025-02-27 [Attention] Flash MLA for V1 (#13867)
22	13	vllm/platforms/cuda.py
2	3	vllm/platforms/interface.py
7	4	vllm/v1/attention/backends/mla/common.py
139	0	vllm/v1/attention/backends/mla/flashmla.py
0	0	vllm/v1/attention/backends/{ => mla}/triton_mla.py

[8294773e4] qli88 2025-02-27 [core] Perf improvement for DSv3 on AMD GPUs (#13718)
72	20	vllm/attention/backends/mla/common.py
10	5	vllm/attention/ops/triton_decode_attention.py
128	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json

[cd813c6d4] Woosuk Kwon 2025-02-27 [V1][Minor] Minor cleanup for GPU Model Runner (#13983)
7	6	vllm/v1/worker/gpu_model_runner.py

[38acae6e9] Sage Moore 2025-02-27 [ROCm] Fix the Kernels, Core, and Prefix Caching AMD CI groups (#13970)
3	1	.buildkite/run-amd-test.sh
10	0	tests/core/block/e2e/test_correctness_sliding_window.py
10	0	tests/prefix_caching/test_prefix_caching.py

[a2dd48c38] Cyrus Leung 2025-02-28 [VLM] Deprecate legacy input mapper for OOT multimodal models (#13979)
23	22	vllm/config.py
9	5	vllm/inputs/preprocess.py

[126f6beeb] dependabot[bot] 2025-02-27 Bump azure/setup-helm from 4.2.0 to 4.3.0 (#13742)
1	1	.github/workflows/lint-and-deploy.yaml

[58d1b2aa7] Yang Chen 2025-02-27 [Attention] MLA support for V1 (#13789)
24	11	vllm/attention/layer.py
11	2	vllm/model_executor/models/deepseek_v2.py
7	2	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py
67	2	vllm/v1/attention/backends/flash_attn.py
0	0	vllm/v1/attention/backends/mla/__init__.py
1022	0	vllm/v1/attention/backends/mla/common.py
110	0	vllm/v1/attention/backends/triton_mla.py
63	1	vllm/v1/worker/gpu_input_batch.py
35	41	vllm/v1/worker/gpu_model_runner.py

[f1579b229] Cyrus Leung 2025-02-28 [VLM] Generalized prompt updates for multi-modal processor (#13964)
13	13	docs/source/contributing/model/multimodal.md
14	9	docs/source/design/mm_processing.md
120	90	tests/multimodal/test_processing.py
6	6	vllm/model_executor/models/aria.py
9	15	vllm/model_executor/models/blip2.py
7	7	vllm/model_executor/models/chameleon.py
5	5	vllm/model_executor/models/deepseek_vl2.py
10	14	vllm/model_executor/models/florence2.py
6	6	vllm/model_executor/models/fuyu.py
6	5	vllm/model_executor/models/glm4v.py
6	5	vllm/model_executor/models/h2ovl.py
5	5	vllm/model_executor/models/idefics3.py
7	6	vllm/model_executor/models/internvl.py
12	11	vllm/model_executor/models/llava.py
6	5	vllm/model_executor/models/llava_next_video.py
11	9	vllm/model_executor/models/llava_onevision.py
5	4	vllm/model_executor/models/minicpmo.py
6	5	vllm/model_executor/models/minicpmv.py
5	5	vllm/model_executor/models/mllama.py
13	22	vllm/model_executor/models/molmo.py
7	6	vllm/model_executor/models/nvlm_d.py
10	11	vllm/model_executor/models/phi3v.py
9	15	vllm/model_executor/models/prithvi_geospatial_mae.py
6	6	vllm/model_executor/models/qwen2_audio.py
9	7	vllm/model_executor/models/qwen2_vl.py
8	7	vllm/model_executor/models/qwen_vl.py
6	5	vllm/model_executor/models/ultravox.py
5	5	vllm/model_executor/models/whisper.py
303	183	vllm/multimodal/processing.py

[786487587] Isotr0py 2025-02-28 [Bugfix] Fix qwen2.5-vl overflow issue (#13968)
3	8	vllm/model_executor/models/minicpmo.py
6	1	vllm/model_executor/models/qwen2_5_vl.py
10	0	vllm/model_executor/models/utils.py
3	6	vllm/model_executor/models/whisper.py

[1dd422b64] Noam Gat 2025-02-27 Update LMFE version to v0.10.11 to support new versions of transforme… (#13930)
1	1	requirements-common.txt

[06c8f8d88] Rui Qiao 2025-02-27 [bugfix] Fix profiling for RayDistributedExecutor (#13945)
13	8	vllm/executor/ray_distributed_executor.py

[5677c9bb3] Harry Mellor 2025-02-27 Deduplicate `.pre-commit-config.yaml`'s `exclude` (#13967)
1	16	.pre-commit-config.yaml

[512d77d58] 王博伟 2025-02-28 Update quickstart.md (#13958)
6	0	docs/source/getting_started/quickstart.md

[7f0be2aa2] Szymon Ożóg 2025-02-27 [Model] Deepseek GGUF support  (#13167)
7	0	docs/source/features/quantization/gguf.md
6	3	vllm/config.py
8	0	vllm/engine/arg_utils.py
21	1	vllm/model_executor/layers/fused_moe/layer.py
14	1	vllm/model_executor/layers/linear.py
125	2	vllm/model_executor/layers/quantization/gguf.py
17	2	vllm/model_executor/model_loader/loader.py
0	1	vllm/model_executor/model_loader/weight_utils.py

[edf309ebb] Isotr0py 2025-02-27 [VLM] Support multimodal inputs for Florence-2 models (#13320)
7	0	docs/source/models/supported_models.md
23	16	examples/offline_inference/florence2_inference.py
17	0	examples/offline_inference/vision_language.py
3	3	tests/conftest.py
2	2	tests/models/decoder_only/audio_language/test_ultravox.py
88	51	tests/models/encoder_decoder/vision_language/test_florence2.py
3	2	tests/models/multimodal/processing/test_common.py
5	5	tests/models/registry.py
19	8	vllm/model_executor/models/bart.py
893	20	vllm/model_executor/models/florence2.py
1	1	vllm/model_executor/models/registry.py
13	7	vllm/multimodal/processing.py
4	2	vllm/multimodal/profiling.py

[788f284b5] Michael Goin 2025-02-27 Fix test_block_fp8.py test for MoE (#13915)
2	2	tests/kernels/test_block_fp8.py

[4b1d141f4] Yang Zheng 2025-02-27 [PP] Correct cache size check (#13873)
7	6	vllm/worker/hpu_worker.py
7	6	vllm/worker/worker.py

[10c3b8c1c] Chauncey 2025-02-27 [Misc] fixed 'required' is an invalid argument for positionals (#13948)
1	1	examples/online_serving/openai_chat_embedding_client_for_multimodal.py

[a7f37314b] Brayden Zhong 2025-02-27 [CI/Build] Add examples/ directory to be labelled by `mergify` (#13944)
1	0	.github/mergify.yml

[cd711c48b] Mark McLoughlin 2025-02-27 [V1][Metrics] Handle preemptions (#13169)
1	0	tests/entrypoints/openai/test_metrics.py
9	1	vllm/v1/core/scheduler.py
1	0	vllm/v1/engine/__init__.py
15	9	vllm/v1/metrics/loggers.py
22	9	vllm/v1/metrics/stats.py

[378b3ef6f] Sage Moore 2025-02-26 [ROCm][V1] Update reshape_and_cache to properly work with CUDA graph padding (#13922)
1	1	csrc/cache_kernels.cu

[c9944acbf] Rui Qiao 2025-02-26 [misc] Rename Ray ADAG to Compiled Graph (#13928)
1	1	tests/basic_correctness/test_basic_correctness.py
1	1	tests/basic_correctness/test_chunked_prefill.py
6	5	tests/distributed/test_pipeline_parallel.py
5	4	vllm/envs.py
5	5	vllm/executor/ray_distributed_executor.py
4	4	vllm/executor/ray_utils.py

[ca377cf1b] Michael Goin 2025-02-26 Use CUDA 12.4 as default for release and nightly wheels (#12098)
12	1	.buildkite/release-pipeline.yaml
8	2	.buildkite/upload-wheels.sh
2	2	docs/source/getting_started/installation/gpu/cuda.inc.md
3	4	setup.py

[a31614e38] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2025-02-27 [ROCm][Quantization][Kernel] Use FP8 FNUZ when OCP flag is 0 or undefined (#13851)
4	4	csrc/quantization/fp8/amd/quant_utils.cuh

[f95903909] Lucas Wilkinson 2025-02-26 [Kernel] FlashMLA integration (#13747)
4	73	CMakeLists.txt
66	0	cmake/external_projects/flashmla.cmake
67	0	cmake/external_projects/vllm_flash_attn.cmake
6	0	setup.py
132	0	tests/kernels/test_flashmla.py
64	0	vllm/_custom_ops.py
239	0	vllm/attention/backends/flashmla.py
15	13	vllm/attention/backends/mla/common.py
115	0	vllm/attention/ops/flashmla.py
24	0	vllm/platforms/cuda.py
1	0	vllm/platforms/interface.py

[b382a7f28] Woosuk Kwon 2025-02-26 [BugFix] Make FP8 Linear compatible with torch.compile (#13918)
1	4	vllm/model_executor/layers/quantization/fp8.py
20	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[4cb6fa0a9] Wallas Henrique 2025-02-26 [Bugfix] Backend option to disable xgrammar any_whitespace (#12744)
54	0	tests/entrypoints/llm/test_guided_generate.py
1	0	vllm/engine/arg_utils.py
33	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[d08b285ad] Chauncey 2025-02-27 [Misc] fixed qwen_vl_utils parameter error (#13906)
1	1	examples/offline_inference/vision_language_multi_image.py

[b27122acc] Chenyaaang 2025-02-26 [TPU] use torch2.6 with whl package (#13860)
3	1	requirements-tpu.txt

[934bb99c7] Cyrus Leung 2025-02-26 [Bugfix] Update expected token counts for Ultravox tests (#13895)
3	3	tests/entrypoints/openai/test_audio.py

[3f808cc04] Joe Runde 2025-02-26 [Bugfix] Do not crash V0 engine on input errors (#13101)
78	0	tests/mq_llm_engine/test_error_handling.py
59	3	vllm/engine/llm_engine.py
9	0	vllm/engine/multiprocessing/engine.py
8	3	vllm/worker/model_runner.py
18	0	vllm/worker/model_runner_base.py

[ec8a5e538] Brayden Zhong 2025-02-26 [Misc]: Add support for goodput on guided benchmarking + TPOT calculation refactor (#13736)
82	5	benchmarks/benchmark_serving_guided.py

[215bf150a] Florian Greinacher 2025-02-26 [Bugfix] Handle None parameters in Mistral function calls. (#13786)
34	1	tests/tokenization/test_mistral_tokenizer.py
2	1	vllm/transformers_utils/tokenizers/mistral.py

[0ecdd9803] Harry Mellor 2025-02-26 Add comments on accessing `kv_cache` and `attn_metadata` (#13887)
13	0	vllm/attention/layer.py

[7b700ec8c] Cyrus Leung 2025-02-26 [Bugfix] Add test example for Ultravox v0.5 (#13890)
1	0	tests/models/registry.py

[7ca1da020] Roger Wang 2025-02-25 [Misc] Fix input processing for Ultravox (#13871)
3	3	tests/models/multimodal/processing/test_common.py
1	1	tests/models/registry.py
2	11	vllm/model_executor/models/ultravox.py

[5157338ed] Jee Jee Li 2025-02-26 [Misc] Improve LoRA spelling (#13831)
1	1	benchmarks/kernels/benchmark_lora.py
1	1	docs/source/features/lora.md
1	1	tests/core/test_scheduler.py
1	1	tests/entrypoints/openai/test_cli_args.py
10	10	tests/entrypoints/openai/test_serving_models.py
9	9	tests/lora/test_layers.py
2	2	tests/lora/test_long_context.py
1	1	vllm/engine/llm_engine.py
5	5	vllm/entrypoints/openai/api_server.py
2	2	vllm/entrypoints/openai/protocol.py
7	7	vllm/entrypoints/openai/serving_models.py
6	6	vllm/lora/fully_sharded_layers.py
4	4	vllm/lora/layers.py
5	5	vllm/lora/models.py
1	1	vllm/lora/peft_helper.py
1	1	vllm/lora/punica_wrapper/punica_base.py
9	9	vllm/lora/utils.py
2	2	vllm/spec_decode/proposer_worker_base.py
2	2	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/transformers_utils/configs/arctic.py
2	2	vllm/worker/neuron_worker.py
2	2	vllm/worker/openvino_worker.py
2	2	vllm/worker/tpu_worker.py
1	1	vllm/worker/worker_base.py
2	2	vllm/worker/xpu_worker.py

[e206b5433] Seth Kimmel 2025-02-25 [v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine (#13837)
23	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[1d35662e6] Sage Moore 2025-02-25 [ROCm] Disable chunked prefill/prefix caching when running MLA on non-cuda platforms (#13844)
30	12	vllm/attention/backends/mla/common.py
14	0	vllm/config.py

[e656f638d] Albert 2025-02-26 [Doc] fix the incorrect module path of tensorize_vllm_model (#13863)
5	5	examples/other/tensorize_vllm_model.py

[145944cb9] Harry Mellor 2025-02-26 Improve pipeline partitioning (#13839)
24	0	tests/distributed/test_pipeline_partition.py
22	8	vllm/distributed/utils.py

[094b7d949] Henry Tsang 2025-02-25 [Kernel][Build/CI] Bump CUTLASS to 3.8 and add initializers for cutlass epilogues (#13797)
4	4	CMakeLists.txt
14	12	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp
15	13	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp

[e1fe7591f] Chenguang Li 2025-02-26 [Misc]Code Cleanup (#13859)
0	1	vllm/executor/ray_distributed_executor.py

[5629f26df] Lily Liu 2025-02-25 [V1][Spec Decode] Change Spec Decode Rejection Sampling API (#13729)
8	9	tests/v1/sample/test_rejection_sampler.py
0	1	tests/v1/sample/test_sampler.py
0	1	tests/v1/worker/test_gpu_input_batch.py
0	3	vllm/v1/sample/metadata.py
68	66	vllm/v1/sample/rejection_sampler.py
8	11	vllm/v1/sample/sampler.py
0	11	vllm/v1/worker/gpu_input_batch.py
20	9	vllm/v1/worker/gpu_model_runner.py

[9ba28043b] Rui Qiao 2025-02-25 [misc] Show driver IP info when Ray fails to allocate driver worker (#13858)
4	3	vllm/executor/ray_distributed_executor.py

[24679788e] Harry Mellor 2025-02-26 DeepSeek V2/V3/R1 only place `lm_head` on last pp rank (#13833)
6	3	vllm/model_executor/models/deepseek_v2.py

[07c435305] Michael Goin 2025-02-25 [Model] Support Grok1 (#13795)
5	0	docs/source/models/supported_models.md
2	0	tests/models/registry.py
31	12	vllm/model_executor/layers/fused_moe/fused_moe.py
17	5	vllm/model_executor/layers/fused_moe/layer.py
2	0	vllm/model_executor/layers/quantization/awq_marlin.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	0	vllm/model_executor/layers/quantization/experts_int8.py
2	0	vllm/model_executor/layers/quantization/fp8.py
3	0	vllm/model_executor/layers/quantization/gptq_marlin.py
565	0	vllm/model_executor/models/grok1.py
1	0	vllm/model_executor/models/registry.py

[34e3494e7] Harry Mellor 2025-02-25 Fix failing `MyGemma2Embedding` test (#13820)
1	6	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py

[f75aa7273] Liangfu Chen 2025-02-25 [Neuron] Add custom_ops for neuron backend (#13246)
42	0	tests/neuron/test_activation.py
56	0	tests/neuron/test_layernorm.py
95	0	tests/neuron/test_logits_processor.py
4	3	tests/neuron/test_prefix_prefill.py
58	0	tests/neuron/test_rotary_embedding.py
7	0	vllm/model_executor/custom_op.py
7	0	vllm/model_executor/layers/activation.py
1	0	vllm/model_executor/layers/logits_processor.py
76	0	vllm/model_executor/layers/rotary_embedding.py

[340e39e38] Chen1022 2025-02-26 Fix string parsing error (#13825)
1	1	vllm/transformers_utils/config.py

[f4133ce4e] Cyrus Leung 2025-02-26 [Bugfix] Revert inspection code in #13743 (#13832)
0	6	vllm/model_executor/models/registry.py

[6522d55b6] Wen Sun 2025-02-25 Fix `/v1/audio/transcriptions ` Bad Request Error (#13811)
1	2	requirements-common.txt
1	1	vllm/entrypoints/openai/protocol.py

[6ff518626] Isotr0py 2025-02-25 [Bugfix] Fix deepseek-vl2 inference with more than 2 images (#13818)
42	8	vllm/model_executor/models/deepseek_vl2.py
4	2	vllm/model_executor/models/h2ovl.py

[fa8207416] Nichols A. Romero 2025-02-25 [Bugfix] Flush TunableOp results before worker processes are destroyed. (#13623)
9	0	vllm/executor/multiproc_worker_utils.py

[75e9d4979] Junlin Zhou 2025-02-25 [Bugfix] Initialize attention bias on the same device as Query/Key/Value (#13468)
6	4	vllm/attention/backends/xformers.py

[32c3b6bfd] Chen1022 2025-02-25 [Misc]Clarify Error Handling for Non-existent Model Paths and HF Repo IDs (#13724)
22	8	vllm/transformers_utils/config.py

[37b6cb498] Jee Jee Li 2025-02-25 [CI/Build]  Fix V1 LoRA failure (#13767)
2	0	tests/lora/test_gemma.py

[aabeb2688] Gregory Shtrasberg 2025-02-25 [ROCm][Quantization][Kernel] Using HIP FP8 header (#12593)
19	0	CMakeLists.txt
0	137	csrc/quantization/fp8/amd/hip_float8.h
0	315	csrc/quantization/fp8/amd/hip_float8_impl.h
230	168	csrc/quantization/fp8/amd/quant_utils.cuh
5	3	csrc/quantization/fp8/common.cuh
13	11	tests/kernels/test_cache.py

[2f42a4888] Jiayi Yao 2025-02-25 [Feature] Support KV cache offloading and disagg prefill with LMCache connector. (#12953)
65	0	examples/offline_inference/cpu_offload_lmcache.py
130	0	examples/offline_inference/disaggregated_prefill_lmcache.py
5	0	vllm/distributed/kv_transfer/kv_connector/factory.py
108	0	vllm/distributed/kv_transfer/kv_connector/lmcache_connector.py
2	2	vllm/distributed/parallel_state.py

[3173c3b34] Rui Qiao 2025-02-25 [misc] Clean up ray compiled graph type hints (#13731)
12	4	vllm/executor/ray_distributed_executor.py
5	2	vllm/executor/ray_utils.py

[2d87d7d1a] Shanshan Shen 2025-02-25 [Bugfix] Modify modelscope api usage in transformer_utils (#13807)
1	2	vllm/transformers_utils/utils.py

[aab392774] Russell Bryant 2025-02-25 [Core] xgrammar: Expand list of unsupported jsonschema keywords (#13783)
12	0	vllm/model_executor/guided_decoding/utils.py

[6724e7916] Cyrus Leung 2025-02-25 [Misc] Check that the model can be inspected upon registration (#13743)
15	1	vllm/model_executor/models/registry.py

[03f48b3db] Varun Sundar Rabindranath 2025-02-25 [Core] LoRA V1 - Add add/pin/list/remove_lora functions   (#13705)
9	4	tests/lora/test_add_lora.py
137	0	tests/lora/test_lora_functions.py
15	3	vllm/v1/engine/async_llm.py
12	3	vllm/v1/engine/core.py
54	9	vllm/v1/engine/core_client.py
17	1	vllm/v1/engine/llm_engine.py
10	1	vllm/v1/worker/gpu_worker.py
16	1	vllm/v1/worker/lora_model_runner_mixin.py

[4d251ad00] Michael Goin 2025-02-25 Fix CompressedTensorsWNA16MoE with grouped scales (#13769)
2	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[18e505930] Michael Goin 2025-02-25 [Bugfix] Support MLA for CompressedTensorsWNA16 (#13725)
7	7	vllm/attention/backends/mla/common.py

[4a8cfc755] Lucas Wilkinson 2025-02-24 [Bugfix] Fix deepseek-v2 error: "missing 1 required positional argument: 'residual'" (#13802)
1	1	vllm/model_executor/models/deepseek_v2.py

[bc32bc73a] Mark McLoughlin 2025-02-25 [V1][Metrics] Implement vllm:lora_requests_info metric (#13504)
19	4	vllm/v1/engine/output_processor.py
30	1	vllm/v1/metrics/loggers.py
72	5	vllm/v1/metrics/stats.py

[ab1091d5f] wangxiyuan 2025-02-25 [Misc][Attention][Quantization] init property earlier (#13733)
5	4	vllm/attention/layer.py

[1e15aaef5] Tyler Michael Smith 2025-02-24 [Bugfix][Quantization] Fix FP8 + EP (#13784)
15	15	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/awq_marlin.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
3	3	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
1	1	vllm/model_executor/layers/quantization/quark/quark_moe.py

[51010a180] cjackal 2025-02-25 [Misc] set single whitespace between log sentences (#13771)
1	1	vllm/attention/backends/flashinfer.py
1	1	vllm/attention/backends/mla/common.py
2	2	vllm/attention/backends/rocm_flash_attn.py
6	6	vllm/config.py
2	2	vllm/distributed/device_communicators/pynccl_wrapper.py
1	1	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
1	1	vllm/entrypoints/chat_utils.py
1	1	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/executor/ray_distributed_executor.py
1	1	vllm/executor/ray_utils.py
1	1	vllm/lora/models.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/gptq.py
1	1	vllm/model_executor/layers/quantization/modelopt.py
2	2	vllm/model_executor/layers/quantization/neuron_quant.py
1	1	vllm/model_executor/layers/quantization/quark/quark_moe.py
1	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py
1	1	vllm/model_executor/model_loader/loader.py
1	1	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/fuyu.py
4	4	vllm/model_executor/models/gritlm.py
1	1	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/prithvi_geospatial_mae.py
1	1	vllm/multimodal/profiling.py
1	1	vllm/platforms/cuda.py
1	1	vllm/platforms/openvino.py
1	1	vllm/platforms/xpu.py
1	1	vllm/prompt_adapter/models.py
1	1	vllm/spec_decode/draft_model_runner.py
4	4	vllm/transformers_utils/configs/jais.py
3	3	vllm/utils.py
1	1	vllm/v1/worker/gpu_worker.py
1	1	vllm/worker/openvino_worker.py
2	2	vllm/worker/worker.py

[7196a3b1d] Eli Boyarski 2025-02-25 [Doc] arg_utils.py: fixed a typo (#13785)
1	1	vllm/engine/arg_utils.py

[cdc1fa12e] Harry Mellor 2025-02-25 Remove unused kwargs from model definitions (#13555)
0	2	docs/source/contributing/model/basic.md
0	2	docs/source/contributing/model/multimodal.md
3	11	tests/kernels/test_encoder_decoder_attn.py
7	12	vllm/attention/layer.py
3	2	vllm/model_executor/layers/mamba/mamba_mixer.py
2	2	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	5	vllm/model_executor/models/adapters.py
5	19	vllm/model_executor/models/arctic.py
0	5	vllm/model_executor/models/aria.py
5	19	vllm/model_executor/models/baichuan.py
7	22	vllm/model_executor/models/bamba.py
20	73	vllm/model_executor/models/bart.py
12	32	vllm/model_executor/models/bert.py
1	6	vllm/model_executor/models/blip2.py
7	24	vllm/model_executor/models/bloom.py
4	23	vllm/model_executor/models/chameleon.py
8	34	vllm/model_executor/models/chatglm.py
5	19	vllm/model_executor/models/commandr.py
7	28	vllm/model_executor/models/dbrx.py
6	20	vllm/model_executor/models/deepseek.py
4	15	vllm/model_executor/models/deepseek_mtp.py
7	24	vllm/model_executor/models/deepseek_v2.py
0	5	vllm/model_executor/models/deepseek_vl2.py
1	6	vllm/model_executor/models/eagle.py
6	24	vllm/model_executor/models/exaone.py
7	24	vllm/model_executor/models/falcon.py
6	28	vllm/model_executor/models/florence2.py
0	5	vllm/model_executor/models/fuyu.py
5	19	vllm/model_executor/models/gemma.py
5	19	vllm/model_executor/models/gemma2.py
3	7	vllm/model_executor/models/glm4v.py
8	24	vllm/model_executor/models/gpt2.py
8	24	vllm/model_executor/models/gpt_bigcode.py
7	24	vllm/model_executor/models/gpt_j.py
7	24	vllm/model_executor/models/gpt_neox.py
6	23	vllm/model_executor/models/granite.py
6	20	vllm/model_executor/models/granitemoe.py
3	6	vllm/model_executor/models/gritlm.py
0	9	vllm/model_executor/models/idefics3.py
3	6	vllm/model_executor/models/interfaces_base.py
7	28	vllm/model_executor/models/internlm2.py
2	12	vllm/model_executor/models/internlm2_ve.py
0	5	vllm/model_executor/models/internvl.py
8	24	vllm/model_executor/models/jais.py
5	24	vllm/model_executor/models/jamba.py
7	21	vllm/model_executor/models/llama.py
0	5	vllm/model_executor/models/llava.py
0	5	vllm/model_executor/models/llava_next.py
0	5	vllm/model_executor/models/llava_next_video.py
0	5	vllm/model_executor/models/llava_onevision.py
4	12	vllm/model_executor/models/mamba.py
7	11	vllm/model_executor/models/mamba2.py
5	19	vllm/model_executor/models/minicpm.py
2	4	vllm/model_executor/models/minicpm3.py
0	5	vllm/model_executor/models/minicpmo.py
0	5	vllm/model_executor/models/minicpmv.py
6	20	vllm/model_executor/models/mixtral.py
6	20	vllm/model_executor/models/mixtral_quant.py
15	35	vllm/model_executor/models/mllama.py
3	22	vllm/model_executor/models/molmo.py
7	24	vllm/model_executor/models/mpt.py
5	23	vllm/model_executor/models/nemotron.py
6	22	vllm/model_executor/models/olmo.py
6	22	vllm/model_executor/models/olmo2.py
5	19	vllm/model_executor/models/olmoe.py
7	25	vllm/model_executor/models/opt.py
6	23	vllm/model_executor/models/orion.py
1	6	vllm/model_executor/models/paligemma.py
5	22	vllm/model_executor/models/persimmon.py
6	23	vllm/model_executor/models/phi.py
5	23	vllm/model_executor/models/phi3_small.py
0	5	vllm/model_executor/models/phi3v.py
5	19	vllm/model_executor/models/phimoe.py
0	5	vllm/model_executor/models/pixtral.py
1	4	vllm/model_executor/models/prithvi_geospatial_mae.py
6	20	vllm/model_executor/models/qwen.py
6	23	vllm/model_executor/models/qwen2.py
0	5	vllm/model_executor/models/qwen2_5_vl.py
2	7	vllm/model_executor/models/qwen2_audio.py
6	20	vllm/model_executor/models/qwen2_moe.py
2	6	vllm/model_executor/models/qwen2_rm.py
2	7	vllm/model_executor/models/qwen2_vl.py
2	6	vllm/model_executor/models/qwen_vl.py
1	6	vllm/model_executor/models/roberta.py
4	17	vllm/model_executor/models/solar.py
6	23	vllm/model_executor/models/stablelm.py
6	20	vllm/model_executor/models/starcoder2.py
2	11	vllm/model_executor/models/transformers.py
5	12	vllm/model_executor/models/ultravox.py
13	76	vllm/model_executor/models/whisper.py
0	2	vllm/spec_decode/draft_model_runner.py
1	21	vllm/v1/worker/gpu_model_runner.py
5	14	vllm/v1/worker/tpu_model_runner.py
0	4	vllm/worker/cpu_enc_dec_model_runner.py
0	2	vllm/worker/cpu_model_runner.py
0	14	vllm/worker/cpu_pooling_model_runner.py
1	13	vllm/worker/enc_dec_model_runner.py
15	21	vllm/worker/hpu_model_runner.py
2	11	vllm/worker/model_runner.py
2	2	vllm/worker/multi_step_model_runner.py
0	4	vllm/worker/openvino_model_runner.py
0	12	vllm/worker/pooling_model_runner.py
9	15	vllm/worker/tpu_model_runner.py
1	12	vllm/worker/xpu_model_runner.py

[f61528d46] Robert Shaw 2025-02-24 [Misc][Chore] Clean Up `AsyncOutputProcessing` Logs (#13780)
0	11	vllm/config.py

[1f0ae3ed0] Robert Shaw 2025-02-24 [Misc] Clean Up `EngineArgs.create_engine_config` (#13734)
4	0	vllm/config.py
25	40	vllm/engine/arg_utils.py

[db986c19e] Michael Goin 2025-02-24 Fix precommit fail in fused_moe intermediate_cache2 chunking (#13772)
2	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[227578480] Roger Wang 2025-02-24 Revert "[V1][Core] Fix memory issue with logits & sampling" (#13775)
29	39	vllm/v1/worker/gpu_model_runner.py
0	10	vllm/v1/worker/gpu_worker.py

[befc402d3] afeldman-nm 2025-02-24 [V1] V1 engine implements parallel sampling (AsyncLLM and LLMEngine) (#10980)
98	5	tests/v1/engine/test_llm_engine.py
102	0	tests/v1/entrypoints/openai/test_completion.py
26	1	vllm/v1/engine/async_llm.py
40	3	vllm/v1/engine/llm_engine.py
375	0	vllm/v1/engine/parallel_sampling.py

[444b0f0f6] Nicolò Lucchesi 2025-02-24 [Misc][Docs] Raise error when flashinfer is not installed and `VLLM_ATTENTION_BACKEND` is set (#12513)
10	0	docs/source/getting_started/quickstart.md
9	0	vllm/config.py

[ccc00515f] Zhonghua Deng 2025-02-24 [BugFix]  Illegal memory access for MoE On H20 (#13693)
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[781096e38] Jongseok Park 2025-02-24 Expert Parallelism (EP) Support for DeepSeek V2 (#12583)
2	1	benchmarks/kernels/benchmark_moe.py
227	0	tests/distributed/test_expert_parallel.py
2	7	tests/kernels/test_awq_marlin.py
59	6	tests/kernels/test_moe.py
3	1	tests/kernels/utils.py
3	3	tests/utils.py
20	0	vllm/config.py
7	0	vllm/envs.py
96	30	vllm/model_executor/layers/fused_moe/fused_moe.py
65	5	vllm/model_executor/layers/fused_moe/layer.py
8	2	vllm/model_executor/layers/fused_moe/moe_torch_iterative.py
7	0	vllm/model_executor/layers/quantization/awq_marlin.py
10	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
4	0	vllm/model_executor/layers/quantization/experts_int8.py
4	0	vllm/model_executor/layers/quantization/fp8.py
2	0	vllm/model_executor/layers/quantization/gptq_marlin.py
4	0	vllm/model_executor/layers/quantization/moe_wna16.py
4	0	vllm/model_executor/layers/quantization/quark/quark_moe.py
0	4	vllm/model_executor/models/deepseek_v2.py

[7940d8a6a] Roger Meier 2025-02-24 [CI/Build] add python-json-logger to requirements-common (#12842)
2	7	examples/other/logging_configuration.md
1	0	requirements-common.txt

[c0e3ecd6d] Roger Meier 2025-02-24 [Bugfix] fix(logging): add missing opening square bracket (#13011)
1	1	vllm/logger.py

[23eca9cf6] Mengqing Cao 2025-02-24 [model][refactor] remove cuda hard code in models and layers (#13658)
2	1	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
9	4	vllm/model_executor/layers/rotary_embedding.py
3	1	vllm/model_executor/layers/spec_decode_base_sampler.py
2	1	vllm/model_executor/model_loader/loader.py
3	2	vllm/model_executor/models/arctic.py
3	2	vllm/model_executor/models/minicpm.py
7	3	vllm/model_executor/models/minicpmv.py

[437b76ff5] Roger Wang 2025-02-24 [V1][Core] Fix memory issue with logits & sampling (#13721)
39	29	vllm/v1/worker/gpu_model_runner.py
10	0	vllm/v1/worker/gpu_worker.py

[f90a37559] Kevin H. Luu 2025-02-23 [ci] Add logic to change model to S3 path only when S3 CI env var is on (#13727)
7	3	tests/metrics/test_metrics.py

[e7ef74e26] Huy Do 2025-02-23 Fix some issues with benchmark data output (#13641)
21	6	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
3	0	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
1	1	.buildkite/nightly-benchmarks/tests/throughput-tests.json
2	3	benchmarks/benchmark_latency.py
2	3	benchmarks/benchmark_serving.py
2	3	benchmarks/benchmark_throughput.py
30	0	benchmarks/benchmark_utils.py

[cbae7af55] Nick Hill 2025-02-23 [V1][BugFix] Fix engine core client shutdown hangs (#13298)
1	3	tests/v1/engine/test_engine_core_client.py
38	13	vllm/v1/engine/core_client.py

[eb24dc4a4] youkaichao 2025-02-23 [v1] torchrun compatibility (#13642)
1	0	.buildkite/test-pipeline.yaml
6	0	tests/distributed/test_torchrun_example.py
4	2	tests/v1/engine/test_engine_core.py
5	0	vllm/config.py
1	1	vllm/executor/ray_distributed_executor.py
3	1	vllm/executor/ray_utils.py
4	3	vllm/executor/uniproc_executor.py
1	1	vllm/v1/engine/core.py
7	2	vllm/v1/engine/llm_engine.py
17	3	vllm/v1/executor/abstract.py
3	2	vllm/v1/executor/multiproc_executor.py
3	4	vllm/v1/worker/gpu_worker.py
3	3	vllm/v1/worker/tpu_worker.py
9	2	vllm/worker/worker_base.py

[9bebc9512] Roger Wang 2025-02-23 [Misc] Deprecate `--dataset` from `benchmark_serving.py` (#13708)
4	19	benchmarks/benchmark_serving.py

[5a2ba16f5] Nick Hill 2025-02-23 [Core][Distributed] Use IPC (domain socket) ZMQ socket for local comms (#13688)
21	21	vllm/distributed/device_communicators/shm_broadcast.py

[ba5106e51] Isotr0py 2025-02-23 [LMM] Implement merged multimodal processor for whisper (#13278)
6	5	tests/models/multimodal/processing/test_common.py
132	74	vllm/model_executor/models/whisper.py
4	1	vllm/multimodal/processing.py
8	3	vllm/multimodal/profiling.py

[d5ca2110f] Kyle Sayers 2025-02-22 [Quant] BaiChuan SupportsQuant (#13710)
3	2	vllm/model_executor/models/baichuan.py

[2c5e637b5] Kevin H. Luu 2025-02-22 [ci] Use env var to control whether to use S3 bucket in CI (#13634)
2	2	.buildkite/test-pipeline.yaml
5	6	tests/basic_correctness/test_basic_correctness.py
2	7	tests/basic_correctness/test_cumem.py
1	72	tests/conftest.py
1	6	tests/engine/test_computed_prefix_blocks.py
2	6	tests/engine/test_detokenization.py
4	17	tests/engine/test_executor.py
5	8	tests/engine/test_skip_tokenizer_init.py
3	10	tests/entrypoints/llm/test_chat.py
1	1	tests/entrypoints/llm/test_collective_rpc.py
1	3	tests/entrypoints/llm/test_encode.py
1	3	tests/entrypoints/llm/test_generate.py
1	3	tests/entrypoints/llm/test_generate_multiple_loras.py
2	5	tests/entrypoints/llm/test_guided_generate.py
2	5	tests/entrypoints/llm/test_lazy_outlines.py
2	7	tests/entrypoints/llm/test_prompt_validation.py
27	28	tests/metrics/test_metrics.py
1	5	tests/models/test_initialization.py
2	2	tests/mq_llm_engine/test_abort.py
2	4	tests/mq_llm_engine/test_error_handling.py
2	4	tests/mq_llm_engine/test_load.py
2	4	tests/multimodal/test_processing.py
1	1	tests/prefix_caching/test_prefix_caching.py
4	10	tests/test_config.py
3	10	tests/test_regression.py
1	1	tests/worker/test_swap.py
9	0	vllm/engine/arg_utils.py
4	0	vllm/envs.py
0	1	vllm/model_executor/model_loader/loader.py
129	0	vllm/test_utils.py

[322d2a27d] Andy Lo 2025-02-23 [BugFix] Minor: logger import in attention backend (#13706)
2	2	vllm/attention/backends/utils.py

[82e0d601f] Roger Wang 2025-02-22 [CI/Build] Fix pre-commit errors from #13571 (#13709)
4	3	csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu
3	2	csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu

[78ac0f591] Daniele 2025-02-22 [CI/Build] fix uv caching in Dockerfile (#13611)
16	16	Dockerfile

[b56155e7f] Yan Ma 2025-02-23 [XPU]fix setuptools version for xpu (#13548)
1	0	requirements-xpu.txt

[382f66fb0] Helena Kloosterman 2025-02-22 [Bugfix] Fix boolean conversion for OpenVINO env variable (#13615)
3	2	vllm/envs.py
2	1	vllm/model_executor/model_loader/openvino.py

[8354f6640] Cyrus Leung 2025-02-22 [Doc] Dockerfile instructions for optional dependencies and dev transformers (#13699)
30	0	docs/source/deployment/docker.md

[c904fdddf] Gregory Shtrasberg 2025-02-22 [ROCm] Apply FP8 weights padding to values not divisible by 512 bytes on ROCm (#13231)
4	0	vllm/envs.py
15	0	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[558db8083] Sage Moore 2025-02-22 [V1][Kernel] Refactor the prefix_prefill kernel so that the caller no longer has to pass in the context lengths (#13095)
2	6	tests/kernels/test_prefix_prefill.py
0	1	vllm/attention/backends/rocm_flash_attn.py
0	1	vllm/attention/backends/xformers.py
1	3	vllm/attention/ops/paged_attn.py
9	8	vllm/attention/ops/prefix_prefill.py
0	12	vllm/v1/attention/backends/rocm_attn.py

[e109e598c] Kaixi Hou 2025-02-22 [NVIDIA] Support nvfp4 cutlass gemm (#13571)
3	1	CMakeLists.txt
5	0	csrc/ops.h
37	0	csrc/quantization/fp4/nvfp4_scaled_mm_entry.cu
280	0	csrc/quantization/fp4/nvfp4_scaled_mm_kernels.cu
7	0	csrc/torch_bindings.cpp
150	0	tests/kernels/test_nvfp4_scaled_mm.py
12	0	vllm/_custom_ops.py

[8db1b9d0a] Keyun Tong 2025-02-22 Support SSL Key Rotation in HTTP Server (#13495)
2	1	requirements-common.txt
72	0	tests/entrypoints/test_ssl_cert_refresher.py
6	0	vllm/entrypoints/api_server.py
13	1	vllm/entrypoints/launcher.py
1	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/entrypoints/openai/cli_args.py
74	0	vllm/entrypoints/ssl.py

[2382ad29d] youkaichao 2025-02-22 [ci] fix linter (#13701)
5	4	examples/offline_inference/data_parallel.py
1	1	vllm/config.py
1	0	vllm/utils.py
2	1	vllm/v1/engine/core_client.py
2	1	vllm/v1/worker/gpu_model_runner.py

[3e472d882] youkaichao 2025-02-22 [core] set up data parallel communication (#13591)
2	0	.buildkite/test-pipeline.yaml
76	0	examples/offline_inference/data_parallel.py
57	0	vllm/config.py
2	2	vllm/distributed/device_communicators/cuda_communicator.py
7	4	vllm/distributed/device_communicators/custom_all_reduce.py
62	14	vllm/distributed/parallel_state.py
90	1	vllm/distributed/utils.py
20	0	vllm/envs.py
31	3	vllm/forward_context.py
18	0	vllm/utils.py
3	0	vllm/v1/engine/core.py
14	0	vllm/v1/engine/core_client.py
24	2	vllm/v1/engine/llm_engine.py
1	1	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/worker/gpu_model_runner.py
3	0	vllm/v1/worker/gpu_worker.py
5	0	vllm/worker/worker_base.py

[7f6bae561] Cyrus Leung 2025-02-22 [CI/Build] Fix pre-commit errors (#13696)
3	3	benchmarks/benchmark_latency.py
1	1	vllm/entrypoints/openai/serving_engine.py
6	1	vllm/entrypoints/openai/serving_score.py
5	8	vllm/model_executor/layers/mamba/mamba_mixer2.py
4	2	vllm/utils.py
5	2	vllm/v1/worker/gpu_model_runner.py

[105b8ce4c] Jee Jee Li 2025-02-22 [Misc] Reduce LoRA-related static variable (#13166)
15	2	tests/lora/conftest.py
9	4	tests/lora/test_lora_checkpoints.py
5	2	tests/lora/test_lora_huggingface.py
8	18	tests/lora/test_lora_manager.py
11	10	vllm/lora/models.py
26	0	vllm/lora/utils.py
5	3	vllm/lora/worker_manager.py
0	9	vllm/model_executor/models/baichuan.py
0	6	vllm/model_executor/models/bamba.py
0	10	vllm/model_executor/models/chatglm.py
0	4	vllm/model_executor/models/commandr.py
0	8	vllm/model_executor/models/exaone.py
0	12	vllm/model_executor/models/gemma.py
0	11	vllm/model_executor/models/gemma2.py
0	15	vllm/model_executor/models/glm4v.py
1	4	vllm/model_executor/models/gpt_bigcode.py
0	4	vllm/model_executor/models/granite.py
0	7	vllm/model_executor/models/granitemoe.py
0	15	vllm/model_executor/models/idefics3.py
5	7	vllm/model_executor/models/interfaces.py
0	10	vllm/model_executor/models/internlm2.py
0	4	vllm/model_executor/models/jamba.py
0	4	vllm/model_executor/models/llama.py
0	8	vllm/model_executor/models/minicpm.py
0	16	vllm/model_executor/models/minicpm3.py
0	42	vllm/model_executor/models/minicpmv.py
0	4	vllm/model_executor/models/mixtral.py
0	20	vllm/model_executor/models/molmo.py
0	3	vllm/model_executor/models/nemotron.py
0	11	vllm/model_executor/models/phi.py
0	10	vllm/model_executor/models/phimoe.py
0	9	vllm/model_executor/models/qwen.py
0	20	vllm/model_executor/models/qwen2.py
0	21	vllm/model_executor/models/qwen2_5_vl.py
0	10	vllm/model_executor/models/qwen2_rm.py
0	18	vllm/model_executor/models/qwen2_vl.py
0	15	vllm/model_executor/models/qwen_vl.py
0	8	vllm/model_executor/models/solar.py
35	0	vllm/model_executor/models/transformers.py
0	8	vllm/model_executor/models/ultravox.py
0	3	vllm/worker/hpu_model_runner.py

[2cb8c1540] Mark McLoughlin 2025-02-22 [Metrics] Add `--show-hidden-metrics-for-version` CLI arg (#13295)
8	0	docs/source/serving/metrics.md
36	0	tests/test_version.py
3	1	vllm/config.py
20	0	vllm/engine/arg_utils.py
5	0	vllm/engine/metrics.py
5	0	vllm/v1/metrics/loggers.py
18	0	vllm/version.py

[1cd981da4] Mark McLoughlin 2025-02-22 [V1][Metrics] Support `vllm:cache_config_info` (#13299)
1	0	tests/entrypoints/openai/test_metrics.py
6	0	vllm/config.py
2	3	vllm/engine/metrics.py
2	8	vllm/engine/metrics_types.py
21	1	vllm/v1/metrics/loggers.py

[fca20841c] Yu Chin Fabian Lim 2025-02-22 Correction to TP logic for Mamba Mixer 2 when Num Groups not divisible by TP Size (#13660)
21	7	vllm/model_executor/layers/mamba/mamba_mixer2.py

[da31b5333] Jennifer Zhao 2025-02-22 [Bugfix] V1 Memory Profiling: V0 Sampler Integration without Rejection Sampler (#13594)
26	2	vllm/v1/worker/gpu_model_runner.py

[bb78fb318] Lu Fang 2025-02-21 [v1] Support allowed_token_ids in v1 Sampler (#13210)
1	0	tests/v1/sample/test_rejection_sampler.py
79	15	tests/v1/sample/test_sampler.py
13	0	tests/v1/worker/test_gpu_input_batch.py
14	0	vllm/v1/engine/processor.py
4	0	vllm/v1/sample/metadata.py
16	2	vllm/v1/sample/sampler.py
41	2	vllm/v1/worker/gpu_input_batch.py

[8aca27fa1] Robin 2025-02-22 [Bugfix] Fix benchmark script bug: inaccurate stats for vllm backend when max_model_len < input_len + output_len (#13691)
13	0	benchmarks/benchmark_guided.py
4	0	benchmarks/benchmark_latency.py
14	4	benchmarks/benchmark_prioritization.py
12	1	benchmarks/benchmark_throughput.py

[95c617e04] Dipika Sikka 2025-02-22 [Misc] Bump compressed-tensors (#13619)
1	1	requirements-common.txt

[9a1f1da5d] Shane A 2025-02-21 [Bugfix][Model] OLMo 2: split qkv correctly for GQA and MQA (#13687)
1	1	vllm/model_executor/models/olmo2.py

[68d630a0c] Gordon Wong 2025-02-22 [ROCM] fix native attention function call (#13650)
0	1	vllm/attention/backends/rocm_flash_attn.py

[68d535ef4] Jun Duan 2025-02-22 [Misc] Capture and log the time of loading weights (#13666)
5	3	vllm/v1/worker/gpu_model_runner.py
5	2	vllm/worker/model_runner.py

[c6ed93860] Robin 2025-02-22 [Bugfix][API Server] Fix invalid usage of 'ge' and 'le' in port valid… (#13672)
1	1	vllm/entrypoints/api_server.py
11	0	vllm/utils.py

[0ffdf8ce0] Keyun Tong 2025-02-21 [HTTP Server] Make model param optional in request (#13568)
32	0	tests/entrypoints/openai/test_chat.py
10	10	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/entrypoints/openai/serving_embedding.py
12	1	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_models.py
1	1	vllm/entrypoints/openai/serving_pooling.py
2	2	vllm/entrypoints/openai/serving_score.py

[8c0dd3d4d] Yuan Tang 2025-02-22 docs: Add a note on full CI run in contributing guide (#13646)
3	0	docs/source/contributing/overview.md

[ada7c780d] Isotr0py 2025-02-22 [Misc] Fix yapf linting tools etc not running on pre-commit (#13695)
0	1	.pre-commit-config.yaml

[288cc6c23] Lucas Wilkinson 2025-02-21 [Attention] MLA with chunked prefill (#12639)
7	0	csrc/cache.h
159	0	csrc/cache_kernels.cu
0	5	csrc/core/math.hpp
18	4	csrc/cuda_utils.h
3	2	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
6	0	csrc/torch_bindings.cpp
73	2	tests/kernels/test_cache.py
10	0	vllm/_custom_ops.py
4	8	vllm/attention/__init__.py
1503	0	vllm/attention/backends/mla/common.py
0	515	vllm/attention/backends/mla/utils.py
14	650	vllm/attention/backends/triton_mla.py
84	0	vllm/attention/ops/triton_merge_attn_states.py
0	13	vllm/config.py
3	4	vllm/engine/arg_utils.py
20	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py
4	0	vllm/utils.py
2	69	vllm/v1/attention/backends/flash_attn.py

[900edbfa4] John Zheng 2025-02-22 fix typo of grafana dashboard, with correct datasource (#13668)
4	4	examples/online_serving/prometheus_grafana/grafana.json

[b2c3fc5d6] Isotr0py 2025-02-21 [Bugfix][CPU] Fix cpu all-reduce using native pytorch implementation  (#13586)
2	1	vllm/distributed/device_communicators/cpu_communicator.py

[839b27c6c] leoneo 2025-02-21 [Kernel]Add streamK for block-quantized CUTLASS kernels (#12978)
11	5	csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh
33	7	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh

[34ad27fe8] Kevin H. Luu 2025-02-20 [ci] Fix metrics test model path (#13635)
1	1	tests/metrics/test_metrics.py

[1c3c97576] Gabriel Marinho 2025-02-21 [FEATURE] Enables /score endpoint for embedding models (#12846)
1	2	docs/source/models/pooling_models.md
5	5	docs/source/serving/openai_compatible_server.md
2	4	tests/entrypoints/openai/test_rerank.py
173	111	tests/entrypoints/openai/test_score.py
13	33	vllm/entrypoints/llm.py
8	9	vllm/entrypoints/openai/api_server.py
17	14	vllm/entrypoints/openai/run_batch.py
2	2	vllm/entrypoints/openai/serving_engine.py
0	208	vllm/entrypoints/openai/serving_rerank.py
329	134	vllm/entrypoints/openai/serving_score.py
49	0	vllm/entrypoints/score_utils.py

[1cdc88614] Szymon Ożóg 2025-02-21 Missing comment explaining VDR variable in GGUF kernels (#13290)
2	0	csrc/quantization/gguf/vecdotq.cuh

[31aa045c1] Nick Hill 2025-02-20 [V1][Sampler] Avoid an operation during temperature application (#13587)
1	1	vllm/v1/sample/metadata.py
4	4	vllm/v1/sample/sampler.py
4	2	vllm/v1/utils.py
9	3	vllm/v1/worker/gpu_input_batch.py

[a30c09350] Roger Wang 2025-02-20 [Bugfix] Add `mm_processor_kwargs` to chat-related protocols (#13644)
8	0	vllm/entrypoints/openai/protocol.py

[c7b07a95a] Harry Mellor 2025-02-21 Use pre-commit to update `requirements-test.txt` (#13617)
7	0	.pre-commit-config.yaml
13	18	requirements-test.txt

[27a09dc52] Kaixi Hou 2025-02-20 [NVIDIA] Fix an issue to use current stream for the nvfp4 quant (#13632)
1	4	csrc/quantization/fp4/nvfp4_quant_kernels.cu

[981f3c831] Edwin Hernandez 2025-02-20 [Misc] Adding script to setup ray for multi-node vllm deployments  (#12913)
94	0	examples/online_serving/multi-node-serving.sh

[44c33f01f] Kante Yin 2025-02-21 Add llmaz as another integration (#13643)
1	0	docs/source/deployment/integrations/index.md
7	0	docs/source/deployment/integrations/llmaz.md

[33170081f] Lingfan Yu 2025-02-20 [Neuron][Kernel] Vectorize KV cache load in FlashPagedAttention to maximize DMA bandwidth (#13245)
153	0	tests/neuron/test_block_table.py
183	149	tests/neuron/test_prefix_prefill.py
428	199	vllm/attention/ops/nki_flash_attn.py

[71face854] Michael Goin 2025-02-20 [Bugfix] Fix max_num_batched_tokens for MLA (#13620)
14	6	vllm/config.py

[bfbc0b32c] Joe Runde 2025-02-20 [Frontend] Add backend-specific options for guided decoding (#13505)
1	1	docs/source/features/structured_outputs.md
24	1	examples/online_serving/openai_chat_completion_structured_outputs.py
16	0	tests/entrypoints/llm/test_guided_generate.py
10	0	tests/model_executor/test_guided_processors.py
4	1	vllm/config.py
5	2	vllm/engine/arg_utils.py
44	37	vllm/model_executor/guided_decoding/__init__.py
19	0	vllm/sampling_params.py

[6a417b860] ajayvohra2005 2025-02-20 fix neuron performance issue (#13589)
2	2	vllm/worker/neuron_worker.py

[d3ea50113] Woosuk Kwon 2025-02-20 [V1][Minor] Print KV cache size in token counts (#13596)
6	4	vllm/v1/core/kv_cache_utils.py

[34aad515c] Harry Mellor 2025-02-20 Update `pre-commit`'s `isort` version to remove warnings (#13614)
1	1	.pre-commit-config.yaml

[ed6e9075d] chenxiaobing 2025-02-20 [Bugfix] Fix deepseekv3 grouped topk error (#13474)
8	5	vllm/model_executor/layers/fused_moe/fused_moe.py

[992e5c3d3] Harry Mellor 2025-02-20 Merge similar examples in `offline_inference` into single `basic` example (#12737)
1	1	.buildkite/run-cpu-test.sh
1	1	.buildkite/run-gh200-test.sh
1	1	.buildkite/run-hpu-test.sh
1	1	.buildkite/run-openvino-test.sh
2	2	.buildkite/run-xpu-test.sh
6	6	.buildkite/test-pipeline.yaml
2	2	docs/source/generate_examples.py
2	2	docs/source/getting_started/installation/cpu/index.md
1	1	docs/source/getting_started/quickstart.md
2	2	docs/source/models/generative_models.md
3	3	docs/source/models/pooling_models.md
0	47	examples/offline_inference/aqlm_example.py
0	28	examples/offline_inference/arctic.py
94	0	examples/offline_inference/basic/README.md
0	0	examples/offline_inference/{ => basic}/basic.py
98	0	examples/offline_inference/basic/chat.py
42	0	examples/offline_inference/basic/classify.py
42	0	examples/offline_inference/basic/embed.py
57	0	examples/offline_inference/basic/generate.py
38	0	examples/offline_inference/basic/score.py
0	32	examples/offline_inference/basic_with_model_default_sampling.py
0	82	examples/offline_inference/chat.py
0	30	examples/offline_inference/classification.py
0	82	examples/offline_inference/cli.py
0	24	examples/offline_inference/cpu_offload.py
0	30	examples/offline_inference/embedding.py
0	34	examples/offline_inference/gguf_inference.py
0	25	examples/offline_inference/scoring.py
1	1	tests/plugins_tests/test_platform_plugins.py

[b69692a2d] Varun Sundar Rabindranath 2025-02-20 [Kernel] LoRA - Refactor sgmv kernels (#13110)
243	0	vllm/lora/ops/triton_ops/kernel_utils.py
44	73	vllm/lora/ops/triton_ops/sgmv_expand.py
40	56	vllm/lora/ops/triton_ops/sgmv_shrink.py

[a64a84433] Kevin H. Luu 2025-02-20 [2/n][ci] S3: Use full model path (#13564)
1	1	tests/basic_correctness/test_cumem.py
1	2	tests/conftest.py
2	1	tests/engine/test_computed_prefix_blocks.py
2	1	tests/engine/test_detokenization.py
8	4	tests/engine/test_executor.py
2	1	tests/engine/test_skip_tokenizer_init.py
7	6	tests/test_config.py
3	3	tests/test_regression.py

[aa1e62d0d] Kevin H. Luu 2025-02-20 [ci] Fix spec decode test (#13600)
0	1	tests/conftest.py

[497bc8312] Michael Goin 2025-02-20 [CI/Build] Use uv in the Dockerfile (#13566)
19	13	Dockerfile

[3738e6fa8] Yuan Tang 2025-02-20 [API Server] Add port number range validation (#13506)
1	1	vllm/entrypoints/api_server.py

[0023cd2b9] Gregory Shtrasberg 2025-02-20 [ROCm] MI300A compile targets deprecation (#13560)
1	1	CMakeLists.txt
1	2	csrc/quantization/fp8/amd/hip_float8_impl.h
1	2	csrc/rocm/attention.cu
1	2	vllm/attention/backends/rocm_flash_attn.py

[041e29471] 燃 2025-02-20 [Misc] add mm_processor_kwargs to extra_body for Qwen2.5-VL (#13533)
4	0	vllm/entrypoints/openai/protocol.py
2	0	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/model_executor/models/qwen2_5_vl.py
11	1	vllm/transformers_utils/processor.py

[962166787] Alex Brooks 2025-02-19 [Misc] Warn if the vLLM version can't be retrieved (#13501)
20	10	vllm/platforms/__init__.py

[8c755c3b6] Simon Mo 2025-02-19 [bugfix] spec decode worker get tp group only when initialized (#13578)
7	2	vllm/spec_decode/spec_decode_worker.py

[ba8116399] youkaichao 2025-02-20 [core] add sleep and wake up endpoint and v1 support (#12987)
8	4	tests/basic_correctness/test_cumem.py
32	0	tests/entrypoints/openai/test_sleep.py
6	0	vllm/engine/async_llm_engine.py
11	1	vllm/engine/multiprocessing/__init__.py
13	2	vllm/engine/multiprocessing/client.py
13	2	vllm/engine/multiprocessing/engine.py
10	0	vllm/engine/protocol.py
18	0	vllm/entrypoints/openai/api_server.py
1	0	vllm/entrypoints/openai/serving_transcription.py
6	0	vllm/v1/engine/async_llm.py
6	0	vllm/v1/engine/core.py
30	0	vllm/v1/engine/core_client.py
6	0	vllm/v1/engine/llm_engine.py

[0d243f2a5] Divakar Verma 2025-02-19 [ROCm][MoE] mi300 mixtral8x7B perf for specific BS (#13577)
2	2	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json
2	2	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json
1	1	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json

[88f6ba328] Kevin H. Luu 2025-02-19 [ci] Add AWS creds for AMD (#13572)
4	0	.buildkite/run-amd-test.sh
2	0	requirements-rocm.txt

[512368e34] Jee Jee Li 2025-02-20 [Misc] Qwen2.5 VL support LoRA (#13261)
1	1	docs/source/models/supported_models.md
5	0	tests/lora/conftest.py
118	58	tests/lora/test_qwen2vl.py
6	4	vllm/model_executor/models/qwen2_5_vl.py

[473f51cfd] Kevin H. Luu 2025-02-19 [3/n][CI] Load Quantization test models with S3 (#13570)
51	0	tests/conftest.py
2	2	vllm/model_executor/model_loader/weight_utils.py

[a4c402a75] Nick Hill 2025-02-19 [BugFix] Avoid error traceback in logs when V1 `LLM` terminates (#13565)
12	8	vllm/v1/engine/core_client.py

[550d97eb5] Isotr0py 2025-02-20 [Misc] Avoid calling unnecessary `hf_list_repo_files` for local model path (#13348)
10	3	vllm/transformers_utils/config.py

[fbbe1fbac] Cody Yu 2025-02-19 [MISC] Logging the message about Ray teardown (#13502)
4	0	vllm/executor/ray_distributed_executor.py

[01c184b8f] Wilson Wu 2025-02-20 Fix copyright year to auto get current year (#13561)
2	1	docs/source/conf.py

[ad5a35c21] youkaichao 2025-02-19 [doc] clarify multi-node serving doc (#13558)
6	2	docs/source/serving/distributed_serving.md

[5ae9f26a5] shangmingc 2025-02-19 [Bugfix] Fix device ordinal for multi-node spec decode (#13269)
2	1	vllm/spec_decode/spec_decode_worker.py

[377d10bd1] Cyrus Leung 2025-02-19 [VLM][Bugfix] Pass processor kwargs properly on init (#13516)
1	0	examples/offline_inference/vision_language_multi_image.py
2	5	tests/models/multimodal/processing/test_common.py
131	94	tests/models/multimodal/processing/test_h2ovl.py
16	8	tests/models/multimodal/processing/test_idefics3.py
107	35	tests/models/multimodal/processing/test_internvl.py
4	13	tests/models/multimodal/processing/test_llava_next.py
4	13	tests/models/multimodal/processing/test_llava_onevision.py
7	6	tests/models/multimodal/processing/test_phi3v.py
8	8	tests/models/multimodal/processing/test_qwen2_vl.py
11	7	tests/models/utils.py
5	5	tests/multimodal/test_processing.py
30	47	vllm/inputs/registry.py
2	2	vllm/model_executor/models/aria.py
2	2	vllm/model_executor/models/chameleon.py
5	10	vllm/model_executor/models/deepseek_vl2.py
2	2	vllm/model_executor/models/fuyu.py
6	9	vllm/model_executor/models/glm4v.py
2	7	vllm/model_executor/models/gritlm.py
31	10	vllm/model_executor/models/h2ovl.py
7	5	vllm/model_executor/models/idefics3.py
35	10	vllm/model_executor/models/internvl.py
14	13	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/llava_next.py
2	2	vllm/model_executor/models/llava_next_video.py
2	2	vllm/model_executor/models/llava_onevision.py
2	5	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/mllama.py
2	2	vllm/model_executor/models/molmo.py
14	5	vllm/model_executor/models/nvlm_d.py
2	2	vllm/model_executor/models/paligemma.py
3	2	vllm/model_executor/models/phi3v.py
6	14	vllm/model_executor/models/pixtral.py
12	35	vllm/model_executor/models/qwen2_5_vl.py
2	1	vllm/model_executor/models/qwen2_audio.py
64	30	vllm/model_executor/models/qwen2_vl.py
7	2	vllm/model_executor/models/qwen_vl.py
2	1	vllm/model_executor/models/ultravox.py
4	4	vllm/model_executor/models/whisper.py
1	4	vllm/multimodal/image.py
4	10	vllm/multimodal/registry.py
1	4	vllm/multimodal/utils.py
2	6	vllm/multimodal/video.py
86	6	vllm/transformers_utils/processor.py
21	1	vllm/transformers_utils/tokenizer.py

[52ce14d31] youkaichao 2025-02-19 [doc] clarify profiling is only for developers (#13554)
4	4	docs/source/contributing/profiling/profiling_index.md

[81dabf24a] Daniele 2025-02-19 [CI/Build] force writing version file (#13544)
1	1	pyproject.toml
1	1	setup.py

[423330263] Yannick Schnider 2025-02-19 [Feature] Pluggable platform-specific scheduler (#13161)
1	0	.buildkite/test-pipeline.yaml
33	0	tests/plugins_tests/test_scheduler_plugins.py
4	0	vllm/config.py
10	0	vllm/engine/arg_utils.py
8	3	vllm/engine/llm_engine.py

[caf7ff445] Nick Hill 2025-02-19 [V1][Core] Generic mechanism for handling engine utility (#13060)
1	1	tests/lora/test_add_lora.py
49	8	tests/v1/engine/test_engine_core_client.py
18	6	vllm/v1/engine/__init__.py
33	16	vllm/v1/engine/core.py
96	25	vllm/v1/engine/core_client.py

[f525c0be8] Lucia Fang 2025-02-19 [Model][Speculative Decoding] DeepSeek MTP spec decode (#12755)
11	11	.buildkite/test-pipeline.yaml
3	0	tests/models/registry.py
318	0	tests/spec_decode/e2e/test_mtp_correctness.py
33	10	vllm/config.py
284	0	vllm/model_executor/models/deepseek_mtp.py
15	7	vllm/model_executor/models/deepseek_v2.py
1	0	vllm/model_executor/models/registry.py
2	0	vllm/sequence.py
15	5	vllm/spec_decode/draft_model_runner.py
20	5	vllm/spec_decode/spec_decode_worker.py
16	4	vllm/worker/model_runner.py
4	1	vllm/worker/model_runner_base.py
3	3	vllm/worker/worker.py
2	0	vllm/worker/worker_base.py

[983a40a8b] Alex Brooks 2025-02-19 [Bugfix] Fix Positive Feature Layers in Llava Models (#13514)
34	0	tests/models/test_vision.py
1	1	vllm/model_executor/models/clip.py
2	2	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/pixtral.py
1	1	vllm/model_executor/models/siglip.py
5	4	vllm/model_executor/models/vision.py

[fdc5df6f5] Zhe Zhang 2025-02-19 use device param in load_model method (#13037)
1	1	vllm/worker/model_runner.py

[3b05cd455] Kevin H. Luu 2025-02-18 [perf-benchmark] Fix ECR path for premerge benchmark (#13512)
93	7	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[d5d214ac7] Kevin H. Luu 2025-02-18 [1/n][CI] Load models in CI from S3 instead of HF (#13205)
2	0	requirements-test.in
8	0	requirements-test.txt
10	9	tests/basic_correctness/test_basic_correctness.py
10	3	tests/basic_correctness/test_cumem.py
1	1	tests/basic_correctness/test_preemption.py
24	1	tests/conftest.py
5	1	tests/engine/test_computed_prefix_blocks.py
5	2	tests/engine/test_detokenization.py
13	4	tests/engine/test_executor.py
7	2	tests/engine/test_skip_tokenizer_init.py
1	1	tests/engine/test_stop_reason.py
10	3	tests/entrypoints/llm/test_chat.py
1	1	tests/entrypoints/llm/test_collective_rpc.py
3	1	tests/entrypoints/llm/test_encode.py
3	1	tests/entrypoints/llm/test_generate.py
3	1	tests/entrypoints/llm/test_generate_multiple_loras.py
5	2	tests/entrypoints/llm/test_guided_generate.py
29	2	tests/entrypoints/llm/test_lazy_outlines.py
7	2	tests/entrypoints/llm/test_prompt_validation.py
1	1	tests/entrypoints/openai/test_rerank.py
15	6	tests/metrics/test_metrics.py
2	1	tests/models/registry.py
5	1	tests/models/test_initialization.py
2	2	tests/mq_llm_engine/test_abort.py
4	2	tests/mq_llm_engine/test_error_handling.py
4	2	tests/mq_llm_engine/test_load.py
5	3	tests/multimodal/test_processing.py
0	0	tests/{runai_model_streamer => runai_model_streamer_test}/__init__.py
0	0	tests/{runai_model_streamer => runai_model_streamer_test}/test_runai_model_streamer_loader.py
0	0	tests/{runai_model_streamer => runai_model_streamer_test}/test_weight_utils.py
1	1	tests/samplers/test_ignore_eos.py
1	1	tests/samplers/test_logits_processor.py
1	1	tests/samplers/test_logprobs.py
1	1	tests/samplers/test_no_bad_words.py
1	1	tests/samplers/test_ranks.py
9	4	tests/test_config.py
10	3	tests/test_regression.py
1	1	tests/worker/test_swap.py
2	1	vllm/config.py
1	1	vllm/model_executor/model_loader/loader.py
2	2	vllm/model_executor/model_loader/weight_utils.py
1	2	vllm/transformers_utils/config.py
9	2	vllm/transformers_utils/s3_utils.py

[fd84857f6] Roger Wang 2025-02-18 [Doc] Add clarification note regarding paligemma (#13511)
5	1	docs/source/models/supported_models.md

[8aada19df] Divakar Verma 2025-02-19 [ROCm][MoE configs] mi325 mixtral & mi300 qwen_moe  (#13503)
200	0	vllm/model_executor/layers/fused_moe/configs/E=60,N=1408,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=60,N=176,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=60,N=352,device_name=AMD_Instinct_MI300X.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=60,N=704,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI325X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI325X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI325X.json

[9aa95b0e6] Kevin H. Luu 2025-02-18 [perf-benchmark] Allow premerge ECR (#13509)
3	3	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
5	1	.buildkite/nightly-benchmarks/scripts/wait-for-image.sh

[d0a7a2769] Yu-Zhou 2025-02-19 [Hardware][Gaudi][Feature] Support Contiguous Cache Fetch  (#12139)
1	5	vllm/attention/backends/hpu_attn.py
1	0	vllm/attention/ops/hpu_paged_attn.py
8	0	vllm/envs.py
71	43	vllm/worker/hpu_model_runner.py

[00b69c2d2] Harry Mellor 2025-02-19 [Misc] Remove dangling references to `--use-v2-block-manager` (#13492)
1	2	.buildkite/nightly-benchmarks/tests/serving-tests.json
1	1	docs/source/features/spec_decode.md

[4c8222989] Woosuk Kwon 2025-02-18 [V1][Spec Decode] Optimize N-gram matching with Numba (#13365)
1	0	requirements-common.txt
55	58	vllm/v1/spec_decode/ngram_proposer.py
11	2	vllm/v1/worker/gpu_model_runner.py

[c8d70e243] Woosuk Kwon 2025-02-18 Pin Ray version to 2.40.0 (#13490)
1	1	requirements-cuda.txt

[30172b494] Nick Hill 2025-02-18 [V1] Optimize handling of sampling metadata and req_ids list (#13244)
2	7	tests/v1/sample/test_rejection_sampler.py
18	26	tests/v1/sample/test_sampler.py
21	26	tests/v1/worker/test_gpu_input_batch.py
22	11	tests/v1/worker/test_gpu_model_runner.py
3	3	vllm/model_executor/layers/utils.py
4	2	vllm/v1/core/scheduler.py
10	11	vllm/v1/sample/metadata.py
6	7	vllm/v1/sample/ops/penalties.py
20	28	vllm/v1/sample/ops/topk_topp_sampler.py
2	0	vllm/v1/sample/rejection_sampler.py
6	7	vllm/v1/sample/sampler.py
11	0	vllm/v1/utils.py
102	111	vllm/v1/worker/gpu_input_batch.py
28	57	vllm/v1/worker/gpu_model_runner.py
0	2	vllm/v1/worker/tpu_model_runner.py

[a4d577b37] Murali Andoorveedu 2025-02-18 [V1][Tests] Adding additional testing for multimodal models to V1 (#13308)
48	12	tests/v1/engine/test_async_llm.py

[7b203b769] youkaichao 2025-02-19 [misc] fix debugging code (#13487)
4	4	docs/source/getting_started/troubleshooting.md

[4fb8142a0] Woosuk Kwon 2025-02-18 [V1][PP] Enable true PP with Ray executor  (#13472)
1	1	vllm/v1/executor/ray_distributed_executor.py

[a02c86b4d] Daniele 2025-02-18 [CI/Build] migrate static project metadata from setup.py to pyproject.toml (#8772)
35	1	pyproject.toml
4	47	setup.py

[380945845] Liangfu Chen 2025-02-18 [Bugfix] Fix invalid rotary embedding unit test (#13431)
1	1	tests/kernels/test_rotary_embedding.py

[d3231cb43] zifeitong 2025-02-18 [Bugfix] Handle content type with optional parameters (#13383)
2	1	vllm/entrypoints/openai/api_server.py

[435b502a6] Cyrus Leung 2025-02-18 [ROCm] Make amdsmi import optional for other platforms (#13460)
6	2	vllm/platforms/rocm.py

[29fc5772c] Isotr0py 2025-02-18 [Bugfix] Remove noisy error logging during local model loading (#13458)
1	2	vllm/transformers_utils/config.py

[2358ca527] Harry Mellor 2025-02-18 [Doc]: Improve feature tables (#13224)
8	0	docs/source/_static/custom.css
4	5	docs/source/conf.py
79	72	docs/source/features/compatibility_matrix.md
47	47	docs/source/features/quantization/supported_hardware.md
4	4	docs/source/models/pooling_models.md

[8cf97f866] Isotr0py 2025-02-18 [Bugfix] Fix failing transformers dynamic module resolving with spawn multiproc method (#13403)
17	8	vllm/model_executor/model_loader/utils.py

[e2603fefb] Yuan Tang 2025-02-18 [Bugfix] Ensure LoRA path from the request can be included in err msg (#13450)
1	1	vllm/lora/worker_manager.py

[b53d79983] Michael Goin 2025-02-18 Add outlines fallback when JSON schema has enum (#13449)
41	0	tests/entrypoints/conftest.py
41	0	tests/entrypoints/llm/test_guided_generate.py
4	0	vllm/model_executor/guided_decoding/utils.py

[9915912f7] Woosuk Kwon 2025-02-17 [V1][PP] Fix & Pin Ray version in requirements-cuda.txt (#13436)
1	1	requirements-cuda.txt

[d1b649f1e] Kyle Sayers 2025-02-18 [Quant] Aria SupportsQuant (#13416)
10	3	vllm/model_executor/models/aria.py

[ac19b519e] youkaichao 2025-02-18 [core] fix sleep mode in pytorch 2.6 (#13456)
10	3	vllm/device_allocator/cumem.py

[a1074b3ef] Yuan Tang 2025-02-18 [Bugfix] Only print out chat template when supplied (#13444)
3	1	vllm/entrypoints/openai/api_server.py

[00294e1bc] Kyle Sayers 2025-02-18 [Quant] Arctic SupportsQuant (#13366)
3	2	vllm/model_executor/models/arctic.py

[88787bce1] Kyle Sayers 2025-02-18 [Quant] Molmo SupportsQuant (#13336)
7	5	vllm/model_executor/models/molmo.py

[932b51ced] youkaichao 2025-02-18 [v1] fix parallel config rank (#13445)
1	0	vllm/v1/worker/worker_base.py

[7c7adf81f] Divakar Verma 2025-02-17 [ROCm] fix get_device_name for rocm (#13438)
43	6	vllm/platforms/rocm.py

[67ef8f666] Isotr0py 2025-02-18 [Model] Enable quantization support for `transformers` backend (#12960)
7	3	docs/source/models/supported_models.md
49	5	tests/models/test_transformers.py
10	15	vllm/model_executor/models/transformers.py

[efbe85444] Harry Mellor 2025-02-18 [Misc] Remove dangling references to `SamplingType.BEAM` (#13402)
0	78	vllm/model_executor/layers/sampler.py

[b3942e157] Tyler Michael Smith 2025-02-17 [Bugfix][CI][V1] Work around V1 + CUDA Graph + torch._scaled_mm fallback issue (#13425)
4	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
3	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py
4	2	vllm/model_executor/layers/quantization/fp8.py
8	6	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[cd4a72a28] Woosuk Kwon 2025-02-17 [V1][Spec decode] Move drafter to model runner  (#13363)
7	0	tests/v1/core/test_scheduler.py
4	7	vllm/v1/core/scheduler.py
0	30	vllm/v1/engine/core.py
3	0	vllm/v1/outputs.py
0	12	vllm/v1/request.py
15	8	vllm/v1/spec_decode/ngram_proposer.py
7	0	vllm/v1/worker/gpu_input_batch.py
47	0	vllm/v1/worker/gpu_model_runner.py
1	0	vllm/v1/worker/tpu_model_runner.py

[6ac485a95] Cody Yu 2025-02-17 [V1][PP] Fix intermediate tensor values (#13417)
3	0	vllm/sequence.py
8	2	vllm/v1/worker/gpu_model_runner.py

[4c21ce9eb] Woosuk Kwon 2025-02-17 [V1] Get input tokens from scheduler (#13339)
1	0	tests/v1/worker/test_gpu_model_runner.py
28	15	vllm/v1/core/scheduler.py
8	7	vllm/v1/core/scheduler_output.py
102	117	vllm/v1/worker/gpu_model_runner.py

[ce77eb941] r.4ntix 2025-02-17 [Bugfix] Fix VLLM_USE_MODELSCOPE issue (#13384)
12	6	vllm/transformers_utils/config.py
20	1	vllm/transformers_utils/utils.py

[30513d1cb] Yan Ma 2025-02-17 [Bugfix] fix xpu communicator (#13368)
54	0	vllm/distributed/device_communicators/xpu_communicator.py
4	0	vllm/platforms/xpu.py

[1f69c4a89] Tyler Michael Smith 2025-02-17 [Model] Support Mamba2 (Codestral Mamba) (#9292)
30	13	tests/models/decoder_only/language/test_mamba.py
2	0	tests/models/registry.py
6	2	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
2	19	vllm/model_executor/models/bamba.py
2	13	vllm/model_executor/models/jamba.py
3	15	vllm/model_executor/models/mamba.py
320	0	vllm/model_executor/models/mamba2.py
10	3	vllm/model_executor/models/mamba_cache.py
1	0	vllm/model_executor/models/registry.py

[7b623fca0] Cyrus Leung 2025-02-17 [VLM] Check required fields before initializing field config in `DictEmbeddingItems` (#13380)
2	2	docs/source/serving/multimodal_inputs.md
8	5	vllm/model_executor/models/minicpmo.py
12	6	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/qwen2_vl.py
11	7	vllm/multimodal/parse.py

[238dfc8ac] Mengqing Cao 2025-02-17 [MISC] tiny fixes (#13378)
1	1	vllm/executor/executor_base.py
1	1	vllm/platforms/interface.py

[45186834a] Huy Do 2025-02-17 Run v1 benchmark and integrate with PyTorch OSS benchmark database (#13068)
5	0	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
1	1	.buildkite/nightly-benchmarks/tests/latency-tests.json
58	33	benchmarks/benchmark_latency.py
39	9	benchmarks/benchmark_serving.py
2	1	benchmarks/benchmark_serving_guided.py
23	1	benchmarks/benchmark_throughput.py
39	0	benchmarks/benchmark_utils.py

[f857311d1] yankooo 2025-02-17 Fix spelling error in index.md (#13369)
1	1	docs/source/index.md

[46cdd5957] shangmingc 2025-02-17 [Feature][Spec Decode] Simplify the use of Eagle Spec Decode (#12304)
7	9	docs/source/features/spec_decode.md
144	0	tests/spec_decode/e2e/test_eagle_correctness.py
39	1	tests/spec_decode/test_spec_decode_worker.py
9	0	vllm/config.py
18	6	vllm/model_executor/models/eagle.py
12	0	vllm/spec_decode/multi_step_worker.py
19	0	vllm/spec_decode/smaller_tp_proposer_worker.py
25	2	vllm/spec_decode/spec_decode_worker.py

[2010f04c1] Jee Jee Li 2025-02-17 [V1][Misc] Avoid unnecessary log output (#13289)
7	5	vllm/v1/worker/gpu_model_runner.py

[69e1d23e1] Woosuk Kwon 2025-02-16 [V1][BugFix] Clean up rejection sampler & Fix warning msg (#13362)
69	40	vllm/v1/sample/rejection_sampler.py

[d67cc21b7] Isotr0py 2025-02-17 [Bugfix][Platform][CPU] Fix cuda platform detection on CPU backend edge case (#13358)
9	2	vllm/platforms/__init__.py

[e18227b04] Woosuk Kwon 2025-02-16 [V1][PP] Cache Intermediate Tensors (#13353)
28	8	vllm/v1/worker/gpu_model_runner.py

[7b8938655] Woosuk Kwon 2025-02-16 [V1][BugFix] Add __init__.py to v1/spec_decode/ (#13359)
0	0	vllm/v1/spec_decode/__init__.py

[da833b0ae] 凌 2025-02-17 [Docs] Change myenv to vllm. Update python_env_setup.inc.md (#13325)
4	4	docs/source/getting_started/installation/python_env_setup.inc.md

[5d2965b7d] Cyrus Leung 2025-02-16 [Bugfix] Fix 2 Node and Spec Decode tests (#13341)
5	5	tests/distributed/test_pipeline_parallel.py
12	4	vllm/spec_decode/ngram_worker.py

[a0231b7c2] youkaichao 2025-02-16 [platform] add base class for communicators (#13208)
117	0	vllm/distributed/device_communicators/base_device_communicator.py
33	0	vllm/distributed/device_communicators/cpu_communicator.py
106	0	vllm/distributed/device_communicators/cuda_communicator.py
14	19	vllm/distributed/device_communicators/hpu_communicator.py
16	13	vllm/distributed/device_communicators/tpu_communicator.py
0	49	vllm/distributed/device_communicators/xpu_communicator.py
48	201	vllm/distributed/parallel_state.py
7	0	vllm/platforms/cpu.py
4	0	vllm/platforms/cuda.py
4	0	vllm/platforms/hpu.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py
4	0	vllm/platforms/tpu.py

[124776ebd] youkaichao 2025-02-16 [ci] skip failed tests for flashinfer (#13352)
2	0	tests/kernels/test_flashinfer.py

[b7d309860] Roger Wang 2025-02-16 [V1] Update doc and examples for H2O-VL (#13349)
2	2	docs/source/models/supported_models.md
2	2	examples/offline_inference/vision_language.py
2	3	examples/offline_inference/vision_language_multi_image.py

[dc0f7ccf8] wchen61 2025-02-16 [BugFix] Enhance test_pos_encoding to support execution on multi-devices (#13187)
3	3	tests/kernels/test_pos_encoding.py

[d3d547e05] Michael Goin 2025-02-15 [Bugfix] Pin xgrammar to 0.1.11 (#13338)
1	1	requirements-common.txt

[12913d17b] Kyle Sayers 2025-02-15 [Quant] Add `SupportsQuant` to phi3 and clip (#13104)
1	0	vllm/model_executor/layers/quantization/aqlm.py
1	0	vllm/model_executor/layers/quantization/awq.py
1	0	vllm/model_executor/layers/quantization/awq_marlin.py
6	2	vllm/model_executor/layers/quantization/base_config.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	0	vllm/model_executor/layers/quantization/deepspeedfp.py
1	1	vllm/model_executor/layers/quantization/experts_int8.py
1	0	vllm/model_executor/layers/quantization/fbgemm_fp8.py
1	0	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/layers/quantization/gguf.py
1	0	vllm/model_executor/layers/quantization/gptq.py
1	0	vllm/model_executor/layers/quantization/gptq_marlin.py
1	0	vllm/model_executor/layers/quantization/gptq_marlin_24.py
1	0	vllm/model_executor/layers/quantization/hqq_marlin.py
1	0	vllm/model_executor/layers/quantization/ipex_quant.py
1	0	vllm/model_executor/layers/quantization/modelopt.py
1	0	vllm/model_executor/layers/quantization/moe_wna16.py
1	0	vllm/model_executor/layers/quantization/neuron_quant.py
1	0	vllm/model_executor/layers/quantization/qqq.py
1	0	vllm/model_executor/layers/quantization/quark/quark.py
1	0	vllm/model_executor/layers/quantization/tpu_int8.py
3	2	vllm/model_executor/models/clip.py
32	0	vllm/model_executor/models/interfaces.py
5	5	vllm/model_executor/models/phi3v.py

[80f63a396] Lily Liu 2025-02-15 [V1][Spec Decode] Ngram Spec Decode  (#12193)
196	6	tests/v1/core/test_scheduler.py
49	0	tests/v1/e2e/test_ngram_spec_decode.py
173	0	tests/v1/sample/test_rejection_sampler.py
2	0	tests/v1/sample/test_sampler.py
32	0	tests/v1/spec_decode/test_ngram.py
3	1	tests/v1/worker/test_gpu_input_batch.py
6	0	tests/v1/worker/test_gpu_model_runner.py
2	3	vllm/platforms/cuda.py
21	12	vllm/v1/core/kv_cache_manager.py
66	31	vllm/v1/core/scheduler.py
4	0	vllm/v1/core/scheduler_output.py
31	0	vllm/v1/engine/core.py
9	3	vllm/v1/outputs.py
17	0	vllm/v1/request.py
2	0	vllm/v1/sample/metadata.py
160	0	vllm/v1/sample/rejection_sampler.py
14	1	vllm/v1/sample/sampler.py
99	0	vllm/v1/spec_decode/ngram_proposer.py
11	1	vllm/v1/worker/gpu_input_batch.py
127	25	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/tpu_model_runner.py

[367cb8ce8] Cyrus Leung 2025-02-15 [Doc] [2/N] Add Fuyu E2E example for multimodal processor (#13331)
525	29	docs/source/contributing/model/multimodal.md
4	2	vllm/model_executor/models/fuyu.py

[54ed913f3] youkaichao 2025-02-15 [ci/build] update flashinfer (#13323)
5	2	Dockerfile

[9206b3d7e] Cody Yu 2025-02-15 [V1][PP] Run engine busy loop with batch queue (#13064)
51	0	tests/v1/core/test_scheduler.py
88	1	tests/v1/engine/test_engine_core.py
17	0	vllm/v1/core/scheduler.py
73	6	vllm/v1/engine/core.py
9	8	vllm/v1/executor/abstract.py
61	0	vllm/v1/executor/ray_distributed_executor.py

[ed0de3e4b] rasmith 2025-02-15 [AMD] [Model] DeepSeek tunings (#13199)
164	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json
164	0	vllm/model_executor/layers/quantization/utils/configs/N=8192,K=1536,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8,block_shape=[128,128].json

[2ad1bc7af] Mark McLoughlin 2025-02-15 [V1][Metrics] Add iteration_tokens_total histogram from V0 (#13288)
10	3	tests/entrypoints/openai/test_metrics.py
1	1	vllm/v1/engine/async_llm.py
24	4	vllm/v1/metrics/loggers.py

[7fdaaf48e] Isotr0py 2025-02-15 [Bugfix] Fix qwen2.5-vl image processor (#13286)
8	5	vllm/model_executor/models/qwen2_5_vl.py
9	1	vllm/model_executor/models/qwen2_vl.py

[067fa2255] Xu Song 2025-02-15 [Bugfix]Fix search start_index of stop_checker (#13280)
1	1	vllm/engine/output_processor/stop_checker.py

[907632567] Nick Hill 2025-02-14 [BugFix] Don't scan entire cache dir when loading model (#13302)
8	10	vllm/model_executor/model_loader/weight_utils.py

[97a3d6d99] Tyler Michael Smith 2025-02-15 [Bugfix] Massage MLA's usage of flash attn for RoCM (#13310)
11	2	vllm/attention/backends/mla/utils.py

[579d7a63b] Nicolò Lucchesi 2025-02-15 [Bugfix][Docs] Fix offline Whisper (#13274)
20	0	docs/source/models/supported_models.md
1	1	vllm/entrypoints/llm.py

[c9f9d5b39] Sage Moore 2025-02-14 [Bugfix][AMD] Update torch_bindings so that scaled_fp4_quant isn't build on ROCm (#13235)
4	4	csrc/ops.h
7	6	csrc/torch_bindings.cpp
1	0	vllm/_custom_ops.py

[0c7302684] Woosuk Kwon 2025-02-14 [V1][PP] Fix memory profiling in PP (#13315)
6	5	vllm/v1/worker/gpu_model_runner.py

[6a854c7a2] Nick Hill 2025-02-14 [V1][Sampler] Don't apply temp for greedy-only (#13311)
15	9	vllm/v1/sample/sampler.py

[e7eea5a52] Woosuk Kwon 2025-02-14 [V1][CI] Fix failed v1-test because of min_p (#13316)
4	1	tests/v1/worker/test_gpu_input_batch.py

[a12934d3e] Aoyu 2025-02-15 [V1][Core] min_p sampling support (#13191)
42	0	tests/v1/sample/test_sampler.py
2	0	vllm/v1/sample/metadata.py
26	0	vllm/v1/sample/sampler.py
26	0	vllm/v1/worker/gpu_input_batch.py

[3bcb8c75d] Joe Runde 2025-02-14 [Core] Reduce TTFT with concurrent partial prefills (#10235)
10	20	tests/basic_correctness/test_chunked_prefill.py
314	2	tests/core/test_chunked_prefill_scheduler.py
46	0	vllm/config.py
291	84	vllm/core/scheduler.py
33	1	vllm/engine/arg_utils.py
7	1	vllm/model_executor/layers/sampler.py

[5e5c8e091] Michael Goin 2025-02-14 [Quant][Perf] Use moe_wna16 kernel by default for MoEs with many experts (#13236)
1	1	tests/weight_loading/test_weight_loading.py
7	1	vllm/model_executor/layers/quantization/awq_marlin.py
16	19	vllm/model_executor/layers/quantization/gptq_marlin.py
15	5	vllm/model_executor/layers/quantization/moe_wna16.py

[c9e2d644e] Yu-Zhou 2025-02-14 [Hardware][Gaudi][Bugfix] Fix error for guided decoding (#12317)
10	1	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[7734e9a29] Russell Bryant 2025-02-14 [Core] choice-based structured output with xgrammar (#12632)
1	1	requirements-common.txt
4	5	vllm/model_executor/guided_decoding/__init__.py
30	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[6224a9f62] Lu Fang 2025-02-14 Support logit_bias in v1 Sampler (#13079)
59	12	tests/v1/sample/test_sampler.py
80	62	tests/v1/worker/test_gpu_input_batch.py
3	1	vllm/sampling_params.py
2	0	vllm/v1/sample/metadata.py
16	0	vllm/v1/sample/sampler.py
40	26	vllm/v1/worker/gpu_input_batch.py

[085b7b2d6] Nick Hill 2025-02-14 [V1] Simplify GPUModelRunner._update_states check (#13265)
4	2	vllm/v1/worker/gpu_model_runner.py

[4da1f667e] Cyrus Leung 2025-02-14 [VLM] Keep track of whether prompt replacements have been applied (#13215)
8	0	vllm/model_executor/models/glm4v.py
1	2	vllm/model_executor/models/llava.py
45	12	vllm/model_executor/models/llava_onevision.py
51	39	vllm/model_executor/models/minicpmo.py
99	122	vllm/model_executor/models/minicpmv.py
0	10	vllm/model_executor/models/qwen2_audio.py
35	65	vllm/model_executor/models/qwen2_vl.py
9	4	vllm/model_executor/models/qwen_vl.py
57	1	vllm/multimodal/parse.py
68	74	vllm/multimodal/processing.py

[556ef7f71] Jun Duan 2025-02-14 [Misc] Log time consumption of sleep and wake-up (#13115)
9	0	vllm/executor/executor_base.py

[83481ceb4] Xu Song 2025-02-14 [Bugfix] Fix missing parentheses (#13263)
2	2	vllm/sequence.py

[185cc19f9] Pooya Davoodi 2025-02-14 [Frontend] Optionally remove memory buffer used for uploading to URLs in run_batch (#12927)
108	15	vllm/entrypoints/openai/run_batch.py

[45f90bcbb] Alexander Matveev 2025-02-14 [WIP] TPU V1 Support Refactored (#13049)
15	5	tests/entrypoints/llm/test_accuracy.py
11	4	tests/entrypoints/openai/correctness/test_lmeval.py
1	0	vllm/platforms/interface.py
38	16	vllm/platforms/tpu.py
353	0	vllm/v1/attention/backends/pallas.py
8	0	vllm/v1/worker/block_table.py
1109	0	vllm/v1/worker/tpu_model_runner.py
203	0	vllm/v1/worker/tpu_worker.py

[b0ccfc565] Kero Liang 2025-02-14 [Bugfix][V1] GPUModelRunner._update_states should return True when there is a finished request in batch (#13126)
236	0	tests/v1/worker/test_gpu_model_runner.py
2	1	vllm/v1/worker/gpu_model_runner.py

[ba59b78a9] Sage Moore 2025-02-13 [ROCm][V1] Add intial ROCm support to V1 (#12790)
16	0	requirements-rocm-build.txt
4	2	vllm/attention/ops/prefix_prefill.py
30	15	vllm/platforms/rocm.py
4	1	vllm/v1/attention/backends/flash_attn.py
182	0	vllm/v1/attention/backends/rocm_attn.py

[cbc40128e] Varun Sundar Rabindranath 2025-02-14 [V1] LoRA - Enable Serving Usecase (#12883)
165	0	tests/lora/test_add_lora.py
1	0	vllm/v1/engine/__init__.py
4	4	vllm/v1/engine/async_llm.py
15	3	vllm/v1/engine/core.py
16	0	vllm/v1/engine/core_client.py
4	0	vllm/v1/worker/gpu_worker.py
5	0	vllm/v1/worker/lora_model_runner_mixin.py

[f0b2da72a] Michael Goin 2025-02-14 Expand MLA to support most types of quantization (#13181)
26	45	vllm/attention/backends/mla/utils.py
1	31	vllm/config.py
34	56	vllm/model_executor/model_loader/loader.py

[f2b20fe49] Harry Mellor 2025-02-14 Consolidate Llama model usage in tests (#13094)
5	5	tests/basic_correctness/test_basic_correctness.py
3	3	tests/basic_correctness/test_chunked_prefill.py
1	1	tests/basic_correctness/test_cpu_offload.py
1	1	tests/basic_correctness/test_cumem.py
1	1	tests/compile/test_basic_correctness.py
4	10	tests/compile/utils.py
2	2	tests/distributed/test_pipeline_parallel.py
1	1	tests/entrypoints/openai/test_serving_models.py
1	1	tests/entrypoints/openai/test_shutdown.py
4	6	tests/kv_transfer/disagg_test.py
4	4	tests/models/decoder_only/language/test_fp8.py
1	1	tests/models/registry.py
1	1	tests/quantization/test_register_quantization_config.py
1	1	tests/samplers/test_ignore_eos.py
3	3	tests/spec_decode/e2e/test_compatibility.py
1	1	tests/test_config.py
4	4	tests/test_sharded_state_loader.py
1	1	tests/tokenization/test_detokenize.py
2	2	tests/tokenization/test_get_eos.py
1	1	tests/v1/engine/test_async_llm.py
1	1	tests/v1/sample/test_logprobs.py
1	1	tests/v1/sample/test_logprobs_e2e.py

[40932d7a0] Wang Ran (汪然) 2025-02-14 [Misc] Remove redundant statements in scheduler.py (#13229)
0	1	vllm/core/scheduler.py

[84683fa27] XiaobingZhang 2025-02-14 [Bugfix] Offline example of disaggregated prefill (#13214)
1	1	examples/offline_inference/disaggregated_prefill.py

[067678262] Tyler Michael Smith 2025-02-13 [Bugfix][CI] Inherit codespell settings from pyproject.toml in the pre-commit-config (#13237)
2	1	.pre-commit-config.yaml
1	1	pyproject.toml

[09545c0a9] Tyler Michael Smith 2025-02-13 [Bugfix/CI] Turn test_compressed_tensors_2of4_sparse back on (#13250)
0	1	tests/quantization/test_compressed_tensors.py

[dd5ede444] Roger Wang 2025-02-13 [V1] Consolidate MM cache size to vllm.envs (#13239)
9	2	vllm/envs.py
2	4	vllm/multimodal/registry.py
7	5	vllm/v1/engine/mm_input_cache.py

[8c32b08a8] Jinzhen Lin 2025-02-14 [Kernel] Fix awq error when n is not divisable by 128 (#13227)
1	1	csrc/quantization/awq/gemm_kernels.cu

[410886950] Gregory Shtrasberg 2025-02-13 [ROCm] Avoid using the default stream on ROCm (#13238)
6	1	vllm/utils.py

[e38be640e] Harry Mellor 2025-02-14 Revert "Add label if pre-commit passes" (#13242)
0	38	.github/workflows/add_label_precommit.yml

[c1e37bf71] Tyler Michael Smith 2025-02-13 [Kernel][Bugfix] Refactor and Fix CUTLASS 2:4 Sparse Kernels (#13198)
5	5	CMakeLists.txt
68	1	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
1	2	csrc/ops.h
9	4	csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh
8	3	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
0	165	csrc/sparse/cutlass/sparse_compressor_c3x.cu
90	0	csrc/sparse/cutlass/sparse_compressor_c3x.cuh
0	42	csrc/sparse/cutlass/sparse_compressor_entry.cu
134	130	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu
154	78	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh
30	0	csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
2	4	csrc/torch_bindings.cpp
66	15	tests/kernels/test_cutlass_2of4_sparse.py
2	15	vllm/_custom_ops.py
0	7	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
7	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py

[2344192a5] Michael Goin 2025-02-13 Optimize moe_align_block_size for deepseek_v3 (#12850)
37	15	csrc/moe/moe_align_sum_kernels.cu
2	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[bffddd9a0] Harry Mellor 2025-02-13 Add label if pre-commit passes (#12527)
38	0	.github/workflows/add_label_precommit.yml

[d84cef76e] Nicolò Lucchesi 2025-02-13 [Frontend] Add `/v1/audio/transcriptions` OpenAI API endpoint (#12909)
10	2	.buildkite/test-pipeline.yaml
13	0	docs/source/serving/openai_compatible_server.md
23	0	examples/online_serving/openai_transcription_client.py
3	4	requirements-common.txt
1	0	requirements-test.in
5	0	requirements-test.txt
0	0	tests/entrypoints/openai/correctness/__init__.py
1	1	tests/entrypoints/openai/{test_accuracy.py => correctness/test_lmeval.py}
166	0	tests/entrypoints/openai/correctness/test_transcription_api_correctness.py
122	0	tests/entrypoints/openai/test_transcription_validation.py
1	0	tests/test_config.py
5	0	vllm/assets/audio.py
8	3	vllm/config.py
41	2	vllm/entrypoints/openai/api_server.py
162	1	vllm/entrypoints/openai/protocol.py
4	2	vllm/entrypoints/openai/serving_engine.py
305	0	vllm/entrypoints/openai/serving_transcription.py
27	0	vllm/model_executor/models/interfaces.py
10	2	vllm/model_executor/models/registry.py
3	2	vllm/model_executor/models/whisper.py

[37dfa6003] Vaibhav Jain 2025-02-13 [Bugfix] Missing Content Type returns 500 Internal Server Error (#13193)
16	0	tests/entrypoints/openai/test_basic.py
27	15	vllm/entrypoints/openai/api_server.py

[1bc3b5e71] Cyrus Leung 2025-02-13 [VLM] Separate text-only and vision variants of the same model architecture (#13157)
8	9	docs/source/models/supported_models.md
3	0	examples/offline_inference/vision_language.py
3	2	examples/offline_inference/vision_language_multi_image.py
87	84	tests/distributed/test_pipeline_parallel.py
3	8	tests/models/decoder_only/vision_language/test_models.py
39	23	tests/models/decoder_only/vision_language/vlm_utils/core.py
3	7	tests/models/decoder_only/vision_language/vlm_utils/types.py
22	15	tests/models/registry.py
1	2	tests/models/test_initialization.py
58	362	vllm/model_executor/models/chatglm.py
0	312	vllm/model_executor/models/glm4_vision_encoder.py
662	0	vllm/model_executor/models/glm4v.py
42	814	vllm/model_executor/models/qwen.py
794	0	vllm/model_executor/models/qwen_vl.py
4	5	vllm/model_executor/models/registry.py

[02ed8a1fb] 燃 2025-02-13 [Misc] Qwen2.5-VL Optimization (#13155)
25	36	vllm/model_executor/models/qwen2_5_vl.py
22	15	vllm/model_executor/models/qwen2_vl.py

[2092a6fa7] Aoyu 2025-02-13 [V1][Core] Add worker_base for v1 worker (#12816)
43	0	vllm/utils.py
9	19	vllm/v1/worker/gpu_worker.py
63	0	vllm/v1/worker/worker_base.py
38	33	vllm/worker/worker_base.py

[c9d3ecf01] Cyrus Leung 2025-02-13 [VLM] Merged multi-modal processor for Molmo (#12966)
1	1	docs/source/models/supported_models.md
1	1	tests/models/decoder_only/language/test_models.py
2	3	tests/models/decoder_only/vision_language/test_models.py
21	77	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
2	0	tests/models/multimodal/processing/test_common.py
1	0	tests/models/registry.py
681	342	vllm/model_executor/models/molmo.py
40	40	vllm/multimodal/inputs.py
1	34	vllm/utils.py

[fdcf64d3c] Roger Wang 2025-02-13 [V1] Clarify input processing and multimodal feature caching logic (#13211)
8	8	vllm/v1/engine/core.py
19	10	vllm/v1/engine/{mm_input_mapper.py => mm_input_cache.py}
14	6	vllm/v1/engine/processor.py
4	3	vllm/v1/worker/gpu_model_runner.py

[578087e56] Russell Bryant 2025-02-13 [Frontend] Pass pre-created socket to uvicorn (#13113)
1	0	vllm/entrypoints/api_server.py
6	3	vllm/entrypoints/launcher.py
10	3	vllm/entrypoints/openai/api_server.py

[fa253f1a7] Isotr0py 2025-02-13 [VLM] Remove input processor from clip and siglip (#13165)
8	141	vllm/model_executor/models/clip.py
2	72	vllm/model_executor/models/siglip.py

[9605c1256] Rui Qiao 2025-02-13 [V1][core] Implement pipeline parallel on Ray (#12996)
39	12	tests/distributed/test_pipeline_parallel.py
9	2	vllm/executor/ray_utils.py
27	14	vllm/v1/core/kv_cache_utils.py
12	7	vllm/v1/engine/core.py
5	7	vllm/v1/executor/abstract.py
15	1	vllm/v1/worker/gpu_model_runner.py
3	2	vllm/v1/worker/gpu_worker.py

[0ccd8769f] Russell Bryant 2025-02-13 [CI/Build] Allow ruff to auto-fix some issues (#13180)
1	1	.pre-commit-config.yaml

[cb944d581] Daniel Han 2025-02-12 Allow Unsloth Dynamic 4bit BnB quants to work (#12974)
10	2	vllm/model_executor/layers/quantization/bitsandbytes.py

[d46d490c2] Russell Bryant 2025-02-13 [Frontend] Move CLI code into vllm.cmd package (#12971)
1	1	docs/source/design/arch_overview.md
1	1	setup.py
0	0	vllm/entrypoints/cli/__init__.py
79	0	vllm/entrypoints/cli/main.py
172	0	vllm/entrypoints/cli/openai.py
63	0	vllm/entrypoints/cli/serve.py
24	0	vllm/entrypoints/cli/types.py
2	1	vllm/entrypoints/openai/api_server.py
6	202	vllm/scripts.py

[04f50ad9d] LikeSundayLikeRain 2025-02-13 [Bugfix] deepseek_r1_reasoning_parser put reason content in wrong field in certain edge case (#13097)
5	5	tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
1	1	vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py

[60c68df6d] Cody Yu 2025-02-12 [Build] Automatically use the wheel of the base commit with Python-only build (#13178)
13	3	docs/source/getting_started/installation/gpu/cuda.inc.md
23	4	setup.py

[009439cae] Lu Fang 2025-02-12 Simplify logic of locating CUDART so file path (#13203)
1	25	vllm/distributed/device_communicators/cuda_wrapper.py

[bc55d1307] Isotr0py 2025-02-13 [VLM] Implement merged multimodal processor for Mllama (#11427)
67	4	tests/models/encoder_decoder/vision_language/test_mllama.py
11	2	tests/models/multimodal/processing/test_common.py
83	7	vllm/inputs/preprocess.py
2	1	vllm/inputs/registry.py
203	205	vllm/model_executor/models/mllama.py
16	0	vllm/multimodal/inputs.py
57	3	vllm/multimodal/processing.py
17	11	vllm/multimodal/profiling.py

[d88c8666a] Michael Goin 2025-02-12 [Bugfix][Example] Fix GCed profiling server for TPU (#12792)
1	1	examples/offline_inference/profiling_tpu/profiling.py

[4fc5c23bb] Kaixi Hou 2025-02-12 [NVIDIA] Support nvfp4 quantization (#12784)
18	0	CMakeLists.txt
13	5	cmake/utils.cmake
12	0	csrc/cuda_utils.h
14	8	csrc/cuda_utils_kernels.cu
4	0	csrc/ops.h
32	0	csrc/quantization/fp4/nvfp4_quant_entry.cu
379	0	csrc/quantization/fp4/nvfp4_quant_kernels.cu
6	0	csrc/torch_bindings.cpp
149	0	tests/kernels/test_nvfp4_quant.py
1	0	tests/test_scalartype.py
57	0	vllm/_custom_ops.py
3	0	vllm/scalar_type.py

[9f9704dca] Kevin H. Luu 2025-02-12 [perf-benchmark] cleanup unused Docker images and volumes in H100 benchmark instance (#12706)
6	0	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[8eafe5eae] Russell Bryant 2025-02-12 [CI/Build] Ignore ruff warning up007 (#13182)
2	1	pyproject.toml

[4c0d93f4b] Murali Andoorveedu 2025-02-12 [V1][Bugfix] Copy encoder input ids to fix set iteration issue during VLM abort (#13173)
1	1	vllm/v1/core/encoder_cache_manager.py

[14b7899d1] Michael Goin 2025-02-12 [CI] Fix failing FP8 cpu offload test (#13170)
6	6	tests/quantization/test_cpu_offload.py

[09972e716] Michael Goin 2025-02-12 [Bugfix] Allow fallback to AWQ from AWQMarlin at per-layer granularity (#13119)
19	16	vllm/model_executor/layers/linear.py
18	10	vllm/model_executor/layers/quantization/awq_marlin.py
6	3	vllm/model_executor/layers/quantization/moe_wna16.py
15	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[36a08630e] Qubitium-ModelCloud 2025-02-13 [CORE] [QUANT] Support for GPTQModel's `dynamic` quantization per module override/control (#7086)
68	0	tests/quantization/test_gptq_dynamic.py
12	13	tests/quantization/test_lm_head.py
1	1	vllm/lora/layers.py
3	3	vllm/model_executor/layers/logits_processor.py
38	9	vllm/model_executor/layers/quantization/gptq.py
47	12	vllm/model_executor/layers/quantization/gptq_marlin.py
94	0	vllm/model_executor/layers/quantization/utils/gptq_utils.py
18	18	vllm/model_executor/layers/vocab_parallel_embedding.py

[2c2b560f4] Russell Bryant 2025-02-12 [CI/Build] Use mypy matcher for pre-commit CI job (#13162)
1	0	.github/workflows/pre-commit.yml

[042c3419f] Lu Fang 2025-02-12 Introduce VLLM_CUDART_SO_PATH to allow users specify the .so path (#12998)
31	1	vllm/distributed/device_communicators/cuda_wrapper.py
6	0	vllm/envs.py

[82cabf53a] Jee Jee Li 2025-02-13 [Misc] Delete unused LoRA modules (#13151)
12	6	tests/lora/test_lora_manager.py
7	1	vllm/lora/models.py
1	1	vllm/lora/punica_wrapper/punica_base.py

[314cfade0] Rafael Vasquez 2025-02-12 [Frontend] Generate valid tool call IDs when using `tokenizer-mode=mistral` (#12332)
0	0	tests/mistral_tool_use/__init__.py
40	0	tests/mistral_tool_use/conftest.py
29	0	tests/mistral_tool_use/test_mistral_tool_calls.py
33	0	tests/mistral_tool_use/utils.py
11	5	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
5	2	vllm/transformers_utils/tokenizers/__init__.py
30	0	vllm/transformers_utils/tokenizers/mistral.py

[985b4a2b1] Cyrus Leung 2025-02-12 [Bugfix] Fix num video tokens calculation for Qwen2-VL (#13148)
5	1	vllm/model_executor/models/qwen2_vl.py

[f4d97e4fc] bnellnm 2025-02-12 [Bug] [V1] Try fetching stop_reason from EngineOutput before checking the request (#13108)
6	4	vllm/v1/engine/output_processor.py

[f1042e86f] Shiyan Deng 2025-02-12 [Misc] AMD Build Improvements (#12923)
1	1	csrc/moe/moe_align_sum_kernels.cu
1	1	csrc/rocm/attention.cu
11	4	vllm/model_executor/models/registry.py
1	1	vllm/transformers_utils/configs/__init__.py

[7c4033acd] Maximilien de Bayser 2025-02-12 Further reduce the HTTP calls to huggingface.co (#13107)
79	56	vllm/transformers_utils/config.py

[d59def473] dependabot[bot] 2025-02-12 Bump actions/setup-python from 5.3.0 to 5.4.0 (#12672)
1	1	.github/workflows/cleanup_pr_body.yml
1	1	.github/workflows/lint-and-deploy.yaml
1	1	.github/workflows/pre-commit.yml

[0c7d9effc] dependabot[bot] 2025-02-12 Bump helm/chart-testing-action from 2.6.1 to 2.7.0 (#12463)
1	1	.github/workflows/lint-and-deploy.yaml

[dd3b4a01f] dependabot[bot] 2025-02-12 Bump actions/stale from 9.0.0 to 9.1.0 (#12462)
1	1	.github/workflows/stale.yml

[a0597c6b7] dependabot[bot] 2025-02-12 Bump helm/kind-action from 1.10.0 to 1.12.0 (#11612)
1	1	.github/workflows/lint-and-deploy.yaml

[e92694b6f] Lingfan Yu 2025-02-11 [Neuron][Kernel] Support Longer Sequences in NKI-based Flash PagedAttention and Improve Efficiency (#12921)
67	51	tests/neuron/test_prefix_prefill.py
87	129	vllm/attention/ops/nki_flash_attn.py

[842b0fd40] Kevin H. Luu 2025-02-11 [ci] Add more source file dependencies for some tests (#13123)
10	2	.buildkite/test-pipeline.yaml

[974dfd497] Christian Pinto 2025-02-12 [Model] IBM/NASA Prithvi Geospatial model  (#12830)
530	0	examples/offline_inference/prithvi_geospatial_mae.py
4	0	tests/models/registry.py
8	3	vllm/attention/backends/placeholder_attn.py
17	5	vllm/inputs/preprocess.py
238	0	vllm/model_executor/models/prithvi_geospatial_mae.py
4	0	vllm/model_executor/models/registry.py
10	1	vllm/worker/pooling_model_runner.py

[3ee696a63] Keyun Tong 2025-02-11 [RFC][vllm-API] Support tokenizer registry for customized tokenizer in vLLM (#12518)
3	2	benchmarks/benchmark_serving.py
123	0	tests/tokenization/test_tokenizer_registry.py
5	4	vllm/config.py
4	2	vllm/engine/arg_utils.py
19	12	vllm/entrypoints/llm.py
1	2	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/entrypoints/openai/serving_score.py
1	1	vllm/logits_process.py
12	6	vllm/transformers_utils/tokenizer.py
146	0	vllm/transformers_utils/tokenizer_base.py
28	11	vllm/transformers_utils/tokenizers/mistral.py

[72c2b68dc] Russell Bryant 2025-02-11 [Misc] Move pre-commit suggestion back to the end (#13114)
9	7	.pre-commit-config.yaml

[14ecab5be] Yuan Tang 2025-02-11 [Bugfix] Guided decoding falls back to outlines when fails to import xgrammar (#12976)
9	0	vllm/model_executor/guided_decoding/__init__.py
2	0	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[deb6c1c6b] Harry Mellor 2025-02-11 [Doc] Improve OpenVINO installation doc (#13102)
5	3	docs/source/getting_started/installation/ai_accelerator/openvino.inc.md

[565c1efa6] Li, Jiang 2025-02-12 [CI/Build][Bugfix] Fix CPU backend default threads num (#13077)
3	0	vllm/platforms/cpu.py

[2b25b7d2e] Szymon Ożóg 2025-02-11 Fix initializing GGUF weights for ColumnParallelLinear when using tensor parallel > 1 (#13023)
12	7	vllm/model_executor/layers/linear.py

[6c4dbe23e] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2025-02-11 [BugFix] Pop instead of del CUDA_VISIBLE_DEVICES (#12962)
1	1	examples/offline_inference/rlhf.py
1	1	examples/offline_inference/rlhf_colocate.py
5	5	tests/distributed/test_comm_ops.py
2	2	tests/distributed/test_custom_all_reduce.py

[21f5d50fa] MoonRide303 2025-02-11 [Bugfix] Do not use resource module on Windows (#12858) (#13029)
5	1	vllm/utils.py

[bf3e05215] Jewon Lee 2025-02-12 [Misc] Fix typo at comments at metrics.py (#13024)
1	1	vllm/engine/metrics.py

[ad9776353] Harry Mellor 2025-02-11 Set `torch_dtype` in `TransformersModel` (#13088)
1	0	vllm/model_executor/models/transformers.py

[75e6e1451] Mark McLoughlin 2025-02-11 [V1][Metrics] Add several request timing histograms (#12644)
31	0	tests/entrypoints/openai/test_metrics.py
2	1	tests/v1/core/test_scheduler.py
4	2	tests/v1/engine/test_engine_core.py
2	0	tests/v1/engine/test_engine_core_client.py
15	8	tests/v1/engine/test_output_processor.py
3	0	vllm/v1/core/kv_cache_manager.py
29	4	vllm/v1/core/scheduler.py
32	1	vllm/v1/engine/__init__.py
14	10	vllm/v1/engine/async_llm.py
6	4	vllm/v1/engine/core.py
10	9	vllm/v1/engine/core_client.py
1	0	vllm/v1/engine/llm_engine.py
44	15	vllm/v1/engine/output_processor.py
49	0	vllm/v1/metrics/loggers.py
74	23	vllm/v1/metrics/stats.py
18	7	vllm/v1/request.py

[110f59a33] மனோஜ்குமார் பழனிச்சாமி 2025-02-11 [Bugfix] fix flaky test (#13089)
6	21	tests/test_seed_behavior.py

[2e3b969ec] wangxiyuan 2025-02-11 [Platform] add pre_register_and_update function (#12432)
2	1	vllm/config.py
21	0	vllm/engine/arg_utils.py
18	0	vllm/platforms/interface.py

[da317197d] Yuhong Guo 2025-02-11 [Build] Fix cuda link target of cumem_allocator in CPU env (#12863)
1	1	CMakeLists.txt

[7539bbc6a] Gregory Shtrasberg 2025-02-11 [ROCm] Using a more precise memory profiling (#12624)
2	1	vllm/platforms/rocm.py

[9cf475949] Mengqing Cao 2025-02-11 [executor] init `local_rank` as device index (#13027)
5	0	vllm/executor/uniproc_executor.py

[41c5dd45b] Cody Yu 2025-02-11 [V1][Metrics] Add GPU prefix cache hit rate % gauge (#12592)
2	0	tests/entrypoints/openai/test_metrics.py
38	1	tests/v1/core/test_kv_cache_utils.py
24	0	vllm/v1/core/kv_cache_manager.py
64	0	vllm/v1/core/kv_cache_utils.py
1	0	vllm/v1/core/scheduler.py
27	2	vllm/v1/metrics/loggers.py
18	2	vllm/v1/metrics/stats.py

[fc6485d27] Ce Gao 2025-02-11 [Bugfix]: Reasoning output bug according to the chat template change (#13025)
4	4	examples/online_serving/openai_chat_completion_with_reasoning.py
90	18	tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
35	23	vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py

[78a141d76] Varun Sundar Rabindranath 2025-02-11 [Misc] LoRA - Refactor Punica ops tests (#12970)
652	0	tests/lora/test_punica_ops.py
0	401	tests/lora/test_punica_ops_sizes.py
0	317	tests/lora/test_punica_ops_variation.py
34	7	tests/lora/utils.py

[c320ca8ed] Russell Bryant 2025-02-11 [Core] Don't do platform detection at import time (#12933)
3	3	vllm/executor/executor_base.py
3	3	vllm/executor/ray_utils.py
2	2	vllm/platforms/cuda.py

[58047c6f0] Woosuk Kwon 2025-02-10 [Benchmark] Add BurstGPT to benchmark_serving (#13063)
8	0	benchmarks/README.md
39	1	benchmarks/benchmark_serving.py

[cb080f32e] Florian Greinacher 2025-02-11 [Bugfix] Support missing tool parameters in mistral tokenizer (#12884)
50	0	tests/tokenization/test_mistral_tokenizer.py
38	19	vllm/transformers_utils/tokenizers/mistral.py

[2c0f58203] Simon Mo 2025-02-10 [Docs] Annouce Meta Meetup (#13065)
4	0	README.md

[2ff485767] Woosuk Kwon 2025-02-10 [V1][Minor] Move scheduler outputs to a separate file (#13062)
3	86	vllm/v1/core/scheduler.py
108	0	vllm/v1/core/scheduler_output.py
1	1	vllm/v1/worker/gpu_model_runner.py
1	2	vllm/v1/worker/gpu_worker.py

[91e876750] Kevin H. Luu 2025-02-10 [misc] Fix setup.py condition to avoid AMD from being mistaken with CPU (#13022)
3	2	setup.py

[08b2d845d] Farzad Abdolhosseini 2025-02-10 [Model] Ultravox Model: Support v0.5 Release (#12912)
1	1	docs/source/models/supported_models.md
2	2	docs/source/serving/multimodal_inputs.md
2	2	examples/offline_inference/audio_language.py
1	1	examples/online_serving/openai_chat_completion_client_for_multimodal.py
2	2	tests/distributed/test_pipeline_parallel.py
1	1	tests/entrypoints/openai/test_audio.py
1	1	tests/entrypoints/test_chat_utils.py
1	1	tests/models/decoder_only/audio_language/test_ultravox.py
1	1	tests/models/multimodal/processing/test_common.py
1	1	tests/models/registry.py
17	9	vllm/model_executor/models/ultravox.py
6	0	vllm/transformers_utils/configs/ultravox.py

[2ae889052] மனோஜ்குமார் பழனிச்சாமி 2025-02-10 Fix seed parameter behavior in vLLM (#13007)
51	0	docs/seed_parameter_behavior.md
39	0	tests/test_seed_behavior.py
5	4	vllm/platforms/interface.py

[51f0b5f7f] Cyrus Leung 2025-02-10 [Bugfix] Clean up and fix multi-modal processors (#13012)
1	1	docs/source/features/compatibility_matrix.md
10	0	tests/models/decoder_only/language/test_models.py
1	1	tests/models/multimodal/processing/test_common.py
0	3	tests/multimodal/utils.py
63	97	vllm/model_executor/models/chatglm.py
46	45	vllm/model_executor/models/qwen.py
3	7	vllm/model_executor/models/qwen2_vl.py

[fde71262e] Kevin H. Luu 2025-02-10 [misc] Add retries with exponential backoff for HF file existence check (#13008)
48	13	vllm/transformers_utils/config.py

[243137143] Yuan Tang 2025-02-10 [Doc] Add link to tool_choice tracking issue in tool_calling.md (#13003)
1	1	docs/source/features/tool_calling.md

[b2496bb07] youkaichao 2025-02-10 [core] fix sleep mode and pytorch checkpoint compatibility (#13001)
8	2	tests/basic_correctness/test_cumem.py
0	1	vllm/model_executor/model_loader/weight_utils.py

[44607e07d] Yuan Tang 2025-02-09 Check if selected backend is None in get_attn_backend_cls() (#12975)
1	1	vllm/platforms/cpu.py

[67c4637cc] Nick Hill 2025-02-09 [V1] Use msgpack for core request serialization (#12918)
14	28	vllm/v1/engine/__init__.py
26	35	vllm/v1/engine/core.py
11	16	vllm/v1/engine/core_client.py
11	16	vllm/v1/serial_utils.py

[aa0ca5ebb] youkaichao 2025-02-10 [core][rlhf] add colocate example for RLHF (#12984)
2	2	.buildkite/test-pipeline.yaml
76	8	examples/offline_inference/{ray_placement.py => rlhf_colocate.py}

[59fff4a01] youkaichao 2025-02-10 [core] improve error handling when wake up from sleep mode (#12981)
51	12	csrc/cumem_allocator.cpp
27	0	tests/basic_correctness/test_cumem.py

[29f1d47e7] Lu Fang 2025-02-09 [MISC] Always import version library first in the vllm package (#12979)
4	2	vllm/__init__.py

[cf797aa85] youkaichao 2025-02-09 [core] port pynvml into vllm codebase (#12963)
18	2	.pre-commit-config.yaml
0	1	requirements-cuda.txt
3	2	tests/utils.py
0	0	vllm/third_party/__init__.py
6139	0	vllm/third_party/pynvml.py
9	30	vllm/utils.py

[24700c346] Woosuk Kwon 2025-02-08 [V1] Cache `uses_mrope` in GPUModelRunner (#12969)
13	10	vllm/v1/worker/gpu_model_runner.py

[d366ccc4e] Patrick von Platen 2025-02-08 [RFC] [Mistral] FP8 format (#10130)
16	4	vllm/model_executor/models/llama.py
5	2	vllm/model_executor/models/pixtral.py
32	5	vllm/transformers_utils/config.py
2	1	vllm/transformers_utils/tokenizers/mistral.py

[870c37481] Woosuk Kwon 2025-02-08 [V1][Minor] Remove outdated comment (#12968)
0	2	vllm/v1/core/kv_cache_manager.py

[86222a3da] Jee Jee Li 2025-02-09 [VLM] Merged multi-modal processor for GLM4V (#12449)
1	1	docs/source/models/supported_models.md
3	1	examples/offline_inference/vision_language.py
1	0	tests/models/multimodal/processing/test_common.py
217	165	vllm/model_executor/models/chatglm.py

[fe743b798] youkaichao 2025-02-09 [bugfix] fix early import of flash attention (#12959)
7	6	vllm/attention/backends/flash_attn.py
3	2	vllm/attention/backends/mla/utils.py
6	8	vllm/attention/backends/utils.py
4	3	vllm/v1/attention/backends/flash_attn.py

[913df14da] shangmingc 2025-02-08 [Bugfix] Remove unused seq_group_metadata_list from ModelInputForGPU (#12935)
0	1	vllm/worker/model_runner.py

[8a69e0e20] Cyrus Leung 2025-02-08 [CI/Build] Auto-fix Markdown files (#12941)
18	28	.buildkite/nightly-benchmarks/README.md
10	11	.buildkite/nightly-benchmarks/nightly-annotation.md
3	3	.buildkite/nightly-benchmarks/nightly-descriptions.md
2	8	.buildkite/nightly-benchmarks/performance-benchmarks-descriptions.md
2	1	.github/PULL_REQUEST_TEMPLATE.md
1	1	.pre-commit-config.yaml
0	1	CODE_OF_CONDUCT.md
9	5	README.md
2	0	benchmarks/README.md
32	12	csrc/quantization/cutlass_w8a8/Epilogues.md
7	7	csrc/quantization/machete/Readme.md
4	5	docs/source/getting_started/installation/gpu/rocm.inc.md
2	2	docs/source/serving/engine_args.md
29	30	examples/offline_inference/openai/openai_batch.md
3	3	examples/offline_inference/profiling_tpu/README.md
1	1	examples/online_serving/chart-helm/README.md
22	10	examples/online_serving/opentelemetry/Otel.md
9	5	examples/online_serving/prometheus_grafana/README.md
0	5	examples/other/logging_configuration.md
2	3	vllm/distributed/kv_transfer/README.md

[4c8dd12ef] Isotr0py 2025-02-08 [Misc] Add qwen2.5-vl BNB support (#12944)
29	30	vllm/model_executor/models/qwen2_5_vl.py

[256a2d29d] Jun Duan 2025-02-08 [Doc] Correct HF repository for TeleChat2 models (#12949)
1	1	docs/source/models/supported_models.md

[c45d398e6] Liangfu Chen 2025-02-08 [CI] Resolve transformers-neuronx version conflict (#12925)
0	3	.buildkite/run-neuron-test.sh
7	1	Dockerfile.neuron
0	1	requirements-neuron.txt
1	6	setup.py

[011e612d9] Jun Duan 2025-02-08 [Misc] Log time consumption on weight downloading (#12926)
10	1	vllm/model_executor/model_loader/weight_utils.py

[7e1837676] Varun Sundar Rabindranath 2025-02-08 [misc]  Add LoRA to benchmark_serving (#12898)
22	2	benchmarks/benchmark_serving.py

[2880e21e3] Sanju C Sudhakaran 2025-02-08 [Hardware][Intel-Gaudi] Enable long-contexts + LoRA support for Intel Gaudi (#12812)
56	1	vllm/lora/punica_wrapper/punica_hpu.py
2	1	vllm/model_executor/layers/rotary_embedding.py
15	2	vllm/worker/hpu_model_runner.py

[407b5537d] wangxiyuan 2025-02-08 [Build] Make pypi install work on CPU platform (#12874)
7	2	setup.py

[4ea48fb35] Woosuk Kwon 2025-02-08 [V1][Minor] Move cascade attn logic outside _prepare_inputs (#12943)
89	61	vllm/v1/worker/gpu_model_runner.py

[e31498bdc] Shaoting 2025-02-08 [Misc] Add offline test for disaggregated prefill (#12418)
111	0	examples/offline_inference/disaggregated_prefill.py

[91dd8f7aa] youkaichao 2025-02-08 [bugfix] respect distributed_executor_backend in world_size=1 (#12934)
20	1	tests/engine/{test_custom_executor.py => test_executor.py}
3	0	vllm/config.py
22	22	vllm/engine/llm_engine.py
8	9	vllm/v1/executor/abstract.py

[d01f66b03] zifeitong 2025-02-07 [Bugfix] Fix multi-round chat error when mistral tokenizer is used (#12859)
10	0	vllm/transformers_utils/tokenizers/mistral.py

[cc01223f3] Ke Zhao 2025-02-08 [Misc] Fix typo in the example file (#12896)
3	3	examples/online_serving/openai_chat_embedding_client_for_multimodal.py

[306923da8] Jee Jee Li 2025-02-08 [Bugfix] Fix Qwen2_5_VLForConditionalGeneration packed_modules_mapping (#12905)
5	2	vllm/model_executor/models/qwen2_5_vl.py

[324315833] Woosuk Kwon 2025-02-07 [V1] Move KV block hashes from Request to KVCacheManager (#12922)
11	10	tests/v1/core/test_prefix_caching.py
23	8	vllm/v1/core/kv_cache_manager.py
1	0	vllm/v1/core/scheduler.py
0	13	vllm/v1/request.py

[b21f0f9d1] Woosuk Kwon 2025-02-07 [V1][Minor] Remove outdated comment (#12928)
1	3	vllm/v1/core/kv_cache_manager.py

[45cbc4991] Lu Fang 2025-02-07 [Bugfix] Fix disagg hang caused by the prefill and decode communication issues (#12723)
40	47	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py

[932c6b746] Robert Shaw 2025-02-07 [V1] LM Eval With Streaming Integration Tests (#11590)
3	0	.buildkite/test-pipeline.yaml

[eaa92d443] TJian 2025-02-08 [ROCm] [Feature] [Doc] [Dockerfile] [BugFix] Support Per-Token-Activation Per-Channel-Weight FP8 Quantization Inferencing (#12501)
1	1	Dockerfile.rocm_base
45	19	docs/source/getting_started/installation/gpu/rocm.inc.md
38	11	tests/quantization/test_fp8.py
55	0	tests/quantization/test_ptpc_fp8.py
3	0	vllm/model_executor/layers/quantization/__init__.py
125	0	vllm/model_executor/layers/quantization/ptpc_fp8.py
27	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
1	1	vllm/platforms/rocm.py

[0630d4537] afeldman-nm 2025-02-07 [V1] Logprobs and prompt logprobs support (#9880)
2	2	tests/v1/core/test_scheduler.py
90	0	tests/v1/engine/conftest.py
45	4	tests/v1/engine/test_async_llm.py
23	0	tests/v1/engine/test_llm_engine.py
462	91	tests/v1/engine/test_output_processor.py
382	0	tests/v1/engine/utils.py
0	0	tests/v1/entrypoints/__init__.py
161	0	tests/v1/entrypoints/conftest.py
475	0	tests/v1/entrypoints/openai/test_completion.py
392	0	tests/v1/sample/test_logprobs.py
52	0	tests/v1/sample/test_logprobs_e2e.py
120	0	tests/v1/sample/utils.py
6	4	vllm/outputs.py
19	0	vllm/transformers_utils/detokenizer_utils.py
31	12	vllm/v1/core/scheduler.py
9	1	vllm/v1/engine/__init__.py
2	3	vllm/v1/engine/core.py
2	3	vllm/v1/engine/core_client.py
12	42	vllm/v1/engine/detokenizer.py
1	0	vllm/v1/engine/llm_engine.py
194	0	vllm/v1/engine/logprobs.py
88	38	vllm/v1/engine/output_processor.py
35	4	vllm/v1/engine/processor.py
11	8	vllm/v1/metrics/stats.py
43	11	vllm/v1/outputs.py
2	1	vllm/v1/sample/metadata.py
63	31	vllm/v1/sample/sampler.py
48	2	vllm/v1/serial_utils.py
12	15	vllm/v1/worker/gpu_input_batch.py
87	15	vllm/v1/worker/gpu_model_runner.py

[538fab93c] Amit Garg 2025-02-07 PR #12718 (#12718)
11	7	vllm/model_executor/layers/rotary_embedding.py
4	1	vllm/model_executor/models/llama.py

[ce26b1626] Cyrus Leung 2025-02-07 [Misc] Remove unnecessary detokenization in multimodal processing (#12868)
3	3	tests/entrypoints/openai/test_audio.py
2	2	tests/entrypoints/openai/test_vision.py
2	2	tests/entrypoints/openai/test_vision_embedding.py
0	3	vllm/inputs/preprocess.py

[1918aa1b8] Lu Fang 2025-02-07 [MISC][EASY] Break check file names into entry and args in the pre-commit hooks (#12880)
4	1	.pre-commit-config.yaml

[6e1fc61f0] Maximilien de Bayser 2025-02-07 Prevent unecessary requests to huggingface hub (#12837)
21	0	tests/entrypoints/offline_mode/test_offline_mode.py
75	40	vllm/transformers_utils/config.py

[aa375dca9] Szymon Ożóg 2025-02-07 [Bugfix] Missing quant_config in deepseek embedding layer (#12836)
2	1	vllm/model_executor/models/deepseek_v2.py

[433c4a492] ZSL98 2025-02-07 Make vllm compatible with verl (#12824)
0	7	vllm/distributed/parallel_state.py
1	1	vllm/executor/uniproc_executor.py

[ef533d25f] Lucas Wilkinson 2025-02-06 [Bugfix] FA2 illegal memory access (#12848)
1	1	CMakeLists.txt

[b26078235] Kevin H. Luu 2025-02-06 [misc] Revert # 12833 (#12857)
3	0	vllm/inputs/preprocess.py

[741429a4c] Lu Fang 2025-02-06 [MISC] Check space in the file names in the pre commit checks (#12804)
6	0	.pre-commit-config.yaml
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128,128].json}

[aff404571] Yu Chin Fabian Lim 2025-02-07 Add Bamba Model (#10909)
125	0	tests/kernels/test_mamba_mixer2.py
304	0	tests/kernels/test_mamba_ssm_ssd.py
23	12	tests/models/decoder_only/language/{test_jamba.py => test_hybrid.py}
1	0	tests/models/registry.py
62	78	vllm/attention/backends/placeholder_attn.py
534	0	vllm/model_executor/layers/mamba/mamba_mixer2.py
1	1	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
261	0	vllm/model_executor/layers/mamba/ops/ssd_bmm.py
615	0	vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py
750	0	vllm/model_executor/layers/mamba/ops/ssd_chunk_state.py
223	0	vllm/model_executor/layers/mamba/ops/ssd_combined.py
207	0	vllm/model_executor/layers/mamba/ops/ssd_state_passing.py
592	0	vllm/model_executor/models/bamba.py
3	8	vllm/model_executor/models/jamba.py
1	9	vllm/model_executor/models/mamba.py
3	4	vllm/model_executor/models/mamba_cache.py
1	0	vllm/model_executor/models/registry.py

[467a96a54] Varun Sundar Rabindranath 2025-02-06 [V1] LoRA Support (#10957)
17	0	tests/lora/conftest.py
8	0	tests/lora/test_baichuan.py
13	0	tests/lora/test_chatglm3_tp.py
8	0	tests/lora/test_gemma.py
12	0	tests/lora/test_llama_tp.py
11	0	tests/lora/test_lora_bias_e2e.py
13	0	tests/lora/test_phi.py
8	0	tests/lora/test_quant_model.py
1	1	tests/v1/core/test_kv_cache_utils.py
5	3	vllm/lora/layers.py
17	11	vllm/model_executor/layers/logits_processor.py
75	26	vllm/v1/core/kv_cache_utils.py
30	2	vllm/v1/core/scheduler.py
62	1	vllm/v1/worker/gpu_input_batch.py
44	12	vllm/v1/worker/gpu_model_runner.py
129	0	vllm/v1/worker/lora_model_runner_mixin.py

[8108ac841] Isotr0py 2025-02-07 [Bugfix] Fix unsupported FA version check for Turing GPU (#12828)
1	1	vllm/attention/backends/utils.py

[afe74f7a9] Jitse Klomp 2025-02-06 [Doc] double quote cmake package in build.inc.md (#12840)
1	1	docs/source/getting_started/installation/cpu/build.inc.md

[09b95e36a] youkaichao 2025-02-07 [torch.compile] PyTorch 2.6 and nightly compatibility (#12393)
1	1	tests/compile/piecewise/test_simple.py
3	3	tests/compile/piecewise/test_toy_llama.py
133	304	vllm/compilation/backends.py
340	0	vllm/compilation/compiler_interface.py
1	1	vllm/compilation/counter.py
0	1	vllm/compilation/inductor_pass.py
15	1	vllm/compilation/pass_manager.py
0	9	vllm/config.py

[85ac82d22] Isotr0py 2025-02-07 [Kernel] Make rotary_embedding ops more flexible with input shape (#12777)
89	14	csrc/pos_encoding_kernels.cu
22	9	tests/kernels/test_pos_encoding.py
3	22	vllm/attention/backends/mla/utils.py
1	12	vllm/model_executor/models/deepseek_v2.py

[1e57b1ee6] Cyrus Leung 2025-02-07 [Misc] Remove unnecessary decode call (#12833)
0	2	vllm/inputs/preprocess.py

[e152f2950] Kevin H. Luu 2025-02-06 [misc] Reduce number of config file requests to HuggingFace (#12797)
23	13	vllm/transformers_utils/config.py

[c786e757f] Lucas Wilkinson 2025-02-06 [Attention] Use FA3 for MLA on Hopper (#12807)
11	33	vllm/attention/backends/flash_attn.py
2	0	vllm/attention/backends/mla/utils.py
34	0	vllm/attention/backends/utils.py
4	26	vllm/v1/attention/backends/flash_attn.py

[cefd56ee3] Simon Mo 2025-02-06 [Docs] Add Google Cloud Slides (#12814)
1	1	README.md

[7ca9934fe] Dipika Sikka 2025-02-06 [Misc] Update w2 scale loading for GPTQMarlinMoE (#12757)
2	0	tests/weight_loading/models-large.txt
2	2	vllm/model_executor/layers/fused_moe/layer.py
17	6	vllm/model_executor/layers/quantization/gptq_marlin.py

[0408efc6d] youkaichao 2025-02-06 [Misc] Improve error message for incorrect pynvml (#12809)
5	1	vllm/platforms/__init__.py

[449d1bce0] Michael Goin 2025-02-06 [Misc] Remove duplicated DeepSeek V2/V3 model definition (#12793)
0	1	vllm/config.py
35	13	vllm/model_executor/models/deepseek_v2.py
0	806	vllm/model_executor/models/deepseek_v3.py
1	1	vllm/model_executor/models/registry.py

[1a6fcad4c] Harry Mellor 2025-02-06 Improve `TransformersModel` UX (#12785)
32	21	vllm/model_executor/models/transformers.py

[56534cd57] Lu Fang 2025-02-05 [Bugfix] Fix the test_ultravox.py's license (#12806)
2	0	tests/lora/test_ultravox.py

[d88506dda] Sumit Vij 2025-02-05 [Model] LoRA Support for Ultravox model (#11253)
1	1	docs/source/models/supported_models.md
12	4	tests/conftest.py
121	0	tests/lora/test_ultravox.py
26	2	vllm/model_executor/models/ultravox.py

[9cdea30b4] Lu Fang 2025-02-05 [Misc][Easy] Remove the space from the file name
0	0	vllm/model_executor/layers/fused_moe/configs/{E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/fused_moe/configs/{E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
0	0	vllm/model_executor/layers/quantization/utils/configs/{N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json => N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128,128].json}
1	1	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[76abd0c88] Lucas Wilkinson 2025-02-05 [Bugfix] Better FP8 supported defaults
17	11	vllm/model_executor/layers/quantization/utils/fp8_utils.py
5	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[5b19b9308] Gregory Shtrasberg 2025-02-05 [ROCm][Kernel] Using the correct warp_size value
2	2	csrc/moe/moe_align_sum_kernels.cu

[75404d041] Cyrus Leung 2025-02-06 [VLM] Update compatibility with transformers 4.49
1	2	docs/source/models/supported_models.md
0	38	examples/template_pixtral_hf.jinja
0	1	tests/entrypoints/test_chat_utils.py
2	2	tests/models/decoder_only/vision_language/test_models.py
4	3	tests/models/embedding/vision_language/test_llava_next.py
23	10	vllm/model_executor/models/llava.py
9	1	vllm/model_executor/models/llava_next.py
9	0	vllm/model_executor/models/minicpmv.py
2	2	vllm/multimodal/inputs.py

[bf3b79efb] Roger Wang 2025-02-05 [VLM] Qwen2.5-VL
11	0	docs/source/models/supported_models.md
31	0	examples/offline_inference/vision_language.py
58	0	examples/offline_inference/vision_language_multi_image.py
22	0	tests/models/decoder_only/vision_language/test_models.py
1	0	tests/models/multimodal/processing/test_common.py
2	0	tests/models/registry.py
2	2	vllm/entrypoints/chat_utils.py
34	24	vllm/model_executor/layers/rotary_embedding.py
1133	0	vllm/model_executor/models/qwen2_5_vl.py
8	8	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/registry.py
6	6	vllm/v1/worker/gpu_model_runner.py
3	6	vllm/worker/cpu_model_runner.py
3	6	vllm/worker/model_runner.py

[9a5b1554b] Russell Bryant 2025-02-05 [Docs] Drop duplicate [source] links
0	1	docs/source/conf.py

[a4ce74c14] Cyrus Leung 2025-02-06 [VLM] Use shared field to pass token ids to model
3	3	vllm/model_executor/models/internvl.py
232	43	vllm/multimodal/inputs.py

[3b2005e1d] Rahul Tuli 2025-02-05 Add: Support for Sparse24Bitmask Compressed Models
11	0	.buildkite/lm-eval-harness/configs/SparseLlama3.1_2of4_fp8_compressed.yaml
276	56	tests/quantization/test_compressed_tensors.py
26	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
190	48	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py

[af8486de4] Sanju C Sudhakaran 2025-02-06 [Hardware][Intel-Gaudi] Enable FusedSDPA support for Intel Gaudi (HPU)
11	1	vllm/attention/backends/hpu_attn.py

[4c3aac51e] Chen Zhang 2025-02-06 Merging PR #12536
22	6	vllm/attention/layer.py

[bc1bdeceb] youkaichao 2025-02-06 [core][distributed] exact ray placement control (#12732)
2	0	.buildkite/test-pipeline.yaml
121	0	examples/offline_inference/ray_placement.py
14	0	vllm/envs.py
23	13	vllm/executor/ray_distributed_executor.py
8	0	vllm/platforms/cuda.py
5	0	vllm/platforms/interface.py

[022bcc701] Akash kaothalkar 2025-02-05 [Bugfix] Fix 'ModuleNotFoundError: No module named 'intel_extension_for_pytorch'' for --tensor-parallel-size more than 1  (#12546)
11	3	vllm/distributed/parallel_state.py

[c53dc466b] Michael Goin 2025-02-05 [Doc] Remove performance warning for auto_awq.md (#12743)
0	6	docs/source/features/quantization/auto_awq.md

[3d09e592a] Nick Hill 2025-02-04 [V1][Misc] Shorten `FinishReason` enum and use constant strings (#12760)
9	3	vllm/v1/engine/__init__.py
3	4	vllm/v1/engine/detokenizer.py
3	3	vllm/v1/metrics/loggers.py
3	4	vllm/v1/metrics/stats.py
7	7	vllm/v1/request.py

[fcf2e3d7f] Harry Mellor 2025-02-05 [Bugfix] Fix OpenVINO model runner (#12750)
4	0	vllm/attention/backends/openvino.py
5	6	vllm/model_executor/model_loader/openvino.py
3	6	vllm/worker/openvino_model_runner.py

[58b218d7a] Michael Goin 2025-02-05 [Doc] Update PR Reminder with link to Developer Slack (#12748)
6	2	.github/workflows/reminder_comment.yml

[7ff7a638b] Kyle Sayers 2025-02-05 [Model][Quant] Fix GLM, Fix fused module mappings for quantization (#12634)
2	1	vllm/model_executor/layers/quantization/base_config.py
17	20	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
55	85	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
5	5	vllm/model_executor/layers/quantization/quark/quark.py
9	8	vllm/model_executor/layers/quantization/quark/utils.py
14	12	vllm/model_executor/layers/quantization/utils/quant_utils.py
4	0	vllm/model_executor/model_loader/loader.py
22	0	vllm/model_executor/model_loader/utils.py
21	5	vllm/model_executor/models/chatglm.py
22	9	vllm/model_executor/models/glm4_vision_encoder.py
11	3	vllm/model_executor/models/minicpmv.py
13	3	vllm/model_executor/models/qwen.py

[686006a22] Dipika Sikka 2025-02-04 [Misc] Bump the compressed-tensors version (#12736)
1	1	requirements-common.txt

[98fd089fc] Isotr0py 2025-02-05 [VLM] Add MLA with pure RoPE support for deepseek-vl2 models (#12729)
26	4	vllm/attention/backends/mla/utils.py
2	1	vllm/model_executor/models/deepseek_v2.py
2	1	vllm/model_executor/models/deepseek_v3.py

[249824c3b] Harry Mellor 2025-02-05 Refactor `Linear` handling in `TransformersModel` (#12727)
15	15	vllm/model_executor/layers/linear.py
33	43	vllm/model_executor/models/transformers.py

[64862d106] Aleksandr Malyshev 2025-02-04 [ROCM][AMD][TRITON] Halving warps number for fw_prefill to reduce spilling (#12713)
1	1	vllm/attention/ops/prefix_prefill.py

[b3a0d01e4] Aviv Keshet 2025-02-04 [Core] add and implement `VLLM_LOGITS_PROCESSOR_THREADS` (#12368)
9	0	vllm/envs.py
35	11	vllm/model_executor/layers/logits_processor.py

[75e94309e] Lucas Wilkinson 2025-02-04 [Perf] Mem align KV caches for CUDA devices (MLA perf improvement) (#12676)
3	0	csrc/cache.h
70	12	csrc/cache_kernels.cu
4	0	csrc/torch_bindings.cpp
262	0	tests/kernels/test_cache.py
5	0	vllm/_custom_ops.py
2	3	vllm/attention/backends/triton_mla.py
8	8	vllm/attention/ops/triton_decode_attention.py
10	0	vllm/envs.py
10	0	vllm/utils.py
55	11	vllm/worker/cache_engine.py

[233df6f5c] Mark McLoughlin 2025-02-05 [V1][Metrics] Add request_success_total counter, labelled with finish reason (#12579)
1	0	tests/entrypoints/openai/test_metrics.py
19	2	vllm/v1/engine/__init__.py
5	4	vllm/v1/engine/detokenizer.py
12	10	vllm/v1/engine/output_processor.py
14	1	vllm/v1/metrics/loggers.py
7	3	vllm/v1/metrics/stats.py
8	7	vllm/v1/request.py

[18016a5e6] Cyrus Leung 2025-02-04 [Bugfix] Fix CI failures for InternVL and Mantis models (#12728)
9	8	tests/models/decoder_only/vision_language/test_models.py
1	2	tests/models/registry.py
69	0	tests/multimodal/test_processing.py
0	402	tests/multimodal/test_processor_kwargs.py

[649550f27] Sophie du Couédic 2025-02-04 [Build] update requirements of no-device for plugin usage (#12630)
1	1	setup.py

[62467a834] Kero Liang 2025-02-04 Avoid unnecessary multi-modal input data copy when len(batch) == 1 (#12722)
16	0	vllm/multimodal/inputs.py

[6469038b1] Michael Greenbaum 2025-02-04 [Bugfix] Fix loading of fine-tuned models based on Phi-3-Small (#12689)
2	0	vllm/model_executor/models/phi3_small.py

[815079de8] Isotr0py 2025-02-04 [VLM] merged multimodal processor and V1 support for idefics3 (#12660)
1	1	docs/source/models/supported_models.md
2	2	tests/models/decoder_only/vision_language/test_models.py
8	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
1	0	tests/models/multimodal/processing/test_common.py
33	146	tests/models/multimodal/processing/test_idefics3.py
18	0	vllm/inputs/registry.py
252	308	vllm/model_executor/models/idefics3.py

[18a88fccc] Woosuk Kwon 2025-02-04 [V1] Remove scheduling constraint on partial requests (#12674)
214	0	tests/v1/core/test_scheduler.py
59	70	vllm/v1/core/scheduler.py
2	0	vllm/v1/worker/block_table.py
75	53	vllm/v1/worker/gpu_model_runner.py

[d1ca7df84] Cyrus Leung 2025-02-04 [VLM] Merged multi-modal processor for InternVL-based models (#12553)
5	1	docs/source/contributing/model/multimodal.md
7	3	docs/source/models/supported_models.md
0	131	tests/models/decoder_only/vision_language/test_h2ovl.py
1	1	tests/models/decoder_only/vision_language/test_models.py
21	16	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
3	1	tests/models/multimodal/processing/test_common.py
142	0	tests/models/multimodal/processing/test_h2ovl.py
32	175	tests/models/multimodal/processing/test_internvl.py
12	3	tests/models/multimodal/processing/test_llava_next.py
12	3	tests/models/multimodal/processing/test_llava_onevision.py
4	1	tests/models/multimodal/processing/test_phi3v.py
4	1	tests/models/multimodal/processing/test_qwen2_vl.py
5	1	vllm/model_executor/models/aria.py
5	1	vllm/model_executor/models/blip2.py
5	1	vllm/model_executor/models/chameleon.py
5	1	vllm/model_executor/models/deepseek_vl2.py
5	1	vllm/model_executor/models/fuyu.py
386	235	vllm/model_executor/models/h2ovl.py
503	320	vllm/model_executor/models/internvl.py
5	1	vllm/model_executor/models/llava.py
5	1	vllm/model_executor/models/llava_next_video.py
5	1	vllm/model_executor/models/llava_onevision.py
13	15	vllm/model_executor/models/minicpmo.py
24	26	vllm/model_executor/models/minicpmv.py
166	20	vllm/model_executor/models/nvlm_d.py
5	1	vllm/model_executor/models/phi3v.py
8	4	vllm/model_executor/models/qwen.py
5	1	vllm/model_executor/models/qwen2_audio.py
15	16	vllm/model_executor/models/qwen2_vl.py
5	1	vllm/model_executor/models/ultravox.py
11	0	vllm/multimodal/inputs.py
5	1	vllm/multimodal/processing.py
2	1	vllm/multimodal/profiling.py
3	1	vllm/multimodal/registry.py

[96b23621c] Jee Jee Li 2025-02-04 [Misc] Add BNB quantization for Whisper (#12381)
60	42	vllm/model_executor/model_loader/loader.py
7	0	vllm/model_executor/model_loader/utils.py
15	2	vllm/model_executor/models/whisper.py

[c36ac98d0] Hongxia Yang 2025-02-04 [AMD][ROCm] Enable DeepSeek model on ROCm (#12662)
31	0	tests/kernels/test_rocm_attention_selector.py
9	0	tests/worker/test_model_runner.py
5	1	vllm/attention/backends/mla/utils.py
10	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
3	0	vllm/platforms/rocm.py

[4896d0c2d] Kyle Sayers 2025-02-04 [Quant] Fix use_mla TypeError and support loading pure-sparsity Compressed Tensors configs (#12711)
3	2	vllm/config.py
5	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[bb392af43] Thomas Parnell 2025-02-04 [Doc] Replace ibm-fms with ibm-ai-platform (#12709)
6	6	docs/source/features/spec_decode.md
1	1	examples/offline_inference/mlpspeculator.py
1	1	tests/models/registry.py
1	1	tests/spec_decode/e2e/test_mlp_correctness.py
1	1	vllm/model_executor/models/mlp_speculator.py

[5d98d5608] Michael Goin 2025-02-03 Support Pixtral-Large HF by using llava multimodal_projector_bias config (#12710)
4	2	vllm/model_executor/models/llava.py
2	1	vllm/model_executor/models/llava_next.py
5	4	vllm/model_executor/models/llava_next_video.py
2	2	vllm/model_executor/models/llava_onevision.py

[73b35cca7] Russell Bryant 2025-02-03 [Core] Improve hash collision avoidance in prefix caching (#12621)
2	2	tests/core/block/test_prefix_caching_block.py
34	8	vllm/core/block/prefix_caching_block.py
9	0	vllm/v1/core/kv_cache_utils.py

[5095e9660] Cody Yu 2025-02-03 [V1] Revert `uncache_blocks` and support recaching full blocks (#12415)
0	30	tests/v1/core/test_prefix_caching.py
16	25	vllm/v1/core/kv_cache_manager.py

[cf58b9c4c] Cody Yu 2025-02-03 [MISC] Remove model input dumping when exception (#12582)
0	9	.github/ISSUE_TEMPLATE/400-bug-report.yml
0	58	tests/basic_correctness/test_basic_correctness.py
1	2	vllm/worker/model_runner.py
2	59	vllm/worker/model_runner_base.py

[4797dad3e] kushanam 2025-02-03 [Model] Add Deepseek V3 fp8_w8a8 configs for B200 (#12707)
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_B200,dtype=fp8_w8a8,block_shape=[128, 128].json

[6dd5e5282] Kyle Sayers 2025-02-03 Squelch MLA warning for Compressed-Tensors Models (#12704)
4	2	vllm/config.py

[c11de33da] Tyler Michael Smith 2025-02-03 [Bugfix][Kernel] Fix per-token/per-channel quantization for Hopper scaled mm (#12696)
24	35	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu

[33e0602e5] Russell Bryant 2025-02-03 [Misc] Fix improper placement of SPDX header in scripts (#12694)
1	2	cmake/hipify.py
1	0	tests/models/test_transformers.py
12	5	tools/check_spdx_header.py
1	1	tools/report_build_time_ninja.py
1	2	vllm/attention/ops/triton_flash_attention.py
1	0	vllm/model_executor/models/transformers.py

[a1a2aaadb] Arthur 2025-02-03 [Model]: Add `transformers` backend support (#11330)
2	0	.buildkite/test-pipeline.yaml
76	0	docs/source/models/supported_models.md
1	1	requirements-common.txt
5	0	tests/models/registry.py
3	1	tests/models/test_oot_registration.py
75	0	tests/models/test_transformers.py
14	0	vllm/config.py
18	4	vllm/engine/arg_utils.py
59	2	vllm/model_executor/model_loader/utils.py
11	1	vllm/model_executor/models/registry.py
264	0	vllm/model_executor/models/transformers.py

[1298a400e] youkaichao 2025-02-03 [ci/build] fix gh200 test (#12681)
2	2	.buildkite/check-wheel-size.py
2	2	.buildkite/run-gh200-test.sh
1	1	Dockerfile

[ad4a9dc81] youkaichao 2025-02-03 [cuda] manually import the correct pynvml module (#12679)
2	1	vllm/platforms/__init__.py
2	8	vllm/platforms/cuda.py
52	0	vllm/utils.py

[b9986454f] Srikanth Srinivas 2025-02-02 Fix for attention layers to remain unquantized during moe_wn16 quant (#12570)
6	4	vllm/model_executor/layers/quantization/moe_wna16.py

[c5932e5da] Eldar Kurtic 2025-02-03 Properly check if all fused layers are in the list of targets (#12666)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[20579c0fa] youkaichao 2025-02-03 make sure mistral_common not imported for non-mistral models (#12669)
2	2	.buildkite/test-pipeline.yaml
14	5	tests/standalone_tests/{lazy_torch_compile.py => lazy_imports.py}
2	1	vllm/multimodal/video.py
22	12	vllm/transformers_utils/tokenizers/mistral.py

[95460fc51] Yang Chen 2025-02-02 [Kernel] port sgl moe_align_block_size kernels (#12574)
92	0	csrc/moe/moe_align_sum_kernels.cu
6	0	csrc/moe/moe_ops.h
9	0	csrc/moe/torch_bindings.cpp
9	0	vllm/_custom_ops.py
8	1	vllm/envs.py
160	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[326fcc8b9] Zhuohan Li 2025-02-02 [Doc] Deprecate Discord (#12668)
1	1	CODE_OF_CONDUCT.md
2	3	README.md

[e64330910] youkaichao 2025-02-03 [doc][misc] clarify VLLM_HOST_IP for multi-node inference (#12667)
9	3	docs/source/serving/distributed_serving.md
4	1	vllm/executor/ray_utils.py

[e489ad7a2] Russell Bryant 2025-02-02 [Misc] Add SPDX-License-Identifier headers to python source files (#12628)
2	0	.buildkite/check-wheel-size.py
2	0	.buildkite/generate_index.py
1	0	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
2	0	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
2	0	.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
2	0	.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
2	0	.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
2	0	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
5	1	.pre-commit-config.yaml
2	0	benchmarks/backend_request_func.py
1	0	benchmarks/benchmark_guided.py
1	0	benchmarks/benchmark_latency.py
1	0	benchmarks/benchmark_long_document_qa_throughput.py
1	0	benchmarks/benchmark_prefix_caching.py
1	0	benchmarks/benchmark_prioritization.py
1	0	benchmarks/benchmark_serving.py
1	0	benchmarks/benchmark_serving_guided.py
1	0	benchmarks/benchmark_throughput.py
2	0	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
2	0	benchmarks/cutlass_benchmarks/utils.py
2	0	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
2	0	benchmarks/cutlass_benchmarks/weight_shapes.py
2	0	benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
2	0	benchmarks/disagg_benchmarks/round_robin_proxy.py
2	0	benchmarks/disagg_benchmarks/visualize_benchmark_results.py
2	0	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
2	0	benchmarks/kernels/benchmark_aqlm.py
2	0	benchmarks/kernels/benchmark_layernorm.py
2	0	benchmarks/kernels/benchmark_lora.py
2	0	benchmarks/kernels/benchmark_machete.py
2	0	benchmarks/kernels/benchmark_marlin.py
2	0	benchmarks/kernels/benchmark_moe.py
2	0	benchmarks/kernels/benchmark_paged_attention.py
2	0	benchmarks/kernels/benchmark_quant.py
2	0	benchmarks/kernels/benchmark_rmsnorm.py
2	0	benchmarks/kernels/benchmark_rope.py
2	0	benchmarks/kernels/benchmark_shapes.py
2	0	benchmarks/kernels/graph_machete_bench.py
2	0	benchmarks/kernels/utils.py
2	0	benchmarks/kernels/weight_shapes.py
2	0	benchmarks/overheads/benchmark_hashing.py
2	0	cmake/hipify.py
2	0	collect_env.py
2	0	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
2	0	csrc/quantization/machete/generate.py
2	0	docs/source/conf.py
2	0	docs/source/generate_examples.py
2	0	examples/offline_inference/aqlm_example.py
2	0	examples/offline_inference/arctic.py
1	0	examples/offline_inference/audio_language.py
2	0	examples/offline_inference/basic.py
2	0	examples/offline_inference/basic_with_model_default_sampling.py
2	0	examples/offline_inference/chat.py
2	0	examples/offline_inference/chat_with_tools.py
2	0	examples/offline_inference/classification.py
2	0	examples/offline_inference/cli.py
2	0	examples/offline_inference/cpu_offload.py
1	0	examples/offline_inference/distributed.py
2	0	examples/offline_inference/embedding.py
1	0	examples/offline_inference/encoder_decoder.py
1	0	examples/offline_inference/florence2_inference.py
2	0	examples/offline_inference/gguf_inference.py
2	0	examples/offline_inference/llm_engine_example.py
1	0	examples/offline_inference/lora_with_quantization_inference.py
2	0	examples/offline_inference/mlpspeculator.py
1	0	examples/offline_inference/multilora_inference.py
2	0	examples/offline_inference/neuron.py
2	0	examples/offline_inference/neuron_int8_quantization.py
2	0	examples/offline_inference/pixtral.py
2	0	examples/offline_inference/prefix_caching.py
2	0	examples/offline_inference/profiling.py
2	0	examples/offline_inference/profiling_tpu/profiling.py
1	0	examples/offline_inference/rlhf.py
1	0	examples/offline_inference/save_sharded_state.py
2	0	examples/offline_inference/scoring.py
2	0	examples/offline_inference/simple_profiling.py
2	0	examples/offline_inference/structured_outputs.py
1	0	examples/offline_inference/torchrun_example.py
2	0	examples/offline_inference/tpu.py
1	0	examples/offline_inference/vision_language.py
1	0	examples/offline_inference/vision_language_embedding.py
1	0	examples/offline_inference/vision_language_multi_image.py
2	0	examples/offline_inference/whisper.py
1	0	examples/online_serving/api_client.py
1	0	examples/online_serving/cohere_rerank_client.py
2	0	examples/online_serving/gradio_openai_chatbot_webserver.py
2	0	examples/online_serving/gradio_webserver.py
1	0	examples/online_serving/jinaai_rerank_client.py
2	0	examples/online_serving/openai_chat_completion_client.py
1	0	examples/online_serving/openai_chat_completion_client_for_multimodal.py
1	0	examples/online_serving/openai_chat_completion_client_with_tools.py
2	0	examples/online_serving/openai_chat_completion_structured_outputs.py
1	0	examples/online_serving/openai_chat_completion_with_reasoning.py
1	0	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
2	0	examples/online_serving/openai_chat_embedding_client_for_multimodal.py
2	0	examples/online_serving/openai_completion_client.py
1	0	examples/online_serving/openai_cross_encoder_score.py
2	0	examples/online_serving/openai_embedding_client.py
1	0	examples/online_serving/openai_pooling_client.py
2	0	examples/online_serving/opentelemetry/dummy_client.py
2	0	examples/other/tensorize_vllm_model.py
2	0	find_cuda_init.py
2	0	python_only_dev.py
2	0	setup.py
1	0	tests/async_engine/api_server_async_engine.py
2	0	tests/async_engine/test_api_server.py
2	0	tests/async_engine/test_async_llm_engine.py
2	0	tests/async_engine/test_request_tracker.py
1	0	tests/basic_correctness/test_basic_correctness.py
1	0	tests/basic_correctness/test_chunked_prefill.py
2	0	tests/basic_correctness/test_cpu_offload.py
2	0	tests/basic_correctness/test_cumem.py
1	0	tests/basic_correctness/test_preemption.py
2	0	tests/compile/backend.py
1	0	tests/compile/piecewise/test_simple.py
1	0	tests/compile/piecewise/test_toy_llama.py
2	0	tests/compile/test_basic_correctness.py
2	0	tests/compile/test_full_graph.py
2	0	tests/compile/test_functionalization.py
2	0	tests/compile/test_fusion.py
2	0	tests/compile/test_pass_manager.py
2	0	tests/compile/test_wrapper.py
2	0	tests/compile/utils.py
2	0	tests/conftest.py
2	0	tests/core/block/conftest.py
2	0	tests/core/block/e2e/conftest.py
2	0	tests/core/block/e2e/test_correctness.py
2	0	tests/core/block/e2e/test_correctness_sliding_window.py
2	0	tests/core/block/test_block_manager.py
2	0	tests/core/block/test_block_table.py
2	0	tests/core/block/test_common.py
2	0	tests/core/block/test_cpu_gpu_block_allocator.py
2	0	tests/core/block/test_naive_block.py
2	0	tests/core/block/test_prefix_caching_block.py
2	0	tests/core/test_chunked_prefill_scheduler.py
2	0	tests/core/test_num_computed_tokens_update.py
2	0	tests/core/test_scheduler.py
2	0	tests/core/test_scheduler_encoder_decoder.py
2	0	tests/core/test_serialization.py
2	0	tests/core/utils.py
2	0	tests/distributed/test_ca_buffer_sharing.py
1	0	tests/distributed/test_comm_ops.py
2	0	tests/distributed/test_custom_all_reduce.py
2	0	tests/distributed/test_distributed_oot.py
1	0	tests/distributed/test_multi_node_assignment.py
1	0	tests/distributed/test_pipeline_parallel.py
2	0	tests/distributed/test_pipeline_partition.py
2	0	tests/distributed/test_pp_cudagraph.py
2	0	tests/distributed/test_pynccl.py
2	0	tests/distributed/test_same_node.py
2	0	tests/distributed/test_shm_broadcast.py
2	0	tests/distributed/test_torchrun_example.py
2	0	tests/distributed/test_utils.py
1	0	tests/encoder_decoder/test_e2e_correctness.py
2	0	tests/engine/output_processor/test_multi_step.py
2	0	tests/engine/output_processor/test_stop_checker.py
2	0	tests/engine/test_arg_utils.py
2	0	tests/engine/test_computed_prefix_blocks.py
2	0	tests/engine/test_custom_executor.py
2	0	tests/engine/test_detokenization.py
2	0	tests/engine/test_multiproc_workers.py
2	0	tests/engine/test_short_mm_context.py
2	0	tests/engine/test_skip_tokenizer_init.py
1	0	tests/engine/test_stop_reason.py
2	0	tests/engine/test_stop_strings.py
2	0	tests/entrypoints/conftest.py
1	0	tests/entrypoints/llm/test_accuracy.py
2	0	tests/entrypoints/llm/test_chat.py
2	0	tests/entrypoints/llm/test_collective_rpc.py
2	0	tests/entrypoints/llm/test_encode.py
2	0	tests/entrypoints/llm/test_generate.py
2	0	tests/entrypoints/llm/test_generate_multiple_loras.py
2	0	tests/entrypoints/llm/test_gpu_utilization.py
2	0	tests/entrypoints/llm/test_guided_generate.py
2	0	tests/entrypoints/llm/test_init.py
2	0	tests/entrypoints/llm/test_lazy_outlines.py
2	0	tests/entrypoints/llm/test_prompt_validation.py
1	0	tests/entrypoints/offline_mode/test_offline_mode.py
2	0	tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
2	0	tests/entrypoints/openai/reasoning_parsers/utils.py
1	0	tests/entrypoints/openai/test_accuracy.py
2	0	tests/entrypoints/openai/test_async_tokenization.py
2	0	tests/entrypoints/openai/test_audio.py
2	0	tests/entrypoints/openai/test_basic.py
2	0	tests/entrypoints/openai/test_chat.py
2	0	tests/entrypoints/openai/test_chat_echo.py
2	0	tests/entrypoints/openai/test_chat_template.py
2	0	tests/entrypoints/openai/test_chunked_prompt.py
2	0	tests/entrypoints/openai/test_cli_args.py
2	0	tests/entrypoints/openai/test_completion.py
2	0	tests/entrypoints/openai/test_embedding.py
2	0	tests/entrypoints/openai/test_encoder_decoder.py
2	0	tests/entrypoints/openai/test_lora_adapters.py
2	0	tests/entrypoints/openai/test_metrics.py
2	0	tests/entrypoints/openai/test_models.py
2	0	tests/entrypoints/openai/test_oot_registration.py
2	0	tests/entrypoints/openai/test_pooling.py
2	0	tests/entrypoints/openai/test_prompt_validation.py
2	0	tests/entrypoints/openai/test_rerank.py
2	0	tests/entrypoints/openai/test_return_tokens_as_ids.py
2	0	tests/entrypoints/openai/test_root_path.py
2	0	tests/entrypoints/openai/test_run_batch.py
2	0	tests/entrypoints/openai/test_score.py
2	0	tests/entrypoints/openai/test_serving_chat.py
2	0	tests/entrypoints/openai/test_serving_models.py
2	0	tests/entrypoints/openai/test_shutdown.py
2	0	tests/entrypoints/openai/test_tokenization.py
2	0	tests/entrypoints/openai/test_video.py
2	0	tests/entrypoints/openai/test_vision.py
2	0	tests/entrypoints/openai/test_vision_embedding.py
2	0	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
2	0	tests/entrypoints/openai/tool_parsers/utils.py
2	0	tests/entrypoints/test_chat_utils.py
2	0	tests/kernels/allclose_default.py
2	0	tests/kernels/conftest.py
2	0	tests/kernels/quant_utils.py
2	0	tests/kernels/test_activation.py
2	0	tests/kernels/test_aqlm.py
2	0	tests/kernels/test_attention.py
2	0	tests/kernels/test_attention_selector.py
2	0	tests/kernels/test_awq.py
1	0	tests/kernels/test_awq_marlin.py
1	0	tests/kernels/test_awq_triton.py
2	0	tests/kernels/test_block_fp8.py
2	0	tests/kernels/test_blocksparse_attention.py
2	0	tests/kernels/test_cache.py
2	0	tests/kernels/test_cascade_flash_attn.py
2	0	tests/kernels/test_causal_conv1d.py
1	0	tests/kernels/test_cutlass.py
1	0	tests/kernels/test_cutlass_2of4_sparse.py
1	0	tests/kernels/test_encoder_decoder_attn.py
2	0	tests/kernels/test_flash_attn.py
2	0	tests/kernels/test_flashinfer.py
2	0	tests/kernels/test_fp8_quant.py
2	0	tests/kernels/test_fused_quant_layernorm.py
2	0	tests/kernels/test_ggml.py
2	0	tests/kernels/test_gguf.py
2	0	tests/kernels/test_gptq.py
2	0	tests/kernels/test_int8_quant.py
2	0	tests/kernels/test_layernorm.py
1	0	tests/kernels/test_machete_mm.py
2	0	tests/kernels/test_mamba_ssm.py
1	0	tests/kernels/test_marlin_gemm.py
1	0	tests/kernels/test_mha_attn.py
1	0	tests/kernels/test_moe.py
2	0	tests/kernels/test_permute_cols.py
2	0	tests/kernels/test_pos_encoding.py
2	0	tests/kernels/test_prefix_prefill.py
1	0	tests/kernels/test_rotary_embedding.py
2	0	tests/kernels/test_triton_decode_attention.py
1	0	tests/kernels/test_triton_scaled_mm.py
1	0	tests/kernels/test_utils.py
1	0	tests/kernels/utils.py
2	0	tests/kv_transfer/disagg_test.py
2	0	tests/kv_transfer/module_test.py
2	0	tests/kv_transfer/test_lookup_buffer.py
2	0	tests/kv_transfer/test_send_recv.py
2	0	tests/lora/conftest.py
2	0	tests/lora/data/long_context_test_data.py
2	0	tests/lora/test_baichuan.py
2	0	tests/lora/test_chatglm3_tp.py
2	0	tests/lora/test_gemma.py
2	0	tests/lora/test_jamba.py
2	0	tests/lora/test_layers.py
2	0	tests/lora/test_llama_tp.py
2	0	tests/lora/test_long_context.py
2	0	tests/lora/test_lora_bias_e2e.py
2	0	tests/lora/test_lora_checkpoints.py
2	0	tests/lora/test_lora_huggingface.py
2	0	tests/lora/test_lora_manager.py
2	0	tests/lora/test_minicpmv_tp.py
2	0	tests/lora/test_mixtral.py
2	0	tests/lora/test_peft_helper.py
2	0	tests/lora/test_phi.py
1	0	tests/lora/test_punica_ops_sizes.py
1	0	tests/lora/test_punica_ops_variation.py
2	0	tests/lora/test_quant_model.py
2	0	tests/lora/test_qwen2vl.py
2	0	tests/lora/test_tokenizer_group.py
2	0	tests/lora/test_utils.py
2	0	tests/lora/test_worker.py
2	0	tests/lora/utils.py
2	0	tests/metrics/test_metrics.py
2	0	tests/model_executor/conftest.py
2	0	tests/model_executor/test_enabled_custom_ops.py
2	0	tests/model_executor/test_guided_processors.py
2	0	tests/model_executor/test_model_load_with_params.py
2	0	tests/model_executor/weight_utils.py
2	0	tests/models/decoder_only/audio_language/test_ultravox.py
1	0	tests/models/decoder_only/language/test_aqlm.py
2	0	tests/models/decoder_only/language/test_fp8.py
1	0	tests/models/decoder_only/language/test_gguf.py
1	0	tests/models/decoder_only/language/test_gptq_marlin.py
1	0	tests/models/decoder_only/language/test_gptq_marlin_24.py
1	0	tests/models/decoder_only/language/test_granite.py
2	0	tests/models/decoder_only/language/test_jamba.py
1	0	tests/models/decoder_only/language/test_mamba.py
1	0	tests/models/decoder_only/language/test_mistral.py
2	0	tests/models/decoder_only/language/test_modelopt.py
1	0	tests/models/decoder_only/language/test_models.py
1	0	tests/models/decoder_only/language/test_phimoe.py
2	0	tests/models/decoder_only/vision_language/test_awq.py
2	0	tests/models/decoder_only/vision_language/test_h2ovl.py
2	0	tests/models/decoder_only/vision_language/test_intern_vit.py
1	0	tests/models/decoder_only/vision_language/test_models.py
2	0	tests/models/decoder_only/vision_language/test_phi3v.py
1	0	tests/models/decoder_only/vision_language/test_pixtral.py
2	0	tests/models/decoder_only/vision_language/test_qwen2_vl.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/builders.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/case_filtering.py
2	1	tests/models/decoder_only/vision_language/vlm_utils/core.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/runners.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/types.py
1	0	tests/models/embedding/language/test_cls_models.py
1	0	tests/models/embedding/language/test_embedding.py
2	0	tests/models/embedding/language/test_gritlm.py
1	0	tests/models/embedding/language/test_scoring.py
2	0	tests/models/embedding/utils.py
2	0	tests/models/embedding/vision_language/test_dse_qwen2_vl.py
2	0	tests/models/embedding/vision_language/test_llava_next.py
2	0	tests/models/embedding/vision_language/test_phi3v.py
1	0	tests/models/encoder_decoder/audio_language/test_whisper.py
1	0	tests/models/encoder_decoder/language/test_bart.py
2	0	tests/models/encoder_decoder/vision_language/test_broadcast.py
2	0	tests/models/encoder_decoder/vision_language/test_florence2.py
2	0	tests/models/encoder_decoder/vision_language/test_mllama.py
2	0	tests/models/multimodal/processing/test_common.py
1	0	tests/models/multimodal/processing/test_idefics3.py
1	0	tests/models/multimodal/processing/test_internvl.py
2	0	tests/models/multimodal/processing/test_llava_next.py
2	0	tests/models/multimodal/processing/test_llava_onevision.py
1	0	tests/models/multimodal/processing/test_phi3v.py
2	0	tests/models/multimodal/processing/test_qwen2_vl.py
2	0	tests/models/registry.py
2	0	tests/models/test_initialization.py
2	0	tests/models/test_oot_registration.py
2	0	tests/models/test_registry.py
2	0	tests/models/utils.py
1	0	tests/mq_llm_engine/test_abort.py
1	0	tests/mq_llm_engine/test_error_handling.py
1	0	tests/mq_llm_engine/test_load.py
2	0	tests/mq_llm_engine/utils.py
2	0	tests/multi_step/test_correctness_async_llm.py
2	0	tests/multi_step/test_correctness_llm.py
2	0	tests/multimodal/test_inputs.py
2	0	tests/multimodal/test_processing.py
2	0	tests/multimodal/test_processor_kwargs.py
2	0	tests/multimodal/test_utils.py
2	0	tests/multimodal/utils.py
2	0	tests/neuron/test_prefix_prefill.py
2	0	tests/plugins/vllm_add_dummy_model/setup.py
2	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py
2	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
2	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
2	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py
2	0	tests/plugins/vllm_add_dummy_platform/setup.py
2	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/__init__.py
2	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py
2	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
2	0	tests/plugins_tests/test_platform_plugins.py
1	0	tests/prefix_caching/test_disable_sliding_window.py
1	0	tests/prefix_caching/test_prefix_caching.py
2	0	tests/prompt_adapter/test_bloom.py
2	0	tests/prompt_adapter/test_multi_adapter_inference.py
2	0	tests/prompt_adapter/test_pa_lora.py
1	0	tests/quantization/test_bitsandbytes.py
1	0	tests/quantization/test_compressed_tensors.py
1	0	tests/quantization/test_configs.py
2	0	tests/quantization/test_cpu_offload.py
2	0	tests/quantization/test_experts_int8.py
1	0	tests/quantization/test_fp8.py
1	0	tests/quantization/test_ipex_quant.py
1	0	tests/quantization/test_lm_head.py
1	0	tests/quantization/test_quark.py
1	0	tests/quantization/test_register_quantization_config.py
2	0	tests/quantization/utils.py
2	0	tests/runai_model_streamer/test_runai_model_streamer_loader.py
2	0	tests/runai_model_streamer/test_weight_utils.py
1	0	tests/samplers/test_beam_search.py
1	0	tests/samplers/test_ignore_eos.py
2	0	tests/samplers/test_logits_processor.py
2	0	tests/samplers/test_logprobs.py
1	0	tests/samplers/test_no_bad_words.py
2	0	tests/samplers/test_ranks.py
1	0	tests/samplers/test_rejection_sampler.py
2	0	tests/samplers/test_sampler.py
1	0	tests/samplers/test_seeded_generate.py
1	0	tests/samplers/test_typical_acceptance_sampler.py
2	0	tests/spec_decode/e2e/conftest.py
2	0	tests/spec_decode/e2e/test_compatibility.py
1	0	tests/spec_decode/e2e/test_eagle_correctness.py
1	0	tests/spec_decode/e2e/test_integration.py
1	0	tests/spec_decode/e2e/test_integration_dist_tp2.py
1	0	tests/spec_decode/e2e/test_integration_dist_tp4.py
2	0	tests/spec_decode/e2e/test_logprobs.py
1	0	tests/spec_decode/e2e/test_medusa_correctness.py
1	0	tests/spec_decode/e2e/test_mlp_correctness.py
1	0	tests/spec_decode/e2e/test_multistep_correctness.py
1	0	tests/spec_decode/e2e/test_ngram_correctness.py
2	0	tests/spec_decode/e2e/test_seed.py
2	0	tests/spec_decode/test_batch_expansion.py
2	0	tests/spec_decode/test_dynamic_spec_decode.py
2	0	tests/spec_decode/test_metrics.py
2	0	tests/spec_decode/test_multi_step_worker.py
2	0	tests/spec_decode/test_ngram_worker.py
2	0	tests/spec_decode/test_scorer.py
2	0	tests/spec_decode/test_spec_decode_worker.py
2	0	tests/spec_decode/test_utils.py
2	0	tests/spec_decode/utils.py
2	0	tests/standalone_tests/lazy_torch_compile.py
2	0	tests/tensorizer_loader/conftest.py
2	0	tests/tensorizer_loader/test_tensorizer.py
1	0	tests/test_cache_block_hashing.py
2	0	tests/test_config.py
2	0	tests/test_embedded_commit.py
2	0	tests/test_inputs.py
2	0	tests/test_logger.py
2	0	tests/test_logits_processor.py
1	0	tests/test_regression.py
1	0	tests/test_sampling_params.py
2	0	tests/test_scalartype.py
2	0	tests/test_sequence.py
2	0	tests/test_sharded_state_loader.py
2	0	tests/test_utils.py
2	0	tests/tokenization/test_cached_tokenizer.py
2	0	tests/tokenization/test_detokenize.py
1	0	tests/tokenization/test_get_eos.py
2	0	tests/tokenization/test_tokenizer.py
2	0	tests/tokenization/test_tokenizer_group.py
2	0	tests/tool_use/conftest.py
2	0	tests/tool_use/test_chat_completion_request_validations.py
2	0	tests/tool_use/test_chat_completions.py
2	0	tests/tool_use/test_jamba_tool_parser.py
2	0	tests/tool_use/test_parallel_tool_calls.py
2	0	tests/tool_use/test_tool_calls.py
2	0	tests/tool_use/utils.py
2	0	tests/tpu/test_compilation.py
2	0	tests/tpu/test_custom_dispatcher.py
2	0	tests/tpu/test_quantization_accuracy.py
2	0	tests/tracing/test_tracing.py
2	0	tests/utils.py
2	0	tests/v1/core/test_kv_cache_utils.py
1	0	tests/v1/core/test_prefix_caching.py
2	0	tests/v1/e2e/test_cascade_attention.py
2	0	tests/v1/engine/test_async_llm.py
2	0	tests/v1/engine/test_engine_args.py
2	0	tests/v1/engine/test_engine_core.py
2	0	tests/v1/engine/test_engine_core_client.py
2	0	tests/v1/engine/test_output_processor.py
2	0	tests/v1/sample/test_sampler.py
2	0	tests/v1/test_stats.py
2	0	tests/v1/test_utils.py
2	0	tests/v1/worker/test_gpu_input_batch.py
2	0	tests/vllm_test_utils/setup.py
1	0	tests/vllm_test_utils/vllm_test_utils/__init__.py
2	0	tests/vllm_test_utils/vllm_test_utils/blame.py
2	0	tests/vllm_test_utils/vllm_test_utils/monitor.py
2	0	tests/weight_loading/test_weight_loading.py
2	0	tests/worker/test_encoder_decoder_model_runner.py
2	0	tests/worker/test_model_input.py
2	0	tests/worker/test_model_runner.py
2	0	tests/worker/test_profile.py
2	0	tests/worker/test_swap.py
43	0	tools/check_spdx_header.py
2	0	tools/profiler/print_layerwise_table.py
2	0	tools/profiler/visualize_layerwise_profile.py
2	0	tools/report_build_time_ninja.py
2	0	use_existing_torch.py
1	0	vllm/__init__.py
2	0	vllm/_custom_ops.py
2	0	vllm/_ipex_ops.py
2	0	vllm/adapter_commons/layers.py
2	0	vllm/adapter_commons/models.py
2	0	vllm/adapter_commons/request.py
2	0	vllm/adapter_commons/utils.py
2	0	vllm/adapter_commons/worker_manager.py
2	0	vllm/assets/audio.py
2	0	vllm/assets/base.py
2	0	vllm/assets/image.py
2	0	vllm/assets/video.py
2	0	vllm/attention/__init__.py
2	0	vllm/attention/backends/abstract.py
2	0	vllm/attention/backends/blocksparse_attn.py
1	0	vllm/attention/backends/flash_attn.py
2	0	vllm/attention/backends/flashinfer.py
2	0	vllm/attention/backends/hpu_attn.py
1	0	vllm/attention/backends/ipex_attn.py
2	0	vllm/attention/backends/mla/utils.py
2	0	vllm/attention/backends/openvino.py
2	0	vllm/attention/backends/pallas.py
2	0	vllm/attention/backends/placeholder_attn.py
1	0	vllm/attention/backends/rocm_flash_attn.py
1	0	vllm/attention/backends/torch_sdpa.py
2	0	vllm/attention/backends/triton_mla.py
1	0	vllm/attention/backends/utils.py
1	0	vllm/attention/backends/xformers.py
1	0	vllm/attention/layer.py
2	0	vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
2	0	vllm/attention/ops/blocksparse_attention/interface.py
2	0	vllm/attention/ops/blocksparse_attention/utils.py
2	0	vllm/attention/ops/hpu_paged_attn.py
2	0	vllm/attention/ops/ipex_attn.py
2	0	vllm/attention/ops/nki_flash_attn.py
2	0	vllm/attention/ops/paged_attn.py
2	0	vllm/attention/ops/prefix_prefill.py
2	0	vllm/attention/ops/triton_decode_attention.py
2	0	vllm/attention/ops/triton_flash_attention.py
2	0	vllm/attention/selector.py
2	0	vllm/beam_search.py
2	0	vllm/compilation/backends.py
2	0	vllm/compilation/counter.py
2	0	vllm/compilation/decorators.py
2	0	vllm/compilation/fix_functionalization.py
2	0	vllm/compilation/fusion.py
2	0	vllm/compilation/fx_utils.py
2	0	vllm/compilation/inductor_pass.py
2	0	vllm/compilation/monitor.py
2	0	vllm/compilation/multi_output_match.py
2	0	vllm/compilation/pass_manager.py
2	0	vllm/compilation/reshapes.py
2	0	vllm/compilation/vllm_inductor_pass.py
2	0	vllm/compilation/wrapper.py
2	0	vllm/config.py
2	0	vllm/connections.py
2	0	vllm/core/block/block_table.py
2	0	vllm/core/block/common.py
2	0	vllm/core/block/cpu_gpu_block_allocator.py
2	0	vllm/core/block/interfaces.py
2	0	vllm/core/block/naive_block.py
1	0	vllm/core/block/prefix_caching_block.py
1	0	vllm/core/block/utils.py
1	0	vllm/core/block_manager.py
2	0	vllm/core/evictor.py
2	0	vllm/core/interfaces.py
2	0	vllm/core/placeholder_block_space_manager.py
2	0	vllm/core/scheduler.py
2	0	vllm/device_allocator/cumem.py
2	0	vllm/distributed/__init__.py
2	0	vllm/distributed/communication_op.py
1	0	vllm/distributed/device_communicators/cuda_wrapper.py
2	0	vllm/distributed/device_communicators/custom_all_reduce.py
2	0	vllm/distributed/device_communicators/custom_all_reduce_utils.py
2	0	vllm/distributed/device_communicators/hpu_communicator.py
2	0	vllm/distributed/device_communicators/pynccl.py
2	0	vllm/distributed/device_communicators/pynccl_wrapper.py
2	0	vllm/distributed/device_communicators/shm_broadcast.py
2	0	vllm/distributed/device_communicators/tpu_communicator.py
2	0	vllm/distributed/device_communicators/xpu_communicator.py
1	0	vllm/distributed/kv_transfer/kv_connector/base.py
2	0	vllm/distributed/kv_transfer/kv_connector/factory.py
1	0	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
1	0	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
1	0	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
1	0	vllm/distributed/kv_transfer/kv_pipe/base.py
2	0	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py
1	0	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
1	0	vllm/distributed/kv_transfer/kv_transfer_agent.py
2	0	vllm/distributed/parallel_state.py
2	0	vllm/distributed/utils.py
2	0	vllm/engine/arg_utils.py
2	0	vllm/engine/async_llm_engine.py
2	0	vllm/engine/async_timeout.py
2	0	vllm/engine/llm_engine.py
2	0	vllm/engine/metrics.py
1	0	vllm/engine/metrics_types.py
2	0	vllm/engine/multiprocessing/__init__.py
2	0	vllm/engine/multiprocessing/client.py
2	0	vllm/engine/multiprocessing/engine.py
2	0	vllm/engine/output_processor/interfaces.py
2	0	vllm/engine/output_processor/multi_step.py
2	0	vllm/engine/output_processor/single_step.py
2	0	vllm/engine/output_processor/stop_checker.py
2	0	vllm/engine/output_processor/util.py
2	0	vllm/engine/protocol.py
1	0	vllm/entrypoints/api_server.py
2	0	vllm/entrypoints/chat_utils.py
2	0	vllm/entrypoints/launcher.py
2	0	vllm/entrypoints/llm.py
2	0	vllm/entrypoints/logger.py
2	0	vllm/entrypoints/openai/api_server.py
1	0	vllm/entrypoints/openai/cli_args.py
2	0	vllm/entrypoints/openai/logits_processors.py
2	0	vllm/entrypoints/openai/protocol.py
2	0	vllm/entrypoints/openai/reasoning_parsers/__init__.py
2	0	vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py
2	0	vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
2	0	vllm/entrypoints/openai/run_batch.py
2	0	vllm/entrypoints/openai/serving_chat.py
2	0	vllm/entrypoints/openai/serving_completion.py
2	0	vllm/entrypoints/openai/serving_embedding.py
2	0	vllm/entrypoints/openai/serving_engine.py
2	0	vllm/entrypoints/openai/serving_models.py
2	0	vllm/entrypoints/openai/serving_pooling.py
2	0	vllm/entrypoints/openai/serving_rerank.py
2	0	vllm/entrypoints/openai/serving_score.py
2	0	vllm/entrypoints/openai/serving_tokenization.py
2	0	vllm/entrypoints/openai/tool_parsers/__init__.py
2	0	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py
2	0	vllm/entrypoints/openai/tool_parsers/utils.py
2	0	vllm/entrypoints/utils.py
2	0	vllm/envs.py
2	0	vllm/executor/executor_base.py
2	0	vllm/executor/mp_distributed_executor.py
2	0	vllm/executor/msgspec_utils.py
2	0	vllm/executor/multiproc_worker_utils.py
2	0	vllm/executor/ray_distributed_executor.py
2	0	vllm/executor/ray_utils.py
2	0	vllm/executor/uniproc_executor.py
2	0	vllm/forward_context.py
2	0	vllm/inputs/__init__.py
2	0	vllm/inputs/data.py
2	0	vllm/inputs/parse.py
2	0	vllm/inputs/preprocess.py
2	0	vllm/inputs/registry.py
1	0	vllm/logger.py
2	0	vllm/logging_utils/__init__.py
2	0	vllm/logging_utils/formatter.py
2	0	vllm/logits_process.py
2	0	vllm/lora/fully_sharded_layers.py
2	0	vllm/lora/layers.py
2	0	vllm/lora/lora.py
2	0	vllm/lora/models.py
2	0	vllm/lora/ops/torch_ops/__init__.py
2	0	vllm/lora/ops/torch_ops/lora_ops.py
2	0	vllm/lora/ops/triton_ops/__init__.py
1	0	vllm/lora/ops/triton_ops/bgmv_expand.py
1	0	vllm/lora/ops/triton_ops/bgmv_expand_slice.py
1	0	vllm/lora/ops/triton_ops/bgmv_shrink.py
1	0	vllm/lora/ops/triton_ops/sgmv_expand.py
1	0	vllm/lora/ops/triton_ops/sgmv_shrink.py
2	0	vllm/lora/ops/triton_ops/utils.py
2	0	vllm/lora/peft_helper.py
2	0	vllm/lora/punica_wrapper/__init__.py
1	0	vllm/lora/punica_wrapper/punica_base.py
2	0	vllm/lora/punica_wrapper/punica_cpu.py
1	0	vllm/lora/punica_wrapper/punica_gpu.py
2	0	vllm/lora/punica_wrapper/punica_hpu.py
2	0	vllm/lora/punica_wrapper/punica_selector.py
2	0	vllm/lora/punica_wrapper/utils.py
2	0	vllm/lora/request.py
2	0	vllm/lora/utils.py
2	0	vllm/lora/worker_manager.py
2	0	vllm/model_executor/__init__.py
2	0	vllm/model_executor/custom_op.py
2	0	vllm/model_executor/guided_decoding/__init__.py
2	0	vllm/model_executor/guided_decoding/guided_fields.py
2	0	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
2	0	vllm/model_executor/guided_decoding/outlines_decoding.py
2	0	vllm/model_executor/guided_decoding/outlines_logits_processors.py
2	0	vllm/model_executor/guided_decoding/utils.py
2	0	vllm/model_executor/guided_decoding/xgrammar_decoding.py
1	0	vllm/model_executor/layers/activation.py
2	0	vllm/model_executor/layers/fused_moe/__init__.py
1	0	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py
2	0	vllm/model_executor/layers/fused_moe/layer.py
2	0	vllm/model_executor/layers/fused_moe/moe_pallas.py
2	0	vllm/model_executor/layers/fused_moe/moe_torch_iterative.py
1	0	vllm/model_executor/layers/layernorm.py
2	0	vllm/model_executor/layers/linear.py
1	0	vllm/model_executor/layers/logits_processor.py
2	0	vllm/model_executor/layers/mamba/mamba_mixer.py
2	0	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
2	0	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
2	0	vllm/model_executor/layers/pooler.py
2	0	vllm/model_executor/layers/quantization/__init__.py
2	0	vllm/model_executor/layers/quantization/aqlm.py
2	0	vllm/model_executor/layers/quantization/awq.py
2	0	vllm/model_executor/layers/quantization/awq_marlin.py
2	0	vllm/model_executor/layers/quantization/awq_triton.py
2	0	vllm/model_executor/layers/quantization/base_config.py
2	0	vllm/model_executor/layers/quantization/bitsandbytes.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
2	0	vllm/model_executor/layers/quantization/deepspeedfp.py
2	0	vllm/model_executor/layers/quantization/experts_int8.py
2	0	vllm/model_executor/layers/quantization/fbgemm_fp8.py
2	0	vllm/model_executor/layers/quantization/fp8.py
2	0	vllm/model_executor/layers/quantization/gguf.py
2	0	vllm/model_executor/layers/quantization/gptq.py
2	0	vllm/model_executor/layers/quantization/gptq_marlin.py
2	0	vllm/model_executor/layers/quantization/gptq_marlin_24.py
2	0	vllm/model_executor/layers/quantization/hqq_marlin.py
2	0	vllm/model_executor/layers/quantization/ipex_quant.py
2	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
2	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
2	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py
2	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py
2	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py
2	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
2	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
2	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
2	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py
2	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
2	0	vllm/model_executor/layers/quantization/kv_cache.py
2	0	vllm/model_executor/layers/quantization/marlin.py
2	0	vllm/model_executor/layers/quantization/modelopt.py
2	0	vllm/model_executor/layers/quantization/moe_wna16.py
2	0	vllm/model_executor/layers/quantization/neuron_quant.py
2	0	vllm/model_executor/layers/quantization/qqq.py
2	0	vllm/model_executor/layers/quantization/quark/quark.py
2	0	vllm/model_executor/layers/quantization/quark/quark_moe.py
2	0	vllm/model_executor/layers/quantization/quark/schemes/__init__.py
2	0	vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py
2	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
2	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
2	0	vllm/model_executor/layers/quantization/quark/utils.py
1	0	vllm/model_executor/layers/quantization/schema.py
2	0	vllm/model_executor/layers/quantization/tpu_int8.py
2	0	vllm/model_executor/layers/quantization/utils/__init__.py
2	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
2	0	vllm/model_executor/layers/quantization/utils/layer_utils.py
2	0	vllm/model_executor/layers/quantization/utils/machete_utils.py
2	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py
2	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
1	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py
2	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test_qqq.py
1	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
2	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
2	0	vllm/model_executor/layers/rejection_sampler.py
2	0	vllm/model_executor/layers/resampler.py
2	0	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/layers/sampler.py
2	0	vllm/model_executor/layers/spec_decode_base_sampler.py
2	0	vllm/model_executor/layers/typical_acceptance_sampler.py
1	0	vllm/model_executor/layers/utils.py
2	0	vllm/model_executor/layers/vocab_parallel_embedding.py
2	0	vllm/model_executor/model_loader/__init__.py
2	0	vllm/model_executor/model_loader/loader.py
1	0	vllm/model_executor/model_loader/neuron.py
2	0	vllm/model_executor/model_loader/openvino.py
2	0	vllm/model_executor/model_loader/tensorizer.py
1	0	vllm/model_executor/model_loader/utils.py
1	0	vllm/model_executor/model_loader/weight_utils.py
2	0	vllm/model_executor/models/__init__.py
2	0	vllm/model_executor/models/adapters.py
1	0	vllm/model_executor/models/arctic.py
2	0	vllm/model_executor/models/aria.py
2	0	vllm/model_executor/models/baichuan.py
2	0	vllm/model_executor/models/bart.py
2	0	vllm/model_executor/models/bert.py
1	0	vllm/model_executor/models/blip.py
2	0	vllm/model_executor/models/blip2.py
2	0	vllm/model_executor/models/bloom.py
2	0	vllm/model_executor/models/chameleon.py
2	0	vllm/model_executor/models/chatglm.py
1	0	vllm/model_executor/models/clip.py
2	0	vllm/model_executor/models/commandr.py
2	0	vllm/model_executor/models/dbrx.py
2	0	vllm/model_executor/models/decilm.py
2	0	vllm/model_executor/models/deepseek.py
2	0	vllm/model_executor/models/deepseek_v2.py
2	0	vllm/model_executor/models/deepseek_v3.py
2	0	vllm/model_executor/models/deepseek_vl2.py
2	0	vllm/model_executor/models/eagle.py
2	0	vllm/model_executor/models/exaone.py
2	0	vllm/model_executor/models/fairseq2_llama.py
2	0	vllm/model_executor/models/falcon.py
2	0	vllm/model_executor/models/florence2.py
2	0	vllm/model_executor/models/fuyu.py
2	0	vllm/model_executor/models/gemma.py
2	0	vllm/model_executor/models/gemma2.py
1	0	vllm/model_executor/models/glm.py
2	0	vllm/model_executor/models/glm4_vision_encoder.py
2	0	vllm/model_executor/models/gpt2.py
2	0	vllm/model_executor/models/gpt_bigcode.py
2	0	vllm/model_executor/models/gpt_j.py
2	0	vllm/model_executor/models/gpt_neox.py
2	0	vllm/model_executor/models/granite.py
2	0	vllm/model_executor/models/granitemoe.py
2	0	vllm/model_executor/models/gritlm.py
2	0	vllm/model_executor/models/h2ovl.py
2	0	vllm/model_executor/models/idefics2_vision_model.py
2	0	vllm/model_executor/models/idefics3.py
2	0	vllm/model_executor/models/interfaces.py
2	0	vllm/model_executor/models/interfaces_base.py
2	0	vllm/model_executor/models/intern_vit.py
2	0	vllm/model_executor/models/internlm2.py
2	0	vllm/model_executor/models/internlm2_ve.py
2	0	vllm/model_executor/models/internvl.py
2	0	vllm/model_executor/models/jais.py
1	0	vllm/model_executor/models/jamba.py
2	0	vllm/model_executor/models/llama.py
2	0	vllm/model_executor/models/llava.py
2	0	vllm/model_executor/models/llava_next.py
2	0	vllm/model_executor/models/llava_next_video.py
2	0	vllm/model_executor/models/llava_onevision.py
1	0	vllm/model_executor/models/mamba.py
2	0	vllm/model_executor/models/mamba_cache.py
2	0	vllm/model_executor/models/medusa.py
2	0	vllm/model_executor/models/minicpm.py
2	0	vllm/model_executor/models/minicpm3.py
2	0	vllm/model_executor/models/minicpmo.py
2	0	vllm/model_executor/models/minicpmv.py
2	0	vllm/model_executor/models/mixtral.py
2	0	vllm/model_executor/models/mixtral_quant.py
2	0	vllm/model_executor/models/mllama.py
2	0	vllm/model_executor/models/mlp_speculator.py
2	0	vllm/model_executor/models/module_mapping.py
2	0	vllm/model_executor/models/molmo.py
2	0	vllm/model_executor/models/mpt.py
2	0	vllm/model_executor/models/nemotron.py
2	0	vllm/model_executor/models/nvlm_d.py
2	0	vllm/model_executor/models/olmo.py
2	0	vllm/model_executor/models/olmo2.py
2	0	vllm/model_executor/models/olmoe.py
2	0	vllm/model_executor/models/opt.py
2	0	vllm/model_executor/models/orion.py
2	0	vllm/model_executor/models/paligemma.py
2	0	vllm/model_executor/models/persimmon.py
2	0	vllm/model_executor/models/phi.py
2	0	vllm/model_executor/models/phi3.py
2	0	vllm/model_executor/models/phi3_small.py
2	0	vllm/model_executor/models/phi3v.py
2	0	vllm/model_executor/models/phimoe.py
2	0	vllm/model_executor/models/pixtral.py
2	0	vllm/model_executor/models/qwen.py
2	0	vllm/model_executor/models/qwen2.py
2	0	vllm/model_executor/models/qwen2_audio.py
2	0	vllm/model_executor/models/qwen2_moe.py
2	0	vllm/model_executor/models/qwen2_rm.py
2	0	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/registry.py
2	0	vllm/model_executor/models/roberta.py
1	0	vllm/model_executor/models/siglip.py
2	0	vllm/model_executor/models/solar.py
2	0	vllm/model_executor/models/stablelm.py
2	0	vllm/model_executor/models/starcoder2.py
2	0	vllm/model_executor/models/telechat2.py
2	0	vllm/model_executor/models/ultravox.py
2	0	vllm/model_executor/models/utils.py
2	0	vllm/model_executor/models/vision.py
2	0	vllm/model_executor/models/whisper.py
2	0	vllm/model_executor/parameter.py
2	0	vllm/model_executor/pooling_metadata.py
2	0	vllm/model_executor/sampling_metadata.py
1	0	vllm/model_executor/utils.py
2	0	vllm/multimodal/__init__.py
2	0	vllm/multimodal/audio.py
2	0	vllm/multimodal/base.py
2	0	vllm/multimodal/hasher.py
2	0	vllm/multimodal/image.py
2	0	vllm/multimodal/inputs.py
2	0	vllm/multimodal/parse.py
2	0	vllm/multimodal/processing.py
2	0	vllm/multimodal/profiling.py
2	0	vllm/multimodal/registry.py
2	0	vllm/multimodal/utils.py
2	0	vllm/multimodal/video.py
2	0	vllm/outputs.py
2	0	vllm/platforms/__init__.py
2	0	vllm/platforms/cpu.py
1	0	vllm/platforms/cuda.py
2	0	vllm/platforms/hpu.py
2	0	vllm/platforms/interface.py
2	0	vllm/platforms/neuron.py
2	0	vllm/platforms/openvino.py
2	0	vllm/platforms/rocm.py
2	0	vllm/platforms/tpu.py
2	0	vllm/platforms/xpu.py
2	0	vllm/plugins/__init__.py
2	0	vllm/pooling_params.py
2	0	vllm/profiler/__init__.py
2	0	vllm/profiler/layerwise_profile.py
2	0	vllm/profiler/utils.py
2	0	vllm/prompt_adapter/layers.py
2	0	vllm/prompt_adapter/models.py
2	0	vllm/prompt_adapter/request.py
2	0	vllm/prompt_adapter/utils.py
2	0	vllm/prompt_adapter/worker_manager.py
1	0	vllm/sampling_params.py
2	0	vllm/scalar_type.py
2	0	vllm/scripts.py
1	0	vllm/sequence.py
2	0	vllm/spec_decode/batch_expansion.py
2	0	vllm/spec_decode/draft_model_runner.py
2	0	vllm/spec_decode/interfaces.py
2	0	vllm/spec_decode/medusa_worker.py
2	0	vllm/spec_decode/metrics.py
2	0	vllm/spec_decode/mlp_speculator_worker.py
2	0	vllm/spec_decode/mqa_scorer.py
2	0	vllm/spec_decode/multi_step_worker.py
2	0	vllm/spec_decode/ngram_worker.py
2	0	vllm/spec_decode/proposer_worker_base.py
2	0	vllm/spec_decode/smaller_tp_proposer_worker.py
2	0	vllm/spec_decode/spec_decode_worker.py
2	0	vllm/spec_decode/target_model_runner.py
2	0	vllm/spec_decode/top1_proposer.py
2	0	vllm/spec_decode/util.py
2	0	vllm/tracing.py
2	0	vllm/transformers_utils/__init__.py
2	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
2	0	vllm/transformers_utils/configs/arctic.py
2	0	vllm/transformers_utils/configs/chatglm.py
2	0	vllm/transformers_utils/configs/cohere2.py
2	0	vllm/transformers_utils/configs/dbrx.py
2	0	vllm/transformers_utils/configs/deepseek_vl2.py
2	0	vllm/transformers_utils/configs/eagle.py
2	0	vllm/transformers_utils/configs/exaone.py
2	0	vllm/transformers_utils/configs/falcon.py
2	0	vllm/transformers_utils/configs/h2ovl.py
2	0	vllm/transformers_utils/configs/internvl.py
2	0	vllm/transformers_utils/configs/jais.py
2	0	vllm/transformers_utils/configs/medusa.py
2	0	vllm/transformers_utils/configs/mllama.py
2	0	vllm/transformers_utils/configs/mlp_speculator.py
2	0	vllm/transformers_utils/configs/mpt.py
2	0	vllm/transformers_utils/configs/nemotron.py
2	0	vllm/transformers_utils/configs/nvlm_d.py
2	0	vllm/transformers_utils/configs/olmo2.py
2	0	vllm/transformers_utils/configs/solar.py
2	0	vllm/transformers_utils/configs/telechat2.py
2	0	vllm/transformers_utils/configs/ultravox.py
2	0	vllm/transformers_utils/detokenizer.py
2	0	vllm/transformers_utils/detokenizer_utils.py
2	0	vllm/transformers_utils/processor.py
2	0	vllm/transformers_utils/processors/__init__.py
2	0	vllm/transformers_utils/processors/deepseek_vl2.py
2	0	vllm/transformers_utils/s3_utils.py
2	0	vllm/transformers_utils/tokenizer.py
2	0	vllm/transformers_utils/tokenizer_group/__init__.py
2	0	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
2	0	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
2	0	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
2	0	vllm/transformers_utils/tokenizers/__init__.py
2	0	vllm/transformers_utils/tokenizers/mistral.py
2	0	vllm/transformers_utils/utils.py
2	0	vllm/triton_utils/__init__.py
2	0	vllm/triton_utils/custom_cache_manager.py
2	0	vllm/triton_utils/importing.py
2	0	vllm/usage/usage_lib.py
2	0	vllm/utils.py
1	0	vllm/v1/attention/backends/flash_attn.py
2	0	vllm/v1/core/encoder_cache_manager.py
2	0	vllm/v1/core/kv_cache_manager.py
1	0	vllm/v1/core/kv_cache_utils.py
2	0	vllm/v1/core/scheduler.py
2	0	vllm/v1/engine/__init__.py
2	0	vllm/v1/engine/async_llm.py
2	0	vllm/v1/engine/core.py
2	0	vllm/v1/engine/core_client.py
2	0	vllm/v1/engine/detokenizer.py
2	0	vllm/v1/engine/llm_engine.py
2	0	vllm/v1/engine/mm_input_mapper.py
2	0	vllm/v1/engine/output_processor.py
2	0	vllm/v1/engine/processor.py
2	0	vllm/v1/executor/abstract.py
2	0	vllm/v1/executor/multiproc_executor.py
2	0	vllm/v1/kv_cache_interface.py
2	0	vllm/v1/metrics/loggers.py
2	0	vllm/v1/metrics/stats.py
2	0	vllm/v1/outputs.py
2	0	vllm/v1/request.py
2	0	vllm/v1/sample/metadata.py
2	0	vllm/v1/sample/ops/penalties.py
2	0	vllm/v1/sample/ops/topk_topp_sampler.py
1	0	vllm/v1/sample/sampler.py
2	0	vllm/v1/serial_utils.py
2	0	vllm/v1/stats/common.py
2	0	vllm/v1/utils.py
2	0	vllm/v1/worker/block_table.py
2	0	vllm/v1/worker/gpu_input_batch.py
2	0	vllm/v1/worker/gpu_model_runner.py
1	0	vllm/v1/worker/gpu_worker.py
2	0	vllm/version.py
1	0	vllm/worker/cache_engine.py
2	0	vllm/worker/cpu_enc_dec_model_runner.py
2	0	vllm/worker/cpu_model_runner.py
2	0	vllm/worker/cpu_pooling_model_runner.py
1	0	vllm/worker/cpu_worker.py
2	0	vllm/worker/enc_dec_model_runner.py
2	0	vllm/worker/hpu_model_runner.py
2	0	vllm/worker/hpu_worker.py
2	0	vllm/worker/model_runner.py
2	0	vllm/worker/model_runner_base.py
2	0	vllm/worker/multi_step_model_runner.py
2	0	vllm/worker/multi_step_tpu_worker.py
2	0	vllm/worker/multi_step_worker.py
2	0	vllm/worker/neuron_model_runner.py
1	0	vllm/worker/neuron_worker.py
2	0	vllm/worker/openvino_model_runner.py
1	0	vllm/worker/openvino_worker.py
2	0	vllm/worker/pooling_model_runner.py
2	0	vllm/worker/tpu_model_runner.py
2	0	vllm/worker/tpu_worker.py
1	0	vllm/worker/utils.py
1	0	vllm/worker/worker.py
2	0	vllm/worker/worker_base.py
2	0	vllm/worker/xpu_model_runner.py
1	0	vllm/worker/xpu_worker.py

[f256ebe4d] Kunshang Ji 2025-02-02 [Hardware][Intel GPU] add XPU bf16 support (#12392)
1	1	docs/source/getting_started/installation/gpu/xpu.inc.md
20	3	vllm/platforms/xpu.py

[f8ece6e17] Shawn Du 2025-02-02 [Core][v1] Unify allocating slots in prefill and decode in KV cache manager (#12608)
13	11	tests/v1/core/test_prefix_caching.py
64	104	vllm/v1/core/kv_cache_manager.py
1	1	vllm/v1/core/scheduler.py

[abfcdcdf2] Woosuk Kwon 2025-02-01 [V1][Minor] Avoid frequently creating ConstantList (#12653)
2	5	vllm/v1/request.py

[e497f3349] Russell Bryant 2025-02-02 [Core] Silence unnecessary deprecation warnings (#12620)
1	1	vllm/lora/request.py

[baaa2b24d] Jinzhen Lin 2025-02-02 [Bugfix] fix moe_wna16 get_quant_method (#12648)
12	15	vllm/model_executor/layers/quantization/moe_wna16.py

[b4e5c0330] Vicente Herrera 2025-02-01 doc: fixing minor typo in readme.md (#12643)
1	1	README.md

[3194039c0] Michael Goin 2025-02-01 Apply torch.compile to fused_moe/grouped_topk (#12637)
1	0	vllm/model_executor/layers/fused_moe/fused_moe.py
2	2	vllm/model_executor/models/deepseek_v3.py

[4f4d427ac] Simon Mo 2025-01-31 Disable chunked prefill and/or prefix caching when MLA is enabled  (#12642)
10	0	vllm/config.py

[1e3698393] Russell Bryant 2025-02-01 [CI/Build] Add label automation for structured-output, speculative-decoding, v1 (#12280)
37	0	.github/mergify.yml

[baeded256] Lucas Wilkinson 2025-02-01 [Attention] Deepseek v3 MLA support with FP8 compute (#12601)
184	36	vllm/attention/backends/mla/utils.py
7	11	vllm/attention/backends/triton_mla.py
2	2	vllm/attention/layer.py
33	6	vllm/config.py
11	1	vllm/envs.py
52	22	vllm/model_executor/layers/quantization/utils/fp8_utils.py
115	1	vllm/model_executor/layers/quantization/utils/quant_utils.py
21	3	vllm/model_executor/model_loader/loader.py
152	2	vllm/model_executor/models/deepseek_v3.py
3	1	vllm/worker/cache_engine.py

[3e1c76cf3] Rahul Tuli 2025-01-31 Fix: Respect `sparsity_config.ignore` in Cutlass Integration (#12517)
37	22	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
54	6	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[cfa134d24] Tyler Michael Smith 2025-02-01 [Bugfix/CI] Fixup benchmark_moe.py (#12562)
12	4	benchmarks/kernels/benchmark_moe.py

[35b7a0550] Kevin H. Luu 2025-01-31 [ci] Upgrade transformers to 4.48.2 in CI dependencies (#12599)
1	1	requirements-common.txt
1	1	requirements-test.in
3	2	requirements-test.txt

[1867c258b] Eldar Kurtic 2025-02-01 Fix target matching for fused layers with compressed-tensors (#12617)
40	1	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[cb3e73e4c] fade_away 2025-02-01 [BugFix] fix wrong output when using lora and num_scheduler_steps=8 (#11161)
4	0	vllm/worker/model_runner.py
0	3	vllm/worker/worker.py

[b1340f9d5] Robert Shaw 2025-01-31 [V1] Bugfix: Validate Model Input Length (#12600)
5	0	vllm/v1/engine/processor.py

[44bbca78d] Brian Dellabetta 2025-01-31 [Doc] int4 w4a16 example (#12585)
1	0	docs/source/features/quantization/index.md
166	0	docs/source/features/quantization/int4.md
2	2	docs/source/features/quantization/int8.md

[60808bd4c] Harry Mellor 2025-01-31 [Doc] Improve installation signposting (#12575)
4	0	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
17	16	docs/source/getting_started/installation/ai_accelerator/index.md
4	0	docs/source/getting_started/installation/ai_accelerator/neuron.inc.md
4	0	docs/source/getting_started/installation/ai_accelerator/openvino.inc.md
4	0	docs/source/getting_started/installation/ai_accelerator/tpu.inc.md
4	0	docs/source/getting_started/installation/cpu/apple.inc.md
4	0	docs/source/getting_started/installation/cpu/arm.inc.md
7	6	docs/source/getting_started/installation/cpu/index.md
8	4	docs/source/getting_started/installation/cpu/x86.inc.md
25	24	docs/source/getting_started/installation/gpu/index.md
11	9	docs/source/getting_started/installation/gpu/rocm.inc.md
4	0	docs/source/getting_started/installation/gpu/xpu.inc.md
15	0	docs/source/getting_started/installation/index.md

[fc542144c] Ryan Nguyen 2025-01-31 [Feature] Fix guided decoding blocking bitmask memcpy (#12563)
2	2	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[eb5741ad4] Tyler Michael Smith 2025-01-31 [Kernel][Quantization] Integrate block-quantized CUTLASS kernels for DeepSeekV3 (#12587)
1	0	csrc/ops.h
7	1	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
14	1	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
7	0	csrc/torch_bindings.cpp
5	0	vllm/_custom_ops.py
4	1	vllm/model_executor/layers/quantization/fp8.py
112	34	vllm/model_executor/layers/quantization/utils/fp8_utils.py
10	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[145c2ff64] Robert Shaw 2025-01-31 [Bugfix] Revert MoE Triton Config Default (#12629)
11	30	vllm/model_executor/layers/fused_moe/fused_moe.py

[415f19474] Kevin H. Luu 2025-01-31 [release] Add input step to ask for Release version (#12631)
7	2	.buildkite/release-pipeline.yaml

[89003c408] Chen Zhang 2025-02-01 [v1][Bugfix] Add extra_keys to block_hash for prefix caching (#12603)
33	1	tests/v1/core/test_kv_cache_utils.py
4	2	vllm/v1/core/kv_cache_utils.py

[60bcef000] Cody Yu 2025-01-31 [Docs][V1] Prefix caching design (#12598)
-	-	docs/source/assets/design/v1/prefix_caching/example-time-1.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-3.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-4.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-5.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-6.png
-	-	docs/source/assets/design/v1/prefix_caching/example-time-7.png
-	-	docs/source/assets/design/v1/prefix_caching/free.png
-	-	docs/source/assets/design/v1/prefix_caching/overview.png
228	0	docs/source/design/v1/prefix_caching.md
7	0	docs/source/index.md

[847f88323] Cody Yu 2025-01-31 [Git] Automatically sign-off commits (#12595)
13	0	.pre-commit-config.yaml
1	1	docs/source/contributing/overview.md
2	1	format.sh

[325f679f3] Robert Shaw 2025-01-31 [BugFix] Fix Torch.Compile For DeepSeek (#12594)
29	25	vllm/model_executor/layers/quantization/fp8.py

[e3f7ff65e] Harry Mellor 2025-01-31 Add favicon to docs (#12611)
-	-	docs/source/assets/logos/vllm-logo-only-light.ico
1	0	docs/source/conf.py

[7a8987dac] Roger Wang 2025-01-31 [Bugfix] Gracefully handle huggingface hub http error (#12571)
9	1	vllm/transformers_utils/config.py

[cabaf4eff] Lucas Wilkinson 2025-01-31 [Attention] MLA decode optimizations (#12528)
5	0	csrc/cache.h
95	0	csrc/cache_kernels.cu
9	0	csrc/torch_bindings.cpp
89	0	tests/kernels/test_triton_decode_attention.py
1	1	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
1	1	tests/weight_loading/models.txt
7	2	tests/weight_loading/run_model_weight_loading_test.sh
13	0	vllm/_custom_ops.py
16	0	vllm/attention/backends/abstract.py
0	0	vllm/attention/backends/mla/__init__.py
365	0	vllm/attention/backends/mla/utils.py
749	0	vllm/attention/backends/triton_mla.py
4	0	vllm/attention/backends/utils.py
15	4	vllm/attention/layer.py
667	0	vllm/attention/ops/triton_decode_attention.py
5	1	vllm/attention/selector.py
26	9	vllm/config.py
0	1	vllm/engine/arg_utils.py
14	0	vllm/envs.py
6	0	vllm/model_executor/model_loader/loader.py
152	2	vllm/model_executor/models/deepseek_v2.py
2	1	vllm/platforms/cpu.py
7	2	vllm/platforms/cuda.py
2	1	vllm/platforms/hpu.py
3	1	vllm/platforms/interface.py
2	1	vllm/platforms/openvino.py
2	1	vllm/platforms/rocm.py
2	1	vllm/platforms/tpu.py
2	1	vllm/platforms/xpu.py
2	1	vllm/worker/cache_engine.py
3	1	vllm/worker/model_runner.py

[a1fc18c03] Aleksandr Malyshev 2025-01-30 [ROCm][AMD][Model] llama 3.2 support upstreaming (#12421)
291	85	vllm/attention/backends/rocm_flash_attn.py
13	3	vllm/model_executor/models/mllama.py

[9798b2fb0] Lucas Wilkinson 2025-01-30 [Kernel] Update `cutlass_scaled_mm` to support 2d group (blockwise) scaling (#11868)
7	2	CMakeLists.txt
142	148	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
8	1	csrc/core/math.hpp
17	0	csrc/cutlass_extensions/common.hpp
123	0	csrc/cutlass_extensions/gemm/collective/collective_builder.hpp
183	0	csrc/cutlass_extensions/gemm/collective/fp8_accumulation.hpp
730	0	csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp
39	0	csrc/cutlass_extensions/gemm/dispatch_policy.hpp
1	1	csrc/cutlass_extensions/vllm_collective_builder.cuh
93	0	csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh
0	74	csrc/quantization/cutlass_w8a8/{scaled_mm_c3x.cuh => c3x/scaled_mm.cuh}
24	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_azp_sm90_int8.cu
24	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8.cu
168	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh
33	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp
24	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu
25	1	csrc/quantization/cutlass_w8a8/{scaled_mm_c3x_sm90_fp8_dispatch.cuh => c3x/scaled_mm_sm90_fp8_dispatch.cuh}
24	0	csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8.cu
24	1	csrc/quantization/cutlass_w8a8/{scaled_mm_c3x_sm90_int8_dispatch.cuh => c3x/scaled_mm_sm90_int8_dispatch.cuh}
47	57	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
0	3	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
4	0	csrc/quantization/machete/machete_mainloop.cuh
132	56	tests/kernels/test_cutlass.py
30	2	tests/kernels/utils.py
22	0	vllm/_custom_ops.py

[4078052f0] Michael Goin 2025-01-30 [V1][Log] Add max request concurrency log to V1 (#12569)
4	0	vllm/v1/core/kv_cache_utils.py

[bd2107e30] Nishidha 2025-01-31 [CPU][PPC] Updated torch, torchvision, torchaudio dependencies (#12555)
2	3	Dockerfile.ppc64le
9	3	requirements-cpu.txt

[9b0c4bab3] Robert Shaw 2025-01-30 [Kernel] Triton Configs for Fp8 Block Quantization (#11589)
5	1	setup.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
66	25	vllm/model_executor/layers/fused_moe/fused_moe.py
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
146	0	vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json
61	16	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[41bf5612f] Beim 2025-01-31 [Misc] fix typo: add missing space in lora adapter error message (#12564)
1	1	vllm/entrypoints/openai/serving_models.py

[a2769032c] Harry Mellor 2025-01-30 Set `?device={device}` when changing tab in installation guides (#12560)
21	1	docs/source/_static/custom.js

[f17f1d460] Mark McLoughlin 2025-01-30 [V1][Metrics] Add GPU cache usage % gauge (#12561)
1	0	tests/entrypoints/openai/test_metrics.py
5	0	vllm/v1/core/kv_cache_manager.py
1	0	vllm/v1/core/scheduler.py
10	1	vllm/v1/metrics/loggers.py
1	1	vllm/v1/metrics/stats.py

[1c1bb0bbf] Divakar Verma 2025-01-29 [Misc][MoE] add Deepseek-V3 moe tuning support (#12558)
8	1	benchmarks/kernels/benchmark_moe.py

[e0cc5f259] Woosuk Kwon 2025-01-29 [V1][BugFix] Free encoder cache for aborted requests (#12545)
8	1	vllm/v1/core/encoder_cache_manager.py
8	6	vllm/v1/core/scheduler.py

[73aa6cfdf] Tyler Michael Smith 2025-01-29 Revert "[Build/CI] Fix libcuda.so linkage" (#12552)
0	4	CMakeLists.txt

[27b78c73c] Jinzhen Lin 2025-01-29 [Kernel] add triton fused moe kernel for gptq/awq (#12185)
91	0	tests/kernels/test_moe.py
354	53	vllm/model_executor/layers/fused_moe/fused_moe.py
5	2	vllm/model_executor/layers/quantization/__init__.py
424	0	vllm/model_executor/layers/quantization/moe_wna16.py

[b02fd288b] Pavani Majety 2025-01-29 [Hardware][NV] Fix Modelopt model loading for k-v-scales for Llama models. (#11787)
10	1	vllm/model_executor/model_loader/weight_utils.py
5	4	vllm/model_executor/models/llama.py
5	1	vllm/model_executor/models/mixtral.py

[ff7424f49] Yanyi Liu 2025-01-29 [Frontend] Support override generation config in args (#12409)
70	0	tests/test_config.py
11	2	vllm/config.py
19	6	vllm/engine/arg_utils.py

[d93bf4da8] Alphi 2025-01-29 [Model] Refactoring of MiniCPM-V and add MiniCPM-o-2.6 support for vLLM (#12069)
8	1	docs/source/models/supported_models.md
31	1	examples/offline_inference/audio_language.py
28	5	examples/offline_inference/vision_language.py
1	0	requirements-cpu.txt
1	0	requirements-cuda.txt
3	0	requirements-test.in
35	2	requirements-test.txt
14	0	tests/models/decoder_only/vision_language/test_models.py
11	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
2	0	tests/models/multimodal/processing/test_common.py
3	1	tests/models/registry.py
5	1	vllm/entrypoints/chat_utils.py
811	0	vllm/model_executor/models/minicpmo.py
668	175	vllm/model_executor/models/minicpmv.py
1	0	vllm/model_executor/models/registry.py

[036ca94c2] Travis Johnson 2025-01-29 [Bugfix] handle alignment of arguments in convert_sparse_cross_attention_mask_to_dense (#12347)
208	0	tests/models/encoder_decoder/vision_language/test_mllama.py
14	4	vllm/model_executor/models/mllama.py

[ef001d98e] Maximilien de Bayser 2025-01-29 Fix the pydantic logging validator (#12420)
22	7	vllm/entrypoints/openai/protocol.py

[5f671cb4c] Robert Shaw 2025-01-28 [V1] Improve Error Message for Unsupported Config (#12535)
7	2	vllm/platforms/cuda.py

[bd02164cf] Michael Goin 2025-01-28 Bugfix for whisper quantization due to fake k_proj bias (#12524)
1	1	vllm/model_executor/models/whisper.py

[46fb05674] Mark McLoughlin 2025-01-29 [V1][Metrics] Add TTFT and TPOT histograms (#12530)
6	0	tests/entrypoints/openai/test_metrics.py
3	1	vllm/v1/engine/output_processor.py
25	0	vllm/v1/metrics/loggers.py
11	0	vllm/v1/metrics/stats.py

[dd6a3a02c] Harry Mellor 2025-01-29 [Doc] Convert docs to use colon fences (#12471)
2	2	docs/requirements-docs.txt
2	2	docs/source/api/engine/index.md
2	2	docs/source/api/model/index.md
2	2	docs/source/api/multimodal/index.md
2	2	docs/source/api/offline_inference/index.md
2	2	docs/source/contributing/dockerfile/dockerfile.md
4	4	docs/source/contributing/model/basic.md
6	6	docs/source/contributing/model/index.md
16	16	docs/source/contributing/model/multimodal.md
8	8	docs/source/contributing/model/registration.md
4	4	docs/source/contributing/model/tests.md
6	6	docs/source/contributing/overview.md
6	6	docs/source/contributing/profiling/profiling_index.md
8	8	docs/source/deployment/docker.md
2	2	docs/source/deployment/frameworks/cerebrium.md
4	4	docs/source/deployment/frameworks/dstack.md
204	204	docs/source/deployment/frameworks/helm.md
2	2	docs/source/deployment/frameworks/index.md
18	18	docs/source/deployment/frameworks/skypilot.md
2	2	docs/source/deployment/integrations/index.md
2	2	docs/source/deployment/nginx.md
10	10	docs/source/design/arch_overview.md
16	16	docs/source/design/kernel/paged_attention.md
2	2	docs/source/design/multiprocessing.md
2	2	docs/source/features/automatic_prefix_caching.md
439	440	docs/source/features/compatibility_matrix.md
10	10	docs/source/features/disagg_prefill.md
2	2	docs/source/features/lora.md
2	2	docs/source/features/quantization/auto_awq.md
8	8	docs/source/features/quantization/fp8.md
6	6	docs/source/features/quantization/gguf.md
2	2	docs/source/features/quantization/index.md
4	4	docs/source/features/quantization/int8.md
115	114	docs/source/features/quantization/supported_hardware.md
4	4	docs/source/features/spec_decode.md
2	2	docs/source/features/structured_outputs.md
2	2	docs/source/generate_examples.py
38	38	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md
136	136	docs/source/getting_started/installation/ai_accelerator/index.md
2	2	docs/source/getting_started/installation/ai_accelerator/neuron.inc.md
26	25	docs/source/getting_started/installation/ai_accelerator/tpu.inc.md
2	2	docs/source/getting_started/installation/cpu/apple.inc.md
43	43	docs/source/getting_started/installation/cpu/index.md
2	2	docs/source/getting_started/installation/cpu/x86.inc.md
6	6	docs/source/getting_started/installation/gpu/cuda.inc.md
103	103	docs/source/getting_started/installation/gpu/index.md
13	12	docs/source/getting_started/installation/gpu/rocm.inc.md
2	2	docs/source/getting_started/installation/gpu/xpu.inc.md
2	2	docs/source/getting_started/installation/index.md
2	2	docs/source/getting_started/installation/python_env_setup.inc.md
6	6	docs/source/getting_started/quickstart.md
6	6	docs/source/getting_started/troubleshooting.md
24	24	docs/source/index.md
2	2	docs/source/models/extensions/index.md
2	2	docs/source/models/extensions/runai_model_streamer.md
2	2	docs/source/models/extensions/tensorizer.md
2	2	docs/source/models/generative_models.md
30	30	docs/source/models/pooling_models.md
633	630	docs/source/models/supported_models.md
6	6	docs/source/serving/distributed_serving.md
2	0	docs/source/serving/engine_args.md
4	4	docs/source/serving/env_vars.md
2	2	docs/source/serving/integrations/index.md
2	2	docs/source/serving/metrics.md
28	25	docs/source/serving/multimodal_inputs.md
4	4	docs/source/serving/offline_inference.md
28	28	docs/source/serving/openai_compatible_server.md
1	0	pyproject.toml

[a7e3eba66] Ce Gao 2025-01-29 [Frontend] Support reasoning content for deepseek r1 (#12473)
151	0	docs/source/features/reasoning_outputs.md
1	0	docs/source/index.md
53	0	examples/online_serving/openai_chat_completion_with_reasoning.py
90	0	examples/online_serving/openai_chat_completion_with_reasoning_streaming.py
0	0	tests/entrypoints/openai/reasoning_parsers/__init__.py
120	0	tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py
93	0	tests/entrypoints/openai/reasoning_parsers/utils.py
29	0	tests/entrypoints/openai/test_cli_args.py
10	0	vllm/entrypoints/openai/api_server.py
30	0	vllm/entrypoints/openai/cli_args.py
2	0	vllm/entrypoints/openai/protocol.py
6	0	vllm/entrypoints/openai/reasoning_parsers/__init__.py
158	0	vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py
133	0	vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py
100	5	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/scripts.py

[fbb5bd4ce] Michael Goin 2025-01-28 [TPU] Add example for profiling TPU inference (#12531)
67	0	examples/offline_inference/profiling_tpu/README.md
101	0	examples/offline_inference/profiling_tpu/profiling.py

[80fcc3ed1] fenghuizhang 2025-01-28 [Kernel] Pipe attn_logits_soft_cap through paged attention TPU kernels (#12482)
0	0	.buildkite/run-tpu-test.sh
16	26	vllm/attention/backends/pallas.py

[c386c43ca] Mark McLoughlin 2025-01-28 [V1][Metrics] Add per-request prompt/generation_tokens histograms (#12516)
6	0	tests/entrypoints/openai/test_metrics.py
1	2	vllm/v1/engine/async_llm.py
9	2	vllm/v1/engine/output_processor.py
54	6	vllm/v1/metrics/loggers.py
32	4	vllm/v1/metrics/stats.py

[f26d79071] Harry Mellor 2025-01-28 Do not run `suggestion` `pre-commit` hook multiple times (#12521)
1	0	.pre-commit-config.yaml

[0f657bdc5] Michael Goin 2025-01-28 Replace missed warning_once for rerank API (#12472)
1	1	vllm/entrypoints/openai/api_server.py

[3fd1fb63e] Mark McLoughlin 2025-01-28 [V1][Metrics] Hook up IterationStats for Prometheus metrics (#12478)
5	2	tests/entrypoints/openai/test_metrics.py
2	1	vllm/v1/engine/async_llm.py
59	9	vllm/v1/metrics/loggers.py

[925d2f190] Jun Duan 2025-01-28 [Doc] Fix typo for x86 CPU installation (#12514)
1	1	docs/source/getting_started/installation/cpu/x86.inc.md

[8f58a5135] Cyrus Leung 2025-01-29 [VLM] Merged multi-modal processor and V1 support for Qwen-VL (#12504)
1	1	docs/source/models/supported_models.md
32	32	tests/models/multimodal/processing/test_common.py
0	144	tests/models/multimodal/processing/test_qwen.py
354	300	vllm/model_executor/models/qwen.py

[2079e43be] Sebastian Schoennenbeck 2025-01-28 [Core] Make raw_request optional in ServingCompletion (#12503)
2	2	vllm/entrypoints/openai/serving_completion.py

[e29d4358e] Robert Shaw 2025-01-28 [V1] Include Engine Version in Logs (#12496)
1	1	vllm/engine/llm_engine.py
1	1	vllm/v1/engine/core.py

[8cbc42497] Roger Wang 2025-01-28 Update README.md with V1 alpha release (#12495)
1	0	README.md

[dd66fd2b0] Mengqing Cao 2025-01-28 [CI] fix pre-commit error (#12494)
25	12	vllm/attention/ops/nki_flash_attn.py
4	4	vllm/spec_decode/spec_decode_worker.py

[0f465ab53] Gabriel Marinho 2025-01-28 [FEATURE] Enables offline /score for embedding models (#12021)
100	0	tests/models/embedding/language/test_scoring.py
116	44	vllm/entrypoints/llm.py

[23a7cbc88] Hossein Sarshar 2025-01-27 [CI/Build] Fixed the xla nightly issue report in #12451 (#12453)
8	11	requirements-tpu.txt

[426a5c362] Michael Goin 2025-01-27 Fix bad path in prometheus example (#12481)
1	1	examples/online_serving/prometheus_grafana/README.md

[ddee88d0f] Liangfu Chen 2025-01-27 [Neuron][Kernel] NKI-based flash-attention kernel with paged KV cache (#11277)
1	1	.buildkite/run-neuron-test.sh
456	0	tests/neuron/test_prefix_prefill.py
669	0	vllm/attention/ops/nki_flash_attn.py

[823ab7963] Harry Mellor 2025-01-28 Update `pre-commit` hooks (#12475)
5	5	.pre-commit-config.yaml
2	2	benchmarks/benchmark_serving.py
6	2	csrc/custom_all_reduce.cuh
4	4	csrc/moe/marlin_kernels/marlin_moe_kernel.h
8	8	csrc/quantization/gptq_marlin/gptq_marlin.cu
2	2	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
2	2	csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu
2	2	csrc/quantization/marlin/sparse/common/mma.h
3	1	csrc/rocm/attention.cu
1	1	setup.py
14	11	tests/kernels/test_block_fp8.py
5	5	tests/kv_transfer/test_lookup_buffer.py
3	3	tests/lora/test_qwen2vl.py
70	60	tests/models/decoder_only/vision_language/test_models.py
8	9	tests/models/decoder_only/vision_language/test_pixtral.py
3	3	tests/quantization/test_compressed_tensors.py
8	7	tests/samplers/test_rejection_sampler.py
3	2	tools/report_build_time_ninja.py
2	2	vllm/_custom_ops.py
14	14	vllm/attention/ops/prefix_prefill.py
2	2	vllm/attention/ops/triton_flash_attention.py
2	2	vllm/attention/selector.py
4	3	vllm/config.py
4	3	vllm/core/block/common.py
2	2	vllm/core/block_manager.py
11	12	vllm/core/scheduler.py
4	4	vllm/distributed/device_communicators/shm_broadcast.py
4	4	vllm/distributed/parallel_state.py
2	2	vllm/entrypoints/chat_utils.py
4	5	vllm/entrypoints/openai/serving_completion.py
2	2	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
8	4	vllm/lora/layers.py
3	2	vllm/lora/models.py
2	3	vllm/lora/ops/triton_ops/sgmv_expand.py
2	2	vllm/lora/ops/triton_ops/sgmv_shrink.py
6	6	vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py
7	7	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
4	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py
2	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
4	3	vllm/model_executor/layers/sampler.py
8	8	vllm/model_executor/layers/vocab_parallel_embedding.py
3	2	vllm/model_executor/model_loader/loader.py
2	2	vllm/model_executor/model_loader/tensorizer.py
2	2	vllm/model_executor/models/gemma.py
3	3	vllm/model_executor/models/granitemoe.py
2	2	vllm/model_executor/models/mllama.py
2	2	vllm/model_executor/models/mlp_speculator.py
4	4	vllm/model_executor/models/phimoe.py
2	1	vllm/model_executor/models/registry.py
4	4	vllm/model_executor/models/ultravox.py
2	3	vllm/model_executor/models/utils.py
8	3	vllm/model_executor/sampling_metadata.py
2	2	vllm/platforms/neuron.py
2	2	vllm/scalar_type.py
2	2	vllm/spec_decode/spec_decode_worker.py
5	5	vllm/spec_decode/top1_proposer.py
7	5	vllm/spec_decode/util.py
2	2	vllm/transformers_utils/configs/nemotron.py
5	5	vllm/utils.py
2	2	vllm/v1/core/scheduler.py
2	2	vllm/v1/stats/common.py
1	1	vllm/v1/worker/gpu_model_runner.py
4	4	vllm/worker/hpu_worker.py
2	2	vllm/worker/tpu_model_runner.py

[6116ca8cd] Nicolò Lucchesi 2025-01-27 [Feature] [Spec decode]: Enable MLPSpeculator/Medusa and `prompt_logprobs` with ChunkedPrefill (#10132)
16	3	tests/spec_decode/e2e/conftest.py
9	1	tests/spec_decode/e2e/test_integration_dist_tp2.py
10	6	tests/spec_decode/e2e/test_logprobs.py
24	7	tests/spec_decode/e2e/test_medusa_correctness.py
42	11	tests/spec_decode/e2e/test_mlp_correctness.py
17	14	tests/spec_decode/e2e/test_multistep_correctness.py
4	9	tests/spec_decode/e2e/test_ngram_correctness.py
1	0	tests/spec_decode/test_scorer.py
1	0	tests/spec_decode/test_spec_decode_worker.py
12	0	tests/spec_decode/utils.py
2	7	vllm/config.py
17	2	vllm/engine/llm_engine.py
95	38	vllm/spec_decode/batch_expansion.py
6	2	vllm/spec_decode/interfaces.py
56	12	vllm/spec_decode/mqa_scorer.py
157	54	vllm/spec_decode/spec_decode_worker.py

[2bc3fbba0] Bowen Wang 2025-01-28 [FlashInfer] Upgrade to 0.2.0 (#11194)
10	1	.buildkite/test-pipeline.yaml
21	2	Dockerfile
3	2	tests/basic_correctness/test_basic_correctness.py
1	1	tests/compile/test_basic_correctness.py
37	37	tests/kernels/test_flashinfer.py
162	21	vllm/attention/backends/flashinfer.py
6	4	vllm/config.py
2	2	vllm/model_executor/model_loader/loader.py
2	1	vllm/model_executor/model_loader/tensorizer.py
13	4	vllm/worker/worker_base.py

[3f1fc7425] Woosuk Kwon 2025-01-27 [V1][CI/Test] Do basic test for top-p & top-k sampling (#12469)
20	8	tests/v1/engine/test_engine_core.py

[01ba92704] Mark McLoughlin 2025-01-27 [V1][Metrics] Add initial Prometheus logger (#12416)
35	6	tests/entrypoints/openai/test_metrics.py
7	4	vllm/v1/engine/async_llm.py
36	0	vllm/v1/metrics/loggers.py

[103bd17ac] Lucas Wilkinson 2025-01-27 [Build] Only build 9.0a for scaled_mm and sparse kernels (#12339)
4	4	CMakeLists.txt
28	15	cmake/utils.cmake

[ce69f7f75] Isotr0py 2025-01-27 [Bugfix] Fix gpt2 GGUF inference (#12467)
8	11	vllm/model_executor/models/gpt2.py

[624a1e471] Woosuk Kwon 2025-01-27 [V1][Minor] Minor optimizations for update_from_output (#12454)
13	7	vllm/v1/core/scheduler.py

[372bf0890] Isotr0py 2025-01-27 [Bugfix] Fix missing seq_start_loc in xformers prefill metadata (#12464)
3	0	vllm/attention/backends/xformers.py

[5204ff5c3] Cyrus Leung 2025-01-27 [Bugfix] Fix Granite 3.0 MoE model loading (#12446)
1	0	vllm/model_executor/models/granitemoe.py

[0cc6b383d] Pooya Davoodi 2025-01-26 [Frontend] Support scores endpoint in run_batch (#12430)
32	1	examples/offline_inference/openai/openai_batch.md
37	0	tests/entrypoints/openai/test_run_batch.py
3	2	vllm/entrypoints/openai/protocol.py
27	4	vllm/entrypoints/openai/run_batch.py

[28e075084] Woosuk Kwon 2025-01-26 [V1] Avoid list creation in input preparation (#12457)
12	5	vllm/v1/worker/gpu_model_runner.py

[582cf7879] Yuan Tang 2025-01-26 [DOC] Add link to vLLM blog (#12460)
3	0	docs/source/community/blog.md
1	0	docs/source/index.md

[0034b09ce] Kyle Mistele 2025-01-26 [Frontend] Rerank API (Jina- and Cohere-compatible API)  (#12376)
92	0	docs/source/serving/openai_compatible_server.md
32	0	examples/online_serving/cohere_rerank_client.py
33	0	examples/online_serving/jinaai_rerank_client.py
87	0	tests/entrypoints/openai/test_rerank.py
1	6	tests/entrypoints/openai/test_score.py
50	1	vllm/entrypoints/openai/api_server.py
46	0	vllm/entrypoints/openai/protocol.py
5	4	vllm/entrypoints/openai/serving_engine.py
206	0	vllm/entrypoints/openai/serving_rerank.py

[72bac7306] Tyler Michael Smith 2025-01-26 [Build/CI] Fix libcuda.so linkage (#12424)
4	0	CMakeLists.txt

[68f11149d] Lucas Wilkinson 2025-01-26 [Bugfix][Kernel] Fix perf regression caused by PR #12405 (#12434)
1	1	CMakeLists.txt

[72f488042] Tyler Michael Smith 2025-01-26 [Bugfix/CI] Fix broken kernels/test_mha.py (#12450)
2	2	tests/kernels/test_mha_attn.py
8	0	vllm/attention/layer.py

[aa2cd2c43] Tyler Michael Smith 2025-01-26 [Bugfix] Disable w16a16 2of4 sparse CompressedTensors24 (#12417)
3	25	tests/kernels/test_cutlass.py
214	0	tests/kernels/test_cutlass_2of4_sparse.py
0	134	tests/kernels/test_semi_structured.py
26	1	tests/kernels/utils.py
3	1	tests/quantization/test_compressed_tensors.py
17	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[9ddc35220] Matthew Hendrey 2025-01-26 [Frontend] generation_config.json for  maximum tokens(#12242)
110	0	tests/entrypoints/openai/test_serving_chat.py
6	0	vllm/config.py
3	1	vllm/engine/arg_utils.py
26	8	vllm/entrypoints/openai/protocol.py

[a5255270c] Roger Wang 2025-01-26 [Misc] Revert FA on ViT #12355 and #12435 (#12445)
4	37	vllm/attention/layer.py

[0ee349b55] Roger Wang 2025-01-26 [V1][Bugfix] Fix assertion when mm hashing is turned off (#12439)
2	1	vllm/v1/request.py

[fa63e710c] Keyun Tong 2025-01-26 [V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)
1	1	vllm/v1/outputs.py
1	2	vllm/v1/sample/sampler.py
19	10	vllm/v1/worker/gpu_model_runner.py

[2a0309a64] Roger Wang 2025-01-25 [Misc][Bugfix] FA3 support to ViT MHA layer (#12435)
22	3	vllm/attention/layer.py

[324960a95] Siyuan Liu 2025-01-24 [TPU][CI] Update torchxla version in requirement-tpu.txt (#12422)
1	1	Dockerfile.tpu
11	10	requirements-tpu.txt

[f1fc0510d] Isotr0py 2025-01-25 [Misc] Add FA2 support to ViT MHA layer (#12355)
126	0	tests/kernels/test_mha_attn.py
20	5	vllm/attention/layer.py

[bf21481dd] Divakar Verma 2025-01-24 [ROCm][MoE] MI300 tuned configs Mixtral-8x(7B,22B) | fp16, fp8 (#12408)
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
32	32	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
43	43	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
35	35	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
38	38	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json
164	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json
200	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X.json

[fb30ee92e] Cyrus Leung 2025-01-25 [Bugfix] Fix BLIP-2 processing (#12412)
2	2	vllm/model_executor/models/blip2.py

[221d388cc] ElizaWszola 2025-01-24 [Bugfix][Kernel] Fix moe align block issue for mixtral (#12413)
3	1	csrc/moe/moe_align_sum_kernels.cu

[3132a933b] Lucas Wilkinson 2025-01-24 [Bugfix][Kernel] FA3 Fix - RuntimeError: This flash attention build only supports pack_gqa (for build size reasons). (#12405)
1	1	CMakeLists.txt

[df5dafaa5] Cyrus Leung 2025-01-25 [Misc] Remove deprecated code (#12383)
14	9	tests/async_engine/test_api_server.py
9	9	tests/basic_correctness/test_preemption.py
2	1	tests/multi_step/test_correctness_async_llm.py
0	10	vllm/config.py
0	6	vllm/engine/arg_utils.py
0	43	vllm/engine/metrics.py

[ab5bbf5ae] Lucas Wilkinson 2025-01-24 [Bugfix][Kernel] Fix CUDA 11.8 being broken by FA3 build (#12375)
1	1	CMakeLists.txt
4	1	setup.py
6	5	tests/kernels/test_cascade_flash_attn.py
10	11	tests/kernels/test_flash_attn.py
11	3	vllm/attention/backends/flash_attn.py
10	1	vllm/v1/attention/backends/flash_attn.py

[3bb8e2c9a] Junichi Sato 2025-01-24 [Misc] Enable proxy support in benchmark script (#12356)
10	5	benchmarks/backend_request_func.py

[e784c6b99] youkaichao 2025-01-24 [ci/build] sync default value for wheel size (#12398)
1	0	.buildkite/check-wheel-size.py
2	2	Dockerfile

[9a0f3bdbe] Mohit Deopujari 2025-01-24 [Hardware][Gaudi][Doc] Add missing step in setup instructions (#12382)
2	0	docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md

[c7c985103] youkaichao 2025-01-24 [ci/build] fix wheel size check (#12396)
4	2	.buildkite/check-wheel-size.py

[3c818bdb4] Roger Wang 2025-01-24 [Misc] Use VisionArena Dataset for VLM Benchmarking (#12389)
12	20	benchmarks/benchmark_serving.py

[6dd94dbe9] youkaichao 2025-01-24 [perf] fix perf regression from #12253 (#12380)
4	1	vllm/worker/model_runner.py

[0e74d797c] Woosuk Kwon 2025-01-23 [V1] Increase default batch size for H100/H200 (#12369)
16	5	vllm/engine/arg_utils.py

[55ef66edf] Dipika Sikka 2025-01-23 Update compressed-tensors version (#12367)
1	1	requirements-common.txt

[5e5630a47] omer-dayan 2025-01-24 [Bugfix] Path join when building local path for S3 clone (#12353)
2	1	vllm/transformers_utils/s3_utils.py

[d3d6bb13f] Russell Bryant 2025-01-23 Set weights_only=True when using torch.load() (#12366)
1	1	vllm/assets/image.py
2	1	vllm/lora/models.py
5	3	vllm/model_executor/model_loader/weight_utils.py
2	1	vllm/prompt_adapter/utils.py

[24b0205f5] Nick Hill 2025-01-23 [V1][Frontend] Coalesce bunched `RequestOutput`s (#12298)
35	16	tests/v1/engine/test_async_llm.py
21	1	vllm/outputs.py
9	1	vllm/v1/engine/async_llm.py

[c5cffcd0c] Russell Bryant 2025-01-23 [Docs] Update spec decode + structured output in compat matrix (#12373)
1	1	docs/source/features/compatibility_matrix.md

[682b55bc0] Woosuk Kwon 2025-01-23 [Docs] Add meetup slides (#12345)
1	4	README.md
1	0	docs/source/community/meetups.md

[9726ad676] Junichi Sato 2025-01-24 [Misc] Fix OpenAI API Compatibility Issues in Benchmark Script (#12357)
6	4	benchmarks/backend_request_func.py

[eb5cb5e52] Dipika Sikka 2025-01-23 [BugFix] Fix parameter names and `process_after_weight_loading` for W4A16 MoE Group Act Order  (#11528)
60	34	vllm/model_executor/layers/fused_moe/layer.py
19	16	vllm/model_executor/layers/quantization/awq_marlin.py
103	50	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
15	12	vllm/model_executor/layers/quantization/experts_int8.py
23	18	vllm/model_executor/layers/quantization/fp8.py
11	8	vllm/model_executor/layers/quantization/gptq_marlin.py
11	9	vllm/model_executor/layers/quantization/quark/quark_moe.py

[2cbeedad0] Isotr0py 2025-01-24 [Docs] Document Phi-4 support (#12362)
2	2	docs/source/models/supported_models.md

[2c85529bf] Siyuan Liu 2025-01-23 [TPU] Update TPU CI to use torchxla nightly on 20250122 (#12334)
1	1	Dockerfile.tpu

[e97f802b2] Gregory Shtrasberg 2025-01-23 [FP8][Kernel] Dynamic kv cache scaling factors computation (#11906)
3	1	benchmarks/kernels/benchmark_paged_attention.py
5	5	csrc/attention/attention_kernels.cuh
10	7	csrc/attention/paged_attention_v1.cu
10	7	csrc/attention/paged_attention_v2.cu
3	3	csrc/cache.h
17	13	csrc/cache_kernels.cu
6	6	csrc/cpu/attention.cpp
2	4	csrc/cpu/cache.cpp
3	3	csrc/cpu/torch_bindings.cpp
6	4	csrc/ops.h
10	7	csrc/rocm/attention.cu
2	2	csrc/rocm/ops.h
1	1	csrc/rocm/torch_bindings.cpp
4	4	csrc/torch_bindings.cpp
6	4	docs/source/features/quantization/quantized_kvcache.md
0	96	examples/other/fp8/README.md
0	367	examples/other/fp8/extract_scales.py
0	32	examples/other/fp8/quantizer/README.md
0	367	examples/other/fp8/quantizer/quantize.py
0	90	tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json
0	42	tests/fp8_kv/llama2-7b-fp8-kv/kv_cache_scales.json
1	1	tests/kernels/test_attention.py
1	1	tests/kernels/test_blocksparse_attention.py
5	5	tests/kernels/test_cache.py
10	0	tests/kernels/test_prefix_prefill.py
2	0	tests/kernels/utils.py
4	11	tests/models/decoder_only/language/test_fp8.py
3	0	tests/worker/test_model_input.py
10	10	vllm/_custom_ops.py
8	2	vllm/attention/backends/abstract.py
2	0	vllm/attention/backends/blocksparse_attn.py
4	1	vllm/attention/backends/flash_attn.py
6	4	vllm/attention/backends/flashinfer.py
1	1	vllm/attention/backends/ipex_attn.py
1	1	vllm/attention/backends/pallas.py
3	0	vllm/attention/backends/placeholder_attn.py
2	0	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/attention/backends/torch_sdpa.py
2	0	vllm/attention/backends/utils.py
2	0	vllm/attention/backends/xformers.py
26	2	vllm/attention/layer.py
8	8	vllm/attention/ops/ipex_attn.py
6	6	vllm/attention/ops/paged_attn.py
6	6	vllm/attention/ops/prefix_prefill.py
6	10	vllm/config.py
12	13	vllm/engine/arg_utils.py
9	0	vllm/envs.py
15	4	vllm/model_executor/layers/quantization/kv_cache.py
1	44	vllm/model_executor/model_loader/weight_utils.py
2	33	vllm/model_executor/models/exaone.py
2	30	vllm/model_executor/models/granite.py
2	33	vllm/model_executor/models/llama.py
4	3	vllm/model_executor/models/mllama.py
2	33	vllm/model_executor/models/solar.py
0	4	vllm/v1/attention/backends/flash_attn.py
5	2	vllm/worker/hpu_model_runner.py
6	31	vllm/worker/model_runner.py
1	0	vllm/worker/openvino_model_runner.py
5	0	vllm/worker/tpu_model_runner.py
2	0	vllm/worker/xpu_model_runner.py

[6e650f56a] youkaichao 2025-01-24 [torch.compile] decouple compile sizes and cudagraph sizes (#12243)
5	5	vllm/compilation/backends.py
37	34	vllm/config.py
2	1	vllm/engine/metrics.py
10	8	vllm/v1/worker/gpu_model_runner.py
12	0	vllm/v1/worker/gpu_worker.py
17	10	vllm/worker/model_runner.py
12	0	vllm/worker/worker.py

[3f50c148f] youkaichao 2025-01-24 [core] add wake_up doc and some sanity check (#12361)
3	0	vllm/entrypoints/llm.py
9	0	vllm/executor/executor_base.py

[8c01b8022] Isotr0py 2025-01-24 [Bugfix] Fix broken internvl2 inference with v1 (#12360)
7	2	vllm/multimodal/utils.py

[99d01a5e3] Roger Wang 2025-01-23 [V1] Simplify M-RoPE (#12352)
10	14	vllm/v1/worker/gpu_model_runner.py

[d07efb31c] Cyrus Leung 2025-01-23 [Doc] Troubleshooting errors during model inspection (#12351)
38	2	docs/source/getting_started/troubleshooting.md
2	31	docs/source/serving/offline_inference.md

[978b45f39] Lucas Wilkinson 2025-01-23 [Kernel] Flash Attention 3 Support (#12093)
20	25	CMakeLists.txt
8	4	setup.py
14	10	tests/kernels/test_cascade_flash_attn.py
18	4	tests/kernels/test_flash_attn.py
24	3	vllm/attention/backends/flash_attn.py
12	0	vllm/envs.py
33	11	vllm/v1/attention/backends/flash_attn.py
21	25	vllm/v1/worker/gpu_model_runner.py

[c5b4b11d7] Isotr0py 2025-01-23 [Bugfix] Fix k_proj's bias for whisper self attention (#12342)
18	3	vllm/model_executor/models/whisper.py

[8ae5ff200] liuzhenwei 2025-01-23 [Hardware][Gaudi][BugFix] Fix dataclass error due to triton package update (#12338)
1	1	requirements-hpu.txt

[511627445] youkaichao 2025-01-23 [doc] explain common errors around torch.compile (#12340)
21	0	docs/source/getting_started/troubleshooting.md

[f0ef37233] Cody Yu 2025-01-22 [V1] Add `uncache_blocks` (#12333)
30	0	tests/v1/core/test_prefix_caching.py
31	2	vllm/v1/core/kv_cache_manager.py

[7551a3403] Russell Bryant 2025-01-22 [Docs] Document vulnerability disclosure process (#12326)
17	0	docs/source/contributing/vulnerability_management.md

[01a55941f] Michael Goin 2025-01-22 [Docs] Update FP8 KV Cache documentation (#12238)
0	44	docs/source/features/quantization/fp8_e4m3_kvcache.md
0	31	docs/source/features/quantization/fp8_e5m2_kvcache.md
1	2	docs/source/features/quantization/index.md
145	0	docs/source/features/quantization/quantized_kvcache.md

[8d7aa9de7] Alexei-V-Ivanov-AMD 2025-01-22 [Bugfix] Fixing  AMD LoRA CI test. (#12329)
2	1	Dockerfile.rocm

[68c4421b6] rasmith 2025-01-22 [AMD][Quantization] Add TritonScaledMMLinearKernel since int8 is broken for AMD (#12282)
17	0	tests/kernels/test_triton_scaled_mm.py
3	5	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
38	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py

[aea94362c] Nick Hill 2025-01-22 [Frontend][V1] Online serving performance improvements (#12287)
6	0	vllm/entrypoints/openai/api_server.py
19	11	vllm/entrypoints/openai/protocol.py
11	0	vllm/envs.py
37	17	vllm/v1/engine/async_llm.py
18	3	vllm/v1/engine/core_client.py
4	2	vllm/v1/engine/output_processor.py
6	12	vllm/v1/request.py

[7206ce4ce] Cody Yu 2025-01-22 [Core] Support `reset_prefix_cache` (#12284)
38	0	tests/core/block/test_prefix_caching_block.py
39	0	tests/v1/core/test_prefix_caching.py
7	0	vllm/core/block/cpu_gpu_block_allocator.py
10	0	vllm/core/block/interfaces.py
14	5	vllm/core/block/naive_block.py
43	1	vllm/core/block/prefix_caching_block.py
3	0	vllm/core/block_manager.py
5	0	vllm/core/interfaces.py
3	0	vllm/core/placeholder_block_space_manager.py
3	0	vllm/core/scheduler.py
3	0	vllm/engine/async_llm_engine.py
8	0	vllm/engine/llm_engine.py
6	1	vllm/engine/multiprocessing/__init__.py
10	2	vllm/engine/multiprocessing/client.py
8	2	vllm/engine/multiprocessing/engine.py
5	0	vllm/engine/protocol.py
4	0	vllm/entrypoints/llm.py
12	0	vllm/entrypoints/openai/api_server.py
7	0	vllm/envs.py
0	5	vllm/executor/executor_base.py
27	0	vllm/v1/core/kv_cache_manager.py
3	0	vllm/v1/core/scheduler.py
8	1	vllm/v1/engine/__init__.py
3	0	vllm/v1/engine/async_llm.py
9	2	vllm/v1/engine/core.py
19	2	vllm/v1/engine/core_client.py
3	0	vllm/v1/engine/llm_engine.py

[96f6a7596] Konrad Zawora 2025-01-22 [Bugfix] Fix HPU multiprocessing executor (#12167)
1	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
18	0	vllm/platforms/hpu.py
2	2	vllm/worker/hpu_worker.py

[84bee4bd5] Jee Jee Li 2025-01-23 [Misc]  Improve the readability of BNB error messages  (#12320)
2	2	vllm/model_executor/model_loader/loader.py

[fc66dee76] Robin 2025-01-23 [Misc] Fix the error in the tip for the --lora-modules parameter (#12319)
1	1	vllm/entrypoints/openai/cli_args.py

[6609cdf01] Cyrus Leung 2025-01-22 [Doc] Add docs for prompt replacement (#12318)
1	1	vllm/model_executor/models/ultravox.py
78	14	vllm/multimodal/processing.py

[16366ee8b] Roger Wang 2025-01-22 [Bugfix][VLM] Fix mixed-modality inference backward compatibility for V0 (#12313)
44	9	vllm/model_executor/models/llava_onevision.py
48	19	vllm/model_executor/models/qwen2_vl.py

[528dbcac7] zhou fan 2025-01-22 [Model][Bugfix]: correct Aria model output (#12309)
2	1	examples/offline_inference/vision_language.py
52	2	vllm/model_executor/models/aria.py

[cd7b6f085] Cyrus Leung 2025-01-22 [VLM] Avoid unnecessary tokenization (#12310)
10	2	vllm/model_executor/models/blip2.py
14	9	vllm/model_executor/models/chameleon.py
4	2	vllm/model_executor/models/deepseek_vl2.py
4	1	vllm/model_executor/models/fuyu.py
9	8	vllm/model_executor/models/llava.py
11	9	vllm/model_executor/models/qwen2_audio.py
7	5	vllm/model_executor/models/qwen2_vl.py
7	3	vllm/model_executor/models/ultravox.py
5	1	vllm/transformers_utils/tokenizer.py

[68ad4e3a8] youkaichao 2025-01-22 [Core] Support fully transparent sleep mode (#11743)
2	0	.buildkite/test-pipeline.yaml
25	0	CMakeLists.txt
310	0	csrc/cumem_allocator.cpp
2	0	setup.py
112	0	tests/basic_correctness/test_cumem.py
43	35	vllm/config.py
0	0	vllm/device_allocator/__init__.py
254	0	vllm/device_allocator/cumem.py
10	1	vllm/engine/arg_utils.py
10	0	vllm/engine/llm_engine.py
23	0	vllm/entrypoints/llm.py
11	0	vllm/executor/executor_base.py
38	2	vllm/v1/worker/gpu_worker.py
37	2	vllm/worker/worker.py

[4004f144f] Mengqing Cao 2025-01-22 [Build] update requirements of no-device (#12299)
1	1	setup.py

[66818e5b6] youkaichao 2025-01-22 [core] separate builder init and builder prepare for each batch (#12253)
6	5	vllm/attention/backends/abstract.py
6	5	vllm/attention/backends/flash_attn.py
8	6	vllm/attention/backends/flashinfer.py
5	3	vllm/attention/backends/placeholder_attn.py
4	1	vllm/attention/backends/torch_sdpa.py
7	6	vllm/attention/backends/utils.py
17	7	vllm/worker/cpu_model_runner.py
24	12	vllm/worker/model_runner.py
5	0	vllm/worker/model_runner_base.py
8	2	vllm/worker/xpu_model_runner.py

[222a9dc35] Nick Hill 2025-01-21 [Benchmark] More accurate TPOT calc in `benchmark_serving.py` (#12288)
27	16	benchmarks/backend_request_func.py
39	30	benchmarks/benchmark_serving.py

[cbdc4ad5a] Cyrus Leung 2025-01-22 [Ci/Build] Fix mypy errors on main (#12296)
5	2	vllm/multimodal/processing.py

[016e3676e] Liangfu Chen 2025-01-21 [CI] add docker volume prune to neuron CI (#12291)
4	1	.buildkite/run-neuron-test.sh

[64ea24d0b] Kevin H. Luu 2025-01-21 [ci/lint] Add back default arg for pre-commit (#12279)
1	1	.github/workflows/pre-commit.yml
7	10	tests/models/decoder_only/language/test_gguf.py
1	1	vllm/model_executor/models/paligemma.py
3	5	vllm/model_executor/models/siglip.py
1	1	vllm/platforms/__init__.py
13	11	vllm/v1/stats/common.py

[df76e5af2] Cyrus Leung 2025-01-22 [VLM] Simplify post-processing of replacement info (#12269)
1	1	tests/models/multimodal/processing/test_common.py
2	1	tests/models/registry.py
23	19	tests/multimodal/test_processing.py
2	8	vllm/model_executor/models/aria.py
8	25	vllm/model_executor/models/blip2.py
13	29	vllm/model_executor/models/chameleon.py
11	27	vllm/model_executor/models/fuyu.py
17	28	vllm/model_executor/models/phi3v.py
14	29	vllm/model_executor/models/qwen2_audio.py
84	41	vllm/multimodal/processing.py

[09ccc9c8f] Hongxia Yang 2025-01-21 [Documentation][AMD] Add information about prebuilt ROCm vLLM docker for perf validation purpose (#12281)
8	0	docs/source/getting_started/installation/gpu/rocm.inc.md

[69196a9bc] Aleksandr Malyshev 2025-01-21 [BUGFIX] When skip_tokenize_init and multistep are set, execution crashes (#12277)
1	1	vllm/engine/output_processor/multi_step.py

[2acba47d9] Divakar Verma 2025-01-21 [bugfix] moe tuning. rm is_navi() (#12273)
2	2	benchmarks/kernels/benchmark_moe.py

[9c485d9e2] Jani Monoses 2025-01-21 [Core] Free CPU pinned memory on environment cleanup (#10477)
5	0	vllm/distributed/parallel_state.py

[fa9ee0812] wangxiyuan 2025-01-22 [Misc] Set default backend to SDPA for get_vit_attn_backend (#12235)
16	14	vllm/model_executor/models/vision.py

[347eeebe3] Adrian Cole 2025-01-21 [Misc] Remove experimental dep from tracing.py (#12007)
30	30	tests/tracing/test_tracing.py
15	17	vllm/engine/llm_engine.py
21	13	vllm/tracing.py

[18fd4a833] Andy Lo 2025-01-21 [Bugfix] Multi-sequence broken (#11898)
6	1	tests/samplers/test_seeded_generate.py
1	1	vllm/outputs.py
52	37	vllm/sequence.py

[132a13210] Ricky Xu 2025-01-21 [v1][stats][1/n] Add RequestStatsUpdate and RequestStats types  (#10907)
300	0	tests/v1/test_stats.py
0	0	vllm/v1/stats/__init__.py
449	0	vllm/v1/stats/common.py

[1e60f87bb] Jinzhen Lin 2025-01-22 [Kernel] fix moe_align_block_size error condition (#12239)
6	4	csrc/moe/moe_align_sum_kernels.cu

[9705b90bc] Jannis Schönleber 2025-01-21 [Bugfix] fix race condition that leads to wrong order of token returned (#10802)
11	3	vllm/engine/multiprocessing/client.py

[3aec49e56] youkaichao 2025-01-21 [ci/build] update nightly torch for gh200 test (#12270)
1	1	Dockerfile

[c64612802] Mengqing Cao 2025-01-21 [Platform] improve platforms getattr (#12264)
4	1	vllm/platforms/__init__.py

[9a7c3a004] Thomas Parnell 2025-01-21 Remove pytorch comments for outlines + compressed-tensors (#12260)
2	2	requirements-common.txt

[b197a5ccf] Roger Wang 2025-01-21 [V1][Bugfix] Fix data item ordering in mixed-modality inference (#12259)
33	1	vllm/multimodal/utils.py
29	13	vllm/v1/worker/gpu_model_runner.py

[c81081fec] youkaichao 2025-01-21 [torch.compile] transparent compilation with more logging (#12246)
25	7	vllm/compilation/backends.py
2	0	vllm/compilation/decorators.py
22	0	vllm/compilation/wrapper.py
1	0	vllm/config.py

[a94eee445] Cyrus Leung 2025-01-21 [Bugfix] Fix mm_limits access for merged multi-modal processor (#12252)
2	2	vllm/multimodal/profiling.py
14	5	vllm/multimodal/registry.py

[f2e9f2a3b] Cyrus Leung 2025-01-21 [Misc] Remove redundant TypeVar from base model (#12248)
3	7	vllm/model_executor/models/interfaces_base.py

[1f1542afa] Jee Jee Li 2025-01-21 [Misc]Add BNB quantization for PaliGemmaForConditionalGeneration  (#12237)
12	1	vllm/model_executor/models/paligemma.py
10	4	vllm/model_executor/models/siglip.py

[96912550c] Cyrus Leung 2025-01-21 [Misc] Rename `MultiModalInputsV2 -> MultiModalInputs` (#12244)
1	1	docs/source/api/multimodal/inputs.md
6	6	vllm/inputs/data.py
3	3	vllm/inputs/preprocess.py
2	2	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/chameleon.py
2	2	vllm/model_executor/models/fuyu.py
3	3	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/qwen2_audio.py
1	1	vllm/multimodal/inputs.py
5	5	vllm/multimodal/processing.py
2	2	vllm/multimodal/profiling.py

[2fc6944c5] youkaichao 2025-01-21 [ci/build] disable failed and flaky tests (#12240)
6	2	.buildkite/test-pipeline.yaml

[5fe6bf29d] Nicolò Lucchesi 2025-01-21 [BugFix] Fix GGUF tp>1 when vocab_size is not divisible by 64 (#12230)
10	0	tests/models/decoder_only/language/test_gguf.py
2	2	vllm/model_executor/layers/vocab_parallel_embedding.py

[d4b62d464] Gregory Shtrasberg 2025-01-20 [AMD][Build] Porting dockerfiles from the ROCm/vllm fork (#11777)
101	157	Dockerfile.rocm
158	0	Dockerfile.rocm_base
6	7	docs/source/getting_started/installation/gpu/rocm.inc.md
18	18	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json
18	18	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json
18	18	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json
18	18	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json

[ecf67814f] Michael Goin 2025-01-20 Add quantization and guided decoding CODEOWNERS (#12228)
2	0	.github/CODEOWNERS

[750f4cabf] Jinzhen Lin 2025-01-21 [Kernel] optimize moe_align_block_size for cuda graph and large num_experts (e.g. DeepSeek-V3) (#12222)
57	36	csrc/moe/moe_align_sum_kernels.cu
1	1	vllm/config.py

[06a760d6e] Cheng Kuan Yong Jason 2025-01-21 [bugfix] catch xgrammar unsupported array constraints (#12210)
7	0	vllm/model_executor/guided_decoding/utils.py

[da7512215] youkaichao 2025-01-21 [misc] add cuda runtime version to usage data (#12190)
3	0	vllm/usage/usage_lib.py

[af69a6ade] Işık 2025-01-20 fix: update platform detection for M-series arm based MacBook processors (#12227)
4	0	vllm/platforms/__init__.py

[7bd363006] Roger Wang 2025-01-20 [Misc] Update CODEOWNERS (#12229)
13	12	.github/CODEOWNERS

[96663699b] Chen Zhang 2025-01-20 [CI] Pass local python version explicitly to pre-commit mypy.sh (#12224)
1	1	.pre-commit-config.yaml
5	1	tools/mypy.sh

[18572e338] Cyrus Leung 2025-01-20 [Bugfix] Fix `HfExampleModels.find_hf_info` (#12223)
5	0	tests/models/registry.py

[86bfb6dba] wangxiyuan 2025-01-20 [Misc] Pass `attention` to impl backend (#12218)
19	4	vllm/attention/backends/abstract.py
6	6	vllm/attention/backends/blocksparse_attn.py
5	5	vllm/attention/backends/flash_attn.py
8	8	vllm/attention/backends/flashinfer.py
2	2	vllm/attention/backends/hpu_attn.py
9	9	vllm/attention/backends/ipex_attn.py
3	3	vllm/attention/backends/pallas.py
10	10	vllm/attention/backends/rocm_flash_attn.py
8	10	vllm/attention/backends/torch_sdpa.py
9	11	vllm/attention/backends/xformers.py
3	5	vllm/attention/layer.py
4	5	vllm/v1/attention/backends/flash_attn.py

[5f0ec3935] Chen Zhang 2025-01-20 [V1] Remove `_get_cache_block_size` (#12214)
1	23	vllm/v1/worker/gpu_worker.py

[c222f4799] youkaichao 2025-01-20 [core][bugfix] configure env var during import vllm (#12209)
1	6	examples/offline_inference/rlhf.py
13	36	vllm/__init__.py
23	0	vllm/plugins/__init__.py
0	3	vllm/worker/worker_base.py

[170eb3507] youkaichao 2025-01-20 [misc] print a message to suggest how to bypass commit hooks (#12217)
9	4	.pre-commit-config.yaml

[b37d82791] Cyrus Leung 2025-01-20 [Model] Upgrade Aria to transformers 4.48 (#12203)
0	3	examples/offline_inference/vision_language.py
2	5	tests/models/decoder_only/vision_language/test_models.py
5	7	tests/models/multimodal/processing/test_common.py
62	5	tests/models/registry.py
2	12	tests/models/test_initialization.py
3	0	tests/models/test_registry.py
100	175	vllm/model_executor/models/aria.py
4	5	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	165	vllm/transformers_utils/configs/aria.py

[3127e975f] Cyrus Leung 2025-01-20 [CI/Build] Make pre-commit faster (#12212)
2	0	.github/workflows/pre-commit.yml
15	1	.pre-commit-config.yaml

[4001ea126] Cyrus Leung 2025-01-20 [CI/Build] Remove dummy CI steps (#12208)
0	20	.github/workflows/dummy.yml

[5c89a29c2] youkaichao 2025-01-20 [misc] add placeholder format.sh (#12206)
5	0	format.sh
1	1	tools/shellcheck.sh

[59a0192fb] Cyrus Leung 2025-01-20 [Core] Interface for accessing model from `VllmRunner` (#10353)
5	0	tests/conftest.py
3	1	tests/engine/test_custom_executor.py
33	31	tests/model_executor/test_model_load_with_params.py
5	2	tests/models/decoder_only/language/test_jamba.py
5	2	tests/models/decoder_only/language/test_mamba.py
5	2	tests/models/decoder_only/language/test_models.py
26	23	tests/models/decoder_only/vision_language/test_qwen2_vl.py
5	2	tests/models/embedding/language/test_cls_models.py
5	2	tests/models/embedding/language/test_embedding.py
137	105	tests/quantization/test_compressed_tensors.py
29	23	tests/quantization/test_fp8.py
20	17	tests/quantization/test_lm_head.py
13	10	tests/quantization/test_quark.py
16	18	tests/tensorizer_loader/test_tensorizer.py
3	14	vllm/engine/llm_engine.py
36	16	vllm/entrypoints/llm.py
41	9	vllm/executor/executor_base.py
1	1	vllm/executor/mp_distributed_executor.py
4	13	vllm/model_executor/model_loader/tensorizer.py
11	1	vllm/spec_decode/ngram_worker.py
12	0	vllm/spec_decode/smaller_tp_proposer_worker.py
4	0	vllm/spec_decode/spec_decode_worker.py
3	13	vllm/v1/executor/multiproc_executor.py
3	0	vllm/v1/worker/gpu_model_runner.py
4	0	vllm/v1/worker/gpu_worker.py
3	0	vllm/worker/cpu_model_runner.py
4	0	vllm/worker/hpu_model_runner.py
3	0	vllm/worker/model_runner.py
7	2	vllm/worker/model_runner_base.py
3	0	vllm/worker/neuron_model_runner.py
3	0	vllm/worker/openvino_model_runner.py
4	0	vllm/worker/openvino_worker.py
3	0	vllm/worker/tpu_model_runner.py
12	0	vllm/worker/worker_base.py
3	0	vllm/worker/xpu_model_runner.py

[83609791d] Isotr0py 2025-01-20 [Model] Add Qwen2 PRM model support (#12202)
5	0	docs/source/models/supported_models.md
5	4	tests/models/embedding/language/test_embedding.py
1	0	tests/models/registry.py
33	9	vllm/model_executor/models/qwen2_rm.py
1	0	vllm/model_executor/models/registry.py

[0974c9bc5] Yuan Tang 2025-01-20 [Bugfix] Fix incorrect types in LayerwiseProfileResults (#12196)
3	3	vllm/profiler/layerwise_profile.py

[d2643128f] Yuan Tang 2025-01-20 [DOC] Add missing docstring in LLMEngine.add_request() (#12195)
2	0	vllm/engine/llm_engine.py

[c5c06209e] Yuan Tang 2025-01-20 [DOC] Fix typo in docstring and assert message (#12194)
2	2	vllm/engine/output_processor/single_step.py

[3ea7b9452] Harry Mellor 2025-01-20 Move linting to `pre-commit` (#11975)
1	1	.buildkite/nightly-benchmarks/scripts/nightly-annotate.sh
0	40	.github/workflows/actionlint.yml
0	53	.github/workflows/clang-format.yml
0	45	.github/workflows/codespell.yml
0	32	.github/workflows/doc-lint.yml
20	0	.github/workflows/dummy.yml
0	17	.github/workflows/matchers/ruff.json
0	51	.github/workflows/mypy.yaml
0	37	.github/workflows/png-lint.yml
17	0	.github/workflows/pre-commit.yml
0	52	.github/workflows/ruff.yml
0	37	.github/workflows/shellcheck.yml
0	38	.github/workflows/yapf.yml
73	0	.pre-commit-config.yaml
1	1	csrc/core/scalar_type.hpp
3	3	csrc/cpu/cpu_types.hpp
284	265	csrc/cpu/cpu_types_arm.hpp
143	111	csrc/cpu/cpu_types_vsx.hpp
165	146	csrc/cpu/cpu_types_x86.hpp
1	2	csrc/cutlass_extensions/common.hpp
8	5	docs/source/contributing/overview.md
0	321	format.sh
8	0	pyproject.toml
1	14	requirements-lint.txt
0	13	tools/actionlint.sh
0	3	tools/doc-lint.sh

[51ef828f1] youkaichao 2025-01-20 [torch.compile] fix sym_tensor_indices (#12191)
5	1	vllm/compilation/backends.py

[df450aa56] shangmingc 2025-01-20 [Bugfix] Fix num_heads value for simple connector when tp enabled (#12074)
2	1	vllm/distributed/kv_transfer/kv_connector/simple_connector.py

[bbe5f9de7] Martin Gleize 2025-01-19 [Model] Support for fairseq2 Llama (#11442)
1	0	tests/models/registry.py
2	1	tests/weight_loading/models.txt
7	6	tests/weight_loading/test_weight_loading.py
22	12	vllm/model_executor/layers/linear.py
13	2	vllm/model_executor/model_loader/loader.py
151	0	vllm/model_executor/models/fairseq2_llama.py
1	0	vllm/model_executor/models/registry.py

[81763c58a] Roger Wang 2025-01-19 [V1] Add V1 support of Qwen2-VL (#12128)
1	1	docs/source/models/supported_models.md
8	10	tests/models/decoder_only/vision_language/test_qwen2_vl.py
12	2	vllm/compilation/decorators.py
43	1	vllm/model_executor/layers/rotary_embedding.py
4	2	vllm/model_executor/models/llava_onevision.py
9	1	vllm/model_executor/models/qwen2.py
78	64	vllm/model_executor/models/qwen2_vl.py
3	0	vllm/v1/worker/gpu_input_batch.py
134	4	vllm/v1/worker/gpu_model_runner.py

[edaae198e] Isotr0py 2025-01-19 [Misc] Add BNB support to GLM4-V model (#12184)
11	4	vllm/model_executor/model_loader/loader.py
47	48	vllm/model_executor/models/chatglm.py
2	1	vllm/model_executor/models/glm4_vision_encoder.py

[936db119e] gujing 2025-01-19 benchmark_serving support --served-model-name param (#12109)
6	3	benchmarks/backend_request_func.py
13	0	benchmarks/benchmark_serving.py

[e66faf480] youkaichao 2025-01-19 [torch.compile] store inductor compiled Python file (#12182)
58	22	vllm/compilation/backends.py
2	11	vllm/config.py

[630eb5b5c] Cyrus Leung 2025-01-19 [Bugfix] Fix multi-modal processors for transformers 4.48 (#12187)
24	1	vllm/model_executor/models/llava.py
49	23	vllm/model_executor/models/qwen2_audio.py
1	8	vllm/model_executor/models/ultravox.py
5	4	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
118	0	vllm/transformers_utils/configs/aria.py

[4e94951bb] Michal Adamczyk 2025-01-19 [BUGFIX] Move scores to float32 in case of running xgrammar on cpu (#12152)
5	2	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[7a8a48d51] Simon Mo 2025-01-18 [V1] Collect env var for usage stats (#12115)
18	0	vllm/usage/usage_lib.py

[32eb0da80] yancong 2025-01-19 [Misc] Support register quantization method out-of-tree (#11969)
117	0	tests/quantization/test_register_quantization_config.py
41	0	vllm/model_executor/layers/quantization/__init__.py

[6d0e3d372] youkaichao 2025-01-18 [core] clean up executor class hierarchy between v1 and v0 (#12171)
0	10	vllm/executor/executor_base.py
58	29	vllm/v1/executor/abstract.py
3	47	vllm/v1/executor/multiproc_executor.py
0	344	vllm/v1/executor/ray_executor.py
0	280	vllm/v1/executor/ray_utils.py
0	88	vllm/v1/executor/uniproc_executor.py

[02798ecab] Isotr0py 2025-01-18 [Model] Port deepseek-vl2 processor, remove dependency (#12169)
0	1	.buildkite/test-pipeline.yaml
2	8	docs/source/models/supported_models.md
1	1	examples/offline_inference/vision_language_multi_image.py
1	1	tests/models/decoder_only/vision_language/test_models.py
3	0	tests/models/multimodal/processing/test_common.py
13	38	vllm/model_executor/models/deepseek_vl2.py
4	0	vllm/transformers_utils/processors/__init__.py
361	0	vllm/transformers_utils/processors/deepseek_vl2.py

[813f249f0] Russell Bryant 2025-01-17 [Docs] Fix broken link in SECURITY.md (#12175)
1	1	SECURITY.md

[da02cb4b2] youkaichao 2025-01-18 [core] further polish memory profiling (#12126)
12	14	tests/test_utils.py
56	39	vllm/utils.py
17	14	vllm/worker/worker.py

[c09503ddd] Hongxia Yang 2025-01-17 [AMD][CI/Build][Bugfix] use pytorch stale wheel (#12172)
3	3	Dockerfile.rocm
1	1	docs/source/getting_started/installation/gpu/rocm.inc.md

[2b8350322] youkaichao 2025-01-18 [misc] fix cross-node TP (#12166)
36	2	vllm/executor/mp_distributed_executor.py
0	22	vllm/platforms/cuda.py

[7b98a65ae] youkaichao 2025-01-18 [torch.compile] disable logging when cache is disabled (#12043)
21	9	vllm/compilation/backends.py

[b5b57e301] Gregory Shtrasberg 2025-01-17 [AMD][FP8] Using MI300 FP8 format on ROCm for block_quant (#12134)
33	0	vllm/model_executor/layers/quantization/fp8.py
11	3	vllm/model_executor/layers/quantization/utils/fp8_utils.py

[54cacf008] Kunshang Ji 2025-01-18 [Bugfix] Mistral tokenizer encode accept list of str (#12149)
30	8	vllm/transformers_utils/tokenizers/mistral.py

[58fd57ff1] Wallas Henrique 2025-01-17 [Bugfix] Fix score api for missing max_model_len validation (#12119)
39	6	tests/entrypoints/openai/test_score.py
9	5	vllm/entrypoints/openai/serving_engine.py
32	22	vllm/entrypoints/openai/serving_score.py

[87a0c076a] youkaichao 2025-01-17 [core] allow callable in collective_rpc (#12151)
3	1	.buildkite/test-pipeline.yaml
2	2	tests/engine/test_custom_executor.py
36	0	tests/entrypoints/llm/test_collective_rpc.py
14	3	vllm/engine/llm_engine.py
9	5	vllm/entrypoints/llm.py
5	4	vllm/executor/executor_base.py
14	7	vllm/executor/mp_distributed_executor.py
6	6	vllm/executor/multiproc_worker_utils.py
10	4	vllm/executor/ray_distributed_executor.py
5	9	vllm/executor/uniproc_executor.py
23	0	vllm/utils.py
15	4	vllm/v1/executor/multiproc_executor.py
5	5	vllm/worker/worker_base.py

[d4e619457] Li, Jiang 2025-01-17 [CI/Build][CPU][Bugfix] Fix CPU CI (#12150)
2	2	.buildkite/run-cpu-test.sh
6	2	vllm/model_executor/layers/activation.py

[07934cc23] Jee Jee Li 2025-01-17 [Misc][LoRA] Improve the readability of LoRA error messages (#12102)
50	19	tests/entrypoints/openai/test_lora_adapters.py
16	0	tests/lora/test_lora_checkpoints.py
3	0	tests/lora/test_lora_huggingface.py
2	57	tests/lora/test_lora_manager.py
109	0	tests/lora/test_peft_helper.py
1	0	vllm/engine/multiprocessing/engine.py
8	16	vllm/entrypoints/openai/serving_models.py
2	10	vllm/lora/models.py
41	8	vllm/lora/peft_helper.py
13	6	vllm/lora/worker_manager.py

[69d765f5a] Chen Zhang 2025-01-17 [V1] Move more control of kv cache initialization from model_executor to EngineCore (#11960)
62	0	tests/v1/test_utils.py
2	0	vllm/attention/layer.py
124	0	vllm/v1/core/kv_cache_utils.py
18	13	vllm/v1/engine/core.py
8	3	vllm/v1/executor/abstract.py
15	10	vllm/v1/executor/multiproc_executor.py
21	19	vllm/v1/executor/ray_executor.py
14	11	vllm/v1/executor/uniproc_executor.py
111	0	vllm/v1/kv_cache_interface.py
54	2	vllm/v1/utils.py
72	12	vllm/v1/worker/gpu_model_runner.py
14	34	vllm/v1/worker/gpu_worker.py

[8027a7246] Divakar Verma 2025-01-17 [ROCm][MoE] moe tuning support for rocm (#12049)
224	48	benchmarks/kernels/benchmark_moe.py

[d75ab55f1] Isotr0py 2025-01-17 [Misc] Add deepseek_vl2 chat template (#12143)
23	0	examples/template_deepseek_vl2.jinja
1	0	tests/entrypoints/test_chat_utils.py

[d1adb9b40] Chen Zhang 2025-01-17 [BugFix] add more `is not None` check in VllmConfig.__post_init__ (#12138)
2	1	vllm/config.py

[b8bfa46a1] Yuan Tang 2025-01-16 [Bugfix] Fix issues in CPU build Dockerfile (#12135)
3	3	Dockerfile.cpu
3	7	setup.py

[1475847a1] Yuan Tang 2025-01-16 [Doc] Add instructions on using Podman when SELinux is active (#12136)
3	0	docs/source/deployment/docker.md

[fead53ba7] Kunshang Ji 2025-01-17 [CI]add genai-perf benchmark in nightly benchmark (#10704)
107	0	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
23	0	.buildkite/nightly-benchmarks/tests/genai-perf-tests.json
3	0	requirements-test.in
63	4	requirements-test.txt

[ebc73f282] Kuntai Du 2025-01-17 [Bugfix] Fix a path bug in disaggregated prefill example script. (#12121)
3	1	examples/online_serving/disaggregated_prefill.sh

[d06e82400] Chen Zhang 2025-01-17 [Bugfix] Set enforce_eager automatically for mllama (#12127)
0	1	examples/offline_inference/vision_language.py
0	1	examples/offline_inference/vision_language_multi_image.py
5	3	vllm/config.py

[62b06ba23] Isotr0py 2025-01-17 [Model] Add support for deepseek-vl2-tiny model (#12068)
2	3	docs/source/models/supported_models.md
1	1	examples/offline_inference/vision_language.py
1	1	examples/offline_inference/vision_language_multi_image.py
9	11	tests/models/decoder_only/vision_language/test_models.py
1	2	tests/models/registry.py
8	3	vllm/model_executor/models/deepseek_vl2.py

[5fd24ec02] Varun Sundar Rabindranath 2025-01-16 [misc] Add LoRA kernel micro benchmarks (#11579)
1147	0	benchmarks/kernels/benchmark_lora.py
210	0	benchmarks/kernels/utils.py

[874f7c292] Roger Wang 2025-01-16 [Bugfix] Fix max image feature size for Llava-one-vision (#12104)
61	0	tests/models/multimodal/processing/test_llava_next.py
62	0	tests/models/multimodal/processing/test_llava_onevision.py
6	2	vllm/model_executor/models/llava_onevision.py

[92e793d91] youkaichao 2025-01-16 [core] LLM.collective_rpc interface and RLHF example (#12084)
4	0	.buildkite/test-pipeline.yaml
191	0	examples/offline_inference/rlhf.py
39	0	vllm/__init__.py
25	0	vllm/entrypoints/llm.py
0	31	vllm/plugins/__init__.py
11	4	vllm/worker/worker_base.py

[bf53e0c70] youkaichao 2025-01-16 Support torchrun and SPMD-style offline inference (#12071)
1	0	.buildkite/test-pipeline.yaml
64	0	examples/offline_inference/torchrun_example.py
56	0	tests/distributed/test_torchrun_example.py
1	1	tests/engine/test_multiproc_workers.py
4	3	vllm/config.py
1	1	vllm/engine/arg_utils.py
5	0	vllm/engine/llm_engine.py
3	3	vllm/executor/ray_distributed_executor.py
80	1	vllm/executor/uniproc_executor.py
2	2	vllm/lora/layers.py
10	6	vllm/model_executor/layers/logits_processor.py
1	1	vllm/v1/executor/multiproc_executor.py
0	3	vllm/worker/worker.py
20	9	vllm/worker/worker_base.py

[dd7c9ad87] Isotr0py 2025-01-16 [Bugfix] Remove hardcoded `head_size=256` for Deepseek v2 and v3 (#12067)
3	3	tests/kernels/test_attention.py
6	3	vllm/config.py
7	17	vllm/model_executor/models/deepseek_v2.py
7	17	vllm/model_executor/models/deepseek_v3.py

[9aa1519f0] Michael Goin 2025-01-16 Various cosmetic/comment fixes (#12089)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
1	2	vllm/model_executor/models/aria.py
1	2	vllm/model_executor/models/commandr.py
1	2	vllm/model_executor/models/dbrx.py
1	2	vllm/model_executor/models/exaone.py
1	2	vllm/model_executor/models/gpt_j.py
1	2	vllm/model_executor/models/granite.py
3	4	vllm/model_executor/models/llama.py
1	2	vllm/model_executor/models/mixtral.py
1	2	vllm/model_executor/models/mllama.py
1	2	vllm/model_executor/models/nemotron.py
1	2	vllm/model_executor/models/phimoe.py
1	2	vllm/model_executor/models/qwen2.py
1	2	vllm/model_executor/models/solar.py

[f8ef146f0] Cyrus Leung 2025-01-16 [Doc] Add documentation for specifying model architecture (#12105)
53	0	docs/source/serving/offline_inference.md

[fa0050db0] Elfie Guo 2025-01-15 [Core] Default to using per_token quantization for fp8 when cutlass is supported. (#8651)
2	1	vllm/model_executor/layers/quantization/fp8.py

[cd9d06fb8] tvirolai-amd 2025-01-15 Allow hip sources to be directly included when compiling for rocm. (#12087)
2	2	cmake/utils.cmake

[ebd8c669e] Varun Sundar Rabindranath 2025-01-16 [Bugfix] Fix _get_lora_device for HQQ marlin (#12090)
3	0	vllm/lora/layers.py

[70755e819] Roger Wang 2025-01-15 [V1][Core] Autotune encoder cache budget (#11895)
10	5	vllm/config.py
24	5	vllm/multimodal/registry.py
77	1	vllm/v1/core/encoder_cache_manager.py
18	8	vllm/v1/core/scheduler.py
6	3	vllm/v1/engine/core.py
32	28	vllm/v1/worker/gpu_model_runner.py

[edce722ea] Joe Runde 2025-01-15 [Bugfix] use right truncation for non-generative tasks (#12050)
7	0	tests/entrypoints/llm/test_encode.py
4	0	vllm/config.py
2	1	vllm/transformers_utils/tokenizer_group/__init__.py

[57e729e87] maang-h 2025-01-16 [Doc]: Update `OpenAI-Compatible Server` documents (#12082)
8	8	vllm/engine/arg_utils.py
30	30	vllm/entrypoints/openai/cli_args.py

[de0526f66] kewang-xlnx 2025-01-16 [Misc][Quark] Upstream Quark format to VLLM (#10765)
30	0	tests/quantization/test_quark.py
1	1	vllm/config.py
1	1	vllm/model_executor/layers/linear.py
4	0	vllm/model_executor/layers/quantization/__init__.py
3	0	vllm/model_executor/layers/quantization/base_config.py
16	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py
0	17	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
0	0	vllm/model_executor/layers/quantization/quark/__init__.py
387	0	vllm/model_executor/layers/quantization/quark/quark.py
225	0	vllm/model_executor/layers/quantization/quark/quark_moe.py
5	0	vllm/model_executor/layers/quantization/quark/schemes/__init__.py
52	0	vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py
140	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py
105	0	vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py
99	0	vllm/model_executor/layers/quantization/quark/utils.py
6	5	vllm/model_executor/models/aria.py
14	0	vllm/model_executor/models/commandr.py
52	21	vllm/model_executor/models/dbrx.py
7	5	vllm/model_executor/models/exaone.py
3	3	vllm/model_executor/models/gemma2.py
14	0	vllm/model_executor/models/gpt_j.py
7	5	vllm/model_executor/models/granite.py
7	5	vllm/model_executor/models/llama.py
14	0	vllm/model_executor/models/mixtral.py
13	0	vllm/model_executor/models/mllama.py
13	0	vllm/model_executor/models/nemotron.py
14	0	vllm/model_executor/models/phimoe.py
13	0	vllm/model_executor/models/qwen2.py
7	5	vllm/model_executor/models/solar.py
7	1	vllm/model_executor/parameter.py
1	1	vllm/platforms/rocm.py

[5ecf3e0aa] Yuan 2025-01-15 Misc: allow to use proxy in `HTTPConnection` (#12042)
1	1	vllm/connections.py

[97eb97b5a] RunningLeon 2025-01-15 [Model]: Support internlm3 (#12037)
5	0	docs/source/models/supported_models.md
2	0	tests/models/registry.py
20	15	vllm/model_executor/models/llama.py
1	0	vllm/model_executor/models/registry.py

[3adf0ffda] wangxiyuan 2025-01-15 [Platform] Do not raise error if _Backend is not found (#12023)
8	3	tests/kernels/test_attention_selector.py
8	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py
4	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
14	0	tests/plugins_tests/test_platform_plugins.py
4	4	vllm/attention/layer.py
11	9	vllm/attention/selector.py

[ad388d25a] Keyun Tong 2025-01-15 Type-fix: make execute_model output type optional (#12020)
1	0	vllm/v1/executor/uniproc_executor.py
1	1	vllm/v1/worker/gpu_worker.py

[cbe94391e] Rahul Tuli 2025-01-15 Fix: cases with empty sparsity config (#12057)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[994fc655b] Chen Zhang 2025-01-15 [V1][Prefix Cache] Move the logic of num_computed_tokens into KVCacheManager (#12003)
47	24	tests/v1/core/test_prefix_caching.py
12	5	vllm/v1/core/kv_cache_manager.py
2	6	vllm/v1/core/scheduler.py

[3f9b7ab9f] Kyle Sayers 2025-01-15 [Doc] Update examples to remove SparseAutoModelForCausalLM (#12062)
5	6	docs/source/features/quantization/fp8.md
3	4	docs/source/features/quantization/int8.md

[ad34c0df0] youkaichao 2025-01-15 [core] platform agnostic executor via collective_rpc (#11256)
12	16	tests/engine/test_custom_executor.py
6	6	tests/engine/test_multiproc_workers.py
5	1	tests/test_utils.py
8	4	vllm/config.py
4	2	vllm/distributed/parallel_state.py
6	82	vllm/engine/async_llm_engine.py
22	72	vllm/engine/llm_engine.py
2	9	vllm/engine/multiprocessing/engine.py
0	299	vllm/executor/cpu_executor.py
0	212	vllm/executor/distributed_gpu_executor.py
232	35	vllm/executor/executor_base.py
0	145	vllm/executor/gpu_executor.py
0	202	vllm/executor/hpu_executor.py
29	52	vllm/executor/{multiproc_gpu_executor.py => mp_distributed_executor.py}
9	3	vllm/executor/multiproc_worker_utils.py
0	26	vllm/executor/multiproc_xpu_executor.py
0	114	vllm/executor/neuron_executor.py
0	125	vllm/executor/openvino_executor.py
144	96	vllm/executor/{ray_gpu_executor.py => ray_distributed_executor.py}
0	515	vllm/executor/ray_hpu_executor.py
0	343	vllm/executor/ray_tpu_executor.py
25	1	vllm/executor/ray_utils.py
0	40	vllm/executor/ray_xpu_executor.py
0	142	vllm/executor/tpu_executor.py
57	0	vllm/executor/uniproc_executor.py
0	39	vllm/executor/xpu_executor.py
27	0	vllm/platforms/cpu.py
22	0	vllm/platforms/cuda.py
8	0	vllm/platforms/neuron.py
9	3	vllm/platforms/openvino.py
10	0	vllm/platforms/tpu.py
18	4	vllm/platforms/xpu.py
3	5	vllm/spec_decode/medusa_worker.py
3	18	vllm/spec_decode/multi_step_worker.py
5	4	vllm/spec_decode/spec_decode_worker.py
3	2	vllm/v1/executor/abstract.py
13	4	vllm/v1/executor/multiproc_executor.py
1	1	vllm/v1/executor/uniproc_executor.py
2	1	vllm/v1/worker/gpu_worker.py
66	0	vllm/worker/hpu_worker.py
23	5	vllm/worker/neuron_worker.py
2	4	vllm/worker/openvino_worker.py
76	10	vllm/worker/worker_base.py

[f218f9c24] Rui Qiao 2025-01-14 [core] Turn off GPU communication overlap for Ray executor (#12051)
4	4	vllm/envs.py

[0794e7446] Elfie Guo 2025-01-14 [Misc] Add multipstep chunked-prefill support for FlashInfer (#10467)
10	0	csrc/prepare_inputs/advance_step.cu
16	1	tests/multi_step/test_correctness_llm.py
24	5	vllm/attention/backends/flashinfer.py
118	102	vllm/worker/model_runner.py
1	1	vllm/worker/multi_step_model_runner.py

[b7ee940a8] Woosuk Kwon 2025-01-14 [V1][BugFix] Fix edge case in VLM scheduling (#12065)
15	11	vllm/v1/core/scheduler.py

[9ddac5631] Shanshan Shen 2025-01-15 [Platform] move current_memory_usage() into platform (#11369)
7	0	vllm/platforms/cuda.py
9	0	vllm/platforms/interface.py
7	0	vllm/platforms/rocm.py
7	0	vllm/platforms/xpu.py
1	7	vllm/utils.py

[1a51b9f87] Konrad Zawora 2025-01-15 [HPU][Bugfix] Don't use /dev/accel/accel0 for HPU autodetection in setup.py (#12046)
12	7	setup.py

[42f5e7c52] Jee Jee Li 2025-01-15 [Kernel] Support MulAndSilu (#11624)
25	7	csrc/activation_kernels.cu
2	0	csrc/ops.h
3	0	csrc/torch_bindings.cpp
13	7	tests/kernels/test_activation.py
35	0	vllm/model_executor/layers/activation.py
3	11	vllm/model_executor/models/molmo.py
2	11	vllm/model_executor/models/ultravox.py

[a3a3ee4e6] Jee Jee Li 2025-01-15 [Misc]  Merge bitsandbytes_stacked_params_mapping and packed_modules_mapping (#11924)
11	16	vllm/model_executor/model_loader/loader.py
25	1	vllm/model_executor/model_loader/utils.py
0	7	vllm/model_executor/models/baichuan.py
0	8	vllm/model_executor/models/exaone.py
3	3	vllm/model_executor/models/falcon.py
0	9	vllm/model_executor/models/gemma.py
0	10	vllm/model_executor/models/gemma2.py
0	8	vllm/model_executor/models/granite.py
0	10	vllm/model_executor/models/idefics3.py
0	10	vllm/model_executor/models/llama.py
4	8	vllm/model_executor/models/llava.py
0	10	vllm/model_executor/models/minicpm.py
0	6	vllm/model_executor/models/minicpm3.py
0	20	vllm/model_executor/models/minicpmv.py
3	8	vllm/model_executor/models/mllama.py
0	6	vllm/model_executor/models/molmo.py
0	6	vllm/model_executor/models/nemotron.py
3	7	vllm/model_executor/models/opt.py
0	8	vllm/model_executor/models/phi.py
0	4	vllm/model_executor/models/phi3.py
0	7	vllm/model_executor/models/qwen.py
0	10	vllm/model_executor/models/qwen2.py
0	10	vllm/model_executor/models/qwen2_vl.py
0	8	vllm/model_executor/models/solar.py

[87054a57a] maang-h 2025-01-15 [Doc]: Update the Json Example of the `Engine Arguments` document (#12045)
4	4	vllm/engine/arg_utils.py

[c9d6ff530] Harry Mellor 2025-01-14 Explain where the engine args go when using Docker (#12041)
2	0	docs/source/deployment/docker.md

[a2d2acb4c] Chen Zhang 2025-01-14 [Bugfix][Kernel] Give unique name to BlockSparseFlashAttention (#12040)
1	2	vllm/attention/backends/blocksparse_attn.py
1	0	vllm/platforms/interface.py

[2e0e01761] wangxiyuan 2025-01-14 [Platform] Add output for Attention Backend (#11981)
4	0	vllm/attention/backends/abstract.py
2	0	vllm/attention/backends/flash_attn.py
1	5	vllm/attention/layer.py
2	0	vllm/v1/attention/backends/flash_attn.py

[1f18adb24] Chen Zhang 2025-01-14 [Kernel] Revert the API change of Attention.forward (#12038)
2	2	vllm/attention/layer.py

[bb354e6b2] Cyrus Leung 2025-01-14 [Bugfix] Fix various bugs in multi-modal processor (#12031)
19	0	tests/multimodal/test_processing.py
39	50	vllm/multimodal/processing.py
4	1	vllm/multimodal/registry.py

[ff39141a4] youkaichao 2025-01-14 [HPU][misc] add comments for explanation (#12034)
5	0	.buildkite/run-hpu-test.sh

[8a1f938e6] TJian 2025-01-14 [Doc] Update Quantization Hardware Support Documentation (#12025)
1	1	docs/source/features/quantization/supported_hardware.md

[078da3190] Konrad Zawora 2025-01-14 [HPU][Bugfix] set_forward_context and CI test execution (#12014)
5	2	.buildkite/run-hpu-test.sh
1	1	Dockerfile.hpu
17	15	vllm/worker/hpu_model_runner.py

[1a401252b] Woosuk Kwon 2025-01-13 [Docs] Add Sky Computing Lab to project intro (#12019)
2	0	README.md
2	0	docs/source/index.md

[f35ec461f] Steve Luo 2025-01-14 [Bugfix] Fix deepseekv3 gate bias error (#12002)
14	5	vllm/model_executor/layers/fused_moe/fused_moe.py

[289b5191d] Yikun Jiang 2025-01-14 [Doc] Fix build from source and installation link in README.md (#12013)
5	5	README.md

[c6db21313] elijah 2025-01-13 bugfix: Fix signature mismatch in benchmark's `get_tokenizer` function (#11982)
24	3	benchmarks/backend_request_func.py

[a7d59688f] Shanshan Shen 2025-01-13 [Platform] Move get_punica_wrapper() function to Platform (#11516)
9	17	vllm/lora/punica_wrapper/punica_selector.py
4	0	vllm/platforms/cpu.py
4	0	vllm/platforms/cuda.py
4	0	vllm/platforms/hpu.py
7	0	vllm/platforms/interface.py
4	0	vllm/platforms/rocm.py

[458e63a2c] youkaichao 2025-01-13 [platform] add device_control env var (#12009)
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
10	0	vllm/platforms/interface.py
1	0	vllm/platforms/neuron.py
2	0	vllm/platforms/rocm.py
1	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py

[e8c23ff98] Harry Mellor 2025-01-13 [Doc] Organise installation documentation into categories and tabs (#11935)
1	1	docs/source/conf.py
4	0	docs/source/deployment/docker.md
3	1	docs/source/features/compatibility_matrix.md
41	31	docs/source/getting_started/installation/{hpu-gaudi.md => ai_accelerator/hpu-gaudi.inc.md}
375	0	docs/source/getting_started/installation/ai_accelerator/index.md
35	32	docs/source/getting_started/installation/{neuron.md => ai_accelerator/neuron.inc.md}
39	39	docs/source/getting_started/installation/{openvino.md => ai_accelerator/openvino.inc.md}
27	11	docs/source/getting_started/installation/{tpu.md => ai_accelerator/tpu.inc.md}
0	46	docs/source/getting_started/installation/cpu-arm.md
18	10	docs/source/getting_started/installation/{cpu-apple.md => cpu/apple.inc.md}
30	0	docs/source/getting_started/installation/cpu/arm.inc.md
21	0	docs/source/getting_started/installation/cpu/build.inc.md
132	58	docs/source/getting_started/installation/{cpu-x86.md => cpu/index.md}
35	0	docs/source/getting_started/installation/cpu/x86.inc.md
17	0	docs/source/getting_started/installation/device.template.md
42	54	docs/source/getting_started/installation/{gpu-cuda.md => gpu/cuda.inc.md}
300	0	docs/source/getting_started/installation/gpu/index.md
75	72	docs/source/getting_started/installation/{gpu-rocm.md => gpu/rocm.inc.md}
24	27	docs/source/getting_started/installation/{xpu.md => gpu/xpu.inc.md}
3	10	docs/source/getting_started/installation/index.md
19	0	docs/source/getting_started/installation/python_env_setup.inc.md

[cd8249903] Roger Wang 2025-01-13 [Doc][V1] Update model implementation guide for V1 support (#11998)
11	1	docs/source/contributing/model/basic.md
72	15	docs/source/contributing/model/multimodal.md

[0f8cafe2d] Chen Zhang 2025-01-13 [Kernel] unified_attention for Attention.forward (#11967)
14	12	vllm/attention/layer.py
0	1	vllm/utils.py
11	2	vllm/worker/hpu_model_runner.py
3	0	vllm/worker/hpu_worker.py
10	7	vllm/worker/neuron_model_runner.py
3	1	vllm/worker/openvino_model_runner.py
11	2	vllm/worker/openvino_worker.py
18	10	vllm/worker/tpu_model_runner.py
5	1	vllm/worker/tpu_worker.py
12	9	vllm/worker/xpu_model_runner.py

[5340a30d0] Alex Brooks 2025-01-13 Fix Max Token ID for Qwen-VL-Chat (#11980)
9	0	vllm/transformers_utils/tokenizer.py

[89ce62a31] youkaichao 2025-01-13 [platform] add ray_device_key (#11948)
13	6	vllm/executor/ray_utils.py
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
4	0	vllm/platforms/interface.py
1	0	vllm/platforms/neuron.py
2	0	vllm/platforms/rocm.py
2	0	vllm/platforms/tpu.py
3	0	vllm/platforms/xpu.py
11	2	vllm/v1/executor/ray_utils.py

[c3f05b09a] Chenguang Li 2025-01-13 [Misc]Minor Changes about Worker (#11555)
0	1	vllm/v1/worker/gpu_worker.py
0	1	vllm/worker/worker.py

[cf6bbcb49] Concurrensee 2025-01-13 [Misc] Fix Deepseek V2 fp8 kv-scale remapping (#11947)
7	1	vllm/model_executor/models/deepseek_v2.py

[80ea3af1a] Sungjae Lee 2025-01-13 [CI][Spec Decode] fix: broken test for EAGLE model (#11972)
3	1	.buildkite/test-pipeline.yaml
10	1	vllm/model_executor/models/eagle.py

[9dd02d85c] Siyuan Li 2025-01-13 [Bug] Fix usage of `.transpose()` and `.view()` consecutively. (#11979)
1	1	vllm/attention/layer.py
1	1	vllm/model_executor/models/intern_vit.py

[f7b3ba82c] Yangcheng Li 2025-01-13 [MISC] fix typo in kv transfer send recv test (#11983)
2	2	tests/kv_transfer/test_send_recv.py

[619ae268c] Robert Shaw 2025-01-12 [V1] [2/n] Logging and Metrics - `OutputProcessor` Abstraction (#11973)
58	11	tests/v1/engine/test_async_llm.py
88	11	tests/v1/engine/{test_detokenizer.py => test_output_processor.py}
37	52	vllm/v1/engine/async_llm.py
4	2	vllm/v1/engine/core_client.py
22	115	vllm/v1/engine/detokenizer.py
14	19	vllm/v1/engine/llm_engine.py
200	0	vllm/v1/engine/output_processor.py
27	0	vllm/v1/metrics/stats.py

[d14e98d92] Isotr0py 2025-01-13 [Model] Support GGUF models newly added in `transformers` 4.46.0 (#9685)
8	14	examples/offline_inference/gguf_inference.py
74	31	tests/models/decoder_only/language/test_gguf.py
35	23	vllm/model_executor/layers/linear.py
8	3	vllm/model_executor/models/gpt2.py
2	1	vllm/model_executor/models/llama.py
20	11	vllm/model_executor/models/stablelm.py
15	4	vllm/model_executor/models/starcoder2.py

[9597a095f] Robert Shaw 2025-01-12 [V1][Core][1/n] Logging and Metrics (#11962)
2	2	tests/v1/engine/test_engine_core.py
2	2	tests/v1/engine/test_engine_core_client.py
15	5	vllm/v1/core/scheduler.py
3	0	vllm/v1/engine/__init__.py
19	7	vllm/v1/engine/async_llm.py
16	37	vllm/v1/engine/core.py
20	28	vllm/v1/engine/core_client.py
2	3	vllm/v1/engine/llm_engine.py
0	0	vllm/v1/metrics/__init__.py
38	0	vllm/v1/metrics/loggers.py
12	0	vllm/v1/metrics/stats.py

[263a870ee] Avshalom Manevich 2025-01-12 [Hardware][TPU] workaround fix for MoE on TPU (#11764)
7	0	tests/kernels/test_moe.py
2	1	vllm/model_executor/layers/fused_moe/layer.py
51	0	vllm/model_executor/layers/fused_moe/moe_torch_iterative.py

[8bddb7351] Akshat Tripathi 2025-01-12 [Hardware][CPU] Multi-LoRA implementation for the CPU backend (#11100)
6	0	.buildkite/run-cpu-test.sh
1	1	docs/source/features/compatibility_matrix.md
19	13	tests/lora/conftest.py
30	9	tests/lora/test_layers.py
11	10	tests/lora/test_lora_manager.py
3	1	tests/lora/test_mixtral.py
75	49	tests/lora/{test_punica_sizes.py => test_punica_ops_sizes.py}
75	59	tests/lora/{test_punica_variation.py => test_punica_ops_variation.py}
2	1	tests/lora/test_quant_model.py
0	27	tests/lora/utils.py
0	3	vllm/executor/cpu_executor.py
13	0	vllm/lora/ops/torch_ops/__init__.py
113	0	vllm/lora/ops/torch_ops/lora_ops.py
13	0	vllm/lora/ops/triton_ops/__init__.py
0	0	vllm/lora/ops/{ => triton_ops}/bgmv_expand.py
0	0	vllm/lora/ops/{ => triton_ops}/bgmv_expand_slice.py
0	0	vllm/lora/ops/{ => triton_ops}/bgmv_shrink.py
0	0	vllm/lora/ops/{ => triton_ops}/sgmv_expand.py
0	0	vllm/lora/ops/{ => triton_ops}/sgmv_shrink.py
0	0	vllm/lora/ops/{ => triton_ops}/utils.py
346	0	vllm/lora/punica_wrapper/punica_cpu.py
5	5	vllm/lora/punica_wrapper/punica_gpu.py
5	0	vllm/lora/punica_wrapper/punica_selector.py
122	11	vllm/worker/cpu_model_runner.py
16	4	vllm/worker/cpu_worker.py

[f967e51f3] Isotr0py 2025-01-12 [Model] Initialize support for Deepseek-VL2 models (#11578)
1	0	.buildkite/test-pipeline.yaml
19	1	docs/source/models/supported_models.md
18	0	examples/offline_inference/vision_language.py
23	0	examples/offline_inference/vision_language_multi_image.py
27	0	tests/models/decoder_only/vision_language/test_models.py
36	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
2	0	tests/models/registry.py
3	0	tests/models/test_initialization.py
2	2	vllm/entrypoints/chat_utils.py
17	1	vllm/model_executor/models/deepseek_v2.py
18	2	vllm/model_executor/models/deepseek_v3.py
662	0	vllm/model_executor/models/deepseek_vl2.py
1	1	vllm/model_executor/models/minicpmv.py
1	0	vllm/model_executor/models/registry.py
4	2	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
214	0	vllm/transformers_utils/configs/deepseek_vl2.py

[43f3d9e69] Rafael Vasquez 2025-01-12 [CI/Build] Add markdown linter (#11857)
2	2	.github/workflows/{sphinx-lint.yml => doc-lint.yml}
1	0	docs/README.md
0	1	docs/source/api/model/index.md
2	0	docs/source/community/sponsors.md
4	0	docs/source/contributing/model/multimodal.md
0	2	docs/source/contributing/overview.md
2	2	docs/source/deployment/docker.md
5	5	docs/source/deployment/frameworks/cerebrium.md
5	5	docs/source/deployment/frameworks/dstack.md
6	6	docs/source/deployment/frameworks/skypilot.md
1	1	docs/source/deployment/integrations/llamastack.md
226	225	docs/source/deployment/k8s.md
4	7	docs/source/design/automatic_prefix_caching.md
2	2	docs/source/features/quantization/auto_awq.md
4	3	docs/source/features/quantization/bnb.md
2	2	docs/source/features/quantization/fp8.md
1	1	docs/source/features/quantization/fp8_e4m3_kvcache.md
5	5	docs/source/features/quantization/gguf.md
1	1	docs/source/features/quantization/int8.md
2	8	docs/source/features/spec_decode.md
26	18	docs/source/features/tool_calling.md
1	1	docs/source/getting_started/faq.md
7	10	docs/source/getting_started/installation/cpu-apple.md
22	22	docs/source/getting_started/installation/cpu-x86.md
48	48	docs/source/getting_started/installation/gpu-cuda.md
58	58	docs/source/getting_started/installation/gpu-rocm.md
32	32	docs/source/getting_started/installation/hpu-gaudi.md
4	4	docs/source/getting_started/installation/neuron.md
7	7	docs/source/getting_started/installation/openvino.md
3	3	docs/source/getting_started/installation/tpu.md
12	12	docs/source/getting_started/installation/xpu.md
25	25	docs/source/getting_started/quickstart.md
4	2	docs/source/getting_started/troubleshooting.md
2	2	docs/source/index.md
6	6	docs/source/models/extensions/runai_model_streamer.md
8	5	docs/source/models/supported_models.md
1	1	docs/source/performance/optimization.md
20	20	docs/source/serving/distributed_serving.md
1	1	docs/source/serving/integrations/langchain.md
1	1	docs/source/serving/integrations/llamaindex.md
1	1	docs/source/serving/metrics.md
1	0	docs/source/serving/multimodal_inputs.md
1	1	docs/source/serving/offline_inference.md
15	4	docs/source/serving/openai_compatible_server.md
5	5	format.sh
6	0	pyproject.toml
1	1	requirements-lint.txt
3	0	tools/doc-lint.sh
0	3	tools/sphinx-lint.sh

[b25cfab9a] Roger Wang 2025-01-11 [V1] Avoid sending text prompt to core engine (#11963)
2	2	vllm/v1/engine/__init__.py
6	0	vllm/v1/engine/core_client.py

[4b657d329] sixgod 2025-01-12 [Model] Add cogagent model support vLLM (#11742)
7	5	vllm/model_executor/models/chatglm.py

[d697dc01b] Nicolò Lucchesi 2025-01-11 [Bugfix] Fix RobertaModel loading (#11940)
26	1	tests/model_executor/test_model_load_with_params.py
1	0	tests/models/embedding/language/test_embedding.py
40	11	vllm/model_executor/models/roberta.py

[a991f7d50] Cyrus Leung 2025-01-11 [Doc] Basic guide for writing unit tests for new models (#11951)
1	1	docs/source/contributing/model/basic.md
1	0	docs/source/contributing/model/index.md
1	2	docs/source/contributing/model/registration.md
63	0	docs/source/contributing/model/tests.md
5	0	tests/models/registry.py
10	0	tests/models/test_initialization.py

[7a3a83e3b] Cyrus Leung 2025-01-11 [CI/Build] Move model-specific multi-modal processing tests (#11934)
1	0	.buildkite/test-pipeline.yaml
0	0	tests/models/{decoder_only/vision_language/processing => multimodal}/__init__.py
0	0	tests/models/multimodal/processing/__init__.py
201	0	tests/models/multimodal/processing/test_common.py
2	2	tests/models/{decoder_only/vision_language => multimodal}/processing/test_idefics3.py
2	2	tests/models/{decoder_only/vision_language => multimodal}/processing/test_internvl.py
1	1	tests/models/{decoder_only/vision_language => multimodal}/processing/test_llava_next.py
1	1	tests/models/{decoder_only/vision_language => multimodal}/processing/test_llava_onevision.py
2	2	tests/models/{decoder_only/vision_language => multimodal}/processing/test_phi3v.py
2	2	tests/models/{decoder_only/vision_language => multimodal}/processing/test_qwen.py
2	2	tests/models/{decoder_only/vision_language => multimodal}/processing/test_qwen2_vl.py
4	228	tests/multimodal/test_processing.py
33	0	tests/multimodal/utils.py

[c32a7c7c0] shaochangxu 2025-01-11 [Bugfix] fused_experts_impl wrong compute type for float32 (#11921)
8	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[2118d0565] Sungjae Lee 2025-01-11 [Bugfix][SpecDecode] Adjust Eagle model architecture to align with intended design (#11672)
24	2	vllm/model_executor/models/eagle.py

[899136b85] youkaichao 2025-01-11 [ci] fix broken distributed-tests-4-gpus (#11937)
2	1	tests/spec_decode/e2e/test_integration_dist_tp4.py

[c9f09a4fe] Fred Reiss 2025-01-10 [mypy] Fix mypy warnings in api_server.py (#11941)
5	3	vllm/entrypoints/openai/api_server.py

[d45cbe70f] Travis Johnson 2025-01-10 [Bugfix] Check that number of images matches number of <|image|> tokens with mllama (#11939)
9	0	vllm/model_executor/models/mllama.py

[8a579408f] minmin 2025-01-11 [Misc] Update benchmark_prefix_caching.py fixed example usage (#11920)
2	1	benchmarks/benchmark_prefix_caching.py

[46fa98cca] Isotr0py 2025-01-11 [Misc] Clean up debug code in Deepseek-V3 (#11930)
0	3	vllm/model_executor/models/deepseek_v3.py

[aa1e77a19] Li, Jiang 2025-01-11 [Hardware][CPU] Support MOE models on x86 CPU (#11831)
1	1	docs/source/getting_started/installation/cpu-x86.md
4	0	tests/models/decoder_only/language/test_models.py
38	3	vllm/model_executor/layers/fused_moe/layer.py

[5959564f9] Kuntai Du 2025-01-10 Doc fix in `benchmark_long_document_qa_throughput.py` (#11933)
1	2	benchmarks/benchmark_long_document_qa_throughput.py

[f33e033e2] Kuntai Du 2025-01-10 [Docs] Fix docstring in `get_ip` function (#11932)
1	1	vllm/utils.py

[482cdc494] Harry Mellor 2025-01-10 [Doc] Rename offline inference examples (#11927)
1	1	.buildkite/run-cpu-test.sh
1	1	.buildkite/run-gh200-test.sh
1	1	.buildkite/run-hpu-test.sh
1	1	.buildkite/run-neuron-test.sh
1	1	.buildkite/run-openvino-test.sh
1	1	.buildkite/run-tpu-test.sh
2	2	.buildkite/run-xpu-test.sh
10	10	.buildkite/test-pipeline.yaml
1	1	docs/source/contributing/profiling/profiling_index.md
1	1	docs/source/features/structured_outputs.md
2	2	docs/source/getting_started/installation/cpu-x86.md
1	1	docs/source/getting_started/quickstart.md
2	2	docs/source/models/generative_models.md
3	3	docs/source/models/pooling_models.md
4	4	docs/source/serving/multimodal_inputs.md
0	0	examples/offline_inference/{offline_inference_arctic.py => arctic.py}
0	0	examples/offline_inference/{offline_inference_audio_language.py => audio_language.py}
0	0	examples/offline_inference/{offline_inference.py => basic.py}
0	0	examples/offline_inference/{offline_inference_with_default_generation_config.py => basic_with_model_default_sampling.py}
0	0	examples/offline_inference/{offline_inference_chat.py => chat.py}
0	0	examples/offline_inference/{offline_chat_with_tools.py => chat_with_tools.py}
0	0	examples/offline_inference/{offline_inference_classification.py => classification.py}
0	0	examples/offline_inference/{offline_inference_cli.py => cli.py}
0	0	examples/offline_inference/{offline_inference_distributed.py => distributed.py}
0	0	examples/offline_inference/{offline_inference_embedding.py => embedding.py}
0	0	examples/offline_inference/{offline_inference_encoder_decoder.py => encoder_decoder.py}
1	1	examples/offline_inference/florence2_inference.py
0	0	examples/offline_inference/{offline_inference_mlpspeculator.py => mlpspeculator.py}
0	0	examples/offline_inference/{offline_inference_neuron.py => neuron.py}
0	0	examples/offline_inference/{offline_inference_neuron_int8_quantization.py => neuron_int8_quantization.py}
9	9	examples/offline_inference/{offline_inference_openai/offline_inference_openai.md => openai/openai_batch.md}
0	0	examples/offline_inference/{offline_inference_openai => openai}/openai_example_batch.jsonl
0	0	examples/offline_inference/{offline_inference_pixtral.py => pixtral.py}
0	0	examples/offline_inference/{offline_inference_with_prefix.py => prefix_caching.py}
1	1	examples/offline_inference/{offline_profile.py => profiling.py}
0	0	examples/offline_inference/{offline_inference_scoring.py => scoring.py}
0	0	examples/offline_inference/{offline_inference_with_profiler.py => simple_profiling.py}
0	0	examples/offline_inference/{offline_inference_structured_outputs.py => structured_outputs.py}
0	0	examples/offline_inference/{offline_inference_tpu.py => tpu.py}
0	0	examples/offline_inference/{offline_inference_vision_language.py => vision_language.py}
0	0	examples/offline_inference/{offline_inference_vision_language_embedding.py => vision_language_embedding.py}
0	0	examples/offline_inference/{offline_inference_vision_language_multi_image.py => vision_language_multi_image.py}
0	0	examples/offline_inference/{offline_inference_whisper.py => whisper.py}
1	1	tests/plugins_tests/test_platform_plugins.py
1	1	tools/profiler/print_layerwise_table.py
1	1	tools/profiler/visualize_layerwise_profile.py

[20410b2fd] wangxiyuan 2025-01-10 [platform] support custom torch.compile backend key (#11318)
2	1	vllm/model_executor/layers/rejection_sampler.py
1	1	vllm/model_executor/layers/vocab_parallel_embedding.py
2	1	vllm/model_executor/models/commandr.py
3	2	vllm/model_executor/models/phi3_small.py
6	0	vllm/platforms/interface.py

[12664ddda] Cyrus Leung 2025-01-10 [Doc] [1/N] Initial guide for merged multi-modal processor (#11925)
1	0	docs/requirements-docs.txt
1	1	docs/source/api/multimodal/index.md
1	1	docs/source/api/multimodal/inputs.md
1	0	docs/source/conf.py
1	1	docs/source/contributing/model/index.md
316	64	docs/source/contributing/model/multimodal.md
1	1	docs/source/contributing/model/registration.md
0	19	docs/source/design/input_processing/input_processing_pipeline.md
0	43	docs/source/design/input_processing/model_inputs_index.md
64	0	docs/source/design/mm_processing.md
1	1	docs/source/index.md
1	1	docs/source/serving/multimodal_inputs.md
1	2	vllm/config.py
0	3	vllm/inputs/__init__.py
2	10	vllm/inputs/registry.py
2	2	vllm/multimodal/__init__.py
0	14	vllm/multimodal/base.py
2	1	vllm/multimodal/inputs.py
8	4	vllm/multimodal/registry.py

[241ad7b30] youkaichao 2025-01-10 [ci] Fix sampler tests (#11922)
1	0	.buildkite/test-pipeline.yaml
9	2	tests/conftest.py

[d85c47d6a] Harry Mellor 2025-01-10 Replace "online inference" with "online serving" (#11923)
1	1	.buildkite/run-cpu-test.sh
2	2	docs/source/features/structured_outputs.md
2	2	docs/source/getting_started/installation/hpu-gaudi.md
1	1	docs/source/getting_started/quickstart.md
1	1	docs/source/models/generative_models.md
1	1	docs/source/models/pooling_models.md
2	2	docs/source/models/supported_models.md
1	1	docs/source/serving/multimodal_inputs.md
2	2	examples/online_serving/openai_chat_completion_client_for_multimodal.py
2	2	tests/models/decoder_only/audio_language/test_ultravox.py
1	1	vllm/model_executor/models/molmo.py

[ef725feaf] wangxiyuan 2025-01-10 [platform] support pytorch custom op pluggable (#11328)
7	0	vllm/model_executor/custom_op.py
4	0	vllm/platforms/interface.py

[d907be7dc] cennn 2025-01-10 [misc] remove python function call for custom activation op (#11885)
0	27	vllm/_custom_ops.py
46	33	vllm/model_executor/layers/activation.py

[d53575a5f] youkaichao 2025-01-10 [ci] fix gh200 tests (#11919)
3	1	vllm/model_executor/model_loader/weight_utils.py

[61af63325] Kunshang Ji 2025-01-10 [BUGFIX] Fix `UnspecifiedPlatform` package name (#11916)
1	1	vllm/platforms/__init__.py

[ac2f3f7fe] Joe Runde 2025-01-10 [Bugfix] Validate lora adapters to avoid crashing server (#11727)
269	0	tests/entrypoints/openai/test_lora_adapters.py
0	109	tests/entrypoints/openai/test_lora_lineage.py
5	3	tests/entrypoints/openai/test_serving_chat.py
7	3	tests/entrypoints/openai/test_serving_models.py
8	19	tests/entrypoints/openai/test_shutdown.py
4	0	vllm/engine/async_llm_engine.py
17	3	vllm/engine/multiprocessing/__init__.py
35	7	vllm/engine/multiprocessing/client.py
24	3	vllm/engine/multiprocessing/engine.py
5	0	vllm/engine/protocol.py
4	3	vllm/entrypoints/openai/api_server.py
1	0	vllm/entrypoints/openai/run_batch.py
59	19	vllm/entrypoints/openai/serving_models.py
17	2	vllm/lora/worker_manager.py
4	0	vllm/v1/engine/async_llm.py

[cf5f000d2] Chen Zhang 2025-01-10 [torch.compile] Hide KV cache behind torch.compile boundary (#11677)
12	6	tests/kernels/test_encoder_decoder_attn.py
83	2	tests/test_utils.py
3	0	tests/v1/engine/test_engine_core.py
3	0	tests/v1/engine/test_engine_core_client.py
17	12	vllm/attention/layer.py
0	1	vllm/config.py
20	13	vllm/forward_context.py
35	0	vllm/utils.py
5	1	vllm/v1/worker/gpu_model_runner.py
2	1	vllm/worker/cpu_enc_dec_model_runner.py
2	1	vllm/worker/cpu_model_runner.py
2	1	vllm/worker/cpu_pooling_model_runner.py
3	1	vllm/worker/cpu_worker.py
2	1	vllm/worker/enc_dec_model_runner.py
3	2	vllm/worker/model_runner.py
2	1	vllm/worker/pooling_model_runner.py
3	1	vllm/worker/worker.py
1	0	vllm/worker/worker_base.py

[3de2b1eaf] Cyrus Leung 2025-01-10 [Doc] Show default pooling method in a table (#11904)
4	4	docs/source/models/generative_models.md
41	18	docs/source/models/pooling_models.md

[b844b99ad] Cyrus Leung 2025-01-10 [VLM] Enable tokenized inputs for merged multi-modal processor (#11900)
25	6	tests/multimodal/test_processing.py
2	2	vllm/inputs/data.py
0	4	vllm/inputs/preprocess.py
20	2	vllm/model_executor/models/blip2.py
30	2	vllm/model_executor/models/chameleon.py
15	9	vllm/model_executor/models/fuyu.py
4	4	vllm/model_executor/models/interfaces.py
4	4	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/phi3v.py
12	6	vllm/model_executor/models/ultravox.py
92	35	vllm/multimodal/processing.py
1	1	vllm/multimodal/profiling.py

[c3cf54dda] Cyrus Leung 2025-01-10 [Doc][5/N] Move Community and API Reference to the bottom (#11896)
1	1	README.md
1	1	docs/source/design/automatic_prefix_caching.md
38	24	docs/source/index.md

[36f530357] Charles Frye 2025-01-09 [Docs] Add Modal to deployment frameworks (#11907)
1	1	docs/source/deployment/frameworks/bentoml.md
1	0	docs/source/deployment/frameworks/index.md
7	0	docs/source/deployment/frameworks/modal.md

[9a228348d] Cyrus Leung 2025-01-10 [Misc] Provide correct Pixtral-HF chat template (#11891)
34	27	docs/source/models/supported_models.md
38	0	examples/template_pixtral_hf.jinja
1	0	tests/entrypoints/test_chat_utils.py

[bd8287221] youkaichao 2025-01-09 [ci]try to fix flaky multi-step tests (#11894)
1	2	tests/multi_step/test_correctness_async_llm.py
7	2	tests/utils.py

[405eb8e39] wangxiyuan 2025-01-09 [platform] Allow platform specify attention backend (#11609)
42	32	tests/kernels/test_attention_selector.py
12	127	vllm/attention/selector.py
5	2	vllm/platforms/cpu.py
76	1	vllm/platforms/cuda.py
5	2	vllm/platforms/hpu.py
5	3	vllm/platforms/interface.py
5	2	vllm/platforms/openvino.py
4	2	vllm/platforms/rocm.py
5	2	vllm/platforms/tpu.py
5	2	vllm/platforms/xpu.py

[65097ca0a] Cyrus Leung 2025-01-09 [Doc] Add model development API Reference (#11884)
1	1	.buildkite/test-pipeline.yaml
2	3	docs/source/api/{params.md => inference_params.md}
9	0	docs/source/api/model/adapters.md
12	0	docs/source/api/model/index.md
9	0	docs/source/api/model/interfaces.md
9	0	docs/source/api/model/interfaces_base.md
2	1	docs/source/index.md
7	4	vllm/model_executor/models/interfaces.py
3	0	vllm/model_executor/models/interfaces_base.py

[1d967acb4] Ye (Charlotte) Qi 2025-01-09 [Bugfix] fix beam search input errors and latency benchmark script (#11875)
17	6	benchmarks/benchmark_latency.py
6	4	vllm/entrypoints/llm.py

[0bd1ff434] Cyrus Leung 2025-01-09 [Bugfix] Override dunder methods of placeholder modules (#11882)
44	3	tests/test_utils.py
176	13	vllm/utils.py

[310aca88c] youkaichao 2025-01-09 [perf]fix current stream (#11870)
8	7	vllm/distributed/device_communicators/pynccl.py
1	4	vllm/distributed/parallel_state.py
33	0	vllm/utils.py
4	4	vllm/worker/multi_step_model_runner.py

[a732900ef] Guspan Tanadi 2025-01-09 [Doc] Intended links Python multiprocessing library (#11878)
1	1	docs/source/design/multiprocessing.md

[d848800e8] Cyrus Leung 2025-01-09 [Misc] Move `print_*_once` from utils to logger (#11298)
1	0	.github/workflows/lint-and-deploy.yaml
6	3	vllm/attention/backends/torch_sdpa.py
5	3	vllm/attention/backends/xformers.py
4	5	vllm/config.py
3	4	vllm/entrypoints/chat_utils.py
11	9	vllm/inputs/preprocess.py
2	2	vllm/inputs/registry.py
52	5	vllm/logger.py
4	2	vllm/lora/peft_helper.py
5	3	vllm/lora/punica_wrapper/punica_selector.py
1	2	vllm/model_executor/custom_op.py
5	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	3	vllm/model_executor/layers/quantization/fp8.py
4	2	vllm/model_executor/layers/quantization/kv_cache.py
4	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
4	4	vllm/model_executor/model_loader/weight_utils.py
4	2	vllm/model_executor/models/chameleon.py
4	2	vllm/model_executor/models/olmoe.py
4	2	vllm/model_executor/models/qwen2_moe.py
4	2	vllm/model_executor/models/vision.py
0	12	vllm/utils.py

[730e9592e] Michael Goin 2025-01-08 [Doc] Recommend uv and python 3.12 for quickstart guide (#11849)
11	2	docs/source/getting_started/quickstart.md

[1fe554bac] Maximilien de Bayser 2025-01-09 treat do_lower_case in the same way as the sentence-transformers library (#11815)
1	0	tests/entrypoints/openai/test_serving_chat.py
1	0	tests/models/embedding/language/test_embedding.py
5	0	vllm/entrypoints/openai/serving_engine.py
6	0	vllm/inputs/preprocess.py
0	5	vllm/transformers_utils/tokenizer_group/__init__.py

[615e4a540] Tyler Michael Smith 2025-01-08 [CI] Turn on basic correctness tests for V1 (#10864)
0	1	tests/basic_correctness/test_basic_correctness.py

[3db0cafdf] Simon Mo 2025-01-08 [Docs] Add Google Cloud Meetup (#11864)
4	0	README.md

[526de822d] rasmith 2025-01-08 [Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models (#11698)
16	1	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py

[56fe4c297] Robert Shaw 2025-01-08 [TPU][Quantization] TPU `W8A8` (#11785)
10	1	.buildkite/run-tpu-test.sh
49	0	tests/tpu/test_quantization_accuracy.py
31	74	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py
0	74	vllm/model_executor/layers/quantization/kernels/__init__.py
0	0	vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/MPLinearKernel.py
74	0	vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py
0	0	vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/exllama.py
0	0	vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/machete.py
0	0	vllm/model_executor/layers/quantization/kernels/{ => mixed_precision}/marlin.py
64	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py
84	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py
134	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py
101	0	vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py
0	38	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
13	0	vllm/model_executor/parameter.py
3	1	vllm/platforms/tpu.py

[47de8821d] WangErXiao 2025-01-09 [Misc]add some explanations for BlockHashType (#11847)
4	2	vllm/v1/core/kv_cache_utils.py

[5984499e4] Cyrus Leung 2025-01-09 [Doc] Expand Multimodal API Reference (#11852)
8	53	docs/source/api/multimodal/index.md
49	0	docs/source/api/multimodal/inputs.md
9	0	docs/source/api/multimodal/parse.md
9	0	docs/source/api/multimodal/processing.md
9	0	docs/source/api/multimodal/profiling.md
9	0	docs/source/api/multimodal/registry.md
21	10	vllm/multimodal/parse.py
20	6	vllm/multimodal/processing.py
5	2	vllm/multimodal/profiling.py

[ca47e176a] Cyrus Leung 2025-01-09 [Misc] Move some model utils into vision file (#11848)
2	3	vllm/model_executor/models/clip.py
2	3	vllm/model_executor/models/pixtral.py
2	1	vllm/model_executor/models/qwen2_vl.py
2	3	vllm/model_executor/models/siglip.py
1	36	vllm/model_executor/models/utils.py
82	1	vllm/model_executor/models/vision.py
3	1	vllm/multimodal/inputs.py
0	44	vllm/multimodal/utils.py

[78f4590b6] Yan Ma 2025-01-09 [Bugfix][XPU] fix silu_and_mul (#11823)
2	2	vllm/model_executor/layers/activation.py
2	3	vllm/plugins/__init__.py

[2f7024987] Li, Jiang 2025-01-08 [CI/Build][Bugfix] Fix CPU CI image clean up (#11836)
2	5	.buildkite/run-cpu-test.sh
1	1	vllm/model_executor/layers/activation.py

[6cd40a5bf] Cyrus Leung 2025-01-08 [Doc][4/N] Reorganize API Reference (#11843)
1	1	.buildkite/test-pipeline.yaml
2	2	Dockerfile
0	0	docs/source/{dev => api}/engine/async_llm_engine.md
0	0	docs/source/{dev/engine/engine_index.md => api/engine/index.md}
0	0	docs/source/{dev => api}/engine/llm_engine.md
0	10	docs/source/{design/multimodal/multimodal_index.md => api/multimodal/index.md}
0	0	docs/source/{dev/offline_inference/offline_index.md => api/offline_inference/index.md}
0	0	docs/source/{dev => api}/offline_inference/llm.md
0	0	docs/source/{dev => api}/offline_inference/llm_inputs.md
22	0	docs/source/api/params.md
-	-	docs/source/assets/{dev => contributing}/dockerfile-stages-dependency.png
1	1	docs/source/contributing/dockerfile/dockerfile.md
1	1	docs/source/design/arch_overview.md
0	16	docs/source/design/multimodal/adding_multimodal_plugin.md
0	6	docs/source/dev/pooling_params.md
0	6	docs/source/dev/sampling_params.md
1	1	docs/source/getting_started/quickstart.md
4	5	docs/source/index.md
1	1	docs/source/serving/offline_inference.md
4	4	docs/source/serving/openai_compatible_server.md
0	3	vllm/multimodal/base.py
0	6	vllm/multimodal/inputs.py
0	3	vllm/multimodal/registry.py
1	1	vllm/pooling_params.py

[aba8d6ee0] Harry Mellor 2025-01-08 [Doc] Move examples into categories (#11840)
1	1	.buildkite/run-cpu-test.sh
1	1	.buildkite/run-gh200-test.sh
1	1	.buildkite/run-hpu-test.sh
1	1	.buildkite/run-neuron-test.sh
1	1	.buildkite/run-openvino-test.sh
1	1	.buildkite/run-tpu-test.sh
2	2	.buildkite/run-xpu-test.sh
13	13	.buildkite/test-pipeline.yaml
2	2	.github/workflows/lint-and-deploy.yaml
1	1	Dockerfile
1	1	docs/source/contributing/profiling/profiling_index.md
2	2	docs/source/deployment/frameworks/skypilot.md
1	1	docs/source/features/disagg_prefill.md
1	1	docs/source/features/lora.md
1	1	docs/source/features/quantization/auto_awq.md
1	1	docs/source/features/quantization/fp8_e4m3_kvcache.md
2	2	docs/source/features/structured_outputs.md
25	20	docs/source/generate_examples.py
2	2	docs/source/getting_started/installation/cpu-x86.md
1	1	docs/source/getting_started/installation/xpu.md
2	2	docs/source/getting_started/quickstart.md
1	1	docs/source/getting_started/troubleshooting.md
1	1	docs/source/models/extensions/tensorizer.md
2	2	docs/source/models/generative_models.md
3	3	docs/source/models/pooling_models.md
1	1	docs/source/serving/distributed_serving.md
8	8	docs/source/serving/multimodal_inputs.md
5	5	docs/source/serving/openai_compatible_server.md
0	0	examples/{ => offline_inference}/aqlm_example.py
0	0	examples/{ => offline_inference}/cpu_offload.py
2	1	examples/{ => offline_inference}/florence2_inference.py
0	0	examples/{ => offline_inference}/gguf_inference.py
0	0	examples/{ => offline_inference}/llm_engine_example.py
0	0	examples/{ => offline_inference}/lora_with_quantization_inference.py
0	0	examples/{ => offline_inference}/multilora_inference.py
0	0	examples/{ => offline_inference}/offline_chat_with_tools.py
0	0	examples/{ => offline_inference}/offline_inference.py
0	0	examples/{ => offline_inference}/offline_inference_arctic.py
0	0	examples/{ => offline_inference}/offline_inference_audio_language.py
0	0	examples/{ => offline_inference}/offline_inference_chat.py
0	0	examples/{ => offline_inference}/offline_inference_classification.py
0	0	examples/{ => offline_inference}/offline_inference_cli.py
0	0	examples/{ => offline_inference}/offline_inference_distributed.py
0	0	examples/{ => offline_inference}/offline_inference_embedding.py
0	0	examples/{ => offline_inference}/offline_inference_encoder_decoder.py
0	0	examples/{ => offline_inference}/offline_inference_mlpspeculator.py
0	0	examples/{ => offline_inference}/offline_inference_neuron.py
0	0	examples/{ => offline_inference}/offline_inference_neuron_int8_quantization.py
9	9	examples/{ => offline_inference/offline_inference_openai}/offline_inference_openai.md
0	0	examples/{ => offline_inference/offline_inference_openai}/openai_example_batch.jsonl
0	0	examples/{ => offline_inference}/offline_inference_pixtral.py
0	0	examples/{ => offline_inference}/offline_inference_scoring.py
0	0	examples/{ => offline_inference}/offline_inference_structured_outputs.py
0	0	examples/{ => offline_inference}/offline_inference_tpu.py
0	0	examples/{ => offline_inference}/offline_inference_vision_language.py
0	0	examples/{ => offline_inference}/offline_inference_vision_language_embedding.py
0	0	examples/{ => offline_inference}/offline_inference_vision_language_multi_image.py
0	0	examples/{ => offline_inference}/offline_inference_whisper.py
0	0	examples/{ => offline_inference}/offline_inference_with_default_generation_config.py
0	0	examples/{ => offline_inference}/offline_inference_with_prefix.py
0	0	examples/{ => offline_inference}/offline_inference_with_profiler.py
1	1	examples/{ => offline_inference}/offline_profile.py
0	0	examples/{ => offline_inference}/save_sharded_state.py
0	0	examples/{ => online_serving}/api_client.py
0	0	examples/{ => online_serving}/chart-helm/.helmignore
0	0	examples/{ => online_serving}/chart-helm/Chart.yaml
21	0	examples/online_serving/chart-helm/README.md
0	0	examples/{ => online_serving}/chart-helm/ct.yaml
0	0	examples/{ => online_serving}/chart-helm/lintconf.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/_helpers.tpl
0	0	examples/{ => online_serving}/chart-helm/templates/configmap.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/custom-objects.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/deployment.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/hpa.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/job.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/poddisruptionbudget.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/pvc.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/secrets.yaml
0	0	examples/{ => online_serving}/chart-helm/templates/service.yaml
0	0	examples/{ => online_serving}/chart-helm/values.schema.json
0	0	examples/{ => online_serving}/chart-helm/values.yaml
0	0	examples/{ => online_serving}/disaggregated_prefill.sh
0	0	examples/{ => online_serving}/gradio_openai_chatbot_webserver.py
0	0	examples/{ => online_serving}/gradio_webserver.py
0	0	examples/{ => online_serving}/openai_chat_completion_client.py
0	0	examples/{ => online_serving}/openai_chat_completion_client_for_multimodal.py
0	0	examples/{ => online_serving}/openai_chat_completion_client_with_tools.py
0	0	examples/{ => online_serving}/openai_chat_completion_structured_outputs.py
0	0	examples/{ => online_serving}/openai_chat_embedding_client_for_multimodal.py
0	0	examples/{ => online_serving}/openai_completion_client.py
0	0	examples/{ => online_serving}/openai_cross_encoder_score.py
0	0	examples/{ => online_serving}/openai_embedding_client.py
0	0	examples/{ => online_serving}/openai_pooling_client.py
0	0	examples/{ => online_serving}/opentelemetry/Otel.md
0	0	examples/{ => online_serving}/opentelemetry/dummy_client.py
0	0	examples/{ => online_serving}/prometheus_grafana/README.md
0	0	examples/{ => online_serving}/prometheus_grafana/docker-compose.yaml
0	0	examples/{ => online_serving}/prometheus_grafana/grafana.json
0	0	examples/{ => online_serving}/prometheus_grafana/prometheus.yaml
0	0	examples/{ => online_serving}/run_cluster.sh
0	0	examples/{ => online_serving}/sagemaker-entrypoint.sh
5	5	examples/{ => other}/fp8/README.md
0	0	examples/{ => other}/fp8/extract_scales.py
0	0	examples/{ => other}/fp8/quantizer/README.md
0	0	examples/{ => other}/fp8/quantizer/quantize.py
0	0	examples/{ => other}/logging_configuration.md
5	5	examples/{ => other}/tensorize_vllm_model.py
1	1	pyproject.toml
1	1	tests/plugins_tests/test_platform_plugins.py
2	2	tests/tensorizer_loader/test_tensorizer.py
1	1	tools/profiler/print_layerwise_table.py
5	5	tools/profiler/visualize_layerwise_profile.py
1	1	vllm/distributed/kv_transfer/README.md
6	5	vllm/model_executor/model_loader/loader.py
7	7	vllm/model_executor/model_loader/tensorizer.py
2	1	vllm/model_executor/model_loader/weight_utils.py

[2a0596bc4] Cyrus Leung 2025-01-08 [VLM] Reorganize profiling/processing-related code (#11812)
14	27	tests/models/decoder_only/vision_language/processing/test_llava_next.py
14	27	tests/models/decoder_only/vision_language/processing/test_llava_onevision.py
10	14	tests/models/decoder_only/vision_language/processing/test_phi3v.py
8	14	tests/models/decoder_only/vision_language/processing/test_qwen2_vl.py
25	27	tests/multimodal/test_processing.py
7	3	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
1	1	vllm/inputs/preprocess.py
3	1	vllm/inputs/registry.py
23	24	vllm/model_executor/models/aria.py
19	20	vllm/model_executor/models/blip2.py
24	23	vllm/model_executor/models/chameleon.py
40	40	vllm/model_executor/models/fuyu.py
89	86	vllm/model_executor/models/llava.py
33	22	vllm/model_executor/models/llava_next.py
54	50	vllm/model_executor/models/llava_next_video.py
52	63	vllm/model_executor/models/llava_onevision.py
43	40	vllm/model_executor/models/phi3v.py
25	24	vllm/model_executor/models/qwen2_audio.py
63	50	vllm/model_executor/models/qwen2_vl.py
23	23	vllm/model_executor/models/ultravox.py
85	134	vllm/multimodal/processing.py
117	35	vllm/multimodal/profiling.py
61	12	vllm/multimodal/registry.py

[f12141170] youkaichao 2025-01-08 [torch.compile] consider relevant code in compilation cache (#11614)
62	8	vllm/compilation/backends.py
27	1	vllm/compilation/decorators.py
3	26	vllm/config.py
7	0	vllm/sequence.py

[cfd3219f5] Wallas Henrique 2025-01-08 [Hardware][Apple] Native support for macOS Apple Silicon (#11696)
46	15	cmake/cpu_extension.cmake
59	2	csrc/cpu/cpu_types_arm.hpp
18	5	csrc/cpu/utils.cpp
51	0	docs/source/getting_started/installation/cpu-apple.md
2	2	docs/source/getting_started/installation/cpu-arm.md
1	0	docs/source/getting_started/installation/index.md
3	3	requirements-cpu.txt
7	2	setup.py
12	0	vllm/config.py
3	0	vllm/entrypoints/openai/api_server.py
7	0	vllm/utils.py

[a1b2b8606] Simon Mo 2025-01-07 [Docs] Update sponsor name: 'Novita' to 'Novita AI' (#11833)
1	1	README.md
1	1	docs/source/community/sponsors.md

[ad9f1aa67] youkaichao 2025-01-08 [doc] update wheels url (#11830)
2	2	docs/source/getting_started/installation/gpu-cuda.md
1	1	python_only_dev.py
1	1	setup.py

[889e662ea] youkaichao 2025-01-08 [misc] improve memory profiling (#11809)
18	1	tests/test_utils.py
2	1	tests/vllm_test_utils/vllm_test_utils/__init__.py
68	0	tests/vllm_test_utils/vllm_test_utils/monitor.py
6	6	vllm/utils.py

[ef68eb28d] Cyrus Leung 2025-01-08 [Bug] Fix pickling of `ModelConfig` when RunAI Model Streamer is used (#11825)
6	6	vllm/config.py

[259abd895] Simon Mo 2025-01-07 [Docs] reorganize sponsorship page (#11639)
10	5	README.md
10	4	docs/source/community/sponsors.md

[f645eb695] Jee Jee Li 2025-01-08 [Bugfix] Add checks for LoRA and CPU offload (#11810)
6	0	vllm/config.py

[f4923cb8b] Ilya Lavrenov 2025-01-08 [OpenVINO] Fixed Docker.openvino build (#11732)
1	0	Dockerfile.openvino

[b640b19cc] Nishidha 2025-01-08 Fixed docker build for ppc64le (#11518)
2	3	Dockerfile.ppc64le

[dc71af0a7] WangErXiao 2025-01-08 Remove the duplicate imports of MultiModalKwargs and PlaceholderRange… (#11824)
0	2	vllm/v1/core/scheduler.py

[4d29e91be] Divakar Verma 2025-01-07 [Misc] sort torch profiler table by kernel timing (#11813)
1	1	benchmarks/benchmark_latency.py

[91445c7bc] Cyrus Leung 2025-01-08 [Bugfix] Fix image input for Pixtral-HF (#11741)
36	5	examples/offline_inference_vision_language_multi_image.py
6	0	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/pixtral.py
9	0	vllm/model_executor/models/utils.py

[5950f555a] Harry Mellor 2025-01-08 [Doc] Group examples into categories (#11782)
1	4	.gitignore
4	0	docs/Makefile
1	0	docs/requirements-docs.txt
4	0	docs/source/conf.py
222	42	docs/source/generate_examples.py
0	8	docs/source/getting_started/examples/examples_index.template.md
3	3	examples/fp8/README.md
0	0	examples/{production_monitoring => opentelemetry}/Otel.md
0	0	examples/{production_monitoring => opentelemetry}/dummy_client.py
5	5	examples/{production_monitoring => prometheus_grafana}/README.md
0	0	examples/{production_monitoring => prometheus_grafana}/docker-compose.yaml
0	0	examples/{production_monitoring => prometheus_grafana}/grafana.json
0	0	examples/{production_monitoring => prometheus_grafana}/prometheus.yaml

[a4e2b2685] Jie Fu (傅杰) 2025-01-08 [Bugfix] Significant performance drop on CPUs with --num-scheduler-steps > 1 (#11794)
6	0	vllm/engine/arg_utils.py

[973f5dc58] sroy745 2025-01-07 [Doc]Add documentation for using EAGLE in vLLM (#11417)
66	0	docs/source/features/spec_decode.md

[c994223d5] jiangjiadi 2025-01-08 [Bugfix] update the prefix for qwen2 (#11795)
1	1	vllm/model_executor/models/qwen2.py

[869579a70] youkaichao 2025-01-08 [optimization] remove python function call for custom op (#11750)
0	4	vllm/_custom_ops.py
11	6	vllm/model_executor/layers/activation.py
2	2	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
2	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[c0efe92d8] Cyrus Leung 2025-01-07 [Doc] Add note to `gte-Qwen2` models (#11808)
3	0	docs/source/models/supported_models.md

[d9fa1c05a] youkaichao 2025-01-07 [doc] update how pip can install nightly wheels (#11806)
6	6	docs/source/getting_started/installation/gpu-cuda.md

[2de197bdd] Roger Wang 2025-01-07 [V1] Support audio language models on V1 (#11733)
2	2	docs/source/models/supported_models.md
6	3	vllm/model_executor/models/qwen2_audio.py
21	7	vllm/model_executor/models/ultravox.py

[869e829b8] youkaichao 2025-01-07 [doc] add doc to explain how to use uv (#11773)
52	15	docs/source/getting_started/installation/gpu-cuda.md

[8f37be38e] Cyrus Leung 2025-01-07 [Bugfix] Comprehensively test and fix LLaVA-NeXT feature size calculation (#11800)
1	0	requirements-test.in
4	0	requirements-test.txt
108	21	tests/models/decoder_only/vision_language/processing/test_llava_next.py
107	22	tests/models/decoder_only/vision_language/processing/test_llava_onevision.py
15	22	vllm/model_executor/models/llava_next.py
18	24	vllm/model_executor/models/llava_onevision.py

[8082ad795] Roger Wang 2025-01-07 [V1][Doc] Update V1 support for `LLaVa-NeXT-Video` (#11798)
1	1	docs/source/models/supported_models.md

[1e4ce295a] Yuan 2025-01-07 [CI][CPU] adding build number to docker image name (#11788)
11	11	.buildkite/run-cpu-test.sh

[ce1917fcf] Russell Bryant 2025-01-07 [Doc] Create a vulnerability management team (#9925)
1	1	SECURITY.md
43	0	docs/source/contributing/vulnerability_management.md
1	0	docs/source/index.md

[e512f76a8] XiaobingZhang 2025-01-07 fix init error for MessageQueue when n_local_reader is zero (#11768)
2	1	vllm/distributed/device_communicators/shm_broadcast.py

[898cdf033] Liangfu Chen 2025-01-06 [CI] Fix neuron CI and run offline tests (#11779)
27	26	.buildkite/run-neuron-test.sh
6	2	Dockerfile.neuron
2	9	examples/offline_inference_neuron.py

[0f3f3c86e] Roger Wang 2025-01-06 [Bugfix] Update attention interface in `Whisper` (#11784)
11	13	vllm/model_executor/models/whisper.py

[b27855793] Jee Jee Li 2025-01-07 [Kernel][LoRA]Punica prefill  kernels fusion (#11234)
2	1	.buildkite/test-pipeline.yaml
0	77	tests/lora/test_minicpmv.py
44	19	tests/lora/test_minicpmv_tp.py
80	86	tests/lora/test_punica_sizes.py
81	87	tests/lora/test_punica_variation.py
123	21	tests/lora/utils.py
128	77	vllm/lora/ops/sgmv_expand.py
0	241	vllm/lora/ops/sgmv_expand_slice.py
74	55	vllm/lora/ops/sgmv_shrink.py
120	1	vllm/lora/ops/utils.py
55	99	vllm/lora/punica_wrapper/punica_gpu.py

[8ceffbf31] Cyrus Leung 2025-01-07 [Doc][3/N] Reorganize Serving section (#11766)
1	1	README.md
-	-	docs/source/{serving => assets/deployment}/architecture_helm_deployment.png
1	1	docs/source/contributing/dockerfile/dockerfile.md
2	2	docs/source/contributing/model/registration.md
2	2	docs/source/{serving/deploying_with_docker.md => deployment/docker.md}
2	2	docs/source/{serving/deploying_with_bentoml.md => deployment/frameworks/bentoml.md}
2	2	docs/source/{serving/deploying_with_cerebrium.md => deployment/frameworks/cerebrium.md}
2	2	docs/source/{serving/deploying_with_dstack.md => deployment/frameworks/dstack.md}
3	3	docs/source/{serving/deploying_with_helm.md => deployment/frameworks/helm.md}
13	0	docs/source/deployment/frameworks/index.md
2	2	docs/source/{serving/deploying_with_lws.md => deployment/frameworks/lws.md}
4	4	docs/source/{serving/run_on_sky.md => deployment/frameworks/skypilot.md}
2	2	docs/source/{serving/deploying_with_triton.md => deployment/frameworks/triton.md}
9	0	docs/source/deployment/integrations/index.md
2	2	docs/source/{serving/deploying_with_kserve.md => deployment/integrations/kserve.md}
2	2	docs/source/{serving/deploying_with_kubeai.md => deployment/integrations/kubeai.md}
2	2	docs/source/{serving/serving_with_llamastack.md => deployment/integrations/llamastack.md}
2	2	docs/source/{serving/deploying_with_k8s.md => deployment/k8s.md}
1	1	docs/source/{serving/deploying_with_nginx.md => deployment/nginx.md}
1	1	docs/source/design/arch_overview.md
6	2	docs/source/features/disagg_prefill.md
1	1	docs/source/features/spec_decode.md
1	1	docs/source/getting_started/installation/gpu-rocm.md
1	1	docs/source/getting_started/installation/hpu-gaudi.md
10	8	docs/source/getting_started/quickstart.md
28	21	docs/source/index.md
8	0	docs/source/models/extensions/index.md
1	1	docs/source/{serving => models/extensions}/runai_model_streamer.md
1	1	docs/source/{serving => models/extensions}/tensorizer.md
22	22	docs/source/models/supported_models.md
6	6	docs/source/serving/distributed_serving.md
0	17	docs/source/serving/integrations.md
8	0	docs/source/serving/integrations/index.md
4	4	docs/source/serving/{serving_with_langchain.md => integrations/langchain.md}
4	4	docs/source/serving/{serving_with_llamaindex.md => integrations/llamaindex.md}
1	1	docs/source/serving/metrics.md
6	6	docs/source/{features => serving}/multimodal_inputs.md
79	0	docs/source/serving/offline_inference.md
5	3	docs/source/serving/openai_compatible_server.md
1	1	docs/source/serving/usage_stats.md

[d93d2d74f] YiSheng5 2025-01-07 [XPU] Make pp group initilized for pipeline-parallelism (#11648)
6	0	vllm/worker/xpu_worker.py

[d0169e1b0] Cyrus Leung 2025-01-07 [Model] Future-proof Qwen2-Audio multi-modal processor (#11776)
4	2	vllm/model_executor/models/qwen2_audio.py

[08fb75c72] Cyrus Leung 2025-01-07 [Bugfix] Fix LLaVA-NeXT feature size precision error (for real) (#11772)
2	1	tests/models/decoder_only/vision_language/processing/test_llava_next.py
2	1	tests/models/decoder_only/vision_language/processing/test_llava_onevision.py
19	20	vllm/model_executor/models/llava_next.py
24	23	vllm/model_executor/models/llava_onevision.py

[91b361ae8] Roger Wang 2025-01-06 [V1] Extend beyond image modality and support mixed-modality inference with Llava-OneVision (#11685)
1	1	docs/source/models/supported_models.md
208	1	tests/multimodal/test_utils.py
11	7	tests/v1/core/test_kv_cache_utils.py
11	6	tests/v1/core/test_prefix_caching.py
5	1	vllm/model_executor/models/interfaces.py
33	32	vllm/model_executor/models/llava_onevision.py
0	3	vllm/model_executor/models/molmo.py
3	0	vllm/multimodal/__init__.py
100	0	vllm/multimodal/hasher.py
6	3	vllm/multimodal/inputs.py
30	62	vllm/multimodal/processing.py
85	1	vllm/multimodal/utils.py
10	8	vllm/v1/engine/__init__.py
0	67	vllm/v1/engine/mm_input_mapper.py
74	27	vllm/v1/engine/processor.py
21	27	vllm/v1/request.py
38	36	vllm/v1/worker/gpu_model_runner.py

[e20c92bb6] Chen Zhang 2025-01-07 [Kernel] Move attn_type to Attention.__init__() (#11690)
49	51	tests/kernels/test_encoder_decoder_attn.py
7	5	tests/kernels/utils.py
1	1	vllm/attention/backends/abstract.py
7	7	vllm/attention/backends/blocksparse_attn.py
3	1	vllm/attention/backends/flash_attn.py
7	8	vllm/attention/backends/flashinfer.py
7	6	vllm/attention/backends/hpu_attn.py
6	6	vllm/attention/backends/ipex_attn.py
7	6	vllm/attention/backends/pallas.py
7	7	vllm/attention/backends/rocm_flash_attn.py
3	1	vllm/attention/backends/torch_sdpa.py
4	2	vllm/attention/backends/xformers.py
10	27	vllm/attention/layer.py
11	33	vllm/model_executor/models/bart.py
3	7	vllm/model_executor/models/bert.py
4	7	vllm/model_executor/models/mllama.py
16	19	vllm/model_executor/models/qwen2.py
7	7	vllm/v1/attention/backends/flash_attn.py

[32c9eff2f] Jee Jee Li 2025-01-06 [Bugfix][V1] Fix molmo text-only inputs (#11676)
10	0	tests/models/decoder_only/vision_language/test_models.py
96	3	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
17	39	vllm/model_executor/models/molmo.py

[4ca5d40ad] youkaichao 2025-01-06 [doc] explain how to add interleaving sliding window support (#11771)
13	0	docs/source/contributing/model/basic.md

[9279b9f83] Roger Wang 2025-01-06 [Bugfix] Fix max image size for LLaVA-Onevision (#11769)
18	2	vllm/model_executor/models/llava_onevision.py

[ee77fdb5d] Cyrus Leung 2025-01-06 [Doc][2/N] Reorganize Models and Usage sections (#11755)
1	1	.github/ISSUE_TEMPLATE/600-new-model.yml
-	-	docs/source/assets/{usage => features}/disagg_prefill/abstraction.jpg
-	-	docs/source/assets/{usage => features}/disagg_prefill/overview.jpg
102	0	docs/source/contributing/model/basic.md
26	0	docs/source/contributing/model/index.md
2	6	docs/source/{models/enabling_multimodal_inputs.md => contributing/model/multimodal.md}
56	0	docs/source/contributing/model/registration.md
4	2	docs/source/{automatic_prefix_caching/details.md => design/automatic_prefix_caching.md}
2	0	docs/source/design/kernel/paged_attention.md
1	0	docs/source/dev/offline_inference/offline_index.md
4	4	docs/source/{automatic_prefix_caching/apc.md => features/automatic_prefix_caching.md}
3	3	docs/source/{usage => features}/compatibility_matrix.md
2	2	docs/source/{usage => features}/disagg_prefill.md
0	0	docs/source/{usage => features}/lora.md
0	0	docs/source/{usage => features}/multimodal_inputs.md
0	0	docs/source/{ => features}/quantization/auto_awq.md
0	0	docs/source/{ => features}/quantization/bnb.md
0	0	docs/source/{ => features}/quantization/fp8.md
0	0	docs/source/{ => features}/quantization/fp8_e4m3_kvcache.md
0	0	docs/source/{ => features}/quantization/fp8_e5m2_kvcache.md
0	0	docs/source/{ => features}/quantization/gguf.md
19	0	docs/source/features/quantization/index.md
0	0	docs/source/{ => features}/quantization/int8.md
5	5	docs/source/{ => features}/quantization/supported_hardware.md
0	0	docs/source/{usage => features}/spec_decode.md
0	0	docs/source/{usage => features}/structured_outputs.md
0	0	docs/source/{usage => features}/tool_calling.md
22	44	docs/source/index.md
0	155	docs/source/models/adding_model.md
1	1	docs/source/models/supported_models.md
2	2	docs/source/{usage/performance.md => performance/optimization.md}
0	0	docs/source/{usage => serving}/engine_args.md
0	0	docs/source/{usage => serving}/env_vars.md
1	1	docs/source/serving/openai_compatible_server.md
0	0	docs/source/{usage => serving}/usage_stats.md
1	1	vllm/attention/backends/rocm_flash_attn.py
3	3	vllm/config.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/engine/output_processor/multi_step.py
1	1	vllm/executor/cpu_executor.py
1	1	vllm/platforms/cpu.py
1	1	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/utils.py
1	1	vllm/worker/multi_step_model_runner.py
1	1	vllm/worker/utils.py

[996357e48] Cyrus Leung 2025-01-06 [VLM] Separate out profiling-related logic (#11746)
4	3	tests/multimodal/test_processing.py
47	32	vllm/model_executor/models/aria.py
44	34	vllm/model_executor/models/blip2.py
43	29	vllm/model_executor/models/chameleon.py
50	35	vllm/model_executor/models/fuyu.py
115	66	vllm/model_executor/models/llava.py
38	37	vllm/model_executor/models/llava_next.py
86	62	vllm/model_executor/models/llava_next_video.py
98	76	vllm/model_executor/models/llava_onevision.py
59	45	vllm/model_executor/models/phi3v.py
59	37	vllm/model_executor/models/qwen2_audio.py
144	87	vllm/model_executor/models/qwen2_vl.py
55	36	vllm/model_executor/models/ultravox.py
8	29	vllm/model_executor/models/vision.py
43	109	vllm/multimodal/processing.py
121	0	vllm/multimodal/profiling.py
1	1	vllm/multimodal/registry.py

[2a622d704] Suraj Deshmukh 2025-01-06 k8s-config: Update the secret to use stringData (#11679)
1	1	docs/source/serving/deploying_with_k8s.md

[9c749713f] Lucas Tucker 2025-01-06 [mypy] Forward pass function type hints in lora (#11740)
9	3	vllm/lora/layers.py
2	1	vllm/lora/models.py
3	1	vllm/model_executor/layers/linear.py

[022c5c694] Rui Qiao 2025-01-05 [V1] Refactor get_executor_cls (#11754)
3	3	tests/v1/engine/test_engine_core.py
3	3	tests/v1/engine/test_engine_core_client.py
1	20	vllm/v1/engine/async_llm.py
1	19	vllm/v1/engine/llm_engine.py
18	1	vllm/v1/executor/abstract.py

[f8fcca100] Rui Qiao 2025-01-05 [Misc] Fix typo for valid_tool_parses  (#11753)
3	3	vllm/entrypoints/openai/api_server.py

[06bfb5196] Woosuk Kwon 2025-01-06 [V1] Add BlockTable class (#11693)
78	0	vllm/v1/worker/block_table.py
9	16	vllm/v1/worker/gpu_input_batch.py
7	9	vllm/v1/worker/gpu_model_runner.py

[408e56001] Cody Yu 2025-01-05 [Bugfix] Remove block size constraint (#11723)
0	5	vllm/config.py

[402d37836] Cyrus Leung 2025-01-06 [Doc] [1/N] Reorganize Getting Started section (#11645)
1	2	docs/source/design/arch_overview.md
1	1	docs/source/design/multiprocessing.md
0	0	docs/source/{usage => getting_started}/faq.md
1	1	docs/source/getting_started/{arm-installation.md => installation/cpu-arm.md}
3	3	docs/source/getting_started/{cpu-installation.md => installation/cpu-x86.md}
2	2	docs/source/getting_started/{installation.md => installation/gpu-cuda.md}
1	1	docs/source/getting_started/{amd-installation.md => installation/gpu-rocm.md}
3	1	docs/source/getting_started/{gaudi-installation.md => installation/hpu-gaudi.md}
19	0	docs/source/getting_started/installation/index.md
1	1	docs/source/getting_started/{neuron-installation.md => installation/neuron.md}
2	2	docs/source/getting_started/{openvino-installation.md => installation/openvino.md}
1	1	docs/source/getting_started/{tpu-installation.md => installation/tpu.md}
1	1	docs/source/getting_started/{xpu-installation.md => installation/xpu.md}
1	1	docs/source/getting_started/quickstart.md
6	5	docs/source/getting_started/{debugging.md => troubleshooting.md}
4	12	docs/source/index.md
1	1	docs/source/models/generative_models.md
1	1	docs/source/models/pooling_models.md
1	1	docs/source/serving/distributed_serving.md
2	2	docs/source/usage/spec_decode.md
1	1	docs/source/usage/structured_outputs.md
1	1	vllm/utils.py

[9e764e7b1] cennn 2025-01-06 [distributed] remove pynccl's redundant change_state (#11749)
27	37	tests/distributed/test_pynccl.py
0	17	vllm/distributed/device_communicators/pynccl.py
1	8	vllm/distributed/parallel_state.py

[33fc1e2e8] Robert Shaw 2025-01-05 [Frontend] Improve `StreamingResponse` Exception Handling (#11752)
2	2	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py

[eba17173d] Lancer 2025-01-06 fix: [doc] fix typo (#11751)
1	1	vllm/core/block/block_table.py

[635b89724] cennn 2025-01-05 [distributed] remove pynccl's redundant stream (#11744)
2	3	tests/distributed/test_pynccl.py
9	19	vllm/distributed/device_communicators/pynccl.py
1	2	vllm/distributed/parallel_state.py

[4068f4b5b] Lu Fang 2025-01-04 [MISC] Replace c10::optional with std::optional (#11730)
2	2	csrc/attention/paged_attention_v1.cu
2	2	csrc/attention/paged_attention_v2.cu
4	4	csrc/cpu/attention.cpp
5	5	csrc/cpu/quant.cpp
3	3	csrc/cpu/torch_bindings.cpp
3	3	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp
3	3	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
1	1	csrc/cutlass_extensions/torch_utils.hpp
12	12	csrc/mamba/causal_conv1d/causal_conv1d.cu
11	11	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
23	23	csrc/ops.h
2	2	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
9	9	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
3	3	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
15	15	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
1	1	csrc/quantization/machete/generate.py
5	5	csrc/quantization/machete/machete_mm_kernel.cuh
12	12	csrc/quantization/machete/machete_mm_launcher.cuh
1	1	csrc/quantization/machete/machete_prepack_launcher.cuh
13	13	csrc/quantization/machete/machete_pytorch.cu
2	2	csrc/rocm/attention.cu
1	1	csrc/rocm/ops.h
1	1	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu
2	2	csrc/sparse/cutlass/sparse_scaled_mm_entry.cu

[47831430c] Jee Jee Li 2025-01-05 [Bugfix][V1] Fix test_kv_cache_utils.py (#11738)
5	5	tests/v1/core/test_kv_cache_utils.py

[65c08928c] Cyrus Leung 2025-01-04 [Model] Remove unnecessary weight initialization logic (#11736)
4	16	vllm/model_executor/layers/resampler.py
1	4	vllm/model_executor/models/aria.py
0	2	vllm/model_executor/models/minicpmv.py

[ba214dffb] Cyrus Leung 2025-01-04 [Bugfix] Fix precision error in LLaVA-NeXT (#11735)
1	2	tests/models/decoder_only/vision_language/processing/test_llava_next.py
10	4	vllm/model_executor/models/llava_next.py
15	8	vllm/model_executor/models/llava_onevision.py

[eed11ebee] Cyrus Leung 2025-01-04 [VLM] Merged multi-modal processors for LLaVA-NeXT-Video and LLaVA-OneVision (#11717)
0	0	tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/__init__.py
0	0	tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_idefics3.py
0	0	tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_internvl.py
58	0	tests/models/decoder_only/vision_language/processing/test_llava_next.py
59	0	tests/models/decoder_only/vision_language/processing/test_llava_onevision.py
20	24	tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_phi3v.py
0	0	tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_qwen.py
13	26	tests/models/decoder_only/vision_language/{mm_processor_kwargs => processing}/test_qwen2_vl.py
2	7	tests/models/decoder_only/vision_language/test_models.py
0	127	tests/models/decoder_only/vision_language/test_qwen2_vl.py
102	68	tests/multimodal/test_processing.py
3	2	vllm/model_executor/models/aria.py
3	2	vllm/model_executor/models/blip2.py
3	2	vllm/model_executor/models/chameleon.py
7	4	vllm/model_executor/models/clip.py
3	2	vllm/model_executor/models/fuyu.py
33	42	vllm/model_executor/models/llava.py
8	7	vllm/model_executor/models/llava_next.py
134	139	vllm/model_executor/models/llava_next_video.py
239	292	vllm/model_executor/models/llava_onevision.py
17	9	vllm/model_executor/models/phi3v.py
7	4	vllm/model_executor/models/pixtral.py
8	7	vllm/model_executor/models/qwen2_audio.py
115	84	vllm/model_executor/models/qwen2_vl.py
7	4	vllm/model_executor/models/siglip.py
4	7	vllm/model_executor/models/ultravox.py
34	3	vllm/model_executor/models/vision.py
14	0	vllm/multimodal/parse.py
206	120	vllm/multimodal/processing.py
2	1	vllm/multimodal/registry.py
13	0	vllm/transformers_utils/tokenizer.py

[300acb834] Yan Burman 2025-01-04 [Core][Bugfix] Use correct device to initialize GPU data during CUDA-graph-capture (#11233)
1	1	tests/distributed/test_custom_all_reduce.py
1	1	tests/distributed/test_pynccl.py
4	3	vllm/distributed/parallel_state.py
1	1	vllm/v1/worker/gpu_model_runner.py
16	9	vllm/worker/model_runner.py

[d91457d52] xcnick 2025-01-04 [V1] Add kv cache utils tests. (#11513)
241	0	tests/v1/core/test_kv_cache_utils.py

[fbf256455] Kunshang Ji 2025-01-04 [V1] Add `RayExecutor` support for `AsyncLLM` (api server) (#11712)
6	1	vllm/v1/engine/async_llm.py

[d1d49397e] Alberto Ferrer 2025-01-04 Update bnb.md with example for OpenAI (#11718)
7	0	docs/source/quantization/bnb.md

[9c93636d8] Hust_YangXian 2025-01-04 Update tool_calling.md (#11701)
1	1	docs/source/usage/tool_calling.md

[e5d7ed0c5] WangErXiao 2025-01-04 [V1] log GPU blocks num for MultiprocExecutor (#11656)
1	0	vllm/v1/executor/multiproc_executor.py

[ad0d567e1] Robert Shaw 2025-01-03 [V1] Chore: cruft removal (#11724)
0	2	vllm/entrypoints/llm.py
0	2	vllm/v1/engine/core_client.py
0	4	vllm/v1/engine/llm_engine.py
0	3	vllm/v1/engine/processor.py

[bf0d97d78] Michael Goin 2025-01-03 Update requirements-tpu.txt to support python 3.9 and 3.11 (#11695)
3	1	requirements-tpu.txt
4	2	vllm/worker/tpu_model_runner.py

[a655eb302] Jee Jee Li 2025-01-04 [Misc]Add BNB quantization for Qwen2VL (#11719)
40	29	vllm/model_executor/models/qwen2_vl.py

[1543914c0] Robert Shaw 2025-01-03 [V1] Improve TP>1 Error Handling + Stack Trace (#11721)
0	16	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/core.py
18	1	vllm/v1/engine/core_client.py
21	3	vllm/v1/executor/multiproc_executor.py

[61fed92c7] ZincCat 2025-01-03 [Bugfix] Fix ColumnParallelLinearWithLoRA slice (#11708)
2	2	vllm/lora/layers.py

[80c751e7f] Robert Shaw 2025-01-03 [V1] Simplify Shutdown (#11659)
0	6	tests/v1/engine/test_engine_core_client.py
0	5	vllm/entrypoints/llm.py
0	3	vllm/v1/engine/async_llm.py
0	1	vllm/v1/engine/core.py
18	16	vllm/v1/engine/core_client.py
0	7	vllm/v1/engine/llm_engine.py
24	22	vllm/v1/utils.py

[e1a5c2f0a] Aurick Qiao 2025-01-03 [Model] Whisper model implementation (#11280)
2	0	.buildkite/test-pipeline.yaml
59	0	examples/offline_inference_whisper.py
0	0	tests/models/encoder_decoder/audio_language/__init__.py
136	0	tests/models/encoder_decoder/audio_language/test_whisper.py
1	0	tests/models/registry.py
2	0	vllm/config.py
28	8	vllm/inputs/preprocess.py
1	0	vllm/model_executor/models/registry.py
737	0	vllm/model_executor/models/whisper.py
7	21	vllm/multimodal/processing.py
15	3	vllm/sequence.py
19	0	vllm/transformers_utils/tokenizer.py
4	2	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
18	10	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
11	5	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
5	6	vllm/worker/enc_dec_model_runner.py

[fd3a62a12] Kevin H. Luu 2025-01-03 [perf-benchmark] Fix dependency for steps in benchmark pipeline (#11710)
4	3	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[07064cb1d] Lu Fang 2025-01-02 [Bugfix] Check chain_speculative_sampling before calling it (#11673)
1	1	vllm/model_executor/layers/rejection_sampler.py

[2f1e8e8f5] Sachin Varghese 2025-01-02 Update default max_num_batch_tokens for chunked prefill (#11694)
4	5	docs/source/usage/performance.md

[68d37809b] Nathan Azrak 2025-01-03 [Misc] Minimum requirements for SageMaker compatibility (#11576)
11	2	Dockerfile
24	0	examples/sagemaker-entrypoint.sh
60	1	vllm/entrypoints/openai/api_server.py

[5dba25750] wchen61 2025-01-03 Resolve race conditions in Marlin kernel (#11493)
21	19	csrc/quantization/gptq_marlin/gptq_marlin.cu

[187e32997] bjmsong 2025-01-03 [Bugfix] Change kv scaling factor by param json on nvidia gpu (#11688)
3	2	vllm/model_executor/models/exaone.py
3	2	vllm/model_executor/models/granite.py
3	2	vllm/model_executor/models/llama.py
3	2	vllm/model_executor/models/solar.py
2	1	vllm/worker/model_runner.py

[b55ed6ef8] Woosuk Kwon 2025-01-03 [V1][Minor] Optimize token_ids_cpu copy (#11692)
8	5	vllm/v1/worker/gpu_input_batch.py
1	0	vllm/v1/worker/gpu_model_runner.py

[2f385183f] Kathy Yu 2025-01-02 [Bugfix] Free cross attention block table for preempted-for-recompute sequence group. (#10013)
1	0	vllm/core/scheduler.py

[84c35c374] Chunyang Wen 2025-01-03 According to vllm.EngineArgs, the name should be distributed_executor_backend (#11689)
1	1	docs/source/serving/distributed_serving.md

[8c38ee700] Cyrus Leung 2025-01-03 [VLM] Merged multi-modal processor for LLaVA-NeXT (#11682)
0	70	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_llava_next.py
0	118	tests/multimodal/test_mapper.py
97	0	tests/multimodal/test_processing.py
1	3	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
25	0	vllm/model_executor/models/clip.py
3	3	vllm/model_executor/models/fuyu.py
215	119	vllm/model_executor/models/llava.py
112	209	vllm/model_executor/models/llava_next.py
14	10	vllm/model_executor/models/phi3v.py
52	14	vllm/model_executor/models/pixtral.py
25	0	vllm/model_executor/models/siglip.py
1	1	vllm/model_executor/models/utils.py
52	0	vllm/model_executor/models/vision.py
8	4	vllm/multimodal/parse.py

[b6087a6be] Tobias Pitters 2025-01-02 [mypy] Pass type checking in vllm/inputs (#11680)
1	0	tools/mypy.sh
11	10	vllm/inputs/data.py
3	3	vllm/inputs/preprocess.py
1	1	vllm/inputs/registry.py

[23c1b10a4] Cyrus Leung 2025-01-02 [VLM][Bugfix] Multi-modal processor compatible with V1 multi-input (#11674)
116	136	vllm/multimodal/inputs.py
18	27	vllm/multimodal/processing.py
17	5	vllm/v1/engine/processor.py

[a115ac46b] Cyrus Leung 2025-01-01 [VLM] Move supported limits and max tokens to merged multi-modal processor (#11669)
2	37	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py
2	34	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py
8	6	tests/multimodal/test_processing.py
1	7	vllm/inputs/registry.py
42	33	vllm/model_executor/models/aria.py
9	10	vllm/model_executor/models/blip2.py
16	19	vllm/model_executor/models/chameleon.py
46	59	vllm/model_executor/models/fuyu.py
6	2	vllm/model_executor/models/llava.py
25	20	vllm/model_executor/models/phi3v.py
30	12	vllm/model_executor/models/qwen2_audio.py
33	42	vllm/model_executor/models/qwen2_vl.py
9	17	vllm/model_executor/models/ultravox.py
8	39	vllm/multimodal/parse.py
98	13	vllm/multimodal/processing.py
5	0	vllm/multimodal/registry.py

[73001445f] Woosuk Kwon 2025-01-01 [V1] Implement Cascade Attention (#11635)
1	1	CMakeLists.txt
7	0	tests/conftest.py
182	0	tests/kernels/test_cascade_flash_attn.py
71	0	tests/system_messages/sonnet3.5_nov2024.txt
0	0	tests/v1/e2e/__init__.py
22	0	tests/v1/e2e/test_cascade_attention.py
254	13	vllm/v1/attention/backends/flash_attn.py
51	1	vllm/v1/core/kv_cache_manager.py
10	0	vllm/v1/core/scheduler.py
95	1	vllm/v1/worker/gpu_model_runner.py

[6d70198b1] Kazuhiro Serizawa 2025-01-01 [Doc] Fix typo (#11666)
1	1	vllm/model_executor/layers/rejection_sampler.py
1	1	vllm/v1/sample/ops/topk_topp_sampler.py

[f962f426b] Lu Fang 2024-12-31 [Misc] Replace space with - in the file names (#11667)
0	0	.github/ISSUE_TEMPLATE/{400-bug report.yml => 400-bug-report.yml}
0	0	.github/ISSUE_TEMPLATE/{500-feature request.yml => 500-feature-request.yml}
0	0	.github/ISSUE_TEMPLATE/{600-new model.yml => 600-new-model.yml}
0	0	.github/ISSUE_TEMPLATE/{700-performance discussion.yml => 700-performance-discussion.yml}
0	0	.github/ISSUE_TEMPLATE/{800-misc discussion.yml => 800-misc-discussion.yml}

[11d8a091c] Jee Jee Li 2025-01-01 [Misc] Optimize Qwen2-VL LoRA test (#11663)
2	3	tests/lora/test_qwen2vl.py
19	1	vllm/model_executor/models/qwen2_vl.py

[365801fed] Cyrus Leung 2025-01-01 [VLM] Add max-count checking in data parser for single image models (#11661)
1	1	docs/source/models/supported_models.md
2	1	tests/multimodal/test_processing.py
4	0	vllm/model_executor/models/blip2.py
4	0	vllm/model_executor/models/chameleon.py
11	7	vllm/model_executor/models/fuyu.py
26	2	vllm/multimodal/parse.py

[4db72e57f] Joe Runde 2024-12-31 [Bugfix][Refactor] Unify model management in frontend (#11660)
1	1	tests/entrypoints/openai/test_cli_args.py
29	3	tests/entrypoints/openai/test_lora_lineage.py
10	10	tests/entrypoints/openai/test_serving_chat.py
32	34	tests/entrypoints/openai/{test_serving_engine.py => test_serving_models.py}
33	29	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/cli_args.py
10	5	vllm/entrypoints/openai/run_batch.py
5	11	vllm/entrypoints/openai/serving_chat.py
5	11	vllm/entrypoints/openai/serving_completion.py
4	5	vllm/entrypoints/openai/serving_embedding.py
13	179	vllm/entrypoints/openai/serving_engine.py
210	0	vllm/entrypoints/openai/serving_models.py
4	5	vllm/entrypoints/openai/serving_pooling.py
4	5	vllm/entrypoints/openai/serving_score.py
4	8	vllm/entrypoints/openai/serving_tokenization.py

[0c6f99855] Yihua Cheng 2024-12-31 [Benchmark] Add benchmark script for CPU offloading  (#11533)
184	0	benchmarks/benchmark_long_document_qa_throughput.py

[e7c7c5e82] Roger Wang 2024-12-31 [V1][VLM] V1 support for selected single-image models. (#11632)
5	5	docs/source/models/supported_models.md
8	2	examples/offline_inference_vision_language.py
2	5	tests/models/decoder_only/vision_language/test_models.py
16	13	tests/multimodal/test_processing.py
80	89	vllm/model_executor/models/aria.py
0	92	vllm/model_executor/models/blip.py
84	88	vllm/model_executor/models/blip2.py
81	110	vllm/model_executor/models/chameleon.py
199	182	vllm/model_executor/models/fuyu.py
4	2	vllm/model_executor/models/idefics2_vision_model.py
2	2	vllm/model_executor/models/llava.py
2	4	vllm/model_executor/models/llava_next.py
6	6	vllm/model_executor/models/pixtral.py
8	6	vllm/model_executor/models/qwen2_audio.py
9	8	vllm/model_executor/models/qwen2_vl.py
8	5	vllm/model_executor/models/ultravox.py
55	13	vllm/multimodal/processing.py
7	3	vllm/multimodal/utils.py
14	1	vllm/v1/worker/gpu_model_runner.py

[8c3230d8c] Chen Zhang 2024-12-31 [V1] Simpify vision block hash for prefix caching by removing offset from hash (#11646)
4	4	tests/v1/core/test_prefix_caching.py
2	2	vllm/v1/core/kv_cache_utils.py

[2c5718809] sakunkun 2024-12-31 [Bugfix] Move the _touch(computed_blocks) call in the allocate_slots method to after the check for allocating new blocks. (#11565)
61	2	tests/v1/core/test_prefix_caching.py
13	6	vllm/v1/core/kv_cache_manager.py

[82c49d326] John Giorgi 2024-12-31 [Misc][LoRA] Support Rank Stabilized LoRA (RSLoRA) (#6909)
13	7	tests/lora/test_lora_manager.py
3	9	vllm/lora/lora.py
1	1	vllm/lora/models.py
13	5	vllm/lora/peft_helper.py

[74fa1d123] Michael Goin 2024-12-30 [Bugfix] Fix OpenAI parallel sampling when using xgrammar (#11637)
6	8	tests/entrypoints/openai/test_completion.py
5	0	vllm/model_executor/guided_decoding/xgrammar_decoding.py
5	4	vllm/sampling_params.py
1	1	vllm/sequence.py

[a2a40bcd0] Matthias Vogler 2024-12-31 [Model][LoRA]LoRA support added for MolmoForCausalLM (#11439)
1	1	docs/source/models/supported_models.md
42	3	vllm/model_executor/models/molmo.py

[ccb1aabcc] Kevin H. Luu 2024-12-30 [benchmark] Remove dependency for H100 benchmark step (#11572)
1	1	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[36e767004] whyiug 2024-12-31 [Bugfix] Validate and concatenate image embeddings in MiniCPMVBaseModel (#11631)
6	0	vllm/model_executor/models/minicpmv.py

[5886aa496] Robert Shaw 2024-12-30 [V1] [6/N] API Server: Better Shutdown (#11586)
12	32	vllm/entrypoints/openai/api_server.py
22	3	vllm/v1/engine/async_llm.py
6	10	vllm/v1/engine/core_client.py

[8d9b6721e] Cyrus Leung 2024-12-30 [VLM] Abstract out multi-modal data parsing in merged processor (#11620)
2	2	.buildkite/test-pipeline.yaml
2	2	vllm/model_executor/models/chatglm.py
11	7	vllm/model_executor/models/llava.py
12	7	vllm/model_executor/models/phi3v.py
9	13	vllm/model_executor/models/qwen2_audio.py
72	81	vllm/model_executor/models/qwen2_vl.py
9	13	vllm/model_executor/models/ultravox.py
4	5	vllm/multimodal/__init__.py
2	2	vllm/multimodal/audio.py
4	4	vllm/multimodal/base.py
2	2	vllm/multimodal/image.py
52	143	vllm/multimodal/inputs.py
344	0	vllm/multimodal/parse.py
33	29	vllm/multimodal/processing.py
2	2	vllm/multimodal/video.py

[b12e87f94] youkaichao 2024-12-30 [platforms] enable platform plugins (#11602)
19	6	.buildkite/test-pipeline.yaml
4	2	docs/source/design/plugin_system.md
1	1	tests/conftest.py
8	8	tests/kernels/test_attention_selector.py
11	0	tests/plugins/vllm_add_dummy_platform/setup.py
5	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/__init__.py
5	0	tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py
16	0	tests/plugins_tests/test_platform_plugins.py
12	3	vllm/config.py
2	1	vllm/distributed/parallel_state.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/executor/ray_utils.py
2	1	vllm/model_executor/guided_decoding/__init__.py
1	1	vllm/model_executor/models/registry.py
2	2	vllm/model_executor/utils.py
210	110	vllm/platforms/__init__.py
41	31	vllm/plugins/__init__.py
1	1	vllm/spec_decode/metrics.py
1	1	vllm/usage/usage_lib.py
6	2	vllm/utils.py
2	3	vllm/worker/model_runner_base.py
1	0	vllm/worker/multi_step_model_runner.py
8	6	vllm/worker/worker_base.py

[5dbf85455] Li, Jiang 2024-12-30 [CI/Build][CPU] Fix CPU CI by lazy importing triton FP8 kernels (#11618)
3	2	vllm/model_executor/layers/quantization/fp8.py

[970d6d077] Tyler Michael Smith 2024-12-30 [Build][Kernel] Update CUTLASS to v3.6.0 (#11607)
2	2	CMakeLists.txt
9	9	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
4	4	csrc/quantization/machete/generate.py
4	6	csrc/quantization/machete/machete_collective_builder.cuh
4	7	csrc/quantization/machete/machete_mainloop.cuh
2	3	csrc/quantization/machete/machete_prepacked_layout.cuh

[628ec6c17] Liangfu Chen 2024-12-29 [Docker] bump up neuron sdk v2.21 (#11593)
3	3	Dockerfile.neuron
2	2	requirements-neuron.txt
1	2	vllm/_custom_ops.py
0	1	vllm/triton_utils/importing.py

[3682e33f9] youkaichao 2024-12-30 [v1] fix compilation cache (#11598)
13	2	tests/compile/piecewise/test_toy_llama.py
13	9	vllm/compilation/backends.py
42	3	vllm/config.py
1	0	vllm/v1/worker/gpu_worker.py

[0aa38d16f] Michael Goin 2024-12-29 Remove print statement in DeepseekScalingRotaryEmbedding (#11604)
0	1	vllm/model_executor/layers/rotary_embedding.py

[faef77c0d] Kuntai Du 2024-12-29 [Misc] KV cache transfer connector registry (#11481)
0	8	vllm/config.py
38	10	vllm/distributed/kv_transfer/kv_connector/factory.py

[dba4d9dec] youkaichao 2024-12-29 [v1][bugfix] fix cudagraph with inplace buffer assignment (#11596)
9	1	vllm/compilation/wrapper.py
1	10	vllm/model_executor/layers/rotary_embedding.py

[32b4c63f0] Cyrus Leung 2024-12-29 [Doc] Convert list tables to MyST (#11594)
1	1	docs/source/getting_started/debugging.md
19	20	docs/source/getting_started/gaudi-installation.md
26	27	docs/source/getting_started/tpu-installation.md
598	608	docs/source/models/supported_models.md
113	114	docs/source/quantization/supported_hardware.md
204	205	docs/source/serving/deploying_with_helm.md

[4fb8e329f] Robert Shaw 2024-12-28 [V1] [5/N] API Server: unify `Detokenizer` and  `EngineCore` input (#11545)
35	22	tests/v1/engine/test_detokenizer.py
1	15	vllm/v1/engine/__init__.py
8	6	vllm/v1/engine/async_llm.py
11	10	vllm/v1/engine/detokenizer.py
7	5	vllm/v1/engine/llm_engine.py
4	19	vllm/v1/engine/processor.py

[328841d00] youkaichao 2024-12-29 [bugfix] interleaving sliding window for cohere2 model (#11583)
1	1	docs/source/models/supported_models.md
0	4	tests/models/test_initialization.py
1	1	vllm/config.py
6	4	vllm/model_executor/models/commandr.py
4	3	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
192	0	vllm/transformers_utils/configs/cohere2.py

[d427e5cfd] Cyrus Leung 2024-12-28 [Doc] Minor documentation fixes (#11580)
3	3	docs/source/contributing/dockerfile/dockerfile.md
1	1	docs/source/contributing/overview.md
1	1	docs/source/getting_started/arm-installation.md
2	2	docs/source/getting_started/cpu-installation.md
5	3	docs/source/getting_started/gaudi-installation.md
1	1	docs/source/getting_started/neuron-installation.md
2	2	docs/source/getting_started/quickstart.md
1	1	docs/source/getting_started/tpu-installation.md
3	3	docs/source/models/supported_models.md
3	3	docs/source/serving/deploying_with_cerebrium.md
1	1	docs/source/serving/deploying_with_dstack.md
3	3	docs/source/serving/distributed_serving.md
1	1	docs/source/serving/runai_model_streamer.md

[42bb201fd] Woosuk Kwon 2024-12-28 [V1][Minor] Set pin_memory=False for token_ids_cpu tensor (#11581)
3	1	vllm/v1/worker/gpu_input_batch.py

[59d6bb4c8] hj-wei 2024-12-28 [Hardware][AMD]: Replace HIPCC version with more precise ROCm version (#11515)
29	23	setup.py

[b7dcc003d] Roger Wang 2024-12-28 [Model] Remove hardcoded image tokens ids from Pixtral (#11582)
13	14	vllm/model_executor/models/pixtral.py

[d34be24bb] Isotr0py 2024-12-28 [Model] Support InternLM2 Reward models (#11571)
5	0	docs/source/models/supported_models.md
2	0	tests/models/registry.py
59	1	vllm/model_executor/models/internlm2.py
1	0	vllm/model_executor/models/registry.py

[b5cbe8eeb] Rajveer Bachkaniwala 2024-12-27 [Bugfix] Last token measurement fix (#11376)
6	2	vllm/engine/llm_engine.py
14	10	vllm/sequence.py

[df04dffad] Robert Shaw 2024-12-27 [V1] [4/N] API Server: ZMQ/MP Utilities (#11541)
1	0	docs/requirements-docs.txt
4	9	tests/v1/engine/test_engine_core.py
4	6	tests/v1/engine/test_engine_core_client.py
10	1	vllm/entrypoints/openai/api_server.py
1	21	vllm/executor/multiproc_worker_utils.py
87	3	vllm/utils.py
3	3	vllm/v1/engine/async_llm.py
22	89	vllm/v1/engine/core.py
47	45	vllm/v1/engine/core_client.py
3	3	vllm/v1/engine/llm_engine.py
5	6	vllm/v1/executor/multiproc_executor.py
60	29	vllm/v1/utils.py

[a60731247] Chen Zhang 2024-12-28 [Doc] Update mllama example based on official doc (#11567)
14	1	examples/offline_inference_vision_language.py

[ac7979940] Selali 2024-12-27 [Bugfix] Fix for ROCM compressed tensor support (#11561)
7	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py

[dde1fa18c] Isotr0py 2024-12-28 [Misc] Improve BNB loader to handle mixture of sharded and merged weights with same suffix (#11566)
5	2	vllm/model_executor/model_loader/loader.py

[0240402c4] Jee Jee Li 2024-12-28 [Misc]Add BNB quantization for MolmoForCausalLM  (#11551)
18	8	vllm/model_executor/model_loader/loader.py
65	25	vllm/model_executor/models/molmo.py

[55509c211] ErezSC42 2024-12-27 [MODEL] LoRA support for Jamba model (#11209)
24	0	tests/lora/conftest.py
54	0	tests/lora/test_jamba.py
18	4	vllm/model_executor/layers/mamba/mamba_mixer.py
26	24	vllm/model_executor/models/jamba.py
10	4	vllm/model_executor/models/mamba.py

[101418096] Cyrus Leung 2024-12-28 [VLM] Support caching in merged multi-modal processor (#11396)
2	1	docs/source/conf.py
12	12	docs/source/design/multimodal/multimodal_index.md
1	2	docs/source/models/supported_models.md
2	2	tests/entrypoints/openai/test_vision_embedding.py
1	1	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py
3	1	tests/models/decoder_only/vision_language/test_models.py
203	6	tests/multimodal/test_processing.py
11	11	vllm/inputs/registry.py
110	68	vllm/model_executor/models/llava.py
82	25	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/qwen.py
40	25	vllm/model_executor/models/qwen2_audio.py
78	37	vllm/model_executor/models/qwen2_vl.py
51	25	vllm/model_executor/models/ultravox.py
23	21	vllm/multimodal/base.py
423	15	vllm/multimodal/inputs.py
347	169	vllm/multimodal/processing.py
30	20	vllm/multimodal/registry.py
9	3	vllm/transformers_utils/processor.py
25	2	vllm/utils.py

[5ce4627a7] Chen1022 2024-12-27 [Doc]  Add xgrammar in doc (#11549)
1	1	docs/source/usage/structured_outputs.md

[7af553ea3] Cyrus Leung 2024-12-27 [Misc] Abstract the logic for reading and writing media content (#11527)
1	0	tests/entrypoints/openai/test_serving_chat.py
1	5	tests/entrypoints/test_chat_utils.py
34	25	tests/multimodal/test_utils.py
2	4	vllm/assets/audio.py
57	72	vllm/entrypoints/chat_utils.py
35	1	vllm/multimodal/audio.py
29	9	vllm/multimodal/base.py
40	1	vllm/multimodal/image.py
211	266	vllm/multimodal/utils.py
83	4	vllm/multimodal/video.py

[2c9b8ea2b] Jee Jee Li 2024-12-27 [Bugfix] Fix TeleChat2ForCausalLM weights mapper (#11546)
13	13	vllm/model_executor/models/telechat2.py

[d003f3ea3] AlexHe99 2024-12-27 Update deploying_with_k8s.md with AMD ROCm GPU example (#11465)
78	1	docs/source/serving/deploying_with_k8s.md

[6c6f7fe8a] Mengqing Cao 2024-12-27 [Platform] Move model arch check to platform (#11503)
1	36	vllm/model_executor/models/registry.py
12	0	vllm/platforms/interface.py
38	1	vllm/platforms/rocm.py

[2339d59f9] Robert Shaw 2024-12-27 [BugFix] Fix quantization for all other methods (#11547)
15	4	vllm/model_executor/layers/fused_moe/layer.py
7	3	vllm/model_executor/layers/quantization/awq_marlin.py
15	7	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
7	3	vllm/model_executor/layers/quantization/experts_int8.py
1	2	vllm/model_executor/layers/quantization/fp8.py
7	3	vllm/model_executor/layers/quantization/gptq_marlin.py

[1b875a0ef] Robert Shaw 2024-12-27 [V1][3/N] API Server: Reduce Task Switching + Handle Abort Properly (#11534)
62	97	vllm/v1/engine/async_llm.py
0	55	vllm/v1/engine/async_stream.py
1	1	vllm/v1/engine/core.py

[eb881ed00] youkaichao 2024-12-27 [misc] fix typing (#11540)
4	3	vllm/compilation/backends.py

[46d435945] Robert Shaw 2024-12-26 [CI] Fix broken CI (#11543)
2	0	tests/models/registry.py

[81b979f2a] Woosuk Kwon 2024-12-27 [V1] Fix yapf (#11538)
13	11	vllm/v1/sample/ops/penalties.py
8	8	vllm/v1/sample/sampler.py

[371d04d39] Woosuk Kwon 2024-12-27 [V1] Use FlashInfer Sampling Kernel for Top-P & Top-K Sampling (#11394)
22	32	tests/v1/sample/test_sampler.py
3	2	vllm/envs.py
0	0	vllm/v1/sample/ops/__init__.py
57	0	vllm/v1/sample/ops/penalties.py
201	0	vllm/v1/sample/ops/topk_topp_sampler.py
72	156	vllm/v1/sample/sampler.py

[0c0c2015c] Robert Shaw 2024-12-26 Update openai_compatible_server.md (#11536)
7	1	docs/source/serving/openai_compatible_server.md

[82d24f7aa] Simon Mo 2024-12-26 [Docs] Document Deepseek V3 support (#11535)
1	1	README.md
6	1	docs/source/models/supported_models.md

[f49777ba6] Simon Mo 2024-12-26 Deepseek v3 (#11502)
141	19	csrc/moe/moe_align_sum_kernels.cu
9	2	vllm/config.py
14	3	vllm/model_executor/layers/fused_moe/fused_moe.py
66	36	vllm/model_executor/layers/fused_moe/layer.py
6	1	vllm/model_executor/layers/quantization/fp8.py
650	0	vllm/model_executor/models/deepseek_v3.py
1	0	vllm/model_executor/models/registry.py

[55fb97f7b] Robert Shaw 2024-12-26 [2/N] API Server: Avoid ulimit footgun (#11530)
3	1	vllm/entrypoints/api_server.py
5	1	vllm/entrypoints/openai/api_server.py
18	0	vllm/utils.py

[2072924d1] Michael Goin 2024-12-26 [Model] [Quantization] Support deepseek_v3 w8a8 fp8 block-wise quantization (#11523)
265	0	tests/kernels/test_block_fp8.py
7	7	vllm/config.py
103	28	vllm/model_executor/layers/fused_moe/fused_moe.py
6	1	vllm/model_executor/layers/fused_moe/layer.py
21	2	vllm/model_executor/layers/linear.py
167	32	vllm/model_executor/layers/quantization/fp8.py
353	0	vllm/model_executor/layers/quantization/utils/fp8_utils.py
9	0	vllm/model_executor/parameter.py

[720b10fdc] Robert Shaw 2024-12-26 [1/N] API Server  (Remove Proxy) (#11529)
12	6	vllm/entrypoints/openai/api_server.py
5	1	vllm/entrypoints/openai/cli_args.py

[b85a97782] Isotr0py 2024-12-27 [Doc] Add video example to openai client for multimodal (#11521)
49	3	docs/source/usage/multimodal_inputs.md
65	8	examples/openai_chat_completion_client_for_multimodal.py

[eec906d81] Cyrus Leung 2024-12-26 [Misc] Add placeholder module (#11501)
7	2	tests/tensorizer_loader/test_tensorizer.py
12	7	vllm/assets/audio.py
3	4	vllm/assets/base.py
1	2	vllm/assets/image.py
3	6	vllm/assets/video.py
1	9	vllm/config.py
1	10	vllm/model_executor/model_loader/loader.py
14	11	vllm/model_executor/model_loader/tensorizer.py
9	8	vllm/model_executor/model_loader/weight_utils.py
6	18	vllm/multimodal/audio.py
16	10	vllm/multimodal/utils.py
1	12	vllm/multimodal/video.py
6	1	vllm/transformers_utils/s3_utils.py
63	0	vllm/utils.py

[f57ee5650] Jee Jee Li 2024-12-26 [Model]  Modify MolmoForCausalLM MLP  (#11510)
24	18	vllm/model_executor/models/molmo.py

[dcb1a944d] sroy745 2024-12-26 [V1] Adding min tokens/repetition/presence/frequence penalties to V1 sampler (#10681)
38	0	tests/v1/engine/test_engine_core.py
0	0	tests/v1/sample/__init__.py
331	0	tests/v1/sample/test_sampler.py
0	0	tests/v1/worker/__init__.py
224	0	tests/v1/worker/test_gpu_input_batch.py
6	45	vllm/model_executor/layers/sampler.py
57	0	vllm/model_executor/layers/utils.py
11	1	vllm/v1/sample/metadata.py
63	2	vllm/v1/sample/sampler.py
142	0	vllm/v1/worker/gpu_input_batch.py
7	1	vllm/v1/worker/gpu_model_runner.py

[7492a3620] Roger Wang 2024-12-26 [Doc] Add `QVQ` and `QwQ` to the list of supported models (#11509)
2	2	docs/source/models/supported_models.md

[aa25985bd] Jee Jee Li 2024-12-26 [Misc][LoRA] Fix LoRA weight mapper (#11495)
10	5	tests/lora/test_lora_checkpoints.py
5	1	tests/lora/test_qwen2vl.py
2	1	vllm/lora/models.py
12	22	vllm/lora/utils.py
2	0	vllm/lora/worker_manager.py

[dbeac95db] Lucas Tucker 2024-12-25 Mypy checking for vllm/compilation (#11496)
5	5	vllm/compilation/backends.py
2	1	vllm/compilation/multi_output_match.py
2	2	vllm/compilation/pass_manager.py

[51a624bf0] Cyrus Leung 2024-12-26 [Misc] Move some multimodal utils to modality-specific modules (#11494)
1	1	tests/models/decoder_only/vision_language/test_awq.py
1	1	tests/models/decoder_only/vision_language/test_h2ovl.py
1	1	tests/models/decoder_only/vision_language/test_phi3v.py
2	2	tests/models/decoder_only/vision_language/test_qwen2_vl.py
3	2	tests/models/decoder_only/vision_language/vlm_utils/builders.py
3	2	tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
1	1	tests/models/encoder_decoder/vision_language/test_mllama.py
1	1	tests/multimodal/test_mapper.py
1	1	vllm/assets/video.py
12	0	vllm/multimodal/audio.py
12	0	vllm/multimodal/image.py
3	65	vllm/multimodal/utils.py
43	0	vllm/multimodal/video.py

[6ad909fdd] Cyrus Leung 2024-12-26 [Doc] Improve GitHub links (#11491)
29	0	docs/source/conf.py
2	2	docs/source/contributing/dockerfile/dockerfile.md
7	7	docs/source/contributing/overview.md
4	4	docs/source/contributing/profiling/profiling_index.md
6	11	docs/source/design/arch_overview.md
14	13	docs/source/design/multiprocessing.md
1	2	docs/source/generate_examples.py
2	2	docs/source/getting_started/amd-installation.md
2	2	docs/source/getting_started/cpu-installation.md
4	3	docs/source/getting_started/debugging.md
2	4	docs/source/getting_started/gaudi-installation.md
1	1	docs/source/getting_started/installation.md
4	3	docs/source/getting_started/quickstart.md
1	2	docs/source/getting_started/tpu-installation.md
1	1	docs/source/getting_started/xpu-installation.md
4	4	docs/source/models/adding_model.md
6	6	docs/source/models/enabling_multimodal_inputs.md
3	3	docs/source/models/generative_models.md
3	3	docs/source/models/pooling_models.md
2	2	docs/source/models/supported_models.md
2	2	docs/source/performance/benchmarks.md
1	1	docs/source/quantization/supported_hardware.md
1	1	docs/source/serving/deploying_with_docker.md
2	2	docs/source/serving/distributed_serving.md
6	17	docs/source/serving/openai_compatible_server.md
17	17	docs/source/usage/compatibility_matrix.md
1	2	docs/source/usage/lora.md
12	12	docs/source/usage/multimodal_inputs.md
4	4	docs/source/usage/spec_decode.md
2	2	docs/source/usage/structured_outputs.md
1	1	docs/source/usage/usage_stats.md

[b689ada91] Cyrus Leung 2024-12-26 [Frontend] Enable decord to load video from base64 (#11492)
19	20	vllm/multimodal/utils.py

[fc601665e] Jiaxin Shan 2024-12-24 [Misc] Update disaggregation benchmark scripts and test logs (#11456)
7	6	benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh
6	7	benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
2	2	tests/kv_transfer/test_lookup_buffer.py
7	2	tests/kv_transfer/test_lookup_buffer.sh
14	11	tests/kv_transfer/test_send_recv.py
7	1	tests/kv_transfer/test_send_recv.sh

[9832e5572] Rui Qiao 2024-12-24 [V1] Unify VLLM_ENABLE_V1_MULTIPROCESSING handling in RayExecutor (#11472)
0	5	tests/basic_correctness/test_basic_correctness.py
0	2	vllm/v1/engine/llm_engine.py
4	1	vllm/v1/executor/ray_executor.py

[3f3e92e1f] Cyrus Leung 2024-12-25 [Model] Automatic conversion of classification and reward models (#11469)
12	10	docs/source/models/supported_models.md
1	4	tests/models/embedding/language/test_cls_models.py
2	2	tests/models/embedding/language/test_scoring.py
7	4	tests/models/test_registry.py
8	2	vllm/model_executor/model_loader/utils.py
170	20	vllm/model_executor/models/adapters.py
2	2	vllm/model_executor/models/qwen2.py
0	104	vllm/model_executor/models/qwen2_cls.py
4	13	vllm/model_executor/models/registry.py

[409475a82] Yuan Tang 2024-12-24 [Bugfix] Fix issues in CPU build Dockerfile. Fixes #9182 (#11435)
3	3	Dockerfile.cpu
7	3	setup.py

[196c34b0a] Jee Jee Li 2024-12-24 [Misc] Move weights mapper (#11443)
3	2	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
10	10	vllm/model_executor/models/aria.py
2	2	vllm/model_executor/models/bert.py
30	28	vllm/model_executor/models/molmo.py
8	8	vllm/model_executor/models/phi3v.py
3	2	vllm/model_executor/models/qwen2.py
14	13	vllm/model_executor/models/telechat2.py
4	3	vllm/model_executor/models/ultravox.py

[5c7963249] Mengqing Cao 2024-12-24 [attn][tiny fix] fix attn backend in MultiHeadAttention (#11463)
1	0	vllm/attention/layer.py

[461cde208] Ilya Lavrenov 2024-12-24 [OpenVINO] Fixed installation conflicts (#11458)
2	2	requirements-openvino.txt

[7a5286cc0] Isotr0py 2024-12-24 [Bugfix][Hardware][CPU] Fix CPU `input_positions` creation for text-only inputs with mrope (#11434)
7	9	vllm/worker/cpu_model_runner.py

[b1b1038fb] Jee Jee Li 2024-12-24 [Bugfix] Fix Qwen2-VL LoRA weight loading  (#11430)
5	0	tests/lora/conftest.py
30	0	tests/lora/test_lora_checkpoints.py
78	0	tests/lora/test_qwen2vl.py
6	3	vllm/lora/models.py
33	4	vllm/lora/utils.py
10	1	vllm/lora/worker_manager.py
6	6	vllm/model_executor/models/qwen2_vl.py

[9edca6bf8] Cyrus Leung 2024-12-24 [Frontend] Online Pooling API (#11457)
3	15	docs/source/models/generative_models.md
4	18	docs/source/models/pooling_models.md
40	8	docs/source/serving/openai_compatible_server.md
1	1	examples/openai_cross_encoder_score.py
51	0	examples/openai_pooling_client.py
46	22	tests/entrypoints/openai/test_embedding.py
238	0	tests/entrypoints/openai/test_pooling.py
17	21	tests/entrypoints/openai/test_vision_embedding.py
66	7	vllm/entrypoints/openai/api_server.py
19	0	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/run_batch.py
45	34	vllm/entrypoints/openai/serving_embedding.py
234	0	vllm/entrypoints/openai/serving_pooling.py
42	29	vllm/entrypoints/openai/serving_score.py
2	1	vllm/outputs.py

[4f074fbf5] dpxa 2024-12-24 [Misc]Suppress irrelevant exception stack trace information when CUDA… (#11438)
2	1	vllm/entrypoints/llm.py

[a491d6f53] Rui Qiao 2024-12-23 [V1] TP Ray executor (#11107)
1	1	tests/basic_correctness/test_basic_correctness.py
6	1	vllm/v1/engine/llm_engine.py
339	0	vllm/v1/executor/ray_executor.py
271	0	vllm/v1/executor/ray_utils.py
0	1	vllm/v1/worker/gpu_worker.py

[32aa2059a] Rafael Vasquez 2024-12-23 [Docs] Convert rST to MyST (Markdown) (#11145)
2	0	.gitignore
1	1	Dockerfile
1	1	docs/requirements-docs.txt
102	0	docs/source/automatic_prefix_caching/apc.md
0	110	docs/source/automatic_prefix_caching/apc.rst
15	0	docs/source/community/meetups.md
0	16	docs/source/community/meetups.rst
1	1	docs/source/conf.py
50	0	docs/source/contributing/dockerfile/dockerfile.md
0	50	docs/source/contributing/dockerfile/dockerfile.rst
62	75	docs/source/contributing/{overview.rst => overview.md}
41	0	docs/source/contributing/profiling/profiling_index.md
0	48	docs/source/contributing/profiling/profiling_index.rst
117	133	docs/source/design/{arch_overview.rst => arch_overview.md}
36	0	docs/source/design/huggingface_integration.md
0	40	docs/source/design/huggingface_integration.rst
19	0	docs/source/design/input_processing/input_processing_pipeline.md
0	20	docs/source/design/input_processing/input_processing_pipeline.rst
43	0	docs/source/design/input_processing/model_inputs_index.md
0	39	docs/source/design/input_processing/model_inputs_index.rst
527	0	docs/source/design/kernel/paged_attention.md
0	525	docs/source/design/kernel/paged_attention.rst
16	0	docs/source/design/multimodal/adding_multimodal_plugin.md
0	17	docs/source/design/multimodal/adding_multimodal_plugin.rst
39	22	docs/source/design/multimodal/{multimodal_index.rst => multimodal_index.md}
54	0	docs/source/design/plugin_system.md
0	62	docs/source/design/plugin_system.rst
3	2	docs/source/dev/engine/{async_llm_engine.rst => async_llm_engine.md}
17	0	docs/source/dev/engine/engine_index.md
0	13	docs/source/dev/engine/engine_index.rst
3	2	docs/source/dev/engine/{llm_engine.rst => llm_engine.md}
3	2	docs/source/dev/offline_inference/{llm.rst => llm.md}
7	2	docs/source/dev/offline_inference/{llm_inputs.rst => llm_inputs.md}
8	0	docs/source/dev/offline_inference/offline_index.md
0	8	docs/source/dev/offline_inference/offline_index.rst
3	2	docs/source/dev/{pooling_params.rst => pooling_params.md}
3	2	docs/source/dev/{sampling_params.rst => sampling_params.md}
9	15	docs/source/generate_examples.py
163	0	docs/source/getting_started/amd-installation.md
0	178	docs/source/getting_started/amd-installation.rst
46	0	docs/source/getting_started/arm-installation.md
0	50	docs/source/getting_started/arm-installation.rst
154	0	docs/source/getting_started/cpu-installation.md
0	164	docs/source/getting_started/cpu-installation.rst
199	0	docs/source/getting_started/debugging.md
0	203	docs/source/getting_started/debugging.rst
8	0	docs/source/getting_started/examples/examples_index.template.md
0	8	docs/source/getting_started/examples/examples_index.template.rst
388	0	docs/source/getting_started/gaudi-installation.md
0	402	docs/source/getting_started/gaudi-installation.rst
199	0	docs/source/getting_started/installation.md
0	214	docs/source/getting_started/installation.rst
132	0	docs/source/getting_started/neuron-installation.md
0	140	docs/source/getting_started/neuron-installation.rst
104	0	docs/source/getting_started/openvino-installation.md
0	116	docs/source/getting_started/openvino-installation.rst
174	0	docs/source/getting_started/quickstart.md
0	181	docs/source/getting_started/quickstart.rst
193	0	docs/source/getting_started/tpu-installation.md
0	200	docs/source/getting_started/tpu-installation.rst
74	0	docs/source/getting_started/xpu-installation.md
0	80	docs/source/getting_started/xpu-installation.rst
200	0	docs/source/index.md
0	194	docs/source/index.rst
155	0	docs/source/models/adding_model.md
0	159	docs/source/models/adding_model.rst
143	0	docs/source/models/enabling_multimodal_inputs.md
0	147	docs/source/models/enabling_multimodal_inputs.rst
138	0	docs/source/models/generative_models.md
0	146	docs/source/models/generative_models.rst
127	0	docs/source/models/pooling_models.md
0	136	docs/source/models/pooling_models.rst
205	196	docs/source/models/{supported_models.rst => supported_models.md}
28	0	docs/source/performance/benchmarks.md
0	33	docs/source/performance/benchmarks.rst
78	0	docs/source/quantization/auto_awq.md
0	79	docs/source/quantization/auto_awq.rst
39	0	docs/source/quantization/bnb.md
0	43	docs/source/quantization/bnb.rst
192	0	docs/source/quantization/fp8.md
0	204	docs/source/quantization/fp8.rst
44	0	docs/source/quantization/fp8_e4m3_kvcache.md
0	47	docs/source/quantization/fp8_e4m3_kvcache.rst
31	0	docs/source/quantization/fp8_e5m2_kvcache.md
0	34	docs/source/quantization/fp8_e5m2_kvcache.rst
72	0	docs/source/quantization/gguf.md
0	73	docs/source/quantization/gguf.rst
136	0	docs/source/quantization/int8.md
0	145	docs/source/quantization/int8.rst
132	132	docs/source/quantization/{supported_hardware.rst => supported_hardware.md}
7	0	docs/source/serving/deploying_with_bentoml.md
0	8	docs/source/serving/deploying_with_bentoml.rst
109	0	docs/source/serving/deploying_with_cerebrium.md
0	112	docs/source/serving/deploying_with_cerebrium.rst
81	0	docs/source/serving/deploying_with_docker.md
0	88	docs/source/serving/deploying_with_docker.rst
102	0	docs/source/serving/deploying_with_dstack.md
0	103	docs/source/serving/deploying_with_dstack.rst
21	23	docs/source/serving/{deploying_with_helm.rst => deploying_with_helm.md}
171	0	docs/source/serving/deploying_with_k8s.md
0	175	docs/source/serving/deploying_with_k8s.rst
7	0	docs/source/serving/deploying_with_kserve.md
0	8	docs/source/serving/deploying_with_kserve.rst
15	0	docs/source/serving/deploying_with_kubeai.md
0	17	docs/source/serving/deploying_with_kubeai.rst
11	0	docs/source/serving/deploying_with_lws.md
0	12	docs/source/serving/deploying_with_lws.rst
133	0	docs/source/serving/deploying_with_nginx.md
0	142	docs/source/serving/deploying_with_nginx.rst
5	0	docs/source/serving/deploying_with_triton.md
0	6	docs/source/serving/deploying_with_triton.rst
105	0	docs/source/serving/distributed_serving.md
0	107	docs/source/serving/distributed_serving.rst
17	0	docs/source/serving/integrations.md
0	17	docs/source/serving/integrations.rst
38	0	docs/source/serving/metrics.md
0	38	docs/source/serving/metrics.rst
11	11	docs/source/serving/openai_compatible_server.md
345	0	docs/source/serving/run_on_sky.md
0	366	docs/source/serving/run_on_sky.rst
53	0	docs/source/serving/runai_model_streamer.md
0	53	docs/source/serving/runai_model_streamer.rst
30	0	docs/source/serving/serving_with_langchain.md
0	31	docs/source/serving/serving_with_langchain.rst
26	0	docs/source/serving/serving_with_llamaindex.md
0	27	docs/source/serving/serving_with_llamaindex.rst
38	0	docs/source/serving/serving_with_llamastack.md
0	42	docs/source/serving/serving_with_llamastack.rst
16	0	docs/source/serving/tensorizer.md
0	15	docs/source/serving/tensorizer.rst
468	0	docs/source/usage/compatibility_matrix.md
0	468	docs/source/usage/compatibility_matrix.rst
64	0	docs/source/usage/disagg_prefill.md
0	69	docs/source/usage/disagg_prefill.rst
8	6	docs/source/usage/{engine_args.rst => engine_args.md}
15	0	docs/source/usage/env_vars.md
0	14	docs/source/usage/env_vars.rst
15	16	docs/source/usage/{faq.rst => faq.md}
215	0	docs/source/usage/lora.md
0	225	docs/source/usage/lora.rst
486	0	docs/source/usage/multimodal_inputs.md
0	492	docs/source/usage/multimodal_inputs.rst
25	26	docs/source/usage/{performance.rst => performance.md}
205	0	docs/source/usage/spec_decode.md
0	210	docs/source/usage/spec_decode.rst
260	0	docs/source/usage/structured_outputs.md
0	267	docs/source/usage/structured_outputs.rst
1	1	docs/source/usage/usage_stats.md
1	1	vllm/attention/backends/rocm_flash_attn.py
3	3	vllm/config.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/engine/llm_engine.py
1	1	vllm/engine/output_processor/multi_step.py
1	1	vllm/entrypoints/llm.py
1	1	vllm/executor/cpu_executor.py
1	1	vllm/inputs/__init__.py
3	3	vllm/inputs/registry.py
1	1	vllm/multimodal/__init__.py
7	7	vllm/multimodal/base.py
1	1	vllm/multimodal/inputs.py
3	3	vllm/multimodal/registry.py
1	1	vllm/platforms/cpu.py
1	1	vllm/scripts.py
1	1	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/utils.py
1	1	vllm/worker/multi_step_model_runner.py
1	1	vllm/worker/utils.py

[94d545a1a] yansh97 2024-12-24 [Doc] Fix typo in the help message of '--guided-decoding-backend' (#11440)
1	1	vllm/engine/arg_utils.py

[60fb4f3bc] Michael Goin 2024-12-23 [Bugfix] Add kv cache scales to gemma2.py (#11269)
17	1	vllm/model_executor/models/gemma2.py

[63afbe921] Michael Goin 2024-12-23 [CI] Expand OpenAI test_chat.py guided decoding tests (#11048)
12	17	tests/entrypoints/openai/test_chat.py

[8cef6e02d] Dipika Sikka 2024-12-23 [Misc] add w8a8 asym models (#11075)
10	6	tests/quantization/test_compressed_tensors.py

[b866cdbd0] Dipika Sikka 2024-12-23 [Misc] Add assertion and helpful message for marlin24 compressed models (#11388)
4	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py

[2e726680b] Yuan Tang 2024-12-23 [Bugfix] torch nightly version in ROCm installation guide (#11423)
1	1	docs/source/getting_started/amd-installation.rst

[5bfb30a52] Michael Goin 2024-12-23 [Bugfix] Fix CFGGuide and use outlines for grammars that can't convert to GBNF (#11389)
0	5	tests/entrypoints/llm/test_guided_generate.py
17	70	vllm/model_executor/guided_decoding/__init__.py
14	9	vllm/model_executor/guided_decoding/outlines_logits_processors.py
70	0	vllm/model_executor/guided_decoding/{xgrammar_utils.py => utils.py}
2	2	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[e51719ae7] Lucas Tucker 2024-12-23 mypy type checking for vllm/worker (#11418)
1	2	vllm/worker/cpu_worker.py
7	6	vllm/worker/multi_step_model_runner.py
1	1	vllm/worker/worker_base.py

[f30581c51] youkaichao 2024-12-23 [misc][perf] remove old code (#11425)
0	51	vllm/_custom_ops.py

[048fc57a0] Simon Mo 2024-12-22 [CI] Unboock H100 Benchmark (#11419)
3	3	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[f1d1bf628] Jason T. Greene 2024-12-22 [Bugfix] Fix fully sharded LoRAs with Mixtral (#11390)
3	1	tests/lora/test_mixtral.py
2	1	vllm/lora/layers.py

[72d9c316d] youkaichao 2024-12-22 [cd][release] fix race conditions (#11407)
17	3	.buildkite/upload-wheels.sh

[4a9139780] youkaichao 2024-12-21 [cd][release] add pypi index for every commit and nightly build (#11404)
24	0	.buildkite/generate_index.py
15	1	.buildkite/upload-wheels.sh

[29c748930] Roger Wang 2024-12-21 [CI] Fix flaky entrypoint tests (#11403)
3	0	tests/entrypoints/openai/test_audio.py
3	0	tests/entrypoints/openai/test_video.py
3	0	tests/entrypoints/openai/test_vision.py

[c2d1b075b] Roger Wang 2024-12-21 [Bugfix] Fix issues for `Pixtral-Large-Instruct-2411` (#11393)
15	8	vllm/model_executor/models/pixtral.py

[584f0ae40] Ricky Xu 2024-12-20 [V1] Make AsyncLLMEngine v1-v0 opaque (#11383)
7	0	vllm/engine/async_llm_engine.py
1	5	vllm/entrypoints/openai/api_server.py
1	5	vllm/v1/engine/async_llm.py

[51ff216d8] George 2024-12-21 [Bugfix] update should_ignore_layer (#11354)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[dd2b5633d] Woosuk Kwon 2024-12-21 [V1][Bugfix] Skip hashing empty or None mm_data (#11386)
4	0	vllm/v1/engine/mm_input_mapper.py

[47a0b615b] Jiaxin Shan 2024-12-20 Add ray[default] to wget to run distributed inference out of box (#11265)
1	1	Dockerfile
1	1	requirements-cuda.txt

[5d2248d81] youkaichao 2024-12-20 [doc] explain nccl requirements for rlhf (#11381)
1	0	docs/source/getting_started/debugging.rst

[d573aeadc] Michael Goin 2024-12-20 [Bugfix] Don't log OpenAI field aliases as ignored (#11378)
9	1	vllm/entrypoints/openai/protocol.py

[995f56236] omer-dayan 2024-12-20 [Core] Loading model from S3 using RunAI Model Streamer as optional loader (#10192)
2	2	Dockerfile
1	0	docs/source/index.rst
53	0	docs/source/serving/runai_model_streamer.rst
1	0	setup.py
0	0	tests/runai_model_streamer/__init__.py
31	0	tests/runai_model_streamer/test_runai_model_streamer_loader.py
39	0	tests/runai_model_streamer/test_weight_utils.py
37	0	vllm/config.py
2	0	vllm/engine/arg_utils.py
117	1	vllm/model_executor/model_loader/loader.py
24	0	vllm/model_executor/model_loader/weight_utils.py
146	0	vllm/transformers_utils/s3_utils.py
4	0	vllm/transformers_utils/utils.py

[7c7aa37c6] Daniele 2024-12-20 [CI/Build] fix pre-compiled wheel install for exact tag (#11373)
1	1	setup.py

[04139ade5] Roger Wang 2024-12-20 [V1] Fix profiling for models with merged input processor (#11370)
32	12	vllm/v1/worker/gpu_model_runner.py

[1ecc645b8] youkaichao 2024-12-19 [doc] backward compatibility for 0.6.4 (#11359)
5	0	docs/source/getting_started/debugging.rst

[c954f21ac] youkaichao 2024-12-19 [misc] add early error message for custom ops (#11355)
11	1	vllm/utils.py

[86c2d8fd1] Wallas Henrique 2024-12-20 [Bugfix] Fix spec decoding when seed is none in a batch (#10863)
63	0	tests/samplers/test_rejection_sampler.py
3	7	vllm/model_executor/layers/rejection_sampler.py

[b880ffb87] Michael Goin 2024-12-19 [Misc] Add tqdm progress bar during graph capture (#11349)
13	5	vllm/worker/model_runner.py

[7801f56ed] youkaichao 2024-12-19 [ci][gh200] dockerfile clean up (#11351)
3	0	.buildkite/run-gh200-test.sh
20	19	Dockerfile
21	9	docs/source/serving/deploying_with_docker.rst
1	1	requirements-build.txt
4	3	requirements-common.txt
0	3	requirements-cuda-arm64.txt
2	2	requirements-cuda.txt

[48edab804] Akash kaothalkar 2024-12-20 [Bugfix][Hardware][POWERPC] Fix auto dtype failure in case of POWER10 (#11331)
12	1	vllm/config.py

[a985f7af9] Yuan 2024-12-20 [CI] Adding CPU docker pipeline (#11261)
15	0	.buildkite/release-pipeline.yaml

[e461c262f] yangzhibin 2024-12-20 [Misc] Remove unused vllm/block.py (#11336)
0	88	vllm/block.py
2	2	vllm/core/evictor.py

[276738ce0] Isotr0py 2024-12-20 [Bugfix] Fix broken CPU compressed-tensors test (#11338)
2	4	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[cdf22afdd] Cyrus Leung 2024-12-20 [Misc] Clean up and consolidate LRUCache (#11339)
4	5	vllm/adapter_commons/models.py
1	1	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
26	33	vllm/utils.py
3	3	vllm/v1/engine/mm_input_mapper.py
0	25	vllm/v1/utils.py

[e24113a8f] Isotr0py 2024-12-20 [Model] Refactor Qwen2-VL to use merged multimodal processor (#11258)
6	2	examples/offline_inference_vision_language.py
65	127	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py
3	1	vllm/model_executor/models/qwen2_audio.py
187	393	vllm/model_executor/models/qwen2_vl.py
16	4	vllm/multimodal/processing.py

[7379b3d4b] Roger Wang 2024-12-19 [V1] Fix multimodal profiling for `Molmo` (#11325)
5	0	vllm/model_executor/models/molmo.py
17	2	vllm/v1/engine/mm_input_mapper.py
1	1	vllm/v1/engine/processor.py
1	1	vllm/v1/worker/gpu_model_runner.py

[6c7f88154] Yehoshua Cohen 2024-12-19 [Model] Add JambaForSequenceClassification model  (#10860)
5	0	docs/source/models/supported_models.rst
1	0	tests/models/registry.py
35	1	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/registry.py
6	1	vllm/worker/pooling_model_runner.py

[a0f7d53be] Cyrus Leung 2024-12-19 [Bugfix] Cleanup Pixtral HF code (#11333)
14	141	vllm/model_executor/models/pixtral.py

[5aef49806] Yanyi Liu 2024-12-19 [Feature] Add load generation config from model (#11164)
30	0	examples/offline_inference_with_default_generation_config.py
61	0	tests/entrypoints/openai/test_serving_chat.py
57	2	vllm/config.py
14	1	vllm/engine/arg_utils.py
4	19	vllm/engine/llm_engine.py
8	1	vllm/entrypoints/llm.py
109	30	vllm/entrypoints/openai/protocol.py
10	2	vllm/entrypoints/openai/serving_chat.py
11	2	vllm/entrypoints/openai/serving_completion.py
3	17	vllm/v1/engine/processor.py

[98356735a] Varun Sundar Rabindranath 2024-12-19 [misc] benchmark_throughput : Add LoRA (#11267)
89	13	benchmarks/benchmark_throughput.py

[f26c4aeec] Rui Qiao 2024-12-18 [Misc] Optimize ray worker initialization time (#11275)
22	13	vllm/executor/ray_gpu_executor.py

[8936316d5] Varun Sundar Rabindranath 2024-12-19 [Kernel] Refactor Cutlass c3x (#10049)
2	0	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp
2	0	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
3	3	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
3	370	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
160	0	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cuh
96	0	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90_fp8_dispatch.cuh
140	0	csrc/quantization/cutlass_w8a8/scaled_mm_c3x_sm90_int8_dispatch.cuh

[6142ef0ad] Cyrus Leung 2024-12-19 [VLM] Merged multimodal processor for Qwen2-Audio (#11303)
6	0	examples/offline_inference_audio_language.py
4	5	tests/models/decoder_only/audio_language/test_ultravox.py
70	30	vllm/inputs/registry.py
2	2	vllm/model_executor/models/llava.py
11	5	vllm/model_executor/models/phi3v.py
113	190	vllm/model_executor/models/qwen2_audio.py
56	57	vllm/model_executor/models/ultravox.py
18	0	vllm/multimodal/audio.py
13	12	vllm/multimodal/inputs.py
100	52	vllm/multimodal/processing.py
23	7	vllm/utils.py

[c6b0a7d3b] Chen Zhang 2024-12-18 [V1] Simplify prefix caching logic by removing `num_evictable_computed_blocks` (#11310)
2	11	vllm/v1/core/kv_cache_manager.py

[a30482f05] Michael Goin 2024-12-18 [CI] Expand test_guided_generate to test all backends (#11313)
69	43	tests/entrypoints/llm/test_guided_generate.py
2	2	tests/model_executor/test_guided_processors.py
58	6	vllm/model_executor/guided_decoding/__init__.py

[17ca96427] Travis Johnson 2024-12-18 [Model] IBM Granite 3.1 (#11307)
2	2	docs/source/models/supported_models.rst
6	1	docs/source/usage/tool_calling.md
9	1	tests/tool_use/utils.py
10	2	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py

[5a9da2e6e] Tyler Michael Smith 2024-12-18 [Bugfix][Build/CI] Fix sparse CUTLASS compilation on CUDA [12.0, 12.2) (#11311)
30	9	CMakeLists.txt
2	0	csrc/ops.h
3	1	csrc/sparse/cutlass/sparse_compressor_c3x.cu
2	2	csrc/sparse/cutlass/sparse_compressor_entry.cu
1	1	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu
13	2	csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
7	0	csrc/torch_bindings.cpp
4	1	tests/kernels/test_semi_structured.py
5	3	tests/quantization/test_compressed_tensors.py
5	0	vllm/_custom_ops.py
6	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py
11	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[fdea8ec16] Alexander Matveev 2024-12-18 [V1] VLM - enable processor cache by default (#11305)
25	25	examples/offline_inference_vision_language.py
5	6	vllm/config.py
5	6	vllm/engine/arg_utils.py
1	1	vllm/v1/core/kv_cache_utils.py
17	3	vllm/v1/engine/mm_input_mapper.py
2	2	vllm/v1/engine/processor.py
17	5	vllm/v1/worker/gpu_model_runner.py

[ca5f54a9b] Joe Runde 2024-12-18 [Bugfix] fix minicpmv test (#11304)
0	1	tests/lora/test_minicpmv.py

[f954fe0e6] Kunshang Ji 2024-12-19 [FIX] update openai version (#11287)
1	1	requirements-common.txt

[362cff1eb] Simon Mo 2024-12-18 [CI][Misc] Remove Github Action Release Workflow (#11274)
62	61	.github/workflows/publish.yml

[996aa70f0] Isotr0py 2024-12-19 [Bugfix] Fix broken phi3-v mm_processor_kwargs tests (#11263)
5	7	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py
12	5	vllm/model_executor/models/phi3v.py

[60508ffda] Dipika Sikka 2024-12-18 [Kernel]: Cutlass 2:4 Sparsity + FP8/Int8 Quant Support (#10995)
16	10	CMakeLists.txt
384	0	benchmarks/cutlass_benchmarks/sparse_benchmarks.py
96	0	benchmarks/cutlass_benchmarks/utils.py
2	26	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
1	1	benchmarks/cutlass_benchmarks/weight_shapes.py
7	0	csrc/core/math.hpp
11	0	csrc/cutlass_extensions/common.cpp
35	0	csrc/cutlass_extensions/common.hpp
2	2	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
9	0	csrc/ops.h
0	27	csrc/quantization/cutlass_w8a8/common.hpp
2	1	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
2	1	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
2	10	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
163	0	csrc/sparse/cutlass/sparse_compressor_c3x.cu
42	0	csrc/sparse/cutlass/sparse_compressor_entry.cu
303	0	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu
496	0	csrc/sparse/cutlass/sparse_scaled_mm_c3x.cuh
59	0	csrc/sparse/cutlass/sparse_scaled_mm_entry.cu
15	0	csrc/torch_bindings.cpp
1	1	pyproject.toml
131	0	tests/kernels/test_semi_structured.py
100	3	tests/quantization/test_compressed_tensors.py
2	0	tests/weight_loading/models.txt
4	0	tests/weight_loading/run_model_weight_loading_test.sh
7	0	tests/weight_loading/test_weight_loading.py
103	0	vllm/_custom_ops.py
160	27	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
7	8	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
203	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py

[f04e407e6] Yan Ma 2024-12-18 [MISC][XPU]update ipex link for CI fix (#11278)
3	3	requirements-xpu.txt

[8b79f9e10] Wallas Henrique 2024-12-18 [Bugfix] Fix guided decoding with tokenizer mode mistral (#11046)
5	1	.buildkite/test-pipeline.yaml
2	1	requirements-common.txt
48	6	tests/model_executor/test_guided_processors.py
84	2	tests/models/decoder_only/language/test_mistral.py
73	40	vllm/model_executor/guided_decoding/xgrammar_decoding.py
1	1	vllm/transformers_utils/tokenizer.py
4	1	vllm/transformers_utils/tokenizers/mistral.py

[866fa4550] Konrad Zawora 2024-12-18 [Bugfix] Restore support for larger block sizes (#11259)
4	0	vllm/config.py
4	2	vllm/engine/arg_utils.py

[bf8717eba] Cody Yu 2024-12-17 [V1] Prefix caching for vision language models (#11187)
86	2	tests/v1/core/test_prefix_caching.py
0	15	tests/v1/engine/test_engine_args.py
14	13	vllm/engine/arg_utils.py
20	0	vllm/inputs/data.py
3	0	vllm/multimodal/inputs.py
50	24	vllm/v1/core/kv_cache_manager.py
105	10	vllm/v1/core/kv_cache_utils.py
2	0	vllm/v1/core/scheduler.py
7	3	vllm/v1/engine/async_llm.py
4	4	vllm/v1/engine/core.py
6	3	vllm/v1/engine/llm_engine.py
15	18	vllm/v1/engine/mm_input_mapper.py
7	5	vllm/v1/engine/processor.py
23	1	vllm/v1/request.py

[c77eb8a33] Michael Goin 2024-12-17 [Bugfix] Set temperature=0.7 in test_guided_choice_chat (#11264)
2	0	tests/entrypoints/openai/test_chat.py

[2d1b9baa8] Joe Runde 2024-12-17 [Bugfix] Fix request cancellation without polling (#11190)
51	0	tests/entrypoints/openai/test_basic.py
1	5	tests/test_utils.py
5	6	tests/utils.py
27	19	vllm/engine/async_llm_engine.py
7	4	vllm/entrypoints/api_server.py
8	0	vllm/entrypoints/openai/api_server.py
0	5	vllm/entrypoints/openai/serving_chat.py
1	2	vllm/entrypoints/openai/serving_completion.py
1	4	vllm/entrypoints/openai/serving_embedding.py
1	4	vllm/entrypoints/openai/serving_score.py
57	0	vllm/entrypoints/utils.py
5	54	vllm/utils.py

[f9ecbb18b] Isotr0py 2024-12-17 [Misc] Allow passing logits_soft_cap for xformers backend (#11252)
3	5	vllm/attention/backends/xformers.py

[02222a025] Roger Wang 2024-12-16 [Misc] Kernel Benchmark for `RMSNorm` (#11241)
262	0	benchmarks/kernels/benchmark_rmsnorm.py

[2bfdbf2a3] Tyler Michael Smith 2024-12-17 [V1][Core] Use weakref.finalize instead of atexit (#11242)
2	11	vllm/v1/engine/core_client.py
3	7	vllm/v1/executor/multiproc_executor.py

[e88db68cf] wangxiyuan 2024-12-17 [Platform] platform agnostic for EngineArgs initialization (#11225)
2	6	vllm/engine/arg_utils.py
3	0	vllm/platforms/cpu.py
4	0	vllm/platforms/cuda.py
6	0	vllm/platforms/hpu.py
6	0	vllm/platforms/neuron.py
3	0	vllm/platforms/openvino.py
4	0	vllm/platforms/rocm.py
5	0	vllm/platforms/tpu.py
4	0	vllm/platforms/xpu.py

[59c9b6ebe] Roger Wang 2024-12-16 [V1][VLM] Proper memory profiling for image language models (#11210)
8	0	vllm/config.py
5	0	vllm/model_executor/models/pixtral.py
20	3	vllm/multimodal/registry.py
3	4	vllm/v1/core/scheduler.py
1	0	vllm/v1/engine/mm_input_mapper.py
61	6	vllm/v1/worker/gpu_model_runner.py

[66d4b1672] kYLe 2024-12-17 [Frontend] Add OpenAI API support for input_audio (#11027)
5	5	docs/source/serving/openai_compatible_server.md
89	1	docs/source/usage/multimodal_inputs.rst
31	3	examples/openai_chat_completion_client_for_multimodal.py
121	4	tests/entrypoints/openai/test_audio.py
55	10	vllm/entrypoints/chat_utils.py

[0064f697d] Michael Goin 2024-12-16 [CI] Add test case with JSON schema using references + use xgrammar by default with OpenAI parse (#10935)
39	0	tests/entrypoints/conftest.py
28	0	tests/entrypoints/llm/test_guided_generate.py
1	1	vllm/entrypoints/openai/protocol.py

[35bae114a] youkaichao 2024-12-16 fix gh200 tests on main (#11246)
2	2	.buildkite/run-gh200-test.sh
1	4	docs/source/serving/deploying_with_docker.rst

[88a412ed3] youkaichao 2024-12-16 [torch.compile] fast inductor (#11108)
210	3	vllm/compilation/backends.py
411	4	vllm/config.py
3	0	vllm/envs.py

[c301616ed] youkaichao 2024-12-16 [ci][tests] add gh200 tests (#11244)
25	0	.buildkite/run-gh200-test.sh

[35ffa682b] bk-TurbaAI 2024-12-16 [Docs] hint to enable use of GPU performance counters in profiling tools for multi-node distributed serving (#11235)
1	1	docs/source/serving/distributed_serving.rst

[551603fef] youkaichao 2024-12-16 [core] overhaul memory profiling and fix backward compatibility (#10511)
25	0	tests/entrypoints/llm/test_gpu_utilization.py
1	1	tests/entrypoints/llm/test_lazy_outlines.py
42	2	tests/test_utils.py
9	9	tests/worker/test_profile.py
6	5	vllm/engine/arg_utils.py
123	2	vllm/utils.py
2	1	vllm/worker/multi_step_model_runner.py
28	40	vllm/worker/worker.py

[efbce85f4] Varun Sundar Rabindranath 2024-12-16 [misc] Layerwise profile updates (#10242)
1	1	.buildkite/test-pipeline.yaml
206	30	examples/offline_profile.py
7	2	tools/profiler/print_layerwise_table.py
80	12	tools/profiler/visualize_layerwise_profile.py
20	2	vllm/profiler/layerwise_profile.py

[2ca830dba] Isotr0py 2024-12-16 [Doc] Reorder vision language examples in alphabet order (#11228)
243	243	examples/offline_inference_vision_language.py
144	144	examples/offline_inference_vision_language_multi_image.py

[d927dbcd8] Isotr0py 2024-12-16 [Model] Refactor Ultravox to use merged input processor (#11198)
5	5	examples/offline_inference_audio_language.py
1	1	tests/distributed/test_pipeline_parallel.py
1	0	tests/entrypoints/openai/test_audio.py
3	2	tests/models/decoder_only/audio_language/test_ultravox.py
1	1	vllm/entrypoints/chat_utils.py
104	140	vllm/model_executor/models/ultravox.py
14	5	vllm/multimodal/processing.py

[bddbbcb13] Jani Monoses 2024-12-16 [Model] Support Cohere2ForCausalLM (Cohere R7B) (#11203)
2	2	docs/source/models/supported_models.rst
2	0	tests/models/registry.py
4	0	tests/models/test_initialization.py
17	2	vllm/model_executor/models/commandr.py
1	0	vllm/model_executor/models/registry.py

[b3b1526f0] cennn 2024-12-16 WIP: [CI/Build] simplify Dockerfile build for ARM64 / GH200 (#11212)
32	8	Dockerfile
26	0	docs/source/serving/deploying_with_docker.rst
1	1	requirements-build.txt
3	0	requirements-cuda-arm64.txt
2	2	requirements-cuda.txt

[17138af7c] yansh97 2024-12-16 [Bugfix] Fix the default value for temperature in ChatCompletionRequest (#11219)
1	1	vllm/entrypoints/openai/protocol.py

[69ba344de] chenqianfzh 2024-12-15 [Bugfix] Fix block size validation (#10938)
1	1	vllm/engine/arg_utils.py

[da6f40924] AlexHe99 2024-12-16 Update deploying_with_k8s.rst (#10922)
2	2	docs/source/serving/deploying_with_k8s.rst

[25ebed2f8] Woosuk Kwon 2024-12-15 [V1][Minor] Cache np arange to reduce input preparation overhead (#11214)
10	2	vllm/v1/worker/gpu_model_runner.py

[d263bd9df] shangmingc 2024-12-16 [Core] Support disaggregated prefill with Mooncake Transfer Engine (#10884)
4	3	vllm/config.py
2	1	vllm/distributed/kv_transfer/kv_connector/factory.py
74	27	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
272	0	vllm/distributed/kv_transfer/kv_pipe/mooncake_pipe.py

[38e599d6a] Kuntai Du 2024-12-15 [Doc] add documentation for disaggregated prefilling (#11197)
-	-	docs/source/assets/usage/disagg_prefill/abstraction.jpg
-	-	docs/source/assets/usage/disagg_prefill/overview.jpg
1	0	docs/source/index.rst
69	0	docs/source/usage/disagg_prefill.rst

[96d673e0f] Cyrus Leung 2024-12-16 [Bugfix] Fix error handling of unsupported sliding window (#11213)
8	6	vllm/model_executor/models/llama.py

[b10609e6a] Cyrus Leung 2024-12-15 [Misc] Clean up multi-modal processor (#11207)
1	4	examples/offline_inference_vision_language.py
8	9	tests/multimodal/test_processing.py
23	25	vllm/multimodal/processing.py

[a1c02058b] youkaichao 2024-12-14 [torch.compile] allow tracking forward time (#11081)
42	19	vllm/forward_context.py

[15859f235] Jee Jee Li 2024-12-15 [[Misc]Upgrade bitsandbytes to the latest version 0.45.0 (#11201)
1	1	Dockerfile
1	1	docs/source/quantization/bnb.rst
1	1	requirements-test.in
1	1	requirements-test.txt
4	4	vllm/model_executor/layers/quantization/bitsandbytes.py

[886936837] Sungjae Lee 2024-12-15 [Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion (#7209)
43	20	vllm/core/evictor.py

[6d917d0ee] Mark McLoughlin 2024-12-14 Enable mypy checking on V1 code (#11105)
1	0	tools/mypy.sh
2	0	vllm/v1/attention/backends/flash_attn.py
5	5	vllm/v1/core/kv_cache_manager.py
9	8	vllm/v1/core/kv_cache_utils.py
1	0	vllm/v1/core/scheduler.py
14	9	vllm/v1/engine/__init__.py
6	5	vllm/v1/engine/async_llm.py
10	10	vllm/v1/engine/core.py
24	19	vllm/v1/engine/core_client.py
2	2	vllm/v1/engine/detokenizer.py
2	1	vllm/v1/engine/llm_engine.py
13	7	vllm/v1/engine/mm_input_mapper.py
1	1	vllm/v1/engine/processor.py
2	10	vllm/v1/executor/abstract.py
8	7	vllm/v1/executor/multiproc_executor.py
4	3	vllm/v1/executor/uniproc_executor.py
1	2	vllm/v1/request.py
27	15	vllm/v1/utils.py
1	0	vllm/v1/worker/gpu_input_batch.py
26	16	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/v1/worker/gpu_worker.py

[93abf23a6] Cyrus Leung 2024-12-15 [VLM] Fully dynamic prompt replacement in merged input processor (#11199)
1	4	examples/offline_inference_vision_language.py
2	2	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py
54	51	tests/multimodal/test_processing.py
2	2	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
61	10	vllm/inputs/registry.py
72	72	vllm/model_executor/models/llava.py
57	61	vllm/model_executor/models/phi3v.py
1	1	vllm/model_executor/models/pixtral.py
2	2	vllm/multimodal/base.py
306	300	vllm/multimodal/processing.py
2	2	vllm/multimodal/registry.py
9	3	vllm/utils.py

[9c3dadd1c] Brad Hilton 2024-12-14 [Frontend] Add `logits_processors` as an extra completion argument (#11150)
1	0	tests/entrypoints/openai/test_serving_chat.py
38	33	vllm/config.py
10	1	vllm/engine/arg_utils.py
74	3	vllm/entrypoints/openai/protocol.py
2	1	vllm/entrypoints/openai/serving_chat.py
2	1	vllm/entrypoints/openai/serving_completion.py

[3cb576988] Jee Jee Li 2024-12-15 [Misc] Minor improvements to the readability of PunicaWrapperBase (#11200)
8	6	vllm/lora/punica_wrapper/punica_base.py
17	17	vllm/lora/punica_wrapper/punica_gpu.py
2	2	vllm/lora/punica_wrapper/punica_hpu.py

[ea7bd68d1] Tyler Michael Smith 2024-12-14 [V1][Bugfix] Fix V1 TP trust-remote-code (#11182)
5	0	vllm/v1/engine/core.py

[48259264a] Russell Bryant 2024-12-14 [Core] Update outlines and increase its threadpool size (#11140)
1	1	requirements-common.txt
10	1	vllm/model_executor/guided_decoding/outlines_decoding.py

[24a3d12b8] dhuangnm 2024-12-13 update compressed-tensors to latest version (#11183)
1	1	requirements-common.txt

[9855aea21] Cody Yu 2024-12-13 [Bugfix][V1] Re-compute an entire block when fully cache hit (#11186)
7	3	vllm/v1/core/scheduler.py

[4b5b8a6a3] Tyler Michael Smith 2024-12-13 [V1][Bugfix] Fix EngineCoreProc profile (#11185)
1	1	vllm/v1/engine/core.py

[4863e5fba] Russell Bryant 2024-12-13 [Core] V1: Use multiprocessing by default (#11074)
195	0	docs/source/design/multiprocessing.md
56	0	docs/source/getting_started/debugging.rst
1	0	docs/source/index.rst
4	0	vllm/entrypoints/llm.py
2	2	vllm/envs.py
14	6	vllm/executor/multiproc_worker_utils.py
2	6	vllm/v1/engine/core.py
9	2	vllm/v1/engine/core_client.py
7	0	vllm/v1/engine/llm_engine.py
9	1	vllm/v1/executor/multiproc_executor.py

[0d8451c3a] Jiaxin Shan 2024-12-13 [Distributed] Allow the placement group more time to wait for resources to be ready (#11138)
7	3	vllm/executor/ray_utils.py

[0a56bcc03] Jani Monoses 2024-12-13 [Bugfix][Hardware][CPU] Enable Gemma2 with SDPA on CPU backend (#11169)
4	3	vllm/attention/backends/torch_sdpa.py

[0920ab913] Cyrus Leung 2024-12-14 [Doc] Reorganize online pooling APIs (#11172)
7	7	docs/source/models/pooling_models.rst
267	195	docs/source/serving/openai_compatible_server.md
4	4	docs/source/usage/multimodal_inputs.rst
46	46	examples/offline_inference_openai.md
1	1	examples/openai_chat_embedding_client_for_multimodal.py
18	17	examples/openai_cross_encoder_score.py
3	3	tests/entrypoints/openai/test_score.py
10	1	vllm/entrypoints/openai/api_server.py
5	2	vllm/entrypoints/openai/protocol.py
7	12	vllm/outputs.py

[238c0d93b] Alexander Matveev 2024-12-13 [Misc] Add tokenizer_mode param to benchmark_serving.py (#11174)
12	0	benchmarks/benchmark_serving.py

[5b0ed8391] zhangjf 2024-12-13 [Bugfix] using len(tokenizer) instead of tokenizer.vocab_size in AllowedTokenIdsLogitsProcessor (#11156)
2	2	vllm/entrypoints/openai/logits_processors.py

[c31d4a57a] Sungjae Lee 2024-12-14 [Core] support LoRA and prompt adapter in content-based hashing for Block Manager v2 prefix caching (#8240)
63	2	tests/core/block/test_prefix_caching_block.py
10	0	tests/core/utils.py
34	12	vllm/core/block/block_table.py
13	6	vllm/core/block/common.py
32	11	vllm/core/block/cpu_gpu_block_allocator.py
23	9	vllm/core/block/interfaces.py
9	1	vllm/core/block/naive_block.py
42	13	vllm/core/block/prefix_caching_block.py
7	1	vllm/core/block_manager.py
13	0	vllm/sequence.py

[d1fa714cb] Chenguang Li 2024-12-13 [Refactor]A simple device-related refactor (#11163)
5	0	vllm/platforms/cpu.py
9	0	vllm/platforms/hpu.py
17	0	vllm/platforms/interface.py
9	0	vllm/platforms/neuron.py
5	5	vllm/platforms/openvino.py
5	0	vllm/platforms/xpu.py
1	26	vllm/utils.py

[969da7d70] Roger Wang 2024-12-13 [V1][VLM] Fix edge case bug for InternVL2 (#11165)
4	1	vllm/model_executor/models/internvl.py

[eeec9e339] Cyrus Leung 2024-12-13 [Frontend] Separate pooling APIs in offline inference (#11129)
5	2	.buildkite/test-pipeline.yaml
45	8	docs/source/models/pooling_models.rst
28	0	examples/offline_inference_classification.py
11	5	examples/offline_inference_embedding.py
23	0	examples/offline_inference_scoring.py
1	1	examples/offline_inference_vision_language_embedding.py
7	11	tests/conftest.py
5	5	tests/entrypoints/openai/test_score.py
5	5	tests/models/embedding/language/test_scoring.py
2	3	tests/models/test_oot_registration.py
11	25	vllm/__init__.py
8	9	vllm/engine/llm_engine.py
128	15	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/protocol.py
6	3	vllm/entrypoints/openai/serving_embedding.py
7	5	vllm/entrypoints/openai/serving_engine.py
6	6	vllm/entrypoints/openai/serving_score.py
196	92	vllm/model_executor/layers/pooler.py
9	6	vllm/model_executor/models/gritlm.py
137	70	vllm/outputs.py
18	22	vllm/sequence.py

[f93bf2b18] Li, Jiang 2024-12-13 [Bugfix][CI][CPU] add missing datasets package to requirements-cpu.txt  (#11159)
2	1	requirements-cpu.txt

[7cd740914] Jani Monoses 2024-12-13 PaliGemma 2 support (#11142)
2	2	docs/source/models/supported_models.rst
13	0	examples/offline_inference_vision_language.py
10	1	vllm/model_executor/models/paligemma.py

[be39e3cd1] youkaichao 2024-12-12 [core] clean up cudagraph batchsize padding logic (#10996)
3	2	tests/models/decoder_only/language/test_jamba.py
3	2	tests/models/decoder_only/language/test_mamba.py
2	2	tests/worker/test_encoder_decoder_model_runner.py
2	2	tests/worker/test_model_runner.py
105	66	vllm/config.py
14	6	vllm/model_executor/models/jamba.py
14	7	vllm/model_executor/models/mamba.py
2	9	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/worker/enc_dec_model_runner.py
4	3	vllm/worker/model_runner.py
0	4	vllm/worker/xpu_model_runner.py

[34f1a806d] Cody Yu 2024-12-12 [Bugfix][V1] Fix 'NoneType' object has no attribute 'hash_value' (#11157)
15	9	vllm/v1/core/kv_cache_manager.py

[00c1bde5d] Gregory Shtrasberg 2024-12-13 [ROCm][AMD] Disable auto enabling chunked prefill on ROCm (#11146)
2	1	vllm/engine/arg_utils.py

[3989a7982] Dipika Sikka 2024-12-13 [Bugfix] Update starcoder2 to remap k/v scale names for kv_cache quantization (#11148)
6	1	vllm/model_executor/models/starcoder2.py

[1efce6860] Pooya Davoodi 2024-12-12 [Bugfix] Use runner_type instead of task in GritLM (#11144)
3	3	tests/models/embedding/language/test_gritlm.py
4	4	vllm/model_executor/models/gritlm.py

[30870b4f6] Luka Govedič 2024-12-12 [torch.compile] Dynamic fp8 + rms_norm fusion (#10906)
2	1	CMakeLists.txt
173	0	benchmarks/fused_kernels/layernorm_rms_benchmarks.py
14	0	csrc/dispatch_utils.h
8	0	csrc/ops.h
7	19	csrc/quantization/fp8/common.cuh
160	0	csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu
327	0	csrc/quantization/fused_kernels/layernorm_utils.cuh
81	0	csrc/quantization/fused_kernels/quant_conversions.cuh
33	0	csrc/quantization/vectorization.cuh
8	0	csrc/torch_bindings.cpp
13	8	tests/compile/test_functionalization.py
43	18	tests/compile/test_fusion.py
171	0	tests/kernels/test_fused_quant_layernorm.py
20	0	vllm/_custom_ops.py
6	3	vllm/compilation/fix_functionalization.py
521	198	vllm/compilation/fusion.py
42	0	vllm/compilation/fx_utils.py
105	0	vllm/compilation/multi_output_match.py
2	1	vllm/compilation/reshapes.py
0	4	vllm/compilation/vllm_inductor_pass.py

[78ed8f57d] Cody Yu 2024-12-12 [Misc][V1] Fix type in v1 prefix caching (#11151)
8	4	tests/v1/core/test_prefix_caching.py
4	4	vllm/v1/core/kv_cache_manager.py
15	7	vllm/v1/core/kv_cache_utils.py

[db6c264a1] shangmingc 2024-12-13 [Bugfix] Fix value unpack error of simple connector for KVCache transfer. (#11058)
6	2	vllm/distributed/kv_transfer/kv_connector/simple_connector.py

[9f3974a31] Jeremy Arnold 2024-12-12 Fix logging of the vLLM Config (#11143)
1	1	vllm/engine/llm_engine.py

[2c97eca1f] Cody Yu 2024-12-12 [Misc] Validate grammar and fail early (#11119)
22	10	vllm/model_executor/guided_decoding/xgrammar_decoding.py
4	8	vllm/model_executor/guided_decoding/xgrammar_utils.py

[5d712571a] Jeff Cook 2024-12-12 [Bugfix] Quick fix to make Pixtral-HF load correctly again after 39e227c7ae. (#11024)
1	5	vllm/model_executor/models/llava.py

[d4d5291cc] Ramon Ziai 2024-12-12 fix(docs): typo in helm install instructions (#11141)
1	1	docs/source/serving/deploying_with_helm.rst

[4816d20aa] Roger Wang 2024-12-12 [V1] Fix torch profiling for offline inference (#11125)
19	12	examples/offline_inference_with_profiler.py
2	2	vllm/v1/engine/core_client.py

[85362f028] Jiaxin Shan 2024-12-12 [Misc][LoRA] Ensure Lora Adapter requests return adapter name (#11094)
11	0	tests/entrypoints/openai/test_serving_engine.py
8	6	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py
13	0	vllm/entrypoints/openai/serving_engine.py

[62de37a38] youkaichao 2024-12-12 [core][distributed] initialization from StatelessProcessGroup (#10986)
3	3	.buildkite/test-pipeline.yaml
25	4	tests/distributed/test_same_node.py
56	28	tests/distributed/test_shm_broadcast.py
26	13	vllm/distributed/device_communicators/shm_broadcast.py
43	21	vllm/distributed/parallel_state.py

[819582420] Sanju C Sudhakaran 2024-12-12 [Hardware][Intel-Gaudi] Enable LoRA support for Intel Gaudi (HPU) (#10565)
1	1	requirements-hpu.txt
6	0	vllm/lora/layers.py
87	0	vllm/lora/punica_wrapper/punica_hpu.py
5	0	vllm/lora/punica_wrapper/punica_selector.py
8	13	vllm/worker/hpu_model_runner.py

[f092153fb] Woosuk Kwon 2024-12-11 [V1] Use more persistent buffers to optimize input preparation overheads (#11111)
14	5	vllm/v1/worker/gpu_input_batch.py
65	54	vllm/v1/worker/gpu_model_runner.py

[1da8f0e1d] Pooya Davoodi 2024-12-11 [Model] Add support for embedding model GritLM (#10816)
10	0	docs/source/models/supported_models.rst
200	0	tests/models/embedding/language/test_gritlm.py
1	0	tests/models/registry.py
245	0	vllm/model_executor/models/gritlm.py
2	0	vllm/model_executor/models/registry.py

[ccede2b26] Russell Bryant 2024-12-11 [Core] cleanup zmq ipc sockets on exit (#11115)
9	0	vllm/entrypoints/openai/api_server.py
14	2	vllm/v1/engine/core.py
20	8	vllm/v1/engine/core_client.py
14	7	vllm/v1/executor/multiproc_executor.py

[24a36d6d5] Yuan Tang 2024-12-11 Update link to LlamaStack remote vLLM guide in serving_with_llamastack.rst (#11112)
1	1	docs/source/serving/serving_with_llamastack.rst

[8fb26dac6] Simon Mo 2024-12-11 [Docs] Add media kit (#11121)
4	0	README.md

[7439a8b5f] Clayton 2024-12-11 [Bugfix] Multiple fixes to tool streaming with hermes and mistral (#10979)
14	2	vllm/entrypoints/openai/serving_chat.py
40	11	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
15	8	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[4e1168336] Alexander Matveev 2024-12-11 [V1] VLM preprocessor hashing (#11020)
109	17	examples/offline_inference_vision_language.py
1	0	requirements-common.txt
1	0	tests/v1/engine/test_engine_core.py
1	0	tests/v1/engine/test_engine_core_client.py
8	2	vllm/config.py
8	0	vllm/engine/arg_utils.py
2	1	vllm/v1/engine/__init__.py
14	4	vllm/v1/engine/core.py
144	12	vllm/v1/engine/mm_input_mapper.py
23	12	vllm/v1/engine/processor.py
21	0	vllm/v1/utils.py

[452a723bf] Tyler Michael Smith 2024-12-11 [V1][Core] Remove should_shutdown to simplify core process termination (#11113)
2	11	vllm/v1/engine/core.py
0	6	vllm/v1/engine/core_client.py

[d1e21a979] Cyrus Leung 2024-12-12 [CI/Build] Split up VLM tests (#11083)
21	11	.buildkite/test-pipeline.yaml
2	1	pyproject.toml
46	26	tests/models/decoder_only/vision_language/test_models.py
25	12	tests/utils.py

[72ff3a968] Rui Qiao 2024-12-11 [core] Bump ray to use _overlap_gpu_communication in compiled graph tests (#10410)
1	1	requirements-test.in
1	1	requirements-test.txt
8	0	vllm/envs.py
10	7	vllm/executor/ray_gpu_executor.py

[66aaa7722] youkaichao 2024-12-11 [torch.compile] remove graph logging in ci (#11110)
5	3	vllm/compilation/backends.py

[d643c2aba] Woosuk Kwon 2024-12-11 [V1] Use input_ids as input for text-only models  (#11032)
47	21	vllm/v1/worker/gpu_model_runner.py

[91642db95] youkaichao 2024-12-11 [torch.compile] use depyf to dump torch.compile internals (#10972)
1	0	requirements-common.txt
36	33	vllm/compilation/backends.py
1	1	vllm/compilation/decorators.py
20	3	vllm/compilation/monitor.py
2	2	vllm/compilation/wrapper.py
4	2	vllm/config.py
2	1	vllm/worker/model_runner.py

[fd2222068] bingps 2024-12-11 [Doc] Installed version of llmcompressor for int8/fp8 quantization (#11103)
1	1	docs/source/quantization/fp8.rst
2	2	docs/source/quantization/int8.rst

[b2f775456] hissu-hyvarinen 2024-12-11 [CI/Build] Enable prefix caching test for AMD (#11098)
1	1	.buildkite/test-pipeline.yaml

[cad5c0a6e] Cyrus Leung 2024-12-11 [Doc] Update docs to refer to pooling models (#11093)
6	1	docs/source/usage/faq.rst
1	1	vllm/attention/backends/placeholder_attn.py
4	4	vllm/config.py
1	1	vllm/core/placeholder_block_space_manager.py
2	2	vllm/engine/arg_utils.py
1	1	vllm/engine/async_llm_engine.py
1	1	vllm/engine/multiprocessing/client.py
1	1	vllm/engine/protocol.py
1	1	vllm/entrypoints/openai/serving_score.py
3	3	vllm/sequence.py
1	1	vllm/v1/engine/processor.py
1	1	vllm/worker/cpu_worker.py
2	2	vllm/worker/hpu_worker.py
1	1	vllm/worker/worker.py

[8f10d5e39] Cyrus Leung 2024-12-11 [Misc] Split up pooling tasks (#10820)
2	0	docs/source/index.rst
146	0	docs/source/models/generative_models.rst
99	0	docs/source/models/pooling_models.rst
102	55	docs/source/models/supported_models.rst
6	6	docs/source/usage/compatibility_matrix.rst
6	1	examples/offline_inference_embedding.py
2	2	examples/offline_inference_vision_language_embedding.py
2	2	tests/compile/test_basic_correctness.py
1	1	tests/core/test_scheduler_encoder_decoder.py
1	1	tests/entrypoints/openai/test_vision_embedding.py
1	1	tests/models/embedding/language/test_embedding.py
3	9	tests/models/embedding/language/test_scoring.py
1	1	tests/models/embedding/vision_language/test_dse_qwen2_vl.py
1	1	tests/models/embedding/vision_language/test_llava_next.py
1	1	tests/models/embedding/vision_language/test_phi3v.py
12	5	tests/test_config.py
95	42	vllm/config.py
1	1	vllm/core/scheduler.py
4	3	vllm/engine/arg_utils.py
2	2	vllm/engine/llm_engine.py
29	24	vllm/entrypoints/llm.py
4	4	vllm/entrypoints/openai/api_server.py
2	2	vllm/entrypoints/openai/run_batch.py
1	1	vllm/model_executor/model_loader/utils.py
1	1	vllm/v1/engine/core.py
1	1	vllm/worker/cpu_worker.py
1	1	vllm/worker/worker.py

[40766ca1b] Rafael Vasquez 2024-12-11 [Bugfix]: Clamp `-inf` logprob values in prompt_logprobs (#11073)
6	0	vllm/entrypoints/openai/serving_completion.py

[2e32f5d28] B-201 2024-12-11 [Bugfix] Fix Idefics3 fails during multi-image inference (#11080)
13	8	vllm/model_executor/models/idefics3.py

[61b1d2f6a] Russell Bryant 2024-12-11 [Core] v1: Use atexit to handle engine core client shutdown (#11076)
2	0	vllm/v1/engine/core_client.py

[9974fca04] Kevin H. Luu 2024-12-11 [ci/build] Fix entrypoints test and pin outlines version (#11088)
1	1	requirements-common.txt
1	1	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[3fb4b4f16] Kevin H. Luu 2024-12-11 [ci/build] Fix AMD CI dependencies (#11087)
2	1	requirements-rocm.txt

[2e33fe419] Cyrus Leung 2024-12-11 [CI/Build] Check transformers v4.47 (#10991)
2	2	requirements-test.txt
0	9	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_idefics3.py
1	1	tests/models/embedding/vision_language/test_llava_next.py

[e39400a4b] Maximilien de Bayser 2024-12-11 Fix streaming for granite tool call when <|tool_call|> is present (#11069)
5	1	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py

[ffa48c914] Mor Zusman 2024-12-11 [Model] PP support for Mamba-like models (#10992)
3	3	docs/source/models/supported_models.rst
4	2	tests/distributed/test_pipeline_parallel.py
44	14	vllm/config.py
37	0	vllm/model_executor/models/interfaces.py
65	28	vllm/model_executor/models/jamba.py
48	20	vllm/model_executor/models/mamba.py
10	1	vllm/model_executor/models/registry.py
5	0	vllm/utils.py
4	4	vllm/v1/worker/gpu_model_runner.py
3	3	vllm/v1/worker/gpu_worker.py
6	6	vllm/worker/cache_engine.py

[d5c5154fc] Aurick Qiao 2024-12-10 [Misc] LoRA + Chunked Prefill (#9057)
6	3	tests/lora/test_chatglm3_tp.py
2	1	tests/lora/test_gemma.py
5	1	tests/lora/test_llama_tp.py
2	1	tests/lora/test_long_context.py
2	1	tests/lora/test_minicpmv.py
2	0	tests/lora/test_minicpmv_tp.py
1	0	tests/lora/test_mixtral.py
2	1	tests/lora/test_phi.py
6	3	tests/lora/test_quant_model.py
2	1	vllm/config.py
12	3	vllm/core/scheduler.py
7	5	vllm/worker/model_runner.py

[9a9397370] Tyler Michael Smith 2024-12-10 [Bugfix] Fix Mamba multistep (#11071)
63	1	vllm/attention/backends/placeholder_attn.py
3	1	vllm/worker/multi_step_model_runner.py

[134810b3d] Woosuk Kwon 2024-12-10 [V1][Bugfix] Always set enable_chunked_prefill = True for V1 (#11061)
12	9	vllm/engine/arg_utils.py

[75f89dc44] youkaichao 2024-12-10 [torch.compile] add a flag to track batchsize statistics (#11059)
3	0	vllm/envs.py
31	1	vllm/forward_context.py
1	0	vllm/v1/attention/backends/flash_attn.py
2	0	vllm/v1/worker/gpu_model_runner.py

[e73919492] Russell Bryant 2024-12-10 [Core] Update to outlines >= 0.1.8 (#10576)
1	1	requirements-common.txt
1	1	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[250ee65d7] Flávia Béo 2024-12-10 [BUG] Remove token param #10921 (#11022)
29	34	vllm/transformers_utils/config.py

[9b9cef314] Joe Runde 2024-12-10 [Bugfix] Backport request id validation to v0 (#11036)
4	0	vllm/engine/multiprocessing/client.py
1	1	vllm/v1/engine/async_llm.py

[d05f88679] Jee Jee Li 2024-12-10 [Misc][LoRA] Add PEFTHelper  for LoRA  (#11003)
55	3	tests/lora/test_lora_manager.py
18	0	vllm/lora/lora.py
17	25	vllm/lora/models.py
70	0	vllm/lora/peft_helper.py

[beb16b2c8] Travis Johnson 2024-12-10 [Bugfix] Handle <|tool_call|> token in granite tool parser (#11039)
3	1	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py

[fe2e10c71] Maxime Fournioux 2024-12-10 Add example of helm chart for vllm deployment on k8s (#9199)
81	0	.github/workflows/lint-and-deploy.yaml
1	0	docs/source/index.rst
-	-	docs/source/serving/architecture_helm_deployment.png
253	0	docs/source/serving/deploying_with_helm.rst
6	0	examples/chart-helm/.helmignore
21	0	examples/chart-helm/Chart.yaml
3	0	examples/chart-helm/ct.yaml
42	0	examples/chart-helm/lintconf.yaml
164	0	examples/chart-helm/templates/_helpers.tpl
11	0	examples/chart-helm/templates/configmap.yaml
6	0	examples/chart-helm/templates/custom-objects.yaml
122	0	examples/chart-helm/templates/deployment.yaml
31	0	examples/chart-helm/templates/hpa.yaml
37	0	examples/chart-helm/templates/job.yaml
7	0	examples/chart-helm/templates/poddisruptionbudget.yaml
13	0	examples/chart-helm/templates/pvc.yaml
10	0	examples/chart-helm/templates/secrets.yaml
14	0	examples/chart-helm/templates/service.yaml
265	0	examples/chart-helm/values.schema.json
119	0	examples/chart-helm/values.yaml

[82c73fd51] Gene Der Su 2024-12-09 [Bugfix] cuda error running llama 3.2 (#11047)
28	7	vllm/platforms/cuda.py

[bfd610430] Diego Marinho 2024-12-10 Update README.md (#11034)
1	0	README.md

[e35879c27] Jeff Cook 2024-12-09 [Bugfix] Fix xgrammar failing to read a vocab_size from LlavaConfig on PixtralHF. (#11043)
3	3	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[ebf778061] youkaichao 2024-12-09 monitor metrics of tokens per step using cudagraph batchsizes (#11031)
1	1	tests/metrics/test_metrics.py
4	2	vllm/engine/llm_engine.py
16	9	vllm/engine/metrics.py
2	1	vllm/engine/metrics_types.py

[28b3a1c7e] Tyler Michael Smith 2024-12-10 [V1] Multiprocessing Tensor Parallel Support for v1 (#9856)
16	0	tests/basic_correctness/test_basic_correctness.py
5	6	tests/conftest.py
52	24	vllm/distributed/device_communicators/shm_broadcast.py
6	41	vllm/executor/multiproc_gpu_executor.py
42	0	vllm/executor/multiproc_worker_utils.py
4	1	vllm/model_executor/layers/logits_processor.py
20	8	vllm/platforms/cuda.py
26	0	vllm/utils.py
3	1	vllm/v1/core/scheduler.py
14	4	vllm/v1/engine/async_llm.py
39	35	vllm/v1/engine/core.py
9	4	vllm/v1/engine/core_client.py
13	6	vllm/v1/engine/llm_engine.py
48	0	vllm/v1/executor/abstract.py
375	0	vllm/v1/executor/multiproc_executor.py
9	3	vllm/v1/executor/{gpu_executor.py => uniproc_executor.py}
4	2	vllm/v1/outputs.py
2	1	vllm/v1/sample/sampler.py
32	1	vllm/v1/utils.py
5	7	vllm/v1/worker/gpu_model_runner.py
9	2	vllm/v1/worker/gpu_worker.py

[bc192a2b0] Patrick von Platen 2024-12-10 [Pixtral] Improve loading (#11040)
25	31	vllm/model_executor/models/pixtral.py

[980ad394a] Joe Runde 2024-12-09 [Frontend] Use request id from header (#10968)
1	0	docs/requirements-docs.txt
2	2	vllm/entrypoints/openai/api_server.py
2	1	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/entrypoints/openai/serving_completion.py
2	2	vllm/entrypoints/openai/serving_embedding.py
10	1	vllm/entrypoints/openai/serving_engine.py
2	2	vllm/entrypoints/openai/serving_score.py
6	3	vllm/entrypoints/openai/serving_tokenization.py

[391d7b276] Cyrus Leung 2024-12-10 [Bugfix] Fix usage of `deprecated` decorator (#11025)
4	4	vllm/engine/llm_engine.py
4	4	vllm/engine/multiprocessing/__init__.py
8	8	vllm/engine/multiprocessing/client.py
36	36	vllm/entrypoints/llm.py

[d1f6d1c8a] Isotr0py 2024-12-10 [Model] Add has_weight to RMSNorm and re-enable weights loading tracker for Mamba (#10739)
9	2	vllm/model_executor/layers/layernorm.py
18	8	vllm/model_executor/layers/mamba/mamba_mixer.py
7	2	vllm/model_executor/models/mamba.py

[6d525288c] Michael Goin 2024-12-09 [Docs] Add dedicated tool calling page to docs (#10554)
1	0	docs/source/index.rst
0	217	docs/source/serving/openai_compatible_server.md
287	0	docs/source/usage/tool_calling.md

[6faec5450] Woosuk Kwon 2024-12-09 [V1] Do not store `None` in self.generators (#11038)
6	1	vllm/v1/worker/gpu_input_batch.py

[5ed5d5f12] Richard Liu 2024-12-09 Build tpu image in release pipeline (#10936)
16	0	.buildkite/release-pipeline.yaml

[b63ba8483] Gregory Shtrasberg 2024-12-09 [ROCm][bugfix] scpecilative decoding worker class (#11035)
2	0	vllm/platforms/rocm.py

[9c6459e4c] xendo 2024-12-09 [Neuron] Upgrade neuron to 2.20.2 (#11016)
2	1	Dockerfile.neuron
1	1	vllm/utils.py

[1a2f8fb82] youkaichao 2024-12-09 [v1] fix use compile sizes (#11000)
1	0	vllm/config.py
3	0	vllm/v1/worker/gpu_model_runner.py

[cbcbdb1ce] Konrad Zawora 2024-12-09 [Bugfix][Hardware][Gaudi] Bump vllm_hpu_extension version (#11028)
1	1	requirements-hpu.txt
11	0	vllm/attention/backends/hpu_attn.py

[a811dd660] Isotr0py 2024-12-10 [Model] merged input processor for Phi-3-Vision models (#10977)
2	2	tests/entrypoints/openai/test_vision.py
2	2	tests/entrypoints/openai/test_vision_embedding.py
27	109	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py
93	76	tests/multimodal/test_processor_kwargs.py
2	2	vllm/inputs/registry.py
84	214	vllm/model_executor/models/phi3v.py
25	4	vllm/multimodal/processing.py

[ca871491e] Jee Jee Li 2024-12-10 [Misc][LoRA] Abstract PunicaWrapper (#10955)
33	16	tests/lora/test_layers.py
3	4	vllm/lora/layers.py
4	4	vllm/lora/models.py
0	725	vllm/lora/punica.py
7	0	vllm/lora/punica_wrapper/__init__.py
480	0	vllm/lora/punica_wrapper/punica_base.py
358	0	vllm/lora/punica_wrapper/punica_gpu.py
14	0	vllm/lora/punica_wrapper/punica_selector.py
159	0	vllm/lora/punica_wrapper/utils.py

[3b61cb450] Woosuk Kwon 2024-12-09 [V1] Further reduce CPU overheads in flash-attn (#10989)
12	2	csrc/cache_kernels.cu
16	5	vllm/v1/attention/backends/flash_attn.py

[edc4fa318] Kevin H. Luu 2024-12-09 [ci/build] Recompile CI dependencies list with Python 3.12 (#11013)
2	23	requirements-test.txt

[25b79d9fd] Varun Sundar Rabindranath 2024-12-09 [V1] Input Batch Relocation (#10962)
280	0	vllm/v1/worker/gpu_input_batch.py
3	270	vllm/v1/worker/gpu_model_runner.py

[aea2fc38c] wangxiyuan 2024-12-10 [Platform] Move `async output` check to platform (#10768)
3	14	vllm/config.py
5	1	vllm/platforms/cpu.py
11	1	vllm/platforms/cuda.py
5	1	vllm/platforms/hpu.py
11	0	vllm/platforms/interface.py
5	1	vllm/platforms/neuron.py
5	1	vllm/platforms/openvino.py
11	1	vllm/platforms/rocm.py
5	1	vllm/platforms/tpu.py
5	1	vllm/platforms/xpu.py

[e691b26f6] Russell Bryant 2024-12-09 [Core] Require xgrammar >= 0.1.6 (#11021)
1	1	requirements-common.txt

[c69035792] Roger Wang 2024-12-09 [V1] Fix Detokenizer loading in `AsyncLLM` (#10997)
6	1	vllm/v1/engine/async_llm.py

[d1c2e15eb] youkaichao 2024-12-08 [torch.compile] add dynamo time tracking (#11005)
6	0	vllm/compilation/backends.py
3	3	vllm/compilation/decorators.py
7	2	vllm/compilation/monitor.py

[af7c4a92e] Roger Wang 2024-12-08 [Doc][V1] Add V1 support column for multimodal models (#10998)
25	1	docs/source/models/supported_models.rst

[46004e83a] youkaichao 2024-12-08 [misc] clean up and unify logging (#10999)
34	39	vllm/config.py
3	51	vllm/engine/llm_engine.py

[43b05fa31] youkaichao 2024-12-08 [torch.compile][misc] fix comments (#10993)
2	2	vllm/config.py

[a11f32652] Roger Wang 2024-12-08 [V1] Initial support of multimodal models for V1 re-arch (#10699)
8	8	vllm/engine/arg_utils.py
5	0	vllm/model_executor/models/interfaces.py
57	11	vllm/model_executor/models/internvl.py
63	9	vllm/model_executor/models/molmo.py
92	29	vllm/model_executor/models/pixtral.py
27	1	vllm/model_executor/models/utils.py
2	1	vllm/multimodal/inputs.py
6	4	vllm/multimodal/utils.py
2	2	vllm/v1/core/scheduler.py
21	3	vllm/v1/engine/llm_engine.py
1	1	vllm/v1/engine/mm_input_mapper.py

[fd57d2b53] youkaichao 2024-12-08 [torch.compile] allow candidate compile sizes (#10984)
4	4	tests/engine/test_arg_utils.py
22	22	vllm/config.py
1	4	vllm/engine/arg_utils.py
1	5	vllm/entrypoints/llm.py

[7be15d935] youkaichao 2024-12-07 [core][misc] remove use_dummy driver for _run_workers (#10920)
12	15	vllm/executor/ray_gpu_executor.py
12	16	vllm/executor/ray_hpu_executor.py
10	11	vllm/executor/ray_tpu_executor.py
9	2	vllm/executor/ray_xpu_executor.py

[1b62745b1] youkaichao 2024-12-07 [core][executor] simplify instance id (#10976)
6	1	vllm/config.py
0	6	vllm/envs.py
1	5	vllm/executor/cpu_executor.py
1	4	vllm/executor/multiproc_gpu_executor.py
1	6	vllm/executor/ray_gpu_executor.py
1	6	vllm/executor/ray_hpu_executor.py
1	5	vllm/executor/ray_tpu_executor.py
1	5	vllm/executor/ray_xpu_executor.py
9	16	vllm/utils.py
1	1	vllm/worker/worker_base.py

[78029b34e] zhou fan 2024-12-08 [BugFix][Kernel]: fix illegal memory access in causal_conv1d when conv_states is None (#10928)
1	1	csrc/mamba/causal_conv1d/causal_conv1d.cu
22	17	tests/kernels/test_causal_conv1d.py

[c889d5888] Cyrus Leung 2024-12-08 [Doc] Explicitly state that PP isn't compatible with speculative decoding yet (#10975)
3	0	docs/source/usage/spec_decode.rst
13	3	tests/distributed/test_pipeline_parallel.py
2	1	vllm/model_executor/models/exaone.py
3	2	vllm/model_executor/models/granite.py
2	1	vllm/model_executor/models/llama.py
3	1	vllm/model_executor/models/nemotron.py
2	1	vllm/model_executor/models/solar.py
4	0	vllm/spec_decode/spec_decode_worker.py

[39e227c7a] Cyrus Leung 2024-12-08 [Model] Update multi-modal processor to support Mantis(LLaVA) model (#10711)
2	0	.buildkite/test-pipeline.yaml
5	1	docs/source/models/supported_models.rst
17	0	examples/offline_inference_vision_language.py
0	3	requirements-test.in
22	8	tests/models/decoder_only/vision_language/test_models.py
14	6	tests/models/decoder_only/vision_language/vlm_utils/core.py
34	1	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
12	7	tests/models/decoder_only/vision_language/vlm_utils/types.py
1	0	tests/models/registry.py
2	4	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
60	8	vllm/model_executor/models/llava.py
1	0	vllm/model_executor/models/registry.py
2	2	vllm/multimodal/processing.py
3	38	vllm/multimodal/registry.py

[1c768fe53] Cyrus Leung 2024-12-08 [Doc] Explicitly state that InternVL 2.5 is supported (#10978)
2	2	docs/source/models/supported_models.rst
1	1	examples/offline_inference_vision_language.py
1	1	examples/offline_inference_vision_language_multi_image.py

[bf0e382e1] Cyrus Leung 2024-12-07 [Model] Composite weight loading for multimodal Qwen2 (#10944)
9	1	vllm/config.py
1	3	vllm/model_executor/model_loader/loader.py
3	7	vllm/model_executor/model_loader/utils.py
10	7	vllm/model_executor/models/qwen2.py
32	85	vllm/model_executor/models/qwen2_audio.py
83	96	vllm/model_executor/models/qwen2_vl.py
9	6	vllm/model_executor/models/utils.py

[b26b4cd03] Isotr0py 2024-12-07 [Misc][LoRA] Refactor and clean MergedQKVParallelLinearWithLora implementation  (#10958)
60	263	vllm/lora/layers.py

[f13cf9ad5] Gregory Shtrasberg 2024-12-07 [Build] Fix for the Wswitch-bool clang warning (#10060)
4	7	csrc/attention/paged_attention_v1.cu
4	7	csrc/attention/paged_attention_v2.cu

[955fa9533] Cyrus Leung 2024-12-07 [3/N] Support and implement merged input processor for LLaVA model (#10676)
3	46	tests/multimodal/test_mapper.py
195	82	tests/multimodal/test_processing.py
5	7	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
30	12	vllm/inputs/registry.py
88	131	vllm/model_executor/models/llava.py
36	15	vllm/multimodal/base.py
197	116	vllm/multimodal/processing.py
60	7	vllm/multimodal/registry.py
1	0	vllm/v1/engine/mm_input_mapper.py
11	5	vllm/v1/engine/processor.py

[acf092d34] Jee Jee Li 2024-12-07 [Bugfix] Fix test-pipeline.yaml (#10973)
1	1	.buildkite/test-pipeline.yaml

[69d357ba1] Russell Bryant 2024-12-06 [Core] Cleanup startup logging a bit (#10961)
1	0	vllm/engine/arg_utils.py
4	4	vllm/entrypoints/openai/api_server.py
1	1	vllm/plugins/__init__.py

[dcdc3fafe] youkaichao 2024-12-06 [ci] fix broken tests (#10956)
6	6	vllm/worker/model_runner.py

[c05cfb67d] youkaichao 2024-12-06 [misc] fix typo (#10960)
1	1	vllm/config.py

[740627404] Sam Stoelinga 2024-12-06 [Doc] add KubeAI to serving integrations (#10837)
17	0	docs/source/serving/deploying_with_kubeai.rst
1	0	docs/source/serving/integrations.rst

[8b5963185] Michael Goin 2024-12-06 [Core] Support Lark grammars for XGrammar (#10870)
0	8	vllm/model_executor/guided_decoding/__init__.py
16	1	vllm/model_executor/guided_decoding/xgrammar_decoding.py
162	0	vllm/model_executor/guided_decoding/xgrammar_utils.py

[a1887f2c9] youkaichao 2024-12-06 [torch.compile] fix deprecated code (#10948)
1	1	vllm/compilation/backends.py

[222f5b082] Cyrus Leung 2024-12-06 [CI/Build] Fix broken multimodal test (#10950)
4	0	tests/models/embedding/vision_language/test_llava_next.py

[b031a455a] youkaichao 2024-12-06 [torch.compile] add logging for compilation time (#10941)
46	10	vllm/compilation/backends.py
5	0	vllm/compilation/decorators.py
14	0	vllm/compilation/monitor.py
2	0	vllm/config.py
4	0	vllm/engine/llm_engine.py
4	0	vllm/v1/engine/core.py

[db87eb6c6] youkaichao 2024-12-05 [torch.compile] use size tuning for specific sizes (#10933)
6	0	vllm/compilation/backends.py

[9743d64e4] youkaichao 2024-12-05 [ci][build] add tests for python only compilation (#10915)
9	2	.buildkite/test-pipeline.yaml
7	6	setup.py
0	0	tests/{test_lazy_torch_compile.py => standalone_tests/lazy_torch_compile.py}
30	0	tests/standalone_tests/python_only_compile.sh

[a43065272] Konrad Zawora 2024-12-05 [Misc][Gaudi] Avoid torch.compile and enable lazy collectives (#10897)
14	0	vllm/plugins/__init__.py

[998eeafe5] Isotr0py 2024-12-06 [CI/Build] Bump test transformers version (#10106)
1	1	requirements-test.txt
1	24	tests/models/decoder_only/vision_language/test_models.py
1	1	tests/models/decoder_only/vision_language/test_pixtral.py
0	4	tests/models/embedding/vision_language/test_llava_next.py
0	5	tests/models/test_initialization.py

[571da8fc4] Jee Jee Li 2024-12-05 [Misc][LoRA] Clean up the function interface of Punica (#10917)
32	10	tests/lora/test_layers.py
75	100	vllm/lora/fully_sharded_layers.py
193	345	vllm/lora/layers.py
4	4	vllm/lora/models.py
193	172	vllm/lora/punica.py

[39c89e71a] Travis Johnson 2024-12-04 [Misc] Update llama 3.2 template to support system prompt with images (#10901)
2	10	examples/tool_chat_template_llama3.2_json.jinja

[1f958a7d5] Jee Jee Li 2024-12-05 [Bugfix] Fix BNB loader target_modules (#10720)
6	58	vllm/model_executor/model_loader/loader.py

[aa39a8e17] Cyrus Leung 2024-12-05 [Doc] Create a new "Usage" section (#10827)
1	4	docs/source/design/multimodal/multimodal_index.rst
15	10	docs/source/index.rst
1	1	docs/source/models/enabling_multimodal_inputs.rst
17	2	docs/source/models/supported_models.rst
2	2	docs/source/serving/openai_compatible_server.md
0	0	docs/source/{serving => usage}/compatibility_matrix.rst
0	0	docs/source/{models => usage}/engine_args.rst
0	0	docs/source/{serving => usage}/env_vars.rst
2	0	docs/source/{serving => usage}/faq.rst
2	2	docs/source/{models => usage}/lora.rst
161	87	docs/source/{models/vlm.rst => usage/multimodal_inputs.rst}
0	0	docs/source/{models => usage}/performance.rst
4	4	docs/source/{models => usage}/spec_decode.rst
0	0	docs/source/{models => usage}/structured_outputs.rst
0	0	docs/source/{serving => usage}/usage_stats.md
1	1	vllm/attention/backends/rocm_flash_attn.py
4	4	vllm/config.py
1	1	vllm/engine/arg_utils.py
1	1	vllm/engine/output_processor/multi_step.py
1	1	vllm/executor/cpu_executor.py
1	1	vllm/platforms/cpu.py
1	1	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/utils.py
1	1	vllm/worker/multi_step_model_runner.py
1	1	vllm/worker/utils.py

[8d370e91c] Michael Goin 2024-12-04 [Bugfix] Fallback to outlines for complex json schemas (#10899)
31	0	tests/entrypoints/conftest.py
28	0	tests/entrypoints/llm/test_guided_generate.py
43	0	vllm/model_executor/guided_decoding/__init__.py

[7883c2bbe] Kevin H. Luu 2024-12-04 [benchmark] Make H100 benchmark optional (#10908)
5	0	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[2a56e1264] Woosuk Kwon 2024-12-04 [V1] Fix when max_model_len is not divisible by block_size (#10903)
10	3	vllm/v1/worker/gpu_model_runner.py

[e4c34c23d] Daniele 2024-12-04 [CI/Build] improve python-only dev setup (#9621)
12	29	docs/source/getting_started/installation.rst
9	87	python_only_dev.py
79	4	setup.py
2	1	vllm/envs.py

[82eb5ea8f] Chendi.Xue 2024-12-04 Benchmark serving structured output (#10880)
6	0	benchmarks/backend_request_func.py
881	0	benchmarks/benchmark_serving_guided.py

[10398b470] Isotr0py 2024-12-05 [Model] Consolidate ViTs attention implementation without mask (#10893)
63	0	vllm/attention/layer.py
4	41	vllm/model_executor/models/blip.py
4	42	vllm/model_executor/models/clip.py
6	16	vllm/model_executor/models/glm4_vision_encoder.py
4	21	vllm/model_executor/models/idefics2_vision_model.py
4	24	vllm/model_executor/models/intern_vit.py
11	12	vllm/model_executor/models/internvl.py
9	29	vllm/model_executor/models/molmo.py
4	41	vllm/model_executor/models/siglip.py

[01d079fd8] Xin Yang 2024-12-04 [LoRA] Change lora_tokenizers capacity (#10796)
20	0	tests/lora/test_tokenizer_group.py
1	1	vllm/engine/llm_engine.py
1	2	vllm/engine/multiprocessing/client.py
5	4	vllm/transformers_utils/tokenizer_group/__init__.py
2	1	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
1	1	vllm/v1/engine/async_llm.py
1	1	vllm/v1/engine/llm_engine.py

[c92acb969] Kevin H. Luu 2024-12-04 [ci/build] Update vLLM postmerge ECR repo (#10887)
3	3	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
2	2	.buildkite/nightly-benchmarks/scripts/wait-for-image.sh
1	1	docs/source/getting_started/installation.rst

[8db957ee3] jianzheng 2024-12-04 [bugfix] fixed parameter “n” when set parameter “bestof” > 1 (#10854)
3	2	vllm/sampling_params.py

[c9ca4fce3] Kevin H. Luu 2024-12-03 [ci/build] Job to build and push release image (#10877)
13	0	.buildkite/release-pipeline.yaml

[fa2dea61d] Kevin H. Luu 2024-12-03 [ci/build] Change queue name for Release jobs (#10875)
2	2	.buildkite/release-pipeline.yaml

[b5b647b08] wangxiyuan 2024-12-04 Drop ROCm load format check (#10767)
3	20	vllm/config.py

[d2bd88b12] Tyler Michael Smith 2024-12-03 [CI/Build] Replace mean with torch.all in test_pynccl.py (#10876)
9	16	tests/distributed/test_pynccl.py

[381ac93bb] Chendi.Xue 2024-12-03 [Benchmark] Benchmark structured output with datasets (#10557)
494	0	benchmarks/benchmark_guided.py
113	0	benchmarks/structured_schemas/structured_schema_1.json

[a061fe601] Gregory Shtrasberg 2024-12-03 [Build][Bugfix] Using the correct type hint (#10866)
2	2	vllm/utils.py

[7c32b6861] tomeras91 2024-12-03 [Frontend] correctly record prefill and decode time metrics  (#10853)
2	2	vllm/engine/metrics.py

[7090c27bb] Michael Goin 2024-12-03 [Bugfix] Only require XGrammar on x86 (#10865)
1	1	requirements-common.txt
7	0	vllm/model_executor/guided_decoding/__init__.py
2	2	vllm/platforms/__init__.py
26	0	vllm/platforms/interface.py

[2f2cdc745] Yan Ma 2024-12-04 [MISC][XPU] quick fix for XPU CI (#10859)
4	3	.buildkite/run-xpu-test.sh

[3bc94cab6] Alexander Matveev 2024-12-03 [V1] VLM - Run the mm_mapper preprocessor in the frontend process (#10640)
1	2	tests/v1/engine/test_engine_core.py
1	2	tests/v1/engine/test_engine_core_client.py
23	1	vllm/inputs/data.py
3	4	vllm/v1/engine/__init__.py
0	7	vllm/v1/engine/core.py
11	2	vllm/v1/engine/processor.py
8	7	vllm/v1/request.py

[f6084f632] Yang Zheng 2024-12-03 [Speculative Decoding] Move indices to device before filtering output (#10850)
6	3	vllm/spec_decode/multi_step_worker.py

[9323a3153] Aaron Pham 2024-12-03 [Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)
1	0	docs/source/conf.py
1	0	requirements-common.txt
27	0	tests/entrypoints/llm/test_guided_generate.py
2	1	tests/model_executor/test_guided_processors.py
8	7	vllm/config.py
5	4	vllm/engine/arg_utils.py
14	4	vllm/engine/async_llm_engine.py
11	4	vllm/engine/llm_engine.py
3	2	vllm/engine/multiprocessing/client.py
62	11	vllm/model_executor/guided_decoding/__init__.py
251	0	vllm/model_executor/guided_decoding/xgrammar_decoding.py

[3257d449f] Cyrus Leung 2024-12-03 [Misc] Remove deprecated names (#10817)
6	2	vllm/engine/async_llm_engine.py
3	2	vllm/engine/llm_engine.py
4	1	vllm/engine/multiprocessing/__init__.py
5	2	vllm/engine/multiprocessing/client.py
11	0	vllm/entrypoints/llm.py
0	31	vllm/inputs/__init__.py
0	31	vllm/inputs/data.py
2	3	vllm/model_executor/models/aria.py
0	15	vllm/multimodal/__init__.py
0	15	vllm/multimodal/base.py

[ef51831ee] Russell Bryant 2024-12-03 [Doc] Add github links for source code references (#10672)
2	1	docs/requirements-docs.txt
66	0	docs/source/conf.py

[dc5ce861b] youkaichao 2024-12-02 [torch.compile] remove compilation_context and simplify code (#10838)
4	5	tests/compile/piecewise/test_simple.py
17	16	tests/compile/piecewise/test_toy_llama.py
3	2	tests/models/decoder_only/language/test_jamba.py
3	2	tests/models/decoder_only/language/test_mamba.py
2	2	tests/worker/test_encoder_decoder_model_runner.py
3	2	tests/worker/test_model_runner.py
0	4	vllm/compilation/backends.py
0	23	vllm/compilation/compile_context.py
75	8	vllm/config.py
2	4	vllm/model_executor/models/jamba.py
2	4	vllm/model_executor/models/mamba.py
8	6	vllm/v1/worker/gpu_model_runner.py
3	3	vllm/worker/enc_dec_model_runner.py
6	62	vllm/worker/model_runner.py

[21fe7b481] youkaichao 2024-12-02 [core][distributed] add pynccl broadcast (#10843)
43	2	tests/distributed/test_pynccl.py
19	0	vllm/distributed/device_communicators/pynccl.py
16	0	vllm/distributed/device_communicators/pynccl_wrapper.py

[a4cf25615] Jee Jee Li 2024-12-03 [Bugfix] Fix QKVParallelLinearWithShardedLora bias bug (#10844)
0	1	.buildkite/test-pipeline.yaml
1	8	vllm/lora/fully_sharded_layers.py

[d746268e9] zixuanzhang226 2024-12-02 [Model] support bitsandbytes quantization with minicpm model (#10842)
10	0	vllm/model_executor/models/minicpm.py

[4433195ab] Michael Goin 2024-12-02 [Bugfix] Prevent benchmark_throughput.py from using duplicated random prompts (#10753)
30	17	benchmarks/benchmark_throughput.py

[4c05edb33] Isotr0py 2024-12-03 [Model] Add TP and BNB quantization support to LlavaMultiModalProjector (#10834)
11	3	vllm/model_executor/model_loader/loader.py
23	12	vllm/model_executor/models/llava.py

[9b14d978a] Jani Monoses 2024-12-02 Fix openvino on GPU (#10793)
3	3	vllm/worker/openvino_worker.py

[519cc6ca1] Yan Ma 2024-12-03 [Misc][XPU] Avoid torch compile for XPU platform (#10747)
4	2	.buildkite/run-xpu-test.sh
4	0	vllm/plugins/__init__.py

[b45f0d794] Jee Jee Li 2024-12-03 [Misc][LoRA] Move the implementation of lora bias to punica.py (#10829)
27	33	tests/lora/test_llama_tp.py
12	29	vllm/lora/fully_sharded_layers.py
12	101	vllm/lora/layers.py
105	12	vllm/lora/punica.py

[a4c4daf36] youkaichao 2024-12-02 [misc] use out argument for flash attention (#10822)
1	0	vllm/attention/backends/abstract.py
2	0	vllm/attention/backends/blocksparse_attn.py
19	36	vllm/attention/backends/flash_attn.py
4	0	vllm/attention/backends/flashinfer.py
1	0	vllm/attention/backends/hpu_attn.py
1	0	vllm/attention/backends/ipex_attn.py
1	0	vllm/attention/backends/pallas.py
1	0	vllm/attention/backends/rocm_flash_attn.py
1	0	vllm/attention/backends/torch_sdpa.py
1	0	vllm/attention/backends/xformers.py
72	4	vllm/attention/layer.py
1	1	vllm/config.py
39	116	vllm/v1/attention/backends/flash_attn.py

[e95f275f5] Cyrus Leung 2024-12-02 [CI/Build] Update `mistral_common` version for tests and docs (#10825)
1	1	docs/requirements-docs.txt
1	1	requirements-test.in
1	1	requirements-test.txt

[ef31eabc6] zhou fan 2024-12-02 [Model]: add some tests for aria model (#10770)
5	1	tests/conftest.py
30	0	tests/models/decoder_only/vision_language/test_models.py
9	2	tests/models/decoder_only/vision_language/vlm_utils/core.py
7	0	tests/models/decoder_only/vision_language/vlm_utils/types.py

[995a14857] wangxiyuan 2024-12-02 [doc]Update config docstring (#10732)
12	1	vllm/config.py

[63a164172] youkaichao 2024-12-01 [misc] remove xverse modeling file (#10814)
1	1	vllm/model_executor/models/registry.py
0	423	vllm/model_executor/models/xverse.py

[e25810ae2] Maximilien de Bayser 2024-12-01 Fill TorchSDPAAttentionMetadata seq_lens_field for prefill (#10799)
5	1	vllm/attention/backends/torch_sdpa.py

[073a4bd1c] Woosuk Kwon 2024-12-01 [Kernel] Use `out` arg in flash_attn_varlen_func (#10811)
1	1	CMakeLists.txt
17	3	tests/kernels/test_flash_attn.py
3	3	vllm/v1/attention/backends/flash_attn.py

[b7954776f] cduk 2024-12-02 [core] Avoid metrics log noise when idle - include speculative decodi… (#10809)
2	2	vllm/engine/metrics.py

[b18c9bbab] Isotr0py 2024-12-02 [Model] Add BNB support to Llava and Pixtral-HF (#10795)
9	0	vllm/model_executor/models/llava.py

[0590ec3fd] Kuntai Du 2024-12-01 [Core] Implement disagg prefill by StatelessProcessGroup (#10502)
4	0	.buildkite/test-pipeline.yaml
144	0	benchmarks/disagg_benchmarks/disagg_overhead_benchmark.sh
164	0	benchmarks/disagg_benchmarks/disagg_performance_benchmark.sh
61	0	benchmarks/disagg_benchmarks/disagg_prefill_proxy_server.py
60	0	benchmarks/disagg_benchmarks/round_robin_proxy.py
46	0	benchmarks/disagg_benchmarks/visualize_benchmark_results.py
109	0	examples/disaggregated_prefill.sh
119	0	tests/kv_transfer/disagg_test.py
64	0	tests/kv_transfer/module_test.py
160	0	tests/kv_transfer/test_lookup_buffer.py
3	0	tests/kv_transfer/test_lookup_buffer.sh
155	0	tests/kv_transfer/test_send_recv.py
3	0	tests/kv_transfer/test_send_recv.sh
84	0	vllm/config.py
30	0	vllm/distributed/kv_transfer/README.md
0	0	vllm/distributed/kv_transfer/__init__.py
-	-	vllm/distributed/kv_transfer/disagg_prefill_workflow.jpg
0	0	vllm/distributed/kv_transfer/kv_connector/__init__.py
122	0	vllm/distributed/kv_transfer/kv_connector/base.py
19	0	vllm/distributed/kv_transfer/kv_connector/factory.py
261	0	vllm/distributed/kv_transfer/kv_connector/simple_connector.py
0	0	vllm/distributed/kv_transfer/kv_lookup_buffer/__init__.py
108	0	vllm/distributed/kv_transfer/kv_lookup_buffer/base.py
242	0	vllm/distributed/kv_transfer/kv_lookup_buffer/simple_buffer.py
0	0	vllm/distributed/kv_transfer/kv_pipe/__init__.py
65	0	vllm/distributed/kv_transfer/kv_pipe/base.py
276	0	vllm/distributed/kv_transfer/kv_pipe/pynccl_pipe.py
75	0	vllm/distributed/kv_transfer/kv_transfer_agent.py
34	1	vllm/distributed/parallel_state.py
14	4	vllm/engine/arg_utils.py
94	11	vllm/worker/model_runner.py
8	5	vllm/worker/worker.py
1	0	vllm/worker/worker_base.py

[c11f17218] Roger Wang 2024-12-01 [Misc] Adding `MMMU-Pro` vision dataset to serving benchmark (#10804)
65	0	benchmarks/benchmark_serving.py

[169a0ff91] youkaichao 2024-12-01 [doc] add warning about comparing hf and vllm outputs (#10805)
3	0	docs/source/models/supported_models.rst

[d2f058e76] Cyrus Leung 2024-12-01 [Misc] Rename embedding classes to pooling (#10801)
1	1	examples/offline_inference_embedding.py
3	3	tests/entrypoints/llm/test_encode.py
2	2	tests/models/test_registry.py
2	2	tests/worker/test_model_input.py
27	4	vllm/__init__.py
1	1	vllm/config.py
12	12	vllm/engine/async_llm_engine.py
4	4	vllm/engine/llm_engine.py
7	7	vllm/engine/multiprocessing/client.py
2	3	vllm/engine/protocol.py
15	15	vllm/entrypoints/llm.py
6	6	vllm/entrypoints/openai/serving_embedding.py
5	5	vllm/entrypoints/openai/serving_score.py
5	6	vllm/model_executor/models/__init__.py
3	3	vllm/model_executor/models/adapters.py
2	2	vllm/model_executor/models/interfaces.py
7	8	vllm/model_executor/models/interfaces_base.py
8	8	vllm/model_executor/models/registry.py
39	16	vllm/outputs.py
2	2	vllm/v1/engine/async_llm.py
4	4	vllm/v1/engine/async_stream.py
2	2	vllm/worker/{cpu_embedding_model_runner.py => cpu_pooling_model_runner.py}
2	2	vllm/worker/cpu_worker.py
3	3	vllm/worker/{embedding_model_runner.py => pooling_model_runner.py}
2	2	vllm/worker/worker.py

[f877a7d12] Cyrus Leung 2024-12-01 [Misc] Improve type annotations for `support_torch_compile` (#10763)
29	9	vllm/compilation/decorators.py

[133707123] Cyrus Leung 2024-12-01 [Model] Replace embedding models with pooling adapter (#10769)
2	2	.buildkite/test-pipeline.yaml
14	1	docs/source/models/supported_models.rst
0	1	tests/conftest.py
5	0	tests/models/embedding/language/test_embedding.py
14	17	tests/models/test_registry.py
40	5	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
1	2	tests/test_config.py
25	0	vllm/config.py
8	8	vllm/inputs/registry.py
1	3	vllm/model_executor/layers/pooler.py
14	4	vllm/model_executor/model_loader/loader.py
14	4	vllm/model_executor/model_loader/utils.py
98	0	vllm/model_executor/models/adapters.py
3	2	vllm/model_executor/models/blip2.py
2	56	vllm/model_executor/models/gemma2.py
3	2	vllm/model_executor/models/internvl.py
7	95	vllm/model_executor/models/llama.py
3	2	vllm/model_executor/models/llava.py
5	21	vllm/model_executor/models/llava_next.py
3	2	vllm/model_executor/models/llava_next_video.py
3	2	vllm/model_executor/models/llava_onevision.py
3	2	vllm/model_executor/models/paligemma.py
14	25	vllm/model_executor/models/phi3v.py
3	2	vllm/model_executor/models/pixtral.py
12	16	vllm/model_executor/models/qwen2.py
2	16	vllm/model_executor/models/qwen2_vl.py
41	18	vllm/model_executor/models/registry.py
3	2	vllm/model_executor/models/ultravox.py
19	5	vllm/model_executor/models/utils.py
3	3	vllm/multimodal/base.py
3	2	vllm/multimodal/registry.py
19	3	vllm/utils.py

[7e4bbda57] wangxiyuan 2024-11-30 [doc] format fix (#10789)
1	1	docs/source/automatic_prefix_caching/details.md
18	18	docs/source/getting_started/gaudi-installation.rst

[e7cfc4ef4] Patrick von Platen 2024-11-30 [Interleaved ATTN] Support for Mistral-8B (#10591)
15	1	vllm/model_executor/models/llama.py

[16ee07f22] Isotr0py 2024-11-30 [Model] Refactor Molmo weights loading to use AutoWeightsLoader (#10771)
111	102	vllm/model_executor/models/molmo.py

[40bc24257] Nicolò Lucchesi 2024-11-30 [Bugfix] Fix OpenVino/Neuron `driver_worker` init (#10779)
4	2	vllm/executor/neuron_executor.py
2	1	vllm/executor/openvino_executor.py

[661175bc8] wangxiyuan 2024-11-29 [platform] Add verify_quantization in platform. (#10757)
1	27	vllm/config.py
1	0	vllm/platforms/cpu.py
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
13	0	vllm/platforms/interface.py
2	0	vllm/platforms/neuron.py
1	0	vllm/platforms/openvino.py
15	0	vllm/platforms/rocm.py
2	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py

[3132aac04] Jee Jee Li 2024-11-29 [Bugfix] Fix Idefics3 bug (#10778)
47	45	vllm/model_executor/models/idefics3.py

[c82b432d4] wang.yuqi 2024-11-29 [Misc] typo find in sampling_metadata.py (#10740)
1	0	vllm/model_executor/sampling_metadata.py

[fa6ecb9aa] Cyrus Leung 2024-11-29 [Model] Clean up MiniCPMV (#10751)
16	3	tests/models/decoder_only/vision_language/test_models.py
12	1	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
5	5	vllm/model_executor/layers/fused_moe/layer.py
79	74	vllm/model_executor/models/minicpm.py
2	3	vllm/model_executor/models/minicpm3.py
33	103	vllm/model_executor/models/minicpmv.py
2	26	vllm/model_executor/models/utils.py

[c83919c7a] Isotr0py 2024-11-29 [Model] Add Internlm2 LoRA support (#5064)
1	1	docs/source/models/supported_models.rst
20	2	vllm/model_executor/models/internlm2.py

[98f47f2a4] Woosuk Kwon 2024-11-28 [V1] Optimize the CPU overheads in FlashAttention custom op (#10733)
9	8	vllm/v1/attention/backends/flash_attn.py

[8c1e77fb5] Woosuk Kwon 2024-11-28 [Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)
1	1	CMakeLists.txt

[5fc5ce0fe] sixgod 2024-11-28 [Model] Added GLM-4 series hf format model support vllm==0.6.4 (#10561)
5	0	docs/source/models/supported_models.rst
1	0	tests/models/registry.py
1	1	tests/models/test_initialization.py
21	0	vllm/model_executor/models/glm.py
2	0	vllm/model_executor/models/registry.py

[3ed5e7314] Richard Liu 2024-11-28 [TPU] Update requirements-tpu (#10726)
5	5	requirements-tpu.txt

[9a8bff028] Woosuk Kwon 2024-11-28 [Kernel] Update vllm-flash-attn version (#10736)
1	1	CMakeLists.txt

[a79b12240] Woosuk Kwon 2024-11-28 [V1] Do not allocate beyond the max_model_len (#10730)
16	8	tests/v1/core/test_prefix_caching.py
17	0	vllm/v1/core/kv_cache_manager.py
8	7	vllm/v1/core/scheduler.py

[d9b4b3f06] Ricky Xu 2024-11-27 [Bug][CLI] Allow users to disable prefix caching explicitly (#10724)
19	0	tests/engine/test_arg_utils.py
19	0	tests/v1/engine/test_engine_args.py
7	3	vllm/engine/arg_utils.py

[278be671a] 罗泽轩 2024-11-28 [Doc] Update model in arch_overview.rst to match comment (#10701)
1	1	docs/source/design/arch_overview.rst

[70dc14fbd] zixuanzhang226 2024-11-27 [Model] support bitsandbytes quantization with minicpm3 model (#10682)
6	0	vllm/model_executor/models/minicpm3.py

[cb4e1c3f3] youkaichao 2024-11-27 [misc] upgrade filelock version (#10731)
1	1	requirements-common.txt

[395b1c745] tomeras91 2024-11-27 [Frontend] don't block event loop in tokenization (preprocess) in OpenAI compatible server (#10635)
137	0	tests/entrypoints/openai/test_async_tokenization.py
1	1	vllm/entrypoints/openai/serving_completion.py
8	7	vllm/entrypoints/openai/serving_embedding.py
40	35	vllm/entrypoints/openai/serving_engine.py
6	4	vllm/entrypoints/openai/serving_score.py
8	7	vllm/entrypoints/openai/serving_tokenization.py
6	2	vllm/utils.py

[9b4b15039] Cyrus Leung 2024-11-28 [Bugfix] Ignore `lm_head` when loading embedding models (#10719)
2	0	vllm/model_executor/models/bert.py
2	0	vllm/model_executor/models/gemma2.py
2	0	vllm/model_executor/models/llama.py
2	0	vllm/model_executor/models/qwen2.py

[197b4484a] Mor Zusman 2024-11-27 [Bugfix][Mamba] Fix Multistep on Mamba-like models (#10705)
38	0	tests/models/decoder_only/language/test_jamba.py
36	0	tests/models/decoder_only/language/test_mamba.py
5	2	vllm/engine/async_llm_engine.py
5	2	vllm/engine/llm_engine.py

[b98c62ba4] Isotr0py 2024-11-28 [Bugfix] Fix GGUF inference with FP16 unquantized checkpoint (#10675)
60	9	vllm/model_executor/layers/quantization/gguf.py

[c411def23] youkaichao 2024-11-27 [torch.compile] fix shape specialization (#10722)
3	4	vllm/config.py

[308cc5e21] youkaichao 2024-11-27 [ci] fix slow tests (#10698)
17	5	tests/entrypoints/llm/test_lazy_outlines.py
17	5	tests/test_lazy_torch_compile.py
5	5	tests/vllm_test_utils/vllm_test_utils/blame.py

[9e0a147d5] Roger Wang 2024-11-27 [V1] Update interface for mistral-format Pixtral (#10703)
28	19	vllm/model_executor/models/pixtral.py

[418cb3b93] Li, Jiang 2024-11-27 [Bugfix][Hardware][CPU] Fix intel-omp version to avoid segfault (#10700)
1	1	Dockerfile.cpu

[1209261e9] shunxing12345 2024-11-27 [Model] Support telechat2 (#10311)
5	0	docs/source/models/supported_models.rst
2	0	tests/models/registry.py
4	2	vllm/model_executor/models/llama.py
2	0	vllm/model_executor/models/registry.py
131	0	vllm/model_executor/models/telechat2.py
3	1	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
61	0	vllm/transformers_utils/configs/telechat2.py

[e2251109c] Tyler Michael Smith 2024-11-27 [Kernel] Remove if-else with identical branches in marlin 2:4 (#10687)
3	7	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu

[15cc2a9f1] Jee Jee Li 2024-11-27 [Misc]Further  reduce BNB static variable (#10597)
130	88	vllm/model_executor/model_loader/loader.py
0	8	vllm/model_executor/models/baichuan.py
0	6	vllm/model_executor/models/falcon.py
0	9	vllm/model_executor/models/gemma.py
0	9	vllm/model_executor/models/gemma2.py
0	15	vllm/model_executor/models/idefics3.py
0	9	vllm/model_executor/models/llama.py
0	34	vllm/model_executor/models/minicpmv.py
0	14	vllm/model_executor/models/mllama.py
0	3	vllm/model_executor/models/opt.py
0	3	vllm/model_executor/models/phi.py
0	6	vllm/model_executor/models/phi3.py
1	6	vllm/model_executor/models/qwen.py
0	9	vllm/model_executor/models/qwen2.py

[e85250b1d] Kunshang Ji 2024-11-27 [Hardware][Gaudi]add get_name method for HPUAttentionBackend (#10667)
4	0	vllm/attention/backends/hpu_attn.py

[cfb3bf25f] yansh97 2024-11-27 [bugfix] fix the default value of llm_int8_threshold in BitsAndBytesConfig (#10657)
2	2	vllm/model_executor/layers/quantization/bitsandbytes.py

[1bf905dda] jeongin601 2024-11-27 [Bugfix][SpecDecode] apply sampling parameters to target probabilities for consistency in rejection sampling. (#10198)
1	1	tests/spec_decode/e2e/test_mlp_correctness.py
8	0	tests/spec_decode/test_batch_expansion.py
1	13	vllm/spec_decode/batch_expansion.py

[0a4d96850] Roger Wang 2024-11-26 [V1] Update interface for idefics3 (#10680)
46	27	vllm/model_executor/models/idefics3.py

[0a71900bc] Chendi.Xue 2024-11-26 Remove hard-dependencies of Speculative decode to CUDA workers (#10587)
2	2	tests/spec_decode/test_spec_decode_worker.py
1	0	vllm/config.py
16	1	vllm/model_executor/layers/spec_decode_base_sampler.py
7	1	vllm/platforms/cpu.py
3	1	vllm/platforms/cuda.py
12	12	vllm/spec_decode/draft_model_runner.py
5	3	vllm/spec_decode/interfaces.py
5	4	vllm/spec_decode/medusa_worker.py
14	1	vllm/spec_decode/metrics.py
24	7	vllm/spec_decode/multi_step_worker.py
2	1	vllm/spec_decode/ngram_worker.py
25	11	vllm/spec_decode/spec_decode_worker.py
11	22	vllm/spec_decode/target_model_runner.py
8	4	vllm/spec_decode/util.py
37	2	vllm/worker/cpu_model_runner.py
25	2	vllm/worker/cpu_worker.py
15	0	vllm/worker/model_runner_base.py
4	3	vllm/worker/worker.py
3	0	vllm/worker/worker_base.py

[2f0a0a17a] Roger Wang 2024-11-26 [V1] Refactor model executable interface for multimodal models (#10570)
37	24	vllm/model_executor/models/blip2.py
41	17	vllm/model_executor/models/chameleon.py
35	19	vllm/model_executor/models/chatglm.py
29	14	vllm/model_executor/models/fuyu.py
35	1	vllm/model_executor/models/interfaces.py
39	15	vllm/model_executor/models/internvl.py
8	7	vllm/model_executor/models/llava.py
33	18	vllm/model_executor/models/llava_next.py
29	15	vllm/model_executor/models/llava_next_video.py
53	21	vllm/model_executor/models/llava_onevision.py
46	42	vllm/model_executor/models/molmo.py
31	21	vllm/model_executor/models/paligemma.py
9	7	vllm/model_executor/models/phi3v.py
37	22	vllm/model_executor/models/qwen2_audio.py
68	34	vllm/model_executor/models/qwen2_vl.py
48	24	vllm/model_executor/models/ultravox.py
1	4	vllm/model_executor/models/utils.py
2	1	vllm/v1/worker/gpu_model_runner.py

[7576cd38d] Michael Goin 2024-11-26 [Bugfix] Check bnb_4bit_quant_storage for bitsandbytes (#10642)
11	0	vllm/model_executor/layers/quantization/bitsandbytes.py

[9a99273b4] Michael Goin 2024-11-26 [Bugfix] Fix using `-O[0,3]` with LLM entrypoint (#10677)
4	1	vllm/engine/arg_utils.py
8	2	vllm/entrypoints/llm.py

[f5792c7c4] Conroy Cheers 2024-11-27 [Hardware][NVIDIA] Add non-NVML CUDA mode for Jetson (#9735)
5	5	CMakeLists.txt
9	1	vllm/platforms/__init__.py
141	81	vllm/platforms/cuda.py

[db66e018e] Murali Andoorveedu 2024-11-26 [Bugfix] Fix for Spec model TP + Chunked Prefill (#10232)
1	1	docs/source/serving/compatibility_matrix.rst
39	0	tests/core/test_chunked_prefill_scheduler.py
0	46	tests/spec_decode/e2e/test_compatibility.py
57	0	tests/spec_decode/e2e/test_integration_dist_tp2.py
2	1	tests/spec_decode/test_spec_decode_worker.py
0	10	vllm/config.py
19	9	vllm/core/scheduler.py
27	6	vllm/spec_decode/spec_decode_worker.py

[1f6584ee8] Kunshang Ji 2024-11-26 [V1] Enable profile for LLMEngine (#10665)
3	3	vllm/v1/engine/llm_engine.py

[334d64d1e] youkaichao 2024-11-26 [ci] add vllm_test_utils (#10659)
4	0	Dockerfile
4	0	Dockerfile.cpu
3	0	Dockerfile.hpu
3	0	Dockerfile.neuron
3	0	Dockerfile.openvino
3	0	Dockerfile.ppc64le
3	0	Dockerfile.rocm
3	0	Dockerfile.tpu
2	1	Dockerfile.xpu
16	7	tests/entrypoints/llm/test_lazy_outlines.py
1	53	tests/test_lazy_torch_compile.py
7	0	tests/vllm_test_utils/setup.py
8	0	tests/vllm_test_utils/vllm_test_utils/__init__.py
53	0	tests/vllm_test_utils/vllm_test_utils/blame.py

[940635343] Cyrus Leung 2024-11-26 [Misc] Remove outdated init protocols (#10655)
0	30	vllm/model_executor/models/interfaces.py
1	1	vllm/model_executor/models/interfaces_base.py

[9a88f8979] Sage Moore 2024-11-26 custom allreduce + torch.compile (#10121)
0	1	docs/source/getting_started/debugging.rst
6	9	tests/distributed/test_pynccl.py
0	2	tests/distributed/test_utils.py
13	13	vllm/distributed/device_communicators/pynccl.py
36	74	vllm/distributed/parallel_state.py
4	2	vllm/v1/worker/gpu_model_runner.py

[519e8e418] Ricky Xu 2024-11-25 [v1] EngineArgs for better config handling for v1 (#10382)
1	1	.buildkite/test-pipeline.yaml
3	0	tests/v1/engine/test_async_llm.py
42	0	tests/v1/engine/test_engine_args.py
2	1	tests/v1/engine/test_engine_core.py
4	2	tests/v1/engine/test_engine_core_client.py
50	3	vllm/engine/arg_utils.py
1	1	vllm/engine/async_llm_engine.py
1	1	vllm/engine/llm_engine.py
1	1	vllm/engine/multiprocessing/engine.py
2	2	vllm/entrypoints/openai/api_server.py
1	1	vllm/v1/engine/async_llm.py
0	13	vllm/v1/engine/core.py
1	1	vllm/v1/engine/llm_engine.py

[a6760f645] Sanket Kale 2024-11-26 [Feature] vLLM ARM Enablement for AARCH64 CPUs (#9228)
62	0	Dockerfile.arm
24	9	cmake/cpu_extension.cmake
17	1	csrc/cpu/attention.cpp
4	2	csrc/cpu/cpu_types.hpp
515	0	csrc/cpu/cpu_types_arm.hpp
50	0	docs/source/getting_started/arm-installation.rst
1	0	docs/source/index.rst
1	1	examples/offline_inference.py
4	3	requirements-cpu.txt

[45ac4ff27] youkaichao 2024-11-25 [bugfix] fix aria model and add torch.compile (#10645)
4	22	vllm/model_executor/models/aria.py
10	6	vllm/model_executor/models/llama.py

[6e9ff050c] youkaichao 2024-11-25 [misc] do not read HOST_IP (#10644)
1	1	vllm/envs.py
2	2	vllm/executor/ray_gpu_executor.py
2	2	vllm/executor/ray_hpu_executor.py
7	0	vllm/utils.py

[9db713a1d] Shane A 2024-11-25 [Model] Add OLMo November 2024 model (#10503)
5	0	docs/source/models/supported_models.rst
1	0	tests/distributed/test_pipeline_parallel.py
1	0	tests/models/registry.py
432	0	vllm/model_executor/models/olmo2.py
1	0	vllm/model_executor/models/registry.py
3	2	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
166	0	vllm/transformers_utils/configs/olmo2.py

[1b583cfef] Cyrus Leung 2024-11-26 [Doc] Fix typos in docs (#10636)
1	1	docs/source/models/supported_models.rst
1	1	docs/source/serving/compatibility_matrix.rst

[cf73f0c95] Cyrus Leung 2024-11-26 [Model] Enable optional prefix when loading embedding models (#10639)
5	4	vllm/model_executor/models/bert.py
3	1	vllm/model_executor/models/gemma2.py
4	1	vllm/model_executor/models/llama.py
6	6	vllm/model_executor/models/qwen2.py
2	1	vllm/model_executor/models/roberta.py

[b1d920531] zhou fan 2024-11-26 [Model]: Add support for Aria model (#10514)
6	0	docs/source/models/supported_models.rst
18	0	examples/offline_inference_vision_language.py
20	0	examples/offline_inference_vision_language_multi_image.py
2	0	tests/models/registry.py
2	0	vllm/entrypoints/chat_utils.py
695	0	vllm/model_executor/models/aria.py
1	0	vllm/model_executor/models/registry.py
47	0	vllm/transformers_utils/configs/aria.py

[452a4e80c] Simon Mo 2024-11-25 [Docs] Add Snowflake Slides (#10641)
1	1	README.md

[c27df94e1] Wallas Henrique 2024-11-25 [Bugfix] Fix chunked prefill with model dtype float32 on Turing Devices (#9850)
1	0	pyproject.toml
19	0	tests/conftest.py
63	0	tests/kernels/test_prefix_prefill.py
28	13	vllm/attention/ops/prefix_prefill.py
10	0	vllm/config.py
1	0	vllm/engine/arg_utils.py

[d04b13a38] Chauncey 2024-11-26 [Bug]: Authorization ignored when root_path is set (#10606)
103	0	tests/entrypoints/openai/test_root_path.py
4	2	vllm/entrypoints/openai/api_server.py

[2b0879bfc] fzyzcjy 2024-11-25 Super tiny little typo fix (#10633)
1	1	docs/source/quantization/fp8_e5m2_kvcache.rst

[ed46f1432] Cyrus Leung 2024-11-25 [Model] Support `is_causal` HF config field for Qwen2 model (#10621)
10	3	docs/source/models/supported_models.rst
10	2	tests/models/embedding/language/test_embedding.py
2	2	tests/models/embedding/utils.py
11	4	vllm/config.py
18	2	vllm/model_executor/models/qwen2.py

[05d1f8c9c] youkaichao 2024-11-25 [misc] move functions to config.py (#10624)
2	2	tests/compile/piecewise/test_simple.py
2	2	tests/compile/piecewise/test_toy_llama.py
1	2	tests/kernels/test_encoder_decoder_attn.py
1	2	tests/model_executor/test_enabled_custom_ops.py
1	2	vllm/attention/layer.py
1	2	vllm/compilation/wrapper.py
51	0	vllm/config.py
1	1	vllm/model_executor/custom_op.py
1	2	vllm/model_executor/model_loader/loader.py
1	2	vllm/model_executor/model_loader/tensorizer.py
0	56	vllm/plugins/__init__.py

[25d806e95] youkaichao 2024-11-24 [misc] add torch.compile compatibility check (#10618)
1	1	tests/v1/engine/test_engine_core_client.py
14	0	vllm/config.py
7	0	vllm/engine/arg_utils.py

[65813781a] youkaichao 2024-11-24 [torch.compile] add warning for unsupported models (#10622)
1	0	vllm/compilation/counter.py
2	0	vllm/compilation/decorators.py
15	0	vllm/plugins/__init__.py

[7c2134bed] Jee Jee Li 2024-11-25 [torch.compile] force inductor threads (#10620)
4	1	vllm/plugins/__init__.py

[a30a605d2] Cyrus Leung 2024-11-25 [Doc] Add encoder-based models to Supported Models page (#10616)
45	0	docs/source/models/supported_models.rst

[571841b7f] youkaichao 2024-11-24 [torch.compile] support encoder based models (#10613)
10	0	tests/compile/test_basic_correctness.py
7	10	vllm/model_executor/models/bert.py

[7ea3cd7c3] Mengqing Cao 2024-11-25 [Refactor][MISC] del redundant code in ParallelConfig.postinit (#10614)
5	10	vllm/config.py

[214efc2c3] Maximilien de Bayser 2024-11-24 Support Cross encoder models (#10400)
142	0	docs/source/serving/openai_compatible_server.md
58	0	examples/openai_cross_encoder_score.py
20	0	tests/conftest.py
93	0	tests/entrypoints/openai/test_score.py
95	0	tests/models/embedding/language/test_scoring.py
9	0	tests/models/registry.py
16	7	tests/models/test_registry.py
5	0	vllm/config.py
1	0	vllm/core/scheduler.py
123	1	vllm/entrypoints/llm.py
33	2	vllm/entrypoints/openai/api_server.py
36	0	vllm/entrypoints/openai/protocol.py
215	0	vllm/entrypoints/openai/serving_score.py
18	0	vllm/inputs/data.py
2	0	vllm/inputs/preprocess.py
64	0	vllm/model_executor/layers/pooler.py
116	12	vllm/model_executor/models/bert.py
36	0	vllm/model_executor/models/interfaces.py
21	2	vllm/model_executor/models/registry.py
144	35	vllm/model_executor/models/roberta.py
4	1	vllm/multimodal/inputs.py
44	1	vllm/outputs.py
9	0	vllm/sequence.py
15	0	vllm/transformers_utils/config.py
4	0	vllm/worker/cpu_embedding_model_runner.py
13	0	vllm/worker/cpu_model_runner.py
6	1	vllm/worker/embedding_model_runner.py
28	0	vllm/worker/model_runner.py

[49628fe13] Zhuohan Li 2024-11-24 [Doc] Update README.md with Ray Summit talk links (#10610)
1	1	README.md

[e4fbb1441] youkaichao 2024-11-24 [doc] update the code to add models (#10603)
57	28	docs/source/models/adding_model.rst

[c05574786] youkaichao 2024-11-23 [model][utils] add extract_layer_index utility function (#10599)
18	23	vllm/model_executor/models/arctic.py
11	8	vllm/model_executor/models/deepseek.py
5	10	vllm/model_executor/models/gemma2.py
2	6	vllm/model_executor/models/olmoe.py
2	4	vllm/model_executor/models/qwen2_moe.py
21	0	vllm/model_executor/models/utils.py

[eda2b3589] youkaichao 2024-11-23 Revert "Print running script to enhance CI log readability" (#10601)
0	11	.buildkite/test-pipeline.yaml

[1c445dca5] Jee Jee Li 2024-11-24 [CI/Build] Print running script to enhance CI log readability (#10594)
11	0	.buildkite/test-pipeline.yaml

[1700c543a] Jee Jee Li 2024-11-24 [Bugfix] Fix LoRA weight sharding (#10450)
9	4	.buildkite/test-pipeline.yaml
53	10	tests/lora/{test_chatglm3.py => test_chatglm3_tp.py}
0	146	tests/lora/test_llama.py
161	0	tests/lora/test_llama_tp.py
5	0	vllm/lora/fully_sharded_layers.py
28	6	vllm/lora/layers.py
2	2	vllm/model_executor/models/chatglm.py

[17d8fc180] Jee Jee Li 2024-11-24 [bugfix] Fix example/tensorize_vllm_model tests (#10595)
3	1	vllm/model_executor/model_loader/tensorizer.py

[04668ebe7] Isotr0py 2024-11-24 [Bugfix] Avoid import AttentionMetadata explicitly in Mllama (#10593)
5	0	vllm/attention/backends/blocksparse_attn.py
2	1	vllm/attention/layer.py
7	7	vllm/model_executor/models/mllama.py
6	2	vllm/platforms/openvino.py
1	1	vllm/v1/attention/backends/flash_attn.py

[651f6c31a] Nishidha 2024-11-23 For ppc64le, disabled tests for now and addressed space issues (#10538)
3	41	.buildkite/run-cpu-test-ppc64le.sh

[86a44fb89] JiHuazhong 2024-11-23 [Platforms] Refactor openvino code (#10573)
3	78	vllm/executor/openvino_executor.py
69	0	vllm/platforms/openvino.py

[4cfe5d2bc] Isotr0py 2024-11-23 [Bugfix] `multi_modal_kwargs` broadcast for CPU tensor parallel (#10541)
1	0	vllm/worker/cpu_enc_dec_model_runner.py
1	0	vllm/worker/cpu_model_runner.py

[c8acd8054] Cyrus Leung 2024-11-23 [2/N] handling placeholders in merged multi-modal processor (#10485)
370	0	tests/multimodal/test_processing.py
2	1	tests/multimodal/test_utils.py
1	8	vllm/multimodal/inputs.py
583	137	vllm/multimodal/processing.py
19	1	vllm/utils.py

[4634a89d1] Ricky Xu 2024-11-22 Prefix Cache Aware Scheduling [1/n] (#10128)
175	6	tests/core/block/test_prefix_caching_block.py
175	4	tests/core/test_scheduler.py
46	5	tests/core/utils.py
100	6	tests/prefix_caching/test_prefix_caching.py
7	8	vllm/core/block/cpu_gpu_block_allocator.py
21	15	vllm/core/block/interfaces.py
4	7	vllm/core/block/naive_block.py
162	96	vllm/core/block/prefix_caching_block.py
14	9	vllm/core/block_manager.py
4	0	vllm/core/interfaces.py
3	0	vllm/core/placeholder_block_space_manager.py
253	85	vllm/core/scheduler.py
3	0	vllm/sequence.py

[7c25fe45a] kliuae 2024-11-23 [AMD] Add support for GGUF quantization on ROCm (#10254)
0	1	.buildkite/run-amd-test.sh
1	1	CMakeLists.txt
2	0	csrc/ops.h
16	1	csrc/quantization/gguf/ggml-common.h
4	2	csrc/quantization/gguf/gguf_kernel.cu
35	35	csrc/quantization/gguf/mmq.cuh
2	2	csrc/quantization/gguf/mmvq.cuh
143	143	csrc/quantization/gguf/vecdotq.cuh
2	0	csrc/torch_bindings.cpp
28	25	vllm/_custom_ops.py
1	1	vllm/config.py

[02a43f82a] Michael Goin 2024-11-23 Update default max_num_batch_tokens for chunked prefill to 2048 (#10544)
3	3	vllm/config.py

[cfea9c04e] Chen Wu 2024-11-23 [Model] Fix Baichuan BNB online quantization (#10572)
15	0	vllm/model_executor/models/baichuan.py

[7d8ffb344] Varun Vinayak Shenoy 2024-11-22 [Bugfix] Internal Server Error when tool_choice is incorrect. (#10567)
14	0	tests/entrypoints/openai/test_chat.py
6	6	vllm/entrypoints/openai/protocol.py

[4aba6e3d1] youkaichao 2024-11-22 [core] gemma2 full context length support (#10584)
18	7	tests/basic_correctness/test_basic_correctness.py
10	2	vllm/attention/layer.py
20	9	vllm/config.py
7	6	vllm/model_executor/models/gemma2.py

[978b39744] Tyler Michael Smith 2024-11-22 [Misc] Add pynccl wrappers for all_gather and reduce_scatter (#9432)
69	0	tests/distributed/test_pynccl.py
42	0	vllm/distributed/device_communicators/pynccl.py
44	0	vllm/distributed/device_communicators/pynccl_wrapper.py

[ebda51968] Russell Bryant 2024-11-22 [Core] Fix broken log configuration (#10458)
1	1	examples/logging_configuration.md
6	1	vllm/logger.py

[9195dbdbc] Travis Johnson 2024-11-22 [Bugfix][Frontend] Update Llama Chat Templates to also support Non-Tool use (#10164)
36	10	examples/tool_chat_template_llama3.1_json.jinja
72	24	examples/tool_chat_template_llama3.2_json.jinja
2	2	tests/entrypoints/test_chat_utils.py

[d559979c5] youkaichao 2024-11-22 [bugfix] fix cpu tests (#10585)
3	1	vllm/worker/cpu_embedding_model_runner.py
3	1	vllm/worker/cpu_enc_dec_model_runner.py
10	8	vllm/worker/cpu_model_runner.py

[d345f409b] Zhonghua Deng 2024-11-23 [V1] EngineCore supports profiling (#10564)
6	0	vllm/v1/engine/__init__.py
2	2	vllm/v1/engine/async_llm.py
12	2	vllm/v1/engine/core.py
23	5	vllm/v1/engine/core_client.py
25	0	vllm/v1/worker/gpu_worker.py

[28598f393] Russell Bryant 2024-11-22 [Core] remove temporary local variables in LLMEngine.__init__ (#10577)
66	77	vllm/engine/llm_engine.py

[948c85957] zixuanzhang226 2024-11-22 support bitsandbytes quantization with qwen model (#10549)
12	0	vllm/model_executor/models/qwen.py

[97814fbf0] Ricky Xu 2024-11-22 [v1] Refactor KVCacheManager for more hash input than token ids (#10507)
206	19	tests/v1/core/test_prefix_caching.py
137	152	vllm/v1/core/kv_cache_manager.py
22	15	vllm/v1/core/kv_cache_utils.py

[eebad39f2] youkaichao 2024-11-22 [torch.compile] support all attention backends (#10558)
26	11	tests/kernels/test_encoder_decoder_attn.py
14	9	vllm/attention/backends/abstract.py
1	1	vllm/attention/backends/blocksparse_attn.py
172	240	vllm/attention/backends/flash_attn.py
111	169	vllm/attention/backends/flashinfer.py
1	1	vllm/attention/backends/hpu_attn.py
1	1	vllm/attention/backends/ipex_attn.py
1	1	vllm/attention/backends/pallas.py
1	1	vllm/attention/backends/rocm_flash_attn.py
6	6	vllm/attention/backends/torch_sdpa.py
2	2	vllm/attention/backends/utils.py
4	4	vllm/attention/backends/xformers.py
72	9	vllm/attention/layer.py
7	2	vllm/config.py
22	5	vllm/forward_context.py
11	4	vllm/model_executor/models/arctic.py
13	5	vllm/model_executor/models/baichuan.py
35	13	vllm/model_executor/models/bart.py
10	4	vllm/model_executor/models/bloom.py
9	2	vllm/model_executor/models/chameleon.py
19	6	vllm/model_executor/models/chatglm.py
9	5	vllm/model_executor/models/commandr.py
16	5	vllm/model_executor/models/dbrx.py
7	2	vllm/model_executor/models/deepseek.py
2	1	vllm/model_executor/models/deepseek_v2.py
2	1	vllm/model_executor/models/exaone.py
15	7	vllm/model_executor/models/falcon.py
6	4	vllm/model_executor/models/florence2.py
2	1	vllm/model_executor/models/gemma.py
11	4	vllm/model_executor/models/gemma2.py
13	4	vllm/model_executor/models/glm4_vision_encoder.py
2	1	vllm/model_executor/models/gpt2.py
10	3	vllm/model_executor/models/gpt_bigcode.py
10	3	vllm/model_executor/models/gpt_j.py
10	3	vllm/model_executor/models/gpt_neox.py
2	1	vllm/model_executor/models/granite.py
2	1	vllm/model_executor/models/granitemoe.py
15	6	vllm/model_executor/models/internlm2.py
6	17	vllm/model_executor/models/internlm2_ve.py
10	3	vllm/model_executor/models/jais.py
6	2	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/llama.py
8	3	vllm/model_executor/models/minicpm.py
6	3	vllm/model_executor/models/minicpm3.py
2	1	vllm/model_executor/models/mixtral.py
9	3	vllm/model_executor/models/mixtral_quant.py
10	3	vllm/model_executor/models/molmo.py
10	3	vllm/model_executor/models/mpt.py
2	1	vllm/model_executor/models/nemotron.py
11	5	vllm/model_executor/models/olmo.py
10	3	vllm/model_executor/models/olmoe.py
6	5	vllm/model_executor/models/orion.py
10	6	vllm/model_executor/models/persimmon.py
12	5	vllm/model_executor/models/phi.py
15	11	vllm/model_executor/models/phi3_small.py
6	2	vllm/model_executor/models/phimoe.py
8	3	vllm/model_executor/models/qwen.py
7	2	vllm/model_executor/models/qwen2_moe.py
1	0	vllm/model_executor/models/solar.py
11	5	vllm/model_executor/models/stablelm.py
10	5	vllm/model_executor/models/starcoder2.py
7	3	vllm/model_executor/models/xverse.py
1	0	vllm/platforms/cpu.py
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
4	0	vllm/platforms/interface.py
1	0	vllm/platforms/openvino.py
1	0	vllm/platforms/rocm.py
1	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py
2	1	vllm/spec_decode/draft_model_runner.py
2	1	vllm/utils.py
2	1	vllm/v1/attention/backends/flash_attn.py
2	2	vllm/v1/worker/gpu_model_runner.py
1	1	vllm/worker/embedding_model_runner.py
1	1	vllm/worker/enc_dec_model_runner.py
2	2	vllm/worker/model_runner.py

[db100c5cd] youkaichao 2024-11-22 [bugfix] fix full graph tests (#10581)
2	2	tests/compile/utils.py

[11fcf0e06] Noam Gat 2024-11-22 Remove token-adding chat embedding params (#10551)
0	16	vllm/entrypoints/openai/protocol.py
4	2	vllm/entrypoints/openai/serving_embedding.py

[b6374e09b] Isotr0py 2024-11-22 [Bugfix] Fix Phi-3 BNB quantization with tensor parallel (#9948)
14	5	vllm/model_executor/layers/linear.py
42	1	vllm/model_executor/model_loader/loader.py

[a111d0151] youkaichao 2024-11-21 [platforms] absorb worker cls difference into platforms folder (#10555)
117	121	vllm/config.py
10	1	vllm/engine/arg_utils.py
1	6	vllm/executor/cpu_executor.py
6	43	vllm/executor/gpu_executor.py
1	4	vllm/executor/hpu_executor.py
1	1	vllm/executor/multiproc_gpu_executor.py
3	2	vllm/executor/neuron_executor.py
3	5	vllm/executor/openvino_executor.py
2	14	vllm/executor/ray_gpu_executor.py
3	33	vllm/executor/ray_hpu_executor.py
2	17	vllm/executor/ray_tpu_executor.py
1	13	vllm/executor/xpu_executor.py
2	0	vllm/platforms/cpu.py
20	1	vllm/platforms/cuda.py
23	0	vllm/platforms/hpu.py
14	0	vllm/platforms/neuron.py
18	0	vllm/platforms/openvino.py
20	0	vllm/platforms/rocm.py
12	0	vllm/platforms/tpu.py
6	0	vllm/platforms/xpu.py
8	22	vllm/worker/worker_base.py

[446c7806b] Woosuk Kwon 2024-11-21 [Minor] Fix line-too-long (#10563)
4	4	vllm/entrypoints/llm.py

[33e0a2540] youkaichao 2024-11-21 [9/N] torch.compile LLM usage (#10552)
2	3	tests/tpu/test_compilation.py
14	1	vllm/entrypoints/llm.py

[aed074860] Simon Mo 2024-11-21 [Benchmark] Add new H100 machine  (#10547)
21	18	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
10	3	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py

[9afa01455] Michael Goin 2024-11-21 Add small example to metrics.rst (#10550)
26	1	docs/source/serving/metrics.rst

[46fe9b46d] Woosuk Kwon 2024-11-21 [Minor] Revert change in offline inference example (#10545)
20	78	examples/offline_inference.py
80	0	examples/offline_inference_cli.py

[cf656f5a0] youkaichao 2024-11-21 [misc] improve error message (#10553)
8	2	vllm/platforms/cuda.py

[edec3385b] Yunmeng 2024-11-22 [CI][Installation] Avoid uploading CUDA 11.8 wheel (#10535)
6	1	.buildkite/upload-wheels.sh

[f9310cbd0] Woosuk Kwon 2024-11-21 [V1] Fix Compilation config & Enable CUDA graph by default (#10528)
2	1	vllm/config.py
34	28	vllm/v1/worker/gpu_model_runner.py
26	13	vllm/v1/worker/gpu_worker.py

[7560ae5ca] youkaichao 2024-11-21 [8/N] enable cli flag without a space (#10529)
2	2	tests/compile/test_basic_correctness.py
28	0	tests/engine/test_arg_utils.py
5	4	tests/tpu/test_custom_dispatcher.py
4	1	vllm/engine/arg_utils.py
4	0	vllm/utils.py

[e7a8341c7] Cyrus Leung 2024-11-22 [Bugfix] Allow token ID-only inputs in Qwen2-Audio (#10536)
1	1	vllm/model_executor/models/qwen2_audio.py

[c51e397fe] Roger Wang 2024-11-21 [Misc] Suppress duplicated logging regarding multimodal input pipeline (#10530)
2	2	vllm/inputs/preprocess.py
6	0	vllm/utils.py

[2385b60d8] Jee Jee Li 2024-11-22 [Kernel] Register punica ops directly (#10522)
17	6	tests/lora/test_punica_variation.py
20	3	vllm/lora/ops/bgmv_expand.py
22	3	vllm/lora/ops/bgmv_expand_slice.py
20	3	vllm/lora/ops/bgmv_shrink.py
26	3	vllm/lora/ops/sgmv_expand.py
27	3	vllm/lora/ops/sgmv_expand_slice.py
25	3	vllm/lora/ops/sgmv_shrink.py

[da7e702c6] Chauncey 2024-11-22 [Bug]: When apply continue_final_message for OpenAI server, the "echo":false is ignored (#10180)
79	0	tests/entrypoints/openai/test_chat_echo.py
2	2	vllm/entrypoints/openai/serving_chat.py

[4d676f085] Xiaoyu Zhang 2024-11-21 [Bugfix] Embedding model pooling_type equals ALL and multi input's bug (#10494)
17	12	vllm/model_executor/layers/pooler.py

[d5ec121f9] Isotr0py 2024-11-21 [Model] Expose `dynamic_image_size` as mm_processor_kwargs for InternVL2 models (#10518)
206	0	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_internvl.py
49	14	vllm/model_executor/models/internvl.py

[8a93a598d] Wang, Yi 2024-11-21 fix the issue that len(tokenizer(prompt)["input_ids"]) > prompt_len (#10524)
1	0	benchmarks/backend_request_func.py

[1cfde82ff] Alex Brooks 2024-11-21 [Model] Add Support for Multimodal Granite Models (#10291)
35	12	vllm/model_executor/models/clip.py
37	8	vllm/model_executor/models/llava.py
18	2	vllm/model_executor/models/llava_next.py
25	3	vllm/model_executor/models/pixtral.py
32	10	vllm/model_executor/models/siglip.py
44	0	vllm/multimodal/utils.py

[f0e023801] Zhong Qishuai 2024-11-21 [Doc] fix a small typo in docstring of llama_tool_parser (#10513)
2	1	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py

[aaddce5d2] youkaichao 2024-11-20 [platforms] improve error message for unspecified platforms (#10520)
2	1	vllm/config.py
1	0	vllm/platforms/interface.py

[3430857b6] Cyrus Leung 2024-11-21 [Misc] Increase default video fetch timeout (#10495)
1	1	vllm/envs.py

[8b0fe06c8] Luka Govedič 2024-11-21 [torch.compile] Inductor code caching fix (#10273)
10	6	tests/compile/backend.py
95	0	tests/compile/test_functionalization.py
5	6	tests/compile/test_fusion.py
35	0	tests/compile/test_pass_manager.py
19	217	vllm/compilation/backends.py
177	0	vllm/compilation/fix_functionalization.py
7	6	vllm/compilation/fusion.py
73	27	vllm/compilation/inductor_pass.py
77	0	vllm/compilation/pass_manager.py
5	3	vllm/compilation/reshapes.py
53	0	vllm/compilation/vllm_inductor_pass.py
47	13	vllm/config.py
0	9	vllm/utils.py
1	1	vllm/v1/worker/gpu_model_runner.py

[9d827170a] Mengqing Cao 2024-11-21 [Platforms] Add `device_type` in `Platform` (#10508)
2	15	vllm/config.py
1	0	vllm/platforms/cpu.py
1	0	vllm/platforms/cuda.py
1	0	vllm/platforms/hpu.py
1	0	vllm/platforms/interface.py
1	0	vllm/platforms/neuron.py
1	0	vllm/platforms/openvino.py
1	0	vllm/platforms/rocm.py
1	0	vllm/platforms/tpu.py
1	0	vllm/platforms/xpu.py

[6c1208d08] Pavani Majety 2024-11-20 [Core] Add Sliding Window Support with Flashinfer (#10462)
10	2	tests/core/block/e2e/test_correctness_sliding_window.py
8	5	vllm/attention/backends/flashinfer.py

[388ee3de6] youkaichao 2024-11-20 [torch.compile] limit inductor threads and lazy import quant (#10482)
2	0	.buildkite/test-pipeline.yaml
2	2	tests/quantization/utils.py
68	0	tests/test_lazy_torch_compile.py
0	3	vllm/_custom_ops.py
5	3	vllm/config.py
73	51	vllm/model_executor/layers/quantization/__init__.py
2	2	vllm/model_executor/models/internvl.py
4	3	vllm/model_executor/models/qwen2_vl.py
2	0	vllm/platforms/cuda.py
11	0	vllm/platforms/rocm.py
9	0	vllm/plugins/__init__.py

[2f77b6cfe] Woosuk Kwon 2024-11-20 [TPU] Implement prefix caching for TPUs (#10307)
3	3	requirements-tpu.txt
43	23	vllm/attention/backends/pallas.py
134	77	vllm/worker/tpu_model_runner.py
2	2	vllm/worker/tpu_worker.py

[c68f7ede6] Guillaume Calmettes 2024-11-20 [Bugfix]: allow extra fields in requests to openai compatible server (#10463)
13	13	tests/entrypoints/openai/test_chat.py
16	2	vllm/entrypoints/openai/protocol.py

[0cd3d9717] youkaichao 2024-11-20 [7/N] torch.compile, reduce compilation time (#10460)
1	1	tests/compile/piecewise/test_simple.py
2	2	tests/compile/piecewise/test_toy_llama.py
1	1	vllm/compilation/backends.py
10	7	vllm/config.py
13	5	vllm/worker/worker.py

[5f1d6af2b] Simon Mo 2024-11-20 [perf bench] H200 development (#9768)
23	0	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
5	0	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
4	7	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh

[772a66732] youkaichao 2024-11-20 [platforms] restore xpu check for parallel config (#10479)
10	0	vllm/platforms/xpu.py

[63f1fde27] Li, Jiang 2024-11-20 [Hardware][CPU] Support chunked-prefill and prefix-caching on CPU (#10355)
8	1	.buildkite/run-cpu-test.sh
5	5	docs/source/getting_started/cpu-installation.rst
2	2	docs/source/serving/compatibility_matrix.rst
62	1	tests/basic_correctness/test_chunked_prefill.py
152	37	vllm/attention/backends/torch_sdpa.py
109	41	vllm/attention/ops/ipex_attn.py
6	9	vllm/platforms/cpu.py
215	273	vllm/worker/cpu_model_runner.py

[d5b28447e] Mengqing Cao 2024-11-20 [Platforms] Refactor xpu code (#10468)
0	27	vllm/executor/xpu_executor.py
21	0	vllm/platforms/xpu.py

[09dbf9ff1] Cyrus Leung 2024-11-20 [Bugfix] Handle conflicts between modern and legacy fields (#10471)
9	0	vllm/transformers_utils/config.py

[343041c4c] Sky Lee 2024-11-20 [model] Reduce medusa weight (#10454)
18	4	vllm/model_executor/models/medusa.py

[ed701ca96] Kevin H. Luu 2024-11-19 [ci/build] Combine nightly and optional (#10465)
5	4	.buildkite/test-pipeline.yaml

[7629a9c6e] wchen61 2024-11-20 [CI/Build] Support compilation with local cutlass path (#10423) (#10424)
15	2	CMakeLists.txt
12	0	docs/source/getting_started/installation.rst

[709c9f1f2] Rafael Vasquez 2024-11-20 [CI/Build] Add sphinx/rst linter for docs (#10366)
32	0	.github/workflows/sphinx-lint.yml
6	0	format.sh
1	0	requirements-lint.txt
3	0	tools/sphinx-lint.sh

[b4be5a8ad] Cyrus Leung 2024-11-20 [Bugfix] Enforce no chunked prefill for embedding models (#10470)
55	14	docs/source/serving/compatibility_matrix.rst
5	1	vllm/engine/arg_utils.py

[ad44437ba] Isotr0py 2024-11-20 [Bugfix] Fix Mamba model initialization and MLP Speculator weights loading (#10456)
2	6	vllm/model_executor/models/mamba.py
2	1	vllm/model_executor/models/mlp_speculator.py

[9e05252b4] Yanyi Liu 2024-11-20 [Misc] Add __setitem__ for LazyDict (#10469)
3	0	vllm/utils.py

[d200972e7] Lucas Wilkinson 2024-11-19 [Bugfix] Marlin 2:4 temp fix for large M dim (>256) (#10464)
11	4	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
2	0	tests/kernels/test_marlin_gemm.py

[d5b68aba2] Alexei-V-Ivanov-AMD 2024-11-19 [CI/Build] Update Dockerfile.rocm (#10434)
2	2	Dockerfile.rocm

[a324d3a1a] Maximilien de Bayser 2024-11-19 Change granite chat template to keep json list formatting for tool calls (#10452)
1	5	examples/tool_chat_template_granite.jinja

[b00b33d77] ElizaWszola 2024-11-19 [Model][Quantization] HQQ support through Marlin kernel expansion (#9766)
2	1	benchmarks/kernels/benchmark_machete.py
2	2	benchmarks/kernels/benchmark_marlin.py
200	77	csrc/quantization/gptq_marlin/gptq_marlin.cu
1	1	csrc/torch_bindings.cpp
87	1	tests/kernels/test_marlin_gemm.py
2	1	tests/weight_loading/models.txt
5	3	vllm/_custom_ops.py
2	1	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
325	0	vllm/model_executor/layers/quantization/hqq_marlin.py
4	2	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[efa908462] Russell Bryant 2024-11-19 [Core] Avoid metrics log noise when idle (#8868)
23	7	vllm/engine/metrics.py

[803f37eaa] youkaichao 2024-11-19 [6/N] torch.compile rollout to users (#10437)
0	5	tests/compile/piecewise/piecewise_compilation_config.json
7	11	tests/compile/piecewise/test_simple.py
17	28	tests/compile/piecewise/test_toy_llama.py
9	4	tests/compile/test_basic_correctness.py
2	2	tests/compile/utils.py
1	3	tests/model_executor/test_enabled_custom_ops.py
35	12	tests/tpu/test_compilation.py
6	4	tests/tpu/test_custom_dispatcher.py
20	23	vllm/config.py
23	6	vllm/engine/arg_utils.py
3	1	vllm/engine/llm_engine.py
0	8	vllm/envs.py
2	2	vllm/platforms/tpu.py
1	13	vllm/plugins/__init__.py
3	19	vllm/v1/worker/gpu_model_runner.py

[fd9f12497] Russell Bryant 2024-11-19 [Doc] fix link for page that was renamed (#10455)
1	1	vllm/model_executor/model_loader/loader.py

[1ea291a41] Manjul Mohan 2024-11-19 Fix: Build error seen on Power Architecture (#10421)
10	4	cmake/cpu_extension.cmake
10	2	csrc/cpu/attention.cpp
6	0	csrc/cpu/quant.cpp

[11fd7ea63] Patrick von Platen 2024-11-19 [Pixtral-Large] Pixtral actually has no bias in vision-lang adapter (#10449)
3	2	vllm/model_executor/models/pixtral.py

[f028dff33] COSMOPlat 2024-11-19 [BugFix] Fix hermes tool parser output error stream arguments in some cases (#10395) (#10398)
12	13	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py

[b4614656b] Yuan 2024-11-19 [CI][CPU] adding numa node number as container name suffix (#10441)
8	8	.buildkite/run-cpu-test.sh

[25f9c7896] youkaichao 2024-11-19 [misc][plugin] improve plugin loading (#10443)
7	0	vllm/plugins/__init__.py

[5390d6664] Russell Bryant 2024-11-19 [Doc] Add the start of an arch overview page (#10368)
37	0	.github/workflows/png-lint.yml
-	-	docs/source/assets/design/arch_overview/entrypoints.excalidraw.png
-	-	docs/source/assets/design/arch_overview/llm_engine.excalidraw.png
274	0	docs/source/design/arch_overview.rst
0	74	docs/source/design/class_hierarchy.rst
2	2	docs/source/design/plugin_system.rst
1	1	docs/source/index.rst
4	0	format.sh
15	0	tools/png-lint.sh
1	1	vllm/engine/arg_utils.py

[382b6a485] Jee Jee Li 2024-11-19 [Misc] Avoid misleading warning messages (#10438)
2	3	vllm/model_executor/models/chatglm.py
5	5	vllm/model_executor/models/qwen.py

[272e31c0b] Travis Johnson 2024-11-18 [Bugfix] Guard for negative counter metrics to prevent crash (#10430)
1	1	vllm/engine/llm_engine.py
5	0	vllm/engine/metrics.py

[74f8c2cf5] Michael Goin 2024-11-18 Add openai.beta.chat.completions.parse example to structured_outputs.rst (#10433)
96	2	docs/source/models/structured_outputs.rst

[8c1fb5070] Mengqing Cao 2024-11-19 [Platform][Refactor] Extract func `get_default_attn_backend` to `Platform` (#10358)
11	8	tests/kernels/test_attention_selector.py
6	50	vllm/attention/selector.py
1	1	vllm/model_executor/models/molmo.py
1	1	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/model_executor/models/utils.py
1	0	vllm/platforms/__init__.py
9	1	vllm/platforms/cpu.py
5	1	vllm/platforms/hpu.py
19	0	vllm/platforms/interface.py
7	1	vllm/platforms/openvino.py
13	1	vllm/platforms/rocm.py
11	1	vllm/platforms/tpu.py
11	1	vllm/platforms/xpu.py
2	1	vllm/worker/enc_dec_model_runner.py

[7eb719df1] Jee Jee Li 2024-11-19 [Bugfix]Fix Phi-3 BNB online quantization    (#10417)
9	3	vllm/model_executor/layers/linear.py
10	0	vllm/model_executor/models/phi3.py

[284203f17] Kevin H. Luu 2024-11-18 [ci/build] Have dependabot ignore all patch update (#10436)
2	3	.github/dependabot.yml

[90a6c759c] Ricky Xu 2024-11-18 [misc] partial prefix & random input generation benchmark (#9929)
91	25	benchmarks/benchmark_prefix_caching.py

[2298e69b5] youkaichao 2024-11-18 [ci][bugfix] fix kernel tests (#10431)
13	10	vllm/plugins/__init__.py

[a03ea4079] youkaichao 2024-11-18 [3/N][torch.compile] consolidate custom op logging (#10399)
10	2	vllm/config.py
6	3	vllm/model_executor/custom_op.py
4	0	vllm/plugins/__init__.py

[96d999fbe] Lucas Wilkinson 2024-11-18 [Kernel] Initial Machete W4A8 support + Refactors (#9855)
385	134	benchmarks/kernels/benchmark_machete.py
3	2	benchmarks/kernels/graph_machete_bench.py
6	0	benchmarks/kernels/weight_shapes.py
2	2	csrc/cutlass_extensions/cute_utils.cuh
1	0	csrc/{quantization/cutlass_w8a8 => cutlass_extensions/epilogue}/broadcast_load_epilogue_c2x.hpp
0	0	csrc/{quantization/cutlass_w8a8 => cutlass_extensions/epilogue}/broadcast_load_epilogue_c3x.hpp
317	0	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp
315	0	csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp
29	0	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
218	21	csrc/cutlass_extensions/vllm_numeric_conversion.cuh
42	0	csrc/cutlass_extensions/vllm_type_utils.cuh
27	26	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
0	302	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
7	305	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
429	303	csrc/quantization/machete/generate.py
11	14	csrc/quantization/machete/machete_mainloop.cuh
141	65	csrc/quantization/machete/machete_mm_kernel.cuh
35	55	csrc/quantization/machete/machete_mm_launcher.cuh
39	24	csrc/quantization/machete/machete_prepack_kernel.cuh
9	6	csrc/quantization/machete/machete_prepack_launcher.cuh
42	12	csrc/quantization/machete/machete_prepacked_layout.cuh
46	74	csrc/quantization/machete/machete_pytorch.cu
29	6	csrc/torch_bindings.cpp
0	284	tests/kernels/test_machete_gemm.py
406	0	tests/kernels/test_machete_mm.py
44	31	vllm/_custom_ops.py
9	7	vllm/model_executor/layers/quantization/kernels/machete.py
24	21	vllm/model_executor/layers/quantization/utils/quant_utils.py

[c2170a5b3] Angus Wang 2024-11-18 [Kernel] Explicitly specify other value in tl.load calls (#9014)
10	3	vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
3	1	vllm/lora/ops/bgmv_expand.py
7	1	vllm/lora/ops/bgmv_expand_slice.py
4	1	vllm/lora/ops/sgmv_expand.py
4	1	vllm/lora/ops/sgmv_expand_slice.py
7	7	vllm/model_executor/layers/quantization/awq_triton.py

[6b2d25efc] Yan Ma 2024-11-19 [Hardware][XPU] AWQ/GPTQ support for xpu backend (#10107)
4	4	docs/source/quantization/supported_hardware.rst
6	4	tests/quantization/test_ipex_quant.py
1	1	vllm/model_executor/layers/linear.py
0	1	vllm/model_executor/layers/quantization/gptq.py
4	0	vllm/model_executor/layers/quantization/gptq_marlin.py
128	41	vllm/model_executor/layers/quantization/ipex_quant.py
3	1	vllm/model_executor/model_loader/loader.py

[281cc4b3c] Michael Goin 2024-11-18 [Model][Bugfix] Support TP for PixtralHF ViT (#10405)
6	2	vllm/model_executor/models/pixtral.py

[4f686d139] Andrew Nesbitt 2024-11-18 Fix open_collective value in FUNDING.yml (#10426)
1	1	.github/FUNDING.yml

[31894a215] ismael-dm 2024-11-18 [Doc] Add documentation for Structured Outputs (#9943)
1	0	docs/source/index.rst
173	0	docs/source/models/structured_outputs.rst
78	0	examples/offline_inference_structured_outputs.py
94	0	examples/openai_chat_completion_structured_outputs.py

[7851b4519] youkaichao 2024-11-18 [5/N][torch.compile] torch.jit.script --> torch.compile (#10406)
1	1	vllm/model_executor/layers/rejection_sampler.py
2	2	vllm/model_executor/layers/vocab_parallel_embedding.py
2	2	vllm/model_executor/models/phi3_small.py
1	1	vllm/worker/model_runner.py

[4186be811] B-201 2024-11-18 [Doc] Update doc for LoRA support in GLM-4V (#10425)
1	1	docs/source/models/supported_models.rst

[e7ebb662d] Isotr0py 2024-11-18 [Model] Remove transformers attention porting in VITs (#10414)
36	30	vllm/model_executor/models/blip.py
36	29	vllm/model_executor/models/clip.py
22	10	vllm/model_executor/models/intern_vit.py
1	1	vllm/model_executor/models/molmo.py
1	1	vllm/model_executor/models/qwen2_vl.py
35	28	vllm/model_executor/models/siglip.py
8	3	vllm/model_executor/models/utils.py

[5be4e52b6] B-201 2024-11-18 [Model][LoRA]LoRA support added for glm-4v (#10418)
79	19	vllm/model_executor/models/chatglm.py

[01aae1cc6] Maybewuss 2024-11-18 [Model] Remove redundant  softmax when using PoolingType.STEP (#10415)
1	2	vllm/model_executor/layers/pooler.py

[c7dec926f] lkchen 2024-11-18 [VLM] Report multi_modal_placeholders in output (#10407)
78	1	tests/models/decoder_only/vision_language/test_pixtral.py
15	1	vllm/model_executor/models/pixtral.py
22	8	vllm/outputs.py

[51bb12d17] youkaichao 2024-11-17 [4/N][torch.compile] clean up set_torch_compile_backend (#10401)
2	14	vllm/compilation/backends.py
3	8	vllm/compilation/wrapper.py
30	1	vllm/config.py
3	4	vllm/platforms/tpu.py
1	13	vllm/plugins/__init__.py
9	0	vllm/utils.py
1	2	vllm/worker/model_runner.py

[47826cacf] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2024-11-18 [Bugfix] Ignore ray reinit error when current platform is ROCm or XPU (#10375)
1	1	vllm/executor/ray_utils.py

[c4e464333] Isotr0py 2024-11-18 [Misc] Add uninitialized params tracking for `AutoWeightsLoader` (#10327)
11	1	vllm/model_executor/model_loader/loader.py
6	2	vllm/model_executor/models/arctic.py
6	2	vllm/model_executor/models/baichuan.py
6	2	vllm/model_executor/models/bert.py
8	4	vllm/model_executor/models/blip.py
4	3	vllm/model_executor/models/blip2.py
6	2	vllm/model_executor/models/bloom.py
6	2	vllm/model_executor/models/chameleon.py
8	2	vllm/model_executor/models/chatglm.py
8	3	vllm/model_executor/models/clip.py
3	1	vllm/model_executor/models/commandr.py
6	2	vllm/model_executor/models/dbrx.py
6	2	vllm/model_executor/models/decilm.py
6	2	vllm/model_executor/models/deepseek.py
6	2	vllm/model_executor/models/deepseek_v2.py
7	2	vllm/model_executor/models/exaone.py
6	2	vllm/model_executor/models/falcon.py
11	6	vllm/model_executor/models/florence2.py
5	3	vllm/model_executor/models/fuyu.py
3	1	vllm/model_executor/models/gemma.py
6	3	vllm/model_executor/models/gemma2.py
6	2	vllm/model_executor/models/gpt2.py
6	2	vllm/model_executor/models/gpt_bigcode.py
6	2	vllm/model_executor/models/gpt_j.py
6	2	vllm/model_executor/models/gpt_neox.py
7	2	vllm/model_executor/models/granite.py
5	3	vllm/model_executor/models/granitemoe.py
8	3	vllm/model_executor/models/idefics2_vision_model.py
4	3	vllm/model_executor/models/idefics3.py
6	2	vllm/model_executor/models/intern_vit.py
6	2	vllm/model_executor/models/internlm2.py
4	3	vllm/model_executor/models/internvl.py
6	2	vllm/model_executor/models/jais.py
6	2	vllm/model_executor/models/jamba.py
10	5	vllm/model_executor/models/llama.py
4	3	vllm/model_executor/models/llava.py
4	3	vllm/model_executor/models/llava_next.py
4	3	vllm/model_executor/models/llava_next_video.py
4	3	vllm/model_executor/models/llava_onevision.py
6	2	vllm/model_executor/models/mamba.py
7	2	vllm/model_executor/models/medusa.py
6	2	vllm/model_executor/models/minicpm.py
9	5	vllm/model_executor/models/minicpmv.py
6	2	vllm/model_executor/models/mixtral.py
6	2	vllm/model_executor/models/mixtral_quant.py
6	3	vllm/model_executor/models/mllama.py
6	2	vllm/model_executor/models/mlp_speculator.py
6	2	vllm/model_executor/models/mpt.py
6	2	vllm/model_executor/models/nemotron.py
6	2	vllm/model_executor/models/olmo.py
6	2	vllm/model_executor/models/olmoe.py
6	2	vllm/model_executor/models/opt.py
6	2	vllm/model_executor/models/orion.py
4	3	vllm/model_executor/models/paligemma.py
6	2	vllm/model_executor/models/persimmon.py
6	2	vllm/model_executor/models/phi.py
6	2	vllm/model_executor/models/phi3_small.py
6	3	vllm/model_executor/models/phi3v.py
6	2	vllm/model_executor/models/phimoe.py
8	4	vllm/model_executor/models/pixtral.py
6	2	vllm/model_executor/models/qwen.py
12	6	vllm/model_executor/models/qwen2.py
7	2	vllm/model_executor/models/qwen2_audio.py
4	3	vllm/model_executor/models/qwen2_cls.py
6	2	vllm/model_executor/models/qwen2_moe.py
4	3	vllm/model_executor/models/qwen2_rm.py
6	2	vllm/model_executor/models/qwen2_vl.py
8	3	vllm/model_executor/models/siglip.py
7	2	vllm/model_executor/models/solar.py
6	2	vllm/model_executor/models/stablelm.py
6	2	vllm/model_executor/models/starcoder2.py
4	3	vllm/model_executor/models/ultravox.py
6	5	vllm/model_executor/models/utils.py
6	2	vllm/model_executor/models/xverse.py

[d1557e66d] wchen61 2024-11-17 [Misc] Enhance offline_inference to support user-configurable paramet… (#10392)
78	20	examples/offline_inference.py

[80d85c5d7] 电脑星人 2024-11-17 [Bugfix] Fix mrope_position_delta in non-last prefill chunk (#10403)
1	1	vllm/model_executor/layers/rotary_embedding.py

[76aab90ab] Kunshang Ji 2024-11-17 [Hardware] [HPU]add `mark_step` for hpu (#10239)
14	0	vllm/worker/hpu_model_runner.py

[8d74b5aee] youkaichao 2024-11-16 [platforms] refactor cpu code (#10402)
1	67	vllm/executor/cpu_executor.py
60	0	vllm/platforms/cpu.py

[cf349c4a9] Isotr0py 2024-11-17 [Bugfix][CPU] Fix CPU embedding runner with tensor parallel (#10394)
4	0	vllm/worker/cpu_embedding_model_runner.py

[905d0f0af] Chendi.Xue 2024-11-17 [CI/Build] Fix IDC hpu [Device not found] issue (#10384)
1	1	.buildkite/run-hpu-test.sh

[643ecf7b1] Roger Wang 2024-11-16 [V1] Refactor model executable interface for all text-only language models (#10374)
14	2	vllm/model_executor/models/arctic.py
14	2	vllm/model_executor/models/baichuan.py
14	3	vllm/model_executor/models/bloom.py
14	2	vllm/model_executor/models/commandr.py
14	2	vllm/model_executor/models/dbrx.py
14	2	vllm/model_executor/models/deepseek.py
14	2	vllm/model_executor/models/deepseek_v2.py
10	3	vllm/model_executor/models/eagle.py
6	1	vllm/model_executor/models/exaone.py
14	2	vllm/model_executor/models/falcon.py
6	1	vllm/model_executor/models/gemma.py
10	2	vllm/model_executor/models/gemma2.py
5	2	vllm/model_executor/models/gpt2.py
13	4	vllm/model_executor/models/gpt_bigcode.py
14	2	vllm/model_executor/models/gpt_j.py
14	2	vllm/model_executor/models/gpt_neox.py
6	1	vllm/model_executor/models/granite.py
14	2	vllm/model_executor/models/granitemoe.py
7	2	vllm/model_executor/models/internlm2.py
12	2	vllm/model_executor/models/jais.py
14	2	vllm/model_executor/models/jamba.py
13	2	vllm/model_executor/models/mamba.py
6	1	vllm/model_executor/models/minicpm.py
14	2	vllm/model_executor/models/mixtral.py
14	2	vllm/model_executor/models/mixtral_quant.py
14	2	vllm/model_executor/models/mpt.py
6	1	vllm/model_executor/models/nemotron.py
13	6	vllm/model_executor/models/olmo.py
14	2	vllm/model_executor/models/olmoe.py
14	2	vllm/model_executor/models/orion.py
7	1	vllm/model_executor/models/persimmon.py
14	2	vllm/model_executor/models/phi.py
11	8	vllm/model_executor/models/phi3_small.py
14	2	vllm/model_executor/models/phimoe.py
14	2	vllm/model_executor/models/qwen.py
1	1	vllm/model_executor/models/qwen2.py
6	1	vllm/model_executor/models/qwen2_cls.py
14	2	vllm/model_executor/models/qwen2_moe.py
6	1	vllm/model_executor/models/qwen2_rm.py
3	1	vllm/model_executor/models/solar.py
14	2	vllm/model_executor/models/stablelm.py
14	2	vllm/model_executor/models/starcoder2.py
14	2	vllm/model_executor/models/xverse.py

[4fd937502] youkaichao 2024-11-16 [2/N][torch.compile] make compilation cfg part of vllm cfg (#10383)
5	3	tests/compile/piecewise/test_simple.py
12	10	tests/compile/piecewise/test_toy_llama.py
1	1	tests/compile/test_basic_correctness.py
1	1	tests/compile/test_full_graph.py
1	1	tests/compile/test_fusion.py
3	1	tests/compile/test_wrapper.py
1	1	tests/compile/utils.py
26	26	tests/model_executor/test_enabled_custom_ops.py
1	1	tests/tpu/test_compilation.py
1	1	tests/tpu/test_custom_dispatcher.py
13	7	vllm/compilation/backends.py
0	159	vllm/compilation/config.py
5	5	vllm/compilation/decorators.py
1	1	vllm/compilation/fusion.py
1	1	vllm/compilation/inductor_pass.py
0	8	vllm/compilation/levels.py
6	5	vllm/compilation/wrapper.py
189	0	vllm/config.py
0	13	vllm/envs.py
15	12	vllm/model_executor/custom_op.py
5	2	vllm/model_executor/model_loader/loader.py
19	1	vllm/platforms/interface.py
14	7	vllm/platforms/tpu.py
28	2	vllm/plugins/__init__.py
3	7	vllm/v1/worker/gpu_model_runner.py
3	4	vllm/worker/model_runner.py
5	3	vllm/worker/tpu_model_runner.py

[661a34fd4] Woosuk Kwon 2024-11-16 [V1] Add code owners for V1 (#10397)
10	7	.github/CODEOWNERS

[361c29e17] 电脑星人 2024-11-17 [Bugfix] Fix M-RoPE position calculation when chunked prefill is enabled (#10388)
132	4	tests/models/decoder_only/vision_language/test_qwen2_vl.py
2	1	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/worker/model_runner.py

[b98d89efd] Sky Lee 2024-11-17 [Misc] Medusa supports custom bias (#10361)
7	3	vllm/model_executor/models/medusa.py

[8b6725b0c] Jaehyun An 2024-11-16 [Misc] Update benchmark to support image_url file or http (#10287)
13	0	benchmarks/benchmark_serving.py

[1d7547262] rasmith 2024-11-16 [BugFix] [Kernel] Fix GPU SEGV occuring in fused_moe kernel (#10385)
5	3	vllm/model_executor/layers/fused_moe/fused_moe.py

[2f427c2d1] youkaichao 2024-11-16 [misc][plugin] improve log messages (#10386)
3	0	vllm/plugins/__init__.py

[755b85359] youkaichao 2024-11-15 [doc] add doc for the plugin system (#10372)
2	0	docs/source/design/class_hierarchy.rst
62	0	docs/source/design/plugin_system.rst
1	0	docs/source/index.rst
7	18	docs/source/models/adding_model.rst
12	4	vllm/plugins/__init__.py

[32e46e000] Cyrus Leung 2024-11-16 [Frontend] Automatic detection of chat content format from AST (#9919)
13	5	docs/source/serving/openai_compatible_server.md
2	1	tests/entrypoints/openai/test_serving_chat.py
390	229	tests/entrypoints/test_chat_utils.py
0	2	vllm/config.py
0	10	vllm/engine/arg_utils.py
1	3	vllm/engine/llm_engine.py
223	23	vllm/entrypoints/chat_utils.py
33	11	vllm/entrypoints/llm.py
10	3	vllm/entrypoints/openai/api_server.py
15	2	vllm/entrypoints/openai/cli_args.py
45	26	vllm/entrypoints/openai/protocol.py
2	0	vllm/entrypoints/openai/run_batch.py
23	17	vllm/entrypoints/openai/serving_chat.py
8	4	vllm/entrypoints/openai/serving_embedding.py
13	4	vllm/entrypoints/openai/serving_engine.py
10	10	vllm/entrypoints/openai/serving_tokenization.py

[4f168f69a] Michael Green 2024-11-15 [Docs] Misc updates to TPU installation instructions (#10165)
35	19	docs/source/getting_started/tpu-installation.rst

[3e8d14d8a] Russell Bryant 2024-11-15 [Doc] Move PR template content to docs (#10159)
1	70	.github/PULL_REQUEST_TEMPLATE.md
21	4	.github/scripts/cleanup_pr_body.sh
104	10	docs/source/contributing/overview.rst

[a067f85e0] Russell Bryant 2024-11-15 [Frontend] Add --version flag to CLI (#10369)
6	0	vllm/scripts.py

[c76ac49d2] Simon Mo 2024-11-15 [Docs] Add Nebius as sponsors (#10371)
1	0	README.md
1	0	docs/source/community/sponsors.md

[a6221a144] Simon Mo 2024-11-15 [Misc] bump mistral common version (#10367)
1	1	requirements-common.txt

[79ee45b42] ElizaWszola 2024-11-15 [Misc] Bump up test_fused_moe tolerance (#10364)
1	1	tests/kernels/test_moe.py

[691a3ec04] Guillaume Calmettes 2024-11-15 [Bugfix] Ensure special tokens are properly filtered out for guided structured output with MistralTokenizer (#10363)
2	2	requirements-common.txt
15	4	vllm/transformers_utils/tokenizers/mistral.py

[3a763ba0c] youkaichao 2024-11-15 [core][misc] keep compatibility for old-style classes (#10356)
28	12	vllm/model_executor/model_loader/loader.py

[f2056f726] shangmingc 2024-11-15 [Misc] Fix some help info of arg_utils to improve readability (#10362)
12	12	vllm/engine/arg_utils.py

[1d65ec7ee] Jee Jee Li 2024-11-15 [Bugfix] Fix fully sharded LoRA bug (#10352)
12	11	vllm/lora/fully_sharded_layers.py
8	7	vllm/lora/layers.py
1	1	vllm/worker/worker.py

[26908554b] Xin Yang 2024-11-15 [Doc] Remove float32 choice from --lora-dtype (#10348)
1	1	vllm/engine/arg_utils.py

[b311efd0b] Cyrus Leung 2024-11-15 [Misc] Fix import error in tensorizer tests and cleanup some code (#10349)
36	34	tests/tensorizer_loader/test_tensorizer.py
0	3	vllm/engine/llm_engine.py
0	3	vllm/entrypoints/llm.py
8	9	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
3	6	vllm/inputs/preprocess.py
20	0	vllm/utils.py
0	3	vllm/v1/engine/llm_engine.py

[3d158cdc8] wchen61 2024-11-15 Add default value to avoid Falcon crash (#5363) (#10347)
3	0	vllm/model_executor/models/falcon.py

[02dbf30e9] Simon Mo 2024-11-14 [Build] skip renaming files for release wheels pipeline (#9671)
8	13	.buildkite/release-pipeline.yaml
38	0	.buildkite/upload-wheels.sh

[2ac6d0e75] Cyrus Leung 2024-11-15 [Misc] Consolidate pooler config overrides (#10351)
8	2	docs/source/models/supported_models.rst
7	2	tests/engine/test_arg_utils.py
22	28	tests/test_config.py
57	55	vllm/config.py
15	70	vllm/engine/arg_utils.py
4	11	vllm/entrypoints/llm.py
30	24	vllm/model_executor/layers/pooler.py

[2ec882728] Sky Lee 2024-11-15 [Bugfix]  Qwen-vl output is inconsistent in speculative decoding (#10350)
2	0	vllm/spec_decode/batch_expansion.py

[b40cf6402] Cyrus Leung 2024-11-15 [Model] Support Qwen2 embeddings and use tags to select model tests (#10184)
3	3	.buildkite/run-cpu-test-ppc64le.sh
3	3	.buildkite/run-cpu-test.sh
23	25	.buildkite/test-pipeline.yaml
9	4	docs/source/models/supported_models.rst
4	14	tests/models/decoder_only/language/test_jamba.py
4	14	tests/models/decoder_only/language/test_mamba.py
43	28	tests/models/decoder_only/language/test_models.py
11	19	tests/models/embedding/language/test_cls_models.py
22	20	tests/models/embedding/language/test_embedding.py
2	0	tests/models/embedding/vision_language/test_llava_next.py
2	0	tests/models/embedding/vision_language/test_phi3v.py
8	3	tests/models/encoder_decoder/language/test_bart.py
3	0	tests/models/encoder_decoder/vision_language/test_mllama.py
4	0	tests/models/registry.py
2	2	tests/models/test_registry.py
99	13	vllm/model_executor/models/qwen2.py
2	13	vllm/model_executor/models/qwen2_cls.py
2	14	vllm/model_executor/models/qwen2_rm.py
6	3	vllm/model_executor/models/registry.py

[2885ba0e2] Tyler Michael Smith 2024-11-14 [Misc] Change RedundantReshapesPass and FusionPass logging from info to debug (#10308)
2	2	vllm/compilation/fusion.py
1	1	vllm/compilation/reshapes.py

[bf2ddc661] Luka Govedič 2024-11-14 [bugfix] Fix static asymmetric quantization case (#10334)
10	9	tests/kernels/test_int8_quant.py
30	0	tests/quantization/test_compressed_tensors.py
7	1	vllm/_custom_ops.py
7	4	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
4	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[972112d82] Cyrus Leung 2024-11-15 [Bugfix] Fix unable to load some models (#10312)
3	2	.buildkite/test-pipeline.yaml
3	3	tests/distributed/test_pipeline_parallel.py
212	0	tests/models/registry.py
55	0	tests/models/test_initialization.py
10	0	tests/models/test_registry.py
26	10	vllm/config.py
4	4	vllm/engine/arg_utils.py
5	3	vllm/entrypoints/llm.py
5	2	vllm/model_executor/models/fuyu.py
1	7	vllm/model_executor/models/internlm2_ve.py
8	24	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/mlp_speculator.py
6	2	vllm/model_executor/models/registry.py

[11cd1ae6a] Patrick von Platen 2024-11-15 [Tool parsing] Improve / correct mistral tool parsing (#10333)
82	11	tests/models/decoder_only/language/test_mistral.py
5	34	vllm/entrypoints/openai/serving_chat.py
17	8	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
2	2	vllm/transformers_utils/tokenizers/__init__.py
66	4	vllm/transformers_utils/tokenizers/mistral.py

[554af9228] Zijin Xiao 2024-11-15 [Bugfix] use AF_INET6 for OpenAI Compatible Server with ipv6 (#9583)
17	5	vllm/entrypoints/openai/api_server.py

[b2e0ad3b5] Murali Andoorveedu 2024-11-14 [Perf] Reduce peak memory usage of llama (#10339)
2	2	vllm/model_executor/models/llama.py

[4a18fd14b] Maximilien de Bayser 2024-11-14 Support Roberta embedding models (#9387)
3	0	csrc/attention/paged_attention_v1.cu
3	0	csrc/attention/paged_attention_v2.cu
6	0	csrc/cpu/attention.cpp
44	0	tests/model_executor/test_model_load_with_params.py
2	0	tests/models/embedding/language/test_embedding.py
1	1	vllm/attention/ops/ipex_attn.py
1	1	vllm/attention/ops/paged_attn.py
23	12	vllm/model_executor/models/bert.py
2	0	vllm/model_executor/models/registry.py
117	0	vllm/model_executor/models/roberta.py

[1dbae0329] Woosuk Kwon 2024-11-14 [Docs] Publish meetup slides (#10331)
1	9	README.md
1	0	docs/source/community/meetups.rst

[675d60340] Cyrus Leung 2024-11-14 [CI/Build] Make shellcheck happy (#10285)
6	6	.buildkite/run-cpu-test.sh
2	1	tools/shellcheck.sh

[03025c023] Isotr0py 2024-11-14 [CI/Build] Fix CPU CI online inference timeout (#10314)
2	2	.buildkite/run-cpu-test.sh

[29f3ef26a] youkaichao 2024-11-14 [ci][distributed] disable hanging tests (#10317)
1	0	tests/distributed/test_utils.py

[294bf467b] B-201 2024-11-14 [Model] Add BNB quantization support for Idefics3 (#10310)
61	7	vllm/model_executor/models/idefics3.py

[52b48c1ea] Guillaume Calmettes 2024-11-14 [BugFix]: properly deserialize `tool_calls` iterator before processing by mistral-common when MistralTokenizer is used (#9951)
36	0	vllm/entrypoints/openai/serving_chat.py

[f67ce05d0] Mike Depinet 2024-11-13 [Frontend] Pythonic tool parser (#9859)
52	24	docs/source/serving/openai_compatible_server.md
98	0	examples/tool_chat_template_llama3.2_pythonic.jinja
65	0	examples/tool_chat_template_toolace.jinja
0	0	tests/entrypoints/openai/tool_parsers/__init__.py
160	0	tests/entrypoints/openai/tool_parsers/test_pythonic_tool_parser.py
123	0	tests/entrypoints/openai/tool_parsers/utils.py
11	1	tests/tool_use/utils.py
5	0	vllm/entrypoints/openai/serving_chat.py
3	1	vllm/entrypoints/openai/tool_parsers/__init__.py
289	0	vllm/entrypoints/openai/tool_parsers/pythonic_tool_parser.py

[e0853b650] Russell Bryant 2024-11-13 [Misc] format.sh: Simplify tool_version_check (#10305)
9	8	format.sh

[504ac53d1] youkaichao 2024-11-13 [misc] error early for old-style class (#10304)
39	0	docs/source/design/class_hierarchy.rst
15	2	vllm/model_executor/model_loader/loader.py
1	1	vllm/model_executor/models/arctic.py
1	5	vllm/model_executor/models/bloom.py
1	5	vllm/model_executor/models/chatglm.py
1	5	vllm/model_executor/models/dbrx.py
1	1	vllm/model_executor/models/eagle.py
1	5	vllm/model_executor/models/falcon.py
1	1	vllm/model_executor/models/fuyu.py
1	5	vllm/model_executor/models/gpt2.py
1	5	vllm/model_executor/models/gpt_bigcode.py
1	5	vllm/model_executor/models/gpt_j.py
1	5	vllm/model_executor/models/gpt_neox.py
1	1	vllm/model_executor/models/internvl.py
1	5	vllm/model_executor/models/jais.py
1	1	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/llava_next.py
1	1	vllm/model_executor/models/llava_next_video.py
1	1	vllm/model_executor/models/llava_onevision.py
1	1	vllm/model_executor/models/medusa.py
3	7	vllm/model_executor/models/utils.py

[15bb8330a] Isotr0py 2024-11-14 [Bugfix] Fix tensor parallel for qwen2 classification model (#10297)
3	3	tests/models/embedding/language/test_cls_models.py
6	1	vllm/model_executor/models/qwen2_cls.py

[ac49b59d8] HoangCongDuc 2024-11-14 [Bugfix] bitsandbytes models fail to run pipeline parallel (#10200)
29	1	tests/quantization/test_bitsandbytes.py
6	0	vllm/model_executor/model_loader/loader.py

[0b8bb86bf] Cyrus Leung 2024-11-13 [1/N] Initial prototype for multi-modal processor (#10044)
1	1	docs/source/models/enabling_multimodal_inputs.rst
1	1	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py
1	1	tests/multimodal/{test_base.py => test_inputs.py}
23	14	tests/multimodal/test_processor_kwargs.py
2	2	tests/v1/core/test_prefix_caching.py
1	1	vllm/config.py
4	0	vllm/engine/async_llm_engine.py
6	10	vllm/engine/llm_engine.py
6	0	vllm/engine/multiprocessing/client.py
11	5	vllm/engine/protocol.py
0	1	vllm/entrypoints/openai/serving_chat.py
0	1	vllm/entrypoints/openai/serving_completion.py
8	4	vllm/inputs/__init__.py
88	11	vllm/inputs/data.py
122	21	vllm/inputs/preprocess.py
50	6	vllm/inputs/registry.py
2	2	vllm/model_executor/models/chatglm.py
1	2	vllm/model_executor/models/fuyu.py
1	2	vllm/model_executor/models/h2ovl.py
1	2	vllm/model_executor/models/internvl.py
1	1	vllm/model_executor/models/llava.py
1	2	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/phi3v.py
1	2	vllm/model_executor/models/pixtral.py
1	2	vllm/model_executor/models/qwen.py
3	3	vllm/model_executor/models/qwen2_vl.py
1	1	vllm/model_executor/models/utils.py
6	4	vllm/multimodal/__init__.py
9	3	vllm/multimodal/audio.py
18	170	vllm/multimodal/base.py
3	7	vllm/multimodal/image.py
225	0	vllm/multimodal/inputs.py
273	0	vllm/multimodal/processing.py
80	4	vllm/multimodal/registry.py
2	1	vllm/multimodal/utils.py
5	15	vllm/multimodal/video.py
17	51	vllm/sequence.py
4	0	vllm/v1/engine/async_llm.py
3	1	vllm/v1/engine/llm_engine.py
55	18	vllm/v1/engine/processor.py
11	15	vllm/v1/request.py
1	1	vllm/v1/worker/gpu_model_runner.py
27	14	vllm/worker/cpu_model_runner.py
3	3	vllm/worker/hpu_model_runner.py
15	10	vllm/worker/model_runner.py
13	9	vllm/worker/neuron_model_runner.py
13	8	vllm/worker/openvino_model_runner.py
12	4	vllm/worker/xpu_model_runner.py

[bb7991aa2] Roger Wang 2024-11-13 [V1] Add missing tokenizer options for `Detokenizer` (#10288)
9	2	vllm/v1/engine/detokenizer.py
6	1	vllm/v1/engine/llm_engine.py

[d909acf9f] B-201 2024-11-13 [Model][LoRA]LoRA support added for idefics3 (#10281)
1	1	docs/source/models/supported_models.rst
46	9	vllm/model_executor/models/idefics3.py

[b6dde3301] Pavani Majety 2024-11-13 [Core] Flashinfer - Remove advance step size restriction (#10282)
38	28	csrc/prepare_inputs/advance_step.cu

[1b886aa10] Austin Veselka 2024-11-13 [Model] Adding Support for Qwen2VL as an Embedding Model. Using MrLight/dse-qwen2-2b-mrl-v1 (#9944)
6	0	docs/source/models/supported_models.rst
17	0	docs/source/models/vlm.rst
105	18	examples/openai_chat_embedding_client_for_multimodal.py
7	0	examples/template_dse_qwen2_vl.jinja
3	0	tests/conftest.py
209	0	tests/models/embedding/vision_language/test_dse_qwen2_vl.py
16	1	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/registry.py

[3945c8234] 电脑星人 2024-11-13 [Model] Add support for Qwen2-VL video embeddings input & multiple image embeddings input with varied resolutions (#10221)
1	1	docs/source/models/supported_models.rst
428	0	tests/models/decoder_only/vision_language/test_qwen2_vl.py
149	31	vllm/model_executor/models/qwen2_vl.py

[032fcf16a] Xin Yang 2024-11-12 [Doc] Fix typo in arg_utils.py (#10264)
2	2	vllm/engine/arg_utils.py

[56a955e77] Dipika Sikka 2024-11-13 Bump to compressed-tensors v0.8.0 (#10279)
1	1	requirements-common.txt

[bbd3e8692] Woosuk Kwon 2024-11-12 [V1] Support VLMs with fine-grained scheduling (#9871)
9	2	vllm/model_executor/models/gpt2.py
6	1	vllm/model_executor/models/llama.py
28	18	vllm/model_executor/models/llava.py
6	1	vllm/model_executor/models/opt.py
40	23	vllm/model_executor/models/phi3v.py
6	1	vllm/model_executor/models/qwen2.py
48	0	vllm/v1/core/encoder_cache_manager.py
181	24	vllm/v1/core/scheduler.py
10	0	vllm/v1/engine/core.py
39	0	vllm/v1/engine/mm_input_mapper.py
35	6	vllm/v1/request.py
134	20	vllm/v1/worker/gpu_model_runner.py

[0d4ea3fb5] youkaichao 2024-11-12 [core][distributed] use tcp store directly (#10275)
16	10	tests/distributed/test_utils.py
13	15	vllm/distributed/utils.py

[112fa0bbe] Woosuk Kwon 2024-11-12 [V1] Fix CI tests on V1 engine (#10272)
3	0	tests/v1/engine/test_engine_core.py
3	0	tests/v1/engine/test_engine_core_client.py
1	1	vllm/v1/engine/core.py

[377b74fe8] youkaichao 2024-11-12 Revert "[ci][build] limit cmake version" (#10271)
1	1	Dockerfile.neuron
1	1	Dockerfile.ppc64le
1	1	docs/source/getting_started/cpu-installation.rst
1	1	pyproject.toml
1	1	requirements-build.txt
1	1	requirements-tpu.txt
1	1	requirements-xpu.txt

[18081451f] youkaichao 2024-11-12 [doc] improve debugging doc (#10270)
4	0	docs/source/getting_started/debugging.rst

[96ae0eaeb] youkaichao 2024-11-12 [doc] fix location of runllm widget (#10266)
3	1	docs/source/_static/custom.js

[1f55e0571] Woosuk Kwon 2024-11-12 [V1] Enable Inductor when using piecewise CUDA graphs (#10268)
7	4	vllm/v1/worker/gpu_model_runner.py

[8a06428c7] Umesh 2024-11-12 [LoRA] Adds support for bias in LoRA (#5733)
5	0	tests/lora/conftest.py
52	0	tests/lora/test_lora_bias_e2e.py
9	5	tests/lora/test_utils.py
1	0	vllm/config.py
5	0	vllm/engine/arg_utils.py
33	0	vllm/lora/fully_sharded_layers.py
295	1	vllm/lora/layers.py
16	1	vllm/lora/lora.py
30	6	vllm/lora/models.py
10	7	vllm/lora/utils.py

[b41fb9d3b] sroy745 2024-11-12 [Encoder Decoder] Update Mllama to run with both FlashAttention and XFormers (#9982)
8	1	tests/encoder_decoder/test_e2e_correctness.py
63	37	tests/models/encoder_decoder/vision_language/test_mllama.py
2	0	tests/test_config.py
38	14	vllm/model_executor/models/mllama.py
6	28	vllm/worker/enc_dec_model_runner.py

[7c6552791] Woosuk Kwon 2024-11-12 [V1] Use pickle for serializing EngineCoreRequest & Add multimodal inputs to EngineCoreRequest (#10245)
7	2	vllm/v1/engine/__init__.py
2	1	vllm/v1/engine/core.py
2	1	vllm/v1/engine/core_client.py
4	1	vllm/v1/engine/processor.py
10	0	vllm/v1/serial_utils.py

[47db6ec83] zifeitong 2024-11-12 [Frontend] Add per-request number of cached token stats (#10174)
22	2	tests/prefix_caching/test_prefix_caching.py
1	0	vllm/entrypoints/openai/api_server.py
5	0	vllm/entrypoints/openai/cli_args.py
5	0	vllm/entrypoints/openai/protocol.py
6	0	vllm/entrypoints/openai/run_batch.py
22	13	vllm/entrypoints/openai/serving_chat.py
13	6	vllm/outputs.py
12	2	vllm/sequence.py
3	0	vllm/worker/model_runner.py

[176fcb1c7] Jie Fu (傅杰) 2024-11-13 [Bugfix] Fix QwenModel argument (#10262)
2	2	vllm/model_executor/models/qwen.py

[a838ba725] Jee Jee Li 2024-11-12 [Misc]Fix Idefics3Model argument (#10255)
4	4	vllm/model_executor/models/idefics3.py

[36c513a07] Guillaume Calmettes 2024-11-12 [BugFix] Do not raise a `ValueError` when `tool_choice` is set to the supported `none` option and `tools` are not defined. (#10000)
2	2	docs/source/serving/openai_compatible_server.md
8	2	vllm/entrypoints/openai/protocol.py
9	2	vllm/entrypoints/openai/serving_engine.py

[d201d4197] Yuan 2024-11-12 [CI][CPU]refactor CPU tests to allow to bind with different cores (#10222)
11	7	.buildkite/run-cpu-test.sh

[3a28f18b0] youkaichao 2024-11-11 [doc] explain the class hierarchy in vLLM (#10240)
-	-	docs/source/assets/design/hierarchy.png
33	0	docs/source/design/class_hierarchy.rst
2	0	docs/source/design/huggingface_integration.rst
2	1	docs/source/index.rst

[812c981fa] Aleksandr Malyshev 2024-11-11 Splitting attention kernel file (#10091)
3	2	CMakeLists.txt
0	326	csrc/attention/{attention_kernels.cu => attention_kernels.cuh}
193	0	csrc/attention/paged_attention_v1.cu
203	0	csrc/attention/paged_attention_v2.cu

[7f5edb590] Jee Jee Li 2024-11-12 [Misc][LoRA] Replace hardcoded cuda device with configurable argument  (#10223)
36	20	tests/lora/test_layers.py
113	40	tests/lora/test_lora_manager.py
5	4	tests/lora/utils.py
9	10	vllm/lora/models.py
9	6	vllm/lora/punica.py
2	0	vllm/lora/worker_manager.py

[eea55cca5] youkaichao 2024-11-11 [1/N] torch.compile user interface design (#10237)
9	5	tests/compile/piecewise/test_simple.py
14	7	tests/compile/piecewise/test_toy_llama.py
14	13	vllm/compilation/decorators.py
18	12	vllm/config.py

[9cdba9669] Russell Bryant 2024-11-11 [Doc] Update help text for `--distributed-executor-backend` (#10231)
6	3	vllm/config.py
8	3	vllm/engine/arg_utils.py

[d1c6799b8] youkaichao 2024-11-11 [doc] update debugging guide (#10236)
2	0	docs/source/getting_started/debugging.rst

[6ace6fba2] Robert Shaw 2024-11-11 [V1] `AsyncLLM` Implementation (#9826)
8	0	.buildkite/test-pipeline.yaml
56	0	tests/entrypoints/llm/test_accuracy.py
22	3	tests/entrypoints/openai/test_accuracy.py
0	0	{vllm/v1/tokenizer => tests/v1}/__init__.py
0	0	tests/v1/engine/__init__.py
66	0	tests/v1/engine/test_async_llm.py
205	0	tests/v1/engine/test_detokenizer.py
137	0	tests/v1/engine/test_engine_core.py
202	0	tests/v1/engine/test_engine_core_client.py
41	0	vllm/config.py
2	11	vllm/engine/multiprocessing/engine.py
27	16	vllm/engine/output_processor/stop_checker.py
3	0	vllm/entrypoints/llm.py
9	2	vllm/entrypoints/openai/api_server.py
5	0	vllm/envs.py
30	0	vllm/outputs.py
0	0	vllm/v1/__init__.py
7	6	vllm/v1/core/kv_cache_manager.py
19	7	vllm/v1/core/scheduler.py
72	0	vllm/v1/engine/__init__.py
368	0	vllm/v1/engine/async_llm.py
55	0	vllm/v1/engine/async_stream.py
352	0	vllm/v1/engine/core.py
218	0	vllm/v1/engine/core_client.py
265	0	vllm/v1/engine/detokenizer.py
99	453	vllm/v1/engine/llm_engine.py
128	0	vllm/v1/engine/processor.py
16	1	vllm/v1/request.py
0	228	vllm/v1/tokenizer/detokenizer.py

[08f93e743] Nikolai Shcheglov 2024-11-11 Make shutil rename in python_only_dev (#10233)
2	1	python_only_dev.py

[9d5b4e4de] Woosuk Kwon 2024-11-11 [V1] Enable custom ops with piecewise CUDA graphs (#10228)
2	0	vllm/v1/worker/gpu_model_runner.py

[8a7fe47d3] youkaichao 2024-11-11 [misc][distributed] auto port selection and disable tests (#10226)
23	16	tests/distributed/test_utils.py

[4800339c6] Yuan Tang 2024-11-11 Add docs on serving with Llama Stack (#10183)
1	0	docs/source/serving/integrations.rst
42	0	docs/source/serving/serving_with_llamastack.rst

[fe15729a2] Woosuk Kwon 2024-11-11 [V1] Use custom ops for piecewise CUDA graphs (#10227)
0	2	vllm/v1/worker/gpu_model_runner.py

[330e82d34] youkaichao 2024-11-11 [v1][torch.compile] support managing cudagraph buffer (#10203)
2	1	tests/compile/piecewise/piecewise_compilation_config.json
6	6	tests/compile/piecewise/test_simple.py
45	1	vllm/compilation/backends.py
6	0	vllm/compilation/config.py

[d7a4f2207] Woosuk Kwon 2024-11-11 [V1] Do not use inductor for piecewise CUDA graphs (#10225)
3	4	vllm/v1/worker/gpu_model_runner.py

[f9dadfbee] Woosuk Kwon 2024-11-11 [V1] Fix detokenizer ports (#10224)
6	4	vllm/v1/tokenizer/detokenizer.py

[25144ceed] dependabot[bot] 2024-11-11 Bump actions/setup-python from 5.2.0 to 5.3.0 (#10209)
1	1	.github/workflows/cleanup_pr_body.yml

[e6de9784d] youkaichao 2024-11-11 [core][distributed] add stateless process group (#10216)
52	27	tests/distributed/test_utils.py
24	14	vllm/distributed/device_communicators/pynccl.py
141	71	vllm/distributed/utils.py

[36fc439de] Yangcheng Li 2024-11-12 [Doc] fix doc string typo in block_manager `swap_out` function (#10212)
3	3	vllm/core/block_manager.py

[874f551b3] harrywu 2024-11-12 [Metrics] add more metrics (#4464)
343	41	examples/production_monitoring/grafana.json
30	1	vllm/engine/llm_engine.py
58	8	vllm/engine/metrics.py
6	0	vllm/engine/metrics_types.py

[2cebda42b] Isotr0py 2024-11-11 [Bugfix][Hardware][CPU] Fix broken encoder-decoder CPU runner (#10218)
2	0	.buildkite/run-cpu-test-ppc64le.sh
2	0	.buildkite/run-cpu-test.sh
1	0	vllm/worker/cpu_embedding_model_runner.py
11	0	vllm/worker/cpu_enc_dec_model_runner.py

[5fb1f935b] Roger Wang 2024-11-11 [V1] Allow `tokenizer_mode` and `trust_remote_code` for Detokenizer (#10211)
4	1	vllm/v1/engine/llm_engine.py
15	4	vllm/v1/tokenizer/detokenizer.py

[36e4acd02] Jee Jee Li 2024-11-11 [LoRA][Kernel] Remove the unused libentry module (#10214)
24	49	tests/lora/test_punica_sizes.py
24	49	tests/lora/test_punica_variation.py
0	3	vllm/lora/ops/sgmv_expand.py
0	3	vllm/lora/ops/sgmv_expand_slice.py
0	3	vllm/lora/ops/sgmv_shrink.py
1	2	vllm/triton_utils/__init__.py
0	167	vllm/triton_utils/libentry.py

[58170d650] Isotr0py 2024-11-11 [Hardware][CPU] Add embedding models support for CPU backend (#10193)
1	2	.buildkite/run-cpu-test-ppc64le.sh
1	2	.buildkite/run-cpu-test.sh
4	3	tests/models/embedding/language/test_embedding.py
10	4	vllm/attention/backends/torch_sdpa.py
0	6	vllm/model_executor/models/bert.py
122	0	vllm/worker/cpu_embedding_model_runner.py
5	6	vllm/worker/cpu_enc_dec_model_runner.py
35	22	vllm/worker/cpu_model_runner.py
7	7	vllm/worker/cpu_worker.py

[9804ac7c7] dependabot[bot] 2024-11-11 Bump the patch-update group with 5 updates (#10210)
5	10	requirements-test.txt

[f89d18ff7] youkaichao 2024-11-10 [6/N] pass whole config to inner model (#10205)
10	13	vllm/model_executor/models/arctic.py
26	22	vllm/model_executor/models/baichuan.py
11	12	vllm/model_executor/models/bart.py
11	14	vllm/model_executor/models/bert.py
3	7	vllm/model_executor/models/blip2.py
11	10	vllm/model_executor/models/bloom.py
11	16	vllm/model_executor/models/chameleon.py
10	9	vllm/model_executor/models/chatglm.py
13	20	vllm/model_executor/models/commandr.py
11	10	vllm/model_executor/models/dbrx.py
1	5	vllm/model_executor/models/decilm.py
11	15	vllm/model_executor/models/deepseek.py
11	18	vllm/model_executor/models/deepseek_v2.py
4	1	vllm/model_executor/models/eagle.py
13	21	vllm/model_executor/models/exaone.py
11	10	vllm/model_executor/models/falcon.py
17	21	vllm/model_executor/models/florence2.py
8	16	vllm/model_executor/models/gemma.py
9	19	vllm/model_executor/models/gemma2.py
11	13	vllm/model_executor/models/gpt2.py
10	12	vllm/model_executor/models/gpt_bigcode.py
11	10	vllm/model_executor/models/gpt_j.py
10	10	vllm/model_executor/models/gpt_neox.py
13	21	vllm/model_executor/models/granite.py
12	21	vllm/model_executor/models/granitemoe.py
12	17	vllm/model_executor/models/idefics3.py
8	16	vllm/model_executor/models/internlm2.py
9	14	vllm/model_executor/models/internlm2_ve.py
3	3	vllm/model_executor/models/internvl.py
11	10	vllm/model_executor/models/jais.py
12	18	vllm/model_executor/models/jamba.py
12	32	vllm/model_executor/models/llama.py
3	3	vllm/model_executor/models/llava.py
3	3	vllm/model_executor/models/llava_next.py
3	3	vllm/model_executor/models/llava_next_video.py
3	3	vllm/model_executor/models/llava_onevision.py
13	18	vllm/model_executor/models/mamba.py
17	21	vllm/model_executor/models/minicpm.py
5	7	vllm/model_executor/models/minicpm3.py
19	37	vllm/model_executor/models/minicpmv.py
13	21	vllm/model_executor/models/mixtral.py
11	15	vllm/model_executor/models/mixtral_quant.py
19	30	vllm/model_executor/models/mllama.py
11	15	vllm/model_executor/models/molmo.py
11	15	vllm/model_executor/models/mpt.py
13	21	vllm/model_executor/models/nemotron.py
11	13	vllm/model_executor/models/olmo.py
11	15	vllm/model_executor/models/olmoe.py
8	16	vllm/model_executor/models/opt.py
11	15	vllm/model_executor/models/orion.py
5	8	vllm/model_executor/models/paligemma.py
11	16	vllm/model_executor/models/persimmon.py
11	13	vllm/model_executor/models/phi.py
11	15	vllm/model_executor/models/phi3_small.py
6	8	vllm/model_executor/models/phi3v.py
13	21	vllm/model_executor/models/phimoe.py
3	7	vllm/model_executor/models/pixtral.py
12	15	vllm/model_executor/models/qwen.py
8	15	vllm/model_executor/models/qwen2.py
4	8	vllm/model_executor/models/qwen2_audio.py
4	7	vllm/model_executor/models/qwen2_cls.py
11	15	vllm/model_executor/models/qwen2_moe.py
4	7	vllm/model_executor/models/qwen2_rm.py
7	12	vllm/model_executor/models/qwen2_vl.py
13	21	vllm/model_executor/models/solar.py
11	13	vllm/model_executor/models/stablelm.py
11	15	vllm/model_executor/models/starcoder2.py
9	7	vllm/model_executor/models/ultravox.py
6	13	vllm/model_executor/models/xverse.py

[f0f2e5638] youkaichao 2024-11-10 [doc] improve debugging code (#10206)
3	0	docs/source/getting_started/debugging.rst

[ad9a78bf6] yansh97 2024-11-11 [Doc] Fix typo error in vllm/entrypoints/openai/cli_args.py (#10196)
1	1	vllm/entrypoints/openai/cli_args.py

[73b9083e9] youkaichao 2024-11-10 [misc] improve cloudpickle registration and tests (#10202)
20	6	tests/distributed/test_pipeline_parallel.py
0	4	vllm/engine/arg_utils.py
30	21	vllm/transformers_utils/config.py

[20cf2f553] Shawn Du 2024-11-11 [Misc] small fixes to function tracing file path (#9543)
2	2	docs/source/index.rst
3	2	vllm/logger.py
3	0	vllm/utils.py

[bfb7d61a7] Yongzao 2024-11-11 [doc] Polish the integration with huggingface doc (#10195)
10	12	docs/source/design/huggingface_integration.rst

[19682023b] FuryMartin 2024-11-10 [Doc] Fix typo error in CONTRIBUTING.md (#10190)
1	1	CONTRIBUTING.md

[9fa4bdde9] youkaichao 2024-11-09 [ci][build] limit cmake version (#10188)
1	1	Dockerfile.neuron
1	1	Dockerfile.ppc64le
1	1	docs/source/getting_started/cpu-installation.rst
1	1	pyproject.toml
1	1	requirements-build.txt
1	1	requirements-tpu.txt
1	1	requirements-xpu.txt

[51c2e1fce] Cyrus Leung 2024-11-10 [CI/Build] Split up models tests (#10069)
14	10	.buildkite/test-pipeline.yaml
1	0	pyproject.toml
1	0	tests/models/decoder_only/language/test_aqlm.py
1	0	tests/models/decoder_only/language/test_fp8.py
16	19	tests/models/decoder_only/language/test_gguf.py
1	0	tests/models/decoder_only/language/test_gptq_marlin.py
1	0	tests/models/decoder_only/language/test_gptq_marlin_24.py
2	1	tests/models/decoder_only/language/test_granite.py
0	39	tests/models/decoder_only/language/test_granitemoe.py
1	0	tests/models/decoder_only/language/test_modelopt.py
1	3	tests/models/decoder_only/language/test_models.py
3	1	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_llava_next.py
2	1	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py
11	4	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py
11	8	tests/models/decoder_only/vision_language/{test_internvl.py => test_awq.py}
9	10	tests/models/decoder_only/vision_language/test_intern_vit.py
23	23	tests/models/decoder_only/vision_language/test_models.py
8	1	vllm/config.py
2	4	vllm/model_executor/models/fuyu.py
3	1	vllm/model_executor/models/internlm2_ve.py
4	4	vllm/model_executor/models/utils.py

[b09895a61] Krishna Mandal 2024-11-09 [Frontend][Core] Override HF `config.json` via CLI (#5836)
7	3	tests/test_config.py
22	8	vllm/config.py
11	3	vllm/engine/arg_utils.py
1	4	vllm/engine/llm_engine.py
6	1	vllm/entrypoints/llm.py
25	30	vllm/transformers_utils/config.py
1	4	vllm/v1/engine/llm_engine.py

[d88bff1b9] cjackal 2024-11-09 [Frontend] add `add_request_id` middleware (#9594)
26	0	docs/source/serving/openai_compatible_server.md
8	0	vllm/entrypoints/openai/api_server.py

[9e3726642] Zhao Yingzhuo 2024-11-09 bugfix: fix the bug that stream generate not work (#2756)
1	1	vllm/entrypoints/api_server.py

[8a4358ecb] youkaichao 2024-11-09 [doc] explaining the integration with huggingface (#10173)
40	0	docs/source/design/huggingface_integration.rst
1	0	docs/source/index.rst

[bd46357ad] youkaichao 2024-11-09 [bugfix] fix broken tests of mlp speculator (#10177)
3	2	vllm/model_executor/models/mlp_speculator.py

[f192aeba7] bnellnm 2024-11-09 [Bugfix] Enable some fp8 and quantized fullgraph tests (#10171)
13	16	tests/compile/utils.py

[8e1529dc5] Chendi.Xue 2024-11-09 [CI/Build] Add run-hpu-test.sh script (#10167)
16	0	.buildkite/run-hpu-test.sh
2	0	Dockerfile.hpu

[1a95f10ee] youkaichao 2024-11-08 [5/N] pass the whole config to model (#9983)
7	93	vllm/model_executor/model_loader/loader.py
3	12	vllm/model_executor/model_loader/tensorizer.py
9	7	vllm/model_executor/models/arctic.py
16	21	vllm/model_executor/models/baichuan.py
6	6	vllm/model_executor/models/bart.py
7	5	vllm/model_executor/models/bert.py
11	9	vllm/model_executor/models/blip2.py
6	4	vllm/model_executor/models/bloom.py
7	5	vllm/model_executor/models/chameleon.py
8	7	vllm/model_executor/models/chatglm.py
7	5	vllm/model_executor/models/commandr.py
6	4	vllm/model_executor/models/dbrx.py
6	12	vllm/model_executor/models/decilm.py
6	4	vllm/model_executor/models/deepseek.py
6	4	vllm/model_executor/models/deepseek_v2.py
4	3	vllm/model_executor/models/eagle.py
7	5	vllm/model_executor/models/exaone.py
6	4	vllm/model_executor/models/falcon.py
5	5	vllm/model_executor/models/florence2.py
7	8	vllm/model_executor/models/fuyu.py
6	5	vllm/model_executor/models/gemma.py
15	12	vllm/model_executor/models/gemma2.py
6	4	vllm/model_executor/models/gpt2.py
7	5	vllm/model_executor/models/gpt_bigcode.py
6	4	vllm/model_executor/models/gpt_j.py
6	4	vllm/model_executor/models/gpt_neox.py
7	5	vllm/model_executor/models/granite.py
7	5	vllm/model_executor/models/granitemoe.py
8	5	vllm/model_executor/models/idefics3.py
4	20	vllm/model_executor/models/interfaces_base.py
5	4	vllm/model_executor/models/internlm2.py
5	4	vllm/model_executor/models/internlm2_ve.py
7	8	vllm/model_executor/models/internvl.py
6	4	vllm/model_executor/models/jais.py
8	6	vllm/model_executor/models/jamba.py
20	10	vllm/model_executor/models/llama.py
7	8	vllm/model_executor/models/llava.py
7	10	vllm/model_executor/models/llava_next.py
6	9	vllm/model_executor/models/llava_next_video.py
6	9	vllm/model_executor/models/llava_onevision.py
8	6	vllm/model_executor/models/mamba.py
3	2	vllm/model_executor/models/medusa.py
7	5	vllm/model_executor/models/minicpm.py
19	29	vllm/model_executor/models/minicpmv.py
7	6	vllm/model_executor/models/mixtral.py
6	4	vllm/model_executor/models/mixtral_quant.py
9	6	vllm/model_executor/models/mllama.py
8	8	vllm/model_executor/models/molmo.py
7	5	vllm/model_executor/models/mpt.py
7	6	vllm/model_executor/models/nemotron.py
9	5	vllm/model_executor/models/olmo.py
6	4	vllm/model_executor/models/olmoe.py
7	5	vllm/model_executor/models/opt.py
6	4	vllm/model_executor/models/orion.py
16	14	vllm/model_executor/models/paligemma.py
9	5	vllm/model_executor/models/persimmon.py
8	7	vllm/model_executor/models/phi.py
7	6	vllm/model_executor/models/phi3_small.py
12	11	vllm/model_executor/models/phi3v.py
7	6	vllm/model_executor/models/phimoe.py
10	10	vllm/model_executor/models/pixtral.py
14	17	vllm/model_executor/models/qwen.py
7	7	vllm/model_executor/models/qwen2.py
11	10	vllm/model_executor/models/qwen2_audio.py
9	11	vllm/model_executor/models/qwen2_cls.py
6	4	vllm/model_executor/models/qwen2_moe.py
9	10	vllm/model_executor/models/qwen2_rm.py
10	9	vllm/model_executor/models/qwen2_vl.py
7	6	vllm/model_executor/models/solar.py
6	4	vllm/model_executor/models/stablelm.py
9	5	vllm/model_executor/models/starcoder2.py
9	11	vllm/model_executor/models/ultravox.py
6	21	vllm/model_executor/models/utils.py
13	9	vllm/model_executor/models/xverse.py
0	12	vllm/plugins/__init__.py

[49d2a41a8] Cyrus Leung 2024-11-09 [Doc] Adjust RunLLM location (#10176)
1	1	docs/source/_static/custom.js

[47672f38b] Isotr0py 2024-11-09 [CI/Build] Fix VLM broadcast tests `tensor_parallel_size` passing (#10161)
1	0	tests/models/decoder_only/vision_language/test_models.py
1	0	tests/models/decoder_only/vision_language/vlm_utils/types.py

[f83feccd7] Michael Goin 2024-11-08 [Bugfix] Ignore GPTQ quantization of Qwen2-VL visual module (#10169)
12	2	vllm/model_executor/models/qwen2_vl.py

[e0191a95d] Cyrus Leung 2024-11-09 [0/N] Rename `MultiModalInputs` to `MultiModalKwargs` (#10040)
1	1	docs/source/design/multimodal/multimodal_index.rst
2	2	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py
11	11	tests/multimodal/test_base.py
2	2	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/fuyu.py
5	5	vllm/model_executor/models/h2ovl.py
2	2	vllm/model_executor/models/idefics3.py
3	3	vllm/model_executor/models/internvl.py
2	2	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/mllama.py
2	2	vllm/model_executor/models/molmo.py
5	5	vllm/model_executor/models/pixtral.py
6	6	vllm/model_executor/models/qwen.py
4	4	vllm/model_executor/models/qwen2_audio.py
4	4	vllm/model_executor/models/qwen2_vl.py
4	4	vllm/model_executor/models/ultravox.py
17	2	vllm/multimodal/__init__.py
2	2	vllm/multimodal/audio.py
24	9	vllm/multimodal/base.py
5	5	vllm/multimodal/image.py
3	3	vllm/multimodal/registry.py
3	3	vllm/multimodal/video.py
2	2	vllm/spec_decode/draft_model_runner.py
2	2	vllm/worker/cpu_enc_dec_model_runner.py
5	5	vllm/worker/cpu_model_runner.py
2	2	vllm/worker/embedding_model_runner.py
2	2	vllm/worker/enc_dec_model_runner.py
4	4	vllm/worker/hpu_model_runner.py
9	9	vllm/worker/model_runner.py
5	5	vllm/worker/neuron_model_runner.py
5	5	vllm/worker/openvino_model_runner.py
5	5	vllm/worker/xpu_model_runner.py

[d7edca1de] Li, Jiang 2024-11-09 [CI/Build] Adding timeout in CPU CI to avoid CPU test queue blocking (#6892)
31	26	.buildkite/run-cpu-test-ppc64le.sh
48	43	.buildkite/run-cpu-test.sh

[127c07480] rasmith 2024-11-08 [Kernel][Triton] Add Triton implementation for scaled_mm_triton to support fp8 and int8 SmoothQuant, symmetric case (#9857)
106	0	tests/kernels/test_triton_scaled_mm.py
9	0	vllm/_custom_ops.py
184	0	vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py

[10b67d865] bnellnm 2024-11-08 [Bugfix] SymIntArrayRef expected to contain concrete integers (#10170)
2	1	vllm/compilation/backends.py

[4f93dfe95] Luka Govedič 2024-11-08 [torch.compile] Fuse RMSNorm with quant (#9138)
1	0	CMakeLists.txt
4	161	csrc/layernorm_kernels.cu
234	0	csrc/layernorm_quant_kernels.cu
10	0	csrc/ops.h
3	172	csrc/quantization/fp8/common.cu
172	0	csrc/quantization/fp8/common.cuh
25	6	csrc/torch_bindings.cpp
165	0	csrc/type_convert.cuh
33	0	tests/compile/backend.py
92	0	tests/compile/test_fusion.py
74	1	tests/kernels/test_layernorm.py
94	15	vllm/compilation/backends.py
12	13	vllm/compilation/config.py
291	0	vllm/compilation/fusion.py
38	0	vllm/compilation/inductor_pass.py
85	0	vllm/compilation/reshapes.py
2	0	vllm/envs.py

[e1b5a8217] Florian Zimmermeister 2024-11-08 Rename vllm.logging to vllm.logging_utils (#10134)
1	1	pyproject.toml
1	1	tests/test_logger.py
1	1	vllm/logger.py
0	5	vllm/logging/__init__.py
5	0	vllm/logging_utils/__init__.py
0	0	vllm/{logging => logging_utils}/formatter.py

[87713c605] Luka Govedič 2024-11-08 [CI/Build] Ignore .gitignored files for shellcheck (#10162)
1	1	tools/shellcheck.sh

[b5815c841] Woosuk Kwon 2024-11-08 [V1] Fix non-cudagraph op name (#10166)
1	1	vllm/v1/worker/gpu_model_runner.py

[6b3047158] Rafael Vasquez 2024-11-08 [Misc] Improve Web UI (#10090)
1	1	docs/source/_static/custom.js

[f6778620a] sroy745 2024-11-08 Disable spec-decode + chunked-prefill for draft models with tensor parallelism > 1 (#10136)
46	0	tests/spec_decode/e2e/test_compatibility.py
37	8	vllm/config.py

[0535e5fe6] Patrick von Platen 2024-11-08 Fix edge case Mistral tokenizer (#10152)
9	9	vllm/transformers_utils/tokenizers/mistral.py

[b489fc3c9] Cyrus Leung 2024-11-08 [CI/Build] Update CPU tests to include all "standard" tests (#5481)
13	8	.buildkite/run-cpu-test-ppc64le.sh
17	8	.buildkite/run-cpu-test.sh
1	2	.buildkite/test-pipeline.yaml
2	1	pyproject.toml
0	5	requirements-test.in
13	4	tests/models/decoder_only/audio_language/test_ultravox.py
0	1	tests/models/decoder_only/vision_language/test_h2ovl.py
4	7	tests/models/decoder_only/vision_language/test_models.py
0	2	tests/models/decoder_only/vision_language/test_phi3v.py
1	2	tests/models/utils.py
1	1	vllm/assets/image.py
2	2	vllm/model_executor/models/ultravox.py
4	4	vllm/multimodal/utils.py
5	1	vllm/worker/cpu_worker.py

[208ce622c] Roger Wang 2024-11-08 [V1]Enable APC by default only for text models (#10148)
4	1	vllm/v1/engine/llm_engine.py

[1ff4aed5b] Isotr0py 2024-11-08 [Model] Expose size to Idefics3 as mm_processor_kwargs (#10146)
14	5	examples/offline_inference_vision_language.py
7	0	examples/offline_inference_vision_language_multi_image.py
187	0	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_idefics3.py
62	18	vllm/model_executor/models/idefics3.py

[f10797c0c] Yan Ma 2024-11-08 [Bugfix][XPU] Fix xpu tp by introducing XpuCommunicator (#10144)
47	0	vllm/distributed/device_communicators/xpu_communicator.py
18	22	vllm/distributed/parallel_state.py

[f4c2187e2] Cyrus Leung 2024-11-08 [Misc] Fix typo in #5895 (#10145)
2	2	benchmarks/backend_request_func.py

[aea6ad629] Michael Goin 2024-11-08 Add hf_transfer to testing image (#10096)
5	0	Dockerfile

[da07a9ead] Tao He 2024-11-08 Fixes a typo about 'max_decode_seq_len' which causes crashes with cuda graph. (#9285)
[3a7f15a39] Russell Bryant 2024-11-08 [Doc] Move CONTRIBUTING to docs site (#9924)
1	58	CONTRIBUTING.md
0	0	docs/source/{dev => contributing}/dockerfile/dockerfile.rst
70	0	docs/source/contributing/overview.rst
0	0	docs/source/{dev => contributing}/profiling/profiling_index.rst
0	0	docs/source/{dev => design}/input_processing/input_processing_pipeline.rst
0	0	docs/source/{dev => design}/input_processing/model_inputs_index.rst
0	0	docs/source/{dev => design}/kernel/paged_attention.rst
0	0	docs/source/{dev => design}/multimodal/adding_multimodal_plugin.rst
0	0	docs/source/{dev => design}/multimodal/multimodal_index.rst
29	10	docs/source/index.rst

[7371749d5] Mengqing Cao 2024-11-08 [Misc] Fix ImportError causing by triton (#9493)
5	2	vllm/executor/multiproc_gpu_executor.py

[ad39bd640] DearPlanet 2024-11-08 [Bugfix] Add error handling when server cannot respond any valid tokens (#5895)
10	3	benchmarks/backend_request_func.py

[40d0e7411] whyiug 2024-11-08 [Doc] Update FAQ links in spec_decode.rst (#9662)
2	2	docs/source/models/spec_decode.rst

[6bb52b0f9] Russell Bryant 2024-11-07 [CI/Build] Give PR cleanup job PR write access (#10139)
5	2	.github/workflows/cleanup_pr_body.yml

[201fc0773] Cody Yu 2024-11-07 [V1] Prefix caching (take 2) (#9972)
1	8	benchmarks/benchmark_prefix_caching.py
219	0	tests/v1/core/test_prefix_caching.py
335	47	vllm/v1/core/kv_cache_manager.py
194	0	vllm/v1/core/kv_cache_utils.py
21	11	vllm/v1/core/scheduler.py
1	0	vllm/v1/engine/llm_engine.py

[42b4f46b7] Woosuk Kwon 2024-11-07 [V1] Add all_token_ids attribute to Request (#10135)
1	1	vllm/v1/core/scheduler.py
1	1	vllm/v1/engine/llm_engine.py
26	3	vllm/v1/request.py
64	0	vllm/v1/utils.py

[073a47272] Jiangtao Hu 2024-11-07 [Misc] report relevant env vars in collect_env.py tool (#9293)
25	3	collect_env.py

[93bff421b] dependabot[bot] 2024-11-07 Bump actions/checkout from 4.2.1 to 4.2.2 (#9746)
1	1	.github/workflows/actionlint.yml
1	1	.github/workflows/clang-format.yml
1	1	.github/workflows/cleanup_pr_body.yml
1	1	.github/workflows/codespell.yml
1	1	.github/workflows/mypy.yaml
2	2	.github/workflows/publish.yml
1	1	.github/workflows/ruff.yml
1	1	.github/workflows/shellcheck.yml
1	1	.github/workflows/yapf.yml

[28b2877d3] litianjian 2024-11-08 Online video support for VLMs (#10020)
1	0	docs/source/conf.py
4	2	requirements-test.in
50	7	requirements-test.txt
2	1	setup.py
345	0	tests/entrypoints/openai/test_video.py
2	2	vllm/assets/video.py
63	6	vllm/entrypoints/chat_utils.py
9	3	vllm/envs.py
3	2	vllm/model_executor/models/llava_onevision.py
3	0	vllm/multimodal/base.py
114	7	vllm/multimodal/utils.py
2	1	vllm/multimodal/video.py

[97b8475be] dependabot[bot] 2024-11-07 Bump actions/setup-python from 5.2.0 to 5.3.0 (#9745)
1	1	.github/workflows/clang-format.yml
1	1	.github/workflows/codespell.yml
1	1	.github/workflows/mypy.yaml
1	1	.github/workflows/publish.yml
1	1	.github/workflows/ruff.yml
1	1	.github/workflows/yapf.yml

[a2f1f3b08] Russell Bryant 2024-11-07 [CI/Build] Automate PR body text cleanup (#10082)
33	0	.github/scripts/cleanup_pr_body.sh
23	0	.github/workflows/cleanup_pr_body.yml

[3be5b26a7] Russell Bryant 2024-11-07 [CI/Build] Add shell script linting using shellcheck (#7925)
3	3	.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh
3	3	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
1	1	.buildkite/lm-eval-harness/run-tests.sh
25	38	.buildkite/nightly-benchmarks/scripts/launch-server.sh
6	6	.buildkite/nightly-benchmarks/scripts/nightly-annotate.sh
14	16	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
9	10	.buildkite/nightly-benchmarks/scripts/run-performance-benchmarks.sh
2	2	.buildkite/nightly-benchmarks/scripts/wait-for-image.sh
18	16	.buildkite/run-amd-test.sh
2	0	.buildkite/run-benchmarks.sh
3	1	.buildkite/run-cpu-test-ppc64le.sh
2	0	.buildkite/run-cpu-test.sh
15	12	.buildkite/run-multi-node-test.sh
5	3	.buildkite/run-neuron-test.sh
2	0	.buildkite/run-openvino-test.sh
3	1	.buildkite/run-tpu-test.sh
2	0	.buildkite/run-xpu-test.sh
4	4	.github/workflows/scripts/cuda-install.sh
1	1	.github/workflows/scripts/pytorch-install.sh
37	0	.github/workflows/shellcheck.yml
1	0	.gitignore
9	0	.shellcheckrc
4	4	benchmarks/launch_tgi_server.sh
2	2	examples/run_cluster.sh
7	3	format.sh
1	1	tests/weight_loading/run_model_weight_loading_test.sh
2	2	tools/mypy.sh
21	0	tools/shellcheck.sh

[de0e61a32] Russell Bryant 2024-11-07 [CI/Build] Always run mypy (#10122)
10	5	.github/workflows/mypy.yaml

[9d43afcc5] Nicolò Lucchesi 2024-11-07 [Feature] [Spec decode]: Combine chunked prefill with speculative decoding (#9291)
0	34	tests/spec_decode/e2e/test_compatibility.py
102	3	tests/spec_decode/e2e/test_multistep_correctness.py
35	1	tests/spec_decode/e2e/test_ngram_correctness.py
7	2	tests/spec_decode/test_ngram_worker.py
27	4	tests/spec_decode/test_scorer.py
82	0	tests/spec_decode/test_spec_decode_worker.py
60	11	tests/spec_decode/utils.py
7	3	vllm/attention/backends/flash_attn.py
6	0	vllm/attention/backends/rocm_flash_attn.py
7	0	vllm/attention/backends/xformers.py
6	8	vllm/config.py
1	0	vllm/core/scheduler.py
5	3	vllm/engine/output_processor/multi_step.py
31	30	vllm/spec_decode/batch_expansion.py
19	12	vllm/spec_decode/mqa_scorer.py
78	32	vllm/spec_decode/spec_decode_worker.py
4	4	vllm/spec_decode/top1_proposer.py

[ae62fd17c] Maximilien de Bayser 2024-11-07 [Frontend] Tool calling parser for Granite 3.0 models (#9027)
26	18	docs/source/serving/openai_compatible_server.md
40	0	examples/tool_chat_template_granite.jinja
6	0	tests/tool_use/conftest.py
24	13	tests/tool_use/utils.py
3	2	vllm/entrypoints/openai/tool_parsers/__init__.py
215	0	vllm/entrypoints/openai/tool_parsers/granite_tool_parser.py

[a62bc0109] Atlas 2024-11-07 [Misc] Add Gamma-Distribution Request Generation Support for Serving Benchmark. (#10105)
52	5	benchmarks/benchmark_serving.py

[999df95b4] Jiahao Li 2024-11-07 [Bugfix] Make image processor respect `mm_processor_kwargs` for Qwen2-VL (#10112)
23	10	vllm/model_executor/models/qwen2_vl.py

[a6f332d0d] Li, Jiang 2024-11-07 [Hardware][CPU][bugfix] Fix half dtype support on AVX2-only target (#10108)
1	1	cmake/cpu_extension.cmake
10	0	csrc/cpu/cpu_types_x86.hpp

[0dfba97b4] Lei Yang 2024-11-07 [Frontend] Fix multiple values for keyword argument error (#10075) (#10076)
11	12	vllm/entrypoints/openai/serving_engine.py

[aa9078fa0] Flávia Béo 2024-11-07 Adds method to read the pooling types from model's files (#9506)
2	2	examples/fp8/quantizer/quantize.py
7	0	tests/engine/test_arg_utils.py
50	0	tests/model_executor/test_model_load_with_params.py
72	0	tests/test_config.py
8	6	tests/utils.py
23	5	vllm/config.py
2	1	vllm/engine/arg_utils.py
13	1	vllm/model_executor/layers/pooler.py
160	10	vllm/transformers_utils/config.py
5	0	vllm/transformers_utils/tokenizer_group/__init__.py

[e036e527a] Russell Bryant 2024-11-07 [CI/Build] Improve mypy + python version matrix (#10041)
1	1	.github/workflows/mypy.yaml
1	3	pyproject.toml
3	2	tools/mypy.sh

[6192e9b8f] Hanzhi Zhou 2024-11-06 [Core][Distributed] Refactor ipc buffer init in CustomAllreduce (#10030)
61	58	csrc/custom_all_reduce.cu
45	42	csrc/custom_all_reduce.cuh
13	11	csrc/custom_all_reduce_test.cu
9	13	csrc/ops.h
6	15	csrc/torch_bindings.cpp
2	2	tests/distributed/test_custom_all_reduce.py
16	16	tools/profiler/visualize_layerwise_profile.py
12	17	vllm/_custom_ops.py
54	86	vllm/distributed/device_communicators/custom_all_reduce.py

[d7263a1bb] Rafael Vasquez 2024-11-07 Doc: Improve benchmark documentation (#9927)
3	2	docs/source/dev/profiling/profiling_index.rst
2	2	docs/source/index.rst
33	0	docs/source/performance/benchmarks.rst
0	23	docs/source/performance_benchmark/benchmarks.rst

[104d72965] Russell Bryant 2024-11-07 [CI/Build] re-add codespell to CI (#10083)
45	0	.github/workflows/codespell.yml

[db7db4aab] Cyrus Leung 2024-11-07 [Misc] Consolidate ModelConfig code related to HF config (#10104)
1	1	docs/source/serving/compatibility_matrix.rst
38	0	tests/test_config.py
8	6	vllm/config.py
1	1	vllm/inputs/preprocess.py
9	0	vllm/transformers_utils/config.py
0	4	vllm/utils.py
1	8	vllm/worker/cpu_model_runner.py
1	4	vllm/worker/cpu_worker.py
8	15	vllm/worker/model_runner.py
1	4	vllm/worker/worker.py

[1fa020c53] Nick Hill 2024-11-07 [V1][BugFix] Fix Generator construction in greedy + seed case (#10097)
3	2	vllm/v1/worker/gpu_model_runner.py

[e7b84c394] youkaichao 2024-11-06 [doc] add back Python 3.8 ABI (#10100)
1	1	docs/source/getting_started/installation.rst

[a4b3e0c1e] Li, Jiang 2024-11-07 [Hardware][CPU] Update torch 2.5 (#9911)
1	1	.buildkite/run-cpu-test.sh
1	1	Dockerfile.cpu
1	0	cmake/cpu_extension.cmake
10	0	csrc/cpu/attention.cpp
45	33	csrc/cpu/cpu_types_x86.hpp
6	0	csrc/cpu/dnnl_helper.hpp
7	0	csrc/cpu/quant.cpp
2	4	docs/source/getting_started/cpu-installation.rst
1	1	requirements-cpu.txt
1	2	tests/models/decoder_only/language/test_models.py
0	5	vllm/executor/cpu_executor.py
1	1	vllm/model_executor/layers/quantization/ipex_quant.py

[29862b884] Nick Hill 2024-11-07 [Frontend] Adjust try/except blocks in API impl (#10056)
2	6	vllm/entrypoints/openai/serving_completion.py
3	5	vllm/entrypoints/openai/serving_embedding.py

[d3859f189] Yan Ma 2024-11-07 [Misc][XPU] Upgrade to Pytorch 2.5 for xpu backend (#9823)
11	1	Dockerfile.xpu
4	4	requirements-xpu.txt
8	25	vllm/_ipex_ops.py
20	16	vllm/attention/backends/ipex_attn.py

[4ab325664] Michael Goin 2024-11-06 [Bugfix] Fix FP8 torch._scaled_mm fallback for torch>2.5 with CUDA<12.4 (#10095)
2	4	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[719c1ca46] youkaichao 2024-11-06 [core][distributed] add stateless_init_process_group (#10072)
1	1	.buildkite/test-pipeline.yaml
73	2	tests/distributed/test_utils.py
73	0	vllm/distributed/utils.py

[74f2f8a0f] Russell Bryant 2024-11-06 [CI/Build] Always run the ruff workflow (#10092)
11	6	.github/workflows/ruff.yml

[d58268c56] Joe Runde 2024-11-06 [V1] Make v1 more testable (#9888)
3	0	Dockerfile
1	0	pyproject.toml
18	0	tests/conftest.py
9	0	tests/entrypoints/llm/test_prompt_validation.py
2	0	tests/kernels/test_attention_selector.py
2	2	tests/kernels/test_encoder_decoder_attn.py
33	10	vllm/attention/selector.py
10	8	vllm/engine/multiprocessing/engine.py
17	9	vllm/entrypoints/llm.py
9	0	vllm/model_executor/layers/sampler.py
2	2	vllm/model_executor/models/arctic.py
2	2	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bart.py
2	2	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/bloom.py
2	2	vllm/model_executor/models/chameleon.py
2	2	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/commandr.py
2	2	vllm/model_executor/models/dbrx.py
2	2	vllm/model_executor/models/deepseek.py
2	2	vllm/model_executor/models/deepseek_v2.py
2	2	vllm/model_executor/models/exaone.py
2	2	vllm/model_executor/models/falcon.py
2	2	vllm/model_executor/models/florence2.py
2	2	vllm/model_executor/models/gemma.py
2	2	vllm/model_executor/models/gemma2.py
2	2	vllm/model_executor/models/gpt2.py
2	2	vllm/model_executor/models/gpt_bigcode.py
2	2	vllm/model_executor/models/gpt_j.py
2	2	vllm/model_executor/models/gpt_neox.py
2	2	vllm/model_executor/models/granite.py
2	2	vllm/model_executor/models/granitemoe.py
2	2	vllm/model_executor/models/internlm2.py
2	2	vllm/model_executor/models/internvl.py
2	2	vllm/model_executor/models/jais.py
2	2	vllm/model_executor/models/jamba.py
2	2	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/llava_next.py
2	2	vllm/model_executor/models/llava_next_video.py
2	2	vllm/model_executor/models/llava_onevision.py
2	2	vllm/model_executor/models/mamba.py
2	2	vllm/model_executor/models/minicpm.py
2	2	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/mixtral.py
2	2	vllm/model_executor/models/mixtral_quant.py
2	2	vllm/model_executor/models/mllama.py
2	2	vllm/model_executor/models/mlp_speculator.py
2	2	vllm/model_executor/models/molmo.py
2	2	vllm/model_executor/models/mpt.py
2	2	vllm/model_executor/models/nemotron.py
2	2	vllm/model_executor/models/olmo.py
2	2	vllm/model_executor/models/olmoe.py
2	2	vllm/model_executor/models/opt.py
2	2	vllm/model_executor/models/orion.py
2	2	vllm/model_executor/models/persimmon.py
2	2	vllm/model_executor/models/phi.py
2	2	vllm/model_executor/models/phi3_small.py
2	2	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/phimoe.py
2	2	vllm/model_executor/models/pixtral.py
2	2	vllm/model_executor/models/qwen.py
2	2	vllm/model_executor/models/qwen2.py
2	2	vllm/model_executor/models/qwen2_audio.py
2	2	vllm/model_executor/models/qwen2_moe.py
2	2	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/model_executor/models/solar.py
2	2	vllm/model_executor/models/stablelm.py
2	2	vllm/model_executor/models/starcoder2.py
2	2	vllm/model_executor/models/ultravox.py
2	2	vllm/model_executor/models/xverse.py
6	6	vllm/v1/attention/backends/flash_attn.py
6	0	vllm/v1/engine/llm_engine.py
4	4	vllm/v1/tokenizer/detokenizer.py
1	4	vllm/v1/worker/gpu_model_runner.py

[87bd7e051] Russell Bryant 2024-11-06 [CI/Build] change conflict PR comment from mergify (#10080)
3	1	.github/mergify.yml

[098f94de4] Russell Bryant 2024-11-06 [CI/Build] Drop Python 3.8 support (#10038)
1	1	.readthedocs.yaml
1	1	CMakeLists.txt
0	2	docs/source/getting_started/amd-installation.rst
1	1	docs/source/getting_started/installation.rst
1	1	docs/source/getting_started/neuron-installation.rst
1	1	docs/source/getting_started/quickstart.rst
4	8	setup.py
2	3	vllm/distributed/parallel_state.py

[399c79860] Michael Goin 2024-11-06 Remove ScaledActivation for AWQ (#10057)
4	33	vllm/model_executor/layers/activation.py
0	3	vllm/model_executor/layers/quantization/aqlm.py
0	3	vllm/model_executor/layers/quantization/awq.py
0	3	vllm/model_executor/layers/quantization/awq_marlin.py
0	8	vllm/model_executor/layers/quantization/base_config.py
0	3	vllm/model_executor/layers/quantization/bitsandbytes.py
0	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
0	3	vllm/model_executor/layers/quantization/deepspeedfp.py
0	3	vllm/model_executor/layers/quantization/experts_int8.py
0	3	vllm/model_executor/layers/quantization/fbgemm_fp8.py
0	3	vllm/model_executor/layers/quantization/fp8.py
0	3	vllm/model_executor/layers/quantization/gguf.py
0	3	vllm/model_executor/layers/quantization/gptq.py
0	3	vllm/model_executor/layers/quantization/gptq_marlin.py
0	3	vllm/model_executor/layers/quantization/gptq_marlin_24.py
0	6	vllm/model_executor/layers/quantization/ipex_quant.py
0	3	vllm/model_executor/layers/quantization/marlin.py
0	3	vllm/model_executor/layers/quantization/modelopt.py
0	3	vllm/model_executor/layers/quantization/neuron_quant.py
0	3	vllm/model_executor/layers/quantization/qqq.py
0	3	vllm/model_executor/layers/quantization/tpu_int8.py
3	5	vllm/model_executor/models/bart.py
1	1	vllm/model_executor/models/bloom.py
1	1	vllm/model_executor/models/falcon.py
1	2	vllm/model_executor/models/gpt2.py
1	2	vllm/model_executor/models/gpt_bigcode.py
1	2	vllm/model_executor/models/gpt_j.py
1	2	vllm/model_executor/models/gpt_neox.py
1	1	vllm/model_executor/models/mpt.py
1	2	vllm/model_executor/models/opt.py
1	1	vllm/model_executor/models/persimmon.py
1	1	vllm/model_executor/models/phi.py
1	1	vllm/model_executor/models/qwen.py
1	2	vllm/model_executor/models/starcoder2.py

[406d4cc48] Eric 2024-11-06 [Model][LoRA]LoRA support added for Qwen2VLForConditionalGeneration (#10022)
1	1	docs/source/models/supported_models.rst
28	4	vllm/model_executor/models/qwen2_vl.py

[a5bba7d23] Jee Jee Li 2024-11-06 [Model] Add Idefics3 support (#9767)
6	0	docs/source/models/supported_models.rst
17	0	examples/offline_inference_vision_language.py
25	0	examples/offline_inference_vision_language_multi_image.py
16	0	tests/models/decoder_only/vision_language/test_models.py
2	0	vllm/entrypoints/chat_utils.py
24	1	vllm/model_executor/models/idefics2_vision_model.py
632	0	vllm/model_executor/models/idefics3.py
1	0	vllm/model_executor/models/registry.py

[2003cc351] Jee Jee Li 2024-11-06 [Model][LoRA]LoRA support added for LlamaEmbeddingModel (#10071)
1	1	docs/source/models/supported_models.rst
19	1	vllm/model_executor/models/llama.py

[6a585a23d] Woosuk Kwon 2024-11-06 [Hotfix] Fix ruff errors (#10073)
1	2	setup.py
1	1	vllm/executor/ray_hpu_executor.py
3	4	vllm/worker/hpu_model_runner.py

[a02a50e6e] Konrad Zawora 2024-11-06 [Hardware][Intel-Gaudi] Add Intel Gaudi (HPU) inference backend (#6143)
16	0	Dockerfile.hpu
402	0	docs/source/getting_started/gaudi-installation.rst
2	1	docs/source/index.rst
11	0	requirements-hpu.txt
45	2	setup.py
1	1	vllm/_custom_ops.py
264	0	vllm/attention/backends/hpu_attn.py
103	0	vllm/attention/ops/hpu_paged_attn.py
8	0	vllm/attention/selector.py
19	3	vllm/config.py
6	1	vllm/core/block/cpu_gpu_block_allocator.py
48	0	vllm/distributed/device_communicators/hpu_communicator.py
19	0	vllm/distributed/parallel_state.py
7	4	vllm/engine/arg_utils.py
8	0	vllm/engine/async_llm_engine.py
8	0	vllm/engine/llm_engine.py
205	0	vllm/executor/hpu_executor.py
554	0	vllm/executor/ray_hpu_executor.py
5	1	vllm/executor/ray_utils.py
3	2	vllm/model_executor/custom_op.py
19	0	vllm/model_executor/layers/layernorm.py
8	2	vllm/model_executor/layers/logits_processor.py
55	0	vllm/model_executor/layers/rotary_embedding.py
15	2	vllm/model_executor/layers/vocab_parallel_embedding.py
2	1	vllm/model_executor/sampling_metadata.py
10	0	vllm/platforms/__init__.py
11	0	vllm/platforms/hpu.py
4	0	vllm/platforms/interface.py
3	0	vllm/utils.py
2008	0	vllm/worker/hpu_model_runner.py
410	0	vllm/worker/hpu_worker.py

[a5fda50a1] Isotr0py 2024-11-06 [CI/Build] Fix large_gpu_mark reason (#10070)
1	1	tests/utils.py

[21063c11c] Aaron Pham 2024-11-06 [CI/Build] drop support for  Python 3.8 EOL (#8464)
5	5	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
2	2	.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
2	2	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
1	1	.github/workflows/mypy.yaml
1	1	.github/workflows/publish.yml
16	16	.github/workflows/ruff.yml
13	13	.github/workflows/yapf.yml
5	6	.readthedocs.yaml
18	18	CMakeLists.txt
7	15	benchmarks/backend_request_func.py
3	3	benchmarks/kernels/benchmark_machete.py
4	4	csrc/quantization/machete/generate.py
5	5	docs/source/getting_started/installation.rst
2	2	pyproject.toml
4	5	setup.py
2	2	tests/compile/piecewise/test_toy_llama.py
8	21	tests/conftest.py
5	7	tests/core/block/test_prefix_caching_block.py
1	1	tests/kernels/test_mamba_ssm.py
1	1	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py
4	6	tests/samplers/test_rejection_sampler.py
1	1	tests/test_logger.py
2	2	tests/tokenization/test_detokenize.py
1	1	tools/profiler/print_layerwise_table.py
1	1	tools/profiler/visualize_layerwise_profile.py
17	15	tools/report_build_time_ninja.py
1	1	use_existing_torch.py
2	4	vllm/attention/ops/blocksparse_attention/interface.py
4	3	vllm/config.py
1	1	vllm/core/evictor.py
1	1	vllm/distributed/device_communicators/custom_all_reduce_utils.py
1	1	vllm/engine/async_llm_engine.py
2	2	vllm/engine/llm_engine.py
1	1	vllm/engine/metrics_types.py
1	1	vllm/engine/output_processor/multi_step.py
1	1	vllm/entrypoints/chat_utils.py
1	1	vllm/entrypoints/openai/run_batch.py
1	1	vllm/executor/ray_gpu_executor.py
1	2	vllm/logger.py
2	2	vllm/lora/models.py
1	1	vllm/model_executor/custom_op.py
0	1	vllm/model_executor/layers/resampler.py
0	1	vllm/model_executor/layers/rotary_embedding.py
1	1	vllm/model_executor/model_loader/loader.py
1	1	vllm/model_executor/model_loader/openvino.py
2	3	vllm/model_executor/model_loader/tensorizer.py
4	5	vllm/model_executor/model_loader/weight_utils.py
2	2	vllm/model_executor/models/arctic.py
0	1	vllm/model_executor/models/baichuan.py
0	1	vllm/model_executor/models/bloom.py
0	1	vllm/model_executor/models/chatglm.py
0	1	vllm/model_executor/models/commandr.py
0	1	vllm/model_executor/models/dbrx.py
0	1	vllm/model_executor/models/decilm.py
0	1	vllm/model_executor/models/deepseek.py
0	1	vllm/model_executor/models/deepseek_v2.py
0	1	vllm/model_executor/models/exaone.py
0	1	vllm/model_executor/models/falcon.py
0	1	vllm/model_executor/models/fuyu.py
0	1	vllm/model_executor/models/gemma.py
0	1	vllm/model_executor/models/gemma2.py
0	1	vllm/model_executor/models/glm4_vision_encoder.py
0	1	vllm/model_executor/models/gpt2.py
0	1	vllm/model_executor/models/gpt_bigcode.py
0	1	vllm/model_executor/models/gpt_j.py
0	1	vllm/model_executor/models/gpt_neox.py
0	1	vllm/model_executor/models/granite.py
0	1	vllm/model_executor/models/granitemoe.py
0	2	vllm/model_executor/models/idefics2_vision_model.py
0	1	vllm/model_executor/models/internlm2.py
0	1	vllm/model_executor/models/internlm2_ve.py
0	1	vllm/model_executor/models/jais.py
0	1	vllm/model_executor/models/jamba.py
0	1	vllm/model_executor/models/llama.py
0	1	vllm/model_executor/models/mamba.py
0	1	vllm/model_executor/models/minicpm.py
0	1	vllm/model_executor/models/minicpm3.py
0	1	vllm/model_executor/models/minicpmv.py
0	1	vllm/model_executor/models/mixtral.py
0	1	vllm/model_executor/models/mixtral_quant.py
0	1	vllm/model_executor/models/mllama.py
1	1	vllm/model_executor/models/mlp_speculator.py
3	3	vllm/model_executor/models/molmo.py
0	1	vllm/model_executor/models/mpt.py
0	1	vllm/model_executor/models/nemotron.py
0	1	vllm/model_executor/models/olmo.py
0	1	vllm/model_executor/models/opt.py
0	1	vllm/model_executor/models/orion.py
0	1	vllm/model_executor/models/persimmon.py
0	1	vllm/model_executor/models/phi.py
0	1	vllm/model_executor/models/phi3.py
0	1	vllm/model_executor/models/phi3v.py
0	1	vllm/model_executor/models/phimoe.py
5	5	vllm/model_executor/models/pixtral.py
0	1	vllm/model_executor/models/qwen.py
3	4	vllm/model_executor/models/qwen2.py
0	1	vllm/model_executor/models/qwen2_audio.py
3	4	vllm/model_executor/models/qwen2_cls.py
0	1	vllm/model_executor/models/qwen2_moe.py
3	4	vllm/model_executor/models/qwen2_rm.py
4	6	vllm/model_executor/models/qwen2_vl.py
0	1	vllm/model_executor/models/solar.py
0	1	vllm/model_executor/models/stablelm.py
0	1	vllm/model_executor/models/starcoder2.py
0	1	vllm/model_executor/models/xverse.py
39	27	vllm/multimodal/base.py
8	9	vllm/prompt_adapter/utils.py
1	1	vllm/transformers_utils/config.py
0	1	vllm/transformers_utils/configs/chatglm.py
0	1	vllm/transformers_utils/configs/exaone.py
0	1	vllm/transformers_utils/configs/jais.py
3	4	vllm/transformers_utils/configs/mpt.py
3	4	vllm/transformers_utils/configs/nemotron.py
0	1	vllm/transformers_utils/configs/solar.py
2	2	vllm/utils.py

[4be3a4515] youkaichao 2024-11-05 [distributed] add function to create ipc buffers directly (#10064)
1	0	.buildkite/test-pipeline.yaml
59	0	tests/distributed/test_ca_buffer_sharing.py
31	0	vllm/distributed/device_communicators/custom_all_reduce.py

[408998555] Woosuk Kwon 2024-11-05 [V1] Integrate Piecewise CUDA graphs (#10058)
5	2	vllm/compilation/backends.py
21	14	vllm/v1/attention/backends/flash_attn.py
107	20	vllm/v1/worker/gpu_model_runner.py

[9d59b7559] zifeitong 2024-11-05 [Bugfix] Remove CustomChatCompletionContentPartParam multimodal input type (#10054)
2	11	vllm/entrypoints/chat_utils.py

[ea928f608] arakowsk-amd 2024-11-05 [Bugfix] Gpt-j-6B patch kv_scale to k_scale path  (#10063)
5	1	vllm/model_executor/models/gpt_j.py

[2bcbae704] Travis Johnson 2024-11-05 [Bugfix] Fix edge-case crash when using chat with the Mistral Tekken Tokenizer (#10051)
6	3	tests/models/decoder_only/language/test_mistral.py
6	2	vllm/transformers_utils/tokenizers/mistral.py

[ffc0f2b47] Peter Salas 2024-11-05 [Model][OpenVINO] Fix regressions from #8346 (#10045)
1	1	.buildkite/run-openvino-test.sh
11	1	vllm/attention/backends/openvino.py
3	3	vllm/model_executor/models/molmo.py

[82bfc38d0] Cyrus Leung 2024-11-06 [Misc] Sort the list of embedding models (#10037)
8	18	vllm/model_executor/models/registry.py

[c4cacbaa7] youkaichao 2024-11-05 [v1] reduce graph capture time for piecewise cudagraph (#10059)
25	11	vllm/compilation/backends.py

[0c63c34f7] Sungjae Lee 2024-11-06 [Bugfix][SpecDecode] kv corruption with bonus tokens in spec decode (#9730)
107	0	tests/spec_decode/test_multi_step_worker.py
3	1	tests/spec_decode/utils.py
30	5	vllm/spec_decode/draft_model_runner.py
19	4	vllm/spec_decode/multi_step_worker.py

[966e31697] Wallas Henrique 2024-11-05 [Bugfix] Fix pickle of input when async output processing is on (#9931)
26	0	tests/basic_correctness/test_basic_correctness.py
12	0	vllm/worker/model_runner.py

[43300bd98] zifeitong 2024-11-05 [Bugfix] Properly propagate trust_remote_code settings (#10047)
4	3	vllm/model_executor/models/chatglm.py
12	10	vllm/model_executor/models/molmo.py

[ca9844b34] youkaichao 2024-11-05 [bugfix] fix weak ref in piecewise cudagraph and tractable test (#10048)
101	10	tests/compile/piecewise/test_toy_llama.py
67	15	vllm/compilation/backends.py

[235366fe2] Michael Goin 2024-11-05 [CI] Prune back the number of tests in tests/kernels/* (#9932)
1	1	tests/kernels/test_activation.py
1	1	tests/kernels/test_attention.py
10	6	tests/kernels/test_awq_marlin.py
3	3	tests/kernels/test_blocksparse_attention.py
1	1	tests/kernels/test_cache.py
24	6	tests/kernels/test_cutlass.py
3	4	tests/kernels/test_int8_quant.py
1	1	tests/kernels/test_marlin_gemm.py
13	10	tests/kernels/test_moe.py
3	3	tests/kernels/test_pos_encoding.py

[02462465e] Michael Goin 2024-11-05 [CI] Prune tests/models/decoder_only/language/* tests (#9940)
1	2	.buildkite/test-pipeline.yaml
0	93	tests/models/decoder_only/language/test_big_models.py
5	5	tests/models/decoder_only/language/test_fp8.py
0	13	tests/models/decoder_only/language/test_gptq_marlin.py
6	6	tests/models/decoder_only/language/test_gptq_marlin_24.py
0	69	tests/models/decoder_only/language/test_marlin.py
21	16	tests/models/decoder_only/language/test_mistral.py
37	32	tests/models/decoder_only/language/test_models.py
0	34	tests/models/decoder_only/language/test_qwen.py

[b9c64c0ca] Jee Jee Li 2024-11-06 [Misc] Modify BNB parameter name (#9997)
5	4	vllm/model_executor/layers/quantization/bitsandbytes.py
1	1	vllm/model_executor/layers/resampler.py
5	9	vllm/model_executor/model_loader/loader.py

[d2e80332a] lkchen 2024-11-05 [Feature] Update benchmark_throughput.py to support image input (#9851)
11	0	benchmarks/README.md
64	18	benchmarks/benchmark_throughput.py

[a53046b16] Michael Goin 2024-11-05 [Model] Support quantization of PixtralHFTransformer for PixtralHF (#9921)
30	0	vllm/model_executor/layers/activation.py
60	40	vllm/model_executor/models/pixtral.py

[731aec5be] Russell Bryant 2024-11-05 [CI/Build] Limit github CI jobs based on files changed (#9928)
2	0	.github/workflows/actionlint.yml
12	0	.github/workflows/clang-format.yml
10	0	.github/workflows/mypy.yaml
13	4	.github/workflows/ruff.yml
8	1	.github/workflows/yapf.yml

[09d355037] Chenghao (Alan) Yang 2024-11-05 [Misc] Add logging for CUDA memory (#10027)
14	10	vllm/worker/model_runner.py

[cd34029e9] Richard Liu 2024-11-05 Refactor TPU requirements file and pin build dependencies (#10010)
0	7	Dockerfile.tpu
5	52	docs/source/getting_started/tpu-installation.rst
18	2	requirements-tpu.txt

[5952d8113] Russell Bryant 2024-11-05 [Frontend] Fix tcp port reservation for api server (#10012)
4	2	vllm/entrypoints/openai/api_server.py

[93dee88f6] Chauncey 2024-11-05 [Misc] vllm CLI flags should be ordered for better user readability (#10017)
14	0	vllm/utils.py

[7a83b1aec] Gene Der Su 2024-11-05 [BugFix] Lazy import ray (#10021)
11	6	vllm/engine/multiprocessing/engine.py

[ad2331892] Tyler Michael Smith 2024-11-04 [Bugfix] Fixup Mamba (#10004)
3	4	vllm/model_executor/models/mamba.py

[bbc3619dc] Cyrus Leung 2024-11-05 [Core] Make encoder-decoder inputs a nested structure to be more composable (#9604)
26	31	tests/core/utils.py
2	1	tests/engine/output_processor/test_stop_checker.py
3	4	tests/test_cache_block_hashing.py
2	4	tests/tokenization/test_detokenize.py
24	27	vllm/engine/llm_engine.py
16	7	vllm/engine/protocol.py
6	5	vllm/inputs/__init__.py
25	26	vllm/inputs/data.py
9	6	vllm/inputs/parse.py
136	133	vllm/inputs/preprocess.py
7	7	vllm/inputs/registry.py
67	29	vllm/model_executor/models/mllama.py
5	0	vllm/model_executor/models/registry.py
44	69	vllm/sequence.py

[04bbf38e0] Tyler Michael Smith 2024-11-04 [Core] Use os.sched_yield in ShmRingBuffer instead of time.sleep (#9994)
5	10	vllm/distributed/device_communicators/shm_broadcast.py

[8f0a9ca89] Michael Goin 2024-11-04 [Bugfix] Respect modules_to_not_convert within awq_marlin (#9895)
24	11	vllm/model_executor/layers/quantization/awq_marlin.py

[2094062b4] youkaichao 2024-11-04 [4.5/N] bugfix for quant config in speculative decode (#10007)
4	0	vllm/spec_decode/spec_decode_worker.py

[d93478b39] bnellnm 2024-11-04 [Bugfix] Upgrade to pytorch 2.5.1 (#10001)
2	2	CMakeLists.txt
1	1	pyproject.toml
1	1	requirements-build.txt
3	3	requirements-cuda.txt
1	1	requirements-openvino.txt
1	1	requirements-test.in
2	2	requirements-test.txt

[ac04a97a9] tomeras91 2024-11-05 [Frontend] Add max_tokens prometheus metric (#9881)
9	2	tests/entrypoints/openai/test_metrics.py
1	0	tests/metrics/test_metrics.py
4	0	vllm/engine/llm_engine.py
8	0	vllm/engine/metrics.py
1	0	vllm/engine/metrics_types.py

[9a5664d4a] lkchen 2024-11-04 [Misc] Refactor benchmark_throughput.py (#9779)
55	26	benchmarks/benchmark_throughput.py

[04cef2c6a] Robert Shaw 2024-11-04 [Bugfix] Fix `MQLLMEngine` hanging (#9973)
10	2	vllm/engine/multiprocessing/client.py
15	9	vllm/engine/multiprocessing/engine.py
17	12	vllm/entrypoints/openai/api_server.py

[6e056bcf0] Roger Wang 2024-11-04 [Doc] Update VLM doc about loading from local files (#9999)
4	0	docs/source/models/vlm.rst

[5208dc7a2] hissu-hyvarinen 2024-11-04 [Bugfix][CI/Build][Hardware][AMD] Shard ID parameters in AMD tests running parallel jobs (#9279)
6	5	.buildkite/run-amd-test.sh
6	1	tests/lora/test_minicpmv.py

[1c45f4c38] Robert Shaw 2024-11-04 [CI] Basic Integration Test For TPU (#9968)
1	1	.buildkite/run-tpu-test.sh
15	2	tests/entrypoints/openai/test_accuracy.py

[603a661ae] Mor Zusman 2024-11-04 [Model] factoring out MambaMixer out of Jamba (#8993)
217	0	vllm/model_executor/layers/mamba/mamba_mixer.py
14	185	vllm/model_executor/models/jamba.py
14	189	vllm/model_executor/models/mamba.py

[fb2716d64] Jee Jee Li 2024-11-05 [Misc]Reduce BNB static variable (#9987)
20	20	vllm/model_executor/model_loader/loader.py
0	2	vllm/model_executor/models/falcon.py
0	3	vllm/model_executor/models/gemma.py
0	2	vllm/model_executor/models/gemma2.py
0	2	vllm/model_executor/models/llama.py
0	8	vllm/model_executor/models/minicpmv.py
0	2	vllm/model_executor/models/mllama.py
0	2	vllm/model_executor/models/opt.py
0	2	vllm/model_executor/models/phi.py
0	3	vllm/model_executor/models/qwen2.py

[8d72bb20f] youkaichao 2024-11-04 [4/N] make quant config first-class citizen (#9978)
38	0	vllm/config.py
3	31	vllm/model_executor/model_loader/loader.py

[ac6b8f19b] Chauncey 2024-11-04 [Frontend] Multi-Modality Support for Loading Local Image Files (#9915)
37	2	tests/multimodal/test_utils.py
8	0	vllm/config.py
9	0	vllm/engine/arg_utils.py
7	2	vllm/entrypoints/chat_utils.py
6	0	vllm/entrypoints/llm.py
65	10	vllm/multimodal/utils.py

[ccb5376a9] Mengqing Cao 2024-11-04 [Bugfix][OpenVINO] Fix circular reference #9939 (#9974)
4	2	vllm/platforms/openvino.py

[ea4adeddc] Tran Quang Dai 2024-11-04 [Bugfix] Fix E2EL mean and median stats (#9984)
2	2	benchmarks/benchmark_serving.py

[4dbcbbeb0] Yang Zheng 2024-11-04 [Misc] Compute query_start_loc/seq_start_loc on CPU (#9447)
10	18	vllm/attention/backends/flash_attn.py
10	18	vllm/attention/backends/utils.py

[b67feb127] Gregory Shtrasberg 2024-11-04 [Bugfix]Using the correct type hints (#9885)
5	3	vllm/sequence.py

[c49f0407b] Jee Jee Li 2024-11-04 [Bugfix] Fix MiniCPMV and Mllama BNB  bug (#9917)
28	21	vllm/model_executor/layers/resampler.py
28	6	vllm/model_executor/model_loader/loader.py
83	37	vllm/model_executor/models/minicpmv.py
6	1	vllm/model_executor/models/mllama.py

[91c9ebbb1] Robert Shaw 2024-11-03 [V1] Fix Configs (#9971)
3	2	vllm/v1/executor/gpu_executor.py

[54597724f] shanshan wang 2024-11-03 [Model] Add support for H2OVL-Mississippi models (#9747)
6	0	docs/source/models/supported_models.rst
27	1	examples/offline_inference_vision_language.py
35	0	examples/offline_inference_vision_language_multi_image.py
130	0	tests/models/decoder_only/vision_language/test_h2ovl.py
17	0	tests/models/decoder_only/vision_language/test_models.py
60	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
2	1	vllm/entrypoints/chat_utils.py
401	0	vllm/model_executor/models/h2ovl.py
2	1	vllm/model_executor/models/registry.py
2	0	vllm/transformers_utils/config.py
3	1	vllm/transformers_utils/configs/__init__.py
13	0	vllm/transformers_utils/configs/h2ovl.py

[1f1b6d6ed] Nick Hill 2024-11-03 [V1] Support per-request seed (#9945)
2	3	vllm/v1/sample/metadata.py
10	13	vllm/v1/sample/sampler.py
29	32	vllm/v1/worker/gpu_model_runner.py

[3bb4befea] youkaichao 2024-11-02 [bugfix] fix tsts (#9959)
1	1	vllm/model_executor/model_loader/loader.py
1	0	vllm/model_executor/models/utils.py

[ae5279a16] Yongzao 2024-11-03 [torch.compile] Adding torch compile to vision-language models (#9946)
7	3	vllm/model_executor/models/llava_next.py
6	1	vllm/model_executor/models/minicpmv.py
8	4	vllm/model_executor/models/molmo.py

[1b73ab2a1] Nikita Furin 2024-11-02 [CI/Build] Quoting around > (#9956)
1	1	Dockerfile
1	1	Dockerfile.neuron
1	1	Dockerfile.ppc64le
1	1	Dockerfile.rocm
1	1	Dockerfile.tpu

[cea808f32] youkaichao 2024-11-02 [3/N] model runner pass the whole config to model (#9958)
4	5	tests/lora/conftest.py
4	16	vllm/model_executor/model_loader/__init__.py
54	78	vllm/model_executor/model_loader/loader.py
20	2	vllm/plugins/__init__.py
1	7	vllm/v1/worker/gpu_model_runner.py
1	7	vllm/worker/cpu_model_runner.py
1	7	vllm/worker/model_runner.py
1	9	vllm/worker/tpu_model_runner.py
1	9	vllm/worker/xpu_model_runner.py

[74b529cee] youkaichao 2024-11-02 [bugfix] fix chatglm dummy_data_for_glmv (#9955)
7	9	vllm/model_executor/models/chatglm.py

[d6459b451] Robert Shaw 2024-11-02 [V1] Fix `EngineArgs` refactor on V1 (#9954)
12	27	vllm/v1/executor/gpu_executor.py

[e89379544] youkaichao 2024-11-02 [2/N] executor pass the complete config to worker/modelrunner (#9938)
1	7	tests/lora/test_long_context.py
8	4	tests/lora/test_worker.py
1	6	tests/spec_decode/utils.py
1	8	tests/worker/test_encoder_decoder_model_runner.py
1	9	tests/worker/test_model_runner.py
1	6	tests/worker/test_profile.py
1	6	tests/worker/test_swap.py
9	15	vllm/config.py
7	6	vllm/engine/arg_utils.py
4	4	vllm/engine/async_llm_engine.py
5	4	vllm/engine/llm_engine.py
2	2	vllm/engine/multiprocessing/client.py
1	8	vllm/executor/cpu_executor.py
2	2	vllm/executor/executor_base.py
1	10	vllm/executor/gpu_executor.py
1	5	vllm/executor/neuron_executor.py
1	7	vllm/executor/openvino_executor.py
1	6	vllm/executor/tpu_executor.py
3	33	vllm/spec_decode/draft_model_runner.py
1	1	vllm/spec_decode/ngram_worker.py
18	17	vllm/spec_decode/spec_decode_worker.py
9	25	vllm/spec_decode/target_model_runner.py
5	4	vllm/v1/engine/llm_engine.py
1	10	vllm/v1/executor/gpu_executor.py
19	22	vllm/v1/worker/gpu_model_runner.py
17	33	vllm/v1/worker/gpu_worker.py
6	19	vllm/worker/cpu_model_runner.py
9	28	vllm/worker/cpu_worker.py
4	22	vllm/worker/embedding_model_runner.py
5	20	vllm/worker/enc_dec_model_runner.py
7	21	vllm/worker/model_runner.py
17	0	vllm/worker/model_runner_base.py
1	0	vllm/worker/multi_step_model_runner.py
1	9	vllm/worker/multi_step_worker.py
4	12	vllm/worker/neuron_model_runner.py
6	14	vllm/worker/neuron_worker.py
13	20	vllm/worker/openvino_model_runner.py
6	28	vllm/worker/openvino_worker.py
3	14	vllm/worker/tpu_model_runner.py
6	22	vllm/worker/tpu_worker.py
10	35	vllm/worker/worker.py
17	1	vllm/worker/worker_base.py
6	23	vllm/worker/xpu_model_runner.py
8	32	vllm/worker/xpu_worker.py

[1d4cfe2be] Michael Green 2024-11-02 [Doc] Updated tpu-installation.rst with more details (#9926)
144	14	docs/source/getting_started/tpu-installation.rst

[eed92f12f] Nick Hill 2024-11-02 [Docs] Update Granite 3.0 models in supported models table (#9930)
4	4	docs/source/models/supported_models.rst

[af7380d83] youkaichao 2024-11-01 [torch.compile] fix cpu broken code (#9947)
8	1	vllm/utils.py

[a78dd3303] sroy745 2024-11-01 [Encoder Decoder] Add flash_attn kernel support for encoder-decoder models (#9559)
51	37	tests/encoder_decoder/test_e2e_correctness.py
115	41	tests/kernels/test_encoder_decoder_attn.py
74	16	tests/kernels/utils.py
1	1	tests/models/encoder_decoder/vision_language/test_florence2.py
278	86	vllm/attention/backends/flash_attn.py
143	16	vllm/attention/backends/utils.py
26	105	vllm/attention/backends/xformers.py
1	1	vllm/attention/selector.py
0	2	vllm/model_executor/models/bart.py
2	2	vllm/utils.py
25	10	vllm/worker/enc_dec_model_runner.py

[d522034c8] Kevin H. Luu 2024-11-01 [ci/build] Have dependabot ignore pinned dependencies (#9935)
9	0	.github/dependabot.yml

[6c0b7f548] Peter Salas 2024-11-01 [Core][VLM] Add precise multi-modal placeholder tracking (#8346)
1	5	examples/offline_inference_audio_language.py
2	0	tests/kernels/utils.py
74	17	tests/models/decoder_only/audio_language/test_ultravox.py
7	7	tests/multimodal/test_processor_kwargs.py
45	12	tests/multimodal/test_utils.py
3	0	tests/worker/test_model_input.py
11	0	vllm/attention/backends/abstract.py
3	0	vllm/attention/backends/blocksparse_attn.py
20	0	vllm/attention/backends/flash_attn.py
18	0	vllm/attention/backends/flashinfer.py
21	1	vllm/attention/backends/placeholder_attn.py
3	0	vllm/attention/backends/rocm_flash_attn.py
18	0	vllm/attention/backends/utils.py
3	0	vllm/attention/backends/xformers.py
2	0	vllm/core/scheduler.py
2	1	vllm/inputs/__init__.py
10	1	vllm/inputs/data.py
23	17	vllm/inputs/registry.py
8	2	vllm/model_executor/models/blip.py
10	5	vllm/model_executor/models/blip2.py
16	6	vllm/model_executor/models/chameleon.py
21	11	vllm/model_executor/models/clip.py
20	11	vllm/model_executor/models/fuyu.py
4	4	vllm/model_executor/models/internvl.py
8	7	vllm/model_executor/models/llava.py
6	5	vllm/model_executor/models/llava_next.py
17	8	vllm/model_executor/models/llava_next_video.py
11	10	vllm/model_executor/models/llava_onevision.py
3	3	vllm/model_executor/models/minicpmv.py
4	3	vllm/model_executor/models/mllama.py
4	4	vllm/model_executor/models/paligemma.py
4	4	vllm/model_executor/models/phi3v.py
22	12	vllm/model_executor/models/pixtral.py
5	5	vllm/model_executor/models/qwen.py
11	4	vllm/model_executor/models/qwen2_audio.py
6	5	vllm/model_executor/models/qwen2_vl.py
17	7	vllm/model_executor/models/siglip.py
32	28	vllm/model_executor/models/ultravox.py
17	1	vllm/model_executor/models/utils.py
5	2	vllm/multimodal/__init__.py
209	5	vllm/multimodal/base.py
5	3	vllm/multimodal/image.py
10	8	vllm/multimodal/registry.py
18	3	vllm/multimodal/utils.py
9	5	vllm/multimodal/video.py
13	4	vllm/sequence.py
31	7	vllm/worker/cpu_model_runner.py
17	13	vllm/worker/enc_dec_model_runner.py
16	5	vllm/worker/model_runner.py
2	3	vllm/worker/model_runner_base.py
31	12	vllm/worker/openvino_model_runner.py
4	0	vllm/worker/tpu_model_runner.py
32	6	vllm/worker/xpu_model_runner.py

[d151fde83] dependabot[bot] 2024-11-01 [ci/build] Bump the patch-update group with 10 updates (#9897)
1	1	requirements-lint.txt
1	1	requirements-test.in
6	6	requirements-test.txt

[27cd36e6e] Gene Der Su 2024-11-01 [Bugfix] PicklingError on RayTaskError (#9934)
6	0	vllm/engine/multiprocessing/engine.py

[18bd7587b] youkaichao 2024-11-01 [1/N] pass the complete config from engine to executor (#9933)
1	1	vllm/engine/async_llm_engine.py
21	29	vllm/engine/llm_engine.py
1	6	vllm/engine/multiprocessing/engine.py
13	24	vllm/executor/executor_base.py
8	36	vllm/executor/xpu_executor.py
21	41	vllm/v1/engine/llm_engine.py

[598b6d7b0] Pavani Majety 2024-11-01 [Bugfix/Core] Flashinfer k_scale and v_scale (#9861)
14	7	tests/kernels/test_cache.py
6	3	vllm/attention/backends/flashinfer.py
5	2	vllm/model_executor/layers/quantization/modelopt.py

[aff1fd818] youkaichao 2024-11-01 [torch.compile] use interpreter with stable api from pytorch (#9889)
89	76	vllm/compilation/backends.py

[4581d2cc0] André Jonasson 2024-11-01 [Core] Refactor: Clean up unused argument in Scheduler._preempt (#9696)
3	8	vllm/core/scheduler.py

[1dd4cb293] Travis Johnson 2024-11-01 [Bugfix] Fix edge cases for MistralTokenizer (#9625)
63	17	tests/tokenization/test_detokenize.py
42	22	vllm/transformers_utils/tokenizers/mistral.py

[ba0d89207] Cyrus Leung 2024-11-01 [Frontend] Use a proper chat template for VLM2Vec (#9912)
10	4	docs/source/models/vlm.rst
0	0	examples/{openai_api_client_for_multimodal.py => openai_chat_completion_client_for_multimodal.py}
33	0	examples/openai_chat_embedding_client_for_multimodal.py
16	0	examples/template_vlm2vec.jinja
8	3	tests/entrypoints/openai/test_vision_embedding.py
11	4	vllm/entrypoints/chat_utils.py

[30a2e8074] Michael Goin 2024-11-01 [CI/Build] Add Model Tests for PixtralHF (#9813)
9	0	tests/models/decoder_only/vision_language/test_models.py

[06386a64d] Cyrus Leung 2024-11-01 [Frontend] Chat-based Embeddings API (#9759)
2	0	docs/requirements-docs.txt
1	1	docs/source/conf.py
5	0	docs/source/dev/pooling_params.rst
4	4	docs/source/getting_started/quickstart.rst
1	0	docs/source/index.rst
51	3	docs/source/models/vlm.rst
44	11	docs/source/serving/openai_compatible_server.md
4	9	tests/entrypoints/openai/test_basic.py
90	47	tests/entrypoints/openai/test_embedding.py
6	8	tests/entrypoints/openai/test_metrics.py
18	14	tests/entrypoints/openai/test_tokenization.py
94	0	tests/entrypoints/openai/test_vision_embedding.py
58	38	vllm/entrypoints/openai/api_server.py
84	3	vllm/entrypoints/openai/protocol.py
26	8	vllm/entrypoints/openai/run_batch.py
90	132	vllm/entrypoints/openai/serving_chat.py
32	43	vllm/entrypoints/openai/serving_completion.py
54	33	vllm/entrypoints/openai/serving_embedding.py
150	9	vllm/entrypoints/openai/serving_engine.py
37	50	vllm/entrypoints/openai/serving_tokenization.py
2	2	vllm/pooling_params.py

[d3aa2a8b2] Cyrus Leung 2024-11-01 [Doc] Update multi-input support (#9906)
2	2	docs/source/models/supported_models.rst

[2b5bf2098] Yongzao 2024-11-01 [torch.compile] Adding torch compile annotations to some models (#9876)
1	1	docs/source/models/supported_models.rst
1	1	tests/distributed/test_pipeline_parallel.py
2	0	vllm/model_executor/models/falcon.py
2	0	vllm/model_executor/models/phi.py
2	0	vllm/model_executor/models/qwen.py
2	0	vllm/model_executor/models/qwen2.py
2	0	vllm/model_executor/models/qwen2_moe.py

[93a76dd21] Michael Goin 2024-11-01 [Model] Support bitsandbytes for MiniCPMV (#9891)
43	0	vllm/model_executor/models/minicpmv.py

[566cd2779] youkaichao 2024-10-31 [torch.compile] rework test plans (#9866)
95	18	tests/compile/test_basic_correctness.py
119	5	tests/utils.py
5	5	vllm/model_executor/models/llava.py
7	3	vllm/model_executor/models/phi3v.py

[37a4947dc] Michael Goin 2024-11-01 [Bugfix] Fix layer skip logic with bitsandbytes (#9887)
6	1	vllm/model_executor/layers/quantization/bitsandbytes.py

[96e0c9cbb] youkaichao 2024-10-31 [torch.compile] directly register custom op (#9896)
16	4	tests/compile/piecewise/test_simple.py
16	4	tests/compile/piecewise/test_toy_llama.py
11	5	vllm/attention/backends/flash_attn.py
11	6	vllm/attention/backends/flashinfer.py
23	11	vllm/distributed/parallel_state.py
19	6	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
41	27	vllm/model_executor/layers/fused_moe/fused_moe.py
45	0	vllm/utils.py
10	4	vllm/v1/attention/backends/flash_attn.py

[031a7995f] Joe Runde 2024-10-31 [Bugfix][Frontend] Reject guided decoding in multistep mode (#9892)
1	1	docs/source/serving/compatibility_matrix.rst
20	0	tests/entrypoints/openai/test_prompt_validation.py
7	0	vllm/engine/llm_engine.py
2	2	vllm/sampling_params.py

[b63c64d95] Kevin H. Luu 2024-10-31 [ci/build] Configure dependabot to update pip dependencies  (#9811)
16	0	.github/dependabot.yml

[9fb12f784] Mor Zusman 2024-10-31 [BugFix][Kernel] Fix Illegal memory access in causal_conv1d in H100 (#9838)
32	2	csrc/mamba/causal_conv1d/causal_conv1d.cu
5	2	tests/kernels/test_causal_conv1d.py
3	3	tests/kernels/test_mamba_ssm.py

[55650c83a] sasha0552 2024-10-31 [Bugfix] Fix `illegal memory access` error with chunked prefill, prefix caching, block manager v2 and xformers enabled together (#9532)
28	0	tests/prefix_caching/test_prefix_caching.py
6	3	vllm/attention/backends/utils.py

[77f7ef290] Alexei-V-Ivanov-AMD 2024-10-31 [CI/Build] Adding a forced docker system prune to clean up space (#9849)
2	2	.buildkite/run-amd-test.sh

[16b8f7a86] Alex Brooks 2024-10-31 [CI/Build] Add Model Tests for Qwen2-VL (#9846)
14	3	.buildkite/test-pipeline.yaml
1	2	examples/offline_inference_vision_language.py
2	0	tests/models/decoder_only/audio_language/test_ultravox.py
1	1	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py
62	39	tests/models/decoder_only/vision_language/test_models.py
11	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
10	1	tests/models/decoder_only/vision_language/vlm_utils/runners.py
2	4	tests/models/decoder_only/vision_language/vlm_utils/types.py
3	2	tests/models/embedding/vision_language/test_llava_next.py

[5608e611c] Jee Jee Li 2024-10-31 [Doc] Update Qwen documentation (#9869)
5	2	docs/source/models/supported_models.rst
1	1	vllm/model_executor/models/qwen.py

[3ea2dc2ec] Roger Wang 2024-10-31 [Misc] Remove deprecated arg for cuda graph capture (#9864)
0	7	vllm/config.py
0	10	vllm/engine/arg_utils.py
0	5	vllm/entrypoints/llm.py
1	1	vllm/worker/model_runner.py

[d087bf863] Michael Goin 2024-10-31 [Model] Support quantization of Qwen2VisionTransformer (#9817)
35	23	vllm/model_executor/models/qwen2_vl.py

[890ca3607] Kevin H. Luu 2024-10-30 Revert "[Bugfix] Use host argument to bind to interface (#9798)" (#9852)
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/cli_args.py

[abbfb6134] Guillaume Calmettes 2024-10-31 [Misc][OpenAI] deprecate max_tokens in favor of new max_completion_tokens field for chat completion endpoint (#9837)
1	1	benchmarks/backend_request_func.py
3	3	docs/source/serving/run_on_sky.rst
4	4	examples/offline_inference_openai.md
6	6	examples/openai_api_client_for_multimodal.py
2	2	examples/openai_example_batch.jsonl
1	1	requirements-common.txt
17	15	tests/entrypoints/openai/test_audio.py
54	49	tests/entrypoints/openai/test_chat.py
20	18	tests/entrypoints/openai/test_vision.py
4	4	tests/tool_use/test_chat_completions.py
4	4	tests/tool_use/test_parallel_tool_calls.py
4	4	tests/tool_use/test_tool_calls.py
10	3	vllm/entrypoints/openai/protocol.py
10	4	vllm/entrypoints/openai/serving_engine.py

[64384bbcd] youkaichao 2024-10-30 [torch.compile] upgrade tests (#9858)
14	12	tests/compile/test_basic_correctness.py

[00d91c8a2] Yongzao 2024-10-31 [CI/Build] Simplify exception trace in api server tests (#9787)
7	3	tests/utils.py

[c2cd1a214] youkaichao 2024-10-30 [doc] update pp support (#9853)
1	4	docs/source/serving/distributed_serving.rst

[c787f2d81] Harsha vardhan manoj Bikki 2024-10-30 [Neuron] Update Dockerfile.neuron to fix build failure (#9822)
1	1	Dockerfile.neuron

[33d257735] Joe Runde 2024-10-30 [Doc] link bug for multistep guided decoding (#9843)
1	1	docs/source/serving/compatibility_matrix.rst

[3b3f1e743] Joe Runde 2024-10-30 [Bugfix][core] replace heartbeat with pid check (#9818)
26	1	tests/mq_llm_engine/test_error_handling.py
1	1	tests/mq_llm_engine/utils.py
19	10	vllm/engine/multiprocessing/client.py
11	48	vllm/engine/multiprocessing/engine.py
5	2	vllm/entrypoints/openai/api_server.py

[9ff4511e4] Elfie Guo 2024-10-30 [Misc] Add chunked-prefill support on FlashInfer. (#9781)
12	0	tests/basic_correctness/test_chunked_prefill.py
60	28	vllm/attention/backends/flashinfer.py

[81f09cfd8] Went-Liang 2024-10-31 [Model] Support math-shepherd-mistral-7b-prm model (#9697)
85	30	vllm/config.py
64	0	vllm/engine/arg_utils.py
3	1	vllm/engine/llm_engine.py
10	0	vllm/entrypoints/llm.py
58	4	vllm/model_executor/layers/pooler.py
10	5	vllm/model_executor/model_loader/loader.py
7	2	vllm/model_executor/models/bert.py
7	3	vllm/model_executor/models/gemma2.py
21	2	vllm/model_executor/models/llama.py
8	4	vllm/model_executor/models/llava_next.py
9	4	vllm/model_executor/models/phi3v.py
7	4	vllm/model_executor/models/qwen2_cls.py
7	3	vllm/model_executor/models/qwen2_rm.py
16	0	vllm/model_executor/models/registry.py

[cc98f1e07] Alex Brooks 2024-10-30 [CI/Build] VLM Test Consolidation (#9372)
5	2	.buildkite/test-pipeline.yaml
3	3	tests/conftest.py
29	0	tests/engine/test_short_mm_context.py
1	1	tests/models/decoder_only/audio_language/test_ultravox.py
34	0	tests/models/decoder_only/language/test_qwen.py
0	0	tests/models/decoder_only/vision_language/mm_processor_kwargs/__init__.py
68	0	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_llava_next.py
181	0	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py
144	0	tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py
2	2	tests/models/decoder_only/vision_language/{ => mm_processor_kwargs}/test_qwen2_vl.py
0	101	tests/models/decoder_only/vision_language/test_blip2.py
0	46	tests/models/decoder_only/vision_language/test_broadcast.py
0	130	tests/models/decoder_only/vision_language/test_chameleon.py
0	139	tests/models/decoder_only/vision_language/test_fuyu.py
0	133	tests/models/decoder_only/vision_language/test_glm4.py
2	288	tests/models/decoder_only/vision_language/test_internvl.py
0	313	tests/models/decoder_only/vision_language/test_llava.py
0	158	tests/models/decoder_only/vision_language/test_llava_image_embeds.py
0	347	tests/models/decoder_only/vision_language/test_llava_next.py
0	226	tests/models/decoder_only/vision_language/test_llava_next_video.py
0	272	tests/models/decoder_only/vision_language/test_llava_onevision.py
0	199	tests/models/decoder_only/vision_language/test_minicpmv.py
594	0	tests/models/decoder_only/vision_language/test_models.py
0	174	tests/models/decoder_only/vision_language/test_paligemma.py
7	178	tests/models/decoder_only/vision_language/test_phi3v.py
0	374	tests/models/decoder_only/vision_language/test_qwen.py
0	0	tests/models/decoder_only/vision_language/vlm_utils/__init__.py
235	0	tests/models/decoder_only/vision_language/vlm_utils/builders.py
157	0	tests/models/decoder_only/vision_language/vlm_utils/case_filtering.py
141	0	tests/models/decoder_only/vision_language/vlm_utils/core.py
102	0	tests/models/decoder_only/vision_language/vlm_utils/custom_inputs.py
338	0	tests/models/decoder_only/vision_language/vlm_utils/model_utils.py
130	0	tests/models/decoder_only/vision_language/vlm_utils/runners.py
187	0	tests/models/decoder_only/vision_language/vlm_utils/types.py
2	0	tests/models/embedding/vision_language/test_llava_next.py
1	1	tests/models/encoder_decoder/vision_language/test_mllama.py
16	8	tests/utils.py
2	1	vllm/utils.py

[211fe91aa] Woosuk Kwon 2024-10-30 [TPU] Correctly profile peak memory usage & Upgrade PyTorch XLA (#9438)
1	1	Dockerfile.tpu
2	2	docs/source/getting_started/tpu-installation.rst
8	7	vllm/worker/tpu_worker.py

[6aa6020f9] Jee Jee Li 2024-10-30 [Misc] Specify minimum pynvml version (#9827)
1	1	requirements-cuda.txt

[ff5ed6e1b] youkaichao 2024-10-29 [torch.compile] rework compile control with piecewise cudagraph (#9715)
3	0	.buildkite/test-pipeline.yaml
0	0	tests/compile/piecewise/__init__.py
4	0	tests/compile/piecewise/piecewise_compilation_config.json
96	0	tests/compile/piecewise/test_simple.py
334	0	tests/compile/piecewise/test_toy_llama.py
1	1	tests/compile/test_full_graph.py
10	8	tests/compile/utils.py
300	84	vllm/compilation/backends.py
154	0	vllm/compilation/config.py
30	0	vllm/compilation/counter.py
9	1	vllm/compilation/decorators.py
1	2	vllm/compilation/levels.py
5	0	vllm/envs.py
2	2	vllm/model_executor/custom_op.py
1	1	vllm/platforms/tpu.py
8	7	vllm/plugins/__init__.py
25	0	vllm/utils.py

[7b0365efe] Russell Bryant 2024-10-30 [Doc] Add the DCO to CONTRIBUTING.md (#9803)
11	1	CONTRIBUTING.md
34	0	DCO

[04a3ae0ac] Yan Ma 2024-10-30 [Bugfix] Fix multi nodes TP+PP for XPU (#8884)
18	0	docs/source/getting_started/xpu-installation.rst
1	1	requirements-xpu.txt
22	0	vllm/distributed/parallel_state.py
11	1	vllm/executor/xpu_executor.py
3	0	vllm/platforms/__init__.py
4	0	vllm/platforms/xpu.py
4	9	vllm/worker/xpu_worker.py

[62fac4b9a] Kevin H. Luu 2024-10-29 [ci/build] Pin CI dependencies version with pip-compile  (#9810)
2	0	Dockerfile.rocm
9	9	requirements-build.txt
37	0	requirements-test.in
560	33	requirements-test.txt

[226688bd6] Michael Goin 2024-10-29 [Bugfix][VLM] Make apply_fp8_linear work with >2D input (#9812)
20	13	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[64cb1cdc3] Lily Liu 2024-10-29 Update README.md (#9819)
1	1	README.md

[1ab6f6b4a] youkaichao 2024-10-29 [core][distributed] fix custom allreduce in pytorch 2.5 (#9815)
13	1	vllm/distributed/device_communicators/custom_all_reduce.py

[bc73e9821] Michael Goin 2024-10-29 [Bugfix] Fix prefix strings for quantized VLMs (#9772)
8	3	vllm/model_executor/model_loader/loader.py
4	1	vllm/model_executor/models/blip2.py
39	19	vllm/model_executor/models/gemma.py
39	17	vllm/model_executor/models/internlm2.py
12	4	vllm/model_executor/models/internlm2_ve.py
4	1	vllm/model_executor/models/internvl.py
5	2	vllm/model_executor/models/llama.py
15	5	vllm/model_executor/models/llava.py
8	2	vllm/model_executor/models/llava_next.py
8	2	vllm/model_executor/models/llava_next_video.py
8	2	vllm/model_executor/models/llava_onevision.py
26	8	vllm/model_executor/models/minicpmv.py
27	7	vllm/model_executor/models/opt.py
5	2	vllm/model_executor/models/paligemma.py
14	5	vllm/model_executor/models/phi3v.py
4	1	vllm/model_executor/models/pixtral.py
37	13	vllm/model_executor/models/qwen2.py
6	2	vllm/model_executor/models/qwen2_vl.py
4	1	vllm/model_executor/models/ultravox.py
15	0	vllm/model_executor/models/utils.py

[8d7724104] Simon Mo 2024-10-29 [Docs] Add notes about Snowflake Meetup (#9814)
12	2	README.md

[882a1ad0d] Will Eaton 2024-10-29 [Model] tool calling support for ibm-granite/granite-20b-functioncalling (#8339)
20	1	docs/source/serving/openai_compatible_server.md
130	0	examples/tool_chat_template_granite_20b_fc.jinja
12	0	tests/tool_use/utils.py
4	3	vllm/entrypoints/openai/tool_parsers/__init__.py
251	0	vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py
4	23	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
35	1	vllm/entrypoints/openai/tool_parsers/utils.py

[67bdf8e52] Joe Runde 2024-10-29 [Bugfix][Frontend] Guard against bad token ids (#9634)
7	1	tests/entrypoints/llm/test_prompt_validation.py
9	9	tests/entrypoints/openai/test_completion.py
15	0	tests/entrypoints/openai/test_prompt_validation.py
12	3	vllm/engine/async_llm_engine.py
36	4	vllm/engine/llm_engine.py
5	0	vllm/transformers_utils/tokenizer.py
5	0	vllm/transformers_utils/tokenizers/mistral.py

[0ad216f57] Kunjan 2024-10-29 [MISC] Set label value to timestamp over 0, to keep track of recent history  (#9777)
6	1	vllm/engine/metrics.py

[7585ec996] Russell Bryant 2024-10-29 [CI/Build] mergify: fix rules for ci/build label (#9804)
8	7	.github/mergify.yml

[ab6f98167] Michael Goin 2024-10-29 [CI][Bugfix] Skip chameleon for transformers 4.46.1 (#9808)
1	1	tests/models/decoder_only/vision_language/test_broadcast.py

[ac3d748db] Junichi Sato 2024-10-30 [Model]  Add LlamaEmbeddingModel as an embedding Implementation of LlamaModel (#9806)
1	0	vllm/model_executor/models/registry.py

[0ce7798f4] yannicks1 2024-10-29 [Misc]: Typo fix: Renaming classes (casualLM -> causalLM) (#9801)
2	2	vllm/model_executor/model_loader/neuron.py
2	2	vllm/model_executor/model_loader/openvino.py

[0f4338715] Sven Seeberg 2024-10-29 [Bugfix] Use host argument to bind to interface (#9798)
1	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/cli_args.py

[08600ddc6] tastelikefeet 2024-10-30 Fix the log to correct guide user to install modelscope (#9793)
1	1	vllm/transformers_utils/__init__.py

[74fc2d77a] 科英 2024-10-30 [Misc] Add metrics for request queue time, forward time, and execute time (#9659)
0	7	vllm/config.py
15	0	vllm/engine/llm_engine.py
52	8	vllm/engine/metrics.py
3	0	vllm/engine/metrics_types.py

[622b7ab95] wangshuai09 2024-10-29 [Hardware] using current_platform.seed_everything (#9785)
3	3	benchmarks/kernels/benchmark_layernorm.py
4	3	benchmarks/kernels/benchmark_moe.py
3	2	benchmarks/kernels/benchmark_paged_attention.py
3	3	benchmarks/kernels/benchmark_quant.py
3	2	benchmarks/kernels/benchmark_rope.py
3	3	tests/kernels/test_activation.py
3	3	tests/kernels/test_attention.py
3	3	tests/kernels/test_awq_triton.py
3	3	tests/kernels/test_blocksparse_attention.py
6	6	tests/kernels/test_cache.py
6	6	tests/kernels/test_causal_conv1d.py
3	3	tests/kernels/test_flash_attn.py
5	5	tests/kernels/test_flashinfer.py
4	4	tests/kernels/test_fp8_quant.py
3	3	tests/kernels/test_gguf.py
5	5	tests/kernels/test_int8_quant.py
2	2	tests/kernels/test_layernorm.py
3	3	tests/kernels/test_mamba_ssm.py
1	2	tests/kernels/test_moe.py
4	4	tests/kernels/test_pos_encoding.py
4	3	tests/kernels/test_prefix_prefill.py
2	2	tests/lora/test_layers.py
5	5	tests/lora/test_punica_sizes.py
6	6	tests/lora/test_punica_variation.py
1	2	vllm/model_executor/utils.py
14	0	vllm/platforms/interface.py
2	19	vllm/utils.py

[09500f7dd] Isotr0py 2024-10-29 [Model] Add BNB quantization support for Mllama (#9720)
31	4	vllm/model_executor/layers/quantization/bitsandbytes.py
16	3	vllm/model_executor/model_loader/loader.py
37	5	vllm/model_executor/models/mllama.py

[ef7865b4f] Zhong Qishuai 2024-10-29 [Frontend] re-enable multi-modality input in the new beam search implementation (#9427)
71	0	tests/entrypoints/openai/test_vision.py
8	1	vllm/beam_search.py
57	31	vllm/engine/protocol.py
2	2	vllm/entrypoints/openai/protocol.py
4	3	vllm/entrypoints/openai/serving_chat.py
7	3	vllm/entrypoints/openai/serving_completion.py
1	0	vllm/sampling_params.py

[eae3d4818] Cyrus Leung 2024-10-29 [Bugfix] Use temporary directory in registry (#9721)
8	3	vllm/model_executor/models/registry.py

[e74f2d448] Cyrus Leung 2024-10-29 [Doc] Specify async engine args in docs (#9726)
2	0	vllm/engine/async_llm_engine.py

[7a4df5f20] Jee Jee Li 2024-10-29 [Model][LoRA]LoRA support added for Qwen (#9622)
3	3	vllm/lora/models.py
98	11	vllm/model_executor/models/qwen.py

[c5d7fb9dd] Russell Bryant 2024-10-28 [Doc] fix third-party model example (#9771)
4	2	docs/source/models/adding_model.rst

[76ed5340f] youkaichao 2024-10-28 [torch.compile] add deepseek v2 compile (#9775)
2	0	vllm/model_executor/models/deepseek_v2.py

[97b61bfae] youkaichao 2024-10-28 [misc] avoid circular import (#9765)
2	1	vllm/sequence.py

[aa0addb39] Yongzao 2024-10-29 Adding "torch compile" annotations to moe models (#9758)
2	0	vllm/model_executor/models/arctic.py
2	0	vllm/model_executor/models/mixtral.py
2	0	vllm/model_executor/models/olmoe.py
2	0	vllm/model_executor/models/phimoe.py

[5f8d8075f] litianjian 2024-10-29 [Model][VLM] Add multi-video support for LLaVA-Onevision (#8905)
48	125	tests/models/decoder_only/vision_language/test_llava_onevision.py
3	1	vllm/model_executor/models/clip.py
66	28	vllm/model_executor/models/llava_onevision.py
3	1	vllm/model_executor/models/siglip.py
3	7	vllm/multimodal/video.py

[8b0e4f2ad] Russell Bryant 2024-10-28 [CI/Build] Adopt Mergify for auto-labeling PRs (#9259)
57	0	.github/mergify.yml

[2adb4409e] Yan Ma 2024-10-28 [Bugfix] Fix ray instance detect issue (#9439)
10	3	vllm/executor/ray_utils.py

[feb92fbe4] Robert Shaw 2024-10-28 Fix beam search eos (#9627)
6	1	vllm/engine/protocol.py

[32176fee7] youkaichao 2024-10-27 [torch.compile] support moe models (#9632)
17	16	benchmarks/kernels/benchmark_moe.py
2	2	tests/compile/test_basic_correctness.py
10	11	tests/kernels/test_awq_marlin.py
3	4	tests/kernels/test_moe.py
24	4	vllm/model_executor/layers/fused_moe/__init__.py
42	9	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
93	7	vllm/model_executor/layers/fused_moe/fused_moe.py
18	11	vllm/model_executor/layers/fused_moe/layer.py
2	5	vllm/model_executor/layers/quantization/awq_marlin.py
2	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	4	vllm/model_executor/layers/quantization/gptq_marlin.py
2	0	vllm/model_executor/models/granitemoe.py

[4e2d95e37] wangshuai09 2024-10-28 [Hardware][ROCM] using current_platform.is_rocm (#9642)
2	2	tests/basic_correctness/test_basic_correctness.py
2	2	tests/compile/utils.py
11	6	tests/kernels/quant_utils.py
13	10	tests/kernels/test_attention.py
2	1	tests/kernels/test_attention_selector.py
4	3	tests/kernels/test_blocksparse_attention.py
40	36	tests/kernels/test_encoder_decoder_attn.py
4	3	tests/kernels/test_moe.py
3	2	tests/lora/test_gemma.py
2	2	tests/lora/test_quant_model.py
5	4	tests/models/decoder_only/vision_language/test_paligemma.py
1	2	tests/models/decoder_only/vision_language/test_phi3v.py
2	2	tests/spec_decode/e2e/test_integration_dist_tp2.py
2	2	tests/utils.py
4	4	vllm/_custom_ops.py
3	3	vllm/attention/ops/blocksparse_attention/interface.py
2	2	vllm/attention/selector.py
25	24	vllm/config.py
2	2	vllm/executor/ray_utils.py
2	2	vllm/model_executor/custom_op.py
3	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	2	vllm/model_executor/layers/quantization/fbgemm_fp8.py
5	5	vllm/model_executor/layers/quantization/fp8.py
3	3	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
2	2	vllm/model_executor/models/exaone.py
2	2	vllm/model_executor/models/granite.py
2	2	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/registry.py
2	2	vllm/model_executor/models/solar.py
1	5	vllm/utils.py
5	4	vllm/worker/model_runner.py

[34a994162] madt2709 2024-10-27 [Bugfix] Fix load config when using bools (#9533)
2	0	tests/data/test_config.yaml
5	1	tests/test_utils.py
1	13	vllm/engine/arg_utils.py
27	8	vllm/utils.py

[e130c40e4] Harry Mellor 2024-10-27 Fix cache management in "Close inactive issues and PRs" actions workflow (#9734)
1	0	.github/workflows/stale.yml

[3cb07a36a] bnellnm 2024-10-27 [Misc] Upgrade to pytorch 2.5 (#9588)
2	2	CMakeLists.txt
1	5	cmake/utils.cmake
1	1	pyproject.toml
1	1	requirements-build.txt
3	3	requirements-cuda.txt
1	1	requirements-openvino.txt
34	12	tests/models/decoder_only/language/test_big_models.py
5	0	vllm/platforms/cuda.py

[8549c8266] youkaichao 2024-10-27 [core] cudagraph output with tensor weak reference (#9724)
24	0	csrc/ops.h
3	0	csrc/torch_bindings.cpp
9	0	vllm/utils.py
14	28	vllm/worker/model_runner.py

[67a6882da] 科英 2024-10-27 [Misc] SpecDecodeWorker supports profiling (#9719)
8	0	vllm/spec_decode/spec_decode_worker.py

[6650e6a93] kakao-kevin-us 2024-10-27 [Model] Add classification Task with Qwen2ForSequenceClassification  (#9704)
22	0	docs/source/models/supported_models.rst
19	0	tests/conftest.py
53	0	tests/models/embedding/language/test_cls_models.py
8	1	vllm/model_executor/layers/pooler.py
107	0	vllm/model_executor/models/qwen2_cls.py
2	0	vllm/model_executor/models/registry.py

[07e981fdf] Vasiliy Alekseev 2024-10-26 [Frontend] Bad words sampling parameter (#9717)
185	0	tests/samplers/test_no_bad_words.py
11	2	vllm/engine/llm_engine.py
119	0	vllm/logits_process.py
2	1	vllm/model_executor/guided_decoding/__init__.py
2	1	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
20	12	vllm/sampling_params.py

[55137e8ee] ErkinSagiroglu 2024-10-26 Fix: MI100 Support By Bypassing Custom Paged Attention (#9560)
6	2	vllm/attention/backends/rocm_flash_attn.py

[5cbdccd15] Mengqing Cao 2024-10-26 [Hardware][openvino] is_openvino --> current_platform.is_openvino (#9716)
2	1	tests/kernels/test_attention_selector.py
2	2	vllm/attention/selector.py
2	2	vllm/config.py
7	13	vllm/executor/openvino_executor.py
2	2	vllm/model_executor/model_loader/openvino.py
10	0	vllm/platforms/__init__.py
4	0	vllm/platforms/interface.py
31	0	vllm/platforms/openvino.py
1	10	vllm/utils.py
8	8	vllm/worker/openvino_worker.py

[067e77f9a] Sam Stoelinga 2024-10-25 [Bugfix] Steaming continuous_usage_stats default to False (#9709)
1	1	vllm/entrypoints/openai/protocol.py

[6567e1372] Travis Johnson 2024-10-25 [Bugfix] Fix crash with llama 3.2 vision models and guided decoding (#9631)
11	3	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[228cfbd03] Rafael Vasquez 2024-10-25 [Doc] Improve quickstart documentation (#9256)
52	46	docs/source/getting_started/quickstart.rst

[ca0d92227] Michael Goin 2024-10-25 [Bugfix] Fix compressed_tensors_moe bad config.strategy (#9677)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py

[9645b9f64] Woosuk Kwon 2024-10-24 [V1] Support sliding window attention (#9679)
4	8	vllm/v1/attention/backends/flash_attn.py

[a6f372186] Will Johnson 2024-10-25 [Model] add a lora module for granite 3.0 MoE models (#9673)
1	0	vllm/model_executor/models/granitemoe.py

[9f7b4ba86] Kevin H. Luu 2024-10-24 [ci/Build] Skip Chameleon for transformers 4.46.0 on broadcast test #9675 (#9676)
4	0	tests/models/decoder_only/vision_language/test_broadcast.py

[c91ed47c4] Michael Goin 2024-10-24 [Bugfix] Remove xformers requirement for Pixtral (#9597)
46	19	vllm/model_executor/models/pixtral.py

[59449095a] Charlie Fu 2024-10-24 [Performance][Kernel] Fused_moe Performance Improvement (#9384)
1	1	CMakeLists.txt
83	15	csrc/{moe_align_block_size_kernels.cu => moe/moe_align_sum_kernels.cu}
7	0	csrc/moe/moe_ops.h
14	0	csrc/moe/torch_bindings.cpp
0	5	csrc/ops.h
0	9	csrc/torch_bindings.cpp
4	2	tests/kernels/test_moe.py
7	3	vllm/_custom_ops.py
2	3	vllm/model_executor/layers/fused_moe/fused_moe.py

[e26d37a18] Michael Goin 2024-10-24 [Log][Bugfix] Fix default value check for `image_url.detail` (#9663)
2	1	vllm/entrypoints/chat_utils.py

[722d46edb] Alex Brooks 2024-10-24 [Model] Compute Llava Next Max Tokens / Dummy Data From Gridpoints (#9650)
65	1	tests/models/decoder_only/vision_language/test_llava_next.py
28	13	vllm/model_executor/models/llava_next.py

[c866e0079] Cyrus Leung 2024-10-25 [CI/Build] Fix VLM test failures when using transformers v4.46 (#9666)
9	7	tests/conftest.py
5	0	tests/models/decoder_only/vision_language/test_chameleon.py
2	2	tests/models/decoder_only/vision_language/test_minicpmv.py
12	3	tests/models/decoder_only/vision_language/test_paligemma.py

[d27cfbf79] Yongzao 2024-10-25 [torch.compile] Adding torch compile annotations to some models (#9641)
2	1	tests/distributed/test_pipeline_parallel.py
2	0	vllm/model_executor/models/opt.py
8	10	vllm/model_executor/models/orion.py
2	0	vllm/model_executor/models/persimmon.py
2	0	vllm/model_executor/models/solar.py
2	0	vllm/model_executor/models/starcoder2.py
3	0	vllm/model_executor/models/xverse.py

[de662d32b] Harry Mellor 2024-10-24 Increase operation per run limit for "Close inactive issues and PRs" workflow (#9661)
4	0	.github/workflows/stale.yml

[f58454968] litianjian 2024-10-24 [Bugfix]Disable the post_norm layer of the vision encoder for LLaVA models (#9653)
2	1	vllm/model_executor/models/llava.py
2	1	vllm/model_executor/models/llava_next.py
2	1	vllm/model_executor/models/llava_next_video.py
2	1	vllm/model_executor/models/llava_onevision.py

[b979143d5] Cyrus Leung 2024-10-24 [Doc] Move additional tips/notes to the top (#9647)
39	40	docs/source/models/supported_models.rst

[ad6f78053] Yongzao 2024-10-24 [torch.compile] expanding support and fix allgather compilation (#9637)
6	1	vllm/distributed/parallel_state.py
2	0	vllm/model_executor/models/gpt_bigcode.py
2	0	vllm/model_executor/models/gpt_j.py
2	0	vllm/model_executor/models/gpt_neox.py
2	0	vllm/model_executor/models/granite.py
2	0	vllm/model_executor/models/internlm2.py

[295a061fb] Jee Jee Li 2024-10-24 [Kernel] add kernel for FATReLU (#9610)
42	0	csrc/activation_kernels.cu
3	0	csrc/ops.h
4	0	csrc/torch_bindings.cpp
16	7	tests/kernels/test_activation.py
6	0	vllm/_custom_ops.py
7	1	vllm/model_executor/layers/activation.py

[8a02cd045] Yongzao 2024-10-24 [torch.compile] Adding torch compile annotations to some models (#9639)
1	1	docs/source/models/supported_models.rst
1	1	tests/distributed/test_pipeline_parallel.py
3	1	vllm/model_executor/models/jais.py
2	0	vllm/model_executor/models/minicpm.py
2	0	vllm/model_executor/models/mpt.py
2	0	vllm/model_executor/models/nemotron.py
2	0	vllm/model_executor/models/olmo.py

[4fdc581f9] youkaichao 2024-10-24 [core] simplify seq group code (#9569)
0	153	tests/core/test_chunked_prefill_scheduler.py
1	203	tests/core/test_scheduler.py
1	1	vllm/core/scheduler.py
19	21	vllm/engine/llm_engine.py
22	105	vllm/engine/output_processor/single_step.py
19	83	vllm/sequence.py

[3770071eb] Woosuk Kwon 2024-10-23 [V1][Bugfix] Clean up requests when aborted (#9629)
12	3	vllm/v1/engine/llm_engine.py

[836e8ef6e] Cyrus Leung 2024-10-24 [Bugfix] Fix PP for ChatGLM and Molmo (#9422)
1	1	docs/source/models/supported_models.rst
18	19	tests/distributed/test_pipeline_parallel.py
75	54	vllm/model_executor/models/chatglm.py
46	27	vllm/model_executor/models/molmo.py
2	1	vllm/model_executor/models/qwen2_rm.py
13	10	vllm/model_executor/models/qwen2_vl.py
42	12	vllm/model_executor/models/utils.py

[056a68c7d] Yan Ma 2024-10-24 [XPU] avoid triton import for xpu (#9440)
7	5	vllm/triton_utils/importing.py

[33bab4106] Vinay R Damodaran 2024-10-24 [Bugfix]: Make chat content text allow type content (#9358)
17	0	docs/source/serving/openai_compatible_server.md
1	0	tests/entrypoints/openai/test_serving_chat.py
47	1	tests/entrypoints/test_chat_utils.py
2	0	vllm/config.py
10	0	vllm/engine/arg_utils.py
2	1	vllm/engine/llm_engine.py
23	8	vllm/entrypoints/chat_utils.py
5	2	vllm/entrypoints/openai/serving_chat.py

[b7df53cd4] Michael Goin 2024-10-23 [Bugfix] Use "vision_model" prefix for MllamaVisionModel (#9628)
2	1	vllm/model_executor/models/mllama.py

[bb01f2915] Michael Goin 2024-10-23 [Bugfix][Model] Fix Mllama SDPA illegal memory access for batched multi-image (#9626)
5	3	vllm/model_executor/models/mllama.py

[b548d7a5f] Russell Bryant 2024-10-23 [CI/Build] Add bot to close stale issues and PRs (#9436)
47	0	.github/workflows/stale.yml

[fc6c27462] Yunfei Chu 2024-10-24 [Model] Add Qwen2-Audio model support (#9248)
6	0	docs/source/models/supported_models.rst
38	16	examples/offline_inference_audio_language.py
1	0	tests/distributed/test_pipeline_parallel.py
4	1	vllm/entrypoints/chat_utils.py
462	0	vllm/model_executor/models/qwen2_audio.py
1	0	vllm/model_executor/models/registry.py
3	0	vllm/model_executor/models/ultravox.py

[150b77908] Alex Brooks 2024-10-23 [Frontend] Enable Online Multi-image Support for MLlama (#9393)
176	0	tests/entrypoints/test_chat_utils.py
54	37	vllm/entrypoints/chat_utils.py

[9013e24f7] Yongzao 2024-10-24 [torch.compile] Adding torch compile annotations to some models (#9614)
2	0	vllm/model_executor/models/baichuan.py
2	0	vllm/model_executor/models/bloom.py
2	0	vllm/model_executor/models/commandr.py
2	0	vllm/model_executor/models/exaone.py
2	0	vllm/model_executor/models/gemma.py
2	0	vllm/model_executor/models/gpt2.py

[fd0e2cfdb] Michael Goin 2024-10-23 [Misc] Separate total and output tokens in benchmark_throughput.py (#8914)
3	1	benchmarks/benchmark_throughput.py

[e5ac6a419] Tyler Michael Smith 2024-10-23 [Bugfix] Fix divide by zero when serving Mamba models (#9617)
2	2	vllm/engine/llm_engine.py

[dbdd3b5e5] youkaichao 2024-10-23 [misc] comment to avoid future confusion about baichuan (#9620)
6	2	vllm/model_executor/models/baichuan.py
4	2	vllm/model_executor/models/registry.py

[e7116c017] Cyrus Leung 2024-10-23 [Bugfix] Fix `_init_vision_model` in NVLM_D model (#9611)
28	9	vllm/model_executor/models/nvlm_d.py

[31a08f5bd] Alex Brooks 2024-10-23 [Model] Add min_pixels / max_pixels to Qwen2VL as mm_processor_kwargs (#9612)
5	0	examples/offline_inference_vision_language.py
160	0	tests/models/decoder_only/vision_language/test_qwen2_vl.py
71	18	vllm/model_executor/models/qwen2_vl.py

[c18e1a341] Cyrus Leung 2024-10-23 [VLM] Enable overriding whether post layernorm is used in vision encoder + fix quant args (#9217)
16	4	vllm/model_executor/layers/quantization/awq.py
59	28	vllm/model_executor/models/blip.py
1	1	vllm/model_executor/models/blip2.py
73	31	vllm/model_executor/models/clip.py
41	10	vllm/model_executor/models/idefics2_vision_model.py
30	11	vllm/model_executor/models/intern_vit.py
35	6	vllm/model_executor/models/internvl.py
25	7	vllm/model_executor/models/llava.py
2	28	vllm/model_executor/models/llava_next.py
2	27	vllm/model_executor/models/llava_next_video.py
2	27	vllm/model_executor/models/llava_onevision.py
25	8	vllm/model_executor/models/minicpmv.py
89	31	vllm/model_executor/models/mllama.py
5	0	vllm/model_executor/models/nvlm_d.py
2	1	vllm/model_executor/models/paligemma.py
10	5	vllm/model_executor/models/phi3v.py
79	11	vllm/model_executor/models/pixtral.py
55	17	vllm/model_executor/models/siglip.py

[3ff57ebfc] Isotr0py 2024-10-23 [Model] Initialize Florence-2 language backbone support (#9555)
44	0	examples/florence2_inference.py
20	8	tests/conftest.py
102	0	tests/models/encoder_decoder/vision_language/test_florence2.py
261	0	vllm/model_executor/models/florence2.py
1	0	vllm/model_executor/models/registry.py

[2394962d7] Mengqing Cao 2024-10-23 [Hardware][XPU] using current_platform.is_xpu (#9605)
3	3	vllm/attention/selector.py
2	2	vllm/config.py
2	2	vllm/executor/ray_utils.py
2	2	vllm/model_executor/custom_op.py
3	26	vllm/utils.py
4	3	vllm/worker/xpu_worker.py

[51c24c973] Luka Govedič 2024-10-23 [Build] Fix `FetchContent` multiple build issue (#9596)
6	4	CMakeLists.txt
8	0	setup.py

[831540cf0] Cyrus Leung 2024-10-23 [Model] Support E5-V (#9576)
14	0	docs/source/models/supported_models.rst
3	3	examples/offline_inference_vision_language.py
169	21	examples/offline_inference_vision_language_embedding.py
4	3	examples/offline_inference_vision_language_multi_image.py
36	24	tests/conftest.py
2	1	tests/models/embedding/utils.py
135	0	tests/models/embedding/vision_language/test_llava_next.py
77	16	tests/models/embedding/vision_language/test_phi3v.py
22	11	vllm/model_executor/models/llava_next.py
0	2	vllm/model_executor/models/phi3v.py
1	0	vllm/model_executor/models/registry.py
69	9	vllm/model_executor/models/utils.py

[29061ed9d] Flex Wang 2024-10-22 [Misc] Add an env var VLLM_LOGGING_PREFIX, if set, it will be prepend to all logging messages (#9590)
5	0	vllm/envs.py
3	1	vllm/logger.py

[65050a40e] Chen Zhang 2024-10-22 [Bugfix] Generate exactly input_len tokens in benchmark_throughput (#9592)
10	1	benchmarks/benchmark_throughput.py

[208cb34c8] Seth Kimmel 2024-10-22 [Doc]: Update tensorizer docs to include vllm[tensorizer] (#7889)
4	1	docs/source/serving/tensorizer.rst

[b17046e29] yulei 2024-10-23 [BugFix] Fix metrics error for --num-scheduler-steps > 1 (#8234)
39	0	tests/metrics/test_metrics.py
9	0	vllm/engine/llm_engine.py

[d1e824087] Lucas Wilkinson 2024-10-22 [Bugfix] Fix spurious "No compiled cutlass_scaled_mm ..." for W8A8 on Turing (#9487)
2	2	CMakeLists.txt
5	3	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[cb6fdaa0a] Jeremy Arnold 2024-10-22 [Misc] Make benchmarks use EngineArgs (#9529)
7	148	benchmarks/benchmark_latency.py
7	17	benchmarks/benchmark_prefix_caching.py
9	125	benchmarks/benchmark_prioritization.py
15	222	benchmarks/benchmark_throughput.py

[23b899a8e] Aurick Qiao 2024-10-22 [Bugfix] fix detokenizer shallow copy (#5919)
1	1	vllm/transformers_utils/detokenizer.py

[17c79f3c3] youkaichao 2024-10-22 [torch.compile] auto infer dynamic_arg_dims from type annotation (#9589)
63	5	vllm/compilation/decorators.py
1	7	vllm/model_executor/models/gemma2.py
1	7	vllm/model_executor/models/llama.py

[cd5601ac3] Ronen Schaffer 2024-10-22 [BugFix] Prevent exporting duplicate OpenTelemetry spans (#9017)
26	4	tests/tracing/test_tracing.py
10	3	vllm/engine/llm_engine.py

[434984e66] Yuhong Guo 2024-10-23 [Frontend] Support custom request_id from request (#9550)
6	0	vllm/entrypoints/openai/protocol.py
2	2	vllm/entrypoints/openai/serving_chat.py

[32a1ee74a] Yuan 2024-10-22 [Hardware][Intel CPU][DOC] Update docs for CPU backend (#6212)
22	1	docs/source/getting_started/cpu-installation.rst
1	0	docs/source/index.rst
142	0	docs/source/serving/deploying_with_nginx.rst

[08075c344] gopalsarda 2024-10-22 [Bugfix] Eagle: change config name for fc bias (#9580)
1	1	vllm/model_executor/models/eagle.py

[bb392ea2d] Isotr0py 2024-10-23 [Model][VLM] Initialize support for Mono-InternVL model (#9528)
1	1	docs/source/models/supported_models.rst
13	8	tests/models/decoder_only/vision_language/test_internvl.py
31	0	vllm/model_executor/models/intern_vit.py
166	0	vllm/model_executor/models/internlm2_ve.py
42	19	vllm/model_executor/models/internvl.py
1	0	vllm/model_executor/models/registry.py

[9dbcce84a] xendo 2024-10-22 [Neuron] [Bugfix] Fix neuron startup (#9374)
2	1	vllm/_custom_ops.py
7	6	vllm/config.py
10	0	vllm/platforms/__init__.py
4	0	vllm/platforms/interface.py
9	0	vllm/platforms/neuron.py
4	1	vllm/triton_utils/importing.py
1	10	vllm/utils.py

[a48e3ec05] Jee Jee Li 2024-10-22 [CI/Build][LoRA] Temporarily fix long context failure issue (#9579)
20	11	tests/lora/test_long_context.py

[6c5af09b3] Woosuk Kwon 2024-10-22 [V1] Implement vLLM V1 [1/N] (#9289)
8	0	vllm/attention/selector.py
17	10	vllm/engine/multiprocessing/engine.py
6	1	vllm/entrypoints/llm.py
5	0	vllm/envs.py
6	4	vllm/model_executor/layers/logits_processor.py
3	165	vllm/transformers_utils/detokenizer.py
167	0	vllm/transformers_utils/detokenizer_utils.py
0	0	vllm/v1/attention/__init__.py
0	0	vllm/v1/attention/backends/__init__.py
241	0	vllm/v1/attention/backends/flash_attn.py
0	0	vllm/v1/core/__init__.py
108	0	vllm/v1/core/kv_cache_manager.py
412	0	vllm/v1/core/scheduler.py
0	0	vllm/v1/engine/__init__.py
523	0	vllm/v1/engine/llm_engine.py
0	0	vllm/v1/executor/__init__.py
100	0	vllm/v1/executor/gpu_executor.py
37	0	vllm/v1/outputs.py
92	0	vllm/v1/request.py
0	0	vllm/v1/sample/__init__.py
22	0	vllm/v1/sample/metadata.py
161	0	vllm/v1/sample/sampler.py
0	0	vllm/v1/tokenizer/__init__.py
215	0	vllm/v1/tokenizer/detokenizer.py
0	0	vllm/v1/worker/__init__.py
690	0	vllm/v1/worker/gpu_model_runner.py
245	0	vllm/v1/worker/gpu_worker.py

[3ddbe2550] wangshuai09 2024-10-22 [Hardware][CPU] using current_platform.is_cpu (#9536)
4	2	tests/conftest.py
3	3	tests/encoder_decoder/test_e2e_correctness.py
2	1	tests/kernels/test_attention_selector.py
2	2	tests/models/decoder_only/language/test_phimoe.py
3	3	tests/models/decoder_only/vision_language/test_fuyu.py
3	3	tests/models/decoder_only/vision_language/test_internvl.py
3	2	tests/models/decoder_only/vision_language/test_phi3v.py
4	4	tests/models/utils.py
6	5	tests/worker/test_encoder_decoder_model_runner.py
4	4	vllm/attention/backends/torch_sdpa.py
10	10	vllm/attention/ops/blocksparse_attention/interface.py
3	3	vllm/attention/selector.py
3	3	vllm/distributed/parallel_state.py
2	2	vllm/model_executor/custom_op.py
4	4	vllm/model_executor/models/qwen2_vl.py
3	3	vllm/model_executor/models/utils.py
1	10	vllm/utils.py

[0d02747f2] chenqianfzh 2024-10-22 support TP in qwen2 bnb (#9574)
14	0	vllm/model_executor/models/qwen2.py

[f7db5f0fa] Rafael Vasquez 2024-10-22 [Doc] Use shell code-blocks and fix section headers (#9508)
4	4	docs/source/getting_started/debugging.rst
17	17	docs/source/getting_started/installation.rst
2	2	docs/source/models/vlm.rst

[ca30c3c84] Kuntai Du 2024-10-21 [Core] Remove evictor_v1 (#9572)
1	1	vllm/core/block/prefix_caching_block.py
0	0	vllm/core/{evictor_v2.py => evictor.py}
0	106	vllm/core/evictor_v1.py

[c0292211c] Wallas Henrique 2024-10-22 [CI/Build] Replaced some models on tests for smaller ones (#9570)
1	1	tests/basic_correctness/test_basic_correctness.py
1	1	tests/basic_correctness/test_chunked_prefill.py
2	2	tests/basic_correctness/test_cpu_offload.py
1	2	tests/compile/test_basic_correctness.py
2	2	tests/entrypoints/llm/test_chat.py
0	3	tests/entrypoints/openai/test_chat.py
1	1	tests/entrypoints/openai/test_shutdown.py
7	3	tests/test_sharded_state_loader.py

[74692421f] Falko1 2024-10-22 [Bugfix]: phi.py get rope_theta from config file (#9503)
3	2	vllm/model_executor/models/phi.py

[29acd2c34] ngrozae 2024-10-22 [Bugfix][OpenVINO] fix_dockerfile_openvino (#9552)
4	4	Dockerfile.openvino

[f085995a7] Cyrus Leung 2024-10-22 [CI/Build] Remove unnecessary `fork_new_process` (#9484)
1	1	tests/utils.py

[b72990113] Travis Johnson 2024-10-21 [Bugfix]: serialize config by value for --trust-remote-code (#6751)
35	28	tests/distributed/test_pipeline_parallel.py
4	0	vllm/engine/arg_utils.py
62	0	vllm/transformers_utils/config.py
2	0	vllm/utils.py

[76a5e1327] youkaichao 2024-10-21 [core] move parallel sampling out from vllm core (#9302)
34	0	tests/entrypoints/openai/test_completion.py
42	10	vllm/engine/llm_engine.py
26	17	vllm/outputs.py
120	2	vllm/sequence.py

[ef7faad1b] Joe Runde 2024-10-21 :bug: Fixup more test failures from memory profiling (#9563)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3.2-1B-Instruct-INT8-compressed-tensors.yaml
1	1	.buildkite/lm-eval-harness/configs/models-small.txt
1	0	tests/lora/test_minicpmv.py

[575dcebe9] Kuntai Du 2024-10-21 [CI] Make format checker error message more user-friendly by using emoji (#9564)
20	4	format.sh

[711f3a780] Wallas Henrique 2024-10-21 [Frontend] Don't log duplicate error stacktrace for every request in the batch (#9023)
41	10	tests/mq_llm_engine/test_error_handling.py
12	0	vllm/engine/multiprocessing/client.py

[15713e3b7] Nick Hill 2024-10-21 [BugFix] Update draft model TP size check to allow matching target TP size (#9394)
3	3	vllm/config.py

[d621c43df] youkaichao 2024-10-21 [doc] fix format (#9562)
1	1	docs/source/getting_started/installation.rst

[9d9186be9] Nick Hill 2024-10-21 [Frontend] Reduce frequency of client cancellation checking (#7959)
38	19	vllm/utils.py

[5241aa149] Michael Goin 2024-10-21 [Model][Bugfix] Fix batching with multi-image in PixtralHF (#9518)
48	12	vllm/model_executor/models/llava.py
6	5	vllm/model_executor/models/pixtral.py

[ec6bd6c4c] Varad Ahirwadkar 2024-10-21 [BugFix] Use correct python3 binary in Docker.ppc64le entrypoint (#9492)
1	1	Dockerfile.ppc64le

[8ca895484] yudian0504 2024-10-22 [Bugfix][Misc]: fix graph capture for decoder (#9549)
1	1	vllm/worker/model_runner.py

[f6b97293a] Dhia Eddine Rhaiem 2024-10-21 [Model] FalconMamba Support (#9325)
5	0	docs/source/models/supported_models.rst
1	1	tests/models/decoder_only/language/test_mamba.py
0	1	vllm/model_executor/layers/layernorm.py
28	10	vllm/model_executor/models/mamba.py
1	0	vllm/model_executor/models/registry.py

[496e991da] Thomas Parnell 2024-10-21 [Doc] Consistent naming of attention backends (#9498)
1	1	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/flashinfer.py
1	1	vllm/attention/backends/ipex_attn.py
1	1	vllm/attention/backends/openvino.py
4	0	vllm/attention/backends/pallas.py
1	1	vllm/attention/backends/placeholder_attn.py
1	1	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/attention/backends/torch_sdpa.py
6	6	vllm/attention/backends/utils.py
1	1	vllm/attention/backends/xformers.py
1	1	vllm/spec_decode/draft_model_runner.py
1	1	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/worker/model_runner.py
2	2	vllm/worker/multi_step_model_runner.py

[696b01af8] Cyrus Leung 2024-10-21 [CI/Build] Split up decoder-only LM tests (#9488)
11	2	.buildkite/test-pipeline.yaml
7	3	tests/models/decoder_only/language/test_big_models.py
0	52	tests/models/decoder_only/language/test_danube3_4b.py

[855e0e6f9] Andy Dai 2024-10-20 [Frontend][Misc] Goodput metric support (#9338)
91	2	benchmarks/benchmark_serving.py

[4fa3e3334] Chen Zhang 2024-10-20 [Kernel] Support sliding window in flash attention backend (#9403)
15	20	tests/kernels/test_attention_selector.py
16	13	tests/kernels/test_flash_attn.py
5	8	vllm/attention/backends/flash_attn.py
3	4	vllm/attention/layer.py
2	8	vllm/attention/selector.py
0	1	vllm/worker/cache_engine.py
0	1	vllm/worker/cpu_model_runner.py
0	1	vllm/worker/cpu_worker.py
0	1	vllm/worker/model_runner.py
0	1	vllm/worker/openvino_model_runner.py
0	1	vllm/worker/openvino_worker.py
0	1	vllm/worker/tpu_model_runner.py
0	1	vllm/worker/xpu_model_runner.py

[962d2c634] Michael Goin 2024-10-20 [Model][Pixtral] Use memory_efficient_attention for PixtralHFVision (#9520)
21	41	vllm/model_executor/models/pixtral.py

[5b59fe0f0] Chen Zhang 2024-10-19 [Bugfix] Pass json-schema to GuidedDecodingParams and make test stronger (#9530)
18	4	tests/entrypoints/openai/test_chat.py
11	5	vllm/entrypoints/openai/protocol.py

[8e3e7f271] Michael Goin 2024-10-19 [Model][Pixtral] Optimizations for input_processor_for_pixtral_hf (#9514)
41	40	vllm/model_executor/models/pixtral.py

[263d8ee15] Cyrus Leung 2024-10-19 [Bugfix] Fix missing task for speculative decoding (#9524)
14	9	vllm/config.py

[c5eea3c8b] Yue Zhang 2024-10-18 [Frontend] Support simpler image input format (#9478)
26	0	tests/entrypoints/test_chat_utils.py
114	25	vllm/entrypoints/chat_utils.py

[85dc92fc9] Russell Bryant 2024-10-19 [CI/Build] Configure matcher for actionlint workflow (#9511)
1	0	.github/workflows/actionlint.yml

[dfd951ed9] Russell Bryant 2024-10-19 [CI/Build] Add error matching for ruff output (#9513)
17	0	.github/workflows/matchers/ruff.json
2	1	.github/workflows/ruff.yml

[82c25151e] Joe Runde 2024-10-18 [Doc] update gpu-memory-utilization flag docs (#9507)
5	1	vllm/engine/arg_utils.py

[1325872ec] Nick Hill 2024-10-19 [Frontend] Avoid creating guided decoding LogitsProcessor unnecessarily (#9521)
5	2	vllm/sampling_params.py

[380e18639] Joe Runde 2024-10-18 :bug: fix torch memory profiling (#9516)
1	2	tests/quantization/test_bitsandbytes.py
6	5	tests/worker/test_profile.py
7	4	vllm/worker/worker.py

[337ed7667] sasha0552 2024-10-19 [Bugfix] Fix offline mode when using `mistral_common` (#9457)
31	25	tests/entrypoints/offline_mode/test_offline_mode.py
31	3	vllm/transformers_utils/tokenizers/mistral.py

[0c9a5258f] Thomas Parnell 2024-10-19 [Kernel] Add env variable to force flashinfer backend to enable tensor cores (#9497)
5	2	vllm/attention/backends/flashinfer.py
6	0	vllm/envs.py

[d11bf435a] Cody Yu 2024-10-18 [MISC] Consolidate cleanup() and refactor offline_inference_with_prefix.py (#9510)
12	7	examples/offline_inference_with_prefix.py
2	2	tests/async_engine/test_async_llm_engine.py
5	18	tests/conftest.py
2	3	tests/core/block/e2e/conftest.py
2	3	tests/entrypoints/llm/test_encode.py
2	3	tests/entrypoints/llm/test_generate.py
2	3	tests/entrypoints/llm/test_generate_multiple_loras.py
2	3	tests/entrypoints/llm/test_guided_generate.py
7	2	tests/entrypoints/llm/test_lazy_outlines.py
2	3	tests/entrypoints/offline_mode/test_offline_mode.py
6	20	tests/lora/conftest.py
4	5	tests/lora/test_baichuan.py
4	5	tests/lora/test_llama.py
4	5	tests/lora/test_quant_model.py
2	3	tests/metrics/test_metrics.py
4	3	tests/models/decoder_only/vision_language/test_intern_vit.py
3	3	tests/prefix_caching/test_disable_sliding_window.py
2	2	tests/spec_decode/e2e/conftest.py
2	11	tests/tensorizer_loader/conftest.py
15	1	vllm/distributed/parallel_state.py

[9bb10a7d2] Kunjan 2024-10-18 [MISC] Add lora requests to metrics (#9477)
23	1	vllm/engine/llm_engine.py
28	1	vllm/engine/metrics.py
3	0	vllm/engine/metrics_types.py

[3921a2f29] Michael Goin 2024-10-18 [Model] Support Pixtral models in the HF Transformers format (#9036)
1	1	docs/source/models/supported_models.rst
17	0	examples/offline_inference_vision_language.py
2	0	vllm/model_executor/layers/activation.py
70	4	vllm/model_executor/models/llava.py
407	3	vllm/model_executor/models/pixtral.py
2	4	vllm/model_executor/models/qwen2_vl.py
4	0	vllm/transformers_utils/processor.py

[67a7e5ef3] Russell Bryant 2024-10-18 [CI/Build] Add error matching config for mypy (#9512)
16	0	.github/workflows/matchers/mypy.json
2	1	.github/workflows/mypy.yaml
4	0	tools/mypy.sh

[051eaf6db] Cyrus Leung 2024-10-19 [Model] Add user-configurable task for models that support both generation and embedding (#9424)
8	0	docs/source/models/supported_models.rst
2	2	docs/source/models/vlm.rst
1	0	examples/offline_inference_vision_language_embedding.py
2	2	examples/openai_api_client_for_multimodal.py
3	1	tests/conftest.py
13	2	tests/core/test_chunked_prefill_scheduler.py
32	24	tests/core/test_scheduler.py
6	1	tests/core/test_scheduler_encoder_decoder.py
19	4	tests/distributed/test_pipeline_parallel.py
92	0	tests/entrypoints/llm/test_chat.py
0	88	tests/entrypoints/llm/test_generate.py
22	0	tests/entrypoints/llm/test_init.py
1	1	tests/entrypoints/openai/test_serving_chat.py
2	0	tests/entrypoints/openai/test_vision.py
2	1	tests/entrypoints/test_chat_utils.py
3	2	tests/lora/test_worker.py
1	0	tests/models/decoder_only/vision_language/test_phi3v.py
1	0	tests/models/embedding/vision_language/test_phi3v.py
4	2	tests/models/utils.py
4	0	tests/multimodal/test_mapper.py
6	1	tests/multimodal/test_processor_kwargs.py
2	1	tests/quantization/test_configs.py
50	7	tests/test_config.py
6	6	tests/test_utils.py
4	4	tests/utils.py
54	23	vllm/config.py
1	1	vllm/core/scheduler.py
14	3	vllm/engine/arg_utils.py
2	5	vllm/engine/llm_engine.py
44	12	vllm/entrypoints/llm.py
2	1	vllm/entrypoints/openai/serving_embedding.py
47	3	vllm/utils.py
1	4	vllm/worker/worker.py

[7dbe738d6] Russell Bryant 2024-10-18 [Misc] benchmark: Add option to set max concurrency (#9390)
37	3	benchmarks/benchmark_serving.py

[ae8b633ba] Tyler Michael Smith 2024-10-18 [Bugfix] Fix offline_inference_with_prefix.py (#9505)
4	2	examples/offline_inference_with_prefix.py

[1bbbcc0b1] Cyrus Leung 2024-10-19 [CI/Build] Fix lint errors in mistral tokenizer (#9504)
3	3	vllm/transformers_utils/tokenizers/mistral.py

[25aeb7d4c] Nick Hill 2024-10-18 [BugFix] Fix and simplify completion API usage streaming (#9475)
61	62	vllm/entrypoints/openai/serving_completion.py

[d2b1bf55e] tomeras91 2024-10-18 [Frontend][Feature] Add jamba tool parser (#9154)
14	6	docs/source/serving/openai_compatible_server.md
275	0	tests/tool_use/test_jamba_tool_parser.py
3	1	vllm/entrypoints/openai/tool_parsers/__init__.py
2	1	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
300	0	vllm/entrypoints/openai/tool_parsers/jamba_tool_parser.py
1	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[1ffc8a736] Nick Hill 2024-10-18 [BugFix] Typing fixes to RequestOutput.prompt and beam search (#9473)
5	2	vllm/beam_search.py
19	10	vllm/engine/protocol.py
1	0	vllm/entrypoints/llm.py
1	2	vllm/outputs.py

[944dd8eda] Russell Bryant 2024-10-18 [CI/Build] Use commit hash references for github actions (#9430)
1	1	.github/workflows/add_label_automerge.yml
3	3	.github/workflows/clang-format.yml
2	2	.github/workflows/mypy.yaml
6	6	.github/workflows/publish.yml
1	1	.github/workflows/reminder_comment.yml
2	2	.github/workflows/ruff.yml
2	2	.github/workflows/yapf.yml

[154a8ae88] Haoyu Wang 2024-10-18 [Qwen2.5] Support bnb quant for Qwen2.5 (#9467)
8	0	vllm/model_executor/models/qwen2.py

[de4008e2a] Joe Runde 2024-10-17 [Bugfix][Core] Use torch.cuda.memory_stats() to profile peak memory usage (#9352)
3	1	tests/entrypoints/llm/test_lazy_outlines.py
1	1	tests/entrypoints/offline_mode/test_offline_mode.py
69	0	tests/worker/test_profile.py
49	15	vllm/worker/worker.py

[48138a841] Dipika Sikka 2024-10-17 [BugFix] Stop silent failures on compressed-tensors parsing (#9381)
1	1	requirements-common.txt
22	12	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[343f8e090] Robert Shaw 2024-10-17 Support `BERTModel` (first `encoder-only` embedding model) (#9056)
12	2	tests/models/embedding/language/test_embedding.py
5	2	vllm/attention/backends/abstract.py
50	9	vllm/attention/backends/xformers.py
10	2	vllm/model_executor/layers/pooler.py
419	0	vllm/model_executor/models/bert.py
1	0	vllm/model_executor/models/registry.py

[bb76538bb] Shashwat Srijan 2024-10-17 [Hardwware][Neuron] Simplify model load for transformers-neuronx library (#9380)
1	30	vllm/model_executor/model_loader/neuron.py

[d615b5c9f] sasha0552 2024-10-17 [Bugfix] Print warnings related to `mistral_common` tokenizer only once (#9468)
4	3	vllm/entrypoints/chat_utils.py

[d65049daa] Kai Wu 2024-10-17 [Bugfix] Add random_seed to sample_hf_requests in benchmark_serving script (#9013)
4	2	benchmarks/benchmark_serving.py

[eca2c5f7c] bnellnm 2024-10-17 [Bugfix] Fix support for dimension like integers and ScalarType (#9299)
6	8	.buildkite/test-pipeline.yaml
0	18	CMakeLists.txt
4	205	csrc/core/scalar_type.hpp
0	16	csrc/core/torch_bindings.cpp
8	7	csrc/moe/marlin_moe_ops.cu
3	2	csrc/moe/torch_bindings.cpp
12	11	csrc/quantization/gptq_marlin/gptq_marlin.cu
9	7	csrc/quantization/machete/machete_pytorch.cu
8	7	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
20	25	csrc/torch_bindings.cpp
0	1	python_only_dev.py
0	7	setup.py
4	4	tests/compile/utils.py
5	4	tests/kernels/test_machete_gemm.py
13	3	tests/kernels/test_marlin_gemm.py
2	2	tests/kernels/test_moe.py
2	2	tests/test_scalartype.py
0	1	tools/report_build_time_ninja.py
0	278	vllm/_core_ext.py
31	62	vllm/_custom_ops.py
3	3	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
297	4	vllm/scalar_type.py

[0f41fbe5a] Luka Govedič 2024-10-17 [torch.compile] Fine-grained CustomOp enabling mechanism (#9300)
92	0	tests/model_executor/test_enabled_custom_ops.py
12	1	vllm/envs.py
64	4	vllm/model_executor/custom_op.py
25	11	vllm/model_executor/layers/activation.py
1	4	vllm/model_executor/layers/fused_moe/layer.py
2	0	vllm/model_executor/layers/layernorm.py
2	1	vllm/model_executor/layers/rotary_embedding.py
22	0	vllm/utils.py

[7871659ab] Cyrus Leung 2024-10-18 [Misc] Remove commit id file (#9470)
0	1	vllm/commit_id.py

[a2c71c540] Daniele 2024-10-17 [CI/Build] remove .github from .dockerignore, add dirty repo check (#9375)
2	2	.buildkite/release-pipeline.yaml
0	1	.dockerignore
4	0	.github/workflows/scripts/build.sh
3	1	Dockerfile
4	1	Dockerfile.cpu
7	7	Dockerfile.neuron
3	0	Dockerfile.openvino
3	0	Dockerfile.ppc64le
3	0	Dockerfile.rocm
7	4	Dockerfile.tpu
4	1	Dockerfile.xpu
14	0	tools/check_repo.sh

[81ede99ca] Kuntai Du 2024-10-17 [Core] Deprecating block manager v1 and make block manager v2 default (#8704)
6	12	.buildkite/test-pipeline.yaml
0	4	benchmarks/benchmark_latency.py
0	6	benchmarks/benchmark_prefix_caching.py
1	10	benchmarks/benchmark_throughput.py
0	4	benchmarks/overheads/benchmark_hashing.py
0	3	docs/source/models/spec_decode.rst
0	2	examples/offline_inference_mlpspeculator.py
1	10	tests/basic_correctness/test_chunked_prefill.py
28	50	tests/core/block/e2e/test_correctness.py
3	16	tests/core/block/e2e/test_correctness_sliding_window.py
29	28	tests/core/block/{test_block_manager_v2.py => test_block_manager.py}
0	637	tests/core/test_block_manager.py
24	44	tests/core/test_chunked_prefill_scheduler.py
0	1	tests/core/test_num_computed_tokens_update.py
55	95	tests/core/test_scheduler.py
8	8	tests/metrics/test_metrics.py
0	1	tests/multi_step/test_correctness_async_llm.py
0	4	tests/multi_step/test_correctness_llm.py
0	89	tests/prefix_caching/test_prefix_caching.py
10	58	tests/spec_decode/e2e/test_compatibility.py
0	18	tests/spec_decode/e2e/test_eagle_correctness.py
0	8	tests/spec_decode/e2e/test_integration.py
0	6	tests/spec_decode/e2e/test_integration_dist_tp2.py
0	6	tests/spec_decode/e2e/test_integration_dist_tp4.py
0	14	tests/spec_decode/e2e/test_logprobs.py
0	21	tests/spec_decode/e2e/test_medusa_correctness.py
0	27	tests/spec_decode/e2e/test_mlp_correctness.py
0	36	tests/spec_decode/e2e/test_multistep_correctness.py
0	16	tests/spec_decode/e2e/test_ngram_correctness.py
0	3	tests/spec_decode/e2e/test_seed.py
0	9	tests/utils.py
3	5	vllm/attention/backends/flash_attn.py
3	5	vllm/attention/backends/flashinfer.py
4	12	vllm/attention/backends/utils.py
1	0	vllm/commit_id.py
0	24	vllm/config.py
1	23	vllm/core/block/utils.py
1	1	vllm/core/{block_manager_v2.py => block_manager.py}
0	743	vllm/core/block_manager_v1.py
3	7	vllm/core/interfaces.py
1	3	vllm/core/scheduler.py
17	21	vllm/engine/arg_utils.py
1	2	vllm/engine/llm_engine.py
0	6	vllm/envs.py
6	11	vllm/worker/model_runner.py

[5eda21e77] Li, Jiang 2024-10-18 [Hardware][CPU] compressed-tensor INT8 W8A8 AZP support  (#9344)
4	4	.buildkite/run-cpu-test.sh
0	13	Dockerfile.cpu
34	6	cmake/cpu_extension.cmake
39	2	csrc/cpu/cpu_types_x86.hpp
360	57	csrc/cpu/quant.cpp
15	0	csrc/cpu/torch_bindings.cpp
0	14	docs/source/getting_started/cpu-installation.rst

[8e1cddcd4] Woosuk Kwon 2024-10-17 [TPU] Call torch._sync(param) during weight loading (#9437)
22	0	vllm/model_executor/utils.py

[5e443b594] sasha0552 2024-10-17 [Bugfix] Allow prefill of assistant response when using `mistral_common` (#9446)
4	0	vllm/transformers_utils/tokenizers/mistral.py

[9d30a056e] Lucas Wilkinson 2024-10-17 [misc] CUDA Time Layerwise Profiler (#8337)
1	0	.buildkite/test-pipeline.yaml
282	0	examples/offline_profile.py
77	0	tools/profiler/print_layerwise_table.py
522	0	tools/profiler/visualize_layerwise_profile.py
5	0	vllm/profiler/__init__.py
354	0	vllm/profiler/layerwise_profile.py
145	0	vllm/profiler/utils.py
4	4	vllm/worker/model_runner.py

[390be7464] Cyrus Leung 2024-10-17 [Misc] Print stack trace using `logger.exception` (#9461)
3	3	vllm/entrypoints/openai/serving_chat.py
5	5	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
2	2	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
4	5	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
4	4	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
3	5	vllm/executor/multiproc_worker_utils.py
2	2	vllm/model_executor/model_loader/weight_utils.py
3	4	vllm/platforms/cuda.py

[e312e52b4] Lucas Wilkinson 2024-10-17 [Kernel] Add Exllama as a backend for compressed-tensors  (#9395)
9	0	vllm/envs.py
4	0	vllm/model_executor/layers/quantization/kernels/MPLinearKernel.py
5	3	vllm/model_executor/layers/quantization/kernels/__init__.py
140	0	vllm/model_executor/layers/quantization/kernels/exllama.py
7	7	vllm/model_executor/layers/quantization/kernels/machete.py
6	6	vllm/model_executor/layers/quantization/utils/quant_utils.py
2	0	vllm/scalar_type.py

[dbfa8d31d] Yuan Tang 2024-10-17 Add notes on the use of Slack (#9442)
1	0	README.md

[92d86da21] rasmith 2024-10-16 [BugFix] [Kernel] Fix GPU SEGV occurring in int8 kernels (#9391)
28	14	csrc/quantization/compressed_tensors/int8_quant_kernels.cu

[c3fab5f76] Tyler Michael Smith 2024-10-16 [Bugfix][Kernel] Prevent integer overflow in fp8 dynamic per-token quantize kernel (#9425)
4	2	csrc/quantization/fp8/common.cu

[776dbd74f] Russell Bryant 2024-10-16 [CI/Build] mypy: Resolve some errors from checking vllm/engine (#9267)
1	11	tools/mypy.sh
1	1	vllm/attention/layer.py
2	2	vllm/compilation/backends.py
5	3	vllm/compilation/decorators.py
1	1	vllm/compilation/wrapper.py
6	4	vllm/config.py
4	3	vllm/core/scheduler.py
7	5	vllm/engine/arg_utils.py
12	8	vllm/engine/llm_engine.py
9	5	vllm/engine/metrics.py
12	5	vllm/engine/multiprocessing/client.py
2	4	vllm/engine/multiprocessing/engine.py
19	6	vllm/engine/output_processor/multi_step.py
5	3	vllm/engine/output_processor/single_step.py
2	2	vllm/engine/output_processor/stop_checker.py
8	5	vllm/engine/output_processor/util.py
4	1	vllm/inputs/parse.py
5	2	vllm/model_executor/layers/sampler.py
2	1	vllm/outputs.py
2	2	vllm/sequence.py

[834504583] Lily Liu 2024-10-16 [Performance][Spec Decode] Optimize ngram lookup performance (#9333)
11	6	vllm/spec_decode/ngram_worker.py

[5b8a1fde8] Junhao Li 2024-10-16 [Model][Bugfix] Add FATReLU activation and support for openbmb/MiniCPM-S-1B-sft (#9396)
1	1	docs/source/models/supported_models.rst
27	0	vllm/model_executor/layers/activation.py
9	4	vllm/model_executor/models/minicpm.py

[fb60ae9b9] Mor Zusman 2024-10-17 [Kernel][Model] Improve continuous batching for Jamba and Mamba (#9189)
25	12	csrc/mamba/causal_conv1d/causal_conv1d.cu
1	0	csrc/mamba/causal_conv1d/causal_conv1d.h
1	0	csrc/mamba/mamba_ssm/selective_scan.h
14	10	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
17	15	csrc/ops.h
6	3	csrc/torch_bindings.cpp
97	94	tests/kernels/test_causal_conv1d.py
89	35	tests/kernels/test_mamba_ssm.py
25	0	tests/models/decoder_only/language/test_jamba.py
40	33	vllm/_custom_ops.py
33	20	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
43	27	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
31	40	vllm/model_executor/models/jamba.py
28	25	vllm/model_executor/models/mamba.py
61	125	vllm/model_executor/models/mamba_cache.py

[415f76a9c] Patrick von Platen 2024-10-16 Support mistral interleaved attn (#9414)
28	10	vllm/config.py

[cf1d62a64] Isotr0py 2024-10-16 [Model] Support SDPA attention for Molmo vision backbone (#9410)
15	37	vllm/model_executor/models/molmo.py
12	40	vllm/model_executor/models/qwen2_vl.py
34	1	vllm/model_executor/models/utils.py

[59230ef32] Roger Wang 2024-10-16 [Misc] Consolidate example usage of OpenAI client for multimodal models (#9412)
1	1	docs/source/models/vlm.rst
236	0	examples/openai_api_client_for_multimodal.py
0	90	examples/openai_audio_api_client.py
0	126	examples/openai_vision_api_client.py

[cee711fdb] Cyrus Leung 2024-10-16 [Core] Rename input data types (#8688)
1	1	docs/source/dev/input_processing/model_inputs_index.rst
13	15	tests/models/decoder_only/vision_language/test_phi3v.py
6	6	tests/models/decoder_only/vision_language/test_qwen.py
9	9	tests/multimodal/test_processor_kwargs.py
5	5	vllm/engine/llm_engine.py
29	8	vllm/inputs/__init__.py
62	16	vllm/inputs/data.py
6	6	vllm/inputs/parse.py
18	18	vllm/inputs/preprocess.py
9	6	vllm/inputs/registry.py
10	10	vllm/model_executor/models/blip.py
11	10	vllm/model_executor/models/blip2.py
12	10	vllm/model_executor/models/chameleon.py
11	11	vllm/model_executor/models/chatglm.py
10	10	vllm/model_executor/models/clip.py
10	9	vllm/model_executor/models/fuyu.py
11	10	vllm/model_executor/models/internvl.py
6	6	vllm/model_executor/models/llava.py
7	6	vllm/model_executor/models/llava_next.py
10	9	vllm/model_executor/models/llava_next_video.py
20	22	vllm/model_executor/models/llava_onevision.py
9	9	vllm/model_executor/models/minicpmv.py
26	26	vllm/model_executor/models/mllama.py
8	9	vllm/model_executor/models/molmo.py
11	9	vllm/model_executor/models/paligemma.py
10	10	vllm/model_executor/models/phi3v.py
7	7	vllm/model_executor/models/pixtral.py
13	12	vllm/model_executor/models/qwen.py
14	11	vllm/model_executor/models/qwen2_vl.py
8	8	vllm/model_executor/models/siglip.py
9	9	vllm/model_executor/models/ultravox.py
47	27	vllm/sequence.py

[1de76a0e5] Cyrus Leung 2024-10-16 [CI/Build] Test VLM embeddings (#9406)
2	0	.buildkite/test-pipeline.yaml

[7abba39ee] Cyrus Leung 2024-10-16 [Model] VLM2Vec, the first multimodal embedding model in vLLM (#9303)
53	26	docs/source/models/supported_models.rst
21	0	examples/offline_inference_vision_language_embedding.py
98	61	tests/conftest.py
12	18	tests/models/embedding/language/test_embedding.py
29	0	tests/models/embedding/utils.py
0	0	tests/models/embedding/vision_language/__init__.py
62	0	tests/models/embedding/vision_language/test_phi3v.py
1	1	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
10	1	vllm/config.py
50	1	vllm/model_executor/models/gemma2.py
0	57	vllm/model_executor/models/gemma2_embedding.py
52	1	vllm/model_executor/models/llama.py
0	59	vllm/model_executor/models/llama_embedding.py
58	25	vllm/model_executor/models/phi3v.py
5	2	vllm/model_executor/models/registry.py
14	9	vllm/model_executor/models/utils.py

[7e7eae338] Cyrus Leung 2024-10-16 [Misc] Standardize RoPE handling for Qwen2-VL (#9250)
2	2	benchmarks/kernels/benchmark_rope.py
1	1	requirements-common.txt
4	4	tests/kernels/test_pos_encoding.py
1	1	tests/lora/test_layers.py
2	2	tests/test_config.py
7	14	vllm/config.py
6	5	vllm/engine/arg_utils.py
28	19	vllm/model_executor/layers/rotary_embedding.py
1	1	vllm/model_executor/models/deepseek_v2.py
1	1	vllm/model_executor/models/phi3_small.py
4	4	vllm/model_executor/models/qwen2_vl.py
41	3	vllm/transformers_utils/config.py
0	4	vllm/transformers_utils/configs/__init__.py
0	131	vllm/transformers_utils/configs/qwen2vl.py
2	4	vllm/worker/cpu_model_runner.py
2	4	vllm/worker/model_runner.py

[ed920135c] Reza Salehi 2024-10-15 [Bugfix] Molmo text-only input bug fix (#9397)
6	3	vllm/model_executor/models/molmo.py

[717a5f82c] Lucas Wilkinson 2024-10-15 [Bugfix][CI/Build] Fix CUDA 11.8 Build (#9386)
5	5	CMakeLists.txt

[ba3094224] Chang Su 2024-10-15 [Bugfix] Fix vLLM UsageInfo and logprobs None AssertionError with empty token_ids (#9034)
126	0	tests/entrypoints/openai/test_chunked_prompt.py
6	0	vllm/entrypoints/openai/serving_chat.py
5	2	vllm/entrypoints/openai/serving_completion.py
3	0	vllm/sequence.py

[22f8a6954] Michael Goin 2024-10-15 [Misc] Directly use compressed-tensors for checkpoint definitions (#8909)
1	0	requirements-common.txt
0	1	requirements-test.txt
1	2	tests/quantization/test_compressed_tensors.py
5	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
2	100	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[5d264f4ab] Grace Ho 2024-10-15 pass ignore_eos parameter to all benchmark_serving calls (#9349)
18	20	benchmarks/benchmark_serving.py

[e9d517f27] Nick Hill 2024-10-15 [BugFix] Fix chat API continuous usage stats (#9357)
12	2	tests/entrypoints/openai/test_chat.py
41	74	vllm/entrypoints/openai/serving_chat.py

[55e081fba] hhzhang16 2024-10-14 [Bugfix] Update InternVL input mapper to support image embeds (#9351)
2	0	vllm/model_executor/models/internvl.py

[8e836d982] Michael Goin 2024-10-15 [Doc] Fix code formatting in spec_decode.rst (#9348)
3	3	docs/source/models/spec_decode.rst

[44eaa5a5d] Steve Grubb 2024-10-15 [Frontend] Clarify model_type error messages (#9345)
3	3	vllm/entrypoints/chat_utils.py

[169b53060] Tyler Michael Smith 2024-10-14 [Bugfix] Clean up some cruft in mamba.py (#9343)
1	1	docs/source/models/supported_models.rst
10	103	vllm/model_executor/models/mamba.py

[f0fe4fe86] Xiang Xu 2024-10-14 [Model] Make llama3.2 support multiple and interleaved images (#9095)
23	0	examples/offline_inference_vision_language_multi_image.py
82	3	tests/models/encoder_decoder/vision_language/test_mllama.py
279	39	vllm/model_executor/models/mllama.py

[4d31cd424] Brendan Wong 2024-10-14 [Frontend] merge beam search implementations (#9296)
6	104	vllm/engine/async_llm_engine.py
17	109	vllm/engine/multiprocessing/client.py
122	7	vllm/engine/protocol.py
0	7	vllm/entrypoints/openai/serving_chat.py
0	7	vllm/entrypoints/openai/serving_completion.py

[473e7b360] Woosuk Kwon 2024-10-14 [TPU] Fix TPU SMEM OOM by Pallas paged attention kernel (#9350)
80	21	vllm/attention/backends/pallas.py
9	0	vllm/worker/tpu_model_runner.py

[fd47e57f4] Simon Mo 2024-10-14 [Docs] Remove PDF build from Readtehdocs (#9347)
2	2	.readthedocs.yaml

[203ab8f80] Daniele 2024-10-14 [CI/Build] setuptools-scm fixes (#8900)
2	2	.buildkite/release-pipeline.yaml
29	1	.dockerignore
1	2	.github/workflows/scripts/build.sh
1	9	Dockerfile
1	10	Dockerfile.openvino
10	17	collect_env.py
3	0	pyproject.toml

[4141608c6] Kunshang Ji 2024-10-15 [Hardware][intel GPU] add async output process for xpu (#8897)
2	2	vllm/config.py
6	2	vllm/worker/xpu_model_runner.py

[dfe43a207] Reza Salehi 2024-10-14 [Model] Molmo vLLM Integration (#9016)
6	0	docs/source/models/supported_models.rst
18	0	examples/offline_inference_vision_language.py
2	0	vllm/entrypoints/chat_utils.py
1	1	vllm/model_executor/models/__init__.py
1290	0	vllm/model_executor/models/molmo.py
1	2	vllm/model_executor/models/qwen2_vl.py
1	0	vllm/model_executor/models/registry.py

[16b24e7dc] Tyler Michael Smith 2024-10-13 [Bugfix] Bandaid fix for speculative decoding tests (#9327)
18	3	vllm/worker/model_runner.py

[f519902c5] Lily Liu 2024-10-12 [CI] Fix merge conflict (#9317)
7	10	vllm/attention/backends/placeholder_attn.py

[250e26a63] Jee Jee Li 2024-10-13 [Bugfix]Fix MiniCPM's LoRA bug (#9286)
5	1	vllm/lora/models.py
13	16	vllm/model_executor/models/minicpm.py
22	0	vllm/model_executor/models/minicpm3.py

[2b184ddd4] Yunmeng 2024-10-13 [Misc][Installation] Improve source installation script and doc (#9309)
19	0	docs/source/getting_started/installation.rst
50	12	python_only_dev.py

[00298e092] Xiang Xu 2024-10-12 [Bugfix] Fix bug of xformer prefill for encoder-decoder (#9026)
18	11	vllm/attention/backends/xformers.py

[89feb4c84] Lily Liu 2024-10-11 [SpecDec] Remove Batch Expansion (2/3) (#9298)
39	13	tests/spec_decode/test_scorer.py
2	5	vllm/attention/backends/blocksparse_attn.py
42	27	vllm/attention/backends/flash_attn.py
2	5	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/attention/backends/utils.py
2	5	vllm/attention/backends/xformers.py
34	8	vllm/spec_decode/mqa_scorer.py
0	6	vllm/spec_decode/spec_decode_worker.py

[ec10cb851] Maximilien de Bayser 2024-10-11 [BugFix] Fix tool call finish reason in streaming case (#9209)
15	11	vllm/entrypoints/openai/serving_chat.py

[d11b46f3a] Prashant Gupta 2024-10-11 [bugfix] fix f-string for error (#9295)
4	4	vllm/transformers_utils/tokenizers/mistral.py

[c6cf9295e] Allen Wang 2024-10-11 [Bugfix] Sets `is_first_step_output` for TPUModelRunner (#9202)
2	1	vllm/worker/tpu_model_runner.py

[de9fb4bef] Lucas Wilkinson 2024-10-11 [Bugfix][CI/Build] Fix docker build where CUDA archs < 7.0 are being detected (#9254)
20	15	CMakeLists.txt

[8baf85e4e] Wallas Henrique 2024-10-11 [Doc] Compatibility matrix for mutual exclusive features (#8512)
1	0	docs/source/index.rst
2	0	docs/source/models/performance.rst
427	0	docs/source/serving/compatibility_matrix.rst
2	0	vllm/attention/backends/rocm_flash_attn.py
10	0	vllm/config.py
2	0	vllm/engine/arg_utils.py
2	0	vllm/engine/output_processor/multi_step.py
8	0	vllm/executor/cpu_executor.py
2	0	vllm/inputs/preprocess.py
2	0	vllm/spec_decode/spec_decode_worker.py
3	0	vllm/utils.py
3	0	vllm/worker/multi_step_model_runner.py
3	0	vllm/worker/utils.py

[1a1823871] homeffjy 2024-10-12 [Doc] Remove outdated comment to avoid misunderstanding (#9287)
3	4	vllm/core/block_manager_v2.py

[6cf1167c1] sixgod 2024-10-12 [Model] Add GLM-4v support and meet vllm==0.6.2  (#9242)
6	0	docs/source/models/supported_models.rst
16	0	examples/offline_inference_vision_language.py
133	0	tests/models/decoder_only/vision_language/test_glm4.py
298	52	vllm/model_executor/models/chatglm.py
298	0	vllm/model_executor/models/glm4_vision_encoder.py
4	2	vllm/model_executor/models/registry.py
21	18	vllm/transformers_utils/tokenizer.py

[f710090d8] Burkhard Ringlein 2024-10-11 [Kernel] adding fused moe kernel config for L40S TP4 (#9245)
173	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_L40S.json

[7342a7d7f] Tyler Michael Smith 2024-10-11 [Model] Support Mamba (#6484)
7	1	.buildkite/run-cpu-test-ppc64le.sh
1	0	.buildkite/run-cpu-test.sh
5	0	docs/source/models/supported_models.rst
21	16	tests/kernels/test_attention_selector.py
295	0	tests/models/decoder_only/language/test_mamba.py
324	0	vllm/attention/backends/placeholder_attn.py
5	3	vllm/attention/layer.py
14	7	vllm/attention/selector.py
28	22	vllm/config.py
4	4	vllm/core/interfaces.py
5	4	vllm/core/{embedding_model_block_manager.py => placeholder_block_space_manager.py}
3	2	vllm/core/scheduler.py
2	5	vllm/engine/arg_utils.py
34	1	vllm/model_executor/model_loader/weight_utils.py
44	1	vllm/model_executor/models/interfaces.py
32	229	vllm/model_executor/models/jamba.py
499	0	vllm/model_executor/models/mamba.py
222	0	vllm/model_executor/models/mamba_cache.py
15	1	vllm/model_executor/models/registry.py
6	9	vllm/worker/cache_engine.py
1	2	vllm/worker/cpu_model_runner.py
1	2	vllm/worker/cpu_worker.py
1	1	vllm/worker/enc_dec_model_runner.py
13	17	vllm/worker/model_runner.py
1	2	vllm/worker/openvino_model_runner.py
1	2	vllm/worker/openvino_worker.py
1	2	vllm/worker/tpu_model_runner.py
17	8	vllm/worker/worker.py
1	2	vllm/worker/xpu_model_runner.py

[df3dcdf49] Sebastian Schoennenbeck 2024-10-11 [Bugfix] Fix priority in multiprocessing engine (#9277)
2	1	vllm/engine/multiprocessing/engine.py

[36ea79079] Jee Jee Li 2024-10-11 [Misc][LoRA] Support loading LoRA weights for target_modules in reg format (#9275)
5	0	tests/lora/conftest.py
15	2	tests/lora/test_lora_checkpoints.py
5	2	vllm/lora/models.py
34	1	vllm/lora/utils.py

[e808156f3] Cyrus Leung 2024-10-11 [Misc] Collect model support info in a single process per model (#9233)
1	1	docs/source/models/adding_model.rst
2	0	vllm/engine/arg_utils.py
3	0	vllm/engine/multiprocessing/engine.py
222	158	vllm/model_executor/models/registry.py

[cbc2ef552] youkaichao 2024-10-10 [misc] hide best_of from engine (#9261)
0	4	tests/entrypoints/openai/test_metrics.py
0	1	tests/metrics/test_metrics.py
0	4	tests/tracing/test_tracing.py
1	1	vllm/core/scheduler.py
2	9	vllm/engine/llm_engine.py
0	8	vllm/engine/metrics.py
0	1	vllm/engine/metrics_types.py
1	1	vllm/engine/output_processor/single_step.py
8	9	vllm/model_executor/layers/sampler.py
1	1	vllm/outputs.py
17	16	vllm/sampling_params.py
5	5	vllm/sequence.py
0	1	vllm/tracing.py
11	12	vllm/worker/tpu_model_runner.py

[94bf9ae4e] Andy Dai 2024-10-10 [Misc] Fix sampling from sonnet for long context case (#9235)
4	4	benchmarks/benchmark_serving.py

[f990bab2a] omrishiv 2024-10-10 [Doc][Neuron] add note to neuron documentation about resolving triton issue (#9257)
4	0	docs/source/getting_started/neuron-installation.rst

[e00c094f1] youkaichao 2024-10-10 [torch.compile] generic decorators (#9258)
58	30	vllm/compilation/decorators.py
8	2	vllm/model_executor/models/gemma2.py
8	2	vllm/model_executor/models/llama.py

[a78c6ba7c] Kevin H. Luu 2024-10-10 [ci/build] Add placeholder command for custom models test (#9262)
2	1	.buildkite/test-pipeline.yaml

[fb870fd49] dependabot[bot] 2024-10-10 Bump actions/setup-python from 3 to 5 (#9195)
1	1	.github/workflows/clang-format.yml
1	1	.github/workflows/mypy.yaml
1	1	.github/workflows/publish.yml
1	1	.github/workflows/ruff.yml
1	1	.github/workflows/yapf.yml

[270953baf] dependabot[bot] 2024-10-10 Bump actions/checkout from 3 to 4 (#9196)
1	1	.github/workflows/actionlint.yml
1	1	.github/workflows/clang-format.yml
1	1	.github/workflows/mypy.yaml
2	2	.github/workflows/publish.yml
1	1	.github/workflows/ruff.yml
1	1	.github/workflows/yapf.yml

[9cc811c4f] dependabot[bot] 2024-10-10 Bump actions/github-script from 6 to 7 (#9197)
1	1	.github/workflows/add_label_automerge.yml
1	1	.github/workflows/publish.yml
1	1	.github/workflows/reminder_comment.yml

[e4d652ea3] youkaichao 2024-10-10 [torch.compile] integration with compilation control (#9058)
12	8	.buildkite/test-pipeline.yaml
48	0	tests/compile/test_basic_correctness.py
11	4	tests/compile/test_full_graph.py
0	22	tests/compile/test_full_graph_multi_gpu.py
0	13	tests/compile/test_full_graph_smoke.py
9	15	tests/compile/utils.py
3	1	tests/tpu/test_compilation.py
8	5	tests/tpu/test_custom_dispatcher.py
114	1	vllm/compilation/backends.py
23	0	vllm/compilation/compile_context.py
85	0	vllm/compilation/decorators.py
9	0	vllm/compilation/levels.py
24	3	vllm/compilation/wrapper.py
3	13	vllm/envs.py
2	1	vllm/model_executor/custom_op.py
2	0	vllm/model_executor/models/gemma2.py
2	0	vllm/model_executor/models/llama.py
5	3	vllm/model_executor/models/llava.py
14	0	vllm/platforms/tpu.py
13	1	vllm/plugins/__init__.py
3	4	vllm/sequence.py
14	4	vllm/worker/model_runner.py

[78c0b4166] Simon Mo 2024-10-10 Suggest codeowners for the core componenets (#9210)
16	5	.github/CODEOWNERS

[21efb603f] jordanyono 2024-10-10 [CI/Build] Make the `Dockerfile.cpu` file's  `PIP_EXTRA_INDEX_URL` Configurable as a Build Argument (#9252)
2	1	Dockerfile.cpu

[055f3270d] Rafael Vasquez 2024-10-10 [Doc] Improve debugging documentation (#9204)
53	36	docs/source/getting_started/debugging.rst

[18511aeda] Lucas Wilkinson 2024-10-10 [Bugfix] Fix Machete unittests failing with `NotImplementedError` (#9218)
4	0	csrc/quantization/machete/machete_pytorch.cu

[83ea5c72b] Ilya Lavrenov 2024-10-10 [OpenVINO] Use torch 2.4.0 and newer optimim version (#9121)
5	5	requirements-openvino.txt

[04de9057a] whyiug 2024-10-10 [Model] support input image embedding for minicpmv (#9237)
1	1	docs/source/models/supported_models.rst
11	4	docs/source/models/vlm.rst
89	38	vllm/model_executor/models/minicpmv.py

[07c11cf4d] Isotr0py 2024-10-10 [Bugfix] Fix lm_head weights tying with lora for llama (#9227)
10	1	vllm/model_executor/layers/vocab_parallel_embedding.py
2	1	vllm/model_executor/models/llama.py

[f3a507f1d] sroy745 2024-10-09 [Core] Add an environment variable which needs to be set explicitly to allow BlockSpaceManagerV1 (#9149)
12	6	.buildkite/test-pipeline.yaml
3	1	benchmarks/benchmark_latency.py
2	0	benchmarks/benchmark_prefix_caching.py
1	0	benchmarks/benchmark_throughput.py
7	1	tests/basic_correctness/test_chunked_prefill.py
7	0	tests/core/block/e2e/test_correctness.py
7	0	tests/core/block/e2e/test_correctness_sliding_window.py
7	0	tests/core/test_chunked_prefill_scheduler.py
7	0	tests/core/test_scheduler.py
7	0	tests/prefix_caching/test_prefix_caching.py
7	0	tests/spec_decode/e2e/test_compatibility.py
9	0	tests/utils.py
12	0	vllm/config.py
6	0	vllm/envs.py

[a64e7b940] Lucas Wilkinson 2024-10-10 [Bugfix] Machete garbage results for some models (large K dim) (#9212)
13	10	csrc/quantization/machete/machete_mainloop.cuh
3	2	tests/kernels/test_machete_gemm.py

[ce00231a8] Michael Goin 2024-10-10 [Bugfix] Fix Weight Loading Multiple GPU Test - Large Models (#9213)
0	1	tests/weight_loading/models-large.txt
1	0	tests/weight_loading/models.txt

[de895f169] youkaichao 2024-10-09 [misc] improve model support check in another process (#9208)
1	0	docs/requirements-docs.txt
35	32	vllm/model_executor/models/registry.py

[cf25b93bd] Russell Bryant 2024-10-10 [Core] Fix invalid args to _process_request (#9201)
2	1	vllm/engine/multiprocessing/client.py

[d5fbb8706] Michael Goin 2024-10-09 [CI/Build] Update Dockerfile install+deploy image to ubuntu 22.04 (#9130)
1	1	Dockerfile

[cdca8994b] Russell Bryant 2024-10-09 [CI/Build] mypy: check vllm/entrypoints (#9194)
1	1	tools/mypy.sh
5	2	vllm/entrypoints/llm.py

[ca77dd7a4] Li, Jiang 2024-10-10 [Hardware][CPU] Support AWQ for CPU backend (#7515)
8	2	.buildkite/run-cpu-test.sh
1	1	Dockerfile.cpu
2	2	docs/source/quantization/supported_hardware.rst
28	0	tests/quantization/test_ipex_quant.py
1	1	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
4	0	vllm/model_executor/layers/quantization/awq_marlin.py
166	0	vllm/model_executor/layers/quantization/ipex_quant.py
2	1	vllm/worker/cpu_worker.py

[7dea28906] Ewout ter Hoeven 2024-10-09 Add Dependabot configuration for GitHub Actions updates (#1217)
7	0	.github/dependabot.yml

[cfaa6008e] Cyrus Leung 2024-10-09 [Bugfix] Access `get_vocab` instead of `vocab` in tool parsers (#9188)
7	0	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
3	4	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
1	2	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[21906a6f5] Ahmad Fahadh Ilyas 2024-10-09 [Bugfix] Fix lora loading for Compressed Tensors in #9120 (#9179)
3	0	vllm/lora/layers.py

[dc4aea677] Jiangtao Hu 2024-10-09 [Doc] Fix VLM prompt placeholder sample bug (#9170)
7	7	docs/source/models/vlm.rst

[c8627cd41] youkaichao 2024-10-09 [ci][test] use load dummy for testing (#9165)
1	1	.buildkite/test-pipeline.yaml
17	0	tests/utils.py
2	0	vllm/envs.py

[8bfaa4e31] Cyrus Leung 2024-10-09 [Bugfix] fix composite weight loading and EAGLE weight loading (#9160)
3	34	vllm/model_executor/models/blip2.py
3	16	vllm/model_executor/models/fuyu.py
7	17	vllm/model_executor/models/gemma2.py
4	19	vllm/model_executor/models/internvl.py
8	20	vllm/model_executor/models/llama.py
4	19	vllm/model_executor/models/llava.py
4	27	vllm/model_executor/models/llava_next.py
7	18	vllm/model_executor/models/llava_next_video.py
4	19	vllm/model_executor/models/llava_onevision.py
3	18	vllm/model_executor/models/paligemma.py
12	59	vllm/model_executor/models/phi3v.py
7	16	vllm/model_executor/models/qwen2.py
3	12	vllm/model_executor/models/qwen2_rm.py
8	32	vllm/model_executor/models/ultravox.py
167	38	vllm/model_executor/models/utils.py

[0b5b5d767] AlpinDale 2024-10-09 [Frontend] Log the maximum supported concurrency (#8831)
4	0	vllm/executor/distributed_gpu_executor.py
4	0	vllm/executor/gpu_executor.py

[cdc72e3c8] Hui Liu 2024-10-08 [Model] Remap FP8 kv_scale in CommandR and DBRX (#9174)
7	1	vllm/model_executor/models/commandr.py
7	1	vllm/model_executor/models/dbrx.py

[7627172bf] Joe Rowell 2024-10-09 [Bugfix][Doc] Report neuron error in output (#9159)
1	1	setup.py

[480b7f40c] Travis Johnson 2024-10-08 [Misc] Improve validation errors around best_of and n (#9167)
7	4	vllm/sampling_params.py

[acce7630c] Yuan Tang 2024-10-08 Update link to KServe deployment guide (#9173)
1	1	docs/source/serving/deploying_with_kserve.rst

[ffc4b27ea] Yuan Tang 2024-10-08 Add classifiers in setup.py (#9171)
4	0	setup.py

[2f4117c38] chenqianfzh 2024-10-08 support bitsandbytes quantization with more models (#9148)
7	6	tests/quantization/test_bitsandbytes.py
25	1	vllm/model_executor/layers/linear.py
2	2	vllm/model_executor/layers/quantization/bitsandbytes.py
44	18	vllm/model_executor/model_loader/loader.py
11	0	vllm/model_executor/models/falcon.py
22	0	vllm/model_executor/models/gemma.py
13	0	vllm/model_executor/models/gemma2.py
13	0	vllm/model_executor/models/llama.py
13	0	vllm/model_executor/models/opt.py
14	0	vllm/model_executor/models/phi.py

[9ba0bd6aa] Michael Goin 2024-10-08 Add `lm-eval` directly to requirements-test.txt (#9161)
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
0	3	.buildkite/test-pipeline.yaml
1	1	docs/source/quantization/fp8.rst
1	0	requirements-test.txt

[2a131965a] Russell Bryant 2024-10-08 mypy: check additional directories (#9162)
2	13	.github/workflows/mypy.yaml
1	11	format.sh
36	0	tools/mypy.sh
2	2	vllm/usage/usage_lib.py

[bd37b9fbe] bnellnm 2024-10-08 [Bugfix] Try to handle older versions of pytorch (#9086)
5	0	tests/kernels/test_awq.py
4	0	tests/kernels/test_awq_marlin.py
32	21	vllm/_custom_ops.py

[de24046fc] Rafael Vasquez 2024-10-08 [Doc] Improve contributing and installation documentation (#9132)
15	21	CONTRIBUTING.md
4	5	SECURITY.md
75	62	docs/source/getting_started/installation.rst

[1874c6a1b] Sayak Paul 2024-10-08 [Doc] Update vlm.rst to include an example on videos (#9155)
27	0	docs/source/models/vlm.rst

[9a94ca4a5] Daniele 2024-10-08 [Bugfix] fix OpenAI API server startup with --disable-frontend-multiprocessing (#8537)
57	1	tests/entrypoints/openai/test_basic.py
6	4	vllm/entrypoints/openai/api_server.py

[cfba685bd] Peter Pan 2024-10-09 [CI/Build] Add examples folder into Docker image so that we can leverage the templates*.jinja when serving models (#8758)
1	0	Dockerfile

[069d3bd8d] Alex Brooks 2024-10-08 [Frontend] Add Early Validation For Chat Template / Tool Call Parser (#9151)
109	69	tests/entrypoints/openai/test_cli_args.py
22	0	vllm/entrypoints/chat_utils.py
3	1	vllm/entrypoints/openai/api_server.py
15	0	vllm/entrypoints/openai/cli_args.py
6	2	vllm/scripts.py

[a3691b6b5] Alex Brooks 2024-10-08 [Core][Frontend] Add Support for Inference Time mm_processor_kwargs (#9131)
1	0	examples/offline_inference_vision_language.py
70	40	tests/multimodal/test_processor_kwargs.py
26	0	tests/test_inputs.py
31	1	tests/test_utils.py
1	0	vllm/core/scheduler.py
7	0	vllm/engine/llm_engine.py
9	0	vllm/entrypoints/llm.py
58	9	vllm/inputs/data.py
51	19	vllm/inputs/preprocess.py
10	3	vllm/inputs/registry.py
2	2	vllm/multimodal/audio.py
20	11	vllm/multimodal/base.py
18	6	vllm/multimodal/image.py
9	4	vllm/multimodal/registry.py
17	7	vllm/multimodal/video.py
14	0	vllm/sequence.py
82	13	vllm/utils.py
5	3	vllm/worker/cpu_model_runner.py
3	1	vllm/worker/model_runner.py
4	1	vllm/worker/neuron_model_runner.py
5	1	vllm/worker/openvino_model_runner.py

[8c746226c] Brendan Wong 2024-10-07 [Frontend] API support for beam search for MQLLMEngine (#9117)
19	24	tests/entrypoints/openai/test_completion.py
61	0	vllm/beam_search.py
5	7	vllm/engine/async_llm_engine.py
107	6	vllm/engine/multiprocessing/client.py
3	34	vllm/entrypoints/llm.py
10	8	vllm/entrypoints/openai/serving_chat.py
10	8	vllm/entrypoints/openai/serving_completion.py
0	19	vllm/utils.py

[e1faa2a59] youkaichao 2024-10-07 [misc] improve ux on readme (#9147)
1	1	README.md

[80b57f00d] Kunshang Ji 2024-10-08 [Intel GPU] Fix xpu decode input  (#9145)
14	7	vllm/worker/xpu_model_runner.py

[04c12f815] youkaichao 2024-10-07 [misc] update utils to support comparing multiple settings (#9140)
44	11	tests/utils.py

[8eeb85708] Simon Mo 2024-10-07 Add Slack to README (#9137)
2	2	README.md

[fa45513a5] youkaichao 2024-10-07 [misc] fix comment and variable name (#9139)
4	3	vllm/core/scheduler.py

[c0d9a98d0] Kuntai Du 2024-10-07 [Doc] Include performance benchmark in README (#9135)
1	1	README.md

[e0dbdb013] Russell Bryant 2024-10-07 [CI/Build] Add linting for github actions workflows (#7876)
37	0	.github/workflows/actionlint.yml
1	1	.github/workflows/add_label_automerge.yml
2	2	.github/workflows/clang-format.yml
17	0	.github/workflows/matchers/actionlint.json
2	2	.github/workflows/mypy.yaml
4	4	.github/workflows/publish.yml
2	2	.github/workflows/ruff.yml
2	2	.github/workflows/yapf.yml
3	0	.gitignore
4	1	format.sh
13	0	tools/actionlint.sh

[93cf74a8a] TimWang 2024-10-08 [Doc]: Add deploying_with_k8s guide (#8451)
1	0	docs/source/index.rst
175	0	docs/source/serving/deploying_with_k8s.rst

[151ef4efd] Cyrus Leung 2024-10-07 [Model] Support NVLM-D and fix QK Norm in InternViT (#9045)
9	0	docs/source/models/supported_models.rst
40	15	examples/offline_inference_vision_language.py
34	0	examples/offline_inference_vision_language_multi_image.py
1	1	vllm/entrypoints/chat_utils.py
30	2	vllm/model_executor/layers/layernorm.py
134	72	vllm/model_executor/models/intern_vit.py
173	121	vllm/model_executor/models/internvl.py
64	0	vllm/model_executor/models/nvlm_d.py
15	22	vllm/model_executor/models/registry.py
4	3	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
12	0	vllm/transformers_utils/configs/nvlm_d.py

[f19da6487] Isotr0py 2024-10-07 [Core] Refactor GGUF parameters packing and forwarding (#8859)
6	6	tests/models/decoder_only/language/test_gguf.py
32	44	vllm/model_executor/layers/linear.py
25	11	vllm/model_executor/layers/quantization/gguf.py
1	1	vllm/model_executor/models/llama.py

[4f95ffee6] Isotr0py 2024-10-07 [Hardware][CPU] Cross-attention and Encoder-Decoder models support on CPU backend (#9089)
1	0	.buildkite/run-cpu-test.sh
211	217	tests/models/encoder_decoder/language/test_bart.py
299	61	vllm/attention/backends/torch_sdpa.py
311	0	vllm/worker/cpu_enc_dec_model_runner.py
3	7	vllm/worker/cpu_model_runner.py
9	2	vllm/worker/cpu_worker.py

[8c6de96ea] Cyrus Leung 2024-10-07 [Model] Explicit interface for vLLM models and support OOT embedding models (#9108)
20	0	tests/conftest.py
15	3	tests/models/test_oot_registration.py
22	2	tests/models/test_registry.py
6	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py
34	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_gemma_embedding.py
7	0	vllm/model_executor/models/__init__.py
7	21	vllm/model_executor/models/interfaces.py
191	0	vllm/model_executor/models/interfaces_base.py
31	11	vllm/model_executor/models/registry.py
9	0	vllm/utils.py

[18b296fdb] youkaichao 2024-10-06 [core] remove beam search from the core (#9105)
0	6	benchmarks/backend_request_func.py
1	2	benchmarks/benchmark_latency.py
11	13	benchmarks/benchmark_prioritization.py
0	7	benchmarks/benchmark_serving.py
9	20	benchmarks/benchmark_throughput.py
0	3	examples/llm_engine_example.py
0	18	examples/multilora_inference.py
2	112	tests/basic_correctness/test_preemption.py
0	14	tests/conftest.py
0	67	tests/core/block/e2e/test_correctness.py
1	6	tests/core/utils.py
2	2	tests/samplers/test_beam_search.py
2	28	tests/samplers/test_sampler.py
2	2	vllm/core/scheduler.py
9	7	vllm/engine/async_llm_engine.py
7	157	vllm/engine/output_processor/single_step.py
10	3	vllm/entrypoints/llm.py
2	8	vllm/entrypoints/openai/protocol.py
0	5	vllm/envs.py
2	7	vllm/model_executor/layers/sampler.py
1	5	vllm/outputs.py
8	65	vllm/sampling_params.py
10	36	vllm/sequence.py
19	0	vllm/utils.py
0	3	vllm/worker/tpu_model_runner.py

[c8f26bb63] sroy745 2024-10-06 [BugFix][Core] Fix BlockManagerV2 when Encoder Input is None (#9103)
0	2	vllm/core/block/block_table.py
3	1	vllm/core/block_manager_v2.py
0	5	vllm/engine/arg_utils.py

[487678d04] Isotr0py 2024-10-07 [Bugfix][Hardware][CPU] Fix CPU model input for decode (#9044)
1	1	vllm/worker/cpu_model_runner.py

[cb3b2b9ba] Varun Sundar Rabindranath 2024-10-06 [Bugfix] Fix incorrect updates to num_computed_tokens in multi-step scheduling (#9038)
81	0	tests/core/test_num_computed_tokens_update.py
5	1	tests/core/utils.py
12	2	vllm/attention/backends/rocm_flash_attn.py
66	90	vllm/engine/llm_engine.py
2	6	vllm/engine/output_processor/interfaces.py
13	11	vllm/engine/output_processor/multi_step.py

[fdf59d30e] Yanyi Liu 2024-10-06 [Bugfix] fix tool_parser error handling when serve a model not support it (#8709)
21	5	vllm/entrypoints/openai/serving_chat.py
4	4	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
6	1	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[b22b79847] Cyrus Leung 2024-10-06 [Model] PP support for embedding models and update docs (#9090)
56	4	docs/source/models/supported_models.rst
3	4	docs/source/models/vlm.rst
114	32	tests/distributed/test_pipeline_parallel.py
137	92	tests/utils.py
57	43	vllm/model_executor/models/gemma2.py
12	37	vllm/model_executor/models/gemma2_embedding.py
103	82	vllm/model_executor/models/llama.py
7	46	vllm/model_executor/models/llama_embedding.py
53	38	vllm/model_executor/models/qwen2.py
21	51	vllm/model_executor/models/qwen2_rm.py
6	4	vllm/model_executor/models/utils.py
41	16	vllm/worker/embedding_model_runner.py

[f22619fe9] Cyrus Leung 2024-10-06 [Misc] Remove user-facing error for removed VLM args (#9104)
0	4	docs/source/models/vlm.rst
1	9	vllm/entrypoints/llm.py

[168cab6bb] Brendan Wong 2024-10-05 [Frontend] API support for beam search (#9087)
8	4	benchmarks/benchmark_throughput.py
4	1	tests/conftest.py
24	19	tests/entrypoints/openai/test_completion.py
103	4	vllm/engine/async_llm_engine.py
10	10	vllm/entrypoints/llm.py
3	2	vllm/entrypoints/logger.py
34	2	vllm/entrypoints/openai/protocol.py
31	12	vllm/entrypoints/openai/serving_chat.py
34	12	vllm/entrypoints/openai/serving_completion.py
3	2	vllm/entrypoints/openai/serving_engine.py
12	0	vllm/sampling_params.py
9	0	vllm/utils.py

[23fea8714] TJian 2024-10-05 [Bugfix] Fix try-catch conditions to import correct Flash Attention Backend in Draft Model (#9101)
10	5	vllm/spec_decode/draft_model_runner.py

[f4dd830e0] youkaichao 2024-10-05 [core] use forward context for flash infer (#9097)
127	67	vllm/attention/backends/flashinfer.py

[5df183489] Andy Dai 2024-10-05 [Bugfix] Fix order of arguments matters in config.yaml (#8960)
1	1	docs/source/serving/openai_compatible_server.md
1	0	tests/data/test_config.yaml
23	7	tests/test_utils.py
11	1	vllm/utils.py

[cfadb9c68] Chen Zhang 2024-10-05 [Bugfix] Deprecate registration of custom configs to huggingface (#9083)
2	1	tests/models/decoder_only/vision_language/test_internvl.py
0	7	tests/models/encoder_decoder/vision_language/test_mllama.py
0	8	vllm/transformers_utils/config.py

[15986f598] Xin Yang 2024-10-04 [Model] Support Gemma2 embedding model (#9004)
1	0	tests/conftest.py
10	1	tests/models/embedding/language/test_embedding.py
5	2	vllm/model_executor/models/gemma2.py
82	0	vllm/model_executor/models/gemma2_embedding.py
1	0	vllm/model_executor/models/registry.py

[53b3a3302] hhzhang16 2024-10-04 [Bugfix] Fixes Phi3v & Ultravox Multimodal EmbeddingInputs (#8979)
14	6	vllm/model_executor/models/phi3v.py
29	19	vllm/model_executor/models/ultravox.py

[dac914b0d] Chen Zhang 2024-10-04 [Bugfix] use blockmanagerv1 for encoder-decoder (#9084)
5	0	vllm/engine/arg_utils.py

[a95354a36] Zhuohan Li 2024-10-04 [Doc] Update README.md with Ray summit slides (#9088)
1	0	README.md

[663874e04] youkaichao 2024-10-04 [torch.compile] improve allreduce registration (#9061)
6	9	vllm/distributed/device_communicators/custom_all_reduce.py
15	23	vllm/distributed/parallel_state.py

[cc90419e8] Chongming Ni 2024-10-04 [Hardware][Neuron] Add on-device sampling support for Neuron (#8746)
50	9	vllm/model_executor/model_loader/neuron.py
78	4	vllm/worker/neuron_model_runner.py

[27302dd58] Cody Yu 2024-10-04 [Misc] Fix CI lint (#9085)
3	1	benchmarks/benchmark_prefix_caching.py

[0cc566ca8] Andy Dai 2024-10-04 [Misc] Add random seed for prefix cache benchmark (#9081)
3	1	benchmarks/benchmark_prefix_caching.py

[05c531be4] Andy Dai 2024-10-04 [Misc] Improved prefix cache example (#9077)
3	9	examples/offline_inference_with_prefix.py

[fbb74420e] Kuntai Du 2024-10-04 [CI] Update performance benchmark: upgrade trt-llm to r24.07, and add SGLang (#7412)
28	0	.buildkite/nightly-benchmarks/nightly-annotation.md
36	42	.buildkite/nightly-benchmarks/nightly-descriptions.md
87	11	.buildkite/nightly-benchmarks/nightly-pipeline.yaml
0	76	.buildkite/nightly-benchmarks/run-nightly-suite.sh
95	0	.buildkite/nightly-benchmarks/scripts/generate-nightly-markdown.py
241	0	.buildkite/nightly-benchmarks/scripts/launch-server.sh
0	102	.buildkite/nightly-benchmarks/scripts/launch-trt-server.sh
48	10	.buildkite/nightly-benchmarks/scripts/nightly-annotate.sh
0	135	.buildkite/nightly-benchmarks/scripts/plot-nightly-results.py
0	218	.buildkite/nightly-benchmarks/scripts/run-lmdeploy-nightly.sh
357	0	.buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh
0	216	.buildkite/nightly-benchmarks/scripts/run-tgi-nightly.sh
0	214	.buildkite/nightly-benchmarks/scripts/run-trt-nightly.sh
0	221	.buildkite/nightly-benchmarks/scripts/run-vllm-nightly.sh
8	1	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
237	30	.buildkite/nightly-benchmarks/tests/nightly-tests.json
7	0	benchmarks/backend_request_func.py
8	0	benchmarks/benchmark_serving.py

[05d686432] ElizaWszola 2024-10-04 [Kernel] Zero point support in fused MarlinMoE kernel + AWQ Fused MoE (#8973)
2	0	CMakeLists.txt
244	53	csrc/moe/marlin_kernels/marlin_moe_kernel.h
31	0	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.cu
20	0	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.h
7	5	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.cu
5	5	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.h
7	5	csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.cu
5	5	csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.h
56	28	csrc/moe/marlin_moe_ops.cu
1	1	csrc/moe/torch_bindings.cpp
1	1	csrc/quantization/gptq_marlin/gptq_marlin.cu
160	0	tests/kernels/test_awq_marlin.py
18	61	tests/kernels/test_moe.py
45	0	tests/kernels/utils.py
1	0	tests/weight_loading/models-large.txt
14	1	tests/weight_loading/run_model_weight_loading_test.sh
20	5	vllm/_custom_ops.py
107	31	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
195	9	vllm/model_executor/layers/quantization/awq_marlin.py
6	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
6	6	vllm/model_executor/layers/quantization/gptq_marlin.py
15	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py
3	1	vllm/model_executor/model_loader/utils.py

[0dcc8cbe5] Flávia Béo 2024-10-04 Adds truncate_prompt_tokens param for embeddings creation (#8999)
61	0	tests/entrypoints/openai/test_embedding.py
1	0	vllm/entrypoints/openai/protocol.py
14	5	vllm/entrypoints/openai/serving_embedding.py

[26aa325f4] Roger Wang 2024-10-04 [Core][VLM] Test registration for OOT multimodal models (#8717)
15	3	docs/source/models/adding_model.rst
33	0	find_cuda_init.py
25	5	tests/conftest.py
3	1	tests/entrypoints/openai/test_audio.py
10	3	tests/entrypoints/openai/test_vision.py
38	0	tests/models/test_oot_registration.py
8	20	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py
28	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py
19	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_opt.py
2	0	vllm/engine/arg_utils.py
0	3	vllm/engine/llm_engine.py
46	14	vllm/model_executor/models/registry.py

[e5dc713c2] Varad Ahirwadkar 2024-10-04 [Hardware][PowerPC] Make oneDNN dependency optional for Power (#9039)
6	1	cmake/cpu_extension.cmake

[36eecfbdd] Simon Mo 2024-10-04 Remove AMD Ray Summit Banner (#9075)
1	11	README.md

[9ade8bbc8] Prashant Gupta 2024-10-04 [Model] add a bunch of supported lora modules for mixtral (#9008)
5	0	tests/lora/conftest.py
62	16	tests/lora/test_mixtral.py
2	4	vllm/model_executor/models/mixtral.py

[22482e495] Lucas Wilkinson 2024-10-04 [Bugfix] Flash attention arches not getting set properly (#9062)
11	0	CMakeLists.txt

[3d826d2c5] whyiug 2024-10-04 [Bugfix] Reshape the dimensions of the input image embeddings in Qwen2VL (#9071)
3	0	vllm/model_executor/models/qwen2_vl.py

[0e36fd490] Cyrus Leung 2024-10-04 [Misc] Move registry to its own file (#9064)
1	1	docs/source/models/adding_model.rst
2	2	tests/models/test_registry.py
1	2	vllm/lora/models.py
2	3	vllm/model_executor/model_loader/loader.py
12	321	vllm/model_executor/models/__init__.py
2	4	vllm/model_executor/models/jamba.py
320	0	vllm/model_executor/models/registry.py
1	2	vllm/worker/model_runner.py

[0f6d7a9a3] Murali Andoorveedu 2024-10-03 [Models] Add remaining model PP support (#7168)
3	1	.buildkite/test-pipeline.yaml
81	16	docs/source/models/supported_models.rst
2	2	requirements-test.txt
240	57	tests/distributed/test_pipeline_parallel.py
49	3	tests/models/test_registry.py
19	8	tests/utils.py
19	43	vllm/config.py
125	22	vllm/model_executor/models/__init__.py
41	18	vllm/model_executor/models/arctic.py
44	21	vllm/model_executor/models/baichuan.py
39	22	vllm/model_executor/models/blip2.py
34	16	vllm/model_executor/models/bloom.py
63	30	vllm/model_executor/models/chameleon.py
39	22	vllm/model_executor/models/chatglm.py
41	18	vllm/model_executor/models/commandr.py
36	15	vllm/model_executor/models/dbrx.py
7	2	vllm/model_executor/models/decilm.py
43	20	vllm/model_executor/models/deepseek.py
13	7	vllm/model_executor/models/deepseek_v2.py
12	23	vllm/model_executor/models/exaone.py
33	19	vllm/model_executor/models/falcon.py
34	39	vllm/model_executor/models/fuyu.py
42	21	vllm/model_executor/models/gemma.py
41	18	vllm/model_executor/models/gemma2.py
11	15	vllm/model_executor/models/gpt2.py
36	18	vllm/model_executor/models/gpt_bigcode.py
35	15	vllm/model_executor/models/gpt_j.py
33	15	vllm/model_executor/models/gpt_neox.py
4	4	vllm/model_executor/models/granite.py
2	2	vllm/model_executor/models/granitemoe.py
141	9	vllm/model_executor/models/interfaces.py
5	5	vllm/model_executor/models/internlm2.py
27	22	vllm/model_executor/models/internvl.py
10	14	vllm/model_executor/models/jais.py
49	65	vllm/model_executor/models/llama.py
16	5	vllm/model_executor/models/llama_embedding.py
31	16	vllm/model_executor/models/llava.py
33	18	vllm/model_executor/models/llava_next.py
34	21	vllm/model_executor/models/llava_next_video.py
39	25	vllm/model_executor/models/llava_onevision.py
51	23	vllm/model_executor/models/minicpm.py
17	9	vllm/model_executor/models/minicpm3.py
16	5	vllm/model_executor/models/minicpmv.py
13	22	vllm/model_executor/models/mixtral.py
42	19	vllm/model_executor/models/mixtral_quant.py
34	15	vllm/model_executor/models/mpt.py
10	19	vllm/model_executor/models/nemotron.py
42	20	vllm/model_executor/models/olmo.py
57	21	vllm/model_executor/models/olmoe.py
47	23	vllm/model_executor/models/opt.py
44	16	vllm/model_executor/models/orion.py
33	26	vllm/model_executor/models/paligemma.py
36	18	vllm/model_executor/models/persimmon.py
36	17	vllm/model_executor/models/phi.py
40	20	vllm/model_executor/models/phi3_small.py
96	85	vllm/model_executor/models/phi3v.py
55	17	vllm/model_executor/models/phimoe.py
32	16	vllm/model_executor/models/pixtral.py
26	28	vllm/model_executor/models/qwen.py
13	22	vllm/model_executor/models/qwen2.py
13	21	vllm/model_executor/models/qwen2_moe.py
38	35	vllm/model_executor/models/qwen2_vl.py
2	2	vllm/model_executor/models/siglip.py
18	29	vllm/model_executor/models/solar.py
38	16	vllm/model_executor/models/stablelm.py
39	18	vllm/model_executor/models/starcoder2.py
33	18	vllm/model_executor/models/ultravox.py
17	13	vllm/model_executor/models/utils.py
39	17	vllm/model_executor/models/xverse.py

[303d44790] Michael Goin 2024-10-03 [Misc] Enable multi-step output streaming by default (#9047)
9	5	vllm/engine/arg_utils.py

[aeb37c2a7] Lucas Wilkinson 2024-10-03 [CI/Build] Per file CUDA Archs (improve wheel size and dev build times) (#8845)
169	55	CMakeLists.txt
172	103	cmake/utils.cmake
5	0	csrc/core/registration.h
5	0	csrc/moe/marlin_moe_ops.cu
0	15	csrc/moe/marlin_moe_ops.h
1	2	csrc/moe/torch_bindings.cpp
0	68	csrc/ops.h
51	25	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
6	0	csrc/quantization/fp8/fp8_marlin.cu
24	37	csrc/quantization/gptq_marlin/awq_marlin_repack.cu
6	0	csrc/quantization/gptq_marlin/gptq_marlin.cu
27	41	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
1	1	csrc/quantization/machete/generate.py
3	4	csrc/quantization/machete/machete_prepack_kernel.cuh
2	2	csrc/quantization/machete/machete_prepack_launcher.cuh
9	5	csrc/quantization/machete/machete_pytorch.cu
5	0	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
5	0	csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu
5	0	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
12	12	csrc/torch_bindings.cpp
311	0	tools/report_build_time_ninja.py
9	0	vllm/_custom_ops.py

[3dbb215b3] 代君 2024-10-04 [Frontend][Feature] support tool calling for internlm/internlm2_5-7b-chat model (#8405)
2	1	docs/requirements-docs.txt
72	2	docs/source/serving/openai_compatible_server.md
60	0	examples/tool_chat_template_internlm2_tool.jinja
13	1	tests/tool_use/utils.py
10	0	vllm/entrypoints/openai/api_server.py
13	1	vllm/entrypoints/openai/cli_args.py
20	18	vllm/entrypoints/openai/serving_chat.py
4	3	vllm/entrypoints/openai/tool_parsers/__init__.py
101	4	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
10	4	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
208	0	vllm/entrypoints/openai/tool_parsers/internlm2_tool_parser.py
8	4	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py
12	8	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[2838d6b38] Domen Vreš 2024-10-04 [Bugfix] Weight loading fix for OPT model (#9042)
1	1	vllm/model_executor/models/opt.py

[91add85ec] sroy745 2024-10-03 Fix failing spec decode test (#9054)
1	0	tests/spec_decode/e2e/test_compatibility.py

[9aaf14c62] youkaichao 2024-10-03 [misc] add forward context for attention (#9029)
7	49	tests/kernels/test_flash_attn.py
178	251	vllm/attention/backends/flash_attn.py
2	2	vllm/attention/backends/flashinfer.py
22	0	vllm/forward_context.py
12	10	vllm/spec_decode/draft_model_runner.py
3	1	vllm/worker/embedding_model_runner.py
13	11	vllm/worker/enc_dec_model_runner.py
13	10	vllm/worker/model_runner.py

[63e39937f] xendo 2024-10-03 [Frontend] [Neuron] Parse literals out of override-neuron-config (#8959)
34	14	tests/engine/test_arg_utils.py
3	6	vllm/engine/arg_utils.py

[f5d72b2fc] sroy745 2024-10-03 [Core] Make BlockSpaceManagerV2 the default BlockManager to use. (#8678)
1	1	vllm/config.py
7	4	vllm/engine/arg_utils.py

[83caf35e0] Guillaume Calmettes 2024-10-03 [BugFix] Enforce Mistral ToolCall id constraint when using the Mistral tool call parser (#9020)
2	2	tests/tool_use/test_parallel_tool_calls.py
2	2	tests/tool_use/test_tool_calls.py
18	2	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[01843c89b] Divakar Verma 2024-10-02 [Misc] log when using default MoE config (#8971)
3	0	vllm/model_executor/layers/fused_moe/fused_moe.py

[19a4dd099] Travis Johnson 2024-10-02 [Bugfix] example template should not add parallel_tool_prompt if tools is none (#9007)
1	2	examples/tool_chat_template_mistral_parallel.jinja

[18c2e30c5] Nick Hill 2024-10-03 [Doc] Update Granite model docs (#9025)
6	2	docs/source/models/supported_models.rst

[19f0d2579] Shawn Tan 2024-10-02 [Model]  Adding Granite MoE. (#8206)
39	0	tests/models/decoder_only/language/test_granitemoe.py
1	0	vllm/model_executor/models/__init__.py
4	3	vllm/model_executor/models/granite.py
448	0	vllm/model_executor/models/granitemoe.py

[f58d4fccc] Sergey Shlyapnikov 2024-10-03 [OpenVINO] Enable GPU support for OpenVINO vLLM backend (#8192)
29	6	docs/source/getting_started/openvino-installation.rst
3	2	requirements-openvino.txt
32	8	vllm/attention/backends/openvino.py
6	0	vllm/envs.py
53	20	vllm/executor/openvino_executor.py
10	16	vllm/model_executor/model_loader/openvino.py
6	5	vllm/worker/openvino_model_runner.py
307	50	vllm/worker/openvino_worker.py

[afb050b29] Varun Sundar Rabindranath 2024-10-02 [Core] CUDA Graphs for Multi-Step + Chunked-Prefill (#8645)
11	0	csrc/prepare_inputs/advance_step.cu
28	20	vllm/attention/backends/flash_attn.py
58	14	vllm/worker/model_runner.py

[7f60520de] Alex Brooks 2024-10-02 [Misc] Update Default Image Mapper Error Log (#8977)
6	1	vllm/multimodal/image.py

[563649aaf] afeldman-nm 2024-10-02 [Core] Combined support for multi-step scheduling, chunked prefill & prefix caching (#8804)
158	0	tests/multi_step/test_correctness_llm.py
22	13	vllm/core/scheduler.py
0	4	vllm/engine/arg_utils.py

[157020386] Lily Liu 2024-10-01 [Spec Decode] (1/2) Remove batch expansion (#8839)
1	1	.buildkite/test-pipeline.yaml
1	1	tests/samplers/test_sampler.py
44	0	tests/spec_decode/e2e/test_integration.py
49	0	tests/spec_decode/e2e/test_medusa_correctness.py
43	0	tests/spec_decode/e2e/test_mlp_correctness.py
46	0	tests/spec_decode/e2e/test_ngram_correctness.py
0	1	tests/spec_decode/test_multi_step_worker.py
65	0	tests/spec_decode/test_scorer.py
5	4	tests/spec_decode/test_spec_decode_worker.py
16	13	tests/spec_decode/utils.py
6	0	vllm/attention/backends/blocksparse_attn.py
29	7	vllm/attention/backends/flash_attn.py
0	2	vllm/attention/backends/flashinfer.py
8	0	vllm/attention/backends/rocm_flash_attn.py
2	1	vllm/attention/backends/utils.py
6	0	vllm/attention/backends/xformers.py
7	0	vllm/config.py
8	0	vllm/engine/arg_utils.py
14	4	vllm/engine/llm_engine.py
6	2	vllm/engine/output_processor/interfaces.py
11	8	vllm/engine/output_processor/multi_step.py
1	1	vllm/model_executor/layers/sampler.py
12	11	vllm/model_executor/sampling_metadata.py
0	7	vllm/spec_decode/batch_expansion.py
0	2	vllm/spec_decode/draft_model_runner.py
7	0	vllm/spec_decode/interfaces.py
80	0	vllm/spec_decode/mqa_scorer.py
54	7	vllm/spec_decode/spec_decode_worker.py
10	27	vllm/worker/model_runner.py

[22f5851b8] vlsav 2024-10-01 Update benchmark_serving.py to read and write json-datasets, results in UTF8, for better compatibility with Windows (#8997)
3	3	benchmarks/benchmark_serving.py

[4f341bd4b] Cyrus Leung 2024-10-02 [Doc] Update list of supported models (#8987)
16	4	docs/source/models/supported_models.rst
5	4	vllm/model_executor/models/__init__.py

[35bd21516] Sebastian Schoennenbeck 2024-10-01 [Core] [Frontend] Priority scheduling for embeddings and in the OpenAI-API (#8965)
4	0	vllm/engine/async_llm_engine.py
5	0	vllm/engine/multiprocessing/__init__.py
16	4	vllm/engine/multiprocessing/client.py
3	1	vllm/engine/protocol.py
22	0	vllm/entrypoints/openai/protocol.py
1	0	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/entrypoints/openai/serving_completion.py
1	0	vllm/entrypoints/openai/serving_embedding.py

[1fe0a4264] Alex Brooks 2024-10-01 [Bugfix] Fix Token IDs Reference for MiniCPM-V When Images are Provided With No Placeholders (#8991)
1	1	vllm/model_executor/models/minicpmv.py

[bc4eb65b5] Isotr0py 2024-10-01 [Bugfix] Fix Fuyu tensor parallel inference (#8986)
3	1	tests/distributed/test_pipeline_parallel.py
2	1	vllm/model_executor/models/fuyu.py
10	10	vllm/model_executor/models/persimmon.py

[82f3937e5] Divakar Verma 2024-09-30 [Misc] add process_weights_after_loading for DummyLoader (#8969)
12	0	vllm/model_executor/model_loader/loader.py

[7da248759] youkaichao 2024-09-30 [torch.compile] fix tensor alias (#8982)
2	1	vllm/worker/embedding_model_runner.py
2	1	vllm/worker/enc_dec_model_runner.py
5	1	vllm/worker/model_runner.py

[aaccca2b4] Kevin H. Luu 2024-09-30 [CI/Build] Fix machete generated kernel files ordering (#8976)
7	1	csrc/quantization/machete/generate.py

[062c89e7c] Joe Runde 2024-09-30 [Frontend][Core] Move guided decoding params into sampling params (#8252)
43	23	tests/entrypoints/llm/test_guided_generate.py
49	0	tests/model_executor/conftest.py
24	11	tests/{entrypoints/openai => model_executor}/test_guided_processors.py
44	0	vllm/engine/async_llm_engine.py
54	0	vllm/engine/llm_engine.py
14	0	vllm/engine/multiprocessing/client.py
30	18	vllm/entrypoints/llm.py
52	30	vllm/entrypoints/openai/protocol.py
0	5	vllm/entrypoints/openai/serving_chat.py
0	4	vllm/entrypoints/openai/serving_completion.py
1	12	vllm/entrypoints/openai/serving_engine.py
18	50	vllm/model_executor/guided_decoding/__init__.py
1	0	vllm/model_executor/guided_decoding/guided_fields.py
16	74	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
19	53	vllm/model_executor/guided_decoding/outlines_decoding.py
76	1	vllm/sampling_params.py

[bce324487] Lily Liu 2024-09-30 [CI][SpecDecode] Fix spec decode tests, use flash attention backend for spec decode CI tests. (#8975)
0	2	.buildkite/test-pipeline.yaml
4	1	tests/spec_decode/test_multi_step_worker.py

[1425a1bcf] Kevin H. Luu 2024-09-30 [ci] Add CODEOWNERS for test directories  (#8795)
10	2	.buildkite/test-pipeline.yaml
19	0	.github/CODEOWNERS

[1cabfcefb] Jee Jee Li 2024-09-30 [Misc] Adjust max_position_embeddings for LoRA compatibility (#8957)
9	2	vllm/worker/model_runner.py

[be76e5aab] Sebastian Schoennenbeck 2024-09-30 [Core] Make scheduling policy settable via EngineArgs (#8956)
14	2	vllm/engine/arg_utils.py

[2ae25f79c] Isotr0py 2024-09-30 [Model] Expose InternVL2 max_dynamic_patch as a mm_processor_kwarg (#8946)
1	0	examples/offline_inference_vision_language_multi_image.py
89	61	vllm/model_executor/models/internvl.py

[8e60afa15] Jee Jee Li 2024-09-30 [Model][LoRA]LoRA support added for MiniCPMV2.6 (#8943)
15	9	vllm/model_executor/models/idefics2_vision_model.py
34	67	vllm/model_executor/models/minicpmv.py
0	804	vllm/model_executor/models/na_vit.py

[b6d739257] Roger Wang 2024-09-29 [Misc][CI/Build] Include `cv2` via `mistral_common[opencv]`  (#8951)
1	1	requirements-common.txt
0	1	setup.py

[e01ab595d] whyiug 2024-09-30 [Model] support input embeddings for qwen2vl (#8856)
1	1	docs/source/models/supported_models.rst
17	0	docs/source/models/vlm.rst
118	70	vllm/model_executor/models/qwen2_vl.py

[f13a07b1f] Mor Zusman 2024-09-30 [Kernel][Model] Varlen prefill + Prefill chunking support for mamba kernels and Jamba model (#8533)
211	316	csrc/mamba/causal_conv1d/causal_conv1d.cu
10	0	csrc/mamba/causal_conv1d/causal_conv1d.h
9	20	csrc/mamba/mamba_ssm/selective_scan.h
179	118	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
18	13	csrc/ops.h
11	6	csrc/torch_bindings.cpp
218	128	tests/kernels/test_causal_conv1d.py
198	69	tests/kernels/test_mamba_ssm.py
115	9	tests/models/decoder_only/language/test_jamba.py
39	38	vllm/_custom_ops.py
42	45	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
59	35	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
67	97	vllm/model_executor/models/jamba.py

[6c9ba48fd] danieljannai21 2024-09-29 [Frontend] Added support for HF's new `continue_final_message` parameter (#8942)
23	7	tests/entrypoints/openai/test_chat_template.py
34	22	tests/entrypoints/openai/test_tokenization.py
8	0	vllm/entrypoints/chat_utils.py
6	0	vllm/entrypoints/llm.py
28	0	vllm/entrypoints/openai/protocol.py
4	2	vllm/entrypoints/openai/serving_chat.py
2	0	vllm/entrypoints/openai/serving_tokenization.py

[1fb9c1b0b] juncheoll 2024-09-30 [Misc] Fix typo in BlockSpaceManagerV1 (#8944)
1	1	vllm/core/block_manager_v1.py

[31f46a0d3] Nick Hill 2024-09-29 [BugFix] Fix seeded random sampling with encoder-decoder models (#8870)
3	1	vllm/worker/enc_dec_model_runner.py

[3d49776bb] Jee Jee Li 2024-09-29 [Model][LoRA]LoRA support added for MiniCPMV2.5 (#7199)
5	0	tests/lora/conftest.py
71	0	tests/lora/test_minicpmv.py
95	0	tests/lora/test_minicpmv_tp.py
41	4	vllm/lora/models.py
72	22	vllm/model_executor/models/minicpmv.py
69	0	vllm/model_executor/models/module_mapping.py
20	2	vllm/model_executor/models/utils.py
5	3	vllm/worker/model_runner.py

[bc2ef1f77] Zilin Zhu 2024-09-29 [Model] Support Qwen2.5-Math-RM-72B (#8896)
7	0	vllm/model_executor/layers/pooler.py
1	0	vllm/model_executor/models/__init__.py
162	0	vllm/model_executor/models/qwen2_rm.py

[2e7fe7e79] Tyler Michael Smith 2024-09-28 [Build/CI] Set FETCHCONTENT_BASE_DIR to one location for better caching (#8930)
1	0	.gitignore
9	0	CMakeLists.txt

[26a68d5d7] Cyrus Leung 2024-09-29 [CI/Build] Add test decorator for minimum GPU memory (#8925)
4	5	tests/lora/test_baichuan.py
8	9	tests/lora/test_quant_model.py
2	11	tests/models/decoder_only/language/test_phimoe.py
4	9	tests/models/decoder_only/vision_language/test_llava_onevision.py
3	9	tests/models/decoder_only/vision_language/test_pixtral.py
20	22	tests/models/encoder_decoder/vision_language/test_mllama.py
33	2	tests/utils.py
5	0	vllm/platforms/cpu.py
12	0	vllm/platforms/cuda.py
6	0	vllm/platforms/interface.py
5	0	vllm/platforms/rocm.py
4	0	vllm/platforms/tpu.py
8	6	vllm/platforms/xpu.py
3	0	vllm/utils.py

[d081da006] ElizaWszola 2024-09-29 [Bugfix] Fix Marlin MoE act order when is_k_full == False (#8741)
3	0	csrc/core/exception.hpp
6	6	csrc/moe/marlin_moe_ops.cu
23	9	tests/kernels/test_moe.py
5	3	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py

[5bf8789b2] sroy745 2024-09-28 [Bugfix] Block manager v2 with preemption and lookahead slots (#8824)
7	2	tests/basic_correctness/test_preemption.py
46	1	tests/core/block/test_block_manager_v2.py
10	9	tests/core/block/test_naive_block.py
13	12	tests/core/block/test_prefix_caching_block.py
7	10	vllm/core/block/cpu_gpu_block_allocator.py
3	7	vllm/core/block/interfaces.py
10	25	vllm/core/block/naive_block.py
15	26	vllm/core/block/prefix_caching_block.py
22	24	vllm/core/block_manager_v2.py

[d1537039c] Russell Bryant 2024-09-28 [Core] Improve choice of Python multiprocessing method (#8823)
9	2	vllm/executor/multiproc_gpu_executor.py
10	7	vllm/executor/multiproc_worker_utils.py
26	0	vllm/scripts.py
7	0	vllm/utils.py

[cc276443b] youkaichao 2024-09-28 [doc] organize installation doc and expose per-commit docker (#8931)
24	12	docs/source/getting_started/installation.rst

[e585b583a] Chen Zhang 2024-09-28 [Bugfix] Support testing prefill throughput with benchmark_serving.py --hf-output-len 1 (#8891)
4	5	benchmarks/benchmark_serving.py

[090e945e3] Edouard B. 2024-09-28 [Frontend] Make beam search emulator temperature modifiable (#8928)
3	1	vllm/entrypoints/llm.py

[e1a3f5e83] Cyrus Leung 2024-09-29 [CI/Build] Update models tests & examples (#8874)
32	19	.buildkite/test-pipeline.yaml
19	9	examples/offline_inference_vision_language.py
10	3	examples/offline_inference_vision_language_multi_image.py
44	40	tests/conftest.py
15	14	tests/models/decoder_only/vision_language/test_llava_onevision.py
1	1	tests/models/decoder_only/vision_language/test_minicpmv.py
1	1	tests/models/decoder_only/vision_language/test_phi3v.py
1	1	tests/models/decoder_only/vision_language/test_qwen.py
35	0	tests/models/encoder_decoder/vision_language/test_broadcast.py
69	84	tests/models/encoder_decoder/vision_language/test_mllama.py
8	1	tests/models/utils.py
2	10	vllm/inputs/registry.py
2	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[19d02ff93] Varun Sundar Rabindranath 2024-09-28 [Bugfix] Fix PP for Multi-Step (#8887)
82	0	tests/multi_step/test_correctness_async_llm.py
26	12	tests/utils.py
3	0	vllm/engine/output_processor/multi_step.py
9	1	vllm/worker/model_runner.py
10	2	vllm/worker/multi_step_model_runner.py

[39d3f8d94] tastelikefeet 2024-09-28 [Bugfix] Fix code for downloading models from modelscope (#8443)
17	0	vllm/transformers_utils/__init__.py

[b0298aa8c] Cyrus Leung 2024-09-28 [Misc] Remove vLLM patch of `BaichuanTokenizer` (#8921)
1	15	vllm/transformers_utils/tokenizer.py
2	3	vllm/transformers_utils/tokenizers/__init__.py
0	255	vllm/transformers_utils/tokenizers/baichuan.py

[260024a37] Tyler Titsworth 2024-09-27 [Bugfix][Intel] Fix XPU Dockerfile Build (#7824)
1	1	.buildkite/run-xpu-test.sh
3	1	.dockerignore
38	9	Dockerfile.xpu
1	1	requirements-common.txt
6	2	requirements-xpu.txt
2	0	setup.py
12	0	vllm/platforms/__init__.py
4	0	vllm/platforms/interface.py
20	0	vllm/platforms/xpu.py

[d86f6b2af] youkaichao 2024-09-27 [misc] fix wheel name (#8919)
3	2	.buildkite/release-pipeline.yaml
12	8	docs/source/getting_started/installation.rst

[bd429f2b7] Sebastian Schoennenbeck 2024-09-28 [Core] Priority-based scheduling in async engine (#8850)
23	2	vllm/engine/async_llm_engine.py
1	1	vllm/engine/llm_engine.py

[18e60d7d1] youkaichao 2024-09-27 [misc][distributed] add VLLM_SKIP_P2P_CHECK flag (#8911)
4	0	vllm/distributed/device_communicators/custom_all_reduce.py
8	0	vllm/envs.py

[c2ec430ab] Varun Sundar Rabindranath 2024-09-27 [Core] Multi-Step + Single Step Prefills via Chunked Prefill code path (#8378)
1	1	csrc/prepare_inputs/advance_step.cu
9	0	tests/multi_step/test_correctness_async_llm.py
4	0	tests/multi_step/test_correctness_llm.py
27	5	vllm/attention/backends/flash_attn.py
12	8	vllm/attention/backends/flashinfer.py
10	3	vllm/config.py
9	4	vllm/core/block/block_table.py
6	1	vllm/core/block_manager_v1.py
4	1	vllm/core/block_manager_v2.py
3	1	vllm/core/embedding_model_block_manager.py
3	1	vllm/core/interfaces.py
101	33	vllm/core/scheduler.py
7	3	vllm/engine/arg_utils.py
8	1	vllm/engine/async_llm_engine.py
113	17	vllm/engine/llm_engine.py
1	0	vllm/engine/output_processor/multi_step.py
43	3	vllm/sequence.py
150	25	vllm/worker/multi_step_model_runner.py
3	2	vllm/worker/multi_step_worker.py

[c5d55356f] Lucas Wilkinson 2024-09-27 [Bugfix] fix for deepseek w4a16 (#8906)
5	4	vllm/model_executor/layers/quantization/kernels/marlin.py

[172d1cd27] Luka Govedič 2024-09-27 [Kernel] AQ AZP 4/4: Integrate asymmetric quantization to linear method (#7271)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-INT8-compressed-tensors-asym.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
6	1	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
27	9	tests/quantization/test_compressed_tensors.py
10	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
52	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
17	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[a9b15c606] youkaichao 2024-09-27 [torch.compile] use empty tensor instead of None for profiling (#8875)
6	2	tests/kernels/test_encoder_decoder_attn.py
4	2	vllm/attention/backends/blocksparse_attn.py
4	2	vllm/attention/backends/flash_attn.py
3	3	vllm/attention/backends/flashinfer.py
6	3	vllm/attention/backends/ipex_attn.py
7	5	vllm/attention/backends/pallas.py
4	2	vllm/attention/backends/rocm_flash_attn.py
6	3	vllm/attention/backends/torch_sdpa.py
5	3	vllm/attention/backends/xformers.py
7	1	vllm/worker/embedding_model_runner.py
7	1	vllm/worker/enc_dec_model_runner.py
7	1	vllm/worker/model_runner.py
2	2	vllm/worker/tpu_model_runner.py
9	1	vllm/worker/tpu_worker.py
7	1	vllm/worker/xpu_model_runner.py

[8df2dc3c8] Brittany 2024-09-27 [TPU] Update pallas.py to support trillium (#8871)
1	1	vllm/attention/backends/pallas.py

[6d792d2f3] Isotr0py 2024-09-27 [Bugfix][VLM] Fix Fuyu batching inference with `max_num_seqs>1` (#8892)
2	4	tests/models/decoder_only/vision_language/test_fuyu.py
35	16	vllm/model_executor/models/fuyu.py

[0e088750a] Peter Pan 2024-09-27 [MISC] Fix invalid escape sequence '\' (#8830)
2	2	benchmarks/benchmark_serving.py

[dc4e3df5c] youkaichao 2024-09-27 [misc] fix collect env (#8894)
14	4	collect_env.py

[3b00b9c26] Cyrus Leung 2024-09-27 [Core] rename`PromptInputs` and `inputs` (#8876)
4	4	benchmarks/benchmark_latency.py
1	1	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/dev/offline_inference/llm_inputs.rst
1	1	docs/source/models/vlm.rst
5	3	tests/async_engine/test_async_llm_engine.py
0	34	tests/entrypoints/llm/test_encode.py
0	37	tests/entrypoints/llm/test_generate.py
6	6	tests/mq_llm_engine/test_error_handling.py
1	1	tests/mq_llm_engine/utils.py
2	2	vllm/__init__.py
92	18	vllm/engine/async_llm_engine.py
45	7	vllm/engine/llm_engine.py
58	3	vllm/engine/multiprocessing/__init__.py
82	13	vllm/engine/multiprocessing/client.py
1	1	vllm/engine/multiprocessing/engine.py
4	4	vllm/engine/protocol.py
35	33	vllm/entrypoints/llm.py
17	3	vllm/inputs/__init__.py
31	22	vllm/inputs/data.py
11	11	vllm/inputs/parse.py
43	43	vllm/inputs/preprocess.py

[344cd2b6f] Maximilien de Bayser 2024-09-26 [Feature] Add support for Llama 3.1 and 3.2 tool use (#8343)
24	2	docs/source/serving/openai_compatible_server.md
94	0	examples/tool_chat_template_llama3.1_json.jinja
93	0	examples/tool_chat_template_llama3.2_json.jinja
10	7	tests/tool_use/test_chat_completions.py
15	3	tests/tool_use/test_parallel_tool_calls.py
58	13	tests/tool_use/utils.py
1	1	vllm/entrypoints/openai/cli_args.py
3	0	vllm/entrypoints/openai/serving_chat.py
5	1	vllm/entrypoints/openai/tool_parsers/__init__.py
273	0	vllm/entrypoints/openai/tool_parsers/llama_tool_parser.py

[1b49148e4] Cyrus Leung 2024-09-27 [Installation] Allow lower versions of FastAPI to maintain Ray 2.9 compatibility (#8764)
2	2	requirements-common.txt

[4b377d6fe] Nick Hill 2024-09-27 [BugFix] Fix test breakages from transformers 4.45 upgrade (#8829)
3	6	.buildkite/test-pipeline.yaml
0	1	tests/conftest.py
0	7	tests/distributed/test_pipeline_parallel.py
4	4	tests/engine/test_custom_executor.py
6	0	tests/entrypoints/openai/test_serving_chat.py
2	2	tests/lora/test_tokenizer_group.py
0	4	tests/models/decoder_only/language/test_granite.py
0	5	tests/models/decoder_only/vision_language/test_llava_next_video.py
5	8	tests/models/decoder_only/vision_language/test_llava_onevision.py
0	6	tests/models/test_registry.py
15	3	tests/samplers/test_sampler.py
2	2	vllm/entrypoints/openai/serving_chat.py
25	1	vllm/transformers_utils/tokenizer.py

[71d21c73a] Tyler Michael Smith 2024-09-26 [Bugfix] Fixup advance_step.cu warning (#8815)
2	2	csrc/prepare_inputs/advance_step.cu

[ee2da3e9e] Chirag Jain 2024-09-27 fix validation: Only set tool_choice `auto` if at least one tool is provided (#8568)
71	0	tests/tool_use/test_chat_completion_request_validations.py
1	1	vllm/entrypoints/openai/protocol.py

[e2f6f26e8] Tyler Michael Smith 2024-09-26 [Bugfix] Fix print_warning_once's line info (#8867)
2	1	vllm/utils.py

[b28d2104d] Michael Goin 2024-09-26 [Misc] Change dummy profiling and BOS fallback warns to log once (#8820)
8	6	vllm/inputs/preprocess.py
4	4	vllm/inputs/registry.py

[93d364da3] Pernekhan Utemuratov 2024-09-26 [Bugfix] Include encoder prompts len to non-stream api usage response (#8861)
2	0	vllm/entrypoints/openai/serving_chat.py

[d9cfbc891] Kevin H. Luu 2024-09-26 [ci] Soft fail Entrypoints, Samplers, LoRA, Decoder-only VLM (#8872)
4	0	.buildkite/test-pipeline.yaml

[70de39f6b] youkaichao 2024-09-26 [misc][installation] build from source without compilation (#8818)
31	3	docs/source/getting_started/installation.rst
54	0	python_only_dev.py

[68988d4e0] fyuan1316 2024-09-27 [CI/Build] Fix missing ci dependencies (#8834)
1	1	.github/workflows/scripts/build.sh

[520db4dbc] Michael Goin 2024-09-26 [Docs] Add README to the build docker image (#8825)
1	0	Dockerfile

[f70bccac7] Tyler Michael Smith 2024-09-26 [Build/CI] Upgrade to gcc 10 in the base build Docker image (#8814)
8	0	Dockerfile

[4bb98f219] Roger Wang 2024-09-26 [Misc] Update config loading for Qwen2-VL and remove Granite (#8837)
1	10	docs/source/models/supported_models.rst
1	1	vllm/model_executor/models/granite.py
2	3	vllm/model_executor/models/qwen2_vl.py
5	7	vllm/transformers_utils/config.py
4	4	vllm/transformers_utils/configs/__init__.py
0	199	vllm/transformers_utils/configs/granite.py
131	0	vllm/transformers_utils/configs/qwen2vl.py

[7193774b1] Michael Goin 2024-09-25 [Misc] Support quantization of MllamaForCausalLM (#8822)
9	2	vllm/model_executor/models/mllama.py

[e2c6e0a82] Roger Wang 2024-09-25 [Doc] Update doc for Transformers 4.45 (#8817)
3	3	docs/source/models/supported_models.rst

[770ec6024] Chen Zhang 2024-09-25 [Model] Add support for the multi-modal Llama 3.2 model (#8811)
5	0	docs/source/models/supported_models.rst
24	0	examples/offline_inference_vision_language.py
2	2	examples/openai_vision_api_client.py
1	1	requirements-common.txt
0	0	tests/models/encoder_decoder/vision_language/__init__.py
283	0	tests/models/encoder_decoder/vision_language/test_mllama.py
3	1	vllm/config.py
5	1	vllm/engine/llm_engine.py
22	6	vllm/entrypoints/chat_utils.py
2	0	vllm/entrypoints/openai/serving_chat.py
6	0	vllm/inputs/data.py
15	7	vllm/inputs/preprocess.py
48	6	vllm/inputs/registry.py
2	0	vllm/model_executor/models/__init__.py
1135	0	vllm/model_executor/models/mllama.py
6	0	vllm/multimodal/base.py
5	0	vllm/multimodal/image.py
10	2	vllm/sequence.py
13	4	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
28	0	vllm/transformers_utils/configs/mllama.py
0	1	vllm/transformers_utils/tokenizer.py
30	10	vllm/worker/enc_dec_model_runner.py
0	4	vllm/worker/utils.py

[4f1ba0844] Simon Mo 2024-09-25 Revert "rename PromptInputs and inputs with backward compatibility (#8760) (#8810)
4	4	benchmarks/benchmark_latency.py
1	1	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/dev/offline_inference/llm_inputs.rst
1	1	docs/source/models/vlm.rst
3	5	tests/async_engine/test_async_llm_engine.py
34	0	tests/entrypoints/llm/test_encode.py
37	0	tests/entrypoints/llm/test_generate.py
6	6	tests/mq_llm_engine/test_error_handling.py
1	1	tests/mq_llm_engine/utils.py
2	2	vllm/__init__.py
18	92	vllm/engine/async_llm_engine.py
7	45	vllm/engine/llm_engine.py
3	58	vllm/engine/multiprocessing/__init__.py
13	82	vllm/engine/multiprocessing/client.py
1	1	vllm/engine/multiprocessing/engine.py
4	4	vllm/engine/protocol.py
33	35	vllm/entrypoints/llm.py
3	17	vllm/inputs/__init__.py
19	29	vllm/inputs/data.py
11	11	vllm/inputs/parse.py
43	43	vllm/inputs/preprocess.py

[873edda6c] Michael Goin 2024-09-25 [Misc] Support FP8 MoE for compressed-tensors (#8588)
1	0	tests/weight_loading/models-large.txt
7	2	vllm/model_executor/layers/fused_moe/layer.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
215	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
2	2	vllm/model_executor/models/phimoe.py

[64840dfae] 科英 2024-09-26 [Frontend] MQLLMEngine supports profiling. (#8761)
7	1	vllm/engine/multiprocessing/__init__.py
18	5	vllm/engine/multiprocessing/client.py
20	1	vllm/engine/multiprocessing/engine.py

[28e1299e6] Cyrus Leung 2024-09-26 rename PromptInputs and inputs with backward compatibility (#8760)
4	4	benchmarks/benchmark_latency.py
1	1	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/dev/offline_inference/llm_inputs.rst
1	1	docs/source/models/vlm.rst
5	3	tests/async_engine/test_async_llm_engine.py
0	34	tests/entrypoints/llm/test_encode.py
0	37	tests/entrypoints/llm/test_generate.py
6	6	tests/mq_llm_engine/test_error_handling.py
1	1	tests/mq_llm_engine/utils.py
2	2	vllm/__init__.py
92	18	vllm/engine/async_llm_engine.py
45	7	vllm/engine/llm_engine.py
58	3	vllm/engine/multiprocessing/__init__.py
82	13	vllm/engine/multiprocessing/client.py
1	1	vllm/engine/multiprocessing/engine.py
4	4	vllm/engine/protocol.py
35	33	vllm/entrypoints/llm.py
17	3	vllm/inputs/__init__.py
29	19	vllm/inputs/data.py
11	11	vllm/inputs/parse.py
43	43	vllm/inputs/preprocess.py

[0c4d2ad5e] DefTruth 2024-09-26 [VLM][Bugfix] internvl with num_scheduler_steps > 1 (#8614)
6	1	vllm/model_executor/models/internvl.py

[c6f2485c8] Jee Jee Li 2024-09-26 [[Misc]] Add extra deps for openai server image (#8792)
1	1	Dockerfile

[300da0917] bnellnm 2024-09-25 [Kernel] Fullgraph and opcheck tests (#8479)
17	2	.buildkite/test-pipeline.yaml
1	1	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
2	2	csrc/torch_bindings.cpp
8	37	tests/compile/test_full_graph.py
22	0	tests/compile/test_full_graph_multi_gpu.py
13	0	tests/compile/test_full_graph_smoke.py
104	0	tests/compile/utils.py
6	0	tests/conftest.py
37	0	tests/kernels/test_aqlm.py
6	3	tests/kernels/test_attention.py
38	0	tests/kernels/test_awq.py
72	2	tests/kernels/test_causal_conv1d.py
10	0	tests/kernels/test_cutlass.py
31	30	tests/kernels/test_flash_attn.py
29	0	tests/kernels/test_fp8_quant.py
22	0	tests/kernels/test_ggml.py
29	0	tests/kernels/test_gptq.py
66	0	tests/kernels/test_mamba_ssm.py
15	0	tests/kernels/test_marlin_gemm.py
59	1	tests/kernels/test_moe.py
62	0	tests/kernels/test_rotary_embedding.py
24	0	tests/kernels/test_utils.py
37	6	tests/kernels/utils.py
31	30	vllm/_custom_ops.py
2	2	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
1	0	vllm/model_executor/layers/quantization/gptq.py

[1c046447a] Hongxia Yang 2024-09-25 [CI/Build][Bugfix][Doc][ROCm] CI fix and doc update after ROCm 6.2 upgrade (#8777)
4	1	.buildkite/test-pipeline.yaml
1	1	Dockerfile.rocm
11	1	docs/source/getting_started/amd-installation.rst

[8fae5ed7f] Woo-Yeon Lee 2024-09-25 [Misc] Fix minor typo in scheduler (#8765)
3	3	vllm/core/scheduler.py

[3368c3ab3] David Newman 2024-09-25 [Bugfix] Ray 2.9.x doesn't expose available_resources_per_node (#8767)
6	1	vllm/executor/ray_utils.py

[1ac3de09c] Adam Tilghman 2024-09-25 [Frontend] OpenAI server: propagate usage accounting to FastAPI middleware layer (#8672)
5	0	vllm/entrypoints/openai/protocol.py
23	3	vllm/entrypoints/openai/serving_chat.py
29	8	vllm/entrypoints/openai/serving_completion.py

[3e073e66f] sohamparikh 2024-09-25 [Bugfix] load fc bias from config for eagle (#8790)
10	2	vllm/model_executor/models/eagle.py

[c23953675] Isotr0py 2024-09-25 [Hardware][CPU] Enable mrope and support Qwen2-VL on CPU backend (#8770)
16	0	vllm/model_executor/models/qwen2_vl.py
83	9	vllm/worker/cpu_model_runner.py

[e3dd0692f] zifeitong 2024-09-24 [BugFix] Propagate 'trust_remote_code' setting in internvl and minicpmv (#8250)
9	6	vllm/model_executor/models/internvl.py
108	29	vllm/model_executor/models/minicpmv.py
9	6	vllm/model_executor/models/qwen.py

[fc3afc20d] sroy745 2024-09-24 Fix tests in test_chunked_prefill_scheduler which fail with BlockManager V2 (#8752)
143	82	tests/core/test_chunked_prefill_scheduler.py

[b4522474a] sasha0552 2024-09-25 [Bugfix][Kernel] Implement acquire/release polyfill for Pascal (#8776)
11	0	csrc/custom_all_reduce.cuh
7	0	csrc/custom_all_reduce_test.cu

[ee777d9c3] sroy745 2024-09-24 Fix test_schedule_swapped_simple in test_scheduler.py (#8780)
10	4	tests/core/test_scheduler.py

[6e0c9d6bd] Joe Runde 2024-09-24 [Bugfix] Use heartbeats instead of health checks (#8583)
4	11	tests/mq_llm_engine/test_error_handling.py
1	6	vllm/engine/multiprocessing/__init__.py
22	29	vllm/engine/multiprocessing/client.py
60	17	vllm/engine/multiprocessing/engine.py

[6da1ab6b4] Archit Patke 2024-09-24 [Core] Adding Priority Scheduling (#5958)
295	0	benchmarks/benchmark_prioritization.py
4	2	vllm/config.py
77	0	vllm/core/scheduler.py
20	4	vllm/engine/llm_engine.py
10	2	vllm/entrypoints/llm.py
4	0	vllm/sequence.py

[01b6f9e1f] Travis Johnson 2024-09-24 [Core][Bugfix] Support prompt_logprobs returned with speculative decoding (#8047)
1	3	tests/conftest.py
92	47	tests/spec_decode/e2e/conftest.py
58	0	tests/spec_decode/e2e/test_eagle_correctness.py
54	41	tests/spec_decode/e2e/test_logprobs.py
59	0	tests/spec_decode/e2e/test_medusa_correctness.py
56	1	tests/spec_decode/e2e/test_mlp_correctness.py
59	0	tests/spec_decode/e2e/test_ngram_correctness.py
5	4	vllm/engine/output_processor/multi_step.py
6	5	vllm/model_executor/layers/sampler.py
2	0	vllm/sequence.py
5	5	vllm/spec_decode/batch_expansion.py
51	11	vllm/spec_decode/spec_decode_worker.py
37	8	vllm/spec_decode/util.py
7	9	vllm/transformers_utils/detokenizer.py

[13f9f7a3d] Jee Jee Li 2024-09-25 [[Misc]Upgrade bitsandbytes to the latest version 0.44.0 (#8768)
1	1	docs/source/quantization/bnb.rst
10	16	examples/lora_with_quantization_inference.py
1	1	requirements-test.txt
1	1	tests/quantization/test_bitsandbytes.py
23	7	vllm/config.py
4	4	vllm/model_executor/layers/quantization/bitsandbytes.py
4	4	vllm/model_executor/model_loader/loader.py

[1e7d5c01f] youkaichao 2024-09-24 [misc] soft drop beam search (#8763)
5	0	vllm/envs.py
5	0	vllm/sampling_params.py

[2467b642d] Daniele 2024-09-24 [CI/Build] fix setuptools-scm usage (#8771)
2	5	.gitignore
0	3	pyproject.toml
5	2	setup.py

[72fc97a0f] Lucas Wilkinson 2024-09-24 [Bugfix] Fix torch dynamo fixes caused by `replace_parameters` (#8748)
8	4	vllm/model_executor/layers/quantization/utils/layer_utils.py

[2529d09b5] Andy 2024-09-24 [Frontend] Batch inference for llm.chat() API  (#8648)
27	0	examples/offline_inference_chat.py
35	0	tests/entrypoints/llm/test_generate.py
49	33	vllm/entrypoints/llm.py

[a928ded99] ElizaWszola 2024-09-24 [Kernel] Split Marlin MoE kernels into multiple files (#8661)
5	0	CMakeLists.txt
1425	0	csrc/moe/marlin_kernels/marlin_moe_kernel.h
29	0	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.cu
20	0	csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.h
29	0	csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.cu
18	0	csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.h
26	1427	csrc/moe/marlin_moe_ops.cu

[cc4325b66] Hanzhi Zhou 2024-09-24 [Bugfix] Fix potentially unsafe custom allreduce synchronization (#8558)
74	54	csrc/custom_all_reduce.cuh
9	5	csrc/custom_all_reduce_test.cu

[8ff7ced99] Alex Brooks 2024-09-24 [Model] Expose Phi3v num_crops as a mm_processor_kwarg (#8658)
14	0	examples/offline_inference_vision_language.py
13	0	examples/offline_inference_vision_language_multi_image.py
181	5	tests/models/decoder_only/vision_language/test_phi3v.py
22	9	vllm/model_executor/models/phi3v.py

[3f06bae90] Peter Salas 2024-09-24 [Core][Model] Support loading weights by ID within models (#7931)
47	13	vllm/model_executor/model_loader/loader.py
26	4	vllm/model_executor/models/ultravox.py

[b8747e8a7] Cody Yu 2024-09-23 [MISC] Skip dumping inputs when unpicklable (#8744)
9	1	vllm/worker/model_runner_base.py

[3185fb0cc] Simon Mo 2024-09-23 Revert "[Core] Rename `PromptInputs` to `PromptType`, and `inputs` to `prompt`" (#8750)
4	4	benchmarks/benchmark_latency.py
1	1	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/dev/offline_inference/llm_inputs.rst
1	1	docs/source/models/vlm.rst
6	6	tests/mq_llm_engine/test_error_handling.py
1	1	tests/mq_llm_engine/utils.py
2	2	vllm/__init__.py
13	11	vllm/engine/async_llm_engine.py
5	4	vllm/engine/llm_engine.py
2	2	vllm/engine/multiprocessing/__init__.py
11	9	vllm/engine/multiprocessing/client.py
1	1	vllm/engine/multiprocessing/engine.py
4	4	vllm/engine/protocol.py
38	42	vllm/entrypoints/llm.py
3	3	vllm/inputs/__init__.py
15	11	vllm/inputs/data.py
11	11	vllm/inputs/parse.py
43	43	vllm/inputs/preprocess.py

[0250dd68c] youkaichao 2024-09-23 re-implement beam search on top of vllm core (#8726)
20	4	benchmarks/benchmark_throughput.py
14	0	tests/conftest.py
3	3	tests/samplers/test_beam_search.py
134	2	vllm/entrypoints/llm.py

[88577ac92] sroy745 2024-09-23 Fix tests in test_scheduler.py that fail with BlockManager V2 (#8728)
260	89	tests/core/test_scheduler.py

[530821d00] Hongxia Yang 2024-09-23 [Hardware][AMD] ROCm6.2 upgrade (#8674)
19	37	Dockerfile.rocm
42	23	docs/source/getting_started/amd-installation.rst

[1a2aef3e5] Alexander Matveev 2024-09-23 Add output streaming support to multi-step + async while ensuring RequestOutput obj reuse (#8335)
5	1	tests/entrypoints/openai/test_accuracy.py
2	0	vllm/config.py
6	0	vllm/engine/arg_utils.py
28	9	vllm/engine/llm_engine.py
8	1	vllm/engine/multiprocessing/engine.py
74	22	vllm/outputs.py
19	9	vllm/sequence.py

[5f7bb5842] jiqing-feng 2024-09-24 Fix typical acceptance sampler with correct recovered token ids (#8562)
8	9	tests/samplers/test_typical_acceptance_sampler.py
9	19	vllm/model_executor/layers/typical_acceptance_sampler.py

[b05f5c923] Russell Bryant 2024-09-23 [Core] Allow IPv6 in VLLM_HOST_IP with zmq (#8575)
6	1	vllm/distributed/device_communicators/shm_broadcast.py
9	0	vllm/utils.py

[9b0e3ec97] Jee Jee Li 2024-09-24 [Kernel][LoRA]  Add assertion for punica sgmv kernels (#7585)
5	0	tests/lora/test_punica_sizes.py
5	0	tests/lora/test_punica_variation.py
1	1	vllm/lora/ops/bgmv_expand.py
1	1	vllm/lora/ops/bgmv_expand_slice.py
10	6	vllm/lora/ops/sgmv_expand.py
11	7	vllm/lora/ops/sgmv_expand_slice.py
10	6	vllm/lora/ops/sgmv_shrink.py
21	17	vllm/lora/punica.py

[86e9c8df2] Lucas Wilkinson 2024-09-23 [Kernel] (2/N) Machete - Integrate into CompressedTensorsWNA16 and GPTQMarlin (#7701)
1	0	CMakeLists.txt
61	13	benchmarks/kernels/benchmark_machete.py
1	0	benchmarks/kernels/requirements.txt
7	1	csrc/cutlass_extensions/torch_utils.hpp
2	0	csrc/ops.h
88	0	csrc/permute_cols.cu
127	46	csrc/quantization/machete/generate.py
2	1	csrc/quantization/machete/machete_mm_kernel.cuh
1	1	csrc/quantization/machete/machete_mm_launcher.cuh
1	1	csrc/quantization/machete/machete_prepack_launcher.cuh
3	0	csrc/torch_bindings.cpp
3	0	tests/kernels/test_machete_gemm.py
15	0	tests/kernels/test_permute_cols.py
18	1	vllm/_custom_ops.py
5	4	vllm/model_executor/layers/quantization/awq_marlin.py
36	78	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
44	89	vllm/model_executor/layers/quantization/gptq_marlin.py
83	0	vllm/model_executor/layers/quantization/kernels/MPLinearKernel.py
72	0	vllm/model_executor/layers/quantization/kernels/__init__.py
118	0	vllm/model_executor/layers/quantization/kernels/machete.py
132	0	vllm/model_executor/layers/quantization/kernels/marlin.py
3	0	vllm/model_executor/layers/quantization/utils/__init__.py
33	0	vllm/model_executor/layers/quantization/utils/layer_utils.py
30	0	vllm/model_executor/layers/quantization/utils/machete_utils.py
18	11	vllm/model_executor/layers/quantization/utils/marlin_utils.py
43	0	vllm/model_executor/layers/quantization/utils/quant_utils.py
58	0	vllm/model_executor/parameter.py

[ee5f34b1c] Daniele 2024-09-23 [CI/Build] use setuptools-scm to set __version__ (#4738)
3	0	.gitignore
2	3	Dockerfile
3	1	Dockerfile.cpu
13	10	Dockerfile.neuron
3	2	Dockerfile.openvino
9	3	Dockerfile.ppc64le
6	3	Dockerfile.rocm
13	4	Dockerfile.tpu
9	4	Dockerfile.xpu
1	1	docs/source/getting_started/cpu-installation.rst
9	1	pyproject.toml
2	1	requirements-build.txt
10	50	setup.py
4	3	tests/test_embedded_commit.py
2	2	vllm/__init__.py
5	7	vllm/version.py

[f2bd246c1] Jani Monoses 2024-09-23 [VLM] Fix paligemma, fuyu and persimmon with transformers 4.45 : use config.text_config.vocab_size (#8707)
1	1	vllm/model_executor/models/fuyu.py
2	1	vllm/model_executor/models/paligemma.py
6	6	vllm/model_executor/models/persimmon.py

[a79e52298] Yanyi Liu 2024-09-23 [Model] Support pp for qwen2-vl (#8696)
8	0	tests/distributed/test_pipeline_parallel.py
1	0	vllm/config.py
15	7	vllm/model_executor/models/qwen2.py
22	7	vllm/model_executor/models/qwen2_vl.py

[3e83c12b5] Li, Jiang 2024-09-23 [Bugfix][CPU] fix missing input intermediate_tensors in the cpu_model_runner (#8733)
2	0	vllm/worker/cpu_model_runner.py

[e551ca155] Isotr0py 2024-09-23 [Hardware][CPU] Refactor CPU model runner (#8729)
193	109	vllm/worker/cpu_model_runner.py

[9b8c8ba11] Alex Brooks 2024-09-23 [Core][Frontend] Support Passing Multimodal Processor Kwargs (#8657)
21	0	tests/engine/test_arg_utils.py
1	28	tests/models/decoder_only/vision_language/test_qwen.py
35	0	tests/models/utils.py
339	0	tests/multimodal/test_processor_kwargs.py
5	1	vllm/config.py
8	0	vllm/engine/arg_utils.py
2	1	vllm/engine/llm_engine.py
2	0	vllm/entrypoints/llm.py
27	11	vllm/inputs/registry.py
16	3	vllm/multimodal/base.py
8	2	vllm/multimodal/image.py
9	0	vllm/multimodal/registry.py
7	2	vllm/multimodal/video.py
0	64	vllm/transformers_utils/image_processor.py
61	4	vllm/transformers_utils/processor.py
48	0	vllm/utils.py

[d23679eb9] Yan Ma 2024-09-23 [Bugfix] fix docker build for xpu (#8652)
1	9	Dockerfile.xpu
3	3	docs/source/getting_started/xpu-installation.rst
2	2	requirements-xpu.txt

[57a0702e6] Luka Govedič 2024-09-22 [Bugfix] Fix CPU CMake build (#8723)
0	1	cmake/cpu_extension.cmake

[3dda7c225] Tyler Michael Smith 2024-09-22 [Bugfix] Avoid some bogus messages RE CUTLASS's revision when building (#8702)
4	0	CMakeLists.txt

[92ba7e747] youkaichao 2024-09-22 [misc] upgrade mistral-common (#8715)
1	1	requirements-common.txt

[d4a2ac830] youkaichao 2024-09-22 [build] enable existing pytorch (for GH200, aarch64, nightly) (#8713)
23	0	docs/source/getting_started/installation.rst
1	1	requirements-common.txt
18	0	use_existing_torch.py

[c6bd70d77] Lily Liu 2024-09-22 [SpecDec][Misc] Cleanup, remove bonus token logic. (#8701)
7	23	tests/samplers/test_rejection_sampler.py
20	59	tests/samplers/test_typical_acceptance_sampler.py
1	1	tests/spec_decode/e2e/test_medusa_correctness.py
1	8	vllm/model_executor/layers/rejection_sampler.py
1	14	vllm/model_executor/layers/spec_decode_base_sampler.py
2	7	vllm/model_executor/layers/typical_acceptance_sampler.py
1	3	vllm/spec_decode/spec_decode_worker.py

[5b5953276] litianjian 2024-09-23 [Model][VLM] Add LLaVA-Onevision model support (#8486)
6	1	docs/source/models/supported_models.rst
47	13	examples/offline_inference_vision_language.py
0	3	tests/models/decoder_only/vision_language/test_llava_next_video.py
356	0	tests/models/decoder_only/vision_language/test_llava_onevision.py
2	1	tests/models/test_registry.py
1	1	vllm/assets/video.py
4	2	vllm/model_executor/models/__init__.py
19	0	vllm/model_executor/models/clip.py
876	0	vllm/model_executor/models/llava_onevision.py
19	0	vllm/model_executor/models/siglip.py

[ca2b628b3] Huazhong Ji 2024-09-23 [MISC] rename CudaMemoryProfiler to DeviceMemoryProfiler (#8703)
1	1	vllm/utils.py
2	2	vllm/worker/model_runner.py
2	2	vllm/worker/xpu_model_runner.py

[8ca5051b9] Alex Brooks 2024-09-22 [Misc] Use NamedTuple in Multi-image example (#8705)
52	22	examples/offline_inference_vision_language_multi_image.py

[06ed2815e] Cyrus Leung 2024-09-22 [Model] Refactor BLIP/BLIP-2 to support composite model loading (#8407)
58	3	vllm/model_executor/models/blip.py
47	74	vllm/model_executor/models/blip2.py
0	3	vllm/model_executor/models/chameleon.py
4	7	vllm/model_executor/models/clip.py
0	3	vllm/model_executor/models/fuyu.py
0	8	vllm/model_executor/models/llava_next.py
0	3	vllm/model_executor/models/llava_next_video.py
0	3	vllm/model_executor/models/minicpmv.py
4	7	vllm/model_executor/models/siglip.py
0	3	vllm/model_executor/models/ultravox.py

[0e40ac9b7] youkaichao 2024-09-21 [ci][build] fix vllm-flash-attn (#8699)
3	0	CMakeLists.txt
15	0	setup.py
0	0	vllm/vllm_flash_attn/.gitkeep

[13d88d413] Isotr0py 2024-09-22 [Bugfix] Refactor composite weight loading logic (#8656)
6	10	vllm/model_executor/models/internvl.py
6	10	vllm/model_executor/models/llava.py
7	13	vllm/model_executor/models/llava_next.py
6	11	vllm/model_executor/models/llava_next_video.py
5	9	vllm/model_executor/models/paligemma.py
5	7	vllm/model_executor/models/ultravox.py
35	1	vllm/model_executor/models/utils.py

[d66ac6285] Tyler Michael Smith 2024-09-21 [Kernel][Bugfix] Delete some more useless code in marlin_moe_ops.cu (#8643)
0	3	csrc/moe/marlin_moe_ops.cu

[9dc7c6c7f] Divakar Verma 2024-09-21 [dbrx] refactor dbrx experts to extend FusedMoe class (#8518)
51	69	vllm/model_executor/models/dbrx.py

[ec4aaad81] rasmith 2024-09-21 [Kernel][Triton][AMD] Remove tl.atomic_add from awq_gemm_kernel, 2-5x speedup MI300, minor improvement for MI250 (#8646)
7	6	vllm/model_executor/layers/quantization/awq_triton.py

[4dfdf4319] Andy Dai 2024-09-21 [Doc] Fix typo in AMD installation guide (#8689)
2	2	docs/source/getting_started/amd-installation.rst

[5e85f4f82] Cyrus Leung 2024-09-21 [VLM] Use `SequenceData.from_token_counts` to create dummy data (#8687)
1	1	vllm/inputs/registry.py
6	7	vllm/model_executor/models/blip.py
5	8	vllm/model_executor/models/blip2.py
5	8	vllm/model_executor/models/chameleon.py
5	7	vllm/model_executor/models/clip.py
2	5	vllm/model_executor/models/minicpmv.py
5	9	vllm/model_executor/models/pixtral.py
5	5	vllm/model_executor/models/qwen.py
9	12	vllm/model_executor/models/qwen2_vl.py
5	7	vllm/model_executor/models/siglip.py
22	8	vllm/model_executor/models/ultravox.py
3	3	vllm/sequence.py

[71c60491f] Luka Govedič 2024-09-21 [Kernel] Build flash-attn from source (#8245)
1	0	.github/workflows/scripts/build.sh
5	0	.gitignore
73	25	CMakeLists.txt
3	0	Dockerfile
1	1	cmake/utils.cmake
0	1	requirements-cuda.txt
30	8	setup.py
7	2	vllm/attention/backends/flash_attn.py
4	4	vllm/attention/selector.py

[0faab90eb] youkaichao 2024-09-20 [beam search] add output for manually checking the correctness (#8684)
10	3	tests/samplers/test_beam_search.py

[0455c46ed] Cyrus Leung 2024-09-21 [Core] Factor out common code in `SequenceData` and `Sequence` (#8675)
7	20	tests/samplers/test_sampler.py
3	9	tests/spec_decode/utils.py
2	6	tests/test_logits_processor.py
2	5	tests/test_sequence.py
7	15	tests/worker/test_encoder_decoder_model_runner.py
5	11	tests/worker/test_model_runner.py
1	7	vllm/inputs/registry.py
37	24	vllm/sequence.py

[d4bf085ad] Kunshang Ji 2024-09-21 [MISC] add support custom_op check (#8557)
27	22	vllm/distributed/parallel_state.py
6	0	vllm/utils.py

[0057894ef] Cyrus Leung 2024-09-21 [Core] Rename `PromptInputs` and `inputs`(#8673)
4	4	benchmarks/benchmark_latency.py
1	1	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/dev/offline_inference/llm_inputs.rst
1	1	docs/source/models/vlm.rst
6	6	tests/mq_llm_engine/test_error_handling.py
1	1	tests/mq_llm_engine/utils.py
2	2	vllm/__init__.py
11	13	vllm/engine/async_llm_engine.py
4	5	vllm/engine/llm_engine.py
2	2	vllm/engine/multiprocessing/__init__.py
9	11	vllm/engine/multiprocessing/client.py
1	1	vllm/engine/multiprocessing/engine.py
4	4	vllm/engine/protocol.py
42	38	vllm/entrypoints/llm.py
3	3	vllm/inputs/__init__.py
11	15	vllm/inputs/data.py
11	11	vllm/inputs/parse.py
43	43	vllm/inputs/preprocess.py

[0f961b3ce] zyddnys 2024-09-20 [Bugfix] Fix incorrect llava next feature size calculation (#8496)
8	6	vllm/model_executor/models/llava_next.py

[7f9c8902e] omrishiv 2024-09-20 [Hardware][AWS] update neuron to 2.20 (#8676)
2	2	Dockerfile.neuron
2	2	requirements-neuron.txt

[7c8566aa4] omrishiv 2024-09-20 [Doc] neuron documentation update (#8671)
2	2	docs/source/getting_started/neuron-installation.rst
1	1	docs/source/index.rst

[b4e4eda92] Patrick von Platen 2024-09-20 [Bugfix][Core] Fix tekken edge case for mistral tokenizer (#8640)
25	1	tests/models/decoder_only/language/test_mistral.py
29	3	vllm/transformers_utils/tokenizers/mistral.py

[2874bac61] Pastel！ 2024-09-21 [Bugfix] Config got an unexpected keyword argument 'engine' (#8556)
0	1	vllm/entrypoints/api_server.py

[035fa895e] Cyrus Leung 2024-09-21 [Misc] Show AMD GPU topology in `collect_env.py` (#8649)
7	2	collect_env.py

[b28298f2f] saumya-saran 2024-09-20 [Bugfix] Validate SamplingParam n is an int (#8548)
6	1	vllm/sampling_params.py

[2940afa04] Alexey Kondratiev(AMD) 2024-09-20 [CI/Build] Removing entrypoints/openai/test_embedding.py test from ROCm build (#8670)
1	0	.buildkite/run-amd-test.sh

[3b63de935] Niklas Muennighoff 2024-09-20 [Model] Add OLMoE (#7922)
4	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
409	0	vllm/model_executor/models/olmoe.py

[260d40b5e] Jiaxin Shan 2024-09-19 [Core] Support Lora lineage and base model metadata management (#6315)
64	0	docs/source/models/lora.rst
91	0	tests/entrypoints/openai/test_cli_args.py
83	0	tests/entrypoints/openai/test_lora_lineage.py
4	2	tests/entrypoints/openai/test_models.py
4	2	tests/entrypoints/openai/test_serving_chat.py
3	2	tests/entrypoints/openai/test_serving_engine.py
10	4	vllm/entrypoints/openai/api_server.py
23	4	vllm/entrypoints/openai/cli_args.py
7	2	vllm/entrypoints/openai/run_batch.py
6	5	vllm/entrypoints/openai/serving_chat.py
5	4	vllm/entrypoints/openai/serving_completion.py
3	3	vllm/entrypoints/openai/serving_embedding.py
29	14	vllm/entrypoints/openai/serving_engine.py
4	3	vllm/entrypoints/openai/serving_tokenization.py
1	0	vllm/lora/request.py

[9e5ec35b1] William Lin 2024-09-19 [bugfix] [AMD] add multi-step advance_step to ROCmFlashAttentionMetadata (#8474)
57	1	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/worker/multi_step_model_runner.py

[18ae428a0] Amit Garg 2024-09-19 [Bugfix] Fix Phi3.5 mini and MoE LoRA inference (#8571)
1	1	vllm/model_executor/models/__init__.py
17	0	vllm/model_executor/models/phi3.py
4	0	vllm/model_executor/models/phimoe.py

[de6f90a13] bnellnm 2024-09-19 [Misc] guard against change in cuda library name (#8609)
4	3	cmake/utils.cmake

[6cb748e19] Alexey Kondratiev(AMD) 2024-09-19 [CI/Build] Re-enabling Entrypoints tests on ROCm, excluding ones that fail (#8551)
9	0	.buildkite/run-amd-test.sh
1	1	.buildkite/test-pipeline.yaml

[9e99407e3] Simon Mo 2024-09-19 Create SECURITY.md (#8642)
12	0	SECURITY.md

[ea4647b7d] Isotr0py 2024-09-20 [Doc] Add documentation for GGUF quantization (#8618)
1	0	docs/source/index.rst
73	0	docs/source/quantization/gguf.rst

[e42c634ac] 盏一 2024-09-20 [Core] simplify logits resort in _apply_top_k_top_p (#8619)
3	6	vllm/model_executor/layers/sampler.py

[9cc373f39] Charlie Fu 2024-09-19 [Kernel][Amd] Add fp8 kv cache support for rocm custom paged attention (#8577)
161	79	csrc/rocm/attention.cu
2	1	csrc/rocm/ops.h
2	1	csrc/rocm/torch_bindings.cpp
63	188	tests/kernels/test_attention.py
3	1	vllm/_custom_ops.py
15	13	vllm/attention/backends/rocm_flash_attn.py

[76515f303] Nick Hill 2024-09-19 [Frontend] Use MQLLMEngine for embeddings models too (#8584)
4	3	vllm/engine/multiprocessing/__init__.py
74	32	vllm/engine/multiprocessing/client.py
12	11	vllm/engine/multiprocessing/engine.py

[855c8ae2c] Kunshang Ji 2024-09-19 [MISC] remove engine_use_ray in benchmark_throughput.py (#8615)
0	1	benchmarks/benchmark_throughput.py

[c52ec5f03] Kuntai Du 2024-09-18 [Bugfix] fixing sonnet benchmark bug in benchmark_serving.py (#8616)
5	5	benchmarks/benchmark_serving.py

[02c9afa2d] Roger Wang 2024-09-18 Revert "[Misc][Bugfix] Disable guided decoding for mistral tokenizer" (#8593)
0	23	vllm/model_executor/guided_decoding/__init__.py

[3118f6338] sroy745 2024-09-18 [Bugfix] [Encoder-Decoder] Bugfix for encoder specific metadata construction during decode of encoder-decoder models.  (#8545)
63	25	tests/worker/test_encoder_decoder_model_runner.py
6	6	vllm/worker/enc_dec_model_runner.py

[4c34ce891] Tyler Michael Smith 2024-09-18 [Kernel] Remove marlin moe templating on thread_m_blocks (#8573)
28	51	csrc/moe/marlin_moe_ops.cu

[0d47bf3bf] Joe Runde 2024-09-18 [Bugfix] add `dead_error` property to engine client (#8574)
7	0	vllm/engine/multiprocessing/client.py

[d9cd78eb7] Nick Hill 2024-09-18 [BugFix] Nonzero exit code if MQLLMEngine startup fails (#8572)
5	11	vllm/entrypoints/openai/api_server.py

[db9120cde] Tyler Michael Smith 2024-09-18 [Kernel] Change interface to Mamba selective_state_update for continuous batching (#8039)
146	0	tests/kernels/test_mamba_ssm.py
28	3	vllm/model_executor/layers/mamba/ops/mamba_ssm.py

[b3195bc9e] Gregory Shtrasberg 2024-09-18 [AMD][ROCm]Quantization methods on ROCm; Fix _scaled_mm call (#8380)
4	1	vllm/config.py
26	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
13	2	vllm/model_executor/layers/quantization/fbgemm_fp8.py
28	21	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[e18749ff0] Geun, Lim 2024-09-19 [Model] Support Solar Model (#8386)
4	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
580	0	vllm/model_executor/models/solar.py
2	1	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
245	0	vllm/transformers_utils/configs/solar.py

[d65798f78] Russell Bryant 2024-09-18 [Core] zmq: bind only to 127.0.0.1 for local-only usage (#8543)
11	6	vllm/distributed/device_communicators/shm_broadcast.py

[a8c1d161a] afeldman-nm 2024-09-18 [Core] *Prompt* logprobs support in Multi-step (#8199)
52	32	tests/conftest.py
102	6	tests/models/utils.py
92	0	tests/multi_step/test_correctness_llm.py
2	1	tests/utils.py
52	20	vllm/worker/multi_step_model_runner.py

[7c7714d85] Alexander Matveev 2024-09-18 [Core][Bugfix][Perf] Introduce `MQLLMEngine` to avoid `asyncio` OH (#8157)
3	1	.buildkite/test-pipeline.yaml
2	2	docs/source/dev/profiling/profiling_index.rst
0	106	tests/async_engine/test_openapi_server.py
0	120	tests/entrypoints/openai/rpc/test_zmq_client.py
25	31	tests/entrypoints/openai/test_accuracy.py
1	1	tests/{async_engine => entrypoints/openai}/test_chat_template.py
0	40	tests/entrypoints/openai/test_mp_api_server.py
3	2	tests/entrypoints/openai/test_serving_chat.py
2	2	tests/entrypoints/openai/test_serving_engine.py
1	1	tests/entrypoints/openai/test_shutdown.py
0	0	tests/{entrypoints/openai/rpc => mq_llm_engine}/__init__.py
67	0	tests/mq_llm_engine/test_abort.py
244	0	tests/mq_llm_engine/test_error_handling.py
57	0	tests/mq_llm_engine/test_load.py
78	0	tests/mq_llm_engine/utils.py
7	0	tests/tpu/test_custom_dispatcher.py
1	1	tests/utils.py
6	3	vllm/engine/async_llm_engine.py
1	0	vllm/engine/llm_engine.py
73	0	vllm/engine/multiprocessing/__init__.py
452	0	vllm/engine/multiprocessing/client.py
321	0	vllm/engine/multiprocessing/engine.py
4	4	vllm/engine/protocol.py
16	14	vllm/entrypoints/launcher.py
55	66	vllm/entrypoints/openai/api_server.py
0	50	vllm/entrypoints/openai/rpc/__init__.py
0	451	vllm/entrypoints/openai/rpc/client.py
0	243	vllm/entrypoints/openai/rpc/server.py
13	8	vllm/entrypoints/openai/serving_chat.py
13	8	vllm/entrypoints/openai/serving_completion.py
5	6	vllm/entrypoints/openai/serving_embedding.py
4	4	vllm/entrypoints/openai/serving_engine.py
5	5	vllm/entrypoints/openai/serving_tokenization.py
3	3	vllm/envs.py
1	0	vllm/executor/cpu_executor.py
4	0	vllm/executor/multiproc_worker_utils.py

[9d104b5be] Aaron Pham 2024-09-18 [CI/Build] Update Ruff version (#8469)
2	2	.github/workflows/ruff.yml
1	3	benchmarks/kernels/graph_machete_bench.py
2	2	format.sh
2	0	pyproject.toml
1	1	requirements-lint.txt
1	4	tests/conftest.py
1	4	tests/lora/conftest.py
1	1	tests/multimodal/test_base.py
1	4	tests/test_cache_block_hashing.py
2	2	tests/test_logger.py
1	3	tests/worker/test_encoder_decoder_model_runner.py
1	3	tests/worker/test_model_runner.py
1	1	vllm/adapter_commons/utils.py
2	4	vllm/attention/backends/utils.py
1	3	vllm/core/block/prefix_caching_block.py
1	3	vllm/core/block_manager_v2.py
3	3	vllm/engine/async_llm_engine.py
3	3	vllm/engine/llm_engine.py
2	2	vllm/model_executor/guided_decoding/outlines_logits_processors.py
3	3	vllm/model_executor/layers/quantization/awq_marlin.py
7	7	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
4	4	vllm/model_executor/layers/quantization/gptq_marlin.py
1	3	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/model_executor/models/minicpmv.py
1	4	vllm/spec_decode/draft_model_runner.py
2	5	vllm/spec_decode/metrics.py
2	2	vllm/triton_utils/libentry.py

[6ffa3f314] Cyrus Leung 2024-09-18 [CI/Build] Avoid CUDA initialization (#8534)
3	6	benchmarks/kernels/benchmark_layernorm.py
3	3	benchmarks/kernels/benchmark_moe.py
2	5	benchmarks/kernels/benchmark_paged_attention.py
3	6	benchmarks/kernels/benchmark_quant.py
2	4	benchmarks/kernels/benchmark_rope.py
3	6	tests/kernels/test_activation.py
5	13	tests/kernels/test_attention.py
1	1	tests/kernels/test_attention_selector.py
3	2	tests/kernels/test_awq_triton.py
3	9	tests/kernels/test_blocksparse_attention.py
7	18	tests/kernels/test_cache.py
3	2	tests/kernels/test_causal_conv1d.py
4	7	tests/kernels/test_cutlass.py
3	2	tests/kernels/test_flash_attn.py
6	4	tests/kernels/test_flashinfer.py
4	6	tests/kernels/test_fp8_quant.py
3	2	tests/kernels/test_gguf.py
5	8	tests/kernels/test_int8_quant.py
2	3	tests/kernels/test_layernorm.py
1	1	tests/kernels/test_machete_gemm.py
3	2	tests/kernels/test_mamba_ssm.py
2	1	tests/kernels/test_moe.py
5	9	tests/kernels/test_pos_encoding.py
3	9	tests/kernels/test_prefix_prefill.py
2	3	tests/lora/test_layers.py
5	13	tests/lora/test_punica_sizes.py
5	13	tests/lora/test_punica_variation.py
2	7	tests/models/decoder_only/language/test_granite.py
1	3	tests/quantization/test_fp8.py
5	3	tests/quantization/utils.py
2	1	vllm/attention/backends/rocm_flash_attn.py
2	3	vllm/attention/ops/blocksparse_attention/interface.py
1	2	vllm/attention/ops/prefix_prefill.py
2	2	vllm/attention/selector.py
6	6	vllm/config.py
2	1	vllm/distributed/parallel_state.py
1	0	vllm/envs.py
3	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	3	vllm/model_executor/layers/quantization/fbgemm_fp8.py
2	3	vllm/model_executor/layers/quantization/fp8.py
6	4	vllm/model_executor/layers/quantization/utils/marlin_utils.py
1	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
3	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
3	3	vllm/model_executor/model_loader/loader.py
1	1	vllm/model_executor/models/qwen2_vl.py
3	7	vllm/model_executor/utils.py
4	4	vllm/platforms/cpu.py
9	8	vllm/platforms/cuda.py
55	7	vllm/platforms/interface.py
7	7	vllm/platforms/rocm.py
6	2	vllm/platforms/tpu.py
3	1	vllm/prompt_adapter/utils.py
2	1	vllm/usage/usage_lib.py
21	7	vllm/utils.py
11	5	vllm/worker/worker.py

[e35157290] Jiaxin Shan 2024-09-18 [Misc] Add argument to disable FastAPI docs (#8554)
7	1	vllm/entrypoints/openai/api_server.py
7	0	vllm/entrypoints/openai/cli_args.py

[95965d31b] Daniele 2024-09-18 [CI/Build] fix Dockerfile.cpu on podman (#8540)
2	0	Dockerfile.cpu

[8110e4452] Tyler Michael Smith 2024-09-17 [Kernel] Change interface to Mamba causal_conv1d_update for continuous batching (#8012)
27	3	csrc/mamba/causal_conv1d/causal_conv1d.cu
4	0	csrc/mamba/causal_conv1d/causal_conv1d.h
4	5	csrc/ops.h
3	2	csrc/torch_bindings.cpp
58	0	tests/kernels/test_causal_conv1d.py
10	4	vllm/_custom_ops.py
8	2	vllm/model_executor/layers/mamba/ops/causal_conv1d.py

[09deb4721] Alexey Kondratiev(AMD) 2024-09-17 [CI/Build] Excluding kernels/test_gguf.py from ROCm (#8520)
1	0	.buildkite/run-amd-test.sh

[fa0c114fa] youkaichao 2024-09-17 [doc] improve installation doc (#8550)
2	0	docs/source/getting_started/installation.rst
4	1	tests/compile/test_full_graph.py

[98f971339] Joe Runde 2024-09-17 [Bugfix] Fix TP > 1 for new granite (#8544)
2	1	vllm/model_executor/models/granite.py

[56c3de018] Nick Hill 2024-09-17 [Misc] Don't dump contents of kvcache tensors on errors (#8527)
20	2	vllm/worker/model_runner_base.py

[a54ed8024] Patrick von Platen 2024-09-17 [Model] Add mistral function calling format to all models loaded with "mistral" format (#8515)
138	0	examples/offline_chat_with_tools.py
67	0	tests/models/decoder_only/language/test_mistral.py
5	1	vllm/entrypoints/llm.py
5	4	vllm/entrypoints/openai/serving_chat.py
4	4	vllm/transformers_utils/tokenizers/mistral.py

[9855b9950] chenqianfzh 2024-09-17 [Feature][kernel] tensor parallelism with bitsandbytes quantization (#8434)
21	5	tests/quantization/test_bitsandbytes.py
0	6	vllm/config.py
16	5	vllm/model_executor/layers/linear.py
43	1	vllm/model_executor/model_loader/loader.py

[1009e93c5] sroy745 2024-09-17 [Encoder decoder] Add cuda graph support during decoding for encoder-decoder models (#7631)
7	0	.buildkite/test-pipeline.yaml
0	0	tests/encoder_decoder/__init__.py
98	0	tests/encoder_decoder/test_e2e_correctness.py
160	22	tests/worker/test_encoder_decoder_model_runner.py
13	4	vllm/attention/backends/abstract.py
9	3	vllm/attention/backends/flashinfer.py
107	6	vllm/attention/backends/utils.py
8	33	vllm/config.py
4	1	vllm/engine/arg_utils.py
4	4	vllm/entrypoints/llm.py
4	2	vllm/model_executor/models/bart.py
0	5	vllm/utils.py
36	7	vllm/worker/enc_dec_model_runner.py
76	21	vllm/worker/model_runner.py
0	4	vllm/worker/utils.py

[1b6de8352] Isotr0py 2024-09-17 [Benchmark] Support sample from HF datasets and image input for benchmark_serving (#8495)
5	1	benchmarks/backend_request_func.py
172	67	benchmarks/benchmark_serving.py

[cbdb25225] Rui Qiao 2024-09-17 [Misc] Limit to ray[adag] 2.35 to avoid backward incompatible change (#8509)
1	1	requirements-test.txt
4	2	vllm/executor/ray_gpu_executor.py

[99aa4edda] youkaichao 2024-09-16 [torch.compile] register allreduce operations as custom ops (#8526)
3	7	.buildkite/test-pipeline.yaml
0	12	csrc/custom_all_reduce.cu
0	2	csrc/ops.h
0	5	csrc/torch_bindings.cpp
0	0	tests/compile/__init__.py
13	2	tests/compile/test_full_graph.py
0	6	vllm/_custom_ops.py
19	2	vllm/distributed/device_communicators/custom_all_reduce.py
102	14	vllm/distributed/parallel_state.py

[ee2bceaaa] Roger Wang 2024-09-16 [Misc][Bugfix] Disable guided decoding for mistral tokenizer (#8521)
23	0	vllm/model_executor/guided_decoding/__init__.py

[1c1bb388e] Alex Brooks 2024-09-16 [Frontend] Improve Nullable kv Arg Parsing (#8525)
19	1	tests/engine/test_arg_utils.py
21	7	vllm/engine/arg_utils.py

[546034b46] Simon Mo 2024-09-16 [refactor] remove triton based sampler (#8524)
0	52	tests/kernels/test_rand.py
0	209	tests/kernels/test_sampler.py
0	0	vllm/model_executor/layers/ops/__init__.py
0	157	vllm/model_executor/layers/ops/rand.py
0	394	vllm/model_executor/layers/ops/sample.py
3	94	vllm/model_executor/layers/sampler.py
58	153	vllm/model_executor/sampling_metadata.py
0	13	vllm/triton_utils/sample.py
14	23	vllm/utils.py

[cca61642e] Joe Runde 2024-09-16 [Bugfix] Fix 3.12 builds on main (#8510)
0	4	Dockerfile
1	0	requirements-common.txt

[5ce45eb54] Simon Mo 2024-09-16 [misc] small qol fixes for release process (#8517)
2	0	Dockerfile
3	1	setup.py

[5478c4b41] Simon Mo 2024-09-16 [perf bench] set timeout to debug hanging (#8516)
1	2	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
3	1	.buildkite/nightly-benchmarks/scripts/wait-for-image.sh

[47f5e03b5] Kevin Lin 2024-09-16 [Bugfix] Bind api server port before starting engine (#8491)
6	0	vllm/entrypoints/openai/api_server.py

[2759a43a2] youkaichao 2024-09-16 [doc] update doc on testing and debugging (#8514)
7	0	docs/source/getting_started/debugging.rst

[5d73ae49d] Luka Govedič 2024-09-16 [Kernel] AQ AZP 3/4: Asymmetric quantization kernels (#7270)
6	3	csrc/cpu/quant.cpp
5	4	csrc/cpu/torch_bindings.cpp
4	2	csrc/ops.h
160	13	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
4	4	csrc/torch_bindings.cpp
138	20	tests/kernels/test_int8_quant.py
20	9	vllm/_custom_ops.py
1	1	vllm/model_executor/layers/quantization/qqq.py
1	1	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[781e3b9a4] sasha0552 2024-09-16 [Bugfix][Kernel] Fix build for sm_60 in GGUF kernel (#8506)
4	0	csrc/quantization/gguf/vecdotq.cuh

[acd5511b6] Nick Hill 2024-09-16 [BugFix] Fix clean shutdown issues (#8492)
8	2	tests/async_engine/test_async_llm_engine.py
45	25	vllm/engine/async_llm_engine.py
13	8	vllm/engine/llm_engine.py
10	11	vllm/entrypoints/launcher.py
109	72	vllm/entrypoints/openai/api_server.py
7	1	vllm/entrypoints/openai/rpc/server.py
0	14	vllm/executor/multiproc_gpu_executor.py
4	1	vllm/executor/multiproc_worker_utils.py
2	0	vllm/executor/ray_tpu_executor.py
2	2	vllm/scripts.py
15	0	vllm/utils.py

[837c1968f] lewtun 2024-09-16 [Frontend] Expose revision arg in OpenAI server (#8501)
4	2	vllm/entrypoints/openai/api_server.py

[a091e2da3] ElizaWszola 2024-09-16 [Kernel] Enable 8-bit weights in Fused Marlin MoE (#8032)
389	148	csrc/moe/marlin_moe_ops.cu
5	2	csrc/moe/marlin_moe_ops.h
5	3	csrc/moe/torch_bindings.cpp
12	6	tests/kernels/test_moe.py
2	1	tests/weight_loading/models-large.txt
0	0	tests/weight_loading/run_model_weight_loading_test.sh
1	1	vllm/_custom_ops.py
30	14	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
6	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	0	vllm/model_executor/layers/quantization/gptq_marlin.py
1	7	vllm/model_executor/model_loader/utils.py

[fc990f979] Isotr0py 2024-09-16 [Bugfix][Kernel] Add `IQ1_M` quantization implementation to GGUF kernel (#8357)
46	9	csrc/quantization/gguf/dequantize.cuh
277	131	csrc/quantization/gguf/ggml-common.h
5	0	csrc/quantization/gguf/gguf_kernel.cu
8	0	csrc/quantization/gguf/mmvq.cuh
81	20	csrc/quantization/gguf/vecdotq.cuh
1	1	requirements-common.txt
126	0	tests/kernels/test_gguf.py
4	1	vllm/model_executor/layers/quantization/gguf.py

[3724d5f6b] Chris 2024-09-15 [Bugfix][Model] Fix Python 3.8 compatibility in Pixtral model by updating type annotations (#8490)
1	1	vllm/model_executor/models/pixtral.py

[50e9ec41f] Woosuk Kwon 2024-09-14 [TPU] Implement multi-step scheduling (#8489)
1	1	vllm/config.py
6	2	vllm/executor/ray_tpu_executor.py
11	5	vllm/executor/tpu_executor.py
105	0	vllm/worker/multi_step_tpu_worker.py
156	68	vllm/worker/tpu_model_runner.py

[47790f3e3] youkaichao 2024-09-14 [torch.compile] add a flag to disable custom op (#8488)
2	1	tests/compile/test_full_graph.py
5	0	vllm/envs.py
5	0	vllm/model_executor/custom_op.py

[a36e070da] youkaichao 2024-09-14 [torch.compile] fix functionalization (#8480)
9	4	tests/compile/test_full_graph.py
156	0	vllm/compilation/backends.py
2	1	vllm/worker/model_runner.py

[8a0cf1ddc] ywfang 2024-09-14 [Model] support minicpm3 (#8297)
1	1	.buildkite/run-cpu-test.sh
4	0	docs/source/models/supported_models.rst
1	0	requirements-test.txt
9	6	tests/models/decoder_only/language/test_big_models.py
1	0	vllm/model_executor/models/__init__.py
49	30	vllm/model_executor/models/minicpm.py
216	0	vllm/model_executor/models/minicpm3.py

[1ef0d2efd] Charlie Fu 2024-09-13 [Kernel][Hardware][Amd]Custom paged attention kernel for rocm (#8310)
23	0	CMakeLists.txt
1038	0	csrc/rocm/attention.cu
13	0	csrc/rocm/ops.h
33	0	csrc/rocm/torch_bindings.cpp
3	0	setup.py
164	2	tests/kernels/test_attention.py
27	0	vllm/_custom_ops.py
70	14	vllm/attention/backends/rocm_flash_attn.py

[851725202] Kunshang Ji 2024-09-14 [Hardware][intel GPU] bump up ipex version to 2.3 (#8365)
10	2	Dockerfile.xpu
5	4	requirements-xpu.txt
29	69	vllm/_ipex_ops.py
6	2	vllm/attention/backends/ipex_attn.py
9	6	vllm/model_executor/layers/activation.py
1	4	vllm/model_executor/layers/layernorm.py

[9ba0817ff] Simon Mo 2024-09-13 bump version to v0.6.1.post2 (#8473)
1	1	vllm/version.py

[18e9e1f7b] Nick Hill 2024-09-13 [HotFix] Fix final output truncation with stop string + streaming (#8468)
21	5	tests/async_engine/test_async_llm_engine.py
3	1	vllm/sequence.py

[f57092c00] Isotr0py 2024-09-14 [Doc] Add oneDNN installation to CPU backend documentation (#8467)
14	0	docs/source/getting_started/cpu-installation.rst

[a84e598e2] Cyrus Leung 2024-09-14 [CI/Build] Reorganize models tests (#7820)
4	6	.buildkite/run-cpu-test.sh
45	25	.buildkite/test-pipeline.yaml
1	1	docs/source/models/supported_models.rst
2	1	pyproject.toml
62	0	tests/basic_correctness/test_basic_correctness.py
55	0	tests/basic_correctness/test_chunked_prefill.py
7	4	tests/basic_correctness/test_preemption.py
12	17	tests/conftest.py
0	80	tests/distributed/test_basic_distributed_correctness.py
0	102	tests/distributed/test_basic_distributed_correctness_enc_dec.py
0	75	tests/distributed/test_chunked_prefill_distributed.py
0	58	tests/distributed/test_multimodal_broadcast.py
7	7	tests/distributed/test_same_node.py
3	1	tests/kernels/utils.py
0	0	tests/models/decoder_only/__init__.py
0	0	tests/models/decoder_only/audio_language/__init__.py
2	4	tests/models/{ => decoder_only/audio_language}/test_ultravox.py
0	0	tests/models/decoder_only/language/__init__.py
0	0	tests/models/{ => decoder_only/language}/test_aqlm.py
1	1	tests/models/{ => decoder_only/language}/test_big_models.py
1	1	tests/models/{ => decoder_only/language}/test_danube3_4b.py
1	1	tests/models/{ => decoder_only/language}/test_fp8.py
1	1	tests/models/{ => decoder_only/language}/test_gguf.py
1	1	tests/models/{ => decoder_only/language}/test_gptq_marlin.py
2	1	tests/models/{ => decoder_only/language}/test_gptq_marlin_24.py
1	1	tests/models/{ => decoder_only/language}/test_granite.py
2	1	tests/models/{ => decoder_only/language}/test_jamba.py
1	1	tests/models/{ => decoder_only/language}/test_marlin.py
1	1	tests/models/{ => decoder_only/language}/test_mistral.py
0	0	tests/models/{ => decoder_only/language}/test_modelopt.py
1	1	tests/models/{ => decoder_only/language}/test_models.py
1	1	tests/models/{ => decoder_only/language}/test_phimoe.py
0	0	tests/models/decoder_only/vision_language/__init__.py
3	5	tests/models/{ => decoder_only/vision_language}/test_blip2.py
42	0	tests/models/decoder_only/vision_language/test_broadcast.py
3	5	tests/models/{ => decoder_only/vision_language}/test_chameleon.py
3	5	tests/models/{ => decoder_only/vision_language}/test_fuyu.py
1	3	tests/models/{ => decoder_only/vision_language}/test_intern_vit.py
4	6	tests/models/{ => decoder_only/vision_language}/test_internvl.py
5	7	tests/models/{ => decoder_only/vision_language}/test_llava.py
3	5	tests/models/{ => decoder_only/vision_language}/test_llava_image_embeds.py
4	6	tests/models/{ => decoder_only/vision_language}/test_llava_next.py
2	4	tests/models/{ => decoder_only/vision_language}/test_llava_next_video.py
3	5	tests/models/{ => decoder_only/vision_language}/test_minicpmv.py
3	5	tests/models/{ => decoder_only/vision_language}/test_paligemma.py
3	5	tests/models/{ => decoder_only/vision_language}/test_phi3v.py
16	7	tests/models/{ => decoder_only/vision_language}/test_pixtral.py
3	5	tests/models/{ => decoder_only/vision_language}/test_qwen.py
0	0	tests/models/embedding/__init__.py
0	0	tests/models/embedding/language/__init__.py
0	0	tests/models/{ => embedding/language}/test_embedding.py
0	0	tests/models/encoder_decoder/__init__.py
0	0	tests/models/encoder_decoder/language/__init__.py
84	31	tests/models/{ => encoder_decoder/language}/test_bart.py
19	1	tests/utils.py

[0a4806f0a] youkaichao 2024-09-13 [plugin][torch.compile] allow to add custom compile backend (#8445)
13	0	vllm/plugins/__init__.py
3	1	vllm/worker/model_runner.py

[ecd7a1d5b] Cyrus Leung 2024-09-14 [Installation] Gate FastAPI version for Python 3.8 (#8456)
2	1	requirements-common.txt

[a2469127d] youkaichao 2024-09-13 [misc][ci] fix quant test (#8449)
20	12	tests/quantization/test_bitsandbytes.py
1	3	tests/quantization/utils.py

[06311e295] Jee Jee Li 2024-09-13 [Misc] Skip loading extra bias for Qwen2-VL GPTQ-Int8 (#8442)
6	0	vllm/model_executor/models/qwen2_vl.py

[cab69a15e] youkaichao 2024-09-12 [doc] recommend pip instead of conda (#8446)
9	5	docs/source/getting_started/installation.rst

[9b4a3b235] Isotr0py 2024-09-13 [CI/Build] Enable InternVL2 PP test only on single node (#8437)
5	4	tests/distributed/test_pipeline_parallel.py

[acda0b35d] Simon Mo 2024-09-12 bump version to v0.6.1.post1 (#8440)
2	1	vllm/version.py

[ba7752795] William Lin 2024-09-12 [bugfix] torch profiler bug for single gpu with GPUExecutor (#8354)
1	1	examples/offline_inference_with_profiler.py
13	2	vllm/engine/async_llm_engine.py
13	2	vllm/engine/llm_engine.py

[682102010] Alexander Matveev 2024-09-12 [Bugfix] Fix async log stats (#8417)
1	0	tests/basic_correctness/test_preemption.py
16	4	vllm/engine/llm_engine.py

[842755048] Cyrus Leung 2024-09-13 [CI/Build] Update pixtral tests to use JSON (#8436)
1	1	pyproject.toml
1	0	tests/models/fixtures/pixtral_chat.json
-	-	tests/models/fixtures/pixtral_chat.pickle
1	0	tests/models/fixtures/pixtral_chat_engine.json
-	-	tests/models/fixtures/pixtral_chat_engine.pickle
39	17	tests/models/test_pixtral.py

[3f79bc3d1] Cyrus Leung 2024-09-13 [Bugfix] Bump fastapi and pydantic version (#8435)
2	2	requirements-common.txt

[40c396533] shangmingc 2024-09-13 [Bugfix] Mapping physical device indices for e2e test utils (#8290)
11	0	tests/utils.py

[5ec9c0fb3] Cyrus Leung 2024-09-13 [Core] Factor out input preprocessing to a separate class (#7329)
3	2	tests/engine/test_skip_tokenizer_init.py
4	141	vllm/engine/async_llm_engine.py
13	394	vllm/engine/llm_engine.py
34	3	vllm/inputs/parse.py
536	0	vllm/inputs/preprocess.py

[8f44a92d8] Dipika Sikka 2024-09-12 [BugFix] fix group_topk (#8430)
3	1	vllm/model_executor/layers/fused_moe/fused_moe.py

[360ddbd37] Roger Wang 2024-09-12 [Misc] Update Pixtral example (#8431)
3	2	examples/offline_inference_pixtral.py

[a480939e8] Wenxiang 2024-09-13 [Bugfix] Fix weight loading issue by rename variable. (#8293)
1	1	vllm/model_executor/models/phimoe.py

[d31174a4e] Patrick von Platen 2024-09-13 [Hotfix][Pixtral] Fix multiple images bugs (#8415)
1	1	tests/conftest.py
-	-	tests/models/fixtures/pixtral_chat.pickle
-	-	tests/models/fixtures/pixtral_chat_engine.pickle
146	42	tests/models/test_pixtral.py
49	34	vllm/model_executor/models/pixtral.py

[b61bd98f9] Roger Wang 2024-09-12 [CI/Build] Disable multi-node test for InternVL2 (#8428)
4	3	tests/distributed/test_pipeline_parallel.py

[c16369455] Roger Wang 2024-09-12 [Hotfix][Core][VLM] Disable chunked prefill by default and prefix caching for multimodal models (#8425)
11	1	vllm/engine/arg_utils.py
2	2	vllm/model_executor/models/__init__.py

[019877253] Alexander Matveev 2024-09-12 [Bugfix] multi-step + flashinfer: ensure cuda graph compatible  (#8427)
11	1	vllm/attention/backends/flashinfer.py

[551ce0107] Nick Hill 2024-09-12 [Core] Add engine option to return only deltas or final output (#7381)
1	0	.buildkite/test-pipeline.yaml
147	14	tests/async_engine/test_async_llm_engine.py
13	11	vllm/engine/llm_engine.py
8	15	vllm/entrypoints/llm.py
6	1	vllm/entrypoints/openai/protocol.py
72	53	vllm/entrypoints/openai/serving_chat.py
19	13	vllm/entrypoints/openai/serving_completion.py
55	24	vllm/outputs.py
16	1	vllm/sampling_params.py
34	5	vllm/sequence.py

[a6c0f3658] William Lin 2024-09-12 [multi-step] add flashinfer backend (#7928)
15	4	csrc/ops.h
200	25	csrc/prepare_inputs/advance_step.cu
13	2	csrc/torch_bindings.cpp
9	3	tests/multi_step/test_correctness_async_llm.py
29	9	vllm/_custom_ops.py
3	1	vllm/attention/backends/abstract.py
9	9	vllm/attention/backends/flash_attn.py
77	10	vllm/attention/backends/flashinfer.py
16	21	vllm/worker/multi_step_model_runner.py

[f2e263b80] Joe Runde 2024-09-12 [Bugfix] Offline mode fix (#8376)
1	0	.buildkite/test-pipeline.yaml
0	0	tests/entrypoints/offline_mode/__init__.py
77	0	tests/entrypoints/offline_mode/test_offline_mode.py
28	2	vllm/transformers_utils/config.py

[1f0c75afa] Luis Vega 2024-09-12 [BugFix] Fix Duplicate Assignment in Hermes2ProToolParser (#8423)
0	1	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py

[8a23e9330] WANGWEI 2024-09-13 [BugFix] lazy init _copy_stream to avoid torch init wrong gpu instance (#8403)
5	2	vllm/worker/multi_step_model_runner.py

[c6202daee] Alex Brooks 2024-09-12 [Model] Support multiple images for qwen-vl (#8247)
1	1	docs/source/models/supported_models.rst
60	24	examples/offline_inference_vision_language_multi_image.py
273	35	tests/models/test_qwen.py
9	5	vllm/model_executor/models/qwen.py

[e56bf2774] Isotr0py 2024-09-13 [Bugfix] Fix InternVL2 inference with various num_patches (#8375)
35	0	tests/models/test_internvl.py
4	3	vllm/model_executor/models/internvl.py

[520ca380a] Roger Wang 2024-09-12 [Hotfix][VLM] Fixing max position embeddings for Pixtral (#8399)
2	0	vllm/transformers_utils/config.py

[7de49aa86] youkaichao 2024-09-12 [torch.compile] hide slicing under custom op for inductor (#8384)
3	1	tests/compile/test_full_graph.py
71	34	vllm/attention/backends/flash_attn.py

[42ffba11a] Woosuk Kwon 2024-09-11 [Misc] Use RoPE cache for MRoPE (#8396)
1	1	vllm/model_executor/layers/rotary_embedding.py

[295c4730a] Kevin Lin 2024-09-12 [Misc] Raise error when using encoder/decoder model with cpu backend (#8355)
4	0	vllm/utils.py
5	1	vllm/worker/cpu_model_runner.py

[1bf2dd9df] Blueyo0 2024-09-12 [Gemma2] add bitsandbytes support for Gemma2 (#8338)
8	0	vllm/model_executor/models/gemma2.py

[5a60699c4] tomeras91 2024-09-12 [Bugfix]: Fix the logic for deciding if tool parsing is used (#8366)
1	1	vllm/entrypoints/openai/serving_chat.py

[b6c75e1cf] Michael Goin 2024-09-11 Fix the AMD weight loading tests (#8390)
1	1	vllm/config.py

[b71c956de] Woosuk Kwon 2024-09-11 [TPU] Use Ray for default distributed backend (#8389)
7	0	vllm/config.py

[f842a7aff] youkaichao 2024-09-11 [misc] remove engine_use_ray (#8126)
4	14	tests/async_engine/test_api_server.py
3	11	tests/async_engine/test_async_llm_engine.py
1	6	tests/async_engine/{test_openapi_server_ray.py => test_openapi_server.py}
0	11	vllm/engine/arg_utils.py
21	142	vllm/engine/async_llm_engine.py
3	3	vllm/engine/llm_engine.py
0	1	vllm/entrypoints/openai/run_batch.py
0	9	vllm/envs.py

[a65cb1606] Cody Yu 2024-09-11 [MISC] Dump model runner inputs when crashing (#8305)
9	0	.github/ISSUE_TEMPLATE/400-bug report.yml
30	0	tests/basic_correctness/test_basic_correctness.py
2	1	vllm/worker/model_runner.py
34	0	vllm/worker/model_runner_base.py

[3fd2b0d21] Simon Mo 2024-09-11 Bump version to v0.6.1 (#8379)
1	1	vllm/version.py

[d394787e5] Patrick von Platen 2024-09-11 Pixtral (#8377)
5	0	docs/source/models/supported_models.rst
164	0	examples/offline_inference_pixtral.py
1	1	requirements-common.txt
64	0	tests/models/test_pixtral.py
2	1	vllm/entrypoints/chat_utils.py
2	0	vllm/model_executor/models/__init__.py
551	0	vllm/model_executor/models/pixtral.py
18	7	vllm/transformers_utils/config.py

[775f00f81] Lily Liu 2024-09-11 [Speculative Decoding] Test refactor (#8317)
2	1	.buildkite/test-pipeline.yaml
176	299	tests/spec_decode/e2e/conftest.py
49	48	tests/spec_decode/e2e/test_eagle_correctness.py
32	20	tests/spec_decode/e2e/test_integration.py
76	79	tests/spec_decode/e2e/test_integration_dist_tp2.py
65	61	tests/spec_decode/e2e/test_integration_dist_tp4.py
93	234	tests/spec_decode/e2e/test_logprobs.py
72	46	tests/spec_decode/e2e/test_medusa_correctness.py
102	65	tests/spec_decode/e2e/test_mlp_correctness.py
171	136	tests/spec_decode/e2e/test_multistep_correctness.py
58	36	tests/spec_decode/e2e/test_ngram_correctness.py
33	19	tests/spec_decode/e2e/test_seed.py

[8baa45493] Aarni Koskela 2024-09-11 [Misc] Move device options to a single place (#8322)
6	8	benchmarks/benchmark_latency.py
6	8	benchmarks/benchmark_throughput.py
11	4	vllm/engine/arg_utils.py

[73202dbe7] bnellnm 2024-09-11 [Kernel][Misc] register ops to prevent graph breaks (#6917)
10	0	.github/PULL_REQUEST_TEMPLATE.md
1	0	cmake/utils.cmake
4	4	csrc/cpu/torch_bindings.cpp
8	0	csrc/ops.h
12	0	csrc/quantization/gptq_marlin/awq_marlin_repack.cu
12	0	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
100	44	csrc/torch_bindings.cpp
19	3	tests/kernels/test_activation.py
16	0	tests/kernels/test_attention.py
22	0	tests/kernels/test_cache.py
13	0	tests/kernels/test_cutlass.py
15	0	tests/kernels/test_int8_quant.py
8	0	tests/kernels/test_layernorm.py
7	0	tests/kernels/test_machete_gemm.py
29	26	tests/kernels/test_marlin_gemm.py
33	1	tests/kernels/utils.py
0	20	tests/models/test_aqlm.py
204	0	vllm/_custom_ops.py
5	0	vllm/envs.py
1	1	vllm/model_executor/models/jamba.py
8	3	vllm/worker/model_runner.py
1	0	vllm/worker/worker.py

[7015417fd] Cyrus Leung 2024-09-12 [Bugfix] Add missing attributes in mistral tokenizer (#8364)
5	2	vllm/entrypoints/chat_utils.py
58	30	vllm/transformers_utils/tokenizers/mistral.py

[aea02f30d] Alexey Kondratiev(AMD) 2024-09-11 [CI/Build] Excluding test_moe.py from AMD Kernels tests for investigation (#8373)
1	0	.buildkite/run-amd-test.sh

[0b952af45] Li, Jiang 2024-09-12 [Hardware][Intel] Support compressed-tensor W8A8 for CPU backend (#7257)
6	0	.buildkite/run-cpu-test.sh
17	1	Dockerfile.cpu
12	6	cmake/cpu_extension.cmake
60	2	csrc/cpu/cpu_types_x86.hpp
168	0	csrc/cpu/dnnl_helper.hpp
294	0	csrc/cpu/quant.cpp
29	2	csrc/cpu/torch_bindings.cpp
32	7	csrc/cpu/utils.cpp
2	2	tests/quantization/test_compressed_tensors.py
2	1	vllm/config.py
14	1	vllm/executor/cpu_executor.py
13	9	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
3	2	vllm/model_executor/model_loader/loader.py
10	0	vllm/platforms/__init__.py
15	0	vllm/platforms/cpu.py
7	3	vllm/platforms/interface.py
0	6	vllm/platforms/tpu.py
2	1	vllm/worker/cpu_worker.py

[3b7fea770] Yang Fan 2024-09-12 [Model][VLM] Add Qwen2-VL model support (#7905)
7	3	docs/source/models/supported_models.rst
18	0	examples/offline_inference_vision_language.py
61	7	examples/offline_inference_vision_language_multi_image.py
1	0	requirements-common.txt
5	0	tests/models/test_registry.py
6	3	vllm/config.py
7	1	vllm/entrypoints/chat_utils.py
184	1	vllm/model_executor/layers/rotary_embedding.py
4	0	vllm/model_executor/models/__init__.py
1088	0	vllm/model_executor/models/qwen2_vl.py
3	5	vllm/multimodal/base.py
11	0	vllm/sequence.py
37	0	vllm/transformers_utils/processor.py
99	11	vllm/worker/model_runner.py

[cea95dfb9] Pooya Davoodi 2024-09-10 [Frontend] Create ErrorResponse instead of raising exceptions in run_batch (#8347)
3	1	tests/entrypoints/openai/test_run_batch.py
28	3	vllm/entrypoints/openai/run_batch.py

[6a512a00d] Yangshen⚡Deng 2024-09-11 [model] Support for Llava-Next-Video model (#7559)
1	0	Dockerfile
1	0	Dockerfile.cpu
3	1	Dockerfile.neuron
2	1	Dockerfile.openvino
1	1	Dockerfile.ppc64le
3	0	Dockerfile.tpu
1	2	Dockerfile.xpu
1	0	docs/source/conf.py
14	0	docs/source/models/supported_models.rst
61	9	examples/offline_inference_vision_language.py
1	0	requirements-test.txt
1	0	setup.py
55	1	tests/conftest.py
236	0	tests/models/test_llava_next_video.py
85	0	vllm/assets/video.py
4	2	vllm/model_executor/models/__init__.py
471	0	vllm/model_executor/models/llava_next_video.py
2	1	vllm/multimodal/registry.py
42	0	vllm/multimodal/utils.py
71	0	vllm/multimodal/video.py
27	0	vllm/transformers_utils/image_processor.py

[efcf946a1] Pavani Majety 2024-09-10 [Hardware][NV] Add support for ModelOpt static scaling checkpoints. (#6112)
2	2	examples/fp8/quantizer/README.md
79	0	tests/models/test_modelopt.py
3	3	vllm/config.py
2	1	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
163	0	vllm/model_executor/layers/quantization/modelopt.py
7	0	vllm/model_executor/model_loader/weight_utils.py

[1230263e1] Isotr0py 2024-09-11 [Bugfix] Fix InternVL2 vision embeddings process with pipeline parallel (#8299)
8	2	tests/distributed/test_pipeline_parallel.py
2	1	vllm/model_executor/models/internvl.py

[e497b8aef] Jee Jee Li 2024-09-11 [Misc] Skip loading extra bias for Qwen2-MOE GPTQ models (#8329)
8	2	vllm/model_executor/models/qwen2_moe.py

[94144e726] Tyler Michael Smith 2024-09-10 [CI/Build][Kernel] Update CUTLASS to 3.5.1 tag (#8043)
13	2	CMakeLists.txt

[1d5e397aa] William Lin 2024-09-10 [Core/Bugfix] pass VLLM_ATTENTION_BACKEND to ray workers (#8172)
3	0	vllm/executor/ray_gpu_executor.py

[22f3a4bc6] Alexander Matveev 2024-09-10 [Bugfix] lookahead block table with cuda graph max capture (#8340)
11	1	vllm/attention/backends/flash_attn.py

[b1f3e1895] Cody Yu 2024-09-10 [MISC] Keep chunked prefill enabled by default with long context when prefix caching is enabled (#8342)
0	1	vllm/engine/arg_utils.py

[04e7c4e77] Prashant Gupta 2024-09-10 [Misc] remove peft as dependency for prompt models (#8162)
0	8	vllm/config.py
1	1	vllm/prompt_adapter/models.py
93	0	vllm/prompt_adapter/utils.py

[5faedf1b6] Kevin Lin 2024-09-10 [Spec Decode] Move ops.advance_step to flash attn advance_step (#8224)
15	6	vllm/attention/backends/flash_attn.py
3	13	vllm/spec_decode/draft_model_runner.py
5	14	vllm/worker/multi_step_model_runner.py

[02751a7a4] sumitd2 2024-09-11 Fix ppc64le buildkite job (#8309)
2	1	.buildkite/run-cpu-test-ppc64le.sh
2	3	Dockerfile.ppc64le

[f421f3cef] Alexey Kondratiev(AMD) 2024-09-10 [CI/Build] Enabling kernels tests for AMD, ignoring some of then that fail (#8130)
23	1	.buildkite/run-amd-test.sh
1	0	.buildkite/test-pipeline.yaml

[8c054b7a6] Cyrus Leung 2024-09-11 [Frontend] Clean up type annotations for mistral tokenizer (#8314)
3	2	tests/async_engine/test_chat_template.py
41	20	vllm/entrypoints/chat_utils.py
18	8	vllm/entrypoints/llm.py
30	18	vllm/entrypoints/openai/serving_chat.py
18	7	vllm/entrypoints/openai/serving_tokenization.py
4	4	vllm/transformers_utils/tokenizers/mistral.py

[6234385f4] Daniele 2024-09-10 [CI/Build] enable ccache/scccache for HIP builds (#8327)
4	1	setup.py

[da1a844e6] Cyrus Leung 2024-09-10 [Bugfix] Fix missing `post_layernorm` in CLIP (#8155)
25	4	vllm/model_executor/models/clip.py
17	15	vllm/model_executor/models/siglip.py

[a1d874224] Simon Mo 2024-09-09 Add NVIDIA Meetup slides, announce AMD meetup, and add contact info (#8319)
12	4	README.md
1	0	docs/source/community/meetups.rst

[6cd5e5b07] Dipika Sikka 2024-09-09 [Misc] Fused MoE Marlin support for GPTQ (#8217)
12	1	.buildkite/test-pipeline.yaml
1	1	csrc/moe/marlin_moe_ops.cu
1	1	csrc/moe/marlin_moe_ops.h
0	1	csrc/moe/torch_bindings.cpp
217	4	tests/kernels/test_moe.py
3	0	tests/weight_loading/models-large.txt
0	2	tests/weight_loading/models.txt
10	4	vllm/model_executor/layers/fused_moe/__init__.py
219	0	vllm/model_executor/layers/fused_moe/fused_marlin_moe.py
20	118	vllm/model_executor/layers/fused_moe/fused_moe.py
51	24	vllm/model_executor/layers/fused_moe/layer.py
28	20	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
297	15	vllm/model_executor/layers/quantization/gptq_marlin.py
17	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py
7	4	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
13	6	vllm/model_executor/layers/quantization/utils/quant_utils.py
8	0	vllm/model_executor/model_loader/utils.py
7	2	vllm/model_executor/models/mixtral.py

[c7cb5c333] Kyle Sayers 2024-09-09 [Misc] GPTQ Activation Ordering (#8135)
1	0	tests/weight_loading/models.txt
2	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
33	12	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
28	2	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[f9b4a2d41] Vladislav Kruglikov 2024-09-09 [Bugfix] Correct adapter usage for cohere and jamba (#8292)
3	2	vllm/model_executor/models/commandr.py
3	1	vllm/model_executor/models/jamba.py

[58fcc8545] Adam Lugowski 2024-09-09 [Frontend] Add progress reporting to run_batch.py (#8060)
48	6	vllm/entrypoints/openai/run_batch.py

[08287ef67] Kyle Mistele 2024-09-09 [Bugfix] Streamed tool calls now more strictly follow OpenAI's format; ensures Vercel AI SDK compatibility (#8272)
1	1	tests/tool_use/utils.py
0	7	vllm/entrypoints/openai/protocol.py
5	1	vllm/entrypoints/openai/serving_chat.py
0	1	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
5	15	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
8	19	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py

[4ef41b847] Alexander Matveev 2024-09-08 [Bugfix] Fix async postprocessor in case of preemption (#8267)
47	40	vllm/core/scheduler.py
12	12	vllm/engine/async_llm_engine.py
99	50	vllm/engine/llm_engine.py
14	12	vllm/worker/multi_step_model_runner.py

[cfe712bf1] Joe Runde 2024-09-07 [CI/Build] Use python 3.12 in cuda image (#8133)
6	2	Dockerfile
1	0	requirements-common.txt
3	3	tests/test_logger.py

[b962ee147] sumitd2 2024-09-07 ppc64le: Dockerfile fixed, and a script for buildkite (#8026)
32	0	.buildkite/run-cpu-test-ppc64le.sh
11	5	Dockerfile.ppc64le

[36bf8150c] Isotr0py 2024-09-08 [Model][VLM] Decouple weight loading logic for `Paligemma` (#8269)
35	77	vllm/model_executor/models/paligemma.py
19	4	vllm/model_executor/models/siglip.py

[e80712593] Isotr0py 2024-09-07 [Model][VLM] Support multi-images inputs for InternVL2 models (#8201)
1	1	docs/source/models/supported_models.rst
76	18	examples/offline_inference_vision_language_multi_image.py
73	19	tests/models/test_internvl.py
3	5	tests/models/test_phi3v.py
46	14	vllm/model_executor/models/internvl.py

[9f68e00d2] Cyrus Leung 2024-09-07 [Bugfix] Fix broken OpenAI tensorizer test (#8258)
6	6	tests/utils.py
39	33	vllm/engine/arg_utils.py
29	1	vllm/model_executor/model_loader/loader.py
7	0	vllm/model_executor/model_loader/tensorizer.py

[ce2702a92] youkaichao 2024-09-06 [tpu][misc] fix typo (#8260)
2	2	tests/compile/test_wrapper.py
1	1	vllm/compilation/wrapper.py
2	2	vllm/worker/tpu_model_runner.py

[795b662cf] Wei-Sheng Chin 2024-09-06 Enable Random Prefix Caching in Serving Profiling Tool (benchmark_serving.py) (#8241)
23	4	benchmarks/benchmark_serving.py

[2f707fcb3] Cyrus Leung 2024-09-07 [Model] Multi-input support for LLaVA (#8238)
8	8	docs/source/models/supported_models.rst
6	6	tests/conftest.py
4	2	tests/distributed/test_multimodal_broadcast.py
129	12	tests/models/test_llava.py
1	1	vllm/model_executor/models/clip.py
1	1	vllm/model_executor/models/internvl.py
20	12	vllm/model_executor/models/llava.py
3	1	vllm/model_executor/models/llava_next.py
3	1	vllm/model_executor/models/phi3v.py
1	1	vllm/model_executor/models/siglip.py

[41e95c524] Kyle Mistele 2024-09-06 [Bugfix] Fix Hermes tool call chat template bug (#8256)
16	15	examples/tool_chat_template_hermes.jinja

[12dd71580] William Lin 2024-09-06 [misc] [doc] [frontend] LLM torch profiler support (#7943)
17	3	docs/source/dev/profiling/profiling_index.rst
33	0	examples/offline_inference_with_profiler.py
6	0	vllm/engine/llm_engine.py
6	0	vllm/entrypoints/llm.py
6	0	vllm/executor/cpu_executor.py
6	0	vllm/executor/gpu_executor.py

[29f49cd6e] Patrick von Platen 2024-09-07 [Model] Allow loading from original Mistral format (#8168)
40	0	tests/models/test_mistral.py
33	29	vllm/config.py
16	5	vllm/engine/arg_utils.py
9	3	vllm/model_executor/model_loader/loader.py
11	10	vllm/model_executor/model_loader/weight_utils.py
51	0	vllm/model_executor/models/llama.py
131	34	vllm/transformers_utils/config.py

[23f322297] Dipika Sikka 2024-09-06 [Misc] Remove `SqueezeLLM` (#8220)
0	1	CMakeLists.txt
0	3	csrc/ops.h
0	216	csrc/quantization/squeezellm/quant_cuda_kernel.cu
0	6	csrc/torch_bindings.cpp
0	11	docs/source/quantization/supported_hardware.rst
2	2	examples/fp8/README.md
0	6	vllm/_custom_ops.py
2	2	vllm/config.py
1	1	vllm/entrypoints/llm.py
1	1	vllm/lora/layers.py
0	2	vllm/model_executor/layers/quantization/__init__.py
0	138	vllm/model_executor/layers/quantization/squeezellm.py

[9db52eab3] rasmith 2024-09-06 [Kernel] [Triton] Memory optimization for awq_gemm and awq_dequantize, 2x throughput (#8248)
23	11	vllm/model_executor/layers/quantization/awq_triton.py

[1447c97e7] Alexey Kondratiev(AMD) 2024-09-06 [CI/Build] Increasing timeout for multiproc worker tests (#8203)
3	3	tests/engine/test_multiproc_workers.py

[de80783b6] Rui Qiao 2024-09-06 [Misc] Use ray[adag] dependency instead of cuda (#7938)
0	2	Dockerfile
0	1	MANIFEST.in
0	3	requirements-adag.txt
1	4	requirements-test.txt
18	2	vllm/executor/ray_gpu_executor.py

[e5cab7153] afeldman-nm 2024-09-06 [Frontend] Add --logprobs argument to `benchmark_serving.py` (#8191)
2	0	benchmarks/backend_request_func.py
16	0	benchmarks/benchmark_serving.py
1	1	tests/multi_step/test_correctness_llm.py

[baa546754] Nick Hill 2024-09-05 [BugFix] Fix Granite model configuration (#8216)
38	24	vllm/transformers_utils/config.py
4	0	vllm/transformers_utils/configs/__init__.py

[db3bf7c99] Jiaxin Shan 2024-09-05 [Core] Support load and unload LoRA in api server (#6566)
0	1	docs/requirements-docs.txt
52	0	docs/source/models/lora.rst
1	1	tests/entrypoints/llm/test_generate_multiple_loras.py
107	0	tests/entrypoints/openai/test_serving_engine.py
38	2	vllm/entrypoints/openai/api_server.py
10	0	vllm/entrypoints/openai/protocol.py
78	1	vllm/entrypoints/openai/serving_engine.py
7	0	vllm/envs.py
18	1	vllm/lora/request.py
25	0	vllm/utils.py

[2febcf277] sroy745 2024-09-05 [Documentation][Spec Decode] Add documentation about lossless guarantees in Speculative Decoding in vLLM (#7962)
40	0	docs/source/models/spec_decode.rst
19	0	docs/source/serving/faq.rst

[2ee45281a] Michael Goin 2024-09-05 Move verify_marlin_supported to GPTQMarlinLinearMethod (#8165)
4	4	vllm/model_executor/layers/quantization/gptq_marlin.py

[9da25a88a] Alex Brooks 2024-09-05 [MODEL] Qwen Multimodal Support (Qwen-VL / Qwen-VL-Chat) (#8029)
5	0	docs/source/models/supported_models.rst
15	0	examples/offline_inference_vision_language.py
142	25	tests/models/test_qwen.py
2	0	vllm/entrypoints/chat_utils.py
273	0	vllm/model_executor/layers/resampler.py
1	1	vllm/model_executor/models/__init__.py
5	155	vllm/model_executor/models/minicpmv.py
667	27	vllm/model_executor/models/qwen.py

[8685ba1a1] manikandan.tm@zucisystems.com 2024-09-05 Inclusion of InternVLChatModel In PP_SUPPORTED_MODELS(Pipeline Parallelism) (#7860)
22	16	tests/distributed/test_pipeline_parallel.py
6	1	tests/utils.py
5	3	vllm/config.py
38	14	vllm/model_executor/models/internlm2.py
3	1	vllm/model_executor/models/internvl.py
16	0	vllm/model_executor/models/utils.py

[288a93887] Cyrus Leung 2024-09-05 [Doc] Indicate more information about supported modalities (#8181)
1	0	.buildkite/test-pipeline.yaml
1	1	docs/source/getting_started/debugging.rst
4	2	docs/source/getting_started/quickstart.rst
12	9	docs/source/models/supported_models.rst
88	35	docs/source/models/vlm.rst
95	0	examples/offline_inference_vision_language_multi_image.py
5	4	examples/openai_vision_api_client.py

[e39ebf5cf] Elfie Guo 2024-09-04 [Core/Bugfix] Add query dtype as per FlashInfer API requirements. (#8173)
2	1	tests/kernels/test_flashinfer.py
8	1	vllm/attention/backends/flashinfer.py

[ba262c4e5] Kevin H. Luu 2024-09-04 [ci] Mark LoRA test as soft-fail (#8160)
1	0	.buildkite/test-pipeline.yaml

[4624d98db] Woosuk Kwon 2024-09-04 [Misc] Clean up RoPE forward_native (#8076)
19	76	vllm/model_executor/layers/rotary_embedding.py

[1afc93198] William Lin 2024-09-04 [bugfix] >1.43 constraint for openai (#8169)
1	1	requirements-common.txt

[e01c2beb7] Maureen McElaney 2024-09-04 [Doc] [Misc] Create CODE_OF_CONDUCT.md (#8161)
128	0	CODE_OF_CONDUCT.md

[32e7db253] Simon Mo 2024-09-04 Bump version to v0.6.0 (#8166)
1	1	vllm/version.py

[008cf886c] Harsha vardhan manoj Bikki 2024-09-04 [Neuron] Adding support for adding/ overriding neuron configuration a… (#8062)
50	0	examples/offline_inference_neuron_int8_quantization.py
41	28	vllm/config.py
14	3	vllm/engine/arg_utils.py
2	0	vllm/engine/llm_engine.py
3	0	vllm/model_executor/layers/quantization/__init__.py
67	0	vllm/model_executor/layers/quantization/neuron_quant.py
57	8	vllm/model_executor/model_loader/neuron.py
9	3	vllm/worker/neuron_model_runner.py

[77d9e514a] Cody Yu 2024-09-04 [MISC] Replace input token throughput with total token throughput (#8164)
5	5	benchmarks/benchmark_serving.py

[e02ce498b] Kyle Mistele 2024-09-04 [Feature] OpenAI-Compatible Tools API + Streaming for Hermes & Mistral models (#5649)
10	0	.buildkite/test-pipeline.yaml
54	4	docs/source/serving/openai_compatible_server.md
162	0	examples/openai_chat_completion_client_with_tools.py
129	0	examples/tool_chat_template_hermes.jinja
86	0	examples/tool_chat_template_mistral.jinja
94	0	examples/tool_chat_template_mistral_parallel.jinja
1	0	requirements-common.txt
0	0	tests/tool_use/__init__.py
32	0	tests/tool_use/conftest.py
143	0	tests/tool_use/test_chat_completions.py
193	0	tests/tool_use/test_parallel_tool_calls.py
192	0	tests/tool_use/test_tool_calls.py
215	0	tests/tool_use/utils.py
82	19	vllm/entrypoints/chat_utils.py
6	2	vllm/entrypoints/openai/api_server.py
18	0	vllm/entrypoints/openai/cli_args.py
113	12	vllm/entrypoints/openai/protocol.py
240	35	vllm/entrypoints/openai/serving_chat.py
5	1	vllm/entrypoints/openai/serving_tokenization.py
5	0	vllm/entrypoints/openai/tool_parsers/__init__.py
58	0	vllm/entrypoints/openai/tool_parsers/abstract_tool_parser.py
344	0	vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py
293	0	vllm/entrypoints/openai/tool_parsers/mistral_tool_parser.py
87	0	vllm/entrypoints/openai/tool_parsers/utils.py
3	2	vllm/model_executor/guided_decoding/__init__.py
23	8	vllm/model_executor/guided_decoding/outlines_decoding.py

[561d6f807] Woosuk Kwon 2024-09-04 [CI] Change test input in Gemma LoRA test (#8163)
3	2	tests/lora/test_gemma.py

[d1dec6424] alexeykondrat 2024-09-04 [CI/Build][ROCm] Enabling LoRA tests on ROCm (#7369)
41	6	.buildkite/run-amd-test.sh
1	2	.buildkite/test-pipeline.yaml
4	0	tests/lora/test_gemma.py
18	6	tests/lora/test_quant_model.py

[2ad2e5608] Cody Yu 2024-09-04 [MISC] Consolidate FP8 kv-cache tests (#8131)
6	1	.buildkite/run-cpu-test.sh
7	36	tests/basic_correctness/test_chunked_prefill.py
81	100	tests/models/test_fp8.py
0	96	tests/models/test_fp8kv_flashinfer.py

[d3311562f] wnma 2024-09-04 [Bugfix] remove post_layernorm in siglip (#8106)
24	3	vllm/model_executor/models/siglip.py

[ccd720719] TimWang 2024-09-04 chore: Update check-wheel-size.py to read MAX_SIZE_MB from env (#8103)
21	14	.buildkite/check-wheel-size.py
10	3	Dockerfile

[855c262a6] Cyrus Leung 2024-09-04 [Frontend] Multimodal support in offline chat (#8098)
34	0	tests/entrypoints/llm/test_generate.py
124	40	tests/entrypoints/test_chat_utils.py
159	49	vllm/entrypoints/chat_utils.py
19	12	vllm/entrypoints/llm.py
3	6	vllm/entrypoints/openai/serving_chat.py
4	3	vllm/entrypoints/openai/serving_tokenization.py
10	0	vllm/multimodal/utils.py
3	2	vllm/transformers_utils/tokenizers/mistral.py

[2be8ec6e7] Peter Salas 2024-09-03 [Model] Add Ultravox support for multiple audio chunks (#7963)
34	24	examples/offline_inference_audio_language.py
77	26	tests/models/test_ultravox.py
87	65	vllm/model_executor/models/ultravox.py

[e16fa99a6] Dipika Sikka 2024-09-03 [Misc] Update fbgemmfp8 to use `vLLMParameters` (#7972)
1	1	vllm/model_executor/layers/linear.py
21	13	vllm/model_executor/layers/quantization/fbgemm_fp8.py
0	27	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[61f4a93d1] Woosuk Kwon 2024-09-03 [TPU][Bugfix] Use XLA rank for persistent cache path (#8137)
1	1	docs/source/getting_started/tpu-installation.rst
2	1	vllm/worker/tpu_worker.py

[d4db9f53c] Nick Hill 2024-09-03 [Benchmark] Add `--async-engine` option to benchmark_throughput.py (#7964)
109	4	benchmarks/benchmark_throughput.py
30	15	vllm/entrypoints/openai/api_server.py
4	0	vllm/entrypoints/openai/rpc/client.py

[2188a60c7] Dipika Sikka 2024-09-03 [Misc] Update `GPTQ` to use `vLLMParameters` (#7976)
6	0	tests/weight_loading/models.txt
6	1	tests/weight_loading/test_weight_loading.py
14	11	vllm/model_executor/layers/linear.py
58	45	vllm/model_executor/layers/quantization/gptq.py
6	3	vllm/model_executor/layers/vocab_parallel_embedding.py
3	2	vllm/model_executor/parameter.py

[dc0b6066a] Simon Mo 2024-09-03 [CI] Change PR remainder to avoid at-mentions (#8134)
1	1	.github/workflows/reminder_comment.yml

[0af3abe3d] Woosuk Kwon 2024-09-03 [TPU][Bugfix] Fix next_token_ids shape (#8128)
4	1	vllm/worker/tpu_model_runner.py

[f1575dc99] Kevin H. Luu 2024-09-03 [ci] Fix GHA workflow  (#8129)
1	1	.github/workflows/reminder_comment.yml

[c02638efb] tomeras91 2024-09-03 [CI/Build] make pip install vllm work in macos (for import only) (#8118)
2	1	setup.py

[652c83b69] Antoni Baum 2024-09-03 [Misc] Raise a more informative exception in add/remove_logger (#7750)
8	0	vllm/engine/llm_engine.py

[6d646d08a] Alexander Matveev 2024-09-03 [Core] Optimize Async + Multi-step (#8050)
2	2	tests/multi_step/test_correctness_async_llm.py
54	55	vllm/engine/async_llm_engine.py
97	125	vllm/engine/llm_engine.py
36	26	vllm/engine/output_processor/multi_step.py
1	3	vllm/sequence.py
3	1	vllm/worker/model_runner.py
132	33	vllm/worker/multi_step_model_runner.py
1	3	vllm/worker/multi_step_worker.py

[95a178f86] Kevin H. Luu 2024-09-03 [CI] Only PR reviewers/committers can trigger CI on PR (#8124)
0	23	.github/workflows/add_label_ready_comment.yml
1	1	.github/workflows/reminder_comment.yml
0	23	.github/workflows/remove_label_not_ready_comment.yml

[bd852f2a8] Cody Yu 2024-09-03 [Performance] Enable chunked prefill and prefix caching together (#8120)
[ec266536b] Isotr0py 2024-09-03 [Bugfix][VLM] Add fallback to SDPA for ViT model running on CPU backend (#8061)
20	5	vllm/model_executor/models/blip.py
22	6	vllm/model_executor/models/clip.py
70	9	vllm/model_executor/models/intern_vit.py
23	19	vllm/model_executor/models/paligemma.py
22	5	vllm/model_executor/models/siglip.py

[0fbc6696c] Woosuk Kwon 2024-09-02 [Bugfix] Fix single output condition in output processor (#7881)
1	1	vllm/engine/output_processor/single_step.py

[6e36f4fa6] wang.yuqi 2024-09-03 improve chunked prefill performance
3	0	tests/basic_correctness/test_chunked_prefill.py
10	5	vllm/core/scheduler.py

[dd2a6a82e] Isotr0py 2024-09-02 [Bugfix] Fix internlm2 tensor parallel inference (#8055)
34	13	vllm/model_executor/models/internlm2.py

[4ca65a976] Isotr0py 2024-09-02 [Core][Bugfix] Accept GGUF model without .gguf extension (#8056)
2	1	vllm/engine/arg_utils.py
3	2	vllm/transformers_utils/config.py
2	2	vllm/transformers_utils/tokenizer.py
16	0	vllm/transformers_utils/utils.py

[e2b2aa5a0] Woosuk Kwon 2024-09-01 [TPU] Align worker index with node boundary (#7932)
28	0	vllm/executor/ray_tpu_executor.py

[e6a26ed03] Lily Liu 2024-09-01 [SpecDecode][Kernel] Flashinfer Rejection Sampling (#7244)
1	1	Dockerfile
97	19	tests/samplers/test_rejection_sampler.py
32	18	tests/samplers/test_typical_acceptance_sampler.py
2	3	tests/spec_decode/test_spec_decode_worker.py
1	0	vllm/envs.py
141	43	vllm/model_executor/layers/rejection_sampler.py
25	18	vllm/model_executor/layers/spec_decode_base_sampler.py
4	3	vllm/model_executor/layers/typical_acceptance_sampler.py
3	4	vllm/spec_decode/spec_decode_worker.py

[f8d60145b] Shawn Tan 2024-09-01 [Model] Add Granite model (#7436)
49	0	tests/models/test_granite.py
1	0	vllm/model_executor/models/__init__.py
543	0	vllm/model_executor/models/granite.py
199	0	vllm/transformers_utils/configs/granite.py

[5b86b1995] Roger Wang 2024-09-01 [Misc] Optional installation of audio related packages (#8063)
1	3	requirements-common.txt
3	1	requirements-test.txt
1	0	setup.py
2	2	tests/models/test_ultravox.py
5	1	vllm/model_executor/models/ultravox.py
17	3	vllm/multimodal/utils.py

[5231f0898] Roger Wang 2024-08-31 [Frontend][VLM] Add support for multiple multi-modal items (#8049)
1	0	.buildkite/test-pipeline.yaml
39	0	examples/openai_vision_api_client.py
2	0	tests/entrypoints/openai/test_serving_chat.py
36	35	tests/entrypoints/openai/test_vision.py
305	0	tests/entrypoints/test_chat_utils.py
136	92	vllm/entrypoints/chat_utils.py
3	7	vllm/entrypoints/openai/serving_chat.py
2	2	vllm/entrypoints/openai/serving_tokenization.py

[8423aef4c] Robert Shaw 2024-08-31 [BugFix][Core] Multistep Fix Crash on Request Cancellation (#8059)
7	1	vllm/engine/output_processor/multi_step.py

[4f5d8446e] Nicolò Lucchesi 2024-08-31 [Bugfix] Fix ModelScope models in v0.5.5 (#8037)
3	0	vllm/transformers_utils/config.py

[d05f0a9db] Cyrus Leung 2024-08-31 [Bugfix] Fix import error in Phi-3.5-MoE (#8052)
2	2	vllm/model_executor/models/phimoe.py

[622f8abff] Pavani Majety 2024-08-30 [Bugfix] bugfix and add model test for flashinfer fp8 kv cache. (#8013)
96	0	tests/models/test_fp8kv_flashinfer.py
13	5	vllm/attention/backends/flashinfer.py

[1248e8506] Wenxiang 2024-08-31 [Model] Adding support for MSFT Phi-3.5-MoE (#7729)
4	0	docs/source/models/supported_models.rst
111	0	tests/models/test_phimoe.py
130	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3200,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
130	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=6400,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
130	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=800,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json
14	5	vllm/model_executor/layers/fused_moe/fused_moe.py
56	34	vllm/model_executor/layers/fused_moe/layer.py
14	10	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
15	11	vllm/model_executor/layers/quantization/experts_int8.py
15	11	vllm/model_executor/layers/quantization/fp8.py
15	11	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/models/__init__.py
620	0	vllm/model_executor/models/phimoe.py

[2684efc46] Woosuk Kwon 2024-08-30 [TPU][Bugfix] Fix tpu type api (#8035)
4	1	vllm/attention/backends/pallas.py

[058344f89] Kaunil Dhruv 2024-08-30 [Frontend]-config-cli-args (#7737)
2	1	docs/requirements-docs.txt
26	0	docs/source/serving/openai_compatible_server.md
1	0	requirements-common.txt
2	0	tests/data/test_config.yaml
44	0	tests/test_utils.py
9	0	vllm/scripts.py
101	0	vllm/utils.py

[98cef6a22] Cyrus Leung 2024-08-30 [Core] Increase default `max_num_batched_tokens` for multimodal models (#8028)
26	10	vllm/config.py
1	0	vllm/engine/arg_utils.py
5	1	vllm/engine/llm_engine.py
1	1	vllm/worker/utils.py

[f97be32d1] Jungho Christopher Cho 2024-08-31 [VLM][Model] TP support for ViTs (#7186)
1	2	tests/models/test_intern_vit.py
31	32	tests/models/test_internvl.py
76	3	vllm/model_executor/models/blip.py
1	2	vllm/model_executor/models/blip2.py
98	7	vllm/model_executor/models/clip.py
45	19	vllm/model_executor/models/intern_vit.py
20	28	vllm/model_executor/models/paligemma.py
36	17	vllm/model_executor/models/phi3v.py
32	179	vllm/model_executor/models/siglip.py

[afd39a451] Cyrus Leung 2024-08-30 [Bugfix] Fix import error in Exaone model (#8034)
2	2	vllm/model_executor/models/exaone.py

[2148441fd] Richard Liu 2024-08-30 [TPU] Support single and multi-host TPUs on GKE (#7613)
1	1	requirements-tpu.txt
4	1	vllm/attention/backends/pallas.py
25	2	vllm/distributed/device_communicators/tpu_communicator.py
15	0	vllm/executor/ray_tpu_executor.py
29	0	vllm/executor/ray_utils.py

[dc13e9934] Yohan Na 2024-08-30 [MODEL] add Exaone model support (#7819)
4	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
617	0	vllm/model_executor/models/exaone.py
6	5	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
190	0	vllm/transformers_utils/configs/exaone.py

[34a0e96d4] Avshalom Manevich 2024-08-30 [Kernel] changing fused moe kernel chunk size default to 32k (#7995)
1	1	vllm/envs.py

[80c7b089b] Woosuk Kwon 2024-08-29 [TPU] Async output processing for TPU (#8011)
3	3	vllm/config.py
7	1	vllm/worker/tpu_model_runner.py

[428dd1445] afeldman-nm 2024-08-29 [Core] Logprobs support in Multi-step (#7652)
26	17	tests/models/utils.py
69	30	tests/multi_step/test_correctness_async_llm.py
74	21	tests/multi_step/test_correctness_llm.py
2	1	tests/spec_decode/test_multi_step_worker.py
2	1	tests/spec_decode/test_spec_decode_worker.py
2	2	tests/spec_decode/utils.py
3	2	tests/test_sequence.py
60	0	tests/utils.py
2	1	vllm/engine/async_llm_engine.py
3	2	vllm/engine/llm_engine.py
12	3	vllm/engine/output_processor/multi_step.py
46	19	vllm/engine/output_processor/single_step.py
2	1	vllm/engine/output_processor/util.py
1	1	vllm/engine/protocol.py
2	1	vllm/executor/cpu_executor.py
2	1	vllm/executor/distributed_gpu_executor.py
2	1	vllm/executor/executor_base.py
2	1	vllm/executor/gpu_executor.py
2	1	vllm/executor/multiproc_gpu_executor.py
2	1	vllm/executor/neuron_executor.py
2	1	vllm/executor/openvino_executor.py
2	1	vllm/executor/ray_gpu_executor.py
2	1	vllm/executor/ray_tpu_executor.py
2	1	vllm/executor/tpu_executor.py
2	1	vllm/executor/xpu_executor.py
247	43	vllm/model_executor/layers/sampler.py
1	2	vllm/model_executor/model_loader/neuron.py
1	2	vllm/model_executor/model_loader/openvino.py
2	2	vllm/model_executor/models/arctic.py
2	2	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bart.py
2	2	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/bloom.py
2	2	vllm/model_executor/models/chameleon.py
2	2	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/commandr.py
2	2	vllm/model_executor/models/dbrx.py
2	2	vllm/model_executor/models/deepseek.py
2	2	vllm/model_executor/models/deepseek_v2.py
2	1	vllm/model_executor/models/eagle.py
2	2	vllm/model_executor/models/falcon.py
2	1	vllm/model_executor/models/fuyu.py
2	2	vllm/model_executor/models/gemma.py
2	2	vllm/model_executor/models/gemma2.py
2	2	vllm/model_executor/models/gpt2.py
2	2	vllm/model_executor/models/gpt_bigcode.py
2	2	vllm/model_executor/models/gpt_j.py
2	2	vllm/model_executor/models/gpt_neox.py
2	2	vllm/model_executor/models/internlm2.py
2	1	vllm/model_executor/models/internvl.py
2	2	vllm/model_executor/models/jais.py
2	2	vllm/model_executor/models/jamba.py
2	2	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/llava.py
2	1	vllm/model_executor/models/llava_next.py
1	1	vllm/model_executor/models/medusa.py
2	2	vllm/model_executor/models/minicpm.py
2	2	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/mixtral.py
2	2	vllm/model_executor/models/mixtral_quant.py
1	2	vllm/model_executor/models/mlp_speculator.py
2	2	vllm/model_executor/models/mpt.py
2	2	vllm/model_executor/models/nemotron.py
2	2	vllm/model_executor/models/olmo.py
2	2	vllm/model_executor/models/opt.py
2	2	vllm/model_executor/models/orion.py
2	2	vllm/model_executor/models/paligemma.py
2	2	vllm/model_executor/models/persimmon.py
2	2	vllm/model_executor/models/phi.py
2	2	vllm/model_executor/models/phi3_small.py
2	2	vllm/model_executor/models/phi3v.py
2	2	vllm/model_executor/models/qwen.py
2	2	vllm/model_executor/models/qwen2.py
2	2	vllm/model_executor/models/qwen2_moe.py
2	2	vllm/model_executor/models/stablelm.py
2	2	vllm/model_executor/models/starcoder2.py
2	1	vllm/model_executor/models/ultravox.py
2	2	vllm/model_executor/models/xverse.py
0	70	vllm/sequence.py
2	1	vllm/spec_decode/batch_expansion.py
2	2	vllm/spec_decode/draft_model_runner.py
2	2	vllm/spec_decode/medusa_worker.py
2	2	vllm/spec_decode/mlp_speculator_worker.py
3	2	vllm/spec_decode/multi_step_worker.py
2	1	vllm/spec_decode/ngram_worker.py
2	1	vllm/spec_decode/proposer_worker_base.py
2	1	vllm/spec_decode/smaller_tp_proposer_worker.py
2	1	vllm/spec_decode/spec_decode_worker.py
2	2	vllm/spec_decode/top1_proposer.py
2	2	vllm/spec_decode/util.py
2	2	vllm/worker/cpu_model_runner.py
2	1	vllm/worker/enc_dec_model_runner.py
2	2	vllm/worker/model_runner.py
2	2	vllm/worker/model_runner_base.py
154	19	vllm/worker/multi_step_model_runner.py
2	1	vllm/worker/multi_step_worker.py
2	2	vllm/worker/neuron_model_runner.py
2	1	vllm/worker/openvino_model_runner.py
2	1	vllm/worker/openvino_worker.py
2	2	vllm/worker/tpu_model_runner.py
2	2	vllm/worker/worker.py
2	2	vllm/worker/worker_base.py
2	2	vllm/worker/xpu_model_runner.py

[4abed65c5] Cyrus Leung 2024-08-30 [VLM] Disallow overflowing `max_model_len` for multimodal models (#7998)
17	0	tests/models/test_llava.py
18	3	vllm/engine/llm_engine.py

[0c785d344] Wei-Sheng Chin 2024-08-29 Add more percentiles and latencies (#7759)
94	38	benchmarks/benchmark_serving.py

[4664ceaad] chenqianfzh 2024-08-29 support bitsandbytes 8-bit and FP4 quantized models (#7445)
6	0	tests/conftest.py
98	68	tests/quantization/test_bitsandbytes.py
2	0	vllm/config.py
10	8	vllm/model_executor/layers/linear.py
195	36	vllm/model_executor/layers/quantization/bitsandbytes.py
126	79	vllm/model_executor/model_loader/loader.py

[257afc37c] Harsha vardhan manoj Bikki 2024-08-29 [Neuron] Adding support for context-lenght, token-gen buckets. (#7885)
9	2	examples/offline_inference_neuron.py
24	9	vllm/model_executor/model_loader/neuron.py

[86a677de4] Dipika Sikka 2024-08-29 [misc] update tpu int8 to use new vLLM Parameters (#7973)
2	1	vllm/model_executor/layers/linear.py
11	10	vllm/model_executor/layers/quantization/tpu_int8.py

[d78789ac1] Isotr0py 2024-08-30 [Bugfix] Fix incorrect vocal embedding shards for GGUF model in tensor parallelism (#7954)
4	1	vllm/model_executor/layers/vocab_parallel_embedding.py

[c334b1898] kushanam 2024-08-29 extend cuda graph size for H200 (#7894)
31	7	vllm/worker/model_runner.py

[6b3421567] Pavani Majety 2024-08-29 [Core][Kernels] Enable FP8 KV Cache with Flashinfer backend.  + BugFix for kv_cache_dtype=auto (#7985)
222	6	tests/kernels/test_flashinfer.py
24	6	vllm/attention/backends/flashinfer.py
4	0	vllm/attention/selector.py

[3f60f2244] Alexander Matveev 2024-08-29 [Core] Combine async postprocessor and multi-step (#7921)
6	4	tests/multi_step/test_correctness_async_llm.py
1	4	vllm/core/scheduler.py
44	21	vllm/engine/async_llm_engine.py
88	26	vllm/engine/llm_engine.py
3	1	vllm/sequence.py
1	0	vllm/worker/model_runner.py
64	9	vllm/worker/multi_step_model_runner.py
8	0	vllm/worker/multi_step_worker.py

[f205c0985] Jonas M. Kübler 2024-08-29 [Bugfix] Unify rank computation across regular decoding and speculative decoding (#7899)
20	1	tests/spec_decode/test_utils.py
2	2	vllm/spec_decode/util.py

[ef99a7876] youkaichao 2024-08-28 Revert "[Core][Kernels] Use FlashInfer backend for FP8 KV Cache when available." (#7982)
6	222	tests/kernels/test_flashinfer.py
6	23	vllm/attention/backends/flashinfer.py
0	4	vllm/attention/selector.py

[74d5543ec] Peter Salas 2024-08-28 [VLM][Core] Fix exceptions on ragged NestedTensors (#7974)
12	0	tests/multimodal/test_base.py
7	9	vllm/model_executor/models/utils.py
2	2	vllm/multimodal/base.py

[a7f65c2be] youkaichao 2024-08-28 [torch.compile] remove reset (#7975)
28	7	tests/tpu/test_compilation.py
0	4	vllm/worker/model_runner.py
0	4	vllm/worker/tpu_worker.py

[4289cad37] Nick Hill 2024-08-28 [Frontend] Minor optimizations to zmq decoupled front-end (#7957)
37	44	vllm/entrypoints/openai/rpc/client.py
27	21	vllm/entrypoints/openai/rpc/server.py

[af59df0a1] Michael Goin 2024-08-28 Remove faulty Meta-Llama-3-8B-Instruct-FP8.yaml lm-eval test (#7961)
0	1	.buildkite/lm-eval-harness/configs/models-small.txt

[ce6bf3a2c] youkaichao 2024-08-28 [torch.compile] avoid Dynamo guard evaluation overhead (#7898)
1	1	.buildkite/run-tpu-test.sh
1	0	.buildkite/test-pipeline.yaml
59	0	tests/compile/test_wrapper.py
0	0	tests/tpu/__init__.py
9	0	tests/tpu/test_custom_dispatcher.py
0	0	vllm/compilation/__init__.py
81	0	vllm/compilation/wrapper.py
4	0	vllm/envs.py
35	10	vllm/worker/tpu_model_runner.py

[3cdfe1f38] bnellnm 2024-08-28 [Bugfix] Make torch registration of punica ops optional (#7970)
6	3	vllm/lora/ops/bgmv_expand.py
6	3	vllm/lora/ops/bgmv_expand_slice.py
6	3	vllm/lora/ops/bgmv_shrink.py
6	3	vllm/lora/ops/sgmv_expand.py
6	3	vllm/lora/ops/sgmv_expand_slice.py
6	3	vllm/lora/ops/sgmv_shrink.py
1	3	vllm/lora/punica.py

[fdd9daafa] Mor Zusman 2024-08-29 [Kernel/Model] Migrate mamba_ssm and causal_conv1d kernels to vLLM (#7651)
2	0	CMakeLists.txt
0	23	Dockerfile
700	0	csrc/mamba/causal_conv1d/causal_conv1d.cu
144	0	csrc/mamba/causal_conv1d/causal_conv1d.h
28	0	csrc/mamba/causal_conv1d/static_switch.h
276	0	csrc/mamba/mamba_ssm/selective_scan.h
593	0	csrc/mamba/mamba_ssm/selective_scan_fwd.cu
28	0	csrc/mamba/mamba_ssm/static_switch.h
22	0	csrc/ops.h
25	0	csrc/torch_bindings.cpp
0	3	requirements-mamba.txt
1	1	requirements-test.txt
205	0	tests/kernels/test_causal_conv1d.py
324	0	tests/kernels/test_mamba_ssm.py
30	0	vllm/_custom_ops.py
0	0	vllm/model_executor/layers/mamba/__init__.py
0	0	vllm/model_executor/layers/mamba/ops/__init__.py
86	0	vllm/model_executor/layers/mamba/ops/causal_conv1d.py
346	0	vllm/model_executor/layers/mamba/ops/mamba_ssm.py
5	4	vllm/model_executor/models/jamba.py

[8c56e57de] Stas Bekman 2024-08-28 [Doc] fix 404 link (#7966)
1	1	docs/source/performance_benchmark/benchmarks.rst

[eeffde1ac] Woosuk Kwon 2024-08-28 [TPU] Upgrade PyTorch XLA nightly (#7967)
1	1	Dockerfile.tpu
4	3	docs/source/getting_started/tpu-installation.rst

[e5697d161] rasmith 2024-08-28 [Kernel] [Triton] [AMD] Adding Triton implementations awq_dequantize and awq_gemm to support AWQ (#7386)
169	0	tests/kernels/test_awq_triton.py
9	0	vllm/_custom_ops.py
7	1	vllm/config.py
4	0	vllm/envs.py
304	0	vllm/model_executor/layers/quantization/awq_triton.py

[b98cc28f9] Pavani Majety 2024-08-28 [Core][Kernels] Use FlashInfer backend for FP8 KV Cache when available. (#7798)
222	6	tests/kernels/test_flashinfer.py
23	6	vllm/attention/backends/flashinfer.py
4	0	vllm/attention/selector.py

[ef9baee3c] Cyrus Leung 2024-08-28 [Bugfix][VLM] Fix incompatibility between #7902 and #7230 (#7948)
2	2	vllm/model_executor/models/blip2.py
1	1	vllm/model_executor/models/chameleon.py
15	31	vllm/model_executor/models/internvl.py
2	2	vllm/model_executor/models/llava.py
26	26	vllm/model_executor/models/llava_next.py
2	2	vllm/model_executor/models/paligemma.py
27	23	vllm/model_executor/models/phi3v.py
1	1	vllm/model_executor/models/ultravox.py
42	2	vllm/model_executor/models/utils.py
2	2	vllm/multimodal/base.py

[98c12cffe] Stas Bekman 2024-08-28 [Doc] fix the autoAWQ example (#7937)
12	8	docs/source/quantization/auto_awq.rst

[f52a43a8b] youkaichao 2024-08-28 [ci][test] fix pp test failure (#7945)
1	1	vllm/executor/multiproc_gpu_executor.py

[e3580537a] Cody Yu 2024-08-28 [Performance] Enable chunked prefill and prefix caching together (#7753)
66	0	tests/basic_correctness/test_chunked_prefill.py
40	0	tests/core/test_block_manager.py
39	0	tests/core/test_chunked_prefill_scheduler.py
13	6	vllm/core/block_manager_v1.py
2	1	vllm/core/block_manager_v2.py
2	1	vllm/core/embedding_model_block_manager.py
2	1	vllm/core/interfaces.py
24	6	vllm/core/scheduler.py
37	12	vllm/worker/model_runner.py

[f508e03e7] Alexander Matveev 2024-08-28 [Core] Async_output_proc: Add virtual engine support (towards pipeline parallel) (#7911)
5	6	vllm/core/scheduler.py
27	10	vllm/engine/async_llm_engine.py
79	42	vllm/engine/llm_engine.py
6	3	vllm/sequence.py
3	3	vllm/worker/model_runner.py
2	3	vllm/worker/worker_base.py

[51f86bf48] Cyrus Leung 2024-08-28 [mypy][CI/Build] Fix mypy errors (#7929)
5	0	tests/samplers/test_sampler.py
3	1	vllm/assets/audio.py
3	2	vllm/entrypoints/openai/rpc/client.py
12	5	vllm/multimodal/base.py
1	1	vllm/sequence.py

[c166e7e43] bnellnm 2024-08-27 [Bugfix] Allow ScalarType to be compiled with pytorch 2.3 and add checks for registering FakeScalarType and dynamo support. (#7886)
2	1	csrc/core/scalar_type.hpp
70	64	vllm/_core_ext.py
9	0	vllm/utils.py
3	2	vllm/worker/model_runner.py

[bc6e42a9b] youkaichao 2024-08-27 [hardware][rocm] allow rocm to override default env var (#7926)
3	2	vllm/core/scheduler.py
11	0	vllm/platforms/rocm.py

[fab5f53e2] Peter Salas 2024-08-27 [Core][VLM] Stack multimodal tensors to represent multiple images within each prompt (#7902)
0	2	docs/source/dev/multimodal/multimodal_index.rst
83	0	tests/multimodal/test_base.py
7	0	vllm/model_executor/models/blip2.py
3	0	vllm/model_executor/models/chameleon.py
3	0	vllm/model_executor/models/fuyu.py
9	0	vllm/model_executor/models/internvl.py
8	0	vllm/model_executor/models/llava.py
11	0	vllm/model_executor/models/llava_next.py
8	3	vllm/model_executor/models/minicpmv.py
8	0	vllm/model_executor/models/paligemma.py
8	0	vllm/model_executor/models/phi3v.py
9	0	vllm/model_executor/models/ultravox.py
37	23	vllm/model_executor/models/utils.py
1	2	vllm/multimodal/__init__.py
19	30	vllm/multimodal/base.py

[9c71c97ae] Jonathan Berkhahn 2024-08-27 [mypy] Enable mypy type checking for `vllm/core` (#7229)
0	1	.github/workflows/mypy.yaml
0	1	format.sh
1	0	pyproject.toml
7	2	vllm/block.py
1	1	vllm/core/block/cpu_gpu_block_allocator.py
4	3	vllm/core/block_manager_v1.py
6	2	vllm/core/block_manager_v2.py
2	2	vllm/core/embedding_model_block_manager.py
10	6	vllm/core/scheduler.py

[5340a2dcc] zifeitong 2024-08-27 [Model] Add multi-image input support for LLaVA-Next offline inference (#7230)
10	11	tests/conftest.py
80	13	tests/models/test_llava_next.py
34	1	tests/multimodal/test_utils.py
4	4	vllm/model_executor/models/clip.py
12	2	vllm/model_executor/models/llava_next.py
2	2	vllm/model_executor/models/siglip.py
32	19	vllm/multimodal/utils.py

[345be0e24] Philipp Schmid 2024-08-28 [benchmark] Update TGI version (#7917)
1	1	benchmarks/launch_tgi_server.sh

[fc911880c] Dipika Sikka 2024-08-27 [Kernel] Expand MoE weight loading + Add Fused Marlin MoE Kernel (#7766)
5	0	CMakeLists.txt
1740	0	csrc/moe/marlin_moe_ops.cu
12	0	csrc/moe/marlin_moe_ops.h
12	0	csrc/moe/torch_bindings.cpp
1	1	tests/quantization/test_compressed_tensors.py
2	0	tests/weight_loading/models.txt
14	0	vllm/_custom_ops.py
6	8	vllm/model_executor/layers/fused_moe/__init__.py
116	18	vllm/model_executor/layers/fused_moe/fused_moe.py
171	37	vllm/model_executor/layers/fused_moe/layer.py
5	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
283	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
11	18	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/model_loader/utils.py
1	1	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/mixtral.py

[ed6f002d3] youkaichao 2024-08-27 [cuda][misc] error on empty CUDA_VISIBLE_DEVICES (#7924)
3	0	vllm/platforms/cuda.py

[b09c755be] Isotr0py 2024-08-28 [Bugfix] Fix phi3v incorrect image_idx when using async engine (#7916)
3	1	vllm/model_executor/models/phi3v.py

[42e932c7d] alexeykondrat 2024-08-27 [CI/Build][ROCm] Enabling tensorizer tests for ROCm (#7237)
1	0	.buildkite/run-amd-test.sh
2	1	.buildkite/test-pipeline.yaml
1	0	requirements-rocm.txt

[076169f60] Kunshang Ji 2024-08-28 [Hardware][Intel GPU] Add intel GPU pipeline parallel support. (#7810)
5	0	vllm/engine/async_llm_engine.py
7	0	vllm/engine/llm_engine.py
22	16	vllm/executor/multiproc_gpu_executor.py
26	0	vllm/executor/multiproc_xpu_executor.py
16	3	vllm/worker/xpu_model_runner.py
6	0	vllm/worker/xpu_worker.py

[9db642138] Isotr0py 2024-08-27 [CI/Build][VLM] Cleanup multiple images inputs model test (#7897)
34	102	tests/models/test_minicpmv.py
40	101	tests/models/test_phi3v.py

[6fc4e6e07] Patrick von Platen 2024-08-27 [Model] Add Mistral Tokenization to improve robustness and chat encoding (#7739)
1	0	docs/requirements-docs.txt
1	0	requirements-common.txt
3	1	tests/models/test_mistral.py
4	3	vllm/config.py
3	2	vllm/engine/arg_utils.py
1	3	vllm/entrypoints/chat_utils.py
9	3	vllm/entrypoints/llm.py
18	8	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/transformers_utils/detokenizer.py
58	36	vllm/transformers_utils/tokenizer.py
2	3	vllm/transformers_utils/tokenizers/__init__.py
174	0	vllm/transformers_utils/tokenizers/mistral.py

[9606c7197] Cody Yu 2024-08-27 Revert #7509 (#7887)
2	4	vllm/attention/backends/flashinfer.py

[64cc64442] youkaichao 2024-08-26 [core][torch.compile] discard the compile for profiling (#7796)
1	2	.buildkite/run-tpu-test.sh
34	0	tests/tpu/test_compilation.py
4	0	vllm/worker/model_runner.py
4	0	vllm/worker/tpu_worker.py

[39178c7fb] Nick Hill 2024-08-26 [Tests] Disable retries and use context manager for openai client (#7565)
5	3	tests/async_engine/test_openapi_server_ray.py
5	3	tests/entrypoints/openai/test_audio.py
5	3	tests/entrypoints/openai/test_basic.py
5	3	tests/entrypoints/openai/test_chat.py
9	2	tests/entrypoints/openai/test_completion.py
5	4	tests/entrypoints/openai/test_embedding.py
5	3	tests/entrypoints/openai/test_encoder_decoder.py
9	2	tests/entrypoints/openai/test_metrics.py
5	3	tests/entrypoints/openai/test_models.py
51	47	tests/entrypoints/openai/test_return_tokens_as_ids.py
9	8	tests/entrypoints/openai/test_shutdown.py
5	3	tests/entrypoints/openai/test_tokenization.py
5	3	tests/entrypoints/openai/test_vision.py
6	6	tests/multi_step/test_correctness_async_llm.py
1	0	tests/utils.py

[2eedede87] Megha Agarwal 2024-08-26 [Core] Asynchronous Output Processor (#7049)
9	1	benchmarks/benchmark_throughput.py
6	0	tests/basic_correctness/test_chunked_prefill.py
0	1	tests/basic_correctness/test_preemption.py
2	2	tests/core/test_chunked_prefill_scheduler.py
1	1	tests/core/utils.py
103	52	tests/engine/test_stop_strings.py
3	0	tests/multi_step/test_correctness_async_llm.py
53	0	vllm/config.py
118	12	vllm/core/scheduler.py
8	0	vllm/engine/arg_utils.py
47	13	vllm/engine/async_llm_engine.py
208	70	vllm/engine/llm_engine.py
5	8	vllm/engine/output_processor/interfaces.py
16	9	vllm/engine/output_processor/multi_step.py
20	12	vllm/engine/output_processor/single_step.py
10	1	vllm/entrypoints/llm.py
4	3	vllm/executor/distributed_gpu_executor.py
1	1	vllm/executor/gpu_executor.py
8	8	vllm/sequence.py
7	3	vllm/worker/model_runner.py
7	1	vllm/worker/worker_base.py

[015e6cc25] Dipika Sikka 2024-08-26 [Misc] Update compressed tensors lifecycle to remove `prefix` from `create_weights` (#7825)
3	6	vllm/model_executor/layers/linear.py
14	18	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
0	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
0	49	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py

[760e9f71a] omrishiv 2024-08-26 [Bugfix] neuron: enable tensor parallelism (#7562)
5	3	vllm/engine/arg_utils.py
12	8	vllm/executor/neuron_executor.py
27	0	vllm/worker/neuron_worker.py

[05826c887] youkaichao 2024-08-26 [misc] fix custom allreduce p2p cache file generation (#7853)
25	16	vllm/distributed/device_communicators/custom_all_reduce_utils.py

[dd9857f5f] Dipika Sikka 2024-08-26 [Misc] Update `gptq_marlin_24` to use vLLMParameters (#7762)
1	1	vllm/model_executor/layers/linear.py
49	53	vllm/model_executor/layers/quantization/gptq_marlin_24.py

[665304092] Dipika Sikka 2024-08-26 [Misc] Update `qqq` to use vLLMParameters (#7805)
3	1	tests/weight_loading/models.txt
1	1	vllm/model_executor/layers/linear.py
51	63	vllm/model_executor/layers/quantization/qqq.py

[2deb029d1] Cody Yu 2024-08-26 [Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)
31	0	tests/core/block/test_prefix_caching_block.py
17	5	vllm/core/block/prefix_caching_block.py
5	5	vllm/core/block_manager_v2.py

[029c71de1] Cyrus Leung 2024-08-26 [CI/Build] Avoid downloading all HF files in `RemoteOpenAIServer` (#7836)
26	14	tests/utils.py
1	1	vllm/engine/arg_utils.py

[0b769992e] ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 2024-08-26 [Bugfix]: Use float32 for base64 embedding (#7855)
0	1	examples/openai_embedding_client.py
10	1	tests/entrypoints/openai/test_embedding.py
3	1	vllm/entrypoints/openai/serving_embedding.py

[1856aff4d] Nick Hill 2024-08-25 [Spec Decoding] Streamline batch expansion tensor manipulation (#7851)
13	18	tests/spec_decode/test_utils.py
79	64	vllm/spec_decode/batch_expansion.py
9	16	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/spec_decode/top1_proposer.py
16	26	vllm/spec_decode/util.py

[70c094ade] youkaichao 2024-08-25 [misc][cuda] improve pynvml warning (#7852)
3	1	vllm/platforms/cuda.py

[2059b8d9c] Isotr0py 2024-08-25 [Misc] Remove snapshot_download usage in InternVL2 test (#7835)
3	9	tests/models/test_internvl.py

[8aaf3d534] Isotr0py 2024-08-25 [Model][VLM] Support multi-images inputs for Phi-3-vision models  (#7783)
111	0	tests/models/test_phi3v.py
57	29	vllm/model_executor/models/phi3v.py

[80162c44b] zifeitong 2024-08-24 [Bugfix] Fix Phi-3v crash when input images are of certain sizes (#7840)
22	5	tests/models/test_phi3v.py
0	2	vllm/model_executor/models/phi3v.py

[aab0fcdb6] youkaichao 2024-08-24 [ci][test] fix RemoteOpenAIServer (#7838)
1	1	tests/utils.py

[ea9fa160e] youkaichao 2024-08-24 [ci][test] exclude model download time in server start time (#7834)
5	0	tests/utils.py

[7d9ffa2ae] youkaichao 2024-08-24 [misc][core] lazy import outlines (#7831)
2	1	.buildkite/test-pipeline.yaml
48	0	tests/entrypoints/llm/test_lazy_outlines.py
6	3	vllm/model_executor/guided_decoding/__init__.py
8	3	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py

[d81abefd2] Tyler Rockwood 2024-08-24 [Frontend] add json_schema support from OpenAI protocol (#7654)
33	0	tests/entrypoints/openai/test_chat.py
12	2	vllm/entrypoints/openai/protocol.py
7	0	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
7	0	vllm/model_executor/guided_decoding/outlines_decoding.py

[8da48e4d9] Pooya Davoodi 2024-08-23 [Frontend] Publish  Prometheus metrics in run_batch API (#7641)
49	0	tests/entrypoints/openai/test_metrics.py
27	0	vllm/entrypoints/openai/run_batch.py

[6885fde31] Pooya Davoodi 2024-08-23 [Bugfix] Fix run_batch logger (#7640)
2	5	vllm/entrypoints/openai/run_batch.py

[9db93de20] Alexander Matveev 2024-08-23 [Core] Add multi-step support to LLMEngine (#7789)
2	1	.buildkite/test-pipeline.yaml
15	2	benchmarks/benchmark_throughput.py
1	1	tests/lora/test_gemma.py
0	0	tests/multi_step/{test_correctness.py => test_correctness_async_llm.py}
49	0	tests/multi_step/test_correctness_llm.py
2	72	vllm/engine/async_llm_engine.py
126	11	vllm/engine/llm_engine.py

[09c779261] Simon Mo 2024-08-23 Bump version to v0.5.5 (#7823)
1	1	vllm/version.py

[f1df5dbfd] Dipika Sikka 2024-08-23 [Misc] Update `marlin` to use vLLMParameters (#7803)
3	1	tests/weight_loading/models.txt
2	1	vllm/model_executor/layers/linear.py
36	32	vllm/model_executor/layers/quantization/marlin.py

[35ee2ad6b] youkaichao 2024-08-23 [github][misc] promote asking llm first (#7809)
7	0	.github/ISSUE_TEMPLATE/100-documentation.yml
7	0	.github/ISSUE_TEMPLATE/200-installation.yml
7	0	.github/ISSUE_TEMPLATE/300-usage.yml
7	0	.github/ISSUE_TEMPLATE/400-bug report.yml
7	0	.github/ISSUE_TEMPLATE/500-feature request.yml
7	0	.github/ISSUE_TEMPLATE/600-new model.yml
7	0	.github/ISSUE_TEMPLATE/700-performance discussion.yml
7	0	.github/ISSUE_TEMPLATE/750-RFC.yml
7	0	.github/ISSUE_TEMPLATE/800-misc discussion.yml

[e25fee57c] Maximilien de Bayser 2024-08-23 [BugFix] Fix server crash on empty prompt (#7746)
9	0	tests/entrypoints/llm/test_prompt_validation.py
22	0	tests/entrypoints/openai/test_prompt_validation.py
8	0	vllm/engine/llm_engine.py

[faeddb565] Jie Fu (傅杰) 2024-08-23 [misc] Add Torch profiler support for CPU-only devices (#7806)
26	0	vllm/worker/cpu_worker.py

[fc5ebbd1d] Kunshang Ji 2024-08-23 [Hardware][Intel GPU] refactor xpu_model_runner for tp (#7712)
17	366	vllm/executor/ray_xpu_executor.py
350	278	vllm/worker/xpu_model_runner.py
3	6	vllm/worker/xpu_worker.py

[c01a6cb23] SangBin Cho 2024-08-22 [Ray backend] Better error when pg topology is bad. (#7584)
1	0	.buildkite/test-pipeline.yaml
64	0	tests/distributed/test_multi_node_assignment.py
132	9	vllm/executor/ray_utils.py

[b903e1ba7] Joe Runde 2024-08-22 [Frontend] error suppression cleanup (#7786)
4	3	tests/entrypoints/openai/rpc/test_zmq_client.py
2	3	vllm/entrypoints/openai/api_server.py
12	1	vllm/entrypoints/openai/rpc/client.py

[a15224642] Siyuan Liu 2024-08-22 [Misc] fix typo in triton import warning (#7794)
1	1	vllm/triton_utils/importing.py

[666ad0aa1] Kevin H. Luu 2024-08-22 [ci] Cleanup & refactor Dockerfile to pass different Python versions and sccache bucket via build args (#7705)
26	35	Dockerfile

[15310b510] Michael Goin 2024-08-22 [Bugfix] Use LoadFormat values for `vllm serve --load-format` (#7784)
3	6	vllm/engine/arg_utils.py

[57792ed46] Peter Salas 2024-08-22 [Doc] Fix incorrect docs from #7615 (#7788)
2	2	docs/source/models/supported_models.rst
1	1	examples/offline_inference_audio_language.py

[d3b5b9802] Jiaxin Shan 2024-08-23 [Misc] Enhance prefix-caching benchmark tool (#6568)
140	4	benchmarks/benchmark_prefix_caching.py

[cc0eaf12b] Travis Johnson 2024-08-22 [Bugfix] spec decode handle None entries in topk args in create_sequence_group_output (#7232)
75	0	tests/spec_decode/e2e/test_logprobs.py
9	7	vllm/spec_decode/util.py

[955b5191c] Dipika Sikka 2024-08-22 [Misc] update fp8 to use `vLLMParameter` (#7437)
1	0	tests/weight_loading/models.txt
13	1	vllm/model_executor/layers/linear.py
29	15	vllm/model_executor/layers/quantization/fp8.py
8	1	vllm/model_executor/parameter.py

[55d63b121] Lucas Wilkinson 2024-08-22 [Bugfix] Don't build machete on cuda <12.0 (#7757)
35	27	CMakeLists.txt
12	0	csrc/quantization/machete/machete_pytorch.cu

[4f419c00a] Flex Wang 2024-08-22 Fix ShardedStateLoader for vllm fp8 quantization (#7708)
4	0	vllm/model_executor/model_loader/loader.py

[a3fce56b8] Abhinav Goyal 2024-08-22 [Speculative Decoding] EAGLE Implementation with Top-1 proposer (#6830)
268	0	tests/spec_decode/e2e/test_eagle_correctness.py
56	12	tests/spec_decode/e2e/test_medusa_correctness.py
35	1	tests/spec_decode/test_multi_step_worker.py
1	0	vllm/model_executor/models/__init__.py
161	0	vllm/model_executor/models/eagle.py
19	0	vllm/model_executor/models/medusa.py
60	9	vllm/sequence.py
19	0	vllm/spec_decode/draft_model_runner.py
8	2	vllm/spec_decode/multi_step_worker.py
72	25	vllm/spec_decode/spec_decode_worker.py
6	4	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
49	0	vllm/transformers_utils/configs/eagle.py
34	10	vllm/worker/model_runner.py
13	8	vllm/worker/multi_step_worker.py
1	1	vllm/worker/worker.py
50	11	vllm/worker/worker_base.py

[b3856bef7] Woosuk Kwon 2024-08-22 [Misc] Use torch.compile for GemmaRMSNorm (#7642)
23	6	vllm/model_executor/layers/layernorm.py

[8c6f694a7] youkaichao 2024-08-22 [ci] refine dependency for distributed tests (#7776)
17	8	.buildkite/test-pipeline.yaml

[eeee1c3b1] Woosuk Kwon 2024-08-21 [TPU] Avoid initializing TPU runtime in is_tpu (#7763)
4	2	vllm/platforms/__init__.py

[aae74ef95] Michael Goin 2024-08-21 Revert "[Kernel]  Expand MoE weight loading + Add Fused Marlin MoE Kernel (#7527)" (#7764)
1	2	CMakeLists.txt
0	1740	csrc/moe/marlin_moe_ops.cu
0	12	csrc/moe/marlin_moe_ops.h
0	9	csrc/moe/torch_bindings.cpp
0	2	tests/weight_loading/models.txt
0	14	vllm/_custom_ops.py
8	6	vllm/model_executor/layers/fused_moe/__init__.py
18	116	vllm/model_executor/layers/fused_moe/fused_moe.py
36	170	vllm/model_executor/layers/fused_moe/layer.py
0	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
0	283	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
18	11	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/model_loader/utils.py
1	1	vllm/model_executor/models/jamba.py
0	1	vllm/model_executor/models/mixtral.py

[cde9183b4] Joe Runde 2024-08-21 [Bug][Frontend] Improve ZMQ client robustness (#7443)
0	0	tests/entrypoints/openai/rpc/__init__.py
119	0	tests/entrypoints/openai/rpc/test_zmq_client.py
3	2	vllm/entrypoints/openai/api_server.py
0	4	vllm/entrypoints/openai/rpc/__init__.py
48	22	vllm/entrypoints/openai/rpc/client.py
6	0	vllm/envs.py

[df1a21131] zifeitong 2024-08-21 [Model] Fix Phi-3.5-vision-instruct 'num_crops' issue (#7710)
2	2	docs/source/models/supported_models.rst
1	1	tests/models/test_phi3v.py
5	1	vllm/config.py
9	2	vllm/inputs/registry.py
6	6	vllm/model_executor/models/phi3v.py
14	1	vllm/transformers_utils/config.py

[7937009a7] Luka Govedič 2024-08-21 [Kernel] Replaced `blockReduce[...]` functions with `cub::BlockReduce` (#7233)
2	2	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-QQQ.yaml
89	0	benchmarks/kernels/benchmark_layernorm.py
103	0	benchmarks/kernels/benchmark_quant.py
19	14	csrc/layernorm_kernels.cu
12	2	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
11	2	csrc/quantization/fp8/common.cu
0	95	csrc/reduction_utils.cuh
1	1	tests/basic_correctness/test_chunked_prefill.py

[998460541] Gregory Shtrasberg 2024-08-21 [AMD][CI/Build] Disambiguation of the function call for ROCm 6.2 headers compatibility (#7477)
1	1	csrc/attention/attention_utils.cuh

[7eebe8cca] youkaichao 2024-08-21 [distributed][misc] error on same VLLM_HOST_IP setting (#7756)
4	1	vllm/envs.py
13	0	vllm/executor/ray_gpu_executor.py

[8678a69ab] Dipika Sikka 2024-08-21 [Kernel]  Expand MoE weight loading + Add Fused Marlin MoE Kernel (#7527)
2	1	CMakeLists.txt
1740	0	csrc/moe/marlin_moe_ops.cu
12	0	csrc/moe/marlin_moe_ops.h
9	0	csrc/moe/torch_bindings.cpp
2	0	tests/weight_loading/models.txt
14	0	vllm/_custom_ops.py
6	8	vllm/model_executor/layers/fused_moe/__init__.py
116	18	vllm/model_executor/layers/fused_moe/fused_moe.py
170	36	vllm/model_executor/layers/fused_moe/layer.py
5	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
283	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
11	18	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/model_loader/utils.py
1	1	vllm/model_executor/models/jamba.py
1	0	vllm/model_executor/models/mixtral.py

[584401728] William Lin 2024-08-21 [ci] [multi-step] narrow multi-step test dependency paths (#7760)
11	3	.buildkite/test-pipeline.yaml

[1ca0d4f86] Peter Salas 2024-08-21 [Model] Add UltravoxModel and UltravoxConfig (#7615)
6	1	docs/source/models/supported_models.rst
97	0	examples/offline_inference_audio_language.py
90	0	examples/openai_audio_api_client.py
19	12	tests/conftest.py
2	1	tests/distributed/test_basic_distributed_correctness_enc_dec.py
23	125	tests/entrypoints/openai/test_audio.py
2	1	tests/models/test_bart.py
3	2	tests/models/test_blip2.py
2	2	tests/models/test_chameleon.py
3	2	tests/models/test_llava.py
3	2	tests/models/test_llava_image_embeds.py
3	2	tests/models/test_llava_next.py
3	2	tests/models/test_paligemma.py
1	1	tests/models/test_qwen.py
151	0	tests/models/test_ultravox.py
26	0	vllm/assets/audio.py
4	2	vllm/entrypoints/chat_utils.py
2	1	vllm/model_executor/models/__init__.py
4	4	vllm/model_executor/models/blip.py
4	4	vllm/model_executor/models/chameleon.py
4	4	vllm/model_executor/models/clip.py
2	2	vllm/model_executor/models/fuyu.py
1	1	vllm/model_executor/models/internvl.py
2	2	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/paligemma.py
1	1	vllm/model_executor/models/phi3v.py
4	4	vllm/model_executor/models/siglip.py
435	0	vllm/model_executor/models/ultravox.py
0	83	vllm/multimodal/image.py
89	1	vllm/multimodal/utils.py
2	1	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
99	0	vllm/transformers_utils/configs/ultravox.py

[dd53c4b02] William Lin 2024-08-21 [misc] Add Torch profiler support (#7451)
2	2	benchmarks/backend_request_func.py
43	0	benchmarks/benchmark_serving.py
33	0	docs/source/dev/profiling/profiling_index.rst
1	0	docs/source/index.rst
6	0	vllm/engine/async_llm_engine.py
8	0	vllm/engine/protocol.py
20	0	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/rpc/__init__.py
14	0	vllm/entrypoints/openai/rpc/client.py
24	0	vllm/entrypoints/openai/rpc/server.py
7	0	vllm/envs.py
31	0	vllm/worker/worker.py

[970dfdc01] Robert Shaw 2024-08-21 [Frontend] Improve Startup Failure UX (#7716)
16	13	tests/entrypoints/openai/test_mp_api_server.py
21	6	vllm/entrypoints/openai/api_server.py

[91f4522cb] William Lin 2024-08-21 [multi-step] Raise error if not using async engine (#7703)
5	0	vllm/engine/llm_engine.py

[1b32e0264] sasha0552 2024-08-21 [Bugfix] Pass PYTHONPATH from setup.py to CMake (#7730)
1	1	CMakeLists.txt
4	0	setup.py

[f7e3b0c5a] Robert Shaw 2024-08-21 [Bugfix][Frontend] Fix Issues Under High Load With `zeromq` Frontend (#7394)
1	0	.buildkite/test-pipeline.yaml
55	0	tests/entrypoints/openai/test_accuracy.py
5	0	vllm/engine/async_llm_engine.py
4	0	vllm/engine/protocol.py
9	0	vllm/entrypoints/launcher.py
6	5	vllm/entrypoints/openai/api_server.py
12	2	vllm/entrypoints/openai/rpc/__init__.py
180	68	vllm/entrypoints/openai/rpc/client.py
50	66	vllm/entrypoints/openai/rpc/server.py

[d3c002ead] Brian Li 2024-08-22 [Bugfix] chat method add_generation_prompt param (#7734)
3	3	vllm/entrypoints/llm.py

[9b73a2f49] Nick Hill 2024-08-21 [Spec Decoding] Use target model max length as default for draft model (#7706)
10	1	vllm/config.py

[6925cdbee] Isotr0py 2024-08-22 [Bugfix][Hardware][CPU] Fix `mm_limits` initialization for CPU backend (#7735)
3	1	vllm/worker/cpu_model_runner.py

[53328d753] LI MOU 2024-08-21 [BUG] fix crash on flashinfer backend with cudagraph disabled, when attention group_size not in [1,2,4,8] (#7509)
5	2	tests/kernels/test_flashinfer.py
4	2	vllm/attention/backends/flashinfer.py

[c75363fbc] Nick Hill 2024-08-21 [BugFix] Avoid premature async generator exit and raise all exception variations (#7698)
88	13	tests/async_engine/test_async_llm_engine.py
13	8	vllm/engine/async_llm_engine.py

[dd3fa0e43] sasha0552 2024-08-21 [Bugfix] Mirror jinja2 in pyproject.toml (#7723)
1	0	pyproject.toml

[baaedfdb2] Cyrus Leung 2024-08-21 [mypy] Enable following imports for entrypoints (#7248)
0	1	.github/workflows/mypy.yaml
1	1	docs/requirements-docs.txt
0	1	format.sh
1	0	pyproject.toml
1	1	requirements-common.txt
83	1	tests/entrypoints/openai/test_chat.py
5	96	tests/entrypoints/openai/test_completion.py
4	4	vllm/engine/async_llm_engine.py
21	10	vllm/engine/llm_engine.py
2	3	vllm/engine/output_processor/interfaces.py
2	3	vllm/engine/output_processor/multi_step.py
2	4	vllm/engine/output_processor/stop_checker.py
11	6	vllm/engine/protocol.py
3	0	vllm/entrypoints/api_server.py
24	18	vllm/entrypoints/chat_utils.py
15	15	vllm/entrypoints/llm.py
28	20	vllm/entrypoints/openai/api_server.py
27	4	vllm/entrypoints/openai/cli_args.py
6	5	vllm/entrypoints/openai/logits_processors.py
58	33	vllm/entrypoints/openai/protocol.py
3	3	vllm/entrypoints/openai/rpc/server.py
33	36	vllm/entrypoints/openai/serving_chat.py
61	36	vllm/entrypoints/openai/serving_completion.py
26	18	vllm/entrypoints/openai/serving_embedding.py
1	1	vllm/entrypoints/openai/serving_engine.py
62	0	vllm/sampling_params.py

[450664121] Roger Wang 2024-08-20 [Doc] Section for Multimodal Language Models (#7719)
13	3	docs/source/models/supported_models.rst

[12e1c65bc] Isotr0py 2024-08-21 [Model] Add AWQ quantization support for InternVL2 model (#7187)
102	1	tests/models/test_internvl.py
4	2	vllm/model_executor/layers/linear.py
4	0	vllm/model_executor/model_loader/weight_utils.py
13	22	vllm/model_executor/models/internlm2.py

[b74a12580] youkaichao 2024-08-20 [ci] try to log process using the port to debug the port usage (#7711)
7	0	vllm/entrypoints/launcher.py
10	0	vllm/utils.py

[66a9e713a] Antoni Baum 2024-08-20 [Core] Pipe `worker_class_fn` argument in Executor (#7707)
17	9	vllm/executor/gpu_executor.py
3	2	vllm/executor/ray_gpu_executor.py
6	3	vllm/executor/xpu_executor.py

[9e51b6a62] youkaichao 2024-08-20 [ci][test] adjust max wait time for cpu offloading test (#7709)
18	9	tests/quantization/test_cpu_offload.py
16	13	tests/utils.py

[6e4658c7a] Kunshang Ji 2024-08-21 [Intel GPU] fix xpu not support punica kernel (which use torch.library.custom_op) (#7685)
3	1	vllm/lora/punica.py

[3b682179d] Antoni Baum 2024-08-20 [Core] Add `AttentionState` abstraction (#7663)
6	1	tests/worker/test_model_input.py
2	1	vllm/attention/__init__.py
50	1	vllm/attention/backends/abstract.py
6	1	vllm/attention/backends/blocksparse_attn.py
6	1	vllm/attention/backends/flash_attn.py
164	1	vllm/attention/backends/flashinfer.py
5	0	vllm/attention/backends/ipex_attn.py
6	1	vllm/attention/backends/openvino.py
5	0	vllm/attention/backends/pallas.py
6	1	vllm/attention/backends/rocm_flash_attn.py
5	0	vllm/attention/backends/torch_sdpa.py
73	2	vllm/attention/backends/utils.py
6	1	vllm/attention/backends/xformers.py
1	46	vllm/spec_decode/draft_model_runner.py
3	2	vllm/worker/enc_dec_model_runner.py
28	188	vllm/worker/model_runner.py

[c6af027a3] Lucas Wilkinson 2024-08-20 [Misc] Add jinja2 as an explicit build requirement (#7695)
1	0	requirements-build.txt

[2aa00d59a] Ronen Schaffer 2024-08-20 [CI/Build] Pin OpenTelemetry versions and make errors clearer (#7266)
4	4	.buildkite/test-pipeline.yaml
4	4	examples/production_monitoring/Otel.md
6	4	vllm/config.py
17	7	vllm/tracing.py

[c42590f97] Kunshang Ji 2024-08-21 [Hardware] [Intel GPU]  refactor xpu worker/executor (#7686)
15	23	vllm/executor/xpu_executor.py
0	1	vllm/worker/xpu_model_runner.py
11	4	vllm/worker/xpu_worker.py

[aae6927be] Isotr0py 2024-08-20 [VLM][Model] Add test for InternViT vision encoder (#7409)
18	1	tests/conftest.py
80	0	tests/models/test_intern_vit.py

[398521ad1] Ilya Lavrenov 2024-08-20 [OpenVINO] Updated documentation (#7687)
1	3	docs/source/getting_started/openvino-installation.rst

[5288c06aa] Lucas Wilkinson 2024-08-20 [Kernel] (1/N) Machete - Hopper Optimized Mixed Precision Linear Kernel  (#7174)
3	0	.gitignore
40	0	CMakeLists.txt
372	0	benchmarks/kernels/benchmark_machete.py
64	0	benchmarks/kernels/graph_machete_bench.py
43	0	benchmarks/kernels/weight_shapes.py
10	0	csrc/cuda_utils.h
68	0	csrc/cutlass_extensions/cute_utils.cuh
154	0	csrc/cutlass_extensions/torch_utils.hpp
43	0	csrc/cutlass_extensions/vllm_collective_builder.cuh
50	0	csrc/cutlass_extensions/vllm_custom_types.cuh
49	0	csrc/cutlass_extensions/vllm_cutlass_library_extension.py
795	0	csrc/cutlass_extensions/vllm_numeric_conversion.cuh
19	0	csrc/ops.h
45	0	csrc/quantization/machete/Readme.md
446	0	csrc/quantization/machete/generate.py
33	0	csrc/quantization/machete/machete_collective_builder.cuh
35	0	csrc/quantization/machete/machete_interleaving_utils.cuh
1473	0	csrc/quantization/machete/machete_mainloop.cuh
237	0	csrc/quantization/machete/machete_mm_kernel.cuh
95	0	csrc/quantization/machete/machete_mm_launcher.cuh
62	0	csrc/quantization/machete/machete_prepack_kernel.cuh
71	0	csrc/quantization/machete/machete_prepack_launcher.cuh
220	0	csrc/quantization/machete/machete_prepacked_layout.cuh
79	0	csrc/quantization/machete/machete_pytorch.cu
15	0	csrc/torch_bindings.cpp
272	0	tests/kernels/test_machete_gemm.py
26	0	vllm/_custom_ops.py
9	2	vllm/model_executor/layers/quantization/utils/quant_utils.py

[b6f99a6ff] Kunshang Ji 2024-08-20 [Core] Refactor executor classes for easier inheritance (#7673)
16	11	vllm/executor/gpu_executor.py
11	10	vllm/executor/ray_gpu_executor.py

[ad28a74be] youkaichao 2024-08-20 [misc][cuda] add warning for pynvml user (#7675)
6	0	vllm/platforms/cuda.py

[e6d811dd1] jianyizh 2024-08-20 [XPU] fallback to native implementation for xpu custom op (#7670)
3	1	vllm/model_executor/custom_op.py

[c4be16e1a] youkaichao 2024-08-19 [misc] add nvidia related library in collect env (#7674)
4	0	collect_env.py

[3d8a5f063] Kuntai Du 2024-08-19 [CI] Organizing performance benchmark files (#7616)
5	4	.buildkite/nightly-benchmarks/README.md
1	1	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
0	0	.buildkite/nightly-benchmarks/{tests/descriptions.md => performance-benchmarks-descriptions.md}
2	2	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
28	17	.buildkite/nightly-benchmarks/{run-benchmarks-suite.sh => scripts/run-performance-benchmarks.sh}

[f4fc7337b] Zijian Hu 2024-08-19 [Bugfix] support `tie_word_embeddings` for all models (#5724)
2	0	vllm/model_executor/models/arctic.py
2	0	vllm/model_executor/models/baichuan.py
2	0	vllm/model_executor/models/bart.py
3	0	vllm/model_executor/models/blip2.py
7	2	vllm/model_executor/models/bloom.py
3	0	vllm/model_executor/models/chatglm.py
3	0	vllm/model_executor/models/commandr.py
3	0	vllm/model_executor/models/dbrx.py
2	0	vllm/model_executor/models/deepseek.py
2	0	vllm/model_executor/models/gemma.py
2	0	vllm/model_executor/models/gemma2.py
6	2	vllm/model_executor/models/gpt2.py
8	2	vllm/model_executor/models/gpt_bigcode.py
3	1	vllm/model_executor/models/gpt_neox.py
2	0	vllm/model_executor/models/internlm2.py
6	2	vllm/model_executor/models/jais.py
2	2	vllm/model_executor/models/llava.py
2	2	vllm/model_executor/models/llava_next.py
4	0	vllm/model_executor/models/minicpmv.py
2	0	vllm/model_executor/models/mixtral.py
2	0	vllm/model_executor/models/mixtral_quant.py
6	2	vllm/model_executor/models/opt.py
2	0	vllm/model_executor/models/orion.py
2	0	vllm/model_executor/models/phi.py
2	1	vllm/model_executor/models/phi3_small.py
2	0	vllm/model_executor/models/phi3v.py
2	0	vllm/model_executor/models/qwen.py
2	0	vllm/model_executor/models/qwen2_moe.py
2	0	vllm/model_executor/models/stablelm.py
2	0	vllm/model_executor/models/xverse.py

[0df7ec0b2] Kevin H. Luu 2024-08-19 [ci] Install Buildkite test suite analysis (#7667)
2	1	requirements-test.txt

[312f76123] Abhinav Goyal 2024-08-20 [Speculative Decoding] Fixing hidden states handling in batch expansion (#7508)
16	9	tests/spec_decode/e2e/conftest.py
42	0	tests/spec_decode/e2e/test_mlp_correctness.py
60	26	vllm/spec_decode/batch_expansion.py
2	3	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/spec_decode/top1_proposer.py
18	2	vllm/spec_decode/util.py

[e54ebc2f8] youkaichao 2024-08-19 [doc] fix doc build error caused by msgspec (#7659)
1	0	docs/requirements-docs.txt
39	8	vllm/platforms/__init__.py

[67e02fa8a] Travis Johnson 2024-08-19 [Bugfix] use StoreBoolean instead of type=bool for --disable-logprobs-during-spec-decoding (#7665)
3	1	vllm/engine/arg_utils.py

[43735bf5e] Woosuk Kwon 2024-08-19 [TPU] Remove redundant input tensor cloning (#7660)
8	20	vllm/worker/tpu_model_runner.py

[da115230f] Andrew Song 2024-08-19 [Bugfix] Don't disable existing loggers (#7664)
1	0	vllm/logger.py

[7601cb044] Isotr0py 2024-08-20 [Core] Support tensor parallelism for GGUF quantization (#7520)
19	5	tests/models/test_gguf.py
20	6	vllm/model_executor/layers/linear.py
0	4	vllm/model_executor/layers/quantization/gguf.py

[47b65a550] William Lin 2024-08-19 [core] Multi Step Scheduling (#7000)
9	0	.buildkite/test-pipeline.yaml
0	0	tests/multi_step/__init__.py
85	0	tests/multi_step/test_correctness.py
77	0	tests/worker/test_model_input.py
6	1	vllm/engine/arg_utils.py
127	8	vllm/engine/async_llm_engine.py
10	4	vllm/executor/gpu_executor.py
3	0	vllm/executor/ray_gpu_executor.py
5	5	vllm/sequence.py
34	12	vllm/worker/model_runner_base.py
453	0	vllm/worker/multi_step_model_runner.py
189	0	vllm/worker/multi_step_worker.py
6	4	vllm/worker/worker_base.py

[dad961ef5] Ali Panahi 2024-08-19 [Bugfix] fix lora_dtype value type in arg_utils.py - part 2 (#5428)
3	1	vllm/engine/arg_utils.py

[3ac50b47d] Cody Yu 2024-08-19 [MISC] Add prefix cache hit rate to metrics (#7606)
26	0	tests/core/block/test_prefix_caching_block.py
7	0	tests/prefix_caching/test_prefix_caching.py
53	0	vllm/core/block/common.py
5	0	vllm/core/block/cpu_gpu_block_allocator.py
10	0	vllm/core/block/interfaces.py
3	0	vllm/core/block/naive_block.py
8	2	vllm/core/block/prefix_caching_block.py
27	4	vllm/core/block_manager_v1.py
3	0	vllm/core/block_manager_v2.py
4	0	vllm/core/embedding_model_block_manager.py
8	7	vllm/core/evictor_v2.py
6	0	vllm/core/interfaces.py
4	1	vllm/core/scheduler.py
11	1	vllm/engine/llm_engine.py
22	1	vllm/engine/metrics.py
3	0	vllm/engine/metrics_types.py

[df845b2b4] Woosuk Kwon 2024-08-19 [Misc] Remove Gemma RoPE (#7638)
0	15	vllm/model_executor/layers/rotary_embedding.py
3	5	vllm/model_executor/models/gemma.py
4	6	vllm/model_executor/models/gemma2.py

[1a36287b8] Kunshang Ji 2024-08-19 [Bugfix] Fix xpu build (#7644)
1	1	setup.py

[f710fb526] Peng Guanwen 2024-08-19 [Core] Use flashinfer sampling kernel when available (#7137)
3	1	.buildkite/test-pipeline.yaml
1	1	Dockerfile
36	1	tests/samplers/test_sampler.py
5	0	vllm/envs.py
85	25	vllm/model_executor/layers/sampler.py

[ff7ec82c4] SangBin Cho 2024-08-18 [Core] Optimize SPMD architecture with delta + serialization optimization (#7109)
1	0	requirements-common.txt
18	0	tests/basic_correctness/test_preemption.py
33	0	tests/core/test_serialization.py
2	1	tests/distributed/test_basic_distributed_correctness.py
7	0	tests/distributed/test_chunked_prefill_distributed.py
19	6	tests/samplers/test_sampler.py
6	3	tests/spec_decode/utils.py
6	2	tests/test_logits_processor.py
5	2	tests/test_sequence.py
11	5	tests/worker/test_encoder_decoder_model_runner.py
10	5	tests/worker/test_model_runner.py
0	2	vllm/adapter_commons/request.py
9	2	vllm/config.py
50	38	vllm/core/scheduler.py
3	0	vllm/engine/arg_utils.py
0	1	vllm/engine/llm_engine.py
27	0	vllm/executor/msgspec_utils.py
15	4	vllm/executor/ray_gpu_executor.py
27	12	vllm/executor/ray_utils.py
7	1	vllm/inputs/registry.py
9	5	vllm/lora/request.py
6	3	vllm/model_executor/models/blip.py
7	3	vllm/model_executor/models/blip2.py
7	3	vllm/model_executor/models/chameleon.py
6	3	vllm/model_executor/models/clip.py
9	4	vllm/model_executor/models/fuyu.py
4	2	vllm/model_executor/models/minicpmv.py
6	3	vllm/model_executor/models/siglip.py
6	3	vllm/model_executor/sampling_metadata.py
7	4	vllm/pooling_params.py
7	3	vllm/prompt_adapter/request.py
72	75	vllm/sampling_params.py
250	146	vllm/sequence.py
8	5	vllm/spec_decode/batch_expansion.py
5	3	vllm/spec_decode/metrics.py
62	2	vllm/worker/worker.py

[200a2ffa6] Woosuk Kwon 2024-08-18 [Misc] Refactor Llama3 RoPE initialization (#7637)
53	29	vllm/model_executor/layers/rotary_embedding.py

[40e1360bb] Alex Brooks 2024-08-18 [CI/Build] Add text-only test for Qwen models  (#7475)
1	1	.buildkite/run-cpu-test.sh
3	1	requirements-test.txt
48	0	tests/models/test_qwen.py

[e3b318216] Robert Shaw 2024-08-18 [ Bugfix ] Fix Prometheus Metrics With `zeromq` Frontend (#7279)
0	9	tests/entrypoints/openai/test_basic.py
179	0	tests/entrypoints/openai/test_metrics.py
1	1	vllm/engine/async_llm_engine.py
8	2	vllm/engine/llm_engine.py
48	101	vllm/engine/metrics.py
85	0	vllm/engine/metrics_types.py
45	3	vllm/entrypoints/openai/api_server.py

[ab7165f2c] Woosuk Kwon 2024-08-18 [TPU] Optimize RoPE forward_native2 (#7636)
27	26	vllm/model_executor/layers/rotary_embedding.py

[0c2fa50b8] Woosuk Kwon 2024-08-18 [TPU] Use mark_dynamic only for dummy run (#7634)
28	48	vllm/worker/tpu_model_runner.py

[ce143353c] Woosuk Kwon 2024-08-17 [TPU] Skip creating empty tensor (#7630)
4	1	vllm/worker/tpu_worker.py

[bbf55c480] Roger Wang 2024-08-17 [VLM] Refactor `MultiModalConfig` initialization and profiling (#7530)
6	2	tests/entrypoints/openai/test_audio.py
9	9	tests/multimodal/test_mapper.py
33	5	vllm/config.py
3	6	vllm/engine/arg_utils.py
1	6	vllm/engine/llm_engine.py
0	1	vllm/executor/cpu_executor.py
2	4	vllm/executor/executor_base.py
0	1	vllm/executor/gpu_executor.py
0	1	vllm/executor/openvino_executor.py
2	6	vllm/executor/ray_xpu_executor.py
0	1	vllm/executor/tpu_executor.py
2	5	vllm/executor/xpu_executor.py
1	15	vllm/inputs/registry.py
1	4	vllm/model_executor/model_loader/__init__.py
14	32	vllm/model_executor/model_loader/loader.py
27	16	vllm/model_executor/models/__init__.py
2	2	vllm/multimodal/registry.py
2	4	vllm/spec_decode/draft_model_runner.py
2	4	vllm/spec_decode/target_model_runner.py
2	5	vllm/worker/cpu_model_runner.py
2	5	vllm/worker/cpu_worker.py
2	4	vllm/worker/embedding_model_runner.py
9	14	vllm/worker/enc_dec_model_runner.py
9	14	vllm/worker/model_runner.py
1	4	vllm/worker/tpu_model_runner.py
1	4	vllm/worker/tpu_worker.py
1	1	vllm/worker/utils.py
2	5	vllm/worker/worker.py
7	10	vllm/worker/xpu_model_runner.py

[1ef13cf92] Jee Jee Li 2024-08-18 [Misc]Fix BitAndBytes exception messages (#7626)
2	2	vllm/model_executor/model_loader/loader.py

[832163b87] youkaichao 2024-08-17 [ci][test] allow longer wait time for api server (#7629)
1	1	tests/utils.py

[e73f76eec] Besher Alkurdi 2024-08-17 [Model] Pipeline parallel support for JAIS (#7603)
1	0	vllm/config.py
50	18	vllm/model_executor/models/jais.py

[d95cc0a55] youkaichao 2024-08-16 [core][misc] update libcudart finding (#7620)
7	2	vllm/distributed/device_communicators/cuda_wrapper.py

[5bf45db7d] youkaichao 2024-08-16 [ci][test] fix engine/logger test (#7621)
2	1	tests/test_logger.py

[eed020f67] youkaichao 2024-08-16 [misc] use nvml to get consistent device name (#7582)
2	1	vllm/model_executor/layers/fused_moe/fused_moe.py
34	0	vllm/platforms/cuda.py
4	0	vllm/platforms/interface.py
5	0	vllm/platforms/rocm.py
1	1	vllm/worker/worker.py

[7c0b7ea21] Xander Johnson 2024-08-16 [Bugfix] add >= 1.0 constraint for openai dependency (#7612)
1	1	requirements-common.txt

[4706eb628] SangBin Cho 2024-08-16 [aDAG] Unflake aDAG + PP tests (#7600)
2	2	.buildkite/test-pipeline.yaml
4	0	tests/distributed/test_pipeline_parallel.py
3	2	tests/utils.py

[bae888cb8] Rui Qiao 2024-08-16 [Bugfix] Clear engine reference in AsyncEngineRPCServer (#7618)
2	0	vllm/entrypoints/openai/rpc/server.py

[6bd19551b] Alexei-V-Ivanov-AMD 2024-08-16 .[Build/CI] Enabling passing AMD tests. (#7610)
6	7	.buildkite/test-pipeline.yaml

[e68034999] bnellnm 2024-08-16 [Bugfix] Fix custom_ar support check (#7617)
1	1	vllm/distributed/device_communicators/custom_all_reduce.py

[44f26a946] Michael Goin 2024-08-16 [Model] Align nemotron config with final HF state and fix lm-eval-small (#7611)
4	4	.buildkite/lm-eval-harness/configs/{Minitron-4B-Base.yaml => Minitron-4B-Base-FP8.yaml}
1	1	.buildkite/lm-eval-harness/configs/models-small.txt
3	3	vllm/model_executor/layers/rotary_embedding.py
3	3	vllm/model_executor/models/nemotron.py
18	24	vllm/transformers_utils/configs/nemotron.py

[37fd47e78] bnellnm 2024-08-16 [Kernel] fix types used in aqlm and ggml kernels to support dynamo (#7596)
8	8	csrc/ops.h
13	12	csrc/quantization/aqlm/gemm_kernels.cu
1	1	csrc/quantization/gguf/dequantize.cuh
4	4	csrc/quantization/gguf/gguf_kernel.cu
7	21	vllm/_custom_ops.py
5	7	vllm/model_executor/layers/quantization/aqlm.py
1	0	vllm/model_executor/layers/quantization/gptq.py

[7759ae958] bnellnm 2024-08-16 [Kernel][Misc] dynamo support for ScalarType (#7594)
30	0	csrc/core/scalar_type.hpp
119	24	vllm/_core_ext.py

[9f6985635] bnellnm 2024-08-16 [Kernel] register punica functions as torch ops (#7591)
8	12	vllm/lora/ops/bgmv_expand.py
8	12	vllm/lora/ops/bgmv_expand_slice.py
9	12	vllm/lora/ops/bgmv_shrink.py
7	2	vllm/lora/ops/sgmv_expand.py
7	2	vllm/lora/ops/sgmv_expand_slice.py
7	2	vllm/lora/ops/sgmv_shrink.py

[d4f0f17b0] Michael Goin 2024-08-16 [Doc] Update quantization supported hardware table (#7595)
127	14	docs/source/quantization/supported_hardware.rst

[b3f4e1793] Michael Goin 2024-08-16 [Doc] Add docs for llmcompressor INT8 and FP8 checkpoints (#7444)
1	0	docs/source/index.rst
106	111	docs/source/quantization/fp8.rst
145	0	docs/source/quantization/int8.rst

[93478b63d] Mahesh Keralapura 2024-08-16 [Core] Fix tracking of model forward time in case of PP>1 (#7440)
5	3	.buildkite/test-pipeline.yaml
66	0	tests/tracing/test_tracing.py
1	1	vllm/core/scheduler.py
0	5	vllm/engine/arg_utils.py
22	2	vllm/worker/model_runner.py

[f366f6339] William Lin 2024-08-16 [spec decode] [4/N] Move update_flash_attn_metadata to attn backend (#7571)
3	0	vllm/attention/backends/abstract.py
45	0	vllm/attention/backends/flash_attn.py
1	33	vllm/spec_decode/draft_model_runner.py

[855866caa] Michael Goin 2024-08-16 [Kernel] Add tuned triton configs for ExpertsInt8 (#7601)
1	1	.gitignore
146	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3072,device_name=NVIDIA_H100_80GB_HBM3.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=1,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=14336,device_name=NVIDIA_A100-SXM4-80GB.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3072,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
218	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB,dtype=int8_w8a16.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=int8_w8a16.json

[7fc23be81] Mor Zusman 2024-08-16 [Kernel] W8A16 Int8 inside FusedMoE  (#7415)
70	38	benchmarks/kernels/benchmark_moe.py
7	6	tests/models/test_jamba.py
28	0	tests/quantization/test_experts_int8.py
2	1	vllm/config.py
0	0	vllm/model_executor/layers/fused_moe/configs/{E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}
0	0	vllm/model_executor/layers/fused_moe/configs/{E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}
0	0	vllm/model_executor/layers/fused_moe/configs/{E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}
0	0	vllm/model_executor/layers/fused_moe/configs/{E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}
0	0	vllm/model_executor/layers/fused_moe/configs/{E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}
0	0	vllm/model_executor/layers/fused_moe/configs/{E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json => E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8.json}
91	53	vllm/model_executor/layers/fused_moe/fused_moe.py
3	0	vllm/model_executor/layers/quantization/__init__.py
175	0	vllm/model_executor/layers/quantization/experts_int8.py
1	1	vllm/model_executor/layers/quantization/fp8.py
35	37	vllm/model_executor/models/jamba.py

[e837b624f] Charlie Fu 2024-08-16 [Feature][Hardware][Amd] Add fp8 Linear Layer for Rocm (#7210)
32	17	csrc/quantization/fp8/common.cu
22	11	tests/kernels/quant_utils.py
3	3	tests/kernels/test_fp8_quant.py
4	1	vllm/_custom_ops.py
1	1	vllm/config.py
55	9	vllm/model_executor/layers/quantization/fp8.py
47	7	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[ec724a725] fzyzcjy 2024-08-17 support tqdm in notebooks (#7510)
1	1	vllm/entrypoints/llm.py

[0e39a33c6] Gordon Wong 2024-08-17 [Bugfix][Hardware][AMD][Frontend] add quantization param to embedding checking method (#7513)
5	2	vllm/entrypoints/openai/api_server.py

[6fc5b0f24] Kuntai Du 2024-08-16 [CI] Fix crashes of performance benchmark  (#7500)
4	14	.buildkite/nightly-benchmarks/run-benchmarks-suite.sh
7	12	.buildkite/nightly-benchmarks/tests/descriptions.md
2	2	.buildkite/nightly-benchmarks/tests/latency-tests.json
6	6	.buildkite/nightly-benchmarks/tests/serving-tests.json
2	2	.buildkite/nightly-benchmarks/tests/throughput-tests.json

[9587b050f] Nick Hill 2024-08-15 [Core] Use uvloop with zmq-decoupled front-end (#7570)
2	1	vllm/entrypoints/openai/rpc/server.py

[54bd9a03c] youkaichao 2024-08-15 register custom op for flash attn and use from torch.ops (#7536)
7	0	.buildkite/test-pipeline.yaml
20	0	tests/compile/test_full_graph.py
61	12	tests/kernels/test_flash_attn.py
129	26	vllm/attention/backends/flash_attn.py
3	3	vllm/attention/backends/flashinfer.py

[50b8d08db] jon-chuang 2024-08-15 [Misc/Testing] Use `torch.testing.assert_close` (#7324)
9	9	tests/distributed/test_comm_ops.py
4	4	tests/distributed/test_custom_all_reduce.py
1	1	tests/kernels/quant_utils.py
5	5	tests/kernels/test_activation.py
2	2	tests/kernels/test_attention.py
2	2	tests/kernels/test_blocksparse_attention.py
27	27	tests/kernels/test_cache.py
13	10	tests/kernels/test_cutlass.py
2	2	tests/kernels/test_flash_attn.py
2	2	tests/kernels/test_flashinfer.py
7	7	tests/kernels/test_fp8_quant.py
7	5	tests/kernels/test_int8_quant.py
3	3	tests/kernels/test_layernorm.py
2	2	tests/kernels/test_marlin_gemm.py
5	5	tests/kernels/test_moe.py
24	24	tests/kernels/test_pos_encoding.py
4	4	tests/kernels/test_sampler.py
2	2	tests/kernels/utils.py
44	44	tests/lora/test_layers.py
12	12	tests/lora/test_lora_manager.py
5	3	tests/quantization/test_fp8.py
1	1	tests/samplers/test_sampler.py
1	1	tests/spec_decode/utils.py
4	2	tests/test_logits_processor.py
9	9	tests/worker/test_model_runner.py

[e16552877] Michael Goin 2024-08-16 [CI] Move quantization cpu offload tests out of fastcheck (#7574)
0	55	tests/basic_correctness/test_cpu_offload.py
59	0	tests/quantization/test_cpu_offload.py

[3b19e39dc] nunjunj 2024-08-16 Chat method for offline llm (#5049)
1	0	.buildkite/test-pipeline.yaml
53	0	examples/offline_inference_chat.py
19	0	tests/entrypoints/llm/test_generate.py
95	29	vllm/entrypoints/llm.py

[4cd7d47fe] youkaichao 2024-08-15 [ci/test] rearrange tests and make adag test soft fail (#7572)
2	0	.buildkite/test-pipeline.yaml
18	31	tests/distributed/test_pipeline_parallel.py
30	0	tests/distributed/test_pp_cudagraph.py

[f878c8feb] Grant Pinkert 2024-08-16 [Feature]: Add OpenAI server prompt_logprobs support #6508 (#7453)
124	1	tests/entrypoints/openai/test_completion.py
9	2	vllm/entrypoints/openai/protocol.py
11	0	vllm/entrypoints/openai/serving_chat.py
10	0	vllm/entrypoints/openai/serving_completion.py

[b67ae00cd] shangmingc 2024-08-16 [Misc] Add quantization config support for speculative model. (#7343)
48	0	tests/spec_decode/e2e/test_integration.py
8	4	vllm/config.py
15	0	vllm/engine/arg_utils.py

[9c8e2d116] Michael Goin 2024-08-15 [Bugfix][Harmless] Fix float16 dtype for model_is_embedding (#7566)
1	1	vllm/entrypoints/openai/api_server.py

[21313e09e] Michael Goin 2024-08-15 [Bugfix] Fix default weight loading for scalars (#7534)
11	5	vllm/model_executor/model_loader/weight_utils.py

[f4da5f7b6] PHILO-HE 2024-08-16 [Misc] Update dockerfile for CPU to cover protobuf installation (#7182)
1	1	.buildkite/run-cpu-test.sh
1	1	Dockerfile.cpu
1	0	requirements-common.txt

[9c1f78d5d] omrishiv 2024-08-15 [Bugfix] update neuron for version > 0.5.0 (#7175)
1	1	vllm/engine/arg_utils.py
2	3	vllm/executor/neuron_executor.py
1	0	vllm/worker/neuron_model_runner.py
3	0	vllm/worker/neuron_worker.py

[fc93e5614] Woosuk Kwon 2024-08-15 [Bugfix][TPU] Correct env variable for XLA cache path (#7544)
1	1	vllm/envs.py

[22b39e11f] Kameshwara Pavan Kumar Mantha 2024-08-15 llama_index serving integration documentation (#6973)
1	0	docs/source/serving/integrations.rst
27	0	docs/source/serving/serving_with_llamaindex.rst

[f55a9aea4] Kyle Sayers 2024-08-14 [Misc] Revert `compressed-tensors` code reuse (#7521)
0	1	requirements-common.txt
1	1	requirements-test.txt
2	1	tests/quantization/test_compressed_tensors.py
2	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
74	2	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[951fdd66d] Woosuk Kwon 2024-08-14 [TPU] Set per-rank XLA cache (#7533)
6	6	vllm/worker/tpu_worker.py

[2ecf7b175] William Lin 2024-08-14 [core] [3/N] multi-step args and sequence.py (#7452)
13	1	vllm/config.py
5	0	vllm/core/scheduler.py
25	3	vllm/engine/arg_utils.py
57	1	vllm/sequence.py

[3f674a49b] Cyrus Leung 2024-08-15 [VLM][Core] Support profiling with multiple multi-modal inputs per prompt (#7126)
1	1	docs/source/dev/input_processing/input_processing_pipeline.rst
3	0	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/models/enabling_multimodal_inputs.rst
24	0	tests/engine/test_arg_utils.py
1	1	tests/models/test_blip2.py
1	1	tests/models/test_fuyu.py
1	1	tests/models/test_internvl.py
1	1	tests/models/test_llava.py
1	1	tests/models/test_llava_next.py
3	2	tests/models/test_minicpmv.py
1	1	tests/models/test_paligemma.py
1	1	tests/models/test_phi3v.py
78	6	tests/multimodal/test_mapper.py
10	4	vllm/config.py
43	5	vllm/engine/arg_utils.py
7	4	vllm/engine/llm_engine.py
69	14	vllm/inputs/registry.py
1	1	vllm/model_executor/model_loader/loader.py
4	3	vllm/model_executor/models/blip.py
30	7	vllm/model_executor/models/blip2.py
14	9	vllm/model_executor/models/chameleon.py
5	3	vllm/model_executor/models/clip.py
16	11	vllm/model_executor/models/fuyu.py
2	5	vllm/model_executor/models/interfaces.py
8	3	vllm/model_executor/models/internvl.py
10	6	vllm/model_executor/models/llava.py
10	4	vllm/model_executor/models/llava_next.py
11	10	vllm/model_executor/models/minicpmv.py
8	5	vllm/model_executor/models/paligemma.py
8	4	vllm/model_executor/models/phi3v.py
5	3	vllm/model_executor/models/siglip.py
33	15	vllm/multimodal/base.py
6	3	vllm/multimodal/image.py
101	16	vllm/multimodal/registry.py
0	28	vllm/utils.py
16	3	vllm/worker/enc_dec_model_runner.py
20	17	vllm/worker/model_runner.py
19	17	vllm/worker/xpu_model_runner.py

[70b746efc] Wallas Henrique 2024-08-14 [Misc] Deprecation Warning when setting --engine-use-ray (#7424)
8	1	tests/async_engine/test_api_server.py
6	0	tests/async_engine/test_async_llm_engine.py
5	1	tests/async_engine/test_openapi_server_ray.py
6	0	tests/spec_decode/e2e/conftest.py
7	1	vllm/engine/arg_utils.py
15	0	vllm/engine/async_llm_engine.py
9	0	vllm/envs.py

[67d115db0] jack 2024-08-15 [Bugfix][Frontend] Disable embedding API for chat models (#7504)
4	1	vllm/entrypoints/openai/serving_embedding.py

[d3d9cb6e4] youkaichao 2024-08-14 [ci] fix model tests (#7507)
2	1	.buildkite/test-pipeline.yaml

[c134a4640] Chang Su 2024-08-13 Fix empty output when temp is too low (#2937)
2	1	vllm/model_executor/layers/sampler.py
7	0	vllm/sampling_params.py

[199adbb7c] youkaichao 2024-08-13 [doc] update test script to include cudagraph (#7501)
37	2	docs/source/getting_started/debugging.rst

[dd164d72f] Cyrus Leung 2024-08-14 [Bugfix][Docs] Update list of mock imports (#7493)
2	0	.buildkite/test-pipeline.yaml
1	1	docs/source/conf.py

[ea49e6a3c] youkaichao 2024-08-13 [misc][ci] fix cpu test with plugins (#7489)
1	1	.buildkite/run-cpu-test.sh
1	0	.buildkite/test-pipeline.yaml
3	4	tests/models/test_oot_registration.py
10	3	tests/utils.py

[97992802f] Jee Jee Li 2024-08-14 [CI/Build]Reduce the time consumption for LoRA tests (#7396)
0	106	tests/lora/test_layer_variation.py
1	1	tests/lora/test_punica_sizes.py
2	21	tests/lora/test_punica_variation.py

[59edd0f13] Woosuk Kwon 2024-08-13 [Bugfix][CI] Import ray under guard (#7486)
1	1	vllm/distributed/device_communicators/tpu_communicator.py

[a08df8322] Woosuk Kwon 2024-08-13 [TPU] Support multi-host inference (#7457)
1	1	docs/source/getting_started/tpu-installation.rst
10	3	vllm/distributed/device_communicators/tpu_communicator.py

[16422ea76] youkaichao 2024-08-13 [misc][plugin] add plugin system implementation (#7426)
4	0	.buildkite/test-pipeline.yaml
1	0	requirements-common.txt
26	0	tests/conftest.py
6	0	tests/distributed/test_distributed_oot.py
27	79	tests/entrypoints/openai/test_oot_registration.py
15	20	tests/models/test_oot_registration.py
9	0	tests/plugins/vllm_add_dummy_model/setup.py
26	0	tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py
3	0	vllm/engine/llm_engine.py
9	1	vllm/envs.py
1	1	vllm/model_executor/models/__init__.py
31	0	vllm/plugins/__init__.py
3	0	vllm/worker/worker_base.py

[373538f97] Kyle Sayers 2024-08-13 [Misc] `compressed-tensors` code reuse (#7277)
1	0	requirements-common.txt
1	1	requirements-test.txt
1	2	tests/quantization/test_compressed_tensors.py
5	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
1	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
2	74	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[33e5d7e6b] youkaichao 2024-08-13 [frontend] spawn engine process from api server process (#7484)
37	0	tests/entrypoints/openai/test_mp_api_server.py
0	35	tests/entrypoints/openai/test_mp_crash.py
5	8	tests/entrypoints/openai/test_oot_registration.py
9	6	vllm/entrypoints/openai/api_server.py

[c5c776826] Simon Mo 2024-08-13 Announce NVIDIA Meetup (#7483)
9	0	README.md

[b1e5afc3e] Dipika Sikka 2024-08-13 [Misc] Update `awq` and `awq_marlin` to use `vLLMParameters` (#7422)
3	1	tests/weight_loading/models.txt
2	1	vllm/model_executor/layers/linear.py
35	39	vllm/model_executor/layers/quantization/awq.py
34	42	vllm/model_executor/layers/quantization/awq_marlin.py

[d3bdfd3ab] Dipika Sikka 2024-08-13 [Misc] Update Fused MoE weight loading (#7334)
180	136	vllm/model_executor/layers/fused_moe/layer.py
80	61	vllm/model_executor/layers/quantization/fp8.py
1	1	vllm/model_executor/models/deepseek_v2.py
1	1	vllm/model_executor/models/jamba.py
1	1	vllm/model_executor/models/mixtral.py
1	1	vllm/model_executor/models/qwen2_moe.py

[fb377d7e7] Dipika Sikka 2024-08-13 [Misc] Update `gptq_marlin` to use new vLLMParameters (#7281)
10	0	.buildkite/test-pipeline.yaml
15	0	tests/weight_loading/models.txt
32	0	tests/weight_loading/run_model_weight_loading_test.sh
20	0	tests/weight_loading/test_weight_loading.py
3	1	vllm/model_executor/layers/linear.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
59	62	vllm/model_executor/layers/quantization/gptq_marlin.py
85	25	vllm/model_executor/parameter.py

[181abbc27] Dipika Sikka 2024-08-13 [Misc] Update LM Eval Tolerance (#7473)
1	1	.buildkite/lm-eval-harness/test_lm_eval_correctness.py

[00c3d68e4] Peter Salas 2024-08-13 [Frontend][Core] Add plumbing to support audio language models (#7446)
2	0	docs/source/conf.py
11	11	docs/source/models/enabling_multimodal_inputs.rst
2	0	requirements-common.txt
351	0	tests/entrypoints/openai/test_audio.py
69	32	vllm/entrypoints/chat_utils.py
6	0	vllm/envs.py
2	2	vllm/model_executor/model_loader/loader.py
6	6	vllm/model_executor/models/blip2.py
2	2	vllm/model_executor/models/chameleon.py
6	6	vllm/model_executor/models/fuyu.py
16	12	vllm/model_executor/models/interfaces.py
6	6	vllm/model_executor/models/internvl.py
4	4	vllm/model_executor/models/llava.py
4	4	vllm/model_executor/models/llava_next.py
2	2	vllm/model_executor/models/minicpmv.py
4	4	vllm/model_executor/models/paligemma.py
6	6	vllm/model_executor/models/phi3v.py
15	14	vllm/model_executor/models/utils.py
17	0	vllm/multimodal/audio.py
5	1	vllm/multimodal/base.py
2	1	vllm/multimodal/registry.py
56	2	vllm/multimodal/utils.py
4	4	vllm/worker/model_runner.py
2	2	vllm/worker/xpu_model_runner.py

[e20233d36] Woosuk Kwon 2024-08-13 Revert "[Doc] Update supported_hardware.rst (#7276)" (#7467)
13	15	docs/source/quantization/supported_hardware.rst

[d6e634f3d] Woosuk Kwon 2024-08-13 [TPU] Suppress import custom_ops warning (#7458)
6	4	vllm/_custom_ops.py
1	1	vllm/utils.py

[4d2dc5072] youkaichao 2024-08-13 [hardware] unify usage of is_tpu to current_platform.is_tpu() (#7102)
2	3	vllm/attention/selector.py
4	3	vllm/config.py
3	2	vllm/executor/ray_utils.py
3	2	vllm/model_executor/custom_op.py
2	2	vllm/model_executor/layers/rotary_embedding.py
3	3	vllm/model_executor/model_loader/loader.py
12	9	vllm/platforms/__init__.py
0	9	vllm/utils.py

[7025b11d9] Cyrus Leung 2024-08-13 [Bugfix] Fix weight loading for Chameleon when TP>1 (#7410)
18	8	tests/conftest.py
4	0	tests/distributed/test_multimodal_broadcast.py
6	2	tests/entrypoints/openai/test_oot_registration.py
58	33	tests/models/test_chameleon.py
13	8	tests/models/test_llava.py
9	24	tests/models/test_minicpmv.py
7	2	tests/models/test_oot_registration.py
1	1	vllm/distributed/communication_op.py
1	1	vllm/distributed/parallel_state.py
8	4	vllm/model_executor/layers/logits_processor.py
25	2	vllm/model_executor/model_loader/weight_utils.py
5	2	vllm/model_executor/models/arctic.py
5	2	vllm/model_executor/models/baichuan.py
5	2	vllm/model_executor/models/bart.py
5	2	vllm/model_executor/models/blip2.py
5	2	vllm/model_executor/models/bloom.py
18	5	vllm/model_executor/models/chameleon.py
5	2	vllm/model_executor/models/chatglm.py
10	19	vllm/model_executor/models/commandr.py
5	2	vllm/model_executor/models/dbrx.py
5	2	vllm/model_executor/models/deepseek.py
5	2	vllm/model_executor/models/deepseek_v2.py
5	2	vllm/model_executor/models/falcon.py
5	2	vllm/model_executor/models/fuyu.py
5	2	vllm/model_executor/models/gemma.py
5	2	vllm/model_executor/models/gemma2.py
5	2	vllm/model_executor/models/gpt2.py
5	2	vllm/model_executor/models/gpt_bigcode.py
5	2	vllm/model_executor/models/gpt_j.py
5	2	vllm/model_executor/models/gpt_neox.py
5	2	vllm/model_executor/models/internlm2.py
5	2	vllm/model_executor/models/internvl.py
5	2	vllm/model_executor/models/jais.py
5	2	vllm/model_executor/models/jamba.py
5	2	vllm/model_executor/models/llama.py
5	2	vllm/model_executor/models/llava.py
5	2	vllm/model_executor/models/llava_next.py
11	5	vllm/model_executor/models/medusa.py
5	2	vllm/model_executor/models/minicpm.py
5	2	vllm/model_executor/models/minicpmv.py
5	2	vllm/model_executor/models/mixtral.py
5	2	vllm/model_executor/models/mixtral_quant.py
5	2	vllm/model_executor/models/mpt.py
5	2	vllm/model_executor/models/nemotron.py
5	2	vllm/model_executor/models/olmo.py
5	2	vllm/model_executor/models/opt.py
5	2	vllm/model_executor/models/orion.py
5	2	vllm/model_executor/models/paligemma.py
5	2	vllm/model_executor/models/persimmon.py
5	2	vllm/model_executor/models/phi.py
5	2	vllm/model_executor/models/phi3_small.py
5	2	vllm/model_executor/models/phi3v.py
5	2	vllm/model_executor/models/qwen.py
5	2	vllm/model_executor/models/qwen2.py
5	2	vllm/model_executor/models/qwen2_moe.py
5	2	vllm/model_executor/models/stablelm.py
5	2	vllm/model_executor/models/starcoder2.py
5	2	vllm/model_executor/models/xverse.py
5	3	vllm/outputs.py

[5469146bc] Kevin H. Luu 2024-08-12 [ci] Remove fast check cancel workflow (#7455)
0	25	.github/workflows/cancel_fastcheck_when_ready.yml

[97a6be95b] Andrew Wang 2024-08-12 [Misc] improve logits processors logging message (#7435)
0	1	tests/entrypoints/openai/test_serving_chat.py
1	1	vllm/entrypoints/openai/logits_processors.py

[9ba85bc15] Cyrus Leung 2024-08-13 [mypy] Misc. typing improvements (#7417)
12	4	tests/tensorizer_loader/conftest.py
8	22	tests/test_utils.py
8	3	tests/utils.py
4	4	vllm/inputs/registry.py
3	3	vllm/model_executor/models/internvl.py
3	3	vllm/model_executor/models/minicpmv.py
2	2	vllm/model_executor/models/phi3v.py
2	3	vllm/multimodal/image.py
7	3	vllm/platforms/cuda.py
8	10	vllm/transformers_utils/detokenizer.py
3	3	vllm/transformers_utils/tokenizer.py
1	2	vllm/transformers_utils/tokenizer_group/__init__.py
6	8	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
2	1	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
3	2	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
2	2	vllm/utils.py

[198d6a289] Rui Qiao 2024-08-12 [Core] Shut down aDAG workers with clean async llm engine exit (#7224)
4	8	tests/distributed/test_pipeline_parallel.py
14	0	vllm/engine/async_llm_engine.py
11	6	vllm/engine/llm_engine.py
1	0	vllm/entrypoints/openai/rpc/server.py
10	11	vllm/executor/ray_gpu_executor.py

[774cd1d3b] Daniele 2024-08-13 [CI/Build] bump minimum cmake version (#6999)
1	1	CMakeLists.txt
1	1	pyproject.toml
1	1	requirements-build.txt
0	2	requirements-common.txt

[91294d56e] sasha0552 2024-08-12 [Bugfix] Handle PackageNotFoundError when checking for xpu version (#7398)
5	2	vllm/utils.py

[a046f8639] jon-chuang 2024-08-12 [Core/Bugfix] Add FP8 K/V Scale and dtype conversion for prefix/prefill Triton Kernel (#7208)
0	2	docs/source/quantization/fp8_e4m3_kvcache.rst
0	2	docs/source/quantization/fp8_e5m2_kvcache.rst
96	8	tests/basic_correctness/test_chunked_prefill.py
26	7	tests/kernels/test_prefix_prefill.py
3	0	vllm/attention/backends/rocm_flash_attn.py
3	0	vllm/attention/backends/xformers.py
1	0	vllm/attention/ops/ipex_attn.py
6	0	vllm/attention/ops/paged_attn.py
73	24	vllm/attention/ops/prefix_prefill.py
0	4	vllm/config.py

[4ddc4743d] Cyrus Leung 2024-08-13 [Core] Consolidate `GB` constant and enable float GB arguments (#7416)
6	7	vllm/config.py
3	3	vllm/engine/arg_utils.py
3	4	vllm/executor/cpu_executor.py
4	5	vllm/executor/openvino_executor.py
3	0	vllm/utils.py
2	2	vllm/worker/tpu_worker.py

[6aa33cb2d] Lucas Wilkinson 2024-08-12 [Misc] Use scalar type to dispatch to different `gptq_marlin` kernels (#7323)
166	31	csrc/core/scalar_type.hpp
168	189	csrc/quantization/gptq_marlin/gptq_marlin.cu

[1137f343a] Kevin H. Luu 2024-08-12 [ci] Cancel fastcheck when PR is ready (#7433)
3	3	.github/workflows/cancel_fastcheck_when_ready.yml

[9b3e2edd3] Kevin H. Luu 2024-08-12 [ci] Cancel fastcheck run when PR is marked ready (#7427)
25	0	.github/workflows/cancel_fastcheck_when_ready.yml

[65950e8f5] Kevin H. Luu 2024-08-12 [ci] Entrypoints run upon changes in vllm/ (#7423)
1	2	.buildkite/test-pipeline.yaml

[cfba4def5] Woosuk Kwon 2024-08-12 [Bugfix] Fix logit soft cap in flash-attn backend (#7425)
1	0	vllm/attention/backends/flash_attn.py

[d2bc4510a] Daniele 2024-08-12 [CI/Build] bump Dockerfile.neuron image base, use public ECR (#6832)
1	1	Dockerfile.neuron

[24154f861] Cyrus Leung 2024-08-12 [Frontend] Disallow passing `model` as both argument and option (#7347)
2	2	vllm/engine/arg_utils.py
7	0	vllm/scripts.py

[e6e42e4b1] Roger Wang 2024-08-12 [Core][VLM] Support image embeddings as input (#6613)
11	0	docs/source/models/vlm.rst
159	0	tests/models/test_llava_image_embeds.py
10	0	vllm/assets/image.py
49	23	vllm/model_executor/models/blip2.py
7	1	vllm/model_executor/models/clip.py
10	3	vllm/model_executor/models/fuyu.py
54	13	vllm/model_executor/models/internvl.py
43	18	vllm/model_executor/models/llava.py
43	15	vllm/model_executor/models/llava_next.py
48	32	vllm/model_executor/models/paligemma.py
73	32	vllm/model_executor/models/phi3v.py
7	1	vllm/model_executor/models/siglip.py
4	1	vllm/multimodal/image.py

[ec2affa8a] Lily Liu 2024-08-12 [Kernel] Flashinfer correctness fix for v0.1.3 (#7319)
0	5	.buildkite/test-pipeline.yaml
1	1	Dockerfile
19	18	vllm/attention/backends/flashinfer.py

[86ab567ba] Roger Wang 2024-08-11 [CI/Build] Minor refactoring for vLLM assets (#7407)
29	1	vllm/assets/base.py
6	25	vllm/assets/image.py

[f020a6297] Simon Mo 2024-08-11 [Docs] Update readme (#7316)
11	8	README.md
7	6	docs/source/index.rst

[6c8e59571] youkaichao 2024-08-11 [misc] add commit id in collect env (#7405)
3	2	collect_env.py

[02b1988b9] tomeras91 2024-08-12 [Doc] building vLLM with VLLM_TARGET_DEVICE=empty (#7403)
10	0	docs/source/getting_started/installation.rst

[386087970] tomeras91 2024-08-11 [CI/Build] build on empty device for better dev experience (#4773)
2	2	requirements-cuda.txt
19	5	setup.py

[c08e2b308] William Lin 2024-08-11 [core] [2/N] refactor worker_base input preparation for multi-step (#7387)
2	0	vllm/worker/worker.py
61	31	vllm/worker/worker_base.py

[4fb7b52a2] Noam Gat 2024-08-11 Updating LM Format Enforcer version to v0.10.6 (#7189)
1	1	requirements-common.txt

[90bab18f2] Woosuk Kwon 2024-08-10 [TPU] Use mark_dynamic to reduce compilation time (#7340)
1	1	Dockerfile.tpu
2	2	docs/source/getting_started/tpu-installation.rst
47	13	vllm/worker/tpu_model_runner.py

[4c5d8e8ea] Isotr0py 2024-08-11 [Bugfix] Fix phi3v batch inference when images have different aspect ratio (#7392)
4	1	tests/models/test_phi3v.py
2	2	tests/tracing/test_tracing.py
12	14	vllm/model_executor/models/phi3v.py
7	2	vllm/multimodal/utils.py

[baa240252] Cade Daniel 2024-08-09 [Core] Fix edge case in chunked prefill + block manager v2 (#7380)
15	3	tests/core/block/e2e/test_correctness.py
6	0	vllm/core/block/block_table.py

[999ef0b91] Antoni Baum 2024-08-09 [Misc] Add numpy implementation of `compute_slot_mapping` (#7377)
41	12	vllm/attention/backends/utils.py

[5c6c54d67] Dipika Sikka 2024-08-09 [Bugfix] Fix `PerTensorScaleParameter` weight loading for fused models (#7376)
14	9	vllm/model_executor/layers/linear.py

[933790c20] Mahesh Keralapura 2024-08-09 [Core] Add span metrics for model_forward, scheduler and sampler time (#7089)
2	0	tests/tracing/test_tracing.py
1	0	tests/worker/test_model_runner.py
15	0	vllm/config.py
12	0	vllm/core/scheduler.py
32	1	vllm/engine/arg_utils.py
29	0	vllm/engine/llm_engine.py
4	3	vllm/executor/executor_base.py
1	0	vllm/executor/gpu_executor.py
17	0	vllm/sequence.py
4	2	vllm/spec_decode/draft_model_runner.py
5	3	vllm/spec_decode/target_model_runner.py
6	0	vllm/tracing.py
5	3	vllm/worker/embedding_model_runner.py
3	2	vllm/worker/enc_dec_model_runner.py
28	4	vllm/worker/model_runner.py
5	2	vllm/worker/worker.py
20	1	vllm/worker/worker_base.py

[70d268a39] Roger Wang 2024-08-09 [Bugfix] Fix ITL recording in serving benchmark (#7372)
3	2	benchmarks/backend_request_func.py

[249b88228] Pooya Davoodi 2024-08-09 [Frontend] Support embeddings in the run_batch API (#7132)
37	4	examples/offline_inference_openai.md
50	2	tests/entrypoints/openai/test_run_batch.py
2	2	vllm/entrypoints/openai/protocol.py
37	11	vllm/entrypoints/openai/run_batch.py
12	5	vllm/entrypoints/openai/serving_embedding.py

[74af2bbd9] Alexander Matveev 2024-08-09 [Bugfix] Fix reinit procedure in ModelInputForGPUBuilder (#7360)
2	0	vllm/worker/model_runner.py

[fc7b8d1ee] Alexander Matveev 2024-08-09 [Performance] e2e overheads reduction: Small followup diff (#7364)
2	2	vllm/core/block_manager_v1.py
3	0	vllm/sequence.py

[67abdbb42] Isotr0py 2024-08-09 [VLM][Doc] Add `stop_token_ids` to InternVL example (#7354)
17	6	examples/offline_inference_vision_language.py

[07ab16074] Mor Zusman 2024-08-09 [Model][Jamba] Mamba cache single buffer  (#6739)
148	121	vllm/model_executor/models/jamba.py
0	3	vllm/worker/model_runner.py

[b4e9528f9] Nick Hill 2024-08-09 [Core] Streamline stream termination in `AsyncLLMEngine` (#7336)
4	2	tests/async_engine/test_request_tracker.py
22	19	vllm/engine/async_llm_engine.py

[57b7be0e1] William Lin 2024-08-08 [Speculative decoding] [Multi-Step] decouple should_modify_greedy_probs_inplace (#6971)
26	1	tests/samplers/test_sampler.py
4	0	vllm/lora/layers.py
2	2	vllm/model_executor/layers/sampler.py
3	0	vllm/spec_decode/medusa_worker.py
4	0	vllm/spec_decode/multi_step_worker.py
4	0	vllm/spec_decode/proposer_worker_base.py
6	0	vllm/spec_decode/smaller_tp_proposer_worker.py
3	0	vllm/spec_decode/spec_decode_worker.py

[99b4cf5f2] Travis Johnson 2024-08-08 [Bugfix] Fix speculative decoding with MLPSpeculator with padded vocabulary (#7218)
60	0	tests/spec_decode/e2e/test_mlp_correctness.py
1	1	vllm/model_executor/layers/logits_processor.py
2	2	vllm/model_executor/layers/rejection_sampler.py
3	2	vllm/model_executor/models/mlp_speculator.py

[e02ac5561] Alexander Matveev 2024-08-09 [Performance] Optimize e2e overheads: Reduce python allocations (#7162)
5	1	vllm/attention/backends/flash_attn.py
10	2	vllm/attention/backends/utils.py
45	3	vllm/block.py
15	12	vllm/core/block_manager_v1.py
127	44	vllm/core/scheduler.py
3	1	vllm/model_executor/__init__.py
71	10	vllm/model_executor/sampling_metadata.py
1	1	vllm/outputs.py
24	4	vllm/sequence.py
38	0	vllm/utils.py
211	47	vllm/worker/model_runner.py

[73388c07a] Woosuk Kwon 2024-08-08 [TPU] Fix dockerfile.tpu (#7331)
4	10	Dockerfile.tpu

[7eb4a51c5] Cyrus Leung 2024-08-09 [Core] Support serving encoder/decoder models (#7258)
1	1	.github/workflows/mypy.yaml
4	4	examples/offline_inference_encoder_decoder.py
1	1	requirements-common.txt
1	1	requirements-lint.txt
19	13	tests/conftest.py
1	1	tests/distributed/test_basic_distributed_correctness_enc_dec.py
50	0	tests/entrypoints/openai/test_encoder_decoder.py
27	11	tests/models/test_bart.py
0	11	tests/models/utils.py
1	1	tests/test_inputs.py
10	0	vllm/config.py
130	24	vllm/engine/async_llm_engine.py
151	171	vllm/engine/llm_engine.py
2	3	vllm/entrypoints/chat_utils.py
2	2	vllm/entrypoints/llm.py
5	3	vllm/entrypoints/openai/logits_processors.py
1	1	vllm/entrypoints/openai/serving_engine.py
10	11	vllm/inputs/__init__.py
72	135	vllm/inputs/data.py
75	0	vllm/inputs/parse.py
11	11	vllm/model_executor/models/interfaces.py
4	2	vllm/multimodal/image.py
1	1	vllm/sequence.py
22	52	vllm/utils.py
2	4	vllm/worker/worker.py

[0fa14907d] Siyuan Liu 2024-08-08 [TPU] Add Load-time W8A16 quantization for TPU Backend (#7005)
6	0	vllm/config.py
2	0	vllm/model_executor/layers/quantization/__init__.py
118	0	vllm/model_executor/layers/quantization/tpu_int8.py
9	8	vllm/model_executor/model_loader/loader.py

[5923532e1] Simon Mo 2024-08-08 Add Skywork AI as Sponsor (#7314)
1	0	README.md
1	0	docs/source/community/sponsors.md

[a049b107e] Jee Jee Li 2024-08-09 [Misc] Temporarily resolve the error of BitAndBytes (#7308)
3	2	vllm/config.py

[8334c39f3] Isotr0py 2024-08-09 [Bugfix] Fix new Llama3.1 GGUF model loading (#7269)
16	14	vllm/model_executor/model_loader/weight_utils.py

[e90457674] Daniele 2024-08-08 [CI/Build] Dockerfile.cpu improvements (#7298)
3	0	.dockerignore
21	9	Dockerfile.cpu

[e14fb22e5] Michael Goin 2024-08-08 [Doc] Put collect_env issue output in a <detail> block (#7310)
6	1	.github/ISSUE_TEMPLATE/400-bug report.yml

[782e53ab5] Zach Zheng 2024-08-08 [Bugfix][fast] Fix the get_num_blocks_touched logic (#6849)
62	0	tests/core/block/test_block_manager_v2.py
42	0	tests/core/block/test_naive_block.py
54	0	tests/core/block/test_prefix_caching_block.py
5	3	tests/core/utils.py
2	3	vllm/core/block/naive_block.py
7	4	vllm/core/block/prefix_caching_block.py

[21b9c49aa] Joe Runde 2024-08-08 [Frontend] Kill the server on engine death (#6594)
47	0	tests/entrypoints/openai/test_shutdown.py
4	2	vllm/engine/async_llm_engine.py
1	0	vllm/entrypoints/api_server.py
42	2	vllm/entrypoints/launcher.py
1	0	vllm/entrypoints/openai/api_server.py
24	2	vllm/entrypoints/openai/rpc/client.py
11	8	vllm/entrypoints/openai/rpc/server.py
6	0	vllm/envs.py

[5fb4a3f67] Luka Govedič 2024-08-08 [Bugfix][Kernel] Increased atol to fix failing tests (#7305)
1	1	tests/kernels/test_flash_attn.py

[757ac70a6] Jee Jee Li 2024-08-08 [Model] Rename MiniCPMVQwen2 to MiniCPMV2.6 (#7273)
1	1	docs/source/models/supported_models.rst
35	16	examples/offline_inference_vision_language.py
15	14	vllm/model_executor/models/minicpmv.py

[6dffa4b0a] Murali Andoorveedu 2024-08-08 [Bugfix] Fix LoRA with PP (#7292)
3	0	vllm/lora/models.py

[48abee9e5] Cherilyn Buren 2024-08-08 [Frontend] remove max_num_batched_tokens limit for lora (#7288)
0	5	vllm/config.py

[746709642] Rui Qiao 2024-08-07 [Misc] Fix typos in scheduler.py (#7285)
3	3	vllm/core/scheduler.py

[e53dfd3ea] Lily Liu 2024-08-07 [Kernel] Fix Flashinfer Correctness (#7284)
7	3	vllm/attention/backends/flashinfer.py

[6d9442024] Michael Goin 2024-08-07 [Doc] Update supported_hardware.rst (#7276)
15	13	docs/source/quantization/supported_hardware.rst

[fc1493a01] Nick Hill 2024-08-07 [FrontEnd] Make `merge_async_iterators` `is_cancelled` arg optional (#7282)
6	5	vllm/utils.py

[311f74383] Lucas Wilkinson 2024-08-07 [Bugfix] Fix gptq failure on T4s (#7264)
1	2	vllm/model_executor/layers/quantization/awq_marlin.py
1	2	vllm/model_executor/layers/quantization/gptq_marlin.py
12	11	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[469b3bc53] Kevin H. Luu 2024-08-07 [ci] Make building wheels per commit optional (#7278)
20	7	.buildkite/release-pipeline.yaml

[5223199e0] Michael Goin 2024-08-07 [Bugfix][FP8] Fix dynamic FP8 Marlin quantization (#7219)
15	4	tests/quantization/test_fp8.py
8	0	vllm/envs.py
10	1	vllm/model_executor/layers/quantization/fp8.py

[fde47d3bc] Maximilien de Bayser 2024-08-07 [BugFix] Fix frontend multiprocessing hang (#7217)
35	0	tests/entrypoints/openai/test_mp_crash.py
10	1	vllm/entrypoints/openai/api_server.py
22	4	vllm/entrypoints/openai/rpc/client.py

[0e12cd67a] Stas Bekman 2024-08-07 [Doc] add online speculative decoding example (#7243)
55	11	docs/source/models/spec_decode.rst

[80cbe10c5] Ilya Lavrenov 2024-08-07 [OpenVINO] migrate to latest dependencies versions (#7251)
1	1	Dockerfile.openvino
1	1	docs/source/getting_started/openvino-installation.rst
3	30	requirements-openvino.txt
1	1	setup.py

[b76454761] Isotr0py 2024-08-08 [Bugfix] Fix input processor for InternVL2 model (#7164)
19	4	tests/models/test_internvl.py
54	30	vllm/model_executor/models/internvl.py

[ab0f5e282] Rafael Vasquez 2024-08-07 Fixes typo in function name (#7275)
2	2	vllm/scripts.py

[564985729] Robert Shaw 2024-08-07 [ BugFix ] Move `zmq` frontend to IPC instead of TCP (#7222)
8	4	vllm/entrypoints/openai/api_server.py
3	3	vllm/entrypoints/openai/rpc/client.py
4	6	vllm/entrypoints/openai/rpc/server.py
6	5	vllm/envs.py
8	4	vllm/utils.py

[0f7052bc7] Dipika Sikka 2024-08-07 [Misc] Refactor linear layer weight loading; introduce `BasevLLMParameter` and `weight_loader_v2` (#5874)
12	8	tests/quantization/test_compressed_tensors.py
4	0	vllm/model_executor/__init__.py
151	2	vllm/model_executor/layers/linear.py
9	12	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
11	9	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
53	59	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
32	19	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
31	20	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
27	20	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
48	54	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
277	0	vllm/model_executor/parameter.py

[639159b2a] youkaichao 2024-08-07 [distributed][misc] add specialized method for cuda platform (#7249)
6	2	vllm/distributed/device_communicators/custom_all_reduce.py
36	1	vllm/platforms/cuda.py
0	50	vllm/utils.py

[66d617e34] Cyrus Leung 2024-08-07 [Frontend] Gracefully handle missing chat template and fix CI failure (#7238)
8	13	tests/async_engine/test_chat_template.py
7	3	tests/async_engine/test_openapi_server_ray.py
55	32	tests/entrypoints/openai/test_oot_registration.py
2	2	tests/utils.py
34	3	vllm/entrypoints/chat_utils.py
3	2	vllm/entrypoints/openai/protocol.py
4	4	vllm/entrypoints/openai/serving_chat.py
8	6	vllm/entrypoints/openai/serving_tokenization.py
4	4	vllm/transformers_utils/tokenizer.py

[7b261092d] Atilla Akkuş 2024-08-07 [BUGFIX]: top_k is expected to be an integer. (#7227)
3	0	vllm/sampling_params.py

[2385c8f37] Roger Wang 2024-08-06 [Doc] Mock new dependencies for documentation (#7245)
2	0	docs/source/conf.py

[9a3f49ae0] Nick Hill 2024-08-06 [BugFix] Overhaul async request cancellation (#7111)
5	4	tests/async_engine/api_server_async_engine.py
12	13	tests/async_engine/test_request_tracker.py
3	2	tests/test_utils.py
67	79	vllm/engine/async_llm_engine.py
5	5	vllm/engine/protocol.py
9	7	vllm/entrypoints/api_server.py
32	30	vllm/entrypoints/openai/rpc/client.py
18	16	vllm/entrypoints/openai/serving_chat.py
6	14	vllm/entrypoints/openai/serving_completion.py
7	7	vllm/entrypoints/openai/serving_embedding.py
62	49	vllm/utils.py

[f9a560064] Michael Goin 2024-08-06 [Bugfix] Fix GPTQ and GPTQ Marlin CPU Offloading (#7225)
21	4	tests/basic_correctness/test_cpu_offload.py
3	2	tests/utils.py
9	7	vllm/model_executor/layers/quantization/gptq.py
0	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[fd95e026e] afeldman-nm 2024-08-06 [Core] Subclass ModelRunner to support cross-attention & encoder sequences (towards eventual encoder/decoder model support) (#4942)
3	1	.buildkite/test-pipeline.yaml
99	0	examples/offline_inference_encoder_decoder.py
185	37	tests/conftest.py
4	26	tests/core/test_scheduler.py
99	0	tests/core/test_scheduler_encoder_decoder.py
64	35	tests/core/utils.py
101	0	tests/distributed/test_basic_distributed_correctness_enc_dec.py
2	2	tests/kernels/test_attention_selector.py
207	159	tests/kernels/test_encoder_decoder_attn.py
1	1	tests/kernels/test_flash_attn.py
3	17	tests/kernels/utils.py
153	0	tests/models/test_bart.py
36	0	tests/models/utils.py
480	0	tests/worker/test_encoder_decoder_model_runner.py
3	1	vllm/attention/__init__.py
1	1	vllm/attention/layer.py
111	12	vllm/attention/selector.py
34	2	vllm/config.py
2	10	vllm/core/block/utils.py
28	0	vllm/core/scheduler.py
1	1	vllm/engine/arg_utils.py
411	24	vllm/engine/llm_engine.py
18	3	vllm/entrypoints/llm.py
18	5	vllm/inputs/__init__.py
122	2	vllm/inputs/data.py
10	1	vllm/model_executor/models/__init__.py
996	0	vllm/model_executor/models/bart.py
19	1	vllm/outputs.py
95	10	vllm/sequence.py
130	0	vllm/utils.py
472	0	vllm/worker/enc_dec_model_runner.py
56	0	vllm/worker/utils.py
12	1	vllm/worker/worker.py

[660470e5a] xiaobochen123 2024-08-07 [Core] Optimize evictor-v2 performance (#7193)
4	2	vllm/core/evictor_v2.py

[8d59dbb00] Luka Govedič 2024-08-06 [Kernel] Add per-tensor and per-token AZP epilogues (#5941)
107	78	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
8	0	csrc/ops.h
147	0	csrc/quantization/cutlass_w8a8/Epilogues.md
151	1	csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c2x.hpp
57	0	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
217	36	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
230	28	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
104	7	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
10	1	csrc/torch_bindings.cpp
119	1	tests/kernels/test_cutlass.py
25	1	vllm/_custom_ops.py

[5c60c8c42] Lily Liu 2024-08-06 [SpecDecode] [Minor] Fix spec decode sampler tests (#7183)
7	7	tests/samplers/test_rejection_sampler.py
9	9	tests/samplers/test_typical_acceptance_sampler.py
6	3	vllm/model_executor/layers/spec_decode_base_sampler.py

[00afc7859] Katarzyna Papis 2024-08-06 [Bugfix] add gguf dependency (#7198)
1	0	requirements-openvino.txt

[541c1852d] Robert Shaw 2024-08-06 [ BugFix ] Fix ZMQ when `VLLM_PORT` is set (#7205)
1	1	vllm/envs.py

[a3bbbfa1d] Dipika Sikka 2024-08-06 [BugFix] Fix DeepSeek remote code (#7178)
1	0	.buildkite/lm-eval-harness/configs/DeepSeek-V2-Lite-Chat.yaml
4	1	.buildkite/lm-eval-harness/test_lm_eval_correctness.py

[1f26efbb3] Cyrus Leung 2024-08-06 [Model] Support SigLIP encoder and alternative decoders for LLaVA models (#7153)
3	0	requirements-test.txt
31	5	tests/models/test_llava.py
5	4	tests/models/test_llava_next.py
5	4	tests/models/test_paligemma.py
1	1	tests/models/test_registry.py
28	10	vllm/model_executor/model_loader/loader.py
1	7	vllm/model_executor/model_loader/utils.py
14	2	vllm/model_executor/models/__init__.py
22	2	vllm/model_executor/models/clip.py
7	17	vllm/model_executor/models/internvl.py
113	98	vllm/model_executor/models/llava.py
135	104	vllm/model_executor/models/llava_next.py
38	11	vllm/model_executor/models/siglip.py
52	4	vllm/model_executor/models/utils.py

[9118217f5] Jee Jee Li 2024-08-06 [LoRA] Relax LoRA condition (#7146)
1	1	tests/lora/test_layers.py
1	1	tests/lora/test_punica_variation.py
3	2	vllm/config.py
3	3	vllm/lora/layers.py

[e3c664bfc] Simon Mo 2024-08-05 [Build] Add initial conditional testing spec (#6841)
234	156	.buildkite/test-pipeline.yaml

[360bd67cf] Isotr0py 2024-08-06 [Core] Support loading GGUF model (#5191)
5	0	.github/workflows/clang-format.yml
1	0	CMakeLists.txt
9	0	csrc/ops.h
531	0	csrc/quantization/gguf/dequantize.cuh
969	0	csrc/quantization/gguf/ggml-common.h
242	0	csrc/quantization/gguf/gguf_kernel.cu
600	0	csrc/quantization/gguf/mmq.cuh
182	0	csrc/quantization/gguf/mmvq.cuh
1745	0	csrc/quantization/gguf/vecdotq.cuh
12	0	csrc/torch_bindings.cpp
38	0	examples/gguf_inference.py
5	0	format.sh
1	0	requirements-common.txt
76	0	tests/models/test_gguf.py
4	2	tests/quantization/test_lm_head.py
32	0	vllm/_custom_ops.py
1	0	vllm/config.py
3	0	vllm/engine/arg_utils.py
81	1	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
25	1	vllm/model_executor/layers/quantization/base_config.py
165	0	vllm/model_executor/layers/quantization/gguf.py
54	5	vllm/model_executor/layers/vocab_parallel_embedding.py
92	2	vllm/model_executor/model_loader/loader.py
46	1	vllm/model_executor/model_loader/weight_utils.py
7	0	vllm/model_executor/models/llama.py
1	0	vllm/model_executor/models/qwen2.py
32	8	vllm/transformers_utils/config.py
9	1	vllm/transformers_utils/tokenizer.py

[ef527be06] Cody Yu 2024-08-05 [MISC] Use non-blocking transfer in prepare_input (#7172)
12	15	vllm/attention/backends/flash_attn.py
11	12	vllm/attention/backends/flashinfer.py
12	15	vllm/attention/backends/utils.py
8	7	vllm/worker/model_runner.py

[89b8db6bb] Jacob Schein 2024-08-05 [Bugfix] Specify device when loading LoRA and embedding tensors (#7129)
3	2	vllm/lora/models.py

[789937af2] Thomas Parnell 2024-08-06 [Doc] [SpecDecode] Update MLPSpeculator documentation (#7100)
49	0	docs/source/models/spec_decode.rst
9	0	vllm/model_executor/models/mlp_speculator.py

[dfb1a15dc] youkaichao 2024-08-05 [ci][frontend] deduplicate tests (#7101)
6	8	tests/entrypoints/openai/test_completion.py
0	715	tests/entrypoints/openai/test_disable_mp.py

[4db5176d9] Simon Mo 2024-08-05 bump version to v0.5.4 (#7139)
1	1	docs/source/getting_started/installation.rst
1	1	vllm/version.py

[4cf1dc39b] Tyler Michael Smith 2024-08-05 [Bugfix][CI/Build] Fix CUTLASS FetchContent (#7171)
0	2	CMakeLists.txt

[6e4852ce2] Tyler Michael Smith 2024-08-05 [CI/Build] Suppress divide-by-zero and missing return statement warnings (#7001)
8	0	csrc/attention/dtype_bfloat16.cuh
1	0	csrc/quantization/awq/dequantize.cuh
3	2	csrc/quantization/fp8/nvidia/quant_utils.cuh
12	6	csrc/quantization/gptq_marlin/gptq_marlin.cu

[8571ac467] Tyler Michael Smith 2024-08-05 [Kernel] Update CUTLASS to 3.5.1 (#7085)
3	3	CMakeLists.txt
111	81	csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c3x.hpp
4	4	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
11	19	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu

[997cf7830] Rui Qiao 2024-08-05 [Misc] Fix typo in GroupCoordinator.recv() (#7167)
2	2	vllm/distributed/parallel_state.py

[57f560aa2] Aditya Paliwal 2024-08-05 [BugFix] Use args.trust_remote_code (#7121)
3	3	vllm/entrypoints/openai/api_server.py

[003f8ee12] Nick Hill 2024-08-05 [BugFix] Use IP4 localhost form for zmq bind (#7163)
3	1	vllm/entrypoints/openai/rpc/server.py

[e9630458c] Bongwon Jang 2024-08-06 [SpecDecode] Support FlashInfer in DraftModelRunner (#6926)
47	0	vllm/spec_decode/draft_model_runner.py

[82a1b1a82] Cade Daniel 2024-08-05 [Speculative decoding] Add periodic log with time spent in proposal/scoring/verification (#6963)
48	20	tests/spec_decode/test_spec_decode_worker.py
7	1	vllm/config.py
1	0	vllm/engine/arg_utils.py
54	14	vllm/spec_decode/spec_decode_worker.py
15	0	vllm/spec_decode/util.py

[c0d8f1636] Jungho Christopher Cho 2024-08-05 [Model] SiglipVisionModel ported from transformers (#6942)
2	1	examples/offline_inference_vision_language.py
27	52	vllm/model_executor/models/paligemma.py
621	0	vllm/model_executor/models/siglip.py

[cc08fc722] Cyrus Leung 2024-08-05 [Frontend] Reapply "Factor out code for running uvicorn" (#7095)
53	24	vllm/entrypoints/api_server.py
46	0	vllm/entrypoints/launcher.py
26	58	vllm/entrypoints/openai/api_server.py

[7b86e7c9c] Alphi 2024-08-05 [Model] Add multi-image support for minicpmv (#7122)
3	2	tests/conftest.py
133	13	tests/models/test_minicpmv.py
35	21	vllm/model_executor/models/minicpmv.py
1	1	vllm/multimodal/image.py

[f80ab3521] Jee Jee Li 2024-08-05 Clean up remaining Punica C information (#7027)
0	6	.github/workflows/clang-format.yml
1	1	cmake/utils.cmake
0	6	format.sh
1	1	vllm/config.py
1	1	vllm/lora/layers.py

[16a1cc9bb] youkaichao 2024-08-04 [misc][distributed] improve libcudart.so finding (#7127)
22	22	vllm/distributed/device_communicators/cuda_wrapper.py
3	1	vllm/distributed/device_communicators/custom_all_reduce_utils.py

[b1c9aa3da] Thomas Parnell 2024-08-04 [Bugfix] [SpecDecode] Default speculative_draft_tensor_parallel_size to 1 when using MLPSpeculator (#7105)
12	4	vllm/config.py

[179a6a36f] Jee Jee Li 2024-08-04 [Model]Refactor MiniCPMV (#7020)
1	1	docs/source/models/supported_models.rst
296	0	vllm/model_executor/models/idefics2_vision_model.py
639	384	vllm/model_executor/models/minicpmv.py
1	1	vllm/model_executor/models/na_vit.py

[83c644fe7] youkaichao 2024-08-04 [core][misc] simply output processing with shortcut code path (#7117)
31	8	vllm/engine/output_processor/single_step.py

[9fadc7b7a] youkaichao 2024-08-03 [misc] add zmq in collect env (#7119)
2	0	collect_env.py

[654bc5ca4] Yihuan Bu 2024-08-03 Support for guided decoding for offline LLM (#6878)
1	0	docs/source/conf.py
21	1	tests/entrypoints/{openai => }/conftest.py
142	0	tests/entrypoints/llm/test_guided_generate.py
43	1	vllm/entrypoints/llm.py
20	6	vllm/entrypoints/openai/protocol.py
24	2	vllm/model_executor/guided_decoding/__init__.py
38	0	vllm/model_executor/guided_decoding/guided_fields.py
39	0	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
24	2	vllm/model_executor/guided_decoding/outlines_decoding.py

[825b04486] Jeff Fialho 2024-08-03 [Frontend] Warn if user `max_model_len` is greater than derived `max_model_len` (#7080)
13	6	vllm/config.py
10	0	vllm/envs.py

[44dcb52e3] youkaichao 2024-08-03 [ci][test] finalize fork_new_process_for_each_test (#7114)
3	0	tests/utils.py

[67d745cc6] Kuntai Du 2024-08-02 [CI] Temporarily turn off H100 performance benchmark (#7104)
16	16	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml

[99d7cabd7] Jee Jee Li 2024-08-03 [LoRA]  ReplicatedLinear support LoRA (#7081)
103	0	tests/lora/test_layers.py
94	0	vllm/lora/layers.py
2	0	vllm/lora/utils.py

[fb2c1c86c] Zach Zheng 2024-08-02 [Bugfix] Fix block table for seqs that have prefix cache hits (#7018)
56	0	tests/prefix_caching/test_prefix_caching.py
9	3	vllm/attention/backends/flash_attn.py

[0c25435da] Isotr0py 2024-08-03 [Model] Refactor and decouple weight loading logic for InternVL2 model (#7067)
10	1	vllm/model_executor/models/intern_vit.py
28	54	vllm/model_executor/models/internvl.py

[a0d164567] youkaichao 2024-08-02 [ci][distributed] disable ray dag tests (#7099)
18	25	tests/distributed/test_pipeline_parallel.py

[04e558342] youkaichao 2024-08-02 [ci][distributed] merge distributed test commands (#7097)
4	23	.buildkite/test-pipeline.yaml
33	17	tests/distributed/test_basic_distributed_correctness.py
14	21	tests/distributed/test_chunked_prefill_distributed.py
27	30	tests/distributed/test_multimodal_broadcast.py

[8c025fa70] Cyrus Leung 2024-08-03 [Frontend] Factor out chat message parsing (#7055)
23	5	vllm/entrypoints/chat_utils.py
5	12	vllm/entrypoints/openai/serving_chat.py
11	10	vllm/entrypoints/openai/serving_tokenization.py

[69ea15e5c] youkaichao 2024-08-02 [ci][distributed] shorten wait time if server hangs (#7098)
1	1	tests/utils.py

[ed812a73f] Robert Shaw 2024-08-02 [ Frontend ] Multiprocessing for OpenAI Server with `zeromq` (#6883)
715	0	tests/entrypoints/openai/test_disable_mp.py
26	1	vllm/engine/async_llm_engine.py
20	16	vllm/engine/llm_engine.py
84	0	vllm/engine/protocol.py
96	36	vllm/entrypoints/openai/api_server.py
7	2	vllm/entrypoints/openai/cli_args.py
11	8	vllm/entrypoints/openai/logits_processors.py
42	0	vllm/entrypoints/openai/rpc/__init__.py
248	0	vllm/entrypoints/openai/rpc/client.py
216	0	vllm/entrypoints/openai/rpc/server.py
9	7	vllm/entrypoints/openai/serving_chat.py
11	8	vllm/entrypoints/openai/serving_completion.py
7	6	vllm/entrypoints/openai/serving_embedding.py
4	4	vllm/entrypoints/openai/serving_engine.py
5	5	vllm/entrypoints/openai/serving_tokenization.py
6	0	vllm/envs.py
19	0	vllm/model_executor/guided_decoding/outlines_logits_processors.py
1	1	vllm/tracing.py
18	1	vllm/transformers_utils/tokenizer_group/__init__.py
22	6	vllm/utils.py

[708989341] youkaichao 2024-08-02 [misc] add a flag to enable compile (#7092)
4	0	vllm/envs.py
6	0	vllm/worker/model_runner.py

[22e718ff1] Rui Qiao 2024-08-02 [Misc] Revive to use loopback address for driver IP (#7091)
10	0	vllm/executor/ray_gpu_executor.py

[05308891e] Rui Qiao 2024-08-02 [Core] Pipeline parallel with Ray ADAG (#6837)
2	0	Dockerfile
1	0	MANIFEST.in
3	0	requirements-adag.txt
3	0	requirements-test.txt
35	16	tests/distributed/test_pipeline_parallel.py
25	6	tests/utils.py
10	2	vllm/envs.py
92	45	vllm/executor/ray_gpu_executor.py
24	6	vllm/executor/ray_utils.py
4	2	vllm/worker/worker_base.py

[a8d604ca2] Lucas Wilkinson 2024-08-02 [Misc] Disambiguate quantized types via a new ScalarType (#6396)
35	17	CMakeLists.txt
3	0	Dockerfile.openvino
27	23	benchmarks/kernels/benchmark_marlin.py
0	1	cmake/cpu_extension.cmake
0	0	csrc/{ => core}/registration.h
382	0	csrc/core/scalar_type.hpp
16	0	csrc/core/torch_bindings.cpp
1	1	csrc/cpu/torch_bindings.cpp
1	1	csrc/moe/torch_bindings.cpp
6	2	csrc/ops.h
44	22	csrc/quantization/gptq_marlin/gptq_marlin.cu
10	7	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
1	1	csrc/torch_bindings.cpp
8	1	setup.py
0	2	tests/kernels/test_int8_quant.py
41	34	tests/kernels/test_marlin_gemm.py
36	0	tests/test_scalartype.py
177	0	vllm/_core_ext.py
19	10	vllm/_custom_ops.py
31	18	vllm/model_executor/layers/quantization/awq_marlin.py
14	4	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
20	9	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
27	16	vllm/model_executor/layers/quantization/gptq_marlin.py
19	10	vllm/model_executor/layers/quantization/gptq_marlin_24.py
57	63	vllm/model_executor/layers/quantization/utils/marlin_utils.py
19	10	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
14	16	vllm/model_executor/layers/quantization/utils/marlin_utils_test_24.py
64	84	vllm/model_executor/layers/quantization/utils/quant_utils.py
35	0	vllm/scalar_type.py

[b482b9a5b] Michael Goin 2024-08-02 [CI/Build] Add support for Python 3.12 (#7035)
1	1	.github/workflows/mypy.yaml
1	1	.github/workflows/publish.yml
1	1	.github/workflows/ruff.yml
1	1	.github/workflows/yapf.yml
1	1	CMakeLists.txt
1	1	docs/source/getting_started/installation.rst
1	0	setup.py

[806949514] youkaichao 2024-08-02 [ci] set timeout for test_oot_registration.py (#7082)
4	0	tests/entrypoints/openai/test_oot_registration.py
3	1	vllm/worker/worker.py
3	1	vllm/worker/xpu_worker.py

[c16eaac50] Jie Fu (傅杰) 2024-08-02 [Hardware][Intel CPU] Update torch 2.4.0 for CPU backend (#6931)
1	1	Dockerfile.cpu
1	1	requirements-cpu.txt

[db3518639] Peng Guanwen 2024-08-02 [Core] Comment out unused code in sampler (#7023)
31	27	vllm/model_executor/sampling_metadata.py

[660dea123] youkaichao 2024-08-02 [cuda][misc] remove error_on_invalid_device_count_status (#7069)
0	3	vllm/executor/multiproc_gpu_executor.py
3	6	vllm/executor/ray_gpu_executor.py
0	23	vllm/utils.py

[cf2a1a4d9] Bongwon Jang 2024-08-02 Fix tracing.py (#7065)
1	1	vllm/tracing.py

[252357793] youkaichao 2024-08-01 [ci][distributed] try to fix pp test (#7054)
3	1	tests/distributed/test_pipeline_parallel.py
39	0	tests/utils.py
1	1	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
2	1	vllm/utils.py

[3bb4b1e4c] Cyrus Leung 2024-08-02 [mypy] Speed up mypy checking (#7056)
1	1	.github/workflows/mypy.yaml
3	2	format.sh

[954f7305a] Lily Liu 2024-08-01 [Kernel] Fix input for flashinfer prefill wrapper. (#7008)
9	2	vllm/attention/backends/flashinfer.py

[6ce01f306] Woosuk Kwon 2024-08-01 [Performance] Optimize `get_seqs` (#7051)
1	1	vllm/core/block_manager_v1.py
20	20	vllm/sequence.py
1	1	vllm/transformers_utils/detokenizer.py

[6a11fdfbb] Tyler Michael Smith 2024-08-01 [CI/Build][Bugfix] Fix CUTLASS header-only line (#7034)
4	1	CMakeLists.txt

[805a8a75f] Woosuk Kwon 2024-08-01 [Misc] Support attention logits soft-capping with flash-attn (#7022)
1	1	requirements-cuda.txt
13	6	tests/kernels/test_flash_attn.py
1	0	vllm/attention/backends/abstract.py
3	0	vllm/attention/backends/blocksparse_attn.py
10	11	vllm/attention/backends/flash_attn.py
5	9	vllm/attention/backends/flashinfer.py
6	2	vllm/attention/backends/ipex_attn.py
4	0	vllm/attention/backends/pallas.py
8	2	vllm/attention/backends/rocm_flash_attn.py
6	2	vllm/attention/backends/torch_sdpa.py
0	9	vllm/attention/backends/utils.py
7	2	vllm/attention/backends/xformers.py
2	1	vllm/attention/layer.py
5	2	vllm/model_executor/models/gemma2.py

[562e580ab] omkar kakarparthi 2024-08-01 Update run-amd-test.sh (#7044)
0	1	.buildkite/run-amd-test.sh

[fc912e088] Murali Andoorveedu 2024-08-01 [Models] Support Qwen model with PP (#6974)
1	1	docs/source/serving/distributed_serving.rst
1	0	vllm/config.py
44	10	vllm/model_executor/models/qwen.py

[f4fd390f5] Michael Goin 2024-08-01 [Bugfix] Lower gemma's unloaded_params exception to warning (#7002)
3	3	vllm/model_executor/models/gemma.py
6	3	vllm/model_executor/models/gemma2.py
3	3	vllm/model_executor/models/paligemma.py

[fb3db6168] Michael Goin 2024-08-01 [CI/Build] Remove sparseml requirement from testing (#7037)
0	1	requirements-test.txt
0	4	tests/conftest.py
0	52	tests/models/test_compressed_tensors.py
1	1	tests/quantization/test_compressed_tensors.py

[2dd34371a] Isotr0py 2024-08-02 [Bugfix] Fix RMSNorm forward in InternViT attention qk_layernorm (#6992)
4	4	vllm/model_executor/models/intern_vit.py

[7e0861bd0] Sage Moore 2024-08-01 [CI/Build] Update PyTorch to 2.4.0 (#6951)
3	3	.buildkite/test-pipeline.yaml
1	1	.github/workflows/publish.yml
1	1	CMakeLists.txt
1	1	Dockerfile
1	1	pyproject.toml
1	1	requirements-build.txt
4	4	requirements-cuda.txt
1	1	vllm/model_executor/layers/ops/sample.py

[a72a424b3] Alexei-V-Ivanov-AMD 2024-08-01 [Build/CI] Fixing Docker Hub quota issue. (#7043)
1	1	.buildkite/run-amd-test.sh

[c8a7e9327] youkaichao 2024-07-31 [core][scheduler] simplify and improve scheduler (#6867)
1	1	tests/core/block/e2e/test_correctness.py
68	95	tests/core/test_scheduler.py
0	45	vllm/core/policy.py
43	73	vllm/core/scheduler.py

[3c10591ef] zifeitong 2024-07-31 [Bugfix] Set SamplingParams.max_tokens for OpenAI requests if not provided by user (#6954)
39	0	tests/entrypoints/openai/test_serving_chat.py
23	7	vllm/entrypoints/openai/protocol.py
8	15	vllm/entrypoints/openai/serving_chat.py
9	18	vllm/entrypoints/openai/serving_completion.py
13	4	vllm/entrypoints/openai/serving_engine.py

[0437492ea] Aurick Qiao 2024-07-31 PP comm optimization: replace send with partial send + allgather (#6695)
36	2	vllm/distributed/parallel_state.py
5	3	vllm/worker/worker_base.py

[630dd9e0a] Travis Johnson 2024-07-31 [Bugfix][Model] Skip loading lm_head weights if using tie_word_embeddings (#6758)
7	0	vllm/model_executor/models/chameleon.py
5	0	vllm/model_executor/models/llama.py
5	1	vllm/model_executor/models/minicpm.py
5	0	vllm/model_executor/models/olmo.py

[23993a799] Woosuk Kwon 2024-07-31 [Bugfix][TPU] Do not use torch.Generator for TPUs (#6981)
6	0	vllm/model_executor/model_loader/weight_utils.py

[1d2e7fb73] xuyi 2024-08-01 [Model] Pipeline parallel support for Qwen2 (#6924)
2	0	vllm/config.py
45	12	vllm/model_executor/models/qwen2.py
54	15	vllm/model_executor/models/qwen2_moe.py

[7ecee3432] Jee Jee Li 2024-08-01 [Kernel][RFC] Refactor the punica kernel based on Triton (#5036)
0	2	.github/workflows/scripts/build.sh
0	62	CMakeLists.txt
0	2	Dockerfile
1	2	Dockerfile.rocm
0	217	csrc/punica/LICENSE
0	5	csrc/punica/bgmv/bgmv_bf16_bf16_bf16.cu
0	5	csrc/punica/bgmv/bgmv_bf16_fp32_bf16.cu
0	218	csrc/punica/bgmv/bgmv_config.h
0	5	csrc/punica/bgmv/bgmv_fp16_fp16_fp16.cu
0	5	csrc/punica/bgmv/bgmv_fp16_fp32_fp16.cu
0	5	csrc/punica/bgmv/bgmv_fp32_bf16_bf16.cu
0	5	csrc/punica/bgmv/bgmv_fp32_fp16_fp16.cu
0	451	csrc/punica/bgmv/bgmv_impl.cuh
0	48	csrc/punica/bgmv/generator.py
0	1325	csrc/punica/bgmv/vec_dtypes.cuh
0	569	csrc/punica/punica_ops.cu
0	11	csrc/punica/punica_ops.h
0	18	csrc/punica/torch_bindings.cpp
0	82	csrc/punica/type_convert.h
0	1	docs/source/getting_started/installation.rst
0	10	setup.py
30	18	tests/kernels/test_sampler.py
1	1	tests/lora/test_gemma.py
81	59	tests/lora/test_layers.py
0	224	tests/lora/test_lora.py
0	258	tests/lora/test_punica.py
408	0	tests/lora/test_punica_sizes.py
342	0	tests/lora/test_punica_variation.py
27	21	tests/lora/test_quant_model.py
148	0	tests/lora/utils.py
1	41	vllm/_custom_ops.py
0	5	vllm/envs.py
80	57	vllm/lora/fully_sharded_layers.py
151	286	vllm/lora/layers.py
16	155	vllm/lora/models.py
0	0	vllm/lora/ops/__init__.py
169	0	vllm/lora/ops/bgmv_expand.py
182	0	vllm/lora/ops/bgmv_expand_slice.py
150	0	vllm/lora/ops/bgmv_shrink.py
192	0	vllm/lora/ops/sgmv_expand.py
205	0	vllm/lora/ops/sgmv_expand_slice.py
189	0	vllm/lora/ops/sgmv_shrink.py
46	0	vllm/lora/ops/utils.py
581	184	vllm/lora/punica.py
2	1	vllm/triton_utils/__init__.py
167	0	vllm/triton_utils/libentry.py
6	6	vllm/worker/model_runner.py

[7eb0cb4a1] Simon Mo 2024-07-31 Revert "[Frontend] Factor out code for running uvicorn" (#7012)
0	1	pyproject.toml
24	50	vllm/entrypoints/api_server.py
51	21	vllm/entrypoints/openai/api_server.py
0	3	vllm/server/__init__.py
0	42	vllm/server/launch.py

[a0dce9383] Michael Goin 2024-07-31 [Misc] Add compressed-tensors to optimized quant list (#7006)
6	4	vllm/config.py

[35e9c12bf] Varun Sundar Rabindranath 2024-07-31 [Kernel] Tuned int8 Cutlass Kernels for SM75 (T4) (#6996)
8	1	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
4	11	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
123	0	csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm75_dispatch.cuh

[93548eb37] Varun Sundar Rabindranath 2024-07-31 [Kernel] Enable FP8 Cutlass for Ada Lovelace (#6950)
1	7	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[460c1884e] Michael Goin 2024-07-31 [Bugfix] Support cpu offloading with fp8 quantization (#6960)
37	6	tests/basic_correctness/test_cpu_offload.py
53	3	vllm/model_executor/model_loader/loader.py
26	24	vllm/model_executor/models/utils.py

[bd7001340] Cody Yu 2024-07-31 [MISC] Introduce pipeline parallelism partition strategies (#6920)
34	0	tests/distributed/test_pipeline_partition.py
27	5	vllm/distributed/utils.py
5	0	vllm/envs.py

[2ee8d3ba5] Avshalom Manevich 2024-07-31 [Model] use FusedMoE layer in Jamba (#6935)
49	108	vllm/model_executor/models/jamba.py

[daed30c4a] Cyrus Leung 2024-07-31 [Bugfix] Fix feature size calculation for LLaVA-NeXT (#6982)
70	18	tests/models/test_llava_next.py
1	1	vllm/model_executor/models/fuyu.py
3	3	vllm/model_executor/models/internvl.py
22	26	vllm/model_executor/models/llava_next.py
2	2	vllm/model_executor/models/phi3v.py

[2f4e108f7] Alphi 2024-07-31 [Bugfix] Clean up MiniCPM-V (#6939)
5	1	docs/source/models/supported_models.rst
1	3	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/minicpm.py
162	87	vllm/model_executor/models/minicpmv.py
804	0	vllm/model_executor/models/na_vit.py
1	1	vllm/model_executor/models/qwen2.py

[6512937de] HandH1998 2024-07-31 Support W4A8 quantization for vllm (#5218)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-QQQ.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
1	0	CMakeLists.txt
7	0	csrc/ops.h
32	0	csrc/quantization/marlin/dense/common/base.h
89	0	csrc/quantization/marlin/dense/common/mem.h
6	84	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
1243	0	csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu
4	0	csrc/torch_bindings.cpp
66	0	tests/kernels/test_marlin_gemm.py
9	0	vllm/_custom_ops.py
2	0	vllm/model_executor/layers/quantization/__init__.py
285	0	vllm/model_executor/layers/quantization/qqq.py
125	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test_qqq.py
82	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[c0644cf9c] Fei 2024-07-31 [Bugfix] fix logit processor excceed vocab size issue (#6927)
6	0	vllm/entrypoints/openai/logits_processors.py

[533d1932d] Woosuk Kwon 2024-07-31 [Bugfix][TPU] Set readonly=True for non-root devices (#6980)
4	1	vllm/worker/tpu_worker.py

[9f0e69b65] Cyrus Leung 2024-07-31 [CI/Build] Fix mypy errors (#6968)
2	2	vllm/_custom_ops.py
2	4	vllm/multimodal/base.py

[f230cc2ca] Cyrus Leung 2024-07-31 [Bugfix] Fix broadcasting logic for `multi_modal_kwargs` (#6836)
2	3	.buildkite/test-pipeline.yaml
2	0	docs/source/dev/multimodal/multimodal_index.rst
5	4	tests/distributed/test_multimodal_broadcast.py
0	57	tests/distributed/test_parallel_state.py
64	32	tests/models/test_llava_next.py
11	35	vllm/distributed/parallel_state.py
4	2	vllm/multimodal/__init__.py
42	20	vllm/multimodal/base.py
3	1	vllm/spec_decode/draft_model_runner.py
50	1	vllm/utils.py
15	12	vllm/worker/cpu_model_runner.py
11	5	vllm/worker/embedding_model_runner.py
7	7	vllm/worker/model_runner.py
8	9	vllm/worker/neuron_model_runner.py
15	11	vllm/worker/openvino_model_runner.py
15	12	vllm/worker/xpu_model_runner.py

[da1f7cc12] Cyrus Leung 2024-07-31 [mypy] Enable following imports for some directories (#6681)
13	18	.github/workflows/mypy.yaml
13	17	format.sh
16	2	pyproject.toml
1	1	vllm/_custom_ops.py
49	13	vllm/_ipex_ops.py
15	15	vllm/adapter_commons/models.py
5	5	vllm/adapter_commons/request.py
7	7	vllm/adapter_commons/worker_manager.py
2	1	vllm/config.py
6	8	vllm/engine/llm_engine.py
1	3	vllm/entrypoints/openai/serving_engine.py
5	5	vllm/scripts.py
2	0	vllm/transformers_utils/detokenizer.py
4	5	vllm/transformers_utils/tokenizer_group/__init__.py
10	8	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
15	15	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
13	15	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
8	5	vllm/utils.py

[c32ab8be1] Cade Daniel 2024-07-30 [Speculative decoding] Add serving benchmark for llama3 70b + speculative decoding (#6964)
22	1	.buildkite/nightly-benchmarks/tests/serving-tests.json

[fb4f530bf] Cade Daniel 2024-07-30 [CI] [nightly benchmark] Do not re-download sharegpt dataset if exists (#6706)
10	1	.buildkite/nightly-benchmarks/run-benchmarks-suite.sh

[79319cedf] Cade Daniel 2024-07-30 [Nightly benchmarking suite] Remove pkill python from run benchmark suite (#6965)
0	5	.buildkite/nightly-benchmarks/run-benchmarks-suite.sh

[40c27a7cb] Simon Mo 2024-07-30 [Build] Temporarily Disable Kernels and LoRA tests (#6961)
20	20	.buildkite/test-pipeline.yaml

[6ca8031e7] youkaichao 2024-07-30 [core][misc] improve free_finished_seq_groups (#6865)
12	7	vllm/core/scheduler.py

[d7a299eda] Tyler Michael Smith 2024-07-30 [Kernel] Remove scaled_fp8_quant kernel padding footgun (#6842)
1	1	tests/quantization/test_fp8.py
13	11	vllm/_custom_ops.py
3	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[052b6f8ca] Sanger Steel 2024-07-30 [Bugfix] Fix tensorizer memory profiling bug during testing (#6881)
19	16	tests/tensorizer_loader/conftest.py
91	78	tests/tensorizer_loader/test_tensorizer.py

[5895b2467] Ilya Lavrenov 2024-07-30 [OpenVINO] Updated OpenVINO requirements and build docs (#6948)
2	2	Dockerfile.openvino
1	1	docs/source/getting_started/openvino-installation.rst
27	1	requirements-openvino.txt

[cbbc90447] Tyler Michael Smith 2024-07-30 [Kernel] Squash a few more warnings (#6914)
2	2	csrc/attention/attention_kernels.cu
0	2	csrc/quantization/aqlm/gemm_kernels.cu
2	0	csrc/quantization/fp8/amd/quant_utils.cuh
2	0	csrc/quantization/fp8/nvidia/quant_utils.cuh
2	1	csrc/quantization/squeezellm/quant_cuda_kernel.cu

[5cf9254a9] Nick Hill 2024-07-30 [BugFix] Fix use of per-request seed with pipeline parallel (#6698)
8	15	tests/samplers/test_rejection_sampler.py
4	1	tests/samplers/test_sampler.py
53	1	tests/spec_decode/e2e/test_mlp_correctness.py
1	1	tests/spec_decode/e2e/test_seed.py
1	0	tests/spec_decode/test_batch_expansion.py
31	0	tests/utils.py
0	1	vllm/core/scheduler.py
39	56	vllm/model_executor/layers/rejection_sampler.py
2	2	vllm/model_executor/layers/spec_decode_base_sampler.py
13	7	vllm/model_executor/sampling_metadata.py
0	12	vllm/sequence.py
22	15	vllm/spec_decode/batch_expansion.py
3	1	vllm/spec_decode/medusa_worker.py
3	1	vllm/spec_decode/mlp_speculator_worker.py
1	2	vllm/spec_decode/ngram_worker.py
11	14	vllm/spec_decode/spec_decode_worker.py
2	1	vllm/worker/cpu_model_runner.py
9	5	vllm/worker/model_runner.py
15	0	vllm/worker/model_runner_base.py
2	1	vllm/worker/neuron_model_runner.py
2	1	vllm/worker/xpu_model_runner.py

[f05840368] fzyzcjy 2024-07-31 [Doc] Super tiny fix doc typo (#6949)
3	3	vllm/engine/arg_utils.py

[c66c7f86a] Roger Wang 2024-07-30 [Bugfix] Fix PaliGemma MMP (#6930)
2	5	vllm/model_executor/models/paligemma.py

[6e063ea35] Woosuk Kwon 2024-07-30 [TPU] Fix greedy decoding (#6933)
18	9	vllm/worker/tpu_model_runner.py

[af647fb8b] Varun Sundar Rabindranath 2024-07-29 [Kernel] Tuned int8 kernels for Ada Lovelace (#6848)
10	17	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
30	24	csrc/quantization/cutlass_w8a8/{scaled_mm_c2x_sm89_dispatch.cuh => scaled_mm_c2x_sm89_fp8_dispatch.cuh}
353	0	csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_int8_dispatch.cuh
2	2	tests/kernels/test_cutlass.py

[61a97c32f] Tyler Michael Smith 2024-07-29 [Kernel] Fix marlin divide-by-zero warnings (#6904)
32	29	csrc/quantization/gptq_marlin/gptq_marlin.cu
13	5	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
13	5	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu

[4fbf4aa12] Kevin H. Luu 2024-07-29 [ci] GHA workflow to remove ready label upon "/notready" comment (#6921)
23	0	.github/workflows/remove_label_not_ready_comment.yml

[aae6d36f7] Tyler Michael Smith 2024-07-29 [Kernel] Remove unused variables in awq/gemm_kernels.cu (#6908)
0	23	csrc/quantization/awq/gemm_kernels.cu

[9f69d8245] Nick Hill 2024-07-29 [Frontend] New `allowed_token_ids` decoding request parameter (#6753)
22	0	tests/entrypoints/openai/test_completion.py
74	0	vllm/entrypoints/openai/logits_processors.py
16	44	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py

[9a7e2d053] Thomas Parnell 2024-07-29 [Bugfix] Allow vllm to still work if triton is not installed. (#6786)
0	1	requirements-cpu.txt
0	2	requirements-openvino.txt
0	1	requirements-tpu.txt
4	3	tests/kernels/test_sampler.py
4	1	vllm/attention/ops/paged_attn.py
15	7	vllm/model_executor/layers/fused_moe/__init__.py
1	13	vllm/model_executor/layers/ops/sample.py
2	2	vllm/model_executor/layers/quantization/fp8.py
5	1	vllm/model_executor/layers/sampler.py
1	1	vllm/model_executor/sampling_metadata.py
9	5	vllm/triton_utils/__init__.py
11	0	vllm/triton_utils/importing.py
13	0	vllm/triton_utils/sample.py

[7f8d612d2] Earthwalker 2024-07-30 [TPU] Support tensor parallelism in async llm engine (#6891)
3	0	Dockerfile.tpu
8	2	vllm/engine/async_llm_engine.py

[60d1c6e58] Tyler Michael Smith 2024-07-29 [Kernel] Fix deprecation function warnings squeezellm quant_cuda_kernel (#6901)
2	2	csrc/quantization/squeezellm/quant_cuda_kernel.cu

[db9e5708a] Peng Guanwen 2024-07-30 [Core] Reduce unnecessary compute when logprobs=None (#6532)
37	2	tests/samplers/test_logprobs.py
81	63	vllm/model_executor/layers/sampler.py
9	8	vllm/outputs.py
8	7	vllm/sampling_params.py

[766435e66] Varun Sundar Rabindranath 2024-07-29 [Kernel] Tuned FP8 Kernels for Ada Lovelace (#6677)
1	1	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
33	487	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
340	0	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cuh
139	0	csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm80_dispatch.cuh
362	0	csrc/quantization/cutlass_w8a8/scaled_mm_c2x_sm89_dispatch.cuh
2	2	tests/kernels/test_cutlass.py

[7cbd9ec7a] Isotr0py 2024-07-29 [Model] Initialize support for InternVL2 series models (#6514)
4	0	docs/source/models/supported_models.rst
15	0	examples/offline_inference_vision_language.py
2	0	examples/openai_vision_api_client.py
1	0	requirements-test.txt
201	0	tests/models/test_internvl.py
1	1	vllm/entrypoints/chat_utils.py
1	0	vllm/model_executor/models/__init__.py
270	0	vllm/model_executor/models/intern_vit.py
9	1	vllm/model_executor/models/internlm2.py
471	0	vllm/model_executor/models/internvl.py
9	1	vllm/model_executor/models/qwen2.py
5	3	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
51	0	vllm/transformers_utils/configs/internvl.py

[3eeb148f4] Elsa Granger 2024-07-28 [Misc] Pass cutlass_fp8_supported correctly in fbgemm_fp8 (#6871)
11	8	vllm/model_executor/layers/quantization/fbgemm_fp8.py

[b1366a953] Michael Goin 2024-07-27 Add Nemotron to PP_SUPPORTED_MODELS (#6863)
1	0	vllm/config.py

[75acdaa4b] Alexander Matveev 2024-07-27 [Kernel] Increase precision of GPTQ/AWQ Marlin kernel (#6795)
18	5	benchmarks/kernels/benchmark_marlin.py
2	1	csrc/ops.h
122	28	csrc/quantization/gptq_marlin/gptq_marlin.cu
10	3	tests/kernels/test_marlin_gemm.py
3	3	vllm/_custom_ops.py
13	4	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[fad5576c5] Woosuk Kwon 2024-07-27 [TPU] Reduce compilation time & Upgrade PyTorch XLA version  (#6856)
1	1	Dockerfile.tpu
8	1	docs/source/getting_started/tpu-installation.rst
0	1	vllm/attention/backends/pallas.py
2	1	vllm/distributed/device_communicators/tpu_communicator.py
13	2	vllm/worker/tpu_model_runner.py
0	1	vllm/worker/tpu_worker.py

[f954d0715] Chenggang Wu 2024-07-27 [Docs] Add RunLLM chat widget (#6857)
16	0	docs/source/_static/custom.js
2	0	docs/source/conf.py

[1ad86acf1] Cyrus Leung 2024-07-27 [Model] Initial support for BLIP-2 (#5920)
8	0	docs/source/models/supported_models.rst
11	0	examples/offline_inference_vision_language.py
11	0	examples/template_blip2.jinja
102	0	tests/models/test_blip2.py
4	4	tests/models/test_fuyu.py
4	4	tests/models/test_minicpmv.py
4	4	tests/models/test_phi3v.py
4	2	vllm/model_executor/models/__init__.py
269	0	vllm/model_executor/models/blip.py
669	0	vllm/model_executor/models/blip2.py
15	2	vllm/model_executor/models/opt.py
6	5	vllm/multimodal/base.py

[ecb33a28c] Roger Wang 2024-07-27 [CI/Build][Doc] Update CI and Doc for VLM example changes (#6860)
1	2	.buildkite/test-pipeline.yaml
1	1	docs/source/models/vlm.rst

[a57d75821] Wang Ran (汪然) 2024-07-27 [bugfix] make args.stream work (#6831)
4	1	examples/api_client.py

[925de97e0] Roger Wang 2024-07-26 [Bugfix] Fix VLM example typo (#6859)
0	1	examples/offline_inference_vision_language.py

[aa46953a2] Roger Wang 2024-07-26 [Misc][VLM][Doc] Consolidate offline examples for vision language models (#6858)
0	31	examples/fuyu_example.py
0	25	examples/llava_example.py
0	36	examples/llava_next_example.py
0	55	examples/minicpmv_example.py
174	0	examples/offline_inference_vision_language.py
0	25	examples/paligemma_example.py
0	40	examples/phi3v_example.py

[593e79e73] Travis Johnson 2024-07-26 [Bugfix] torch.set_num_threads() in multiproc_gpu_executor (#6802)
19	4	vllm/executor/multiproc_gpu_executor.py

[c53041ae3] Harry Mellor 2024-07-27 [Doc] Add missing mock import to docs `conf.py` (#6834)
1	0	.readthedocs.yaml
2	0	docs/source/conf.py

[52f07e3de] Woosuk Kwon 2024-07-26 [Hardware][TPU] Implement tensor parallelism with Ray (#5871)
1	0	requirements-tpu.txt
2	2	vllm/attention/backends/pallas.py
8	2	vllm/engine/llm_engine.py
313	0	vllm/executor/ray_tpu_executor.py
31	11	vllm/worker/tpu_model_runner.py
10	6	vllm/worker/tpu_worker.py

[14dbd5a76] Joe 2024-07-26 [Model] H2O Danube3-4b (#6451)
1	1	.buildkite/run-cpu-test.sh
1	1	benchmarks/kernels/benchmark_paged_attention.py
1	1	benchmarks/kernels/benchmark_rope.py
6	0	csrc/attention/attention_kernels.cu
3	1	tests/kernels/test_attention.py
7	1	tests/kernels/test_cache.py
1	1	tests/kernels/test_pos_encoding.py
52	0	tests/models/test_danube3_4b.py
1	1	vllm/attention/ops/paged_attn.py
6	0	vllm/utils.py

[ed94e4f42] tomeras91 2024-07-27 [Bugfix][Model] Jamba assertions and no chunked prefill by default for Jamba (#6784)
5	1	vllm/engine/arg_utils.py
5	0	vllm/model_executor/models/jamba.py

[3c3012398] omrishiv 2024-07-26 [Doc] add VLLM_TARGET_DEVICE=neuron to documentation for neuron (#6844)
1	1	docs/source/getting_started/neuron-installation.rst

[ced36cd89] Woosuk Kwon 2024-07-26 [ROCm] Upgrade PyTorch nightly version (#6845)
2	2	Dockerfile.rocm
1	1	docs/source/getting_started/amd-installation.rst

[969d03226] Sanger Steel 2024-07-26 [Bugfix]: Fix Tensorizer test failures (#6835)
0	1	.buildkite/test-pipeline.yaml
45	0	tests/tensorizer_loader/conftest.py
2	5	tests/tensorizer_loader/test_tensorizer.py

[55712941e] Lucas Wilkinson 2024-07-26 [Bug Fix] Illegal memory access, FP8 Llama 3.1 405b  (#6852)
37	9	csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c3x.hpp

[981b0d567] Cyrus Leung 2024-07-27 [Frontend] Factor out code for running uvicorn (#6828)
50	24	vllm/entrypoints/api_server.py
21	51	vllm/entrypoints/openai/api_server.py
3	0	vllm/server/__init__.py
42	0	vllm/server/launch.py

[d09b94ca5] Woosuk Kwon 2024-07-26 [TPU] Support collective communications in XLA devices (#6813)
30	0	vllm/distributed/device_communicators/tpu_communicator.py
22	0	vllm/distributed/parallel_state.py
4	0	vllm/lora/layers.py
14	2	vllm/model_executor/layers/logits_processor.py

[bb5494676] chenqianfzh 2024-07-26 enforce eager mode with bnb quantization temporarily (#6846)
4	0	vllm/config.py

[b5f49ee55] Gurpreet Singh Dhami 2024-07-26 Update README.md (#6847)
1	1	examples/fp8/quantizer/README.md

[150a1ffbf] Zhanghao Wu 2024-07-26 [Doc] Update SkyPilot doc for wrong indents and instructions for update service (#4283)
243	187	docs/source/serving/run_on_sky.rst

[281977bd6] Michael Goin 2024-07-26 [Doc] Add Nemotron to supported model docs (#6843)
4	0	docs/source/models/supported_models.rst
1	3	vllm/model_executor/layers/activation.py

[3bbb4936d] Li, Jiang 2024-07-27 [Hardware] [Intel] Enable Multiprocessing and tensor parallel in CPU backend and update documentation  (#6125)
20	8	.buildkite/run-cpu-test.sh
5	4	Dockerfile.cpu
4	0	cmake/cpu_extension.cmake
7	0	csrc/cpu/torch_bindings.cpp
65	0	csrc/cpu/utils.cpp
47	8	docs/source/getting_started/cpu-installation.rst
2	2	requirements-cpu.txt
3	0	vllm/distributed/parallel_state.py
0	2	vllm/engine/async_llm_engine.py
7	1	vllm/envs.py
227	37	vllm/executor/cpu_executor.py
0	21	vllm/utils.py
4	3	vllm/worker/cpu_model_runner.py
13	4	vllm/worker/cpu_worker.py

[aa4867791] Woosuk Kwon 2024-07-26 [Misc][TPU] Support TPU in initialize_ray_cluster (#6812)
21	15	vllm/executor/ray_utils.py

[71734f1bf] Woosuk Kwon 2024-07-26 [Build/CI][ROCm] Minor simplification to Dockerfile.rocm (#6811)
2	9	Dockerfile.rocm

[50704f52c] Tyler Michael Smith 2024-07-26 [Bugfix][Kernel] Promote another index to int64_t (#6838)
1	1	csrc/quantization/fp8/common.cu

[07278c37d] Michael Goin 2024-07-26 [Model] Support Nemotron models (Nemotron-3, Nemotron-4, Minitron) (#6611)
11	0	.buildkite/lm-eval-harness/configs/Minitron-4B-Base.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
16	0	vllm/model_executor/layers/activation.py
3	0	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/models/__init__.py
531	0	vllm/model_executor/models/nemotron.py
2	1	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
209	0	vllm/transformers_utils/configs/nemotron.py

[85ad7e2d0] youkaichao 2024-07-25 [doc][debugging] add known issues for hangs (#6816)
4	0	docs/source/getting_started/debugging.rst

[89a84b0bb] Peng Guanwen 2024-07-26 [Core] Use array to speedup padding (#6779)
1	1	vllm/model_executor/layers/sampler.py
12	9	vllm/model_executor/sampling_metadata.py
16	7	vllm/sequence.py

[084a01fd3] Anthony Platanios 2024-07-26 [Bugfix] [Easy] Fixed a bug in the multiprocessing GPU executor. (#6770)
4	2	vllm/executor/multiproc_gpu_executor.py

[062a1d0fa] QQSong 2024-07-25 Fix ReplicatedLinear weight loading (#6793)
5	1	vllm/model_executor/layers/linear.py

[2eb9f4ff2] Kevin H. Luu 2024-07-25 [ci] Mark tensorizer as soft fail and separate from grouped test (#6810)
3	2	.buildkite/test-pipeline.yaml

[443c7cf4c] youkaichao 2024-07-25 [ci][distributed] fix flaky tests (#6806)
8	1	tests/distributed/test_pipeline_parallel.py

[1adddb14b] SangBin Cho 2024-07-25 [Core] Fix ray forward_dag error mssg (#6792)
1	2	vllm/executor/ray_gpu_executor.py

[b7215de2c] Woosuk Kwon 2024-07-25 [Docs] Publish 5th meetup slides (#6799)
1	9	README.md
1	0	docs/source/community/meetups.rst

[f3ff63c3f] youkaichao 2024-07-25 [doc][distributed] improve multinode serving doc (#6804)
2	2	docs/source/serving/distributed_serving.rst

[cd7edc4e8] Lucas Wilkinson 2024-07-25 [Bugfix] Fix empty (nullptr) channelwise  scales when loading wNa16 using compressed tensors (#6798)
8	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py

[6a1e25b15] Kuntai Du 2024-07-25 [Doc] Add documentations for nightly benchmarks (#6412)
64	16	.buildkite/nightly-benchmarks/README.md
1	1	README.md
6	0	docs/source/index.rst
23	0	docs/source/performance_benchmark/benchmarks.rst

[95db75de6] Tyler Michael Smith 2024-07-25 [Bugfix] Add synchronize to prevent possible data race (#6788)
7	0	vllm/distributed/parallel_state.py

[65b1f121c] Michael Goin 2024-07-25 [Bugfix] Fix `kv_cache_dtype=fp8` without scales for FP8 checkpoints (#6761)
10	2	tests/quantization/test_fp8.py
2	4	vllm/model_executor/layers/quantization/kv_cache.py

[889da130e] Robert Shaw 2024-07-25 [ Misc ] `fp8-marlin` channelwise via `compressed-tensors` (#6524)
11	0	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-FP8W8.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
51	10	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
105	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a16_fp8.py
6	4	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
10	9	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
22	11	vllm/model_executor/layers/quantization/fp8.py
3	11	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[b75e314ff] Alphi 2024-07-26 [Bugfix] Add image placeholder for OpenAI Compatible Server of MiniCPM-V (#6787)
2	0	examples/minicpmv_example.py
3	1	vllm/entrypoints/chat_utils.py

[316a41ac1] Chang Su 2024-07-24 [Bugfix] Fix encoding_format in examples/openai_embedding_client.py (#6755)
8	5	examples/openai_embedding_client.py
0	1	tests/entrypoints/openai/test_embedding.py

[0310029a2] Alexander Matveev 2024-07-25 [Bugfix] Fix awq_marlin and gptq_marlin flags (#6745)
3	2	vllm/model_executor/layers/quantization/awq_marlin.py
2	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[309aaef82] Cody Yu 2024-07-24 [Bugfix] Fix decode tokens w. CUDA graph (#6757)
1	0	tests/worker/test_model_runner.py
10	2	vllm/attention/backends/flash_attn.py
10	1	vllm/attention/backends/flashinfer.py
10	1	vllm/attention/backends/utils.py

[9e169a4c6] Alphi 2024-07-25 [Model] Adding support for MiniCPM-V (#4087)
2	0	docs/source/dev/multimodal/multimodal_index.rst
4	0	docs/source/models/supported_models.rst
53	0	examples/minicpmv_example.py
6	5	tests/conftest.py
163	0	tests/models/test_minicpmv.py
1	0	vllm/model_executor/models/__init__.py
3	1	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/minicpm.py
682	0	vllm/model_executor/models/minicpmv.py
2	1	vllm/multimodal/__init__.py
24	10	vllm/multimodal/base.py

[5689e256b] Evan Z. Liu 2024-07-24 [Frontend] Represent tokens with identifiable strings (#6626)
7	3	tests/entrypoints/openai/test_completion.py
83	0	tests/entrypoints/openai/test_return_tokens_as_ids.py
2	0	vllm/entrypoints/openai/api_server.py
6	0	vllm/entrypoints/openai/cli_args.py
16	7	vllm/entrypoints/openai/serving_chat.py
15	4	vllm/entrypoints/openai/serving_completion.py
9	5	vllm/entrypoints/openai/serving_engine.py

[740374d45] youkaichao 2024-07-24 [core][distributed] fix zmq hang (#6759)
2	2	vllm/connections.py
21	39	vllm/distributed/device_communicators/shm_broadcast.py

[d88c458f4] Hongxia Yang 2024-07-24 [Doc][AMD][ROCm]Added tips to refer to mi300x tuning guide for mi300x users (#6754)
7	0	docs/source/getting_started/amd-installation.rst

[421e218b3] Michael Goin 2024-07-24 [Bugfix] Bump transformers to 4.43.2 (#6752)
1	1	requirements-common.txt
28	27	tests/test_config.py

[5448f6763] Antoni Baum 2024-07-24 [Core] Tweaks to model runner/input builder developer APIs (#6712)
19	16	vllm/attention/backends/flashinfer.py
3	1	vllm/worker/embedding_model_runner.py
87	47	vllm/worker/model_runner.py

[0e63494cf] Antoni Baum 2024-07-24 Add fp8 support to `reshape_and_cache_flash` (#6667)
2	1	csrc/cache.h
45	30	csrc/cache_kernels.cu
2	1	csrc/torch_bindings.cpp
34	8	tests/kernels/test_cache.py
4	1	vllm/_custom_ops.py
2	0	vllm/attention/backends/flash_attn.py
2	0	vllm/attention/backends/flashinfer.py
7	2	vllm/utils.py

[ee812580f] Daniele 2024-07-24 [Frontend] split run_server into build_server and run_server (#6740)
50	27	vllm/entrypoints/openai/api_server.py
2	1	vllm/scripts.py

[40468b13f] Allen.Dou 2024-07-24 [Bugfix] Miscalculated latency lead to time_to_first_token_seconds inaccurate. (#6686)
2	1	vllm/engine/llm_engine.py
1	1	vllm/spec_decode/spec_decode_worker.py

[2cf0df338] Nick Hill 2024-07-24 [Bugfix] Fix speculative decode seeded test (#6743)
2	1	tests/spec_decode/e2e/conftest.py
17	5	tests/spec_decode/e2e/test_seed.py

[545146349] LF Marques 2024-07-24 Adding f-string to validation error which is missing (#6748)
1	1	vllm/entrypoints/chat_utils.py

[f4f8a9d89] liuyhwangyh 2024-07-24 [Bugfix]fix modelscope compatible issue (#6730)
1	1	vllm/model_executor/model_loader/loader.py

[b57081170] Alexei-V-Ivanov-AMD 2024-07-24 [Build/CI] Update run-amd-test.sh. Enable Docker Hub login. (#6711)
1	0	.buildkite/run-amd-test.sh

[ccc4a7325] Woosuk Kwon 2024-07-24 [Docs][ROCm] Detailed instructions to build from source (#6680)
29	0	docs/source/getting_started/amd-installation.rst

[0a740a11b] Roger Wang 2024-07-24 [Bugfix] Fix token padding for chameleon (#6724)
2	1	vllm/model_executor/models/chameleon.py

[c882a7f5b] Nick Hill 2024-07-24 [SpecDecoding] Update MLPSpeculator CI tests to use smaller model (#6714)
3	3	tests/spec_decode/e2e/test_mlp_correctness.py

[5e8ca973e] William Lin 2024-07-23 [Bugfix] fix flashinfer cudagraph capture for PP (#6708)
24	0	tests/distributed/test_pipeline_parallel.py
7	7	vllm/worker/model_runner.py

[87525fab9] dongmao zhang 2024-07-23 [bitsandbytes]: support read bnb pre-quantized model (#5753)
1	0	docs/source/index.rst
43	0	docs/source/quantization/bnb.rst
14	4	tests/quantization/test_bitsandbytes.py
2	0	vllm/config.py
2	2	vllm/engine/arg_utils.py
4	21	vllm/model_executor/layers/quantization/bitsandbytes.py
76	12	vllm/model_executor/model_loader/loader.py
1	0	vllm/model_executor/model_loader/weight_utils.py

[2f808e69a] Thomas Parnell 2024-07-24 [Bugfix] StatLoggers: cache spec decode metrics when they get collected. (#6645)
91	0	tests/metrics/test_metrics.py
31	16	vllm/engine/metrics.py

[01c16ede6] Michael Goin 2024-07-23 [CI] Add smoke test for non-uniform AutoFP8 quantization (#6702)
1	0	tests/quantization/test_fp8.py

[72fc70480] youkaichao 2024-07-23 [build] relax wheel size limit (#6704)
1	1	.buildkite/check-wheel-size.py

[1bedf210e] Roger Wang 2024-07-23 Bump `transformers` version for Llama 3.1 hotfix and patch Chameleon  (#6690)
1	1	requirements-common.txt
27	26	tests/test_config.py
0	2	vllm/model_executor/models/__init__.py
1	2	vllm/model_executor/models/chameleon.py
4	5	vllm/transformers_utils/config.py
0	4	vllm/transformers_utils/configs/__init__.py
0	138	vllm/transformers_utils/configs/chameleon.py

[507ef787d] Travis Johnson 2024-07-23 [Model] Pipeline Parallel Support for DeepSeek v2 (#6519)
1	0	vllm/config.py
114	39	vllm/model_executor/models/deepseek_v2.py

[58f53034a] Yehoshua Cohen 2024-07-23 [Frontend] Add Usage data in each chunk for chat_serving. #6540 (#6652)
32	8	tests/entrypoints/openai/test_chat.py
46	4	vllm/entrypoints/openai/serving_chat.py

[0eb0757be] Michael Goin 2024-07-23 [Misc] Add ignored layers for `fp8` quantization (#6657)
5	9	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
3	36	vllm/model_executor/layers/quantization/fbgemm_fp8.py
11	2	vllm/model_executor/layers/quantization/fp8.py
38	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[38c4b7e86] Simon Mo 2024-07-23 Bump version to 0.5.3.post1 (#6696)
1	1	vllm/version.py

[a112a84aa] Woosuk Kwon 2024-07-23 [BugFix] Fix RoPE error in Llama 3.1 (#6693)
26	27	vllm/config.py
4	3	vllm/model_executor/layers/rotary_embedding.py

[461089a21] Woosuk Kwon 2024-07-23 [Bugfix] Fix a log error in chunked prefill (#6694)
1	1	vllm/config.py

[71950af72] youkaichao 2024-07-23 [doc][distributed] fix doc argument order (#6691)
4	4	docs/source/serving/distributed_serving.rst

[cb1362a88] Woosuk Kwon 2024-07-23 [Docs] Announce llama3.1 support (#6688)
1	0	README.md
2	2	docs/source/models/supported_models.rst

[bb2fc0807] Simon Mo 2024-07-23 Bump version to v0.5.3 (#6674)
1	1	vllm/version.py

[3eda4ec78] Simon Mo 2024-07-22 support ignore patterns in model loader (#6673)
14	1	vllm/config.py
10	0	vllm/engine/arg_utils.py
21	8	vllm/model_executor/model_loader/loader.py
6	1	vllm/model_executor/model_loader/weight_utils.py

[22fa2e35c] Roger Wang 2024-07-22 [VLM][Model] Support image input for Chameleon  (#6633)
4	0	docs/source/models/supported_models.rst
102	0	tests/models/test_chameleon.py
2	1	vllm/entrypoints/chat_utils.py
4	3	vllm/model_executor/models/__init__.py
534	43	vllm/model_executor/models/chameleon.py
3	1	vllm/transformers_utils/configs/__init__.py
47	10	vllm/transformers_utils/configs/chameleon.py

[c5201240a] youkaichao 2024-07-22 [misc] only tqdm for first rank (#6672)
31	6	vllm/model_executor/model_loader/weight_utils.py

[97234be0e] Cyrus Leung 2024-07-23 [Misc] Manage HTTP connections in one place (#6600)
8	0	tests/conftest.py
4	6	tests/entrypoints/openai/test_vision.py
5	5	tests/multimodal/test_utils.py
6	7	vllm/assets/image.py
167	0	vllm/connections.py
22	66	vllm/multimodal/utils.py
3	1	vllm/usage/usage_lib.py

[c051bfe4e] youkaichao 2024-07-22 [doc][distributed] doc for setting up multi-node environment (#6529)
54	11	docs/source/serving/distributed_serving.rst
49	0	examples/run_cluster.sh

[9e0b558a0] Michael Goin 2024-07-23 [Misc] Support FP8 kv cache scales from compressed-tensors (#6528)
7	0	tests/quantization/test_compressed_tensors.py
11	12	vllm/attention/layer.py
58	5	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
17	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
5	58	vllm/model_executor/layers/quantization/fp8.py
78	0	vllm/model_executor/layers/quantization/kv_cache.py
10	0	vllm/model_executor/models/llama.py

[e519ae097] zhaotyer 2024-07-23 add tqdm when loading checkpoint shards (#6569)
6	3	vllm/model_executor/model_loader/weight_utils.py

[7c2749a4f] youkaichao 2024-07-22 [misc] add start loading models for users information (#6670)
1	0	vllm/worker/model_runner.py

[729171ae5] Woosuk Kwon 2024-07-22 [Misc] Enable chunked prefill by default for long context models (#6666)
52	2	vllm/engine/arg_utils.py

[c5e833099] Cheng Li 2024-07-22 [Bugfix] Fix null `modules_to_not_convert`  in FBGEMM Fp8 quantization (#6665)
1	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py

[e0c15758b] Cody Yu 2024-07-22 [Core] Modulize prepare input and attention metadata builder (#6596)
3	17	vllm/attention/backends/abstract.py
22	21	vllm/attention/backends/flash_attn.py
31	29	vllm/attention/backends/flashinfer.py
25	24	vllm/attention/backends/utils.py
5	0	vllm/utils.py
323	207	vllm/worker/model_runner.py

[bdf5fd138] Woosuk Kwon 2024-07-22 [Misc] Remove deprecation warning for beam search (#6659)
0	13	vllm/sampling_params.py

[5a96ee52a] youkaichao 2024-07-22 [ci][build] add back vim in docker (#6661)
1	1	Dockerfile

[42c7f66a3] Jiaxin Shan 2024-07-22 [Core] Support dynamically loading Lora adapter from HuggingFace (#6234)
2	2	tests/core/test_scheduler.py
8	2	tests/lora/conftest.py
1	1	tests/lora/test_long_context.py
39	0	tests/lora/test_lora_huggingface.py
56	1	tests/lora/test_utils.py
2	2	vllm/entrypoints/openai/serving_engine.py
39	3	vllm/lora/request.py
47	0	vllm/lora/utils.py
4	3	vllm/lora/worker_manager.py
2	3	vllm/transformers_utils/tokenizer.py
1	1	vllm/worker/model_runner.py

[69d5ae38d] Kevin H. Luu 2024-07-22 [ci] Use different sccache bucket for CUDA 11.8 wheel build (#6656)
3	1	.buildkite/release-pipeline.yaml
5	1	Dockerfile

[fea59c771] Tyler Michael Smith 2024-07-22 [Bugfix][Kernel] Use int64_t for indices in fp8 quant kernels (#6649)
6	6	csrc/quantization/fp8/common.cu
25	0	tests/kernels/test_fp8_quant.py

[739b61a34] Cyrus Leung 2024-07-23 [Frontend] Refactor prompt processing (#4028)
2	2	benchmarks/benchmark_latency.py
1	1	docs/source/dev/multimodal/multimodal_index.rst
1	1	docs/source/dev/offline_inference/llm_inputs.rst
1	1	docs/source/models/vlm.rst
2	2	tests/engine/output_processor/test_stop_checker.py
4	1	tests/entrypoints/openai/test_serving_chat.py
2	2	vllm/__init__.py
0	7	vllm/engine/arg_utils.py
22	41	vllm/engine/async_llm_engine.py
5	4	vllm/engine/llm_engine.py
13	24	vllm/entrypoints/llm.py
41	0	vllm/entrypoints/logger.py
36	11	vllm/entrypoints/openai/api_server.py
8	0	vllm/entrypoints/openai/cli_args.py
58	50	vllm/entrypoints/openai/protocol.py
19	1	vllm/entrypoints/openai/run_batch.py
72	47	vllm/entrypoints/openai/serving_chat.py
61	54	vllm/entrypoints/openai/serving_completion.py
55	23	vllm/entrypoints/openai/serving_embedding.py
209	62	vllm/entrypoints/openai/serving_engine.py
79	27	vllm/entrypoints/openai/serving_tokenization.py
3	4	vllm/inputs/__init__.py
1	23	vllm/inputs/data.py
3	2	vllm/sequence.py

[89c1c6a19] Jae-Won Chung 2024-07-22 [Bugfix] Fix `vocab_size` field access in `llava_next.py` (#6624)
2	1	vllm/model_executor/models/llava.py
2	1	vllm/model_executor/models/llava_next.py

[42de2cefc] Woosuk Kwon 2024-07-21 [Misc] Add a wrapper for torch.inference_mode (#6618)
7	2	vllm/platforms/__init__.py
21	0	vllm/platforms/interface.py
17	0	vllm/platforms/tpu.py
2	1	vllm/worker/model_runner_base.py
2	1	vllm/worker/worker_base.py

[c9eef37f3] Roger Wang 2024-07-21 [Model] Initial Support for Chameleon (#5770)
3	0	vllm/model_executor/models/__init__.py
554	0	vllm/model_executor/models/chameleon.py
5	4	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
101	0	vllm/transformers_utils/configs/chameleon.py

[396d92d5e] Alexander Matveev 2024-07-21 [Kernel][Core] Add AWQ support to the Marlin kernel  (#6612)
1	0	CMakeLists.txt
8	4	csrc/ops.h
15	18	csrc/quantization/fp8/fp8_marlin.cu
269	0	csrc/quantization/gptq_marlin/awq_marlin_repack.cu
425	100	csrc/quantization/gptq_marlin/gptq_marlin.cu
27	33	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
13	2	csrc/quantization/gptq_marlin/{gptq_marlin.cuh => marlin.cuh}
5	3	csrc/quantization/gptq_marlin/{gptq_marlin_dtypes.cuh => marlin_dtypes.cuh}
24	22	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
4	0	csrc/torch_bindings.cpp
129	12	tests/kernels/test_marlin_gemm.py
2	2	tests/quantization/test_configs.py
15	7	vllm/_custom_ops.py
1	1	vllm/config.py
2	0	vllm/model_executor/layers/quantization/__init__.py
268	0	vllm/model_executor/layers/quantization/awq_marlin.py
10	6	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
21	12	vllm/model_executor/layers/quantization/gptq_marlin.py
173	51	vllm/model_executor/layers/quantization/utils/marlin_utils.py
41	10	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
149	1	vllm/model_executor/layers/quantization/utils/quant_utils.py

[25e778aa1] Isotr0py 2024-07-22 [Model] Refactor and decouple phi3v image embedding (#6621)
118	119	vllm/model_executor/models/phi3v.py

[b6df37f94] Woosuk Kwon 2024-07-21 [Misc] Remove abused noqa (#6619)
6	5	vllm/distributed/device_communicators/shm_broadcast.py

[14f91fe67] sroy745 2024-07-20 [Spec Decode] Disable Log Prob serialization to CPU for spec decoding for both draft and target models. (#6485)
29	19	tests/spec_decode/e2e/test_logprobs.py
1	0	tests/spec_decode/test_dynamic_spec_decode.py
13	7	tests/spec_decode/test_spec_decode_worker.py
17	0	vllm/config.py
14	0	vllm/engine/arg_utils.py
185	33	vllm/spec_decode/spec_decode_worker.py
69	0	vllm/spec_decode/target_model_runner.py
4	4	vllm/spec_decode/util.py

[d7f4178dd] Cyrus Leung 2024-07-21 [Frontend] Move chat utils (#6602)
1	1	tests/async_engine/test_chat_template.py
45	6	vllm/entrypoints/{openai => }/chat_utils.py
2	36	vllm/entrypoints/openai/protocol.py
3	3	vllm/entrypoints/openai/serving_chat.py
3	3	vllm/entrypoints/openai/serving_tokenization.py

[082ecd80d] Robert Shaw 2024-07-20 [ Bugfix ] Fix AutoFP8 fp8 marlin (#6609)
2	1	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[f952bbc8f] Michael Goin 2024-07-20 [Misc] Fix input_scale typing in w8a8_utils.py (#6579)
2	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[9364f74ee] Robert Shaw 2024-07-20 [ Kernel ] Enable `fp8-marlin` for `fbgemm-fp8` models (#6606)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct-FBGEMM-nonuniform.yaml
1	0	.buildkite/lm-eval-harness/configs/models-large.txt
25	1	vllm/model_executor/layers/quantization/fbgemm_fp8.py
7	2	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py

[06d6c5fe9] Matt Wong 2024-07-20 [Bugfix][CI/Build][Hardware][AMD] Fix AMD tests, add HF cache, update CK FA, add partially supported model notes (#6543)
7	0	.buildkite/run-amd-test.sh
2	1	.buildkite/test-pipeline.yaml
2	2	CMakeLists.txt
35	25	Dockerfile.rocm
4	3	docs/source/getting_started/amd-installation.rst
4	0	requirements-rocm.txt
7	2	tests/basic_correctness/test_cpu_offload.py
17	1	tests/models/test_paligemma.py
8	1	tests/models/test_phi3v.py
8	0	vllm/attention/backends/rocm_flash_attn.py
14	3	vllm/model_executor/models/__init__.py
8	1	vllm/spec_decode/draft_model_runner.py

[683e3cb9c] Robert Shaw 2024-07-20 [ Misc ] `fbgemm` checkpoints (#6559)
2	2	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FBGEMM-nonuniform.yaml
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
2	0	vllm/_custom_ops.py
2	1	vllm/attention/layer.py
1	1	vllm/config.py
1	1	vllm/model_executor/layers/fused_moe/layer.py
16	10	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
2	2	vllm/model_executor/layers/quantization/aqlm.py
2	2	vllm/model_executor/layers/quantization/awq.py
3	2	vllm/model_executor/layers/quantization/base_config.py
2	3	vllm/model_executor/layers/quantization/bitsandbytes.py
5	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	3	vllm/model_executor/layers/quantization/deepspeedfp.py
158	0	vllm/model_executor/layers/quantization/fbgemm_fp8.py
2	2	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/layers/quantization/gptq.py
2	3	vllm/model_executor/layers/quantization/gptq_marlin.py
2	3	vllm/model_executor/layers/quantization/gptq_marlin_24.py
2	2	vllm/model_executor/layers/quantization/marlin.py
2	2	vllm/model_executor/layers/quantization/squeezellm.py
2	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py
8	4	vllm/model_executor/layers/vocab_parallel_embedding.py

[9042d6836] Cyrus Leung 2024-07-20 [Misc] Consolidate and optimize logic for building padded tensors (#6541)
3	9	tests/conftest.py
0	3	vllm/attention/backends/flash_attn.py
0	3	vllm/attention/backends/flashinfer.py
0	3	vllm/attention/backends/utils.py
19	37	vllm/model_executor/sampling_metadata.py
51	10	vllm/utils.py
0	3	vllm/worker/cpu_model_runner.py
4	4	vllm/worker/neuron_model_runner.py
0	3	vllm/worker/xpu_model_runner.py

[3f8d42c81] Travis Johnson 2024-07-19 Pipeline Parallel: Guard for KeyErrors at request abort (#6587)
4	1	vllm/engine/async_llm_engine.py
5	1	vllm/engine/output_processor/single_step.py

[7bd82002a] Antoni Baum 2024-07-19 [Core] Allow specifying custom Executor (#6557)
4	0	tests/conftest.py
91	0	tests/engine/test_custom_executor.py
17	4	tests/tokenization/test_tokenizer_group.py
28	11	vllm/config.py
15	3	vllm/engine/arg_utils.py
34	19	vllm/engine/async_llm_engine.py
27	13	vllm/engine/llm_engine.py
2	0	vllm/executor/cpu_executor.py
2	0	vllm/executor/executor_base.py
2	0	vllm/executor/gpu_executor.py
2	0	vllm/executor/multiproc_gpu_executor.py
2	0	vllm/executor/neuron_executor.py
2	0	vllm/executor/openvino_executor.py
21	18	vllm/executor/ray_gpu_executor.py
13	11	vllm/executor/ray_xpu_executor.py
2	0	vllm/executor/tpu_executor.py
2	0	vllm/executor/xpu_executor.py
9	5	vllm/transformers_utils/tokenizer_group/__init__.py
7	0	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
3	1	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
6	0	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
19	7	vllm/worker/worker_base.py

[2e2656425] Varun Sundar Rabindranath 2024-07-19 [ Kernel ] FP8 Dynamic Per Token Quant - Add scale_ub (#6593)
3	3	csrc/ops.h
48	25	csrc/quantization/fp8/common.cu
1	1	csrc/torch_bindings.cpp
23	7	tests/kernels/quant_utils.py
9	2	tests/kernels/test_fp8_quant.py
2	1	vllm/_custom_ops.py

[e81522e87] youkaichao 2024-07-19 [build] add ib in image for out-of-the-box infiniband support (#6599)
1	1	Dockerfile

[45ceb85a0] Murali Andoorveedu 2024-07-19 [Docs] Update PP docs (#6598)
2	3	docs/source/serving/distributed_serving.rst

[4cc24f01b] Robert Shaw 2024-07-19 [ Kernel ] Enable Dynamic Per Token `fp8` (#6547)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
2	1	tests/kernels/test_fp8_quant.py
13	13	vllm/_custom_ops.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
2	1	vllm/model_executor/layers/quantization/fp8.py
37	23	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[07eb6f19f] youkaichao 2024-07-19 [bugfix][distributed] fix multi-node bug for shared memory (#6597)
8	2	vllm/distributed/device_communicators/shm_broadcast.py

[f0bbfaf91] Thomas Parnell 2024-07-19 [Bugfix] [SpecDecode] AsyncMetricsCollector: update time since last collection (#6578)
43	0	tests/spec_decode/test_metrics.py
4	0	vllm/spec_decode/metrics.py

[30efe4153] Simon Mo 2024-07-19 [Docs] Update docs for wheel location (#6580)
2	3	docs/source/getting_started/installation.rst

[9ed82e707] Antoni Baum 2024-07-19 [Misc] Small perf improvements (#6520)
7	4	tests/core/block/test_block_manager_v2.py
4	4	tests/core/block/test_cpu_gpu_block_allocator.py
14	5	vllm/core/block/block_table.py
4	1	vllm/core/block/prefix_caching_block.py
10	4	vllm/model_executor/models/__init__.py
4	3	vllm/sequence.py
3	2	vllm/utils.py

[51f8aa90a] Daniele 2024-07-19 [Bugfix][Frontend] remove duplicate init logger (#6581)
0	1	vllm/entrypoints/openai/api_server.py

[a5314e869] Thomas Parnell 2024-07-19 [Model] RowParallelLinear: pass bias to quant_method.apply  (#6327)
3	0	tests/spec_decode/e2e/test_integration_dist_tp2.py
11	9	vllm/model_executor/layers/linear.py

[a921e8639] Woo-Yeon Lee 2024-07-19 [BUGFIX] Raise an error for no draft token case when draft_tp>1 (#6369)
62	0	tests/spec_decode/e2e/test_integration_dist_tp4.py
3	0	vllm/spec_decode/interfaces.py
19	4	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/spec_decode/top1_proposer.py

[6366efc67] Cyrus Leung 2024-07-19 [Bugfix][Frontend] Fix missing `/metrics` endpoint (#6463)
61	0	tests/entrypoints/openai/test_basic.py
9	5	vllm/entrypoints/openai/api_server.py

[dbe558855] Robert Shaw 2024-07-18 [ Misc ] non-uniform quantization via `compressed-tensors` for `Llama` (#6515)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-nonuniform-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
1	0	vllm/model_executor/layers/fused_moe/layer.py
32	12	vllm/model_executor/layers/linear.py
58	34	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
0	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
118	21	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
23	4	vllm/model_executor/models/gpt2.py
19	6	vllm/model_executor/models/llama.py
22	8	vllm/model_executor/models/mixtral.py
15	4	vllm/model_executor/models/utils.py

[d4201e06d] Thomas Parnell 2024-07-19 [Bugfix] Make spec. decode respect per-request seed. (#6034)
53	3	tests/samplers/test_rejection_sampler.py
44	8	tests/spec_decode/e2e/conftest.py
44	0	tests/spec_decode/e2e/test_seed.py
86	20	vllm/model_executor/layers/rejection_sampler.py
34	11	vllm/model_executor/layers/spec_decode_base_sampler.py
2	2	vllm/model_executor/layers/typical_acceptance_sampler.py
12	1	vllm/spec_decode/batch_expansion.py
18	1	vllm/spec_decode/spec_decode_worker.py

[b5672a112] Nick Hill 2024-07-18 [Core] Multiprocessing Pipeline Parallel support (#6130)
3	3	.buildkite/test-pipeline.yaml
23	9	tests/distributed/test_pipeline_parallel.py
0	4	vllm/config.py
0	21	vllm/executor/executor_base.py
29	15	vllm/executor/gpu_executor.py
75	20	vllm/executor/multiproc_gpu_executor.py
13	25	vllm/executor/ray_gpu_executor.py
7	0	vllm/utils.py
2	2	vllm/worker/worker_base.py

[c5df56f88] Simon Mo 2024-07-18 Add support for a rope extension method (#6553)
12	2	vllm/config.py
36	2	vllm/model_executor/layers/rotary_embedding.py

[1689219eb] Tyler Michael Smith 2024-07-18 [CI/Build] Build on Ubuntu 20.04 instead of 22.04 (#6517)
2	2	.buildkite/test-pipeline.yaml
27	9	Dockerfile
1	1	Dockerfile.openvino
1	1	Dockerfile.xpu

[4ffffccb7] Tyler Michael Smith 2024-07-18 [Kernel] Implement fallback for FP8 channelwise using torch._scaled_mm (#6552)
0	11	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
40	10	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[f53b8f0d0] youkaichao 2024-07-18 [ci][test] add correctness test for cpu offloading (#6549)
1	0	.buildkite/test-pipeline.yaml
8	0	tests/basic_correctness/test_cpu_offload.py
2	85	tests/distributed/test_pipeline_parallel.py
94	0	tests/utils.py

[2d4733ba2] Kevin H. Luu 2024-07-18 Fix PR comment bot (#6554)
1	1	.github/workflows/reminder_comment.yml

[15c6a079b] Michael Goin 2024-07-18 [Model] Support Mistral-Nemo (#6548)
5	1	vllm/model_executor/models/llama.py

[ecdb462c2] Kevin H. Luu 2024-07-18 [ci] Reword Github bot comment  (#6534)
1	1	.github/workflows/reminder_comment.yml

[58ca66322] Robert Shaw 2024-07-18 [ Misc ] Improve Min Capability Checking in `compressed-tensors` (#6522)
14	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
7	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
4	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py

[4634c8728] Woosuk Kwon 2024-07-18 [TPU] Refactor TPU worker & model runner (#6506)
200	97	vllm/worker/tpu_model_runner.py
72	69	vllm/worker/tpu_worker.py

[c8a7d51c4] Noam Gat 2024-07-18 [Bugfix] Update flashinfer.py with PagedAttention forwards - Fixes Gemma2 OpenAI Server Crash (#6501)
3	2	vllm/attention/backends/flashinfer.py

[e2fbaee72] Nick Hill 2024-07-18 [BugFix][Frontend] Use LoRA tokenizer in OpenAI APIs (#6227)
6	27	tests/async_engine/test_chat_template.py
4	9	tests/entrypoints/openai/test_chat.py
49	2	tests/entrypoints/openai/test_completion.py
1	2	tests/entrypoints/openai/test_serving_chat.py
40	16	tests/entrypoints/openai/test_tokenization.py
9	4	vllm/engine/async_llm_engine.py
5	2	vllm/engine/llm_engine.py
2	1	vllm/entrypoints/openai/api_server.py
33	39	vllm/entrypoints/openai/chat_utils.py
44	29	vllm/entrypoints/openai/serving_chat.py
26	21	vllm/entrypoints/openai/serving_completion.py
4	7	vllm/entrypoints/openai/serving_embedding.py
13	17	vllm/entrypoints/openai/serving_engine.py
20	10	vllm/entrypoints/openai/serving_tokenization.py
8	0	vllm/transformers_utils/detokenizer.py
3	0	vllm/transformers_utils/tokenizer.py

[8a74c68bd] Cody Yu 2024-07-17 [Misc] Minor patch for draft model runner (#6523)
6	2	vllm/spec_decode/draft_model_runner.py

[61e592747] Rui Qiao 2024-07-17 [Core] Introduce SPMD worker execution using Ray accelerated DAG (#6032)
3	0	.buildkite/test-pipeline.yaml
5	0	vllm/engine/llm_engine.py
8	0	vllm/envs.py
5	3	vllm/executor/distributed_gpu_executor.py
138	74	vllm/executor/ray_gpu_executor.py
8	6	vllm/executor/ray_utils.py
21	37	vllm/executor/ray_xpu_executor.py
30	1	vllm/worker/worker_base.py

[d25877dd9] Nick Hill 2024-07-17 [BugFix] Avoid secondary error in ShmRingBuffer destructor (#6530)
4	4	vllm/distributed/device_communicators/shm_broadcast.py

[1c27d25fb] youkaichao 2024-07-17 [core][model] yet another cpu offload implementation (#6496)
1	0	.buildkite/test-pipeline.yaml
22	0	examples/cpu_offload.py
2	0	vllm/config.py
23	1	vllm/engine/arg_utils.py
6	0	vllm/entrypoints/llm.py
70	3	vllm/model_executor/models/utils.py
4	0	vllm/worker/model_runner.py

[18fecc355] Robert Shaw 2024-07-17 [ Kernel ] Fp8 Channelwise Weight Support (#6487)
2	1	vllm/config.py
11	7	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
53	27	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
10	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[b5af8c223] Cody Yu 2024-07-17 [Model] Pipeline parallel support for Mixtral (#6516)
11	6	tests/distributed/test_pipeline_parallel.py
1	0	vllm/config.py
48	13	vllm/model_executor/models/mixtral.py

[b5241e41d] Varun Sundar Rabindranath 2024-07-17 [ Kernel ] FP8 Dynamic-Per-Token Quant Kernel (#6511)
7	3	csrc/ops.h
124	20	csrc/quantization/fp8/common.cu
9	1	csrc/torch_bindings.cpp
56	0	tests/kernels/quant_utils.py
54	0	tests/kernels/test_fp8_quant.py
10	16	tests/kernels/test_int8_quant.py
11	0	vllm/_custom_ops.py

[e76466dde] Alexander Matveev 2024-07-17 [Core] draft_model_runner: Implement prepare_inputs on GPU for advance_step (#6338)
1	0	CMakeLists.txt
5	0	csrc/ops.h
131	0	csrc/prepare_inputs/advance_step.cu
19	0	csrc/prepare_inputs/advance_step.cuh
4	0	csrc/torch_bindings.cpp
1	0	tests/spec_decode/e2e/conftest.py
48	0	tests/spec_decode/test_multi_step_worker.py
12	0	vllm/_custom_ops.py
100	47	vllm/model_executor/layers/sampler.py
10	0	vllm/model_executor/sampling_metadata.py
225	80	vllm/spec_decode/draft_model_runner.py
12	3	vllm/spec_decode/multi_step_worker.py

[5f0b9933e] Antoni Baum 2024-07-17 [Bugfix] Fix Ray Metrics API usage (#6354)
54	0	tests/metrics/test_metrics.py
19	0	vllm/engine/async_llm_engine.py
2	0	vllm/engine/llm_engine.py
120	40	vllm/engine/metrics.py

[a38524f33] milo157 2024-07-17 [DOC] - Add docker image to Cerebrium Integration (#6510)
3	0	docs/source/serving/deploying_with_cerebrium.rst

[2fa4623d9] Cody Yu 2024-07-17 [Core] Refactor _prepare_model_input_tensors - take 2 (#6164)
5	1	tests/worker/test_model_input.py
3	1	vllm/attention/__init__.py
43	2	vllm/attention/backends/abstract.py
11	0	vllm/attention/backends/blocksparse_attn.py
181	2	vllm/attention/backends/flash_attn.py
236	2	vllm/attention/backends/flashinfer.py
11	0	vllm/attention/backends/rocm_flash_attn.py
233	1	vllm/attention/backends/utils.py
10	0	vllm/attention/backends/xformers.py
3	2	vllm/attention/selector.py
299	459	vllm/worker/model_runner.py
15	0	vllm/worker/model_runner_base.py

[a9a2e74d2] Woosuk Kwon 2024-07-17 [Misc] Use `torch.Tensor` for type annotation (#6505)
17	17	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
1	1	vllm/worker/worker.py

[e09ce759a] Woosuk Kwon 2024-07-17 [TPU] Remove multi-modal args in TPU backend (#6504)
6	40	vllm/worker/tpu_model_runner.py

[5fa6e9876] Murali Andoorveedu 2024-07-17 [Bugfix] Fix for multinode crash on 4 PP (#6495)
3	5	tests/distributed/test_pipeline_parallel.py
14	0	vllm/executor/ray_gpu_executor.py

[5bf35a91e] Cyrus Leung 2024-07-17 [Doc][CI/Build] Update docs and tests to use `vllm serve` (#6431)
2	5	docs/source/getting_started/quickstart.rst
2	2	docs/source/models/adding_model.rst
2	2	docs/source/models/engine_args.rst
1	2	docs/source/models/lora.rst
1	3	docs/source/models/vlm.rst
1	1	docs/source/serving/deploying_with_dstack.rst
2	4	docs/source/serving/distributed_serving.rst
3	5	docs/source/serving/openai_compatible_server.md
2	3	examples/api_client.py
3	9	examples/logging_configuration.md
1	3	examples/openai_vision_api_client.py
3	3	examples/production_monitoring/Otel.md
1	2	examples/production_monitoring/README.md
11	11	tests/async_engine/test_openapi_server_ray.py
1	5	tests/distributed/test_pipeline_parallel.py
21	21	tests/entrypoints/openai/test_chat.py
30	30	tests/entrypoints/openai/test_completion.py
11	11	tests/entrypoints/openai/test_embedding.py
21	21	tests/entrypoints/openai/test_models.py
12	12	tests/entrypoints/openai/test_tokenization.py
11	11	tests/entrypoints/openai/test_vision.py
2	2	tests/tensorizer_loader/test_tensorizer.py
11	7	tests/utils.py

[a19e8d372] shangmingc 2024-07-17 [Misc][Speculative decoding] Typos and typing fixes (#6467)
1	1	vllm/spec_decode/multi_step_worker.py
2	2	vllm/spec_decode/ngram_worker.py
1	1	vllm/spec_decode/proposer_worker_base.py
1	1	vllm/spec_decode/spec_decode_worker.py
2	2	vllm/spec_decode/top1_proposer.py

[10383887e] Hongxia Yang 2024-07-17 [ROCm] Cleanup Dockerfile and remove outdated patch (#6482)
1	26	Dockerfile.rocm
13	36	docs/source/getting_started/amd-installation.rst
0	15	rocm_patch/rocm_bf16.patch

[1d094fd7c] Wushi Dong 2024-07-16 [Distributed][PP] only create embedding & lm head when necessary (#6455)
38	27	vllm/model_executor/models/llama.py

[ce37be7ba] youkaichao 2024-07-16 [misc][distributed] add seed to dummy weights (#6491)
11	2	vllm/model_executor/model_loader/weight_utils.py

[7f62077af] youkaichao 2024-07-16 [misc][distributed] improve tests (#6488)
5	3	tests/distributed/test_pipeline_parallel.py
3	1	vllm/distributed/device_communicators/shm_broadcast.py

[09c2eb85d] youkaichao 2024-07-16 [ci][distributed] add pipeline parallel correctness test (#6410)
2	7	.buildkite/test-pipeline.yaml
112	121	tests/distributed/test_pipeline_parallel.py
15	0	vllm/executor/multiproc_gpu_executor.py

[978aed530] Michael Goin 2024-07-16 [Kernel][Attention] Separate `Attention.kv_scale` into `k_scale` and `v_scale` (#6081)
5	3	benchmarks/kernels/benchmark_paged_attention.py
27	26	csrc/attention/attention_kernels.cu
2	2	csrc/cache.h
11	10	csrc/cache_kernels.cu
6	6	csrc/cpu/attention.cpp
3	2	csrc/cpu/cache.cpp
5	5	csrc/cpu/torch_bindings.cpp
4	4	csrc/ops.h
5	5	csrc/torch_bindings.cpp
5	3	tests/kernels/test_attention.py
5	3	tests/kernels/test_blocksparse_attention.py
2	2	tests/kernels/test_cache.py
35	5	tests/quantization/test_fp8.py
11	7	vllm/_custom_ops.py
6	3	vllm/_ipex_ops.py
2	1	vllm/attention/backends/abstract.py
6	3	vllm/attention/backends/blocksparse_attn.py
4	2	vllm/attention/backends/flash_attn.py
4	2	vllm/attention/backends/flashinfer.py
9	5	vllm/attention/backends/ipex_attn.py
3	2	vllm/attention/backends/pallas.py
6	3	vllm/attention/backends/rocm_flash_attn.py
7	4	vllm/attention/backends/torch_sdpa.py
5	3	vllm/attention/backends/xformers.py
8	6	vllm/attention/layer.py
4	2	vllm/attention/ops/ipex_attn.py
10	5	vllm/attention/ops/paged_attn.py
9	0	vllm/model_executor/layers/linear.py
38	13	vllm/model_executor/layers/quantization/fp8.py
53	5	vllm/model_executor/model_loader/weight_utils.py
5	14	vllm/model_executor/models/llama.py
6	15	vllm/model_executor/models/mixtral.py
6	14	vllm/model_executor/models/qwen2.py

[160e1d8c9] Cody Yu 2024-07-16 [Misc] Log spec decode metrics (#6454)
49	0	tests/metrics/test_metrics.py
36	8	tests/spec_decode/e2e/conftest.py
12	6	tests/spec_decode/e2e/test_multistep_correctness.py
40	0	vllm/engine/metrics.py

[94162beb9] Jiaxin Shan 2024-07-16 [Doc] Fix the lora adapter path in server startup script (#6230)
4	1	docs/source/models/lora.rst

[c467dff24] Woosuk Kwon 2024-07-16 [Hardware][TPU] Support MoE with Pallas GMM kernel  (#6457)
3	1	Dockerfile.tpu
2	2	docs/source/getting_started/tpu-installation.rst
18	0	vllm/model_executor/layers/fused_moe/layer.py
62	0	vllm/model_executor/layers/fused_moe/moe_pallas.py
4	5	vllm/worker/tpu_model_runner.py

[9f4ccec76] youkaichao 2024-07-16 [doc][misc] remind to cancel debugging environment variables (#6481)
4	3	docs/source/getting_started/debugging.rst

[38ef94888] Cyrus Leung 2024-07-16 [CI/Build] Remove "boardwalk" image asset (#6460)
1	6	tests/conftest.py
4	3	tests/models/test_fuyu.py
0	2	tests/models/test_llava.py
0	2	tests/models/test_llava_next.py
4	3	tests/models/test_paligemma.py
0	2	tests/models/test_phi3v.py
3	10	vllm/assets/image.py

[2bb0489cb] Peng Guanwen 2024-07-16 [Core] Use numpy to speed up padded token processing (#6442)
18	20	vllm/model_executor/sampling_metadata.py

[7508a3dc3] Thomas Parnell 2024-07-16 [Misc] Fix typos in spec. decode metrics logging. (#6470)
3	3	vllm/engine/metrics.py

[7a3d2a5b9] sasha0552 2024-07-16 [Frontend] Support for chat completions input in the tokenize endpoint (#5923)
5	9	tests/async_engine/test_chat_template.py
0	49	tests/entrypoints/openai/test_completion.py
128	0	tests/entrypoints/openai/test_tokenization.py
8	2	vllm/entrypoints/openai/api_server.py
156	0	vllm/entrypoints/openai/chat_utils.py
5	3	vllm/entrypoints/openai/protocol.py
10	151	vllm/entrypoints/openai/serving_chat.py
1	30	vllm/entrypoints/openai/serving_completion.py
73	0	vllm/entrypoints/openai/serving_tokenization.py

[d97011512] Cyrus Leung 2024-07-16 [CI/Build] vLLM cache directory for images (#6444)
0	14	.buildkite/download-images.sh
0	4	.buildkite/test-pipeline.yaml
3	30	examples/llava_example.py
3	30	examples/paligemma_example.py
2	24	examples/phi3v_example.py
2	23	tests/conftest.py
0	0	vllm/assets/__init__.py
11	0	vllm/assets/base.py
47	0	vllm/assets/image.py
4	4	vllm/distributed/device_communicators/custom_all_reduce_utils.py
48	6	vllm/envs.py
2	3	vllm/usage/usage_lib.py
1	2	vllm/worker/tpu_worker.py

[37d776606] Woosuk Kwon 2024-07-15 [Docs] Announce 5th meetup (#6458)
9	0	README.md

[d92b3c5cd] Joe 2024-07-15 [Bugfix][CI/Build] Test prompt adapters in openai entrypoint tests (#6419)
51	29	tests/entrypoints/openai/test_completion.py
3	2	vllm/entrypoints/openai/serving_engine.py

[9ad32dacd] Mor Zusman 2024-07-16 [BugFix][Model] Jamba - Handle aborted requests, Add tests and fix cleanup bug (#6425)
83	0	tests/models/test_jamba.py
1	0	vllm/core/scheduler.py
20	12	vllm/model_executor/model_loader/loader.py
47	1	vllm/model_executor/models/interfaces.py
25	11	vllm/model_executor/models/jamba.py

[d6f3b3d5c] Kevin H. Luu 2024-07-15 Pin sphinx-argparse version (#6453)
1	1	docs/requirements-docs.txt

[4552e37b5] Woosuk Kwon 2024-07-15 [CI/Build][TPU] Add TPU CI test (#6277)
16	0	.buildkite/run-tpu-test.sh
0	5	Dockerfile.tpu
28	0	examples/offline_inference_tpu.py

[ec9933f4a] Woosuk Kwon 2024-07-15 [Misc] Add CustomOp Interface to UnquantizedFusedMoEMethod (#6289)
35	13	vllm/model_executor/layers/fused_moe/layer.py
0	4	vllm/model_executor/model_loader/loader.py

[3dee97b05] Woosuk Kwon 2024-07-15 [Docs] Add Google Cloud to sponsor list (#6450)
1	0	README.md
1	0	docs/source/community/sponsors.md

[4cf256ae7] youkaichao 2024-07-15 [misc][distributed] fix pp missing layer condition (#6446)
4	1	vllm/model_executor/models/utils.py

[64fdc08c7] Simon Mo 2024-07-15 bump version to v0.5.2 (#6433)
1	1	vllm/version.py

[4ef95b0f0] Thomas Parnell 2024-07-15 [Bugfix] use float32 precision in samplers/test_logprobs.py for comparing with HF  (#6409)
2	1	tests/samplers/test_logprobs.py
6	0	vllm/attention/ops/prefix_prefill.py

[eaec4b915] Thomas Parnell 2024-07-15 [Bugfix] Add custom Triton cache manager to resolve MoE MP issue  (#6140)
5	0	vllm/executor/multiproc_gpu_executor.py
6	0	vllm/triton_utils/__init__.py
53	0	vllm/triton_utils/custom_cache_manager.py

[a63a4c634] Pernekhan Utemuratov 2024-07-15 [Misc] Use 0.0.9 version for flashinfer (#6447)
1	1	Dockerfile

[c8fd97f26] Tyler Michael Smith 2024-07-15 [Kernel] Use CUTLASS kernels for the FP8 layers with Bias (#6270)
3	2	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[94b82e8c1] youkaichao 2024-07-15 [doc][distributed] add suggestion for distributed inference (#6418)
16	0	docs/source/serving/distributed_serving.rst

[6ae1597dd] Roger Wang 2024-07-15 [VLM] Minor space optimization for `ClipVisionModel` (#6436)
25	21	vllm/model_executor/models/clip.py
12	4	vllm/model_executor/models/llava.py
12	4	vllm/model_executor/models/llava_next.py
17	10	vllm/model_executor/models/phi3v.py

[22e79ee8f] youkaichao 2024-07-14 [doc][misc] doc update (#6439)
4	0	docs/source/getting_started/debugging.rst

[de1991631] Cyrus Leung 2024-07-15 [Bugfix] Convert image to RGB by default (#6430)
30	10	vllm/multimodal/utils.py

[69672f116] youkaichao 2024-07-14 [core][distributed] simplify code to support pipeline parallel (#6406)
1	3	.buildkite/test-pipeline.yaml
8	3	tests/basic_correctness/test_basic_correctness.py
20	27	vllm/model_executor/models/gpt2.py
22	28	vllm/model_executor/models/llama.py
56	0	vllm/model_executor/models/utils.py

[44874a0bf] DefTruth 2024-07-15 [Doc] add env docs for flashinfer backend (#6437)
1	0	vllm/envs.py

[b47008b4d] zifeitong 2024-07-14 [BugFix] BatchResponseData body should be optional (#6345)
2	1	tests/entrypoints/openai/test_run_batch.py
1	1	vllm/entrypoints/openai/protocol.py

[9bfece89f] Simon Mo 2024-07-14 Add FUNDING.yml (#6435)
2	0	.github/FUNDING.yml

[32c9d7f76] Simon Mo 2024-07-14 Report usage for beam search (#6404)
5	0	vllm/sampling_params.py
13	2	vllm/usage/usage_lib.py

[ccb20db8b] Fish 2024-07-15 [Bugfix] Benchmark serving script used global parameter 'args' in function 'sample_random_requests' (#6428)
1	1	benchmarks/benchmark_serving.py

[a754dc2cb] Robert Shaw 2024-07-14 [CI/Build] Cross python wheel (#6394)
2	36	.buildkite/release-pipeline.yaml
2	2	Dockerfile

[61e85dbad] Robert Cohn 2024-07-14 [Doc] xpu backend requires running setvars.sh (#6393)
2	1	docs/source/getting_started/xpu-installation.rst

[dbfe254ed] Ethan Xu 2024-07-14 [Feature] vLLM CLI (#5090)
2	2	benchmarks/benchmark_serving.py
1	1	docs/source/serving/openai_compatible_server.md
5	0	setup.py
4	2	tests/utils.py
50	28	vllm/entrypoints/openai/api_server.py
7	3	vllm/entrypoints/openai/cli_args.py
154	0	vllm/scripts.py

[73030b7da] Robert Shaw 2024-07-14 [ Misc ] Enable Quantizing All Layers of DeekSeekv2 (#6423)
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
5	0	vllm/model_executor/model_loader/weight_utils.py

[ccd3c0457] youkaichao 2024-07-14 [ci][build] fix commit id (#6420)
3	1	.buildkite/test-pipeline.yaml
3	0	Dockerfile
7	2	setup.py

[9dad5cc85] Tyler Michael Smith 2024-07-14 [Kernel] Turn off CUTLASS scaled_mm for Ada Lovelace (#6384)
4	4	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8-compressed-tensors.yaml
3	3	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8.yaml
8	2	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu

[6ef3bf912] Yuan Tang 2024-07-14 Remove unnecessary trailing period in spec_decode.rst (#6405)
1	1	docs/source/models/spec_decode.rst

[540c0368b] Isotr0py 2024-07-14 [Model] Initialize Fuyu-8B support (#3924)
8	0	docs/source/models/supported_models.rst
31	0	examples/fuyu_example.py
142	0	tests/models/test_fuyu.py
2	0	vllm/model_executor/models/__init__.py
328	0	vllm/model_executor/models/fuyu.py
333	0	vllm/model_executor/models/persimmon.py

[fb6af8bc0] Robert Shaw 2024-07-13 [ Misc ] Apply MoE Refactor to Deepseekv2 To Support Fp8 (#6417)
11	0	.buildkite/lm-eval-harness/configs/DeepSeek-V2-Lite-Chat.yaml
1	0	.buildkite/lm-eval-harness/configs/models-large.txt
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
26	10	vllm/model_executor/layers/fused_moe/fused_moe.py
78	15	vllm/model_executor/layers/fused_moe/layer.py
8	2	vllm/model_executor/layers/quantization/fp8.py
69	73	vllm/model_executor/models/deepseek_v2.py
7	25	vllm/model_executor/models/mixtral.py
22	11	vllm/model_executor/models/qwen2_moe.py

[eeceadaec] Woosuk Kwon 2024-07-13 [Misc] Add deprecation warning for beam search (#6402)
5	0	vllm/envs.py
12	0	vllm/sampling_params.py

[babf52dad] Robert Shaw 2024-07-13 [ Misc ] More Cleanup of Marlin (#6359)
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
31	47	vllm/model_executor/layers/quantization/gptq_marlin.py
12	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[9da4aad44] Noam Gat 2024-07-13 Updating LM Format Enforcer version to v10.3 (#6411)
1	1	requirements-common.txt

[41708e503] youkaichao 2024-07-12 [ci] try to add multi-node tests (#6280)
40	12	.buildkite/run-multi-node-test.sh
15	1	.buildkite/test-pipeline.yaml
14	23	tests/async_engine/test_openapi_server_ray.py
4	13	tests/distributed/test_pipeline_parallel.py
1	0	tests/distributed/test_same_node.py
24	33	tests/entrypoints/openai/test_chat.py
24	33	tests/entrypoints/openai/test_completion.py
14	21	tests/entrypoints/openai/test_embedding.py
24	33	tests/entrypoints/openai/test_models.py
13	20	tests/entrypoints/openai/test_vision.py
14	19	tests/tensorizer_loader/test_tensorizer.py
36	57	tests/utils.py
7	10	vllm/executor/ray_gpu_executor.py

[d80aef377] Woosuk Kwon 2024-07-12 [Docs] Clean up latest news (#6401)
2	17	README.md

[e1684a766] Thomas Parnell 2024-07-13 [Bugfix] Fix hard-coded value of x in context_attention_fwd (#6373)
2	2	vllm/attention/ops/prefix_prefill.py

[a27f87da3] Saliya Ekanayake 2024-07-12 [Doc] Fix Typo in Doc (#6392)
1	1	README.md
1	1	docs/source/index.rst

[16ff6bd58] Kevin H. Luu 2024-07-12 [ci] Fix wording for GH bot (#6398)
2	2	.github/workflows/reminder_comment.yml

[f8f9ff57e] Woosuk Kwon 2024-07-12 [Bugfix][TPU] Fix megacore setting for v5e-litepod (#6397)
1	1	vllm/attention/backends/pallas.py

[6bc9710f6] Simon Mo 2024-07-12 Fix release pipeline's dir permission (#6391)
6	6	.buildkite/release-pipeline.yaml

[111fc6e7e] Michael Goin 2024-07-12 [Misc] Add generated git commit hash as `vllm.__commit__` (#6386)
3	0	.gitignore
24	0	setup.py
7	0	tests/test_embedded_commit.py
2	1	vllm/__init__.py
11	0	vllm/version.py

[75f64d8b9] Cody Yu 2024-07-12 [Bugfix] Fix illegal memory access in FP8 MoE kernel (#6382)
5	3	vllm/model_executor/layers/fused_moe/fused_moe.py

[21b2dceda] Simon Mo 2024-07-12 Fix release pipeline's -e flag (#6390)
1	1	.buildkite/release-pipeline.yaml

[07b35af86] Simon Mo 2024-07-12 Fix interpolation in release pipeline (#6389)
2	2	.buildkite/release-pipeline.yaml

[bb1a784b0] Simon Mo 2024-07-12 Fix release-pipeline.yaml (#6388)
1	1	.buildkite/release-pipeline.yaml

[d719ba24c] Simon Mo 2024-07-12 Build some nightly wheels by default (#6380)
32	2	.buildkite/release-pipeline.yaml
14	0	docs/source/getting_started/installation.rst

[aa48e502f] Cody Yu 2024-07-12 [MISC] Upgrade dependency to PyTorch 2.3.1 (#5327)
1	1	.github/workflows/publish.yml
1	1	CMakeLists.txt
1	1	pyproject.toml
1	1	requirements-build.txt
4	4	requirements-cuda.txt

[4dbebd03c] Kevin H. Luu 2024-07-12 [ci] Add GHA workflows to enable full CI run (#6381)
21	0	.github/workflows/add_label_automerge.yml
23	0	.github/workflows/add_label_ready_comment.yml
21	0	.github/workflows/reminder_comment.yml

[b75bce100] Kevin H. Luu 2024-07-12 [ci] Add grouped tests & mark tests to run by default for fastcheck pipeline (#6365)
30	0	.buildkite/test-pipeline.yaml

[b039cbbce] Yihuan Bu 2024-07-12 [Misc] add fixture to guided processor tests (#6341)
69	0	tests/entrypoints/openai/conftest.py
39	80	tests/entrypoints/openai/test_chat.py
23	74	tests/entrypoints/openai/test_completion.py
13	52	tests/entrypoints/openai/test_guided_processors.py

[f9d25c251] Alexei-V-Ivanov-AMD 2024-07-12 [Build/CI] Checking/Waiting for the GPU's clean state (#6379)
9	0	.buildkite/run-amd-test.sh

[024ad87cd] Cyrus Leung 2024-07-12 [Bugfix] Fix dtype mismatch in PaliGemma (#6367)
1	1	tests/models/test_paligemma.py
1	0	vllm/model_executor/models/gemma.py
10	4	vllm/model_executor/models/paligemma.py

[aea19f098] Robert Shaw 2024-07-12 [ Misc ] Support Models With Bias in `compressed-tensors` integration (#6356)
11	0	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-INT8-compressed-tensors.yaml
11	0	.buildkite/lm-eval-harness/configs/Qwen2-1.5B-Instruct-W8A16-compressed-tensors.yaml
1	0	.buildkite/lm-eval-harness/configs/models-small.txt
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
3	0	tests/models/test_compressed_tensors.py
1	4	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
5	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
5	4	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
7	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
6	3	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
5	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
2	4	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[f7160d946] Roger Wang 2024-07-12 [Misc][Bugfix] Update transformers for tokenizer issue (#6364)
1	1	requirements-common.txt

[6047187cd] Robert Shaw 2024-07-12 [ Misc ] Remove separate bias add (#6353)
3	15	vllm/model_executor/layers/linear.py

[b6c16cf8f] Hongxia Yang 2024-07-12 [ROCm][AMD] unify CUDA_VISIBLE_DEVICES usage in cuda/rocm (#6352)
7	7	Dockerfile.rocm
1	6	tests/distributed/test_utils.py
1	8	vllm/config.py
0	4	vllm/utils.py
1	9	vllm/worker/worker_base.py

[d26a8b3f1] adityagoel14 2024-07-12 [CI/Build] (2/2) Switching AMD CI to store images in Docker Hub (#6350)
4	9	.buildkite/run-amd-test.sh

[d59eb9848] Michael Goin 2024-07-11 [Model][Phi3-Small] Remove scipy from blocksparse_attention (#6343)
27	8	vllm/attention/ops/blocksparse_attention/utils.py

[adf32e0a0] Helena Kloosterman 2024-07-12 [Bugfix] Fix usage stats logging exception warning with OpenVINO (#6349)
1	1	requirements-openvino.txt
1	1	vllm/engine/llm_engine.py

[2b0fb5348] youkaichao 2024-07-11 [distributed][misc] be consistent with pytorch for libcudart.so (#6346)
24	3	vllm/distributed/device_communicators/cuda_wrapper.py

[d6ab52899] Lily Liu 2024-07-11 [Misc] Remove flashinfer warning, add flashinfer tests to CI (#6351)
5	3	.buildkite/test-pipeline.yaml
5	0	tests/basic_correctness/test_basic_correctness.py
0	3	vllm/attention/selector.py

[7ed6a4f0e] Robert Shaw 2024-07-11 [ BugFix ] Prompt Logprobs Detokenization (#6223)
4	1	.buildkite/test-pipeline.yaml
87	22	tests/tokenization/test_detokenize.py
14	5	vllm/engine/output_processor/single_step.py
12	4	vllm/transformers_utils/detokenizer.py

[a4feba929] Kuntai Du 2024-07-11 [CI/Build] Add nightly benchmarking for tgi, tensorrt-llm and lmdeploy (#5362)
1	0	.buildkite/nightly-benchmarks/README.md
0	27	.buildkite/nightly-benchmarks/kickoff-pipeline.sh
45	0	.buildkite/nightly-benchmarks/nightly-descriptions.md
120	0	.buildkite/nightly-benchmarks/nightly-pipeline.yaml
76	0	.buildkite/nightly-benchmarks/run-nightly-suite.sh
26	0	.buildkite/nightly-benchmarks/scripts/download-tokenizer.py
6	0	.buildkite/nightly-benchmarks/scripts/get-lmdeploy-modelname.py
102	0	.buildkite/nightly-benchmarks/scripts/launch-trt-server.sh
40	0	.buildkite/nightly-benchmarks/scripts/nightly-annotate.sh
135	0	.buildkite/nightly-benchmarks/scripts/plot-nightly-results.py
218	0	.buildkite/nightly-benchmarks/scripts/run-lmdeploy-nightly.sh
216	0	.buildkite/nightly-benchmarks/scripts/run-tgi-nightly.sh
214	0	.buildkite/nightly-benchmarks/scripts/run-trt-nightly.sh
221	0	.buildkite/nightly-benchmarks/scripts/run-vllm-nightly.sh
76	0	.buildkite/nightly-benchmarks/scripts/summary-nightly-results.py
116	0	.buildkite/nightly-benchmarks/tests/nightly-tests.json
2	0	README.md
9	0	benchmarks/benchmark_serving.py

[2d23b42d9] youkaichao 2024-07-11 [doc] update pipeline parallel in readme (#6347)
1	1	README.md
1	1	docs/source/index.rst

[1df43de9b] xwjiang2010 2024-07-11 [bug fix] Fix llava next feature size calculation. (#6339)
13	1	tests/models/test_llava_next.py
10	8	vllm/model_executor/models/llava_next.py

[52b7fcb35] Simon Mo 2024-07-11 Benchmark: add H100 suite (#6047)
17	18	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
23	5	.buildkite/nightly-benchmarks/run-benchmarks-suite.sh

[b675069d7] Robert Shaw 2024-07-11 [ Misc ] Refactor Marlin Python Utilities (#6082)
6	4	benchmarks/kernels/benchmark_marlin.py
25	21	tests/kernels/test_marlin_gemm.py
14	9	tests/quantization/test_compressed_tensors.py
69	82	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
1	1	vllm/model_executor/layers/quantization/fp8.py
67	196	vllm/model_executor/layers/quantization/gptq_marlin.py
0	60	vllm/model_executor/layers/quantization/utils/marlin_24_perms.py
0	60	vllm/model_executor/layers/quantization/utils/marlin_perms.py
133	306	vllm/model_executor/layers/quantization/utils/marlin_utils.py
109	0	vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py
120	0	vllm/model_executor/layers/quantization/utils/marlin_utils_test.py
160	3	vllm/model_executor/layers/quantization/utils/{format_24.py => marlin_utils_test_24.py}

[55f692b46] Mor Zusman 2024-07-11 [BugFix] get_and_reset only when scheduler outputs are not empty (#6266)
2	2	vllm/engine/async_llm_engine.py
2	2	vllm/engine/llm_engine.py

[8a1415cf7] Thomas Parnell 2024-07-11 [Bugfix] GPTBigCodeForCausalLM: Remove lm_head from supported_lora_modules. (#6326)
1	1	vllm/model_executor/models/gpt_bigcode.py

[546b101fa] pushan 2024-07-11 [BugFix]: fix engine timeout due to request abort (#6255)
3	1	vllm/engine/async_llm_engine.py

[3963a5335] aniaan 2024-07-11 [Misc] refactor(config): clean up unused code (#6320)
2	4	vllm/config.py
0	3	vllm/worker/xpu_model_runner.py

[c4774eb84] Roger Wang 2024-07-11 [Bugfix] Fix snapshot download in serving benchmark (#6318)
9	9	benchmarks/backend_request_func.py

[fc17110bb] Lim Xiang Yang 2024-07-11 [BugFix]: set outlines pkg version (#6262)
1	1	requirements-common.txt

[439c84581] Jie Fu (傅杰) 2024-07-11 [Doc] Update description of vLLM support for CPUs (#6003)
1	1	README.md
1	1	docs/source/getting_started/cpu-installation.rst

[99ded1e1c] daquexian 2024-07-11 [Doc] Remove comments incorrectly copied from another project (#6286)
0	1	vllm/model_executor/layers/linear.py

[997df46a3] Woosuk Kwon 2024-07-10 [Bugfix][Neuron] Fix soft prompt method error in NeuronExecutor (#6313)
16	0	vllm/executor/neuron_executor.py

[ae151d73b] sroy745 2024-07-10 [Speculative Decoding] Enabling bonus token in speculative decoding for KV cache based models (#5765)
7	4	tests/spec_decode/test_dynamic_spec_decode.py
207	5	tests/spec_decode/test_multi_step_worker.py
6	3	tests/spec_decode/test_ngram_worker.py
143	4	tests/spec_decode/test_spec_decode_worker.py
17	1	vllm/sequence.py
4	1	vllm/spec_decode/interfaces.py
6	2	vllm/spec_decode/medusa_worker.py
4	1	vllm/spec_decode/mlp_speculator_worker.py
170	36	vllm/spec_decode/multi_step_worker.py
9	3	vllm/spec_decode/ngram_worker.py
8	1	vllm/spec_decode/proposer_worker_base.py
8	3	vllm/spec_decode/smaller_tp_proposer_worker.py
52	15	vllm/spec_decode/spec_decode_worker.py
4	1	vllm/spec_decode/top1_proposer.py

[44cc76610] sangjune.park 2024-07-11 [Bugfix] Fix OpenVINOExecutor abstractmethod error (#6296)
16	0	vllm/executor/openvino_executor.py

[b422d4961] Benjamin Muskalla 2024-07-10 [CI/Build] Enable mypy typing for remaining folders (#6268)
10	8	.github/workflows/mypy.yaml
9	9	format.sh
2	3	vllm/platforms/cuda.py

[c38eba304] Thomas Parnell 2024-07-10 [Bugfix] MLPSpeculator: Use ParallelLMHead in tie_weights=False case. (#6303)
1	1	vllm/model_executor/models/mlp_speculator.py

[e72ae80b0] Woosuk Kwon 2024-07-10 [Bugfix] Support 2D input shape in MoE layer (#6287)
3	2	vllm/model_executor/models/mixtral.py
4	2	vllm/model_executor/models/qwen2_moe.py

[8a924d224] Cyrus Leung 2024-07-10 [Doc] Guide for adding multi-modal plugins (#6205)
1	0	docs/source/_templates/sections/header.html
17	0	docs/source/dev/multimodal/adding_multimodal_plugin.rst
16	8	docs/source/dev/multimodal/multimodal_index.rst
3	2	vllm/multimodal/__init__.py
13	8	vllm/multimodal/base.py
1	0	vllm/multimodal/image.py
13	5	vllm/multimodal/registry.py

[5ed3505d8] Woosuk Kwon 2024-07-09 [Bugfix][TPU] Add prompt adapter methods to TPUExecutor (#6279)
25	6	vllm/executor/tpu_executor.py

[da78caecf] youkaichao 2024-07-09 [core][distributed] zmq fallback for broadcasting large objects (#6183)
1	0	requirements-common.txt
3	2	tests/distributed/test_same_node.py
3	14	tests/distributed/test_shm_broadcast.py
2	2	vllm/distributed/device_communicators/custom_all_reduce.py
232	37	vllm/distributed/device_communicators/shm_broadcast.py
33	25	vllm/distributed/parallel_state.py

[2416b26e1] Abhinav Goyal 2024-07-10 [Speculative Decoding] Medusa Implementation with Top-1 proposer (#4978)
226	0	tests/spec_decode/e2e/test_medusa_correctness.py
1	0	vllm/model_executor/models/__init__.py
159	0	vllm/model_executor/models/medusa.py
127	0	vllm/spec_decode/medusa_worker.py
5	0	vllm/spec_decode/spec_decode_worker.py
4	2	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
60	0	vllm/transformers_utils/configs/medusa.py
3	2	vllm/worker/worker.py

[d3a245138] Baoyuan Qi 2024-07-10 [Bugfix]fix and needs_scalar_to_array logic check (#6238)
2	2	vllm/model_executor/layers/linear.py

[673dd4cae] Murali Andoorveedu 2024-07-09 [Docs] Docs update for Pipeline Parallel (#6222)
15	2	docs/source/serving/distributed_serving.rst

[4d6ada947] Swapnil Parekh 2024-07-09 [CORE] Adding support for insertion of soft-tuned prompts (#4645)
1	0	format.sh
4	5	tests/lora/test_long_context.py
164	162	tests/lora/test_lora_manager.py
45	0	tests/prompt_adapter/test_bloom.py
53	0	tests/prompt_adapter/test_multi_adapter_inference.py
61	0	tests/prompt_adapter/test_pa_lora.py
2	0	tests/spec_decode/e2e/conftest.py
1	0	tests/worker/test_model_runner.py
0	0	vllm/adapter_commons/__init__.py
14	0	vllm/adapter_commons/layers.py
104	0	vllm/adapter_commons/models.py
25	0	vllm/adapter_commons/request.py
90	0	vllm/adapter_commons/utils.py
36	0	vllm/adapter_commons/worker_manager.py
37	0	vllm/config.py
12	0	vllm/core/scheduler.py
22	2	vllm/engine/arg_utils.py
29	9	vllm/engine/async_llm_engine.py
50	15	vllm/engine/llm_engine.py
23	10	vllm/entrypoints/llm.py
3	2	vllm/entrypoints/openai/api_server.py
20	1	vllm/entrypoints/openai/cli_args.py
1	1	vllm/entrypoints/openai/serving_chat.py
13	4	vllm/entrypoints/openai/serving_completion.py
51	10	vllm/entrypoints/openai/serving_engine.py
15	0	vllm/executor/cpu_executor.py
25	2	vllm/executor/executor_base.py
21	0	vllm/executor/gpu_executor.py
4	1	vllm/executor/ray_xpu_executor.py
4	1	vllm/executor/xpu_executor.py
3	9	vllm/lora/layers.py
80	95	vllm/lora/models.py
14	11	vllm/lora/request.py
70	145	vllm/lora/worker_manager.py
0	0	vllm/prompt_adapter/__init__.py
80	0	vllm/prompt_adapter/layers.py
355	0	vllm/prompt_adapter/models.py
30	0	vllm/prompt_adapter/request.py
176	0	vllm/prompt_adapter/worker_manager.py
42	6	vllm/sequence.py
10	1	vllm/spec_decode/draft_model_runner.py
3	1	vllm/worker/cpu_model_runner.py
4	1	vllm/worker/cpu_worker.py
10	1	vllm/worker/embedding_model_runner.py
118	20	vllm/worker/model_runner.py
19	1	vllm/worker/worker.py
3	1	vllm/worker/xpu_model_runner.py
4	1	vllm/worker/xpu_worker.py

[a0550cbc8] Kevin H. Luu 2024-07-09 Add support for multi-node on CI (#5955)
77	0	.buildkite/run-multi-node-test.sh

[08c5bdeca] Woosuk Kwon 2024-07-09 [Bugfix][TPU] Fix outlines installation in TPU Dockerfile (#6256)
7	3	Dockerfile.tpu

[5d5b4c5fe] Woosuk Kwon 2024-07-09 [Bugfix][TPU] Add missing None to model input (#6245)
1	1	vllm/worker/tpu_model_runner.py

[70c232f85] youkaichao 2024-07-08 [core][distributed] fix ray worker rank assignment (#6235)
28	6	vllm/executor/ray_gpu_executor.py

[a3c9435d9] youkaichao 2024-07-08 [hardware][cuda] use device id under CUDA_VISIBLE_DEVICES for get_device_capability (#6216)
20	4	vllm/platforms/cuda.py

[4f0e0ea13] Simon Mo 2024-07-08 Add FlashInfer to default Dockerfile (#6172)
3	0	Dockerfile

[ddc369fba] tomeras91 2024-07-08 [Bugfix] Mamba cache Cuda Graph padding (#6214)
28	0	tests/models/test_jamba.py
2	2	vllm/model_executor/models/jamba.py

[185ad31f3] Eric 2024-07-09 [Bugfix] use diskcache in outlines _get_guide  #5436  (#6203)
3	2	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[543aa4857] afeldman-nm 2024-07-08 [Kernel] Correctly invoke prefill & decode kernels for cross-attention (towards eventual encoder/decoder model support) (#4888)
7	7	tests/kernels/test_attention_selector.py
953	0	tests/kernels/test_encoder_decoder_attn.py
920	0	tests/kernels/utils.py
8	0	vllm/attention/backends/abstract.py
8	1	vllm/attention/backends/blocksparse_attn.py
8	1	vllm/attention/backends/flash_attn.py
7	1	vllm/attention/backends/flashinfer.py
7	1	vllm/attention/backends/ipex_attn.py
7	1	vllm/attention/backends/pallas.py
8	1	vllm/attention/backends/rocm_flash_attn.py
7	1	vllm/attention/backends/torch_sdpa.py
7	0	vllm/attention/backends/utils.py
394	78	vllm/attention/backends/xformers.py
10	3	vllm/attention/layer.py

[f7a8fa39d] Avshalom Manevich 2024-07-08 [Kernel] reloading fused_moe config on the last chunk (#6210)
36	15	vllm/model_executor/layers/fused_moe/fused_moe.py

[717f4bcea] Haichuan 2024-07-08 Feature/add benchmark testing (#5947)
3	3	benchmarks/benchmark_serving.py

[16620f439] kczimm 2024-07-07 do not exclude `object` field in CompletionStreamResponse (#6196)
2	2	vllm/entrypoints/openai/serving_completion.py

[3b08fe2b1] youkaichao 2024-07-07 [misc][frontend] log all available endpoints (#6195)
11	0	vllm/entrypoints/api_server.py
8	0	vllm/entrypoints/openai/api_server.py

[abfe705a0] Robert Shaw 2024-07-07 [ Misc ] Support Fp8 via `llm-compressor` (#6110)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8-compressed-tensors.yaml
1	1	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8.yaml
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-INT8-compressed-tensors.yaml
2	0	.buildkite/lm-eval-harness/configs/models-small.txt
1	1	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
2	1	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
27	5	tests/quantization/test_compressed_tensors.py
50	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
19	8	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
0	109	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8.py
87	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_fp8.py
85	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py
41	228	vllm/model_executor/layers/quantization/fp8.py
5	9	vllm/model_executor/layers/quantization/gptq_marlin.py
97	2	vllm/model_executor/layers/quantization/utils/marlin_utils.py
163	0	vllm/model_executor/layers/quantization/utils/w8a8_utils.py

[333306a25] Haichuan 2024-07-07 add benchmark for fix length input and output (#5857)
60	5	benchmarks/benchmark_serving.py

[6206dcb29] Roger Wang 2024-07-06 [Model] Add PaliGemma (#5189)
4	0	docs/source/models/supported_models.rst
52	0	examples/paligemma_example.py
147	0	tests/models/test_paligemma.py
2	0	vllm/model_executor/models/__init__.py
8	2	vllm/model_executor/models/gemma.py
344	0	vllm/model_executor/models/paligemma.py

[938938001] Cyrus Leung 2024-07-06 [Doc] Move guide for multimodal model and other improvements (#6168)
3	3	docs/source/dev/input_processing/model_inputs_index.rst
6	14	docs/source/dev/multimodal/multimodal_index.rst
2	0	docs/source/index.rst
21	17	docs/source/models/adding_model.rst
21	26	docs/source/{dev/multimodal/adding_multimodal_model.rst => models/enabling_multimodal_inputs.rst}
1	1	docs/source/models/supported_models.rst
1	1	vllm/inputs/registry.py
6	5	vllm/multimodal/base.py

[175c43eca] Roger Wang 2024-07-05 [Doc] Reorganize Supported Models by Type (#6167)
34	17	docs/source/models/supported_models.rst
2	1	docs/source/models/vlm.rst

[bc96d5c33] Simon Mo 2024-07-05 Move release wheel env var to Dockerfile instead (#6163)
2	2	.buildkite/release-pipeline.yaml
1	0	Dockerfile

[f0250620d] Simon Mo 2024-07-05 Fix release wheel build env var (#6162)
2	2	.buildkite/release-pipeline.yaml

[2de490d60] Simon Mo 2024-07-05 Update wheel builds to strip debug (#6161)
1	1	.buildkite/release-pipeline.yaml

[79d406e91] Simon Mo 2024-07-05 [Docs] Fix readthedocs for tag build (#6158)
4	1	docs/source/conf.py

[abad5746a] Simon Mo 2024-07-05 bump version to v0.5.1 (#6157)
1	1	vllm/version.py

[e58294ddf] JGSweets 2024-07-05 [Bugfix] Add verbose error if scipy is missing for blocksparse attention (#5695)
13	6	vllm/attention/ops/blocksparse_attention/utils.py

[f1e15da6f] jvlunteren 2024-07-05 [Frontend] Continuous usage stats in OpenAI completion API (#5742)
94	18	tests/entrypoints/openai/test_completion.py
2	1	vllm/entrypoints/openai/protocol.py
14	12	vllm/entrypoints/openai/serving_completion.py

[0097bb182] Christian Rohmann 2024-07-05 [Bugfix] Use templated datasource in grafana.json to allow automatic imports (#6136)
15	12	examples/production_monitoring/grafana.json

[ea4b57048] Cyrus Leung 2024-07-05 [VLM] Cleanup validation and update docs (#6149)
30	18	vllm/model_executor/models/llava.py
41	50	vllm/model_executor/models/llava_next.py
16	14	vllm/model_executor/models/phi3v.py

[a41357e94] Roger Wang 2024-07-04 [VLM] Improve consistency between feature size calculation and dummy data for profiling (#6146)
8	13	vllm/model_executor/models/llava_next.py
10	13	vllm/model_executor/models/phi3v.py

[ae96ef8fb] Cyrus Leung 2024-07-05 [VLM] Calculate maximum number of multi-modal tokens by model (#6121)
48	20	docs/source/dev/multimodal/adding_multimodal_model.rst
4	14	docs/source/models/vlm.rst
1	1	vllm/inputs/registry.py
4	0	vllm/model_executor/models/clip.py
13	1	vllm/model_executor/models/llava.py
12	0	vllm/model_executor/models/llava_next.py
12	0	vllm/model_executor/models/phi3v.py
99	6	vllm/multimodal/base.py
3	0	vllm/multimodal/image.py
39	34	vllm/multimodal/registry.py
13	6	vllm/worker/model_runner.py
12	8	vllm/worker/xpu_model_runner.py

[69ec3ca14] Lily Liu 2024-07-04 [Kernel][Model] logits_soft_cap for Gemma2 with flashinfer (#6051)
5	2	.buildkite/test-pipeline.yaml
248	0	tests/kernels/test_flashinfer.py
8	4	vllm/attention/backends/flashinfer.py
3	3	vllm/attention/selector.py
0	7	vllm/model_executor/models/gemma2.py
15	4	vllm/worker/model_runner.py

[81d7a50f2] Yuan 2024-07-05 [Hardware][Intel CPU] Adding intel openmp tunings in Docker file (#6008)
4	2	.buildkite/run-cpu-test.sh
8	2	Dockerfile.cpu
21	0	vllm/utils.py
4	1	vllm/worker/cpu_worker.py

[27902d42b] youkaichao 2024-07-04 [misc][doc] try to add warning for latest html (#5979)
38	0	docs/source/_templates/sections/header.html
8	0	docs/source/conf.py

[56b325e97] Gregory Shtrasberg 2024-07-04 [ROCm][AMD][Model]Adding alibi slopes support in ROCm triton flash attention and naive flash attention (#6043)
51	2	vllm/attention/backends/rocm_flash_attn.py

[3dd507083] Cyrus Leung 2024-07-04 [CI/Build] Cleanup VLM tests (#6107)
2	5	tests/models/test_llava_next.py
1	2	tests/models/test_phi3v.py
1	0	tests/models/utils.py
1	1	vllm/multimodal/image.py

[0ed646b7a] Murali Andoorveedu 2024-07-03 [Distributed][Core] Support Py39 and Py38 for PP (#6120)
1	6	vllm/executor/executor_base.py
9	0	vllm/executor/ray_gpu_executor.py

[1dab9bc8a] Travis Johnson 2024-07-03 [Bugfix] set OMP_NUM_THREADS to 1 by default for multiprocessing (#6109)
5	0	vllm/executor/multiproc_gpu_executor.py

[3de6e6a30] youkaichao 2024-07-03 [core][distributed] support n layers % pp size != 0 (#6115)
1	0	.buildkite/test-pipeline.yaml
6	9	vllm/config.py
8	1	vllm/distributed/utils.py
1	0	vllm/worker/openvino_worker.py
1	0	vllm/worker/tpu_worker.py
1	0	vllm/worker/worker.py
1	0	vllm/worker/xpu_worker.py

[966fe7214] youkaichao 2024-07-03 [doc][misc] bump up py version in installation doc (#6119)
2	2	docs/source/getting_started/installation.rst

[62963d129] Robert Shaw 2024-07-03 [ Misc ] Clean Up `CompressedTensorsW8A8` (#6113)
5	4	tests/quantization/test_compressed_tensors.py
5	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	4	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
33	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8.py
0	33	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py
0	47	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py

[d9e98f42e] xwjiang2010 2024-07-03 [vlm] Remove vision language config. (#6089)
5	0	docs/source/dev/multimodal/multimodal_index.rst
39	39	docs/source/models/vlm.rst
1	6	examples/llava_example.py
1	7	examples/llava_next_example.py
0	3	examples/openai_vision_api_client.py
2	4	examples/phi3v_example.py
3	3	tests/distributed/test_multimodal_broadcast.py
0	6	tests/entrypoints/openai/test_vision.py
17	43	tests/models/test_llava.py
13	41	tests/models/test_llava_next.py
15	39	tests/models/test_phi3v.py
7	31	vllm/config.py
7	52	vllm/engine/arg_utils.py
8	8	vllm/engine/llm_engine.py
5	0	vllm/entrypoints/llm.py
5	16	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/executor/cpu_executor.py
6	7	vllm/executor/executor_base.py
1	1	vllm/executor/gpu_executor.py
1	1	vllm/executor/openvino_executor.py
5	5	vllm/executor/ray_xpu_executor.py
1	1	vllm/executor/tpu_executor.py
4	4	vllm/executor/xpu_executor.py
2	2	vllm/inputs/registry.py
4	4	vllm/model_executor/model_loader/__init__.py
23	25	vllm/model_executor/model_loader/loader.py
3	3	vllm/model_executor/models/interfaces.py
10	11	vllm/model_executor/models/llava.py
47	6	vllm/model_executor/models/llava_next.py
45	17	vllm/model_executor/models/phi3v.py
7	0	vllm/multimodal/registry.py
4	4	vllm/spec_decode/draft_model_runner.py
12	13	vllm/worker/cpu_model_runner.py
5	5	vllm/worker/cpu_worker.py
4	4	vllm/worker/embedding_model_runner.py
17	11	vllm/worker/model_runner.py
4	4	vllm/worker/openvino_model_runner.py
5	5	vllm/worker/openvino_worker.py
4	4	vllm/worker/tpu_model_runner.py
4	4	vllm/worker/tpu_worker.py
5	8	vllm/worker/worker.py
15	10	vllm/worker/xpu_model_runner.py
5	8	vllm/worker/xpu_worker.py

[3c6325f0f] youkaichao 2024-07-03 [core][distributed] custom allreduce when pp size > 1 (#6117)
5	11	vllm/config.py
12	4	vllm/distributed/parallel_state.py

[47f0954af] Michael Goin 2024-07-03 [Kernel] Expand FP8 support to Ampere GPUs using FP8 Marlin (#5975)
1	0	CMakeLists.txt
5	0	csrc/ops.h
1308	0	csrc/quantization/fp8/fp8_marlin.cu
4	0	csrc/torch_bindings.cpp
2	1	docs/source/quantization/fp8.rst
1	1	docs/source/quantization/supported_hardware.rst
84	4	tests/kernels/test_marlin_gemm.py
14	5	tests/quantization/test_fp8.py
9	0	vllm/_custom_ops.py
134	30	vllm/model_executor/layers/quantization/fp8.py
25	3	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[7cd2ebb02] Roger Wang 2024-07-03 [Bugfix] Fix `compute_logits` in Jamba (#6093)
1	1	vllm/model_executor/models/jamba.py

[f1c78138a] Roger Wang 2024-07-03 [Doc] Fix Mock Import (#6094)
1	0	docs/source/conf.py

[3a86b54fb] Roger Wang 2024-07-02 [VLM][Frontend] Proper Image Prompt Formatting from OpenAI API (#6091)
27	10	vllm/entrypoints/openai/serving_chat.py

[f66620716] youkaichao 2024-07-02 [misc][distributed] error on invalid state (#6092)
3	0	vllm/executor/multiproc_gpu_executor.py
4	1	vllm/executor/ray_gpu_executor.py
22	0	vllm/utils.py

[d830656a9] Nick Hill 2024-07-02 [BugFix] Avoid unnecessary Ray import warnings (#6079)
7	2	vllm/config.py
5	0	vllm/engine/async_llm_engine.py
16	7	vllm/executor/ray_utils.py

[d18bab358] SangBin Cho 2024-07-03 [CI] Fix base url doesn't strip "/" (#6087)
1	1	tests/entrypoints/openai/test_completion.py

[9831aec49] Cyrus Leung 2024-07-03 [Core] Dynamic image size support for VLMs (#5276)
1	1	docs/source/dev/input_processing/model_inputs_index.rst
124	0	docs/source/dev/multimodal/adding_multimodal_model.rst
15	3	docs/source/dev/multimodal/multimodal_index.rst
16	8	docs/source/models/vlm.rst
1	2	examples/llava_example.py
3	8	examples/llava_next_example.py
5	3	examples/phi3v_example.py
81	33	tests/conftest.py
5	2	tests/distributed/test_multimodal_broadcast.py
71	35	tests/models/test_llava.py
79	50	tests/models/test_llava_next.py
82	58	tests/models/test_phi3v.py
67	21	tests/models/utils.py
16	47	tests/multimodal/test_mapper.py
18	11	tests/multimodal/test_utils.py
1	11	vllm/config.py
51	46	vllm/entrypoints/openai/serving_chat.py
1	0	vllm/entrypoints/openai/serving_engine.py
2	1	vllm/inputs/registry.py
37	0	vllm/model_executor/models/clip.py
27	22	vllm/model_executor/models/llava.py
108	76	vllm/model_executor/models/llava_next.py
157	65	vllm/model_executor/models/phi3v.py
41	0	vllm/model_executor/models/utils.py
5	2	vllm/multimodal/__init__.py
85	16	vllm/multimodal/base.py
94	6	vllm/multimodal/image.py
22	11	vllm/multimodal/registry.py
61	34	vllm/multimodal/utils.py
1	1	vllm/sequence.py
9	9	vllm/transformers_utils/image_processor.py
15	19	vllm/worker/cpu_model_runner.py
2	3	vllm/worker/embedding_model_runner.py
14	13	vllm/worker/model_runner.py
29	6	vllm/worker/neuron_model_runner.py
26	12	vllm/worker/openvino_model_runner.py
39	5	vllm/worker/tpu_model_runner.py
44	26	vllm/worker/xpu_model_runner.py

[482045ee7] youkaichao 2024-07-02 [hardware][misc] introduce platform abstraction (#6080)
2	2	tests/kernels/test_cutlass.py
2	2	tests/quantization/utils.py
3	2	vllm/attention/ops/blocksparse_attention/interface.py
2	2	vllm/attention/ops/prefix_prefill.py
2	2	vllm/lora/punica.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
3	2	vllm/model_executor/layers/quantization/fp8.py
2	2	vllm/model_executor/layers/quantization/gptq_marlin.py
2	2	vllm/model_executor/layers/quantization/utils/marlin_utils.py
3	2	vllm/model_executor/model_loader/loader.py
18	0	vllm/platforms/__init__.py
34	0	vllm/platforms/cuda.py
21	0	vllm/platforms/interface.py
15	0	vllm/platforms/rocm.py
0	7	vllm/utils.py
2	2	vllm/worker/worker.py

[9d6a8daa8] Mor Zusman 2024-07-03 [Model] Jamba support (#4115)
1	1	.buildkite/run-cpu-test.sh
23	0	Dockerfile
4	0	docs/source/models/supported_models.rst
3	0	requirements-mamba.txt
65	0	tests/models/test_jamba.py
28	1	vllm/config.py
15	1	vllm/core/scheduler.py
3	1	vllm/engine/async_llm_engine.py
3	1	vllm/engine/llm_engine.py
1	0	vllm/model_executor/models/__init__.py
955	0	vllm/model_executor/models/jamba.py
3	1	vllm/sequence.py
8	4	vllm/spec_decode/draft_model_runner.py
9	6	vllm/worker/cache_engine.py
4	3	vllm/worker/cpu_model_runner.py
2	1	vllm/worker/embedding_model_runner.py
57	10	vllm/worker/model_runner.py
1	0	vllm/worker/model_runner_base.py
1	0	vllm/worker/neuron_model_runner.py
2	1	vllm/worker/worker_base.py
4	3	vllm/worker/xpu_model_runner.py

[ee93f4f92] Qubitium-ModelCloud 2024-07-03 [CORE] Quantized lm-head Framework (#4442)
5	5	tests/lora/test_layers.py
45	0	tests/quantization/test_lm_head.py
1	1	tests/spec_decode/e2e/test_mlp_correctness.py
1	1	tests/test_logits_processor.py
2	2	vllm/lora/layers.py
9	7	vllm/model_executor/layers/logits_processor.py
9	0	vllm/model_executor/layers/quantization/base_config.py
10	3	vllm/model_executor/layers/quantization/gptq.py
11	4	vllm/model_executor/layers/quantization/gptq_marlin.py
10	3	vllm/model_executor/layers/quantization/marlin.py
53	17	vllm/model_executor/layers/vocab_parallel_embedding.py
2	1	vllm/model_executor/models/arctic.py
4	2	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bloom.py
4	3	vllm/model_executor/models/chatglm.py
4	4	vllm/model_executor/models/commandr.py
2	1	vllm/model_executor/models/dbrx.py
4	2	vllm/model_executor/models/deepseek.py
4	2	vllm/model_executor/models/deepseek_v2.py
3	3	vllm/model_executor/models/falcon.py
2	2	vllm/model_executor/models/gemma.py
2	2	vllm/model_executor/models/gemma2.py
2	2	vllm/model_executor/models/gpt2.py
2	2	vllm/model_executor/models/gpt_bigcode.py
2	1	vllm/model_executor/models/gpt_j.py
2	1	vllm/model_executor/models/gpt_neox.py
4	2	vllm/model_executor/models/internlm2.py
2	2	vllm/model_executor/models/jais.py
2	1	vllm/model_executor/models/llama.py
3	2	vllm/model_executor/models/llava.py
3	2	vllm/model_executor/models/llava_next.py
4	3	vllm/model_executor/models/minicpm.py
2	1	vllm/model_executor/models/mixtral.py
4	2	vllm/model_executor/models/mixtral_quant.py
4	4	vllm/model_executor/models/mlp_speculator.py
2	2	vllm/model_executor/models/mpt.py
3	3	vllm/model_executor/models/olmo.py
2	2	vllm/model_executor/models/opt.py
4	2	vllm/model_executor/models/orion.py
3	2	vllm/model_executor/models/phi.py
2	1	vllm/model_executor/models/phi3_small.py
4	2	vllm/model_executor/models/phi3v.py
4	2	vllm/model_executor/models/qwen.py
4	4	vllm/model_executor/models/qwen2.py
4	2	vllm/model_executor/models/qwen2_moe.py
4	2	vllm/model_executor/models/stablelm.py
3	3	vllm/model_executor/models/starcoder2.py
4	2	vllm/model_executor/models/xverse.py

[7c008c51a] Robert Shaw 2024-07-02 [ Misc ] Refactor MoE to isolate Fp8 From Mixtral (#5970)
11	0	.buildkite/lm-eval-harness/configs/Mixtral-8x22B-Instruct-v0.1-FP8-Dynamic.yaml
11	0	.buildkite/lm-eval-harness/configs/Mixtral-8x7B-Instruct-v0.1-FP8.yaml
11	0	.buildkite/lm-eval-harness/configs/Qwen2-57B-A14-Instruct.yaml
1	0	.buildkite/lm-eval-harness/configs/models-large.txt
2	2	tests/kernels/test_moe.py
4	0	vllm/model_executor/layers/fused_moe/__init__.py
197	0	vllm/model_executor/layers/fused_moe/layer.py
191	1	vllm/model_executor/layers/quantization/fp8.py
41	235	vllm/model_executor/models/mixtral.py
68	68	vllm/model_executor/models/qwen2_moe.py

[4d26d806e] Robert Shaw 2024-07-02 Update conftest.py (#6076)
1	1	tests/conftest.py

[c5832d2ae] Murali Andoorveedu 2024-07-02 [Core] Pipeline Parallel Support (#4412)
10	0	.buildkite/test-pipeline.yaml
13	1	tests/async_engine/test_async_llm_engine.py
2	2	tests/async_engine/test_openapi_server_ray.py
12	12	tests/basic_correctness/test_preemption.py
17	3	tests/distributed/test_comm_ops.py
149	0	tests/distributed/test_pipeline_parallel.py
4	4	tests/engine/output_processor/test_multi_step.py
2	2	tests/entrypoints/openai/test_chat.py
2	2	tests/entrypoints/openai/test_completion.py
2	2	tests/entrypoints/openai/test_embedding.py
2	2	tests/entrypoints/openai/test_models.py
2	2	tests/entrypoints/openai/test_vision.py
3	3	tests/spec_decode/utils.py
3	1	tests/tensorizer_loader/test_tensorizer.py
10	6	tests/utils.py
2	2	tests/worker/test_swap.py
22	3	vllm/config.py
3	0	vllm/core/block_manager_v1.py
3	0	vllm/core/block_manager_v2.py
11	2	vllm/core/scheduler.py
28	22	vllm/distributed/parallel_state.py
10	1	vllm/distributed/utils.py
63	16	vllm/engine/async_llm_engine.py
50	15	vllm/engine/llm_engine.py
1	1	vllm/engine/output_processor/interfaces.py
3	2	vllm/engine/output_processor/multi_step.py
13	7	vllm/engine/output_processor/single_step.py
6	6	vllm/executor/distributed_gpu_executor.py
25	0	vllm/executor/executor_base.py
2	1	vllm/executor/gpu_executor.py
6	6	vllm/executor/multiproc_gpu_executor.py
59	12	vllm/executor/ray_gpu_executor.py
2	1	vllm/model_executor/models/arctic.py
2	1	vllm/model_executor/models/baichuan.py
2	1	vllm/model_executor/models/bloom.py
2	1	vllm/model_executor/models/chatglm.py
2	1	vllm/model_executor/models/commandr.py
2	1	vllm/model_executor/models/dbrx.py
2	1	vllm/model_executor/models/deepseek.py
2	1	vllm/model_executor/models/deepseek_v2.py
2	1	vllm/model_executor/models/falcon.py
2	1	vllm/model_executor/models/gemma.py
2	1	vllm/model_executor/models/gemma2.py
61	27	vllm/model_executor/models/gpt2.py
2	1	vllm/model_executor/models/gpt_bigcode.py
2	1	vllm/model_executor/models/gpt_j.py
2	1	vllm/model_executor/models/gpt_neox.py
2	1	vllm/model_executor/models/internlm2.py
2	1	vllm/model_executor/models/jais.py
73	28	vllm/model_executor/models/llama.py
3	1	vllm/model_executor/models/llava.py
3	1	vllm/model_executor/models/llava_next.py
2	1	vllm/model_executor/models/minicpm.py
2	1	vllm/model_executor/models/mixtral.py
2	1	vllm/model_executor/models/mixtral_quant.py
2	1	vllm/model_executor/models/mpt.py
2	1	vllm/model_executor/models/olmo.py
2	1	vllm/model_executor/models/opt.py
2	1	vllm/model_executor/models/orion.py
2	1	vllm/model_executor/models/phi.py
2	1	vllm/model_executor/models/phi3_small.py
8	3	vllm/model_executor/models/phi3v.py
2	1	vllm/model_executor/models/qwen.py
2	1	vllm/model_executor/models/qwen2.py
2	1	vllm/model_executor/models/qwen2_moe.py
2	1	vllm/model_executor/models/stablelm.py
2	1	vllm/model_executor/models/starcoder2.py
2	1	vllm/model_executor/models/xverse.py
31	0	vllm/sequence.py
10	5	vllm/spec_decode/draft_model_runner.py
4	0	vllm/worker/cache_engine.py
4	1	vllm/worker/cpu_model_runner.py
24	14	vllm/worker/cpu_worker.py
7	2	vllm/worker/embedding_model_runner.py
201	125	vllm/worker/model_runner.py
4	1	vllm/worker/model_runner_base.py
4	1	vllm/worker/neuron_model_runner.py
1	1	vllm/worker/neuron_worker.py
23	13	vllm/worker/worker.py
31	9	vllm/worker/worker_base.py
4	1	vllm/worker/xpu_model_runner.py
2	2	vllm/worker/xpu_worker.py

[15aba081f] Sirej Dua 2024-07-02 [Speculative Decoding] MLPSpeculator Tensor Parallel support (1/2) (#6050)
24	12	tests/spec_decode/e2e/test_integration_dist_tp2.py
0	6	vllm/config.py
11	7	vllm/spec_decode/spec_decode_worker.py

[31354e563] Cyrus Leung 2024-07-02 [Doc] Reinstate doc dependencies (#6061)
8	0	docs/requirements-docs.txt

[98d6682cd] xwjiang2010 2024-07-02 [VLM] Remove `image_input_type` from VLM config (#5852)
0	4	.buildkite/download-images.sh
4	12	docs/requirements-docs.txt
5	3	docs/source/dev/multimodal/multimodal_index.rst
7	4	docs/source/models/vlm.rst
8	48	examples/llava_example.py
35	26	examples/llava_next_example.py
0	1	examples/openai_vision_api_client.py
3	3	examples/phi3v_example.py
8	30	tests/conftest.py
0	2	tests/entrypoints/openai/test_vision.py
8	14	tests/models/test_llava.py
10	13	tests/models/test_llava_next.py
8	13	tests/models/test_phi3v.py
6	34	tests/multimodal/test_mapper.py
2	2	tests/spec_decode/e2e/conftest.py
0	20	tests/tokenization/test_image_processor.py
1	33	vllm/config.py
4	52	vllm/engine/arg_utils.py
0	9	vllm/entrypoints/openai/api_server.py
29	36	vllm/entrypoints/openai/serving_chat.py
5	6	vllm/inputs/data.py
4	3	vllm/inputs/registry.py
2	3	vllm/model_executor/model_loader/loader.py
2	18	vllm/model_executor/models/clip.py
19	83	vllm/model_executor/models/llava.py
40	86	vllm/model_executor/models/llava_next.py
8	17	vllm/model_executor/models/phi3v.py
5	3	vllm/multimodal/__init__.py
25	28	vllm/multimodal/base.py
12	81	vllm/multimodal/image.py
52	44	vllm/multimodal/registry.py
7	6	vllm/multimodal/utils.py
5	5	vllm/sequence.py
0	4	vllm/transformers_utils/image_processor.py
1	1	vllm/worker/model_runner.py

[2c37540aa] danieljannai21 2024-07-02 [Frontend] Add template related params to request (#5709)
1	1	requirements-common.txt
21	0	vllm/entrypoints/openai/protocol.py
8	0	vllm/entrypoints/openai/serving_chat.py

[3476ed080] Alexander Matveev 2024-07-01 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)
4	0	benchmarks/benchmark_latency.py
1	1	tests/conftest.py
3	2	tests/core/block/test_block_table.py
12	12	tests/core/block/test_cpu_gpu_block_allocator.py
3	3	tests/core/block/test_naive_block.py
67	39	tests/core/block/test_prefix_caching_block.py
4	4	tests/spec_decode/test_batch_expansion.py
56	29	vllm/core/block/block_table.py
154	44	vllm/core/block/common.py
56	28	vllm/core/block/cpu_gpu_block_allocator.py
44	12	vllm/core/block/interfaces.py
149	67	vllm/core/block/naive_block.py
479	214	vllm/core/block/prefix_caching_block.py
101	49	vllm/core/block_manager_v2.py
4	1	vllm/engine/llm_engine.py
1	1	vllm/entrypoints/openai/serving_completion.py
2	2	vllm/model_executor/sampling_metadata.py
2	2	vllm/outputs.py
47	22	vllm/sequence.py

[54600709b] Thomas Parnell 2024-07-02 [Model] Changes to MLPSpeculator to support tie_weights and input_scale (#5965)
69	25	vllm/model_executor/models/mlp_speculator.py
12	0	vllm/transformers_utils/configs/mlp_speculator.py

[e373853e1] James Whedbee 2024-07-01 [Frontend] Relax api url assertion for openai benchmarking (#6046)
4	4	benchmarks/backend_request_func.py

[c87ebc3ef] Nick Hill 2024-07-01 [BugFix] Ensure worker model loop is always stopped at the right time (#5987)
1	1	vllm/engine/llm_engine.py

[c4059ea54] Antoni Baum 2024-07-01 [Bugfix] Add explicit `end_forward` calls to flashinfer (#6044)
2	0	vllm/attention/backends/flashinfer.py

[8e0817c26] Roger Wang 2024-07-01 [Bugfix][Doc] Fix Doc Formatting (#6048)
2	2	docs/source/serving/faq.rst

[83bdcb6ac] ning.zhang 2024-07-01 add FAQ doc under 'serving' (#5946)
1	0	docs/source/index.rst
12	0	docs/source/serving/faq.rst

[12a59959e] Avshalom Manevich 2024-07-02 [Bugfix] adding chunking mechanism to fused_moe to handle large inputs (#6029)
1	1	tests/kernels/test_moe.py
3	0	vllm/envs.py
70	47	vllm/model_executor/layers/fused_moe/fused_moe.py

[dec6fc6f3] Antoni Baum 2024-07-01 [Bugfix] Use RayActorError for older versions of Ray in  RayTokenizerGroupPool (#6039)
5	1	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py

[8893130b6] youkaichao 2024-07-01 [doc][misc] further lower visibility of simple api server (#6041)
6	1	examples/api_client.py

[bb6032683] zhyncs 2024-07-02 [Misc] update benchmark backend for scalellm (#6018)
1	0	benchmarks/backend_request_func.py

[4050d646e] youkaichao 2024-07-01 [doc][misc] remove deprecated api server in doc (#6037)
1	1	docs/source/serving/distributed_serving.rst

[d76084c12] Robert Shaw 2024-07-01 [ CI ] Re-enable Large Model LM Eval (#6031)
9	0	.buildkite/test-pipeline.yaml

[80ca1e6a3] sroy745 2024-07-01 [Speculative Decoding 2/2 ] Integrate typical acceptance sampler into Spec Decode Worker (#5348)
64	32	tests/samplers/test_typical_acceptance_sampler.py
53	1	tests/spec_decode/e2e/test_multistep_correctness.py
7	5	tests/spec_decode/test_dynamic_spec_decode.py
47	47	tests/spec_decode/test_metrics.py
85	69	tests/spec_decode/test_spec_decode_worker.py
22	0	tests/spec_decode/test_utils.py
73	2	vllm/config.py
41	1	vllm/engine/arg_utils.py
1	1	vllm/engine/metrics.py
9	9	vllm/model_executor/layers/rejection_sampler.py
14	1	vllm/model_executor/layers/spec_decode_base_sampler.py
10	12	vllm/model_executor/layers/typical_acceptance_sampler.py
13	11	vllm/spec_decode/metrics.py
43	19	vllm/spec_decode/spec_decode_worker.py

[614aa5120] youkaichao 2024-06-30 [misc][cuda] use nvml to avoid accidentally cuda initialization (#6007)
2	1	tests/kernels/test_cutlass.py
2	1	tests/quantization/utils.py
3	3	vllm/attention/ops/blocksparse_attention/interface.py
3	1	vllm/attention/ops/prefix_prefill.py
5	53	vllm/distributed/device_communicators/custom_all_reduce.py
2	1	vllm/lora/punica.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	2	vllm/model_executor/layers/quantization/fp8.py
2	1	vllm/model_executor/layers/quantization/gptq_marlin.py
2	1	vllm/model_executor/layers/quantization/utils/marlin_utils.py
2	2	vllm/model_executor/model_loader/loader.py
57	0	vllm/utils.py
2	1	vllm/worker/worker.py

[af9ad46fc] Robert Shaw 2024-06-30 [ Misc ] Refactor w8a8 to use `process_weights_after_load` (Simplify Weight Loading) (#5940)
19	9	tests/quantization/test_compressed_tensors.py
17	0	tests/quantization/test_fp8.py
39	71	vllm/model_executor/layers/linear.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
8	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
42	49	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8.py
3	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py
16	29	vllm/model_executor/layers/quantization/fp8.py

[7836fdcc1] Dipika Sikka 2024-06-30 [Misc] Fix `get_min_capability` (#5971)
2	1	vllm/model_executor/layers/quantization/awq.py
2	1	vllm/model_executor/layers/quantization/base_config.py
1	1	vllm/model_executor/layers/quantization/bitsandbytes.py
10	2	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	1	vllm/model_executor/layers/quantization/squeezellm.py

[deacb7ec4] Robert Shaw 2024-06-30 [ CI ] Temporarily Disable Large LM-Eval Tests (#6005)
0	9	.buildkite/test-pipeline.yaml

[f5e73c9f1] SangBin Cho 2024-07-01 [Lora] Use safetensor keys instead of adapter_config.json to find unexpected modules.  (#5909)
1	0	.buildkite/test-pipeline.yaml
3	1	tests/lora/conftest.py
2	2	tests/lora/test_mixtral.py
46	17	vllm/lora/models.py

[c6c240aa0] llmpros 2024-06-30 [Frontend]: Support base64 embedding (#5935)
33	0	tests/entrypoints/openai/test_embedding.py
1	1	vllm/entrypoints/openai/protocol.py
13	13	vllm/entrypoints/openai/serving_embedding.py

[2be6955a3] youkaichao 2024-06-30 [ci][distributed] fix device count call
1	11	.buildkite/test-pipeline.yaml
16	5	tests/conftest.py
10	5	tests/distributed/test_basic_distributed_correctness.py
10	4	tests/distributed/test_chunked_prefill_distributed.py
20	10	tests/models/test_llava.py
26	16	tests/models/test_phi3v.py

[9d47f64eb] Cyrus Leung 2024-06-30 [CI/Build] [3/3] Reorganize entrypoints tests (#5966)
2	2	.buildkite/test-pipeline.yaml
0	2	pyproject.toml
0	0	tests/entrypoints/llm/__init__.py
1	3	tests/entrypoints/{test_llm_encode.py => llm/test_encode.py}
1	3	tests/entrypoints/{test_llm_generate.py => llm/test_generate.py}
1	3	tests/entrypoints/{test_llm_generate_multiple_loras.py => llm/test_generate_multiple_loras.py}
0	0	tests/entrypoints/openai/__init__.py
1	3	tests/entrypoints/{test_openai_chat.py => openai/test_chat.py}
1	3	tests/entrypoints/{test_openai_completion.py => openai/test_completion.py}
1	3	tests/entrypoints/{test_openai_embedding.py => openai/test_embedding.py}
0	2	tests/entrypoints/{ => openai}/test_guided_processors.py
1	3	tests/entrypoints/{test_openai_server.py => openai/test_models.py}
0	3	tests/entrypoints/{test_server_oot_registration.py => openai/test_oot_registration.py}
0	0	tests/entrypoints/{test_openai_run_batch.py => openai/test_run_batch.py}
0	4	tests/entrypoints/openai/test_serving_chat.py
3	10	tests/entrypoints/{test_openai_vision.py => openai/test_vision.py}
7	4	tests/utils.py

[cff6a1fec] Cyrus Leung 2024-06-30 [CI/Build] Reuse code for checking output consistency (#5988)
8	7	tests/basic_correctness/test_basic_correctness.py
8	7	tests/basic_correctness/test_chunked_prefill.py
9	7	tests/basic_correctness/test_preemption.py
8	7	tests/distributed/test_basic_distributed_correctness.py
8	7	tests/distributed/test_chunked_prefill_distributed.py
8	7	tests/models/test_big_models.py
10	8	tests/models/test_llava.py
10	8	tests/models/test_llava_next.py
8	7	tests/models/test_models.py
10	8	tests/models/test_phi3v.py
38	2	tests/models/utils.py

[bcc6a09b6] Roger Wang 2024-06-29 [CI/Build] Temporarily Remove Phi3-Vision from TP Test (#5989)
3	2	.buildkite/test-pipeline.yaml

[9def10664] Matt Wong 2024-06-29 [Bugfix][CI/Build][Hardware][AMD] Install matching torchvision to fix AMD tests (#5949)
12	6	Dockerfile.rocm
2	2	tests/entrypoints/test_openai_chat.py

[75aa1442d] Robert Shaw 2024-06-29 [ CI/Build ] LM Eval Harness Based CI Testing (#5838)
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct.yaml
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FP8.yaml
11	0	.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct.yaml
11	0	.buildkite/lm-eval-harness/configs/Mixtral-8x7B-Instruct-v0.1.yaml
2	0	.buildkite/lm-eval-harness/configs/models-large.txt
2	0	.buildkite/lm-eval-harness/configs/models-small.txt
46	0	.buildkite/lm-eval-harness/run-lm-eval-gsm-hf-baseline.sh
51	0	.buildkite/lm-eval-harness/run-lm-eval-gsm-vllm-baseline.sh
59	0	.buildkite/lm-eval-harness/run-tests.sh
54	0	.buildkite/lm-eval-harness/test_lm_eval_correctness.py
16	0	.buildkite/test-pipeline.yaml

[99397da53] Cyrus Leung 2024-06-29 [CI/Build] Add TP test for vision models (#5892)
5	0	.buildkite/test-pipeline.yaml
51	0	tests/distributed/test_multimodal_broadcast.py
31	8	tests/models/test_llava.py
36	13	tests/models/test_phi3v.py
1	0	vllm/distributed/device_communicators/shm_broadcast.py
3	1	vllm/distributed/parallel_state.py
1	1	vllm/model_executor/models/llava.py
1	1	vllm/model_executor/models/llava_next.py
2	3	vllm/model_executor/models/phi3v.py

[8dbfcd35b] Robert Shaw 2024-06-29 [ CI/Build ] Added E2E Test For Compressed Tensors (#5839)
2	0	requirements-test.txt
4	0	tests/conftest.py
49	0	tests/models/test_compressed_tensors.py
2	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py

[f7dac83d9] Cody Yu 2024-06-29 [Kernel] Raise an exception in MoE kernel if the batch size is larger then 65k (#5939)
5	0	vllm/model_executor/layers/fused_moe/fused_moe.py

[7c01f7064] Antoni Baum 2024-06-29 [Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)
11	14	vllm/sequence.py

[51e971d39] Cyrus Leung 2024-06-29 [Bugfix] Support `eos_token_id` from `config.json` (#5954)
31	0	tests/tokenization/test_get_eos.py
13	10	vllm/engine/llm_engine.py
23	1	vllm/transformers_utils/config.py

[329df38f1] Roger Wang 2024-06-28 [Misc] Update Phi-3-Vision Example (#5981)
5	3	examples/phi3v_example.py

[580353da9] Woosuk Kwon 2024-06-28 [Bugfix] Fix precisions in Gemma 1 (#5913)
1	0	tests/models/test_models.py
11	14	vllm/model_executor/models/gemma.py

[ba4994443] Joe Runde 2024-06-28 [Kernel] Add punica dimensions for Granite 3b and 8b (#5930)
2	0	csrc/punica/bgmv/bgmv_config.h
1	0	tests/lora/test_punica.py

[906a19cdb] William Lin 2024-06-28 [Misc] Extend vLLM Metrics logging API (#5925)
6	6	tests/metrics/test_metrics.py
30	8	vllm/engine/llm_engine.py
189	104	vllm/engine/metrics.py

[c4bca740e] mcalman 2024-06-28 [Bugfix] fix missing last itl in openai completions benchmark (#5926)
5	6	benchmarks/backend_request_func.py

[7f83f40de] Woosuk Kwon 2024-06-28 [Bugfix][TPU] Fix pad slot id (#5977)
1	1	vllm/worker/tpu_model_runner.py

[54814fd85] Woosuk Kwon 2024-06-28 [Bugfix][TPU] Fix TPU sampler output (#5978)
1	1	vllm/worker/tpu_worker.py

[7041de438] Lily Liu 2024-06-28 [Kernel] Flashinfer for prefill & decode, with Cudagraph support for decode (#4628)
3	0	.buildkite/test-pipeline.yaml
1	1	requirements-test.txt
0	6	tests/basic_correctness/test_basic_correctness.py
0	5	tests/distributed/test_basic_distributed_correctness.py
58	25	vllm/attention/backends/flashinfer.py
3	2	vllm/attention/selector.py
248	78	vllm/worker/model_runner.py

[6a62cb82c] Robert Shaw 2024-06-28 [Bugfix] Fix Engine Failing After Invalid Request - AsyncEngineDeadError (#5963)
24	10	vllm/entrypoints/openai/protocol.py

[5d2a1a9cf] Tyler Michael Smith 2024-06-28 Unmark more files as executable (#5962)
0	0	csrc/punica/bgmv/bgmv_config.h
0	0	examples/offline_inference_neuron.py
0	0	vllm/model_executor/models/__init__.py

[4bf35ed9a] Michael Goin 2024-06-28 [Bugfix] Only add `Attention.kv_scale` if kv cache quantization is enabled (#5936)
14	9	vllm/attention/layer.py

[be0b3af9e] wangding zeng 2024-06-29 Support Deepseek-V2 (#4650)
6	0	vllm/config.py
2	1	vllm/model_executor/layers/fused_moe/__init__.py
31	0	vllm/model_executor/layers/fused_moe/fused_moe.py
126	0	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/models/__init__.py
534	0	vllm/model_executor/models/deepseek_v2.py

[2cd402e16] Robert Shaw 2024-06-28 [ Bugfix ] Enabling Loading Models With Fused QKV/MLP on Disk with FP8 (#5921)
12	2	vllm/model_executor/layers/linear.py
20	21	vllm/model_executor/layers/quantization/fp8.py

[b18523074] Robert Shaw 2024-06-28 [ Misc ] Remove `fp8_shard_indexer` from Col/Row Parallel Linear (Simplify Weight Loading) (#5928)
8	20	vllm/model_executor/layers/linear.py

[6a2d659d2] Tyler Michael Smith 2024-06-28 [Bugfix] Fix compute datatype for cutlass 3.x epilogues (#5931)
2	2	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
68	57	tests/kernels/test_cutlass.py

[b2c620230] Cody Yu 2024-06-28 [Spec Decode] Introduce DraftModelRunner (#5799)
3	0	tests/spec_decode/test_multi_step_worker.py
4	1	tests/spec_decode/utils.py
3	0	vllm/sequence.py
170	0	vllm/spec_decode/draft_model_runner.py
16	13	vllm/spec_decode/multi_step_worker.py
3	0	vllm/spec_decode/spec_decode_worker.py
8	3	vllm/worker/cpu_model_runner.py
11	4	vllm/worker/embedding_model_runner.py
7	3	vllm/worker/model_runner.py
2	1	vllm/worker/model_runner_base.py
7	2	vllm/worker/neuron_model_runner.py
7	2	vllm/worker/tpu_model_runner.py
4	1	vllm/worker/worker.py
5	4	vllm/worker/worker_base.py
8	3	vllm/worker/xpu_model_runner.py

[b90d8cd83] xwjiang2010 2024-06-28 [Distributed] Make it clear that % should not be in tensor dict keys. (#5927)
9	1	tests/distributed/test_parallel_state.py
3	0	vllm/distributed/parallel_state.py

[3b752a655] Cyrus Leung 2024-06-28 [CI/Build] [2/3] Reorganize entrypoints tests (#5904)
1	1	tests/entrypoints/test_llm_generate_multiple_loras.py
1	1	tests/entrypoints/test_openai_chat.py
650	0	tests/entrypoints/test_openai_completion.py
1	596	tests/entrypoints/test_openai_server.py

[ec1ad0046] Thomas Parnell 2024-06-28 [Bugfix] Better error message for MLPSpeculator when `num_speculative_tokens` is set too high (#5894)
3	3	vllm/config.py

[57f09a419] Ilya Lavrenov 2024-06-28 [Hardware][Intel] OpenVINO vLLM backend (#5379)
14	0	.buildkite/run-openvino-test.sh
26	0	Dockerfile.openvino
4	3	benchmarks/benchmark_latency.py
4	3	benchmarks/benchmark_throughput.py
95	0	docs/source/getting_started/openvino-installation.rst
1	0	docs/source/index.rst
9	0	requirements-openvino.txt
10	1	setup.py
7	2	tests/kernels/test_attention_selector.py
101	0	vllm/attention/backends/openvino.py
11	1	vllm/attention/selector.py
5	3	vllm/config.py
8	6	vllm/engine/arg_utils.py
6	0	vllm/engine/async_llm_engine.py
3	0	vllm/engine/llm_engine.py
21	1	vllm/envs.py
163	0	vllm/executor/openvino_executor.py
2	2	vllm/model_executor/layers/sampler.py
210	0	vllm/model_executor/model_loader/openvino.py
10	1	vllm/utils.py
330	0	vllm/worker/openvino_model_runner.py
353	0	vllm/worker/openvino_worker.py

[593263440] Tyler Michael Smith 2024-06-28 Unmark fused_moe config json file as executable (#5960)
0	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json

[5cbe8d155] Cyrus Leung 2024-06-28 [Core] Registry for processing model inputs (#5214)
20	0	docs/source/dev/input_processing/input_processing_pipeline.rst
39	0	docs/source/dev/input_processing/model_inputs_index.rst
1	7	docs/source/dev/multimodal/multimodal_index.rst
1	0	docs/source/index.rst
2	2	docs/source/models/adding_model.rst
2	1	examples/phi3v_example.py
32	37	tests/multimodal/{test_processor.py => test_mapper.py}
3	0	vllm/config.py
32	32	vllm/engine/arg_utils.py
5	3	vllm/engine/async_llm_engine.py
9	4	vllm/engine/llm_engine.py
19	0	vllm/inputs/__init__.py
16	2	vllm/{inputs.py => inputs/data.py}
207	0	vllm/inputs/registry.py
69	8	vllm/model_executor/models/clip.py
34	6	vllm/model_executor/models/llava.py
109	29	vllm/model_executor/models/llava_next.py
54	20	vllm/model_executor/models/phi3v.py
10	1	vllm/multimodal/__init__.py
41	43	vllm/multimodal/base.py
18	60	vllm/multimodal/image.py
40	93	vllm/multimodal/registry.py
2	2	vllm/sequence.py
0	4	vllm/transformers_utils/image_processor.py
4	16	vllm/worker/cpu_model_runner.py
9	22	vllm/worker/model_runner.py

[0d0e3a42a] Isotr0py 2024-06-28 [Bugfix][Hardware][Intel CPU] Fix unpassed multi_modal_kwargs for CPU runner (#5956)
1	0	vllm/worker/cpu_model_runner.py

[74d55c065] xwjiang2010 2024-06-28 [VLM][BugFix] Make sure that `multi_modal_kwargs` can broadcast properly with ring buffer. (#5905)
10	10	vllm/distributed/parallel_state.py

[f136da15e] Woosuk Kwon 2024-06-27 [Hardware][TPU] Optimize KV cache swapping (#5878)
4	12	vllm/attention/backends/pallas.py
32	10	vllm/worker/tpu_worker.py

[c3dde367f] Divakar Verma 2024-06-27 [Kernel][ROCm][AMD] fused_moe Triton configs v2 for mi300X (#5932)
118	46	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json
136	46	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json
122	50	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json
124	52	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json

[64e8d2a78] youkaichao 2024-06-27 [core][misc] remove logical block (#5882)
1	81	vllm/block.py
9	10	vllm/core/block_manager_v1.py
6	29	vllm/sequence.py

[79c92c7c8] Woosuk Kwon 2024-06-27 [Model] Add Gemma 2 (#5908)
4	0	docs/source/models/supported_models.rst
1	1	requirements-common.txt
23	7	vllm/config.py
4	0	vllm/lora/layers.py
46	0	vllm/model_executor/layers/layernorm.py
9	1	vllm/model_executor/layers/logits_processor.py
10	0	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/models/__init__.py
401	0	vllm/model_executor/models/gemma2.py

[736ed3884] Roger Wang 2024-06-27 [CI/Build] Fix Args for `_get_logits_warper` in Sampler Test (#5922)
1	1	tests/samplers/test_sampler.py

[365791ff8] Nick Hill 2024-06-27 [BugFix] Fix `min_tokens` behaviour for multiple eos tokens (#5849)
2	5	vllm/engine/llm_engine.py
21	8	vllm/sampling_params.py

[691e29ecf] Nick Hill 2024-06-27 [BugFix] Fix `MLPSpeculator` handling of `num_speculative_tokens` (#5876)
7	3	vllm/config.py
8	7	vllm/model_executor/models/mlp_speculator.py
3	0	vllm/transformers_utils/configs/mlp_speculator.py

[3fd02bda5] youkaichao 2024-06-27 [doc][misc] add note for Kubernetes users (#5916)
2	0	docs/source/serving/env_vars.rst

[98cf2ed67] Cyrus Leung 2024-06-28 [Model][Bugfix] Implicit model flags and reenable Phi-3-Vision (#5896)
0	2	vllm/model_executor/models/baichuan.py
0	2	vllm/model_executor/models/chatglm.py
0	2	vllm/model_executor/models/gemma.py
0	2	vllm/model_executor/models/gpt_bigcode.py
16	2	vllm/model_executor/models/interfaces.py
0	2	vllm/model_executor/models/llama.py
0	2	vllm/model_executor/models/llava.py
0	2	vllm/model_executor/models/llava_next.py
0	2	vllm/model_executor/models/minicpm.py
0	2	vllm/model_executor/models/mixtral.py
0	2	vllm/model_executor/models/phi.py
10	6	vllm/model_executor/models/phi3v.py
0	2	vllm/model_executor/models/qwen2.py
0	2	vllm/model_executor/models/xverse.py

[e9d32d077] Cyrus Leung 2024-06-27 [CI/Build] [1/3] Reorganize entrypoints tests (#5526)
875	0	tests/entrypoints/test_openai_chat.py
19	772	tests/entrypoints/test_openai_server.py
2	2	tests/entrypoints/test_openai_vision.py

[2061f0b8a] Roger Wang 2024-06-27 [Bugfix] Fix img_sizes Parsing in Phi3-Vision (#5888)
6	20	vllm/model_executor/models/phi3v.py

[96354d6a2] Cyrus Leung 2024-06-27 [Model] Add base class for LoRA-supported models (#5018)
3	0	docs/source/models/lora.rst
2	1	vllm/lora/lora.py
3	3	vllm/lora/models.py
13	7	vllm/model_executor/model_loader/loader.py
9	2	vllm/model_executor/models/baichuan.py
9	2	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/decilm.py
8	2	vllm/model_executor/models/gemma.py
8	1	vllm/model_executor/models/gpt_bigcode.py
130	0	vllm/model_executor/models/interfaces.py
8	1	vllm/model_executor/models/llama.py
12	10	vllm/model_executor/models/llava.py
11	9	vllm/model_executor/models/llava_next.py
10	2	vllm/model_executor/models/minicpm.py
8	1	vllm/model_executor/models/mixtral.py
14	8	vllm/model_executor/models/phi.py
8	2	vllm/model_executor/models/qwen2.py
0	12	vllm/model_executor/models/vlm_base.py
9	2	vllm/model_executor/models/xverse.py
3	8	vllm/worker/model_runner.py

[d12af207d] xwjiang2010 2024-06-27 [VLM][Bugfix] Make sure that `multi_modal_kwargs` is broadcasted properly (#5880)
3	1	.buildkite/test-pipeline.yaml
49	0	tests/distributed/test_parallel_state.py
29	8	vllm/distributed/parallel_state.py

[6eabc6cb0] Cyrus Leung 2024-06-27 [Doc] Add note about context length in Phi-3-Vision example (#5887)
3	1	examples/phi3v_example.py

[2110557da] Nick Hill 2024-06-26 [BugFix] Fix cuda graph for MLPSpeculator (#5875)
0	1	examples/offline_inference_mlpspeculator.py
6	3	vllm/worker/model_runner.py

[b9e84259e] Roger Wang 2024-06-26 [Misc] Add example for LLaVA-NeXT (#5879)
38	0	examples/llava_next_example.py

[294104c3f] youkaichao 2024-06-26 [doc] update usage of env var to avoid conflict (#5873)
3	0	docs/source/serving/env_vars.rst

[38a1674ab] Chip Kerchner 2024-06-26 Support CPU inference with VSX PowerPC ISA (#5652)
22	0	Dockerfile.ppc64le
10	1	cmake/cpu_extension.cmake
7	507	csrc/cpu/cpu_types.hpp
491	0	csrc/cpu/cpu_types_vsx.hpp
515	0	csrc/cpu/cpu_types_x86.hpp
1	0	csrc/ops.h
3	3	requirements-cpu.txt

[f5c8628fd] Woosuk Kwon 2024-06-26 [Bugfix][TPU] Fix CPU cache allocation (#5869)
2	3	vllm/attention/backends/pallas.py
6	2	vllm/worker/tpu_worker.py

[cbc53b6b8] Woosuk Kwon 2024-06-26 [Hardware][TPU] Support parallel sampling & Swapping (#5855)
22	8	vllm/attention/backends/pallas.py
50	26	vllm/worker/tpu_model_runner.py
75	22	vllm/worker/tpu_worker.py

[c54269d96] sasha0552 2024-06-26 [Frontend] Add tokenize/detokenize endpoints (#5054)
49	0	tests/entrypoints/test_openai_server.py
30	1	vllm/entrypoints/openai/api_server.py
21	0	vllm/entrypoints/openai/protocol.py
31	1	vllm/entrypoints/openai/serving_completion.py
12	4	vllm/entrypoints/openai/serving_engine.py

[5bfd1bbc9] Luka Govedič 2024-06-26 [Kernel] Adding bias epilogue support for `cutlass_scaled_mm` (#5560)
2	1	CMakeLists.txt
2	1	csrc/ops.h
170	58	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
108	31	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu
21	11	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
1	1	csrc/torch_bindings.cpp
72	28	tests/kernels/test_cutlass.py
7	3	vllm/_custom_ops.py

[6984c02a2] Cyrus Leung 2024-06-26 [CI/Build] Refactor image test assets (#5821)
70	41	tests/conftest.py
14	12	tests/models/test_llava.py
16	14	tests/models/test_llava_next.py
15	13	tests/models/test_phi3v.py
12	12	tests/multimodal/test_processor.py

[3439c5a8e] Woosuk Kwon 2024-06-26 [Bugfix][TPU] Fix KV cache size calculation (#5860)
7	6	vllm/worker/tpu_worker.py

[6806998bf] Woosuk Kwon 2024-06-26 [Bugfix] Fix embedding to support 2D inputs (#5829)
2	2	vllm/model_executor/layers/vocab_parallel_embedding.py

[515080ad2] youkaichao 2024-06-25 [bugfix][distributed] fix shm broadcast when the queue size is full (#5801)
33	16	tests/distributed/test_shm_broadcast.py
43	30	vllm/distributed/device_communicators/shm_broadcast.py

[3aa7b6cf6] Roger Wang 2024-06-25 [Misc][Doc] Add Example of using OpenAI Server with VLM (#5832)
2	0	docs/source/models/vlm.rst
90	0	examples/openai_vision_api_client.py
9	3	vllm/multimodal/utils.py

[dda481159] Stephanie Wang 2024-06-25 [Core] Refactor Worker and ModelRunner to consolidate control plane communication (#5408)
152	0	tests/worker/test_model_input.py
30	27	tests/worker/test_model_runner.py
5	1	vllm/attention/backends/abstract.py
2	2	vllm/attention/backends/blocksparse_attn.py
2	2	vllm/attention/backends/flash_attn.py
2	2	vllm/attention/backends/flashinfer.py
2	2	vllm/attention/backends/ipex_attn.py
2	2	vllm/attention/backends/pallas.py
2	2	vllm/attention/backends/rocm_flash_attn.py
2	2	vllm/attention/backends/torch_sdpa.py
2	2	vllm/attention/backends/xformers.py
8	8	vllm/executor/distributed_gpu_executor.py
2	2	vllm/executor/executor_base.py
1	1	vllm/executor/gpu_executor.py
3	5	vllm/executor/multiproc_gpu_executor.py
1	2	vllm/executor/neuron_executor.py
2	3	vllm/executor/ray_gpu_executor.py
2	1	vllm/sequence.py
1	2	vllm/spec_decode/mlp_speculator_worker.py
95	66	vllm/worker/cpu_model_runner.py
34	51	vllm/worker/cpu_worker.py
52	77	vllm/worker/embedding_model_runner.py
205	162	vllm/worker/model_runner.py
157	0	vllm/worker/model_runner_base.py
49	15	vllm/worker/neuron_model_runner.py
19	20	vllm/worker/neuron_worker.py
37	92	vllm/worker/worker.py
165	5	vllm/worker/worker_base.py
72	19	vllm/worker/xpu_model_runner.py

[82079729c] aws-patlange 2024-06-25 [Bugfix] Fix assertion in NeuronExecutor (#5841)
3	3	vllm/executor/neuron_executor.py

[c2a8ac75e] Thomas Parnell 2024-06-26 [CI/Build] Add E2E tests for MLPSpeculator (#5791)
216	0	tests/spec_decode/e2e/test_mlp_correctness.py

[f178e56c6] Woosuk Kwon 2024-06-25 [Hardware][TPU] Raise errors for unsupported sampling params (#5850)
44	19	vllm/worker/tpu_model_runner.py

[dd793d1de] Matt Wong 2024-06-25 [Hardware][AMD][CI/Build][Doc] Upgrade to ROCm 6.1, Dockerfile improvements, test fixes (#5422)
6	14	CMakeLists.txt
145	64	Dockerfile.rocm
12	8	cmake/utils.cmake
3	3	docs/source/getting_started/amd-installation.rst
2	2	tests/async_engine/test_openapi_server_ray.py
11	6	tests/distributed/test_utils.py
2	2	tests/entrypoints/test_openai_embedding.py
2	2	tests/entrypoints/test_openai_server.py
2	2	tests/entrypoints/test_openai_vision.py
32	6	tests/utils.py
9	1	vllm/config.py
7	4	vllm/distributed/device_communicators/custom_all_reduce_utils.py
5	3	vllm/executor/multiproc_gpu_executor.py
12	4	vllm/utils.py
9	1	vllm/worker/worker_base.py

[bc34937d6] Woosuk Kwon 2024-06-25 [Hardware][TPU] Refactor TPU backend (#5831)
37	21	vllm/executor/tpu_executor.py
4	0	vllm/worker/tpu_model_runner.py
24	11	vllm/worker/tpu_worker.py

[dd248f767] Dipika Sikka 2024-06-25 [Misc] Update `w4a16` `compressed-tensors` support to include `w8a16` (#5794)
12	11	tests/quantization/test_compressed_tensors.py
17	11	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
3	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
3	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/{compressed_tensors_w4a16.py => compressed_tensors_wNa16.py}

[d9b34baed] Michael Goin 2024-06-25 [CI/Build] Add unit testing for FlexibleArgumentParser (#5798)
60	1	tests/test_utils.py

[c18ebfdd7] youkaichao 2024-06-25 [doc][distributed] add both gloo and nccl tests (#5834)
10	3	docs/source/getting_started/debugging.rst

[67882dbb4] Antoni Baum 2024-06-25 [Core] Add fault tolerance for `RayTokenizerGroupPool` (#5748)
99	0	tests/tokenization/test_tokenizer_group.py
2	0	vllm/engine/async_llm_engine.py
2	0	vllm/engine/llm_engine.py
4	0	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
88	24	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py

[7b9931430] Jie Fu (傅杰) 2024-06-26 [Misc] Remove useless code in cpu_worker (#5824)
0	1	vllm/worker/cpu_worker.py

[2ce5d6688] Woo-Yeon Lee 2024-06-25  [Speculative Decoding] Support draft model on different tensor-parallel size than target model (#5414)
2	1	.buildkite/test-pipeline.yaml
6	0	benchmarks/benchmark_latency.py
111	0	tests/spec_decode/e2e/test_integration_dist_tp2.py
18	23	tests/spec_decode/e2e/{test_integration_dist.py => test_integration_dist_tp4.py}
19	5	vllm/config.py
55	21	vllm/distributed/parallel_state.py
10	0	vllm/engine/arg_utils.py
6	5	vllm/spec_decode/multi_step_worker.py
2	2	vllm/spec_decode/proposer_worker_base.py
149	0	vllm/spec_decode/smaller_tp_proposer_worker.py
10	2	vllm/spec_decode/spec_decode_worker.py

[f23871e9e] Cyrus Leung 2024-06-25 [Doc] Add notice about breaking changes to VLMs (#5818)
13	0	docs/source/models/vlm.rst

[e9de9dd55] Kevin H. Luu 2024-06-24 [ci] Remove aws template (#5757)
5	2	.buildkite/test-pipeline.yaml
0	145	.buildkite/test-template-aws.j2

[ba991d5c8] Chang Su 2024-06-24 [Bugfix] Fix FlexibleArgumentParser replaces _ with - for actual args (#5795)
7	1	vllm/utils.py

[1744cc99b] Michael Goin 2024-06-24 [Doc] Add Phi-3-medium to list of supported models (#5788)
1	1	docs/source/models/supported_models.rst

[e72dc6cb3] Michael Goin 2024-06-24 [Doc] Add "Suggest edit" button to doc pages (#5789)
1	0	docs/source/conf.py

[c24621295] youkaichao 2024-06-24 [doc][faq] add warning to download models for every nodes (#5783)
4	1	docs/source/serving/distributed_serving.rst

[edd5fe5fa] Isotr0py 2024-06-24 [Bugfix] Add phi3v resize for dynamic shape and fix torchvision requirement (#5772)
1	0	requirements-cpu.txt
2	0	requirements-cuda.txt
0	1	requirements-test.txt
4	0	tests/models/test_phi3v.py
65	4	vllm/model_executor/models/phi3v.py

[5d4d90536] Murali Andoorveedu 2024-06-23 [Distributed] Add send and recv helpers (#5719)
74	4	tests/distributed/test_comm_ops.py
2	3	tests/distributed/test_custom_all_reduce.py
12	4	tests/distributed/test_pynccl.py
1	1	tests/utils.py
2	12	vllm/distributed/device_communicators/pynccl.py
187	0	vllm/distributed/parallel_state.py

[6c916ac8a] Varun Sundar Rabindranath 2024-06-24 [BugFix] [Kernel] Add Cutlass2x fallback kernels (#5744)
8	0	csrc/quantization/cutlass_w8a8/common.hpp
46	6	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu

[832ea88fc] youkaichao 2024-06-22 [core][distributed] improve shared memory broadcast (#5754)
32	10	vllm/distributed/device_communicators/shm_broadcast.py

[8c00f9c15] Woosuk Kwon 2024-06-21 [Docs][TPU] Add installation tip for TPU (#5761)
18	0	docs/source/getting_started/tpu-installation.rst

[0cbc1d2b4] Woosuk Kwon 2024-06-21 [Bugfix] Fix pin_lora error in TPU executor (#5760)
3	0	vllm/executor/tpu_executor.py

[ff9ddbcee] zifeitong 2024-06-21 [Misc] Remove #4789 workaround left in vllm/entrypoints/openai/run_batch.py (#5756)
0	4	vllm/entrypoints/openai/run_batch.py

[9c62db07e] Jie Fu (傅杰) 2024-06-22 [Model] Support Qwen-VL and Qwen-VL-Chat models with text-only inputs (#5710)
10	0	vllm/model_executor/models/qwen.py

[cf90ae012] Kunshang Ji 2024-06-22 [CI][Hardware][Intel GPU] add Intel GPU(XPU) ci pipeline (#5616)
8	2	.buildkite/test-template-aws.j2
1	1	README.md

[f5dda63eb] rohithkrn 2024-06-21 [LoRA] Add support for pinning lora adapters in the LRU cache (#5603)
64	0	tests/lora/test_lora_manager.py
3	0	vllm/engine/llm_engine.py
3	0	vllm/executor/cpu_executor.py
7	0	vllm/executor/distributed_gpu_executor.py
4	0	vllm/executor/executor_base.py
4	0	vllm/executor/gpu_executor.py
3	0	vllm/executor/neuron_executor.py
26	0	vllm/lora/models.py
3	0	vllm/lora/worker_manager.py
38	5	vllm/utils.py
5	0	vllm/worker/model_runner.py
3	0	vllm/worker/worker.py
8	0	vllm/worker/worker_base.py

[718750730] youkaichao 2024-06-21 [ci][test] fix ca test in main (#5746)
3	0	.buildkite/test-pipeline.yaml

[f1e72cc19] zhyncs 2024-06-22 [BugFix] exclude version 1.15.0 for modelscope (#5668)
1	1	Dockerfile

[5b15bde53] Michael Goin 2024-06-21 [Doc] Documentation on supported hardware for quantization methods (#5745)
1	0	docs/source/index.rst
3	1	docs/source/quantization/fp8.rst
30	0	docs/source/quantization/supported_hardware.rst

[bd620b01f] Roger Wang 2024-06-20 [Kernel][CPU] Add Quick `gelu` to CPU (#5717)
19	0	csrc/cpu/activation.cpp
4	0	csrc/cpu/torch_bindings.cpp
3	0	vllm/_ipex_ops.py
3	0	vllm/model_executor/layers/activation.py

[d9a252bc8] youkaichao 2024-06-20 [Core][Distributed] add shm broadcast (#5399)
3	1	.buildkite/test-pipeline.yaml
82	0	tests/distributed/test_shm_broadcast.py
259	0	vllm/distributed/device_communicators/shm_broadcast.py
35	9	vllm/distributed/parallel_state.py
5	0	vllm/envs.py

[67005a07b] Jee Li 2024-06-21 [Bugfix] Add  fully sharded layer for QKVParallelLinearWithLora (#5665)
9	5	tests/lora/test_baichuan.py
5	2	tests/lora/test_layers.py
55	3	vllm/lora/fully_sharded_layers.py
21	15	vllm/lora/layers.py
3	1	vllm/lora/utils.py

[c35e4a3dd] Chang Su 2024-06-20 [BugFix] Fix test_phi3v.py (#5725)
3	1	tests/conftest.py
6	4	tests/models/test_phi3v.py

[1f5674218] Jinzhen Lin 2024-06-21 [Kernel] Add punica dimension for Qwen2 LoRA (#5441)
36	2	csrc/punica/bgmv/bgmv_config.h
17	0	tests/lora/test_punica.py

[b12518d3c] Joshua Rosenkranz 2024-06-20 [Model] MLPSpeculator speculative decoding support (#4947)
59	0	examples/offline_inference_mlpspeculator.py
6	2	tests/spec_decode/test_spec_decode_worker.py
2	2	tests/spec_decode/test_utils.py
39	15	vllm/config.py
1	0	vllm/model_executor/models/__init__.py
143	0	vllm/model_executor/models/mlp_speculator.py
46	0	vllm/sequence.py
3	3	vllm/spec_decode/batch_expansion.py
4	0	vllm/spec_decode/interfaces.py
87	0	vllm/spec_decode/mlp_speculator_worker.py
40	2	vllm/spec_decode/spec_decode_worker.py
4	0	vllm/spec_decode/top1_proposer.py
0	8	vllm/spec_decode/util.py
13	5	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
50	0	vllm/transformers_utils/configs/mlp_speculator.py
15	3	vllm/worker/model_runner.py
9	0	vllm/worker/worker.py

[6c5b7af15] youkaichao 2024-06-20 [distributed][misc] use fork by default for mp (#5669)
9	0	.buildkite/test-pipeline.yaml
27	1	vllm/distributed/device_communicators/custom_all_reduce_utils.py
2	2	vllm/envs.py

[8065a7e22] Michael Goin 2024-06-20 [Frontend] Add FlexibleArgumentParser to support both underscore and dash in names (#5718)
2	1	benchmarks/benchmark_latency.py
2	2	benchmarks/benchmark_prefix_caching.py
6	1	benchmarks/benchmark_serving.py
2	1	benchmarks/benchmark_throughput.py
2	1	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
2	2	benchmarks/kernels/benchmark_aqlm.py
2	2	benchmarks/kernels/benchmark_marlin.py
2	1	benchmarks/kernels/benchmark_moe.py
3	3	benchmarks/kernels/benchmark_paged_attention.py
2	2	benchmarks/kernels/benchmark_rope.py
2	2	benchmarks/overheads/benchmark_hashing.py
2	3	examples/aqlm_example.py
2	1	examples/llm_engine_example.py
2	2	examples/save_sharded_state.py
2	1	examples/tensorize_vllm_model.py
2	2	tests/async_engine/api_server_async_engine.py
8	9	vllm/engine/arg_utils.py
2	3	vllm/entrypoints/api_server.py
2	1	vllm/entrypoints/openai/cli_args.py
2	3	vllm/entrypoints/openai/run_batch.py
2	2	vllm/model_executor/model_loader/tensorizer.py
19	0	vllm/utils.py

[3f3b6b215] Tyler Michael Smith 2024-06-20 [Bugfix] Fix the CUDA version check for FP8 support in the CUTLASS kernels (#5715)
2	0	csrc/ops.h
16	0	csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu
6	0	csrc/torch_bindings.cpp
4	0	vllm/_custom_ops.py
2	13	vllm/model_executor/layers/quantization/fp8.py

[a7dcc6208] Varun Sundar Rabindranath 2024-06-20 [Kernel] Update Cutlass int8 kernel configs for SM80 (#5275)
7	0	csrc/quantization/cutlass_w8a8/common.hpp
116	11	csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu
0	5	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu

[ad137cd11] Roger Wang 2024-06-20 [Model] Port over CLIPVisionModel for VLMs (#5591)
12	0	csrc/activation_kernels.cu
2	0	csrc/ops.h
4	0	csrc/torch_bindings.cpp
4	0	vllm/_custom_ops.py
16	0	vllm/model_executor/layers/activation.py
203	0	vllm/model_executor/models/clip.py
9	8	vllm/model_executor/models/llava.py
10	9	vllm/model_executor/models/llava_next.py
9	4	vllm/model_executor/models/phi3v.py

[111af1fa2] Varun Sundar Rabindranath 2024-06-20 [Kernel] Update Cutlass int8 kernel configs for SM90 (#5514)
143	22	csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu

[1b2eaac31] Roger Wang 2024-06-19 [Bugfix][Doc] FIx Duplicate Explicit Target Name Errors (#5703)
2	2	docs/source/dev/dockerfile/dockerfile.rst

[3730a1c83] Cyrus Leung 2024-06-20 [Misc] Improve conftest (#5681)
3	1	tests/conftest.py

[949e49a68] Kevin H. Luu 2024-06-19 [ci] Limit num gpus if specified for A100 (#5694)
1	0	.buildkite/test-pipeline.yaml
1	1	.buildkite/test-template-aws.j2

[4a30d7e3c] Dipika Sikka 2024-06-19 [Misc] Add per channel support for static activation quantization; update w8a8 schemes to share base classes (#5650)
10	4	tests/quantization/test_compressed_tensors.py
7	3	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
84	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8.py
10	79	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py
10	50	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py

[e83db9e7e] Rafael Vasquez 2024-06-19 [Doc] Update docker references (#5614)
10	10	docs/source/dev/dockerfile/dockerfile.rst
3	4	docs/source/serving/deploying_with_docker.rst

[78687504f] zifeitong 2024-06-19 [Bugfix] AsyncLLMEngine hangs with asyncio.run (#5654)
37	1	tests/async_engine/test_async_llm_engine.py
1	42	tests/spec_decode/e2e/conftest.py
41	2	tests/utils.py
3	2	vllm/engine/async_llm_engine.py
189	0	vllm/engine/async_timeout.py

[d571ca010] youkaichao 2024-06-19 [ci][distributed] add tests for custom allreduce (#5689)
6	2	.buildkite/test-pipeline.yaml
4	3	tests/distributed/test_custom_all_reduce.py

[afed90a03] Michael Goin 2024-06-19 [Frontend][Bugfix] Fix preemption_mode -> preemption-mode for CLI arg in arg_utils.py (#5688)
1	1	vllm/engine/arg_utils.py

[3ee5c4bca] Kevin H. Luu 2024-06-19 [ci] Add A100 queue into AWS CI template (#5648)
1	0	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
5	0	.buildkite/test-pipeline.yaml
46	0	.buildkite/test-template-aws.j2

[e9c2732b9] Cyrus Leung 2024-06-19 [CI/Build] Add tqdm to dependencies (#5680)
1	0	requirements-common.txt

[d8714530d] DearPlanet 2024-06-19 [Misc]Add param max-model-len in benchmark_latency.py (#5629)
7	0	benchmarks/benchmark_latency.py

[7d46c8d37] Isotr0py 2024-06-19 [Bugfix] Fix sampling_params passed incorrectly in Phi3v example (#5684)
6	6	examples/phi3v_example.py

[da971ec7a] Michael Goin 2024-06-19 [Model] Add FP8 kv cache for Qwen2 (#5656)
14	0	vllm/model_executor/models/qwen2.py

[3eea74889] youkaichao 2024-06-19 [misc][distributed] use 127.0.0.1 for single-node (#5619)
5	2	vllm/executor/multiproc_gpu_executor.py
10	0	vllm/executor/ray_gpu_executor.py

[f758aed0e] Hongxia Yang 2024-06-19 [Bugfix][CI/Build][AMD][ROCm]Fixed the cmake build bug which generate garbage on certain devices (#5641)
8	9	Dockerfile.rocm
4	1	cmake/utils.cmake

[e5150f2c2] Thomas Parnell 2024-06-19 [Bugfix] Added test for sampling repetition penalty bug. (#5659)
69	0	tests/samplers/test_sampler.py

[59a1eb59c] Shukant Pal 2024-06-18 [Bugfix] Fix Phi-3 Long RoPE scaling implementation (#5628)
14	4	vllm/model_executor/layers/rotary_embedding.py

[6820724e5] Tyler Michael Smith 2024-06-18 [Bugfix] Fix w8a8 benchmarks for int8 case (#5643)
2	3	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py

[b23ce9203] Tyler Michael Smith 2024-06-18 [Bugfix] Fix CUDA version check for mma warning suppression (#5642)
2	1	csrc/quantization/marlin/sparse/common/mma.h

[2bd231a7b] milo157 2024-06-18 [Doc] Added cerebrium as Integration option (#5553)
109	0	docs/source/serving/deploying_with_cerebrium.rst
1	0	docs/source/serving/integrations.rst

[8a173382c] Thomas Parnell 2024-06-18 [Bugfix] Fix for inconsistent behaviour related to sampling and repetition penalties  (#5639)
14	8	vllm/model_executor/sampling_metadata.py

[07feecde1] sergey-tinkoff 2024-06-18 [Model] LoRA support added for command-r (#5178)
6	0	csrc/punica/bgmv/bgmv_config.h
2	0	tests/lora/test_punica.py
42	6	vllm/model_executor/models/commandr.py

[19091efc4] Kevin H. Luu 2024-06-18 [ci] Setup Release pipeline and build release wheels with cache (#5610)
21	0	.buildkite/release-pipeline.yaml
41	17	Dockerfile

[95db455e7] Dipika Sikka 2024-06-18 [Misc] Add channel-wise quantization support for w8a8 dynamic per token activation quantization (#5542)
9	4	tests/quantization/test_compressed_tensors.py
0	13	vllm/model_executor/layers/linear.py
8	6	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
28	9	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py

[7879f24dc] Ronen Schaffer 2024-06-18 [Misc] Add OpenTelemetry support (#4687)
9	0	.buildkite/test-pipeline.yaml
28	20	benchmarks/benchmark_latency.py
82	0	examples/production_monitoring/Otel.md
35	0	examples/production_monitoring/dummy_client.py
0	0	tests/tracing/__init__.py
116	0	tests/tracing/test_tracing.py
13	0	vllm/config.py
28	12	vllm/engine/arg_utils.py
22	0	vllm/engine/async_llm_engine.py
93	9	vllm/engine/llm_engine.py
11	0	vllm/entrypoints/openai/serving_chat.py
11	0	vllm/entrypoints/openai/serving_completion.py
3	0	vllm/sequence.py
104	0	vllm/tracing.py
12	0	vllm/utils.py

[13db4369d] Kevin H. Luu 2024-06-18 [ci] Deprecate original CI template (#5624)
1	1	.buildkite/test-pipeline.yaml
0	101	.buildkite/test-template.j2

[4ad7b53e5] Roger Wang 2024-06-18 [CI/Build][Misc] Update Pytest Marker for VLMs (#5623)
1	1	.buildkite/run-cpu-test.sh
3	3	.buildkite/test-pipeline.yaml
1	1	pyproject.toml
1	1	tests/models/test_llava.py
1	1	tests/models/test_llava_next.py
1	1	tests/models/test_phi3v.py

[f0cc0e68e] Chang Su 2024-06-18 [Misc] Remove import from transformers logging (#5625)
0	3	vllm/model_executor/models/phi3v.py

[db5ec52ad] youkaichao 2024-06-18 [bugfix][distributed] improve p2p capability test (#5612)
8	2	vllm/distributed/device_communicators/custom_all_reduce_utils.py

[114d7270f] Kuntai Du 2024-06-17 [CI] Avoid naming different metrics with the same name in performance benchmark (#5615)
14	7	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py

[32c86e494] Cyrus Leung 2024-06-18 [Misc] Fix typo (#5618)
1	1	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py

[8eadcf0b9] youkaichao 2024-06-17 [misc][typo] fix typo (#5620)
1	1	vllm/block.py

[5002175e8] Joe Runde 2024-06-17 [Kernel] Add punica dimensions for Granite 13b (#5559)
8	0	csrc/punica/bgmv/bgmv_config.h
4	0	tests/lora/test_punica.py

[daef218b5] Isotr0py 2024-06-18 [Model] Initialize Phi-3-vision support (#4986)
4	0	docs/source/models/supported_models.rst
57	0	examples/phi3v_example.py
1	0	requirements-test.txt
3	0	tests/conftest.py
124	0	tests/models/test_phi3v.py
1	0	vllm/model_executor/models/__init__.py
379	0	vllm/model_executor/models/phi3v.py
2	0	vllm/multimodal/utils.py

[fa9e38522] sroy745 2024-06-17 [Speculative Decoding 1/2 ] Add typical acceptance sampling as one of the sampling techniques in the verifier (#5131)
464	0	tests/samplers/test_typical_acceptance_sampler.py
10	164	vllm/model_executor/layers/rejection_sampler.py
206	0	vllm/model_executor/layers/spec_decode_base_sampler.py
186	0	vllm/model_executor/layers/typical_acceptance_sampler.py

[26e1188e5] zifeitong 2024-06-17 [Fix] Use utf-8 encoding in entrypoints/openai/run_batch.py (#5606)
2	2	vllm/entrypoints/openai/run_batch.py

[a3e8a05d4] Bruce Fontaine 2024-06-17 [Bugfix] Fix KV head calculation for MPT models when using GQA (#5142)
5	1	vllm/config.py

[e441bad67] youkaichao 2024-06-17 [Optimization] use a pool to reuse LogicalTokenBlock.token_ids (#5584)
39	2	vllm/block.py

[1b44aaf4e] youkaichao 2024-06-17 [bugfix][distributed] fix 16 gpus local rank arrangement (#5604)
6	0	vllm/executor/ray_gpu_executor.py

[9e4e6fe20] Kuntai Du 2024-06-17 [CI] the readability of benchmarking and prepare for dashboard (#5571)
13	8	.buildkite/nightly-benchmarks/README.md
3	3	.buildkite/nightly-benchmarks/run-benchmarks-suite.sh
145	115	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
67	0	.buildkite/nightly-benchmarks/tests/descriptions.md
1	1	.buildkite/nightly-benchmarks/{ => tests}/latency-tests.json
1	1	.buildkite/nightly-benchmarks/{ => tests}/serving-tests.json
1	1	.buildkite/nightly-benchmarks/{ => tests}/throughput-tests.json
1	1	benchmarks/benchmark_latency.py

[ab66536db] Jie Fu (傅杰) 2024-06-18 [CI/BUILD] Support non-AVX512 vLLM building and testing (#5574)
4	1	.buildkite/run-cpu-test.sh
4	0	Dockerfile.cpu
12	1	cmake/cpu_extension.cmake

[728c4c8a0] Kunshang Ji 2024-06-18 [Hardware][Intel GPU] Add Intel GPU(XPU) inference backend (#3814)
14	0	.buildkite/run-xpu-test.sh
5	0	.buildkite/test-template.j2
22	0	Dockerfile.xpu
1	1	benchmarks/benchmark_latency.py
1	1	benchmarks/benchmark_throughput.py
61	0	docs/source/getting_started/xpu-installation.rst
1	0	docs/source/index.rst
11	0	requirements-xpu.txt
8	0	setup.py
2	1	vllm/_custom_ops.py
241	0	vllm/_ipex_ops.py
355	0	vllm/attention/backends/ipex_attn.py
13	2	vllm/attention/selector.py
3	1	vllm/config.py
1	1	vllm/distributed/parallel_state.py
6	5	vllm/engine/arg_utils.py
11	0	vllm/engine/async_llm_engine.py
8	0	vllm/engine/llm_engine.py
2	2	vllm/executor/ray_utils.py
401	0	vllm/executor/ray_xpu_executor.py
98	0	vllm/executor/xpu_executor.py
4	4	vllm/model_executor/custom_op.py
35	0	vllm/model_executor/layers/activation.py
24	0	vllm/model_executor/layers/layernorm.py
23	0	vllm/model_executor/layers/rotary_embedding.py
1	1	vllm/model_executor/layers/vocab_parallel_embedding.py
29	2	vllm/utils.py
5	2	vllm/worker/cache_engine.py
2	1	vllm/worker/worker.py
417	0	vllm/worker/xpu_model_runner.py
193	0	vllm/worker/xpu_worker.py

[1f12122b1] zhyncs 2024-06-18 [Misc] use AutoTokenizer for benchmark serving when vLLM not installed (#5588)
28	1	benchmarks/backend_request_func.py
4	1	benchmarks/benchmark_serving.py

[890d8d960] Dipika Sikka 2024-06-17 [Kernel] `compressed-tensors` marlin 24 support (#5435)
20	3	tests/quantization/test_compressed_tensors.py
32	16	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
134	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py
8	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[9e74d9d00] Charles Riggins 2024-06-18 Correct alignment in the seq_len diagram. (#5592)
1	1	vllm/attention/backends/flash_attn.py

[9333fb8eb] Amit Garg 2024-06-17 [Model] Rename Phi3 rope scaling type (#5595)
4	1	vllm/config.py
12	7	vllm/model_executor/layers/rotary_embedding.py

[e2b85cf86] Cody Yu 2024-06-16 Fix w8a8 benchmark and add Llama-3-8B (#5562)
13	8	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
6	0	benchmarks/cutlass_benchmarks/weight_shapes.py

[845a3f26f] youkaichao 2024-06-16 [Doc] add debugging tips for crash and multi-node debugging (#5581)
16	3	docs/source/getting_started/debugging.rst

[f07d51332] youkaichao 2024-06-16 [build][misc] limit numpy version (#5582)
1	1	requirements-common.txt

[4a6769053] Michael Goin 2024-06-16 [CI][BugFix] Flip is_quant_method_supported condition (#5577)
1	1	tests/quantization/utils.py

[f31c1f90e] Antoni Baum 2024-06-16 Add basic correctness 2 GPU tests to 4 GPU pipeline (#5518)
6	2	.buildkite/test-pipeline.yaml

[3ce2c050d] zifeitong 2024-06-15 [Fix] Correct OpenAI batch response format (#5554)
12	1	vllm/entrypoints/openai/protocol.py
13	4	vllm/entrypoints/openai/run_batch.py

[1c0afa13c] Nick Hill 2024-06-15 [BugFix] Don't start a Ray cluster when not using Ray (#5570)
7	2	vllm/config.py

[d919ecc77] Alexander Matveev 2024-06-15 add gptq_marlin test for bug report https://github.com/vllm-project/vllm/issues/5088 (#5145)
3	0	tests/models/test_gptq_marlin.py

[e691918e3] SangBin Cho 2024-06-15 [misc] Do not allow to use lora with chunked prefill. (#5538)
2	0	vllm/config.py

[81fbb3655] Cyrus Leung 2024-06-15 [CI/Build] Test both text and token IDs in batched OpenAI Completions API (#5568)
45	43	tests/entrypoints/test_openai_server.py

[0e9164b40] Cyrus Leung 2024-06-15 [mypy] Enable type checking for test directory (#5017)
1	1	.github/workflows/mypy.yaml
9	9	benchmarks/benchmark_serving.py
2	2	benchmarks/benchmark_throughput.py
5	5	benchmarks/kernels/benchmark_aqlm.py
5	3	benchmarks/kernels/benchmark_marlin.py
18	8	benchmarks/kernels/benchmark_moe.py
7	4	benchmarks/kernels/benchmark_paged_attention.py
4	3	benchmarks/kernels/benchmark_rope.py
6	6	examples/fp8/extract_scales.py
4	4	examples/offline_inference_distributed.py
1	1	format.sh
5	3	tests/core/block/test_block_table.py
2	2	tests/core/block/test_prefix_caching_block.py
5	5	tests/core/test_chunked_prefill_scheduler.py
26	26	tests/core/test_scheduler.py
7	5	tests/core/utils.py
3	2	tests/distributed/test_pynccl.py
3	2	tests/distributed/test_utils.py
3	2	tests/entrypoints/test_openai_server.py
17	16	tests/kernels/test_attention.py
11	11	tests/kernels/test_blocksparse_attention.py
17	15	tests/kernels/test_cache.py
2	2	tests/kernels/test_cutlass.py
2	2	tests/kernels/test_flash_attn.py
12	16	tests/kernels/test_pos_encoding.py
16	5	tests/lora/conftest.py
23	1	tests/lora/data/long_context_test_data.py
4	2	tests/lora/test_baichuan.py
4	2	tests/lora/test_chatglm3.py
4	2	tests/lora/test_gemma.py
3	3	tests/lora/test_layer_variation.py
13	10	tests/lora/test_layers.py
4	2	tests/lora/test_llama.py
9	6	tests/lora/test_long_context.py
3	1	tests/lora/test_lora_checkpoints.py
3	3	tests/lora/test_lora_manager.py
4	2	tests/lora/test_mixtral.py
4	2	tests/lora/test_phi.py
5	2	tests/lora/test_quant_model.py
9	9	tests/lora/utils.py
2	1	tests/models/test_fp8.py
4	1	tests/prefix_caching/test_prefix_caching.py
2	1	tests/quantization/test_configs.py
7	4	tests/samplers/test_logprobs.py
2	2	tests/samplers/test_rejection_sampler.py
23	18	tests/samplers/test_sampler.py
7	6	tests/spec_decode/e2e/conftest.py
4	2	tests/spec_decode/test_batch_expansion.py
12	7	tests/spec_decode/test_multi_step_worker.py
12	5	tests/spec_decode/test_spec_decode_worker.py
9	5	tests/spec_decode/utils.py
1	1	tests/test_cache_block_hashing.py
1	0	tests/test_logger.py
2	2	tests/tokenization/test_detokenize.py
1	1	tests/utils.py
12	11	tests/worker/test_model_runner.py
2	2	vllm/attention/backends/torch_sdpa.py
2	2	vllm/attention/backends/xformers.py
1	1	vllm/core/block/block_table.py
1	1	vllm/core/block/naive_block.py
1	1	vllm/core/block/prefix_caching_block.py
1	1	vllm/core/block_manager_v2.py
4	4	vllm/distributed/device_communicators/custom_all_reduce_utils.py
1	1	vllm/distributed/device_communicators/pynccl_wrapper.py
2	2	vllm/engine/llm_engine.py
2	2	vllm/engine/metrics.py
3	3	vllm/engine/output_processor/single_step.py
2	1	vllm/entrypoints/openai/run_batch.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_embedding.py
2	1	vllm/lora/lora.py
1	1	vllm/lora/worker_manager.py
1	1	vllm/model_executor/layers/linear.py
6	5	vllm/model_executor/layers/quantization/gptq_marlin.py
10	8	vllm/model_executor/layers/quantization/utils/marlin_24_perms.py
10	8	vllm/model_executor/layers/quantization/utils/marlin_perms.py
14	11	vllm/model_executor/layers/sampler.py
4	3	vllm/model_executor/model_loader/loader.py
1	1	vllm/model_executor/model_loader/weight_utils.py
2	2	vllm/model_executor/models/__init__.py
2	2	vllm/model_executor/models/arctic.py
2	2	vllm/model_executor/models/commandr.py
2	2	vllm/model_executor/models/gemma.py
1	1	vllm/sequence.py
5	5	vllm/spec_decode/multi_step_worker.py
3	3	vllm/spec_decode/ngram_worker.py
4	4	vllm/spec_decode/spec_decode_worker.py
2	2	vllm/spec_decode/util.py
1	1	vllm/transformers_utils/detokenizer.py
23	15	vllm/utils.py
2	2	vllm/worker/model_runner.py
2	2	vllm/worker/worker_base.py

[1b8a0d71c] leiwen83 2024-06-15 [Core][Bugfix]: fix prefix caching for blockv2 (#5364)
67	0	tests/core/block/e2e/test_correctness.py
5	2	vllm/core/block/prefix_caching_block.py

[bd7efe95d] Simon Mo 2024-06-14 Add ccache to amd (#5555)
1	0	.buildkite/test-template-aws.j2
4	1	Dockerfile.rocm

[f5bb85b43] youkaichao 2024-06-14 [Core][Distributed] improve p2p cache generation (#5528)
146	0	vllm/distributed/device_communicators/cuda_wrapper.py
119	96	vllm/distributed/device_communicators/custom_all_reduce_utils.py

[28c145eb5] Woosuk Kwon 2024-06-14 [Bugfix] Fix typo in Pallas backend (#5558)
1	1	vllm/attention/backends/pallas.py

[e2afb03c9] Thomas Parnell 2024-06-14 [Bugfix] Enable loading FP8 checkpoints for gpt_bigcode models  (#5460)
7	1	vllm/model_executor/models/gpt_bigcode.py

[6e2527a7c] Sanger Steel 2024-06-14 [Doc] Update documentation on Tensorizer (#5471)
1	0	docs/source/index.rst
12	0	docs/source/serving/tensorizer.rst
1	1	vllm/engine/arg_utils.py

[cdab68dcd] Simon Mo 2024-06-14 [Docs] Add ZhenFund as a Sponsor (#5548)
1	0	README.md
1	0	docs/source/community/sponsors.md

[d1c3d7d13] youkaichao 2024-06-14 [misc][distributed] fix benign error in `is_in_the_same_node` (#5512)
10	10	vllm/distributed/parallel_state.py

[77490c6f2] Cyrus Leung 2024-06-15 [Core] Remove duplicate processing in async engine (#5525)
1	13	vllm/engine/async_llm_engine.py

[48f589e18] youkaichao 2024-06-14 [mis] fix flaky test of test_cuda_device_count_stateless (#5546)
2	1	tests/distributed/test_utils.py

[348616ac4] Tyler Michael Smith 2024-06-14 [Kernel] Suppress mma.sp warning on CUDA 12.5 and later (#5401)
42	32	csrc/quantization/marlin/sparse/common/mma.h

[15985680e] Robert Shaw 2024-06-14 [ Misc ] Rs/compressed tensors cleanup (#5432)
1	1	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
12	9	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16.py
8	10	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py
0	16	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py

[d74674bbd] Allen.Dou 2024-06-15 [Misc] Fix arg names (#5524)
1	1	benchmarks/kernels/benchmark_paged_attention.py
1	1	examples/aqlm_example.py
4	4	examples/fp8/extract_scales.py

[703475f6c] Tyler Michael Smith 2024-06-14 [Kernel] Fix CUTLASS 3.x custom broadcast load epilogue (#5516)
1	1	csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c3x.hpp
1	3	vllm/model_executor/layers/quantization/fp8.py

[d47af2bc0] Cyrus Leung 2024-06-15 [CI/Build] Disable LLaVA-NeXT CPU test (#5529)
1	2	.buildkite/run-cpu-test.sh

[319ad7f1d] Kuntai Du 2024-06-13 [CI/Build][Misc] Add CI that benchmarks vllm performance on those PRs with `perf-benchmarks` label (#5073)
98	0	.buildkite/nightly-benchmarks/README.md
61	0	.buildkite/nightly-benchmarks/benchmark-pipeline.yaml
2	1	.buildkite/nightly-benchmarks/kickoff-pipeline.sh
32	0	.buildkite/nightly-benchmarks/latency-tests.json
358	0	.buildkite/nightly-benchmarks/run-benchmarks-suite.sh
0	39	.buildkite/nightly-benchmarks/sample.yaml
155	0	.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py
17	0	.buildkite/nightly-benchmarks/scripts/wait-for-image.sh
59	0	.buildkite/nightly-benchmarks/serving-tests.json
35	0	.buildkite/nightly-benchmarks/throughput-tests.json
25	0	benchmarks/benchmark_latency.py
11	0	benchmarks/benchmark_serving.py
27	1	benchmarks/benchmark_throughput.py

[0f0d8bc06] Simon Mo 2024-06-13 bump version to v0.5.0.post1 (#5522)
1	1	vllm/version.py

[55d6361b1] Allen.Dou 2024-06-14 [Misc] Fix arg names in quantizer script (#5507)
8	8	examples/fp8/quantizer/quantize.py

[cd9c0d65d] Jie Fu (傅杰) 2024-06-14 [Hardware][Intel] Support CPU inference with AVX2 ISA (#5452)
5	1	cmake/cpu_extension.cmake
164	1	csrc/cpu/cpu_types.hpp

[50eed24d2] Antoni Baum 2024-06-13 Add `cuda_device_count_stateless` (#5473)
1	0	.buildkite/test-pipeline.yaml
2	15	tests/conftest.py
31	0	tests/distributed/test_utils.py
3	3	vllm/config.py
2	1	vllm/distributed/device_communicators/custom_all_reduce.py
2	1	vllm/distributed/device_communicators/custom_all_reduce_utils.py
3	3	vllm/executor/multiproc_gpu_executor.py
35	0	vllm/utils.py

[e38042d4a] Tyler Michael Smith 2024-06-13 [Kernel] Disable CUTLASS kernels for fp8 (#5505)
3	1	vllm/model_executor/layers/quantization/fp8.py

[33e3b3724] Tyler Michael Smith 2024-06-13 [CI/Build] Disable test_fp8.py (#5508)
8	0	tests/models/test_fp8.py

[1696efe6c] youkaichao 2024-06-13 [misc] fix format.sh (#5511)
6	6	format.sh

[6b0511a57] Antoni Baum 2024-06-13 Revert "[Core] Remove unnecessary copies in flash attn backend" (#5478)
6	7	vllm/attention/backends/flash_attn.py

[a8fda4f66] Antoni Baum 2024-06-13 Seperate dev requirements into lint and test (#5474)
2	0	Dockerfile
4	36	requirements-dev.txt
14	0	requirements-lint.txt
22	0	requirements-test.txt

[30299a41f] Cody Yu 2024-06-13 [MISC] Remove FP8 warning (#5472)
1	1	vllm/config.py

[85657b560] Tyler Michael Smith 2024-06-13 [Kernel] Factor out epilogues from cutlass kernels (#5391)
4	4	CMakeLists.txt
1	5	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
3	3	csrc/ops.h
111	88	csrc/quantization/cutlass_w8a8/{scaled_mm_dq_c2x.cu => scaled_mm_c2x.cu}
109	85	csrc/quantization/cutlass_w8a8/{scaled_mm_dq_c3x.cu => scaled_mm_c3x.cu}
24	24	csrc/quantization/cutlass_w8a8/{scaled_mm_dq_entry.cu => scaled_mm_entry.cu}
4	4	csrc/torch_bindings.cpp
9	9	tests/kernels/test_cutlass.py
4	5	vllm/_custom_ops.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py
2	2	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py
1	1	vllm/model_executor/layers/quantization/fp8.py

[0ce7b952f] Cyrus Leung 2024-06-14 [Doc] Update LLaVA docs (#5437)
2	2	docs/source/models/vlm.rst
16	13	vllm/model_executor/models/llava.py
11	23	vllm/model_executor/models/llava_next.py

[39873476f] Cyrus Leung 2024-06-14 [CI/Build] Simplify OpenAI server setup in tests (#5100)
15	16	tests/async_engine/test_openapi_server_ray.py
113	0	tests/entrypoints/test_openai_embedding.py
47	159	tests/entrypoints/test_openai_server.py
17	18	tests/entrypoints/test_openai_vision.py
5	9	tests/tensorizer_loader/test_tensorizer.py
88	36	tests/utils.py

[03dccc886] Cyrus Leung 2024-06-14 [Misc] Add vLLM version getter to utils (#5098)
1	1	setup.py
2	1	vllm/__init__.py
2	2	vllm/engine/llm_engine.py
3	3	vllm/entrypoints/openai/api_server.py
2	2	vllm/entrypoints/openai/run_batch.py
2	2	vllm/usage/usage_lib.py
1	0	vllm/version.py

[a65634d3a] Woosuk Kwon 2024-06-13 [Docs] Add 4th meetup slides (#5509)
1	7	README.md
1	0	docs/source/community/meetups.rst

[80aa7e91f] Li, Jiang 2024-06-14 [Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)
6	2	Dockerfile.cpu
1	1	README.md
21	2	docs/source/getting_started/cpu-installation.rst
1	1	requirements-cpu.txt
16	7	vllm/attention/backends/torch_sdpa.py
120	0	vllm/attention/ops/ipex_attn.py

[bd4397352] wenyujin333 2024-06-14 [Kernel] Tune Qwen2MoE kernel configurations with tp2,4 (#5497)
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=1280,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=64,N=640,device_name=NVIDIA_H100_80GB_HBM3.json

[23ec72fa0] Michael Goin 2024-06-13 [CI/Build][REDO] Add is_quant_method_supported to control quantization test configurations (#5466)
2	11	tests/models/test_aqlm.py
2	10	tests/models/test_fp8.py
2	11	tests/models/test_gptq_marlin.py
2	11	tests/models/test_gptq_marlin_24.py
2	11	tests/models/test_marlin.py
3	7	tests/quantization/test_bitsandbytes.py
5	10	tests/quantization/test_fp8.py
14	0	tests/quantization/utils.py

[c2637a613] Dipika Sikka 2024-06-13 [Kernel] `w4a16` support for `compressed-tensors` (#5385)
25	2	tests/quantization/test_compressed_tensors.py
36	8	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
1	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
168	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16.py

[88407532e] Wang, Yi 2024-06-13 [Bugfix]if the content is started with ":"(response of ping), client should i… (#5303)
6	2	benchmarks/backend_request_func.py

[916d219d6] Kevin H. Luu 2024-06-12 [ci] Use sccache to build images (#5419)
1	1	.buildkite/test-template-aws.j2
20	2	Dockerfile
1	1	setup.py

[ea3890a5f] youkaichao 2024-06-12 [Core][Distributed] code deduplication in tp&pp with coordinator(#5293)
3	1	tests/conftest.py
3	3	tests/distributed/test_custom_all_reduce.py
8	4	tests/distributed/test_pynccl.py
13	10	tests/lora/conftest.py
3	1	tests/worker/test_model_runner.py
1	1	vllm/attention/backends/pallas.py
13	298	vllm/distributed/communication_op.py
4	9	vllm/distributed/device_communicators/custom_all_reduce.py
3	4	vllm/distributed/device_communicators/custom_all_reduce_utils.py
3	8	vllm/distributed/device_communicators/pynccl.py
567	242	vllm/distributed/parallel_state.py
1	1	vllm/worker/model_runner.py

[2135cacb4] Isotr0py 2024-06-13 [Bugfix] Fix wrong multi_modal_input format for CPU runner (#5451)
2	2	vllm/worker/cpu_model_runner.py

[7d19de2e9] Michael Goin 2024-06-12 [Frontend] Add "input speed" to tqdm postfix alongside output speed (#5425)
12	5	vllm/entrypoints/llm.py

[94a07bbdd] Michael Goin 2024-06-12 [Bugfix] Fix typo in scheduler.py (requeset -> request) (#5470)
10	10	vllm/core/scheduler.py

[b8d4dfff9] Cyrus Leung 2024-06-13 [Doc] Update debug docs (#5438)
12	9	docs/source/getting_started/debugging.rst

[622d45128] youkaichao 2024-06-12 [misc] add hint for AttributeError (#5462)
46	2	vllm/_custom_ops.py

[51602eefd] Travis Johnson 2024-06-12  [Frontend] [Core] Support for sharded tensorized models (#4990)
60	65	examples/tensorize_vllm_model.py
90	9	tests/tensorizer_loader/test_tensorizer.py
17	1	vllm/model_executor/model_loader/loader.py
75	32	vllm/model_executor/model_loader/tensorizer.py
11	0	vllm/worker/model_runner.py
8	0	vllm/worker/worker.py

[5cc50a531] Arthur Kim 2024-06-13 [Bugfix] TYPE_CHECKING for MultiModalData (#5444)
1	1	vllm/inputs.py

[5985e3427] Cody Yu 2024-06-12 [Kernel] Vectorized FP8 quantize kernel (#5396)
47	6	csrc/quantization/fp8/common.cu
47	0	tests/quantization/test_fp8.py

[8b82a8999] Kevin H. Luu 2024-06-12 [ci] Add AMD, Neuron, Intel tests for AWS CI and turn off default soft fail for GPU tests (#5464)
29	1	.buildkite/test-template-aws.j2

[c3c2903e7] Li, Jiang 2024-06-13 [Bugfix] Add device assertion to TorchSDPA (#5402)
3	0	vllm/attention/selector.py

[1a8bfd92d] Woosuk Kwon 2024-06-12 [Hardware] Initial TPU integration (#5292)
19	0	Dockerfile.tpu
1	1	benchmarks/benchmark_latency.py
1	1	benchmarks/benchmark_throughput.py
75	0	docs/source/getting_started/tpu-installation.rst
2	1	docs/source/index.rst
7	0	requirements-tpu.txt
17	5	setup.py
232	0	vllm/attention/backends/pallas.py
11	2	vllm/attention/selector.py
5	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
3	0	vllm/engine/async_llm_engine.py
3	0	vllm/engine/llm_engine.py
6	0	vllm/envs.py
101	0	vllm/executor/tpu_executor.py
3	1	vllm/model_executor/custom_op.py
74	3	vllm/model_executor/layers/rotary_embedding.py
21	6	vllm/model_executor/model_loader/loader.py
14	0	vllm/utils.py
3	6	vllm/worker/cache_engine.py
525	0	vllm/worker/tpu_model_runner.py
198	0	vllm/worker/tpu_worker.py

[847cdcca1] SangBin Cho 2024-06-13 [CI] Upgrade codespell version. (#5381)
1	1	.github/workflows/ruff.yml
1	1	requirements-dev.txt
1	1	tests/core/test_chunked_prefill_scheduler.py
1	1	tests/test_sharded_state_loader.py

[e3c12bf6d] Simon Mo 2024-06-12 Revert "[CI/Build] Add `is_quant_method_supported` to control quantization test configurations" (#5463)
11	2	tests/models/test_aqlm.py
10	2	tests/models/test_fp8.py
11	2	tests/models/test_gptq_marlin.py
11	2	tests/models/test_gptq_marlin_24.py
11	2	tests/models/test_marlin.py
7	3	tests/quantization/test_bitsandbytes.py
7	3	tests/quantization/test_fp8.py
0	14	tests/quantization/utils.py

[3dd6853bc] Michael Goin 2024-06-12 [CI/Build] Add `is_quant_method_supported` to control quantization test configurations (#5253)
2	11	tests/models/test_aqlm.py
2	10	tests/models/test_fp8.py
2	11	tests/models/test_gptq_marlin.py
2	11	tests/models/test_gptq_marlin_24.py
2	11	tests/models/test_marlin.py
3	7	tests/quantization/test_bitsandbytes.py
3	7	tests/quantization/test_fp8.py
14	0	tests/quantization/utils.py

[8f89d7209] youkaichao 2024-06-11 [Doc] add common case for long waiting time (#5430)
7	1	docs/source/getting_started/debugging.rst

[99dac099a] Nick Hill 2024-06-11 [Core][Doc] Default to multiprocessing for single-node distributed case (#5230)
5	5	docs/source/serving/distributed_serving.rst
4	0	tests/spec_decode/e2e/conftest.py
17	1	vllm/config.py
0	4	vllm/executor/multiproc_gpu_executor.py
5	4	vllm/executor/multiproc_worker_utils.py

[c4bd03c7c] youkaichao 2024-06-11 [Core][Distributed] add same-node detection (#5369)
1	0	.buildkite/test-pipeline.yaml
11	0	tests/distributed/test_same_node.py
8	1	vllm/distributed/device_communicators/custom_all_reduce.py
67	0	vllm/distributed/parallel_state.py

[dcbf4286a] sasha0552 2024-06-11 [Frontend] Customizable RoPE theta (#5197)
6	1	tests/test_config.py
3	1	vllm/config.py
8	0	vllm/engine/arg_utils.py
2	1	vllm/engine/llm_engine.py
8	5	vllm/transformers_utils/config.py

[00e6a2dc5] Ali Panahi 2024-06-11 [Bugfix] fix lora_dtype value type in arg_utils.py (#5398)
1	1	vllm/engine/arg_utils.py

[2e02311a1] Junichi Sato 2024-06-12 [Bugfix] Fix `MultiprocessingGPUExecutor.check_health` when world_size == 1 (#5254)
3	1	vllm/executor/multiproc_gpu_executor.py

[89ec06c33] Cade Daniel 2024-06-11 [Docs] [Spec decode] Fix docs error in code example (#5427)
2	0	docs/source/models/spec_decode.rst

[9fde251bf] Kuntai Du 2024-06-11 [Doc] Add an automatic prefix caching section in vllm documentation (#5324)
110	0	docs/source/automatic_prefix_caching/apc.rst
43	0	docs/source/automatic_prefix_caching/details.md
7	1	docs/source/index.rst

[4c2ffb28f] Cade Daniel 2024-06-11 [Speculative decoding] Initial spec decode docs (#5400)
1	0	docs/source/index.rst
75	0	docs/source/models/spec_decode.rst

[246598a6b] SangBin Cho 2024-06-11 [CI] docfix (#5410)
2	1	docs/source/models/vlm.rst
10	6	docs/source/quantization/fp8.rst

[8bab4959b] Woosuk Kwon 2024-06-11 [Misc] Remove VLLM_BUILD_WITH_NEURON env variable (#5389)
1	1	Dockerfile.neuron
1	1	setup.py
0	5	vllm/envs.py

[3c4cebf75] Roger Wang 2024-06-11 [Doc][Typo] Fixing Missing Comma (#5403)
1	1	docs/source/conf.py

[d8f31f2f8] youkaichao 2024-06-10 [Doc] add debugging tips (#5409)
36	0	docs/source/getting_started/debugging.rst
1	0	docs/source/index.rst

[640052b06] Cyrus Leung 2024-06-11 [Bugfix][Frontend] Cleanup "fix chat logprobs" (#5026)
11	14	tests/async_engine/test_openapi_server_ray.py
84	85	tests/entrypoints/test_openai_server.py
2	3	tests/tensorizer_loader/test_tensorizer.py
4	3	vllm/entrypoints/openai/protocol.py
9	6	vllm/entrypoints/openai/serving_chat.py
12	12	vllm/entrypoints/openai/serving_completion.py

[351d5e7b8] maor-ps 2024-06-11 [Bugfix] OpenAI entrypoint limits logprobs while ignoring server defined --max-logprobs (#5312)
6	2	tests/entrypoints/test_openai_server.py
1	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
4	5	vllm/entrypoints/openai/protocol.py

[a00862980] Nick Hill 2024-06-10 [Misc] Various simplifications and typing fixes (#5368)
1	1	vllm/engine/output_processor/multi_step.py
4	2	vllm/model_executor/layers/rejection_sampler.py
12	27	vllm/spec_decode/batch_expansion.py
7	7	vllm/spec_decode/spec_decode_worker.py
18	27	vllm/spec_decode/top1_proposer.py
3	8	vllm/spec_decode/util.py
3	3	vllm/transformers_utils/config.py
15	15	vllm/worker/model_runner.py

[76477a93b] Kevin H. Luu 2024-06-10 [ci] Fix Buildkite agent path (#5392)
1	1	.buildkite/run-benchmarks.sh

[77c87beb0] Michael Goin 2024-06-10 [Doc] Add documentation for FP8 W8A8 (#5388)
1	0	docs/source/index.rst
202	0	docs/source/quantization/fp8.rst

[114332b88] Simon Mo 2024-06-10 Bump version to v0.5.0 (#5384)
1	1	vllm/__init__.py

[cb77ad836] Woosuk Kwon 2024-06-10 [Docs] Alphabetically sort sponsors (#5386)
1	1	README.md
2	2	docs/source/community/sponsors.md

[856c99004] Roger Wang 2024-06-10 [Docs] Add Docs on Limitations of VLM Support (#5383)
1	0	docs/source/conf.py
8	1	docs/source/models/vlm.rst

[c5602f0ba] Kevin H. Luu 2024-06-10 [ci] Mount buildkite agent on Docker container to upload benchmark results (#5330)
3	3	.buildkite/run-benchmarks.sh
3	0	.buildkite/test-template-aws.j2

[f7f9c5f97] Kevin H. Luu 2024-06-10 [ci] Use small_cpu_queue for doc build (#5331)
3	1	.buildkite/test-template-aws.j2

[2c0d93359] Cyrus Leung 2024-06-10 [Bugfix] Fix LLaVA-NeXT (#5380)
24	0	vllm/model_executor/models/llava_next.py
1	1	vllm/multimodal/utils.py

[774d1035e] Itay Etelis 2024-06-10 [Feature][Frontend]:  Continued `stream_options` implementation also in CompletionRequest (#5319)
132	104	tests/entrypoints/test_openai_server.py
9	0	vllm/entrypoints/openai/protocol.py
17	18	vllm/entrypoints/openai/serving_chat.py
22	4	vllm/entrypoints/openai/serving_completion.py

[6b29d6fe7] Cyrus Leung 2024-06-10 [Model] Initial support for LLaVA-NeXT (#4199)
5	1	docs/source/models/supported_models.rst
0	2	tests/models/test_llava.py
123	0	tests/models/test_llava_next.py
55	7	tests/multimodal/test_processor.py
2	0	vllm/model_executor/models/__init__.py
10	8	vllm/model_executor/models/llava.py
445	0	vllm/model_executor/models/llava_next.py

[0bfa1c4f1] Cyrus Leung 2024-06-10 [Misc] Improve error message when LoRA parsing fails (#5194)
13	1	tests/lora/test_utils.py
7	8	vllm/lora/utils.py

[c81da5f56] youkaichao 2024-06-10 [misc][typo] fix typo (#5372)
1	1	vllm/distributed/device_communicators/custom_all_reduce_utils.py

[68bc81703] Roger Wang 2024-06-10 [Frontend][Misc] Enforce Pixel Values as Input Type for VLMs in API Server (#5374)
10	0	vllm/entrypoints/openai/api_server.py

[5884c2b45] Dipika Sikka 2024-06-09 [Misc] Update to comply with the new `compressed-tensors` config (#5350)
13	7	tests/quantization/test_compressed_tensors.py
2	6	vllm/config.py
1	1	vllm/model_executor/layers/quantization/__init__.py
3	6	vllm/model_executor/model_loader/weight_utils.py

[45f92c00c] Bla_ckB 2024-06-10 [Bugfix] Fix KeyError: 1 When Using LoRA adapters (#5164)
3	1	vllm/core/scheduler.py

[5467ac319] bnellnm 2024-06-09 [Kernel][Misc] Use TORCH_LIBRARY instead of PYBIND11_MODULE for custom ops (#5047)
6	16	CMakeLists.txt
3	3	Dockerfile.rocm
6	6	cmake/cpu_extension.cmake
8	3	cmake/utils.cmake
1	1	csrc/activation_kernels.cu
18	16	csrc/attention/attention_kernels.cu
9	5	csrc/cache.h
8	5	csrc/cache_kernels.cu
14	12	csrc/cpu/attention.cpp
8	5	csrc/cpu/cache.cpp
1	1	csrc/cpu/cpu_types.hpp
2	2	csrc/cpu/layernorm.cpp
1	1	csrc/cpu/pos_encoding.cpp
0	43	csrc/cpu/pybind.cpp
106	0	csrc/cpu/torch_bindings.cpp
2	4	csrc/cuda_utils.h
3	3	csrc/cuda_utils_kernels.cu
14	8	csrc/custom_all_reduce.cu
1	1	csrc/dispatch_utils.h
3	3	csrc/layernorm_kernels.cu
0	8	csrc/moe/moe_ops.cpp
1	1	csrc/moe/moe_ops.h
1	1	csrc/moe/topk_softmax_kernels.cu
12	0	csrc/moe/torch_bindings.cpp
3	3	csrc/moe_align_block_size_kernels.cu
35	33	csrc/ops.h
6	6	csrc/pos_encoding_kernels.cu
3	3	csrc/punica/punica_ops.cu
3	3	csrc/punica/punica_ops.h
0	13	csrc/punica/punica_pybind.cpp
18	0	csrc/punica/torch_bindings.cpp
0	114	csrc/pybind.cpp
1	1	csrc/quantization/aqlm/gemm_kernels.cu
4	4	csrc/quantization/awq/gemm_kernels.cu
1	1	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
1	1	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu
1	1	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
1	1	csrc/quantization/cutlass_w8a8/scaled_mm_dq_entry.cu
1	1	csrc/quantization/fp8/common.cu
3	3	csrc/quantization/gptq/q_gemm.cu
1	1	csrc/quantization/gptq_marlin/gptq_marlin.cu
1	1	csrc/quantization/gptq_marlin/gptq_marlin.cuh
1	1	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
1	1	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
0	1	csrc/quantization/squeezellm/quant_cuda_kernel.cu
22	0	csrc/registration.h
283	0	csrc/torch_bindings.cpp
1	1	setup.py
4	3	tests/kernels/test_int8_quant.py
164	53	vllm/_custom_ops.py
5	5	vllm/attention/backends/flash_attn.py
19	15	vllm/distributed/device_communicators/custom_all_reduce.py
19	26	vllm/lora/punica.py
1	2	vllm/model_executor/layers/fused_moe/fused_moe.py
2	5	vllm/utils.py

[5d7e3d017] youkaichao 2024-06-08 [mis][ci/test] fix flaky test in test_sharded_state_loader.py (#5361)
2	1	tests/test_sharded_state_loader.py

[0373e1837] youkaichao 2024-06-08 [Core][CUDA Graph] add output buffer for cudagraph (#5074)
20	4	vllm/worker/model_runner.py

[c09dade2a] Michael Goin 2024-06-08 [Misc][Breaking] Change FP8 checkpoint format from act_scale -> input_scale (#5353)
15	15	vllm/model_executor/layers/quantization/fp8.py
8	8	vllm/model_executor/models/mixtral.py

[8ea5e44a4] youkaichao 2024-06-08 [CI/Test] improve robustness of test (vllm_runner) (#5357)
5	6	tests/basic_correctness/test_basic_correctness.py
10	11	tests/basic_correctness/test_chunked_prefill.py
73	75	tests/basic_correctness/test_preemption.py
4	1	tests/conftest.py
7	8	tests/distributed/test_basic_distributed_correctness.py
10	11	tests/distributed/test_chunked_prefill_distributed.py
2	3	tests/engine/test_stop_reason.py
2	1	tests/engine/test_stop_strings.py
42	42	tests/metrics/test_metrics.py
3	4	tests/models/test_aqlm.py
7	9	tests/models/test_big_models.py
2	3	tests/models/test_embedding.py
17	20	tests/models/test_gptq_marlin.py
9	13	tests/models/test_gptq_marlin_24.py
7	8	tests/models/test_llava.py
10	14	tests/models/test_marlin.py
3	5	tests/models/test_mistral.py
7	9	tests/models/test_models.py
62	62	tests/quantization/test_bitsandbytes.py
33	30	tests/quantization/test_compressed_tensors.py
5	5	tests/quantization/test_fp8.py
3	10	tests/samplers/test_beam_search.py
8	7	tests/samplers/test_ignore_eos.py
41	41	tests/samplers/test_logits_processor.py
15	15	tests/samplers/test_logprobs.py
21	17	tests/samplers/test_ranks.py
2	3	tests/samplers/test_seeded_generate.py
45	61	tests/tensorizer_loader/test_tensorizer.py

[9fb900f90] youkaichao 2024-06-07 [CI/Test] improve robustness of test (hf_runner) (#5347)
2	3	tests/basic_correctness/test_basic_correctness.py
2	3	tests/basic_correctness/test_chunked_prefill.py
7	10	tests/basic_correctness/test_preemption.py
4	1	tests/conftest.py
2	3	tests/distributed/test_basic_distributed_correctness.py
2	3	tests/distributed/test_chunked_prefill_distributed.py
2	3	tests/models/test_big_models.py
2	3	tests/models/test_embedding.py
4	5	tests/models/test_llava.py
3	4	tests/models/test_mistral.py
2	3	tests/models/test_models.py
3	4	tests/samplers/test_beam_search.py
5	6	tests/samplers/test_logprobs.py
8	10	tests/tensorizer_loader/test_tensorizer.py

[c96fc0674] Hongxia Yang 2024-06-07 [ROCm][AMD] Use pytorch sdpa math backend to do naive attention (#4965)
29	33	vllm/attention/backends/rocm_flash_attn.py

[b3376e5c7] Benjamin Kitor 2024-06-07 [Misc] Add args for selecting distributed executor to benchmarks (#5335)
9	1	benchmarks/benchmark_latency.py
11	2	benchmarks/benchmark_throughput.py

[e69ded7d1] Cheng Li 2024-06-07 [Bug Fix] Fix the support check for FP8 CUTLASS  (#5352)
7	7	vllm/model_executor/layers/quantization/fp8.py

[767c727a8] Calvinn Ng 2024-06-08 fix DbrxFusedNormAttention missing cache_config (#5340)
2	1	vllm/model_executor/models/dbrx.py

[6840a7161] Jie Fu (傅杰) 2024-06-08 [Misc] Remove unused cuda_utils.h in CPU backend (#5345)
0	1	csrc/cpu/pybind.cpp

[7a9cb294a] Roger Wang 2024-06-07 [Frontend] Add OpenAI Vision API Support (#5237)
67	1	docs/source/models/vlm.rst
3	1	docs/source/serving/openai_compatible_server.md
23	0	examples/template_llava.jinja
286	0	tests/entrypoints/test_openai_vision.py
75	0	tests/multimodal/test_utils.py
11	1	vllm/config.py
97	16	vllm/entrypoints/openai/serving_chat.py
6	0	vllm/envs.py
85	0	vllm/multimodal/utils.py

[ca3ea51bd] Dipika Sikka 2024-06-07 [Kernel] Dynamic Per-Token Activation Quantization (#5037)
3	0	csrc/ops.h
3	0	csrc/pybind.cpp
64	11	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
41	13	csrc/reduction_utils.cuh
38	6	tests/kernels/test_int8_quant.py
18	1	tests/quantization/test_compressed_tensors.py
20	8	vllm/_custom_ops.py
51	36	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
2	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
85	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_dynamictoken.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py
114	0	vllm/model_executor/layers/quantization/compressed_tensors/utils.py

[dc49fb892] limingshu 2024-06-07 Addition of lacked ignored_seq_groups in _schedule_chunked_prefill (#5296)
2	1	vllm/core/scheduler.py

[18a277b52] Antoni Baum 2024-06-07 Remove Ray health check (#4693)
0	17	vllm/executor/ray_gpu_executor.py

[8d75fe48c] Tyler Michael Smith 2024-06-07 [Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)
2	2	vllm/_custom_ops.py
50	16	vllm/model_executor/layers/quantization/fp8.py

[388596c91] youkaichao 2024-06-06 [Misc][Utils] allow get_open_port to be called for multiple times (#5333)
15	1	tests/test_utils.py
3	0	vllm/envs.py
9	1	vllm/utils.py

[baa15a9ec] Itay Etelis 2024-06-07 [Feature][Frontend]: Add support for `stream_options` in `ChatCompletionRequest` (#5135)
101	0	tests/entrypoints/test_openai_server.py
14	0	vllm/entrypoints/openai/protocol.py
34	10	vllm/entrypoints/openai/serving_chat.py

[15063741e] Jie Fu (傅杰) 2024-06-07 [Misc] Missing error message for custom ops import (#5282)
4	2	vllm/_custom_ops.py

[ccdc490dd] Antoni Baum 2024-06-06 [Core] Change LoRA embedding sharding to support loading methods (#5038)
2	8	.buildkite/test-pipeline.yaml
21	0	tests/conftest.py
16	2	tests/lora/conftest.py
217	2	tests/lora/test_layers.py
7	10	tests/lora/test_llama.py
13	10	tests/lora/test_long_context.py
80	44	tests/test_sharded_state_loader.py
57	19	vllm/lora/layers.py
2	1	vllm/lora/utils.py
235	25	vllm/model_executor/layers/vocab_parallel_embedding.py
11	8	vllm/worker/model_runner.py

[a31cab755] Antoni Baum 2024-06-06 [Core] Avoid copying prompt/output tokens if no penalties are used (#5289)
50	30	vllm/model_executor/sampling_metadata.py

[828da0d44] Matthew Goldey 2024-06-06 [Frontend] enable passing multiple LoRA adapters at once to generate() (#5300)
69	0	tests/entrypoints/test_llm_generate_multiple_loras.py
22	17	vllm/entrypoints/llm.py

[abe855d63] Philipp Moritz 2024-06-06 [Kernel] Retune Mixtral 8x22b configs for FP8 on H100 (#5294)
55	55	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
36	36	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json

[4efff036f] liuyhwangyh 2024-06-07 Bugfix: fix broken of download models from modelscope (#5233)
21	0	tests/test_regression.py
5	1	vllm/config.py
6	1	vllm/transformers_utils/config.py

[89c920785] Cyrus Leung 2024-06-06 [CI/Build] Update vision tests (#5307)
2	3	.buildkite/test-pipeline.yaml
1	0	pyproject.toml
8	27	tests/conftest.py
42	55	tests/models/test_llava.py
21	1	vllm/config.py
16	2	vllm/multimodal/image.py

[7b0a0dfb2] Breno Faria 2024-06-06 [Frontend][Core] Update Outlines Integration from `FSM` to `Guide` (#4109)
1	1	requirements-common.txt
0	2	tests/entrypoints/test_guided_processors.py
12	19	vllm/model_executor/guided_decoding/outlines_decoding.py
36	26	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[3a6ae1d33] Simon Mo 2024-06-05 [CI] Disable flash_attn backend for spec decode (#5286)
5	2	.buildkite/test-pipeline.yaml

[8f1729b82] Simon Mo 2024-06-05 [Docs] Add Ray Summit CFP (#5295)
7	0	README.md

[6a7c7711a] Woosuk Kwon 2024-06-05 [Misc] Skip for logits_scale == 1.0 (#5291)
3	2	vllm/model_executor/layers/logits_processor.py

[0f83ddd4d] Alex Wu 2024-06-05 [Bugfix][Frontend/Core] Don't log exception when AsyncLLMEngine gracefully shuts down. (#5290)
19	11	vllm/engine/async_llm_engine.py

[065aff6c1] Michael Goin 2024-06-05 [Bugfix] Make EngineArgs use named arguments for config construction (#5285)
40	26	vllm/engine/arg_utils.py

[3d33e372a] Nick Hill 2024-06-05 [BugFix] Fix log message about default max model length (#5284)
1	1	vllm/config.py

[faf71bcd4] Nick Hill 2024-06-05 [Speculative Decoding] Add `ProposerWorkerBase` abstract class (#5252)
2	2	tests/spec_decode/test_dynamic_spec_decode.py
12	9	tests/spec_decode/test_multi_step_worker.py
12	9	tests/spec_decode/test_ngram_worker.py
1	1	vllm/spec_decode/interfaces.py
9	6	vllm/spec_decode/multi_step_worker.py
5	28	vllm/spec_decode/ngram_worker.py
44	0	vllm/spec_decode/proposer_worker_base.py
3	2	vllm/spec_decode/spec_decode_worker.py
3	3	vllm/spec_decode/top1_proposer.py

[f270a3953] Simon Mo 2024-06-05 [Docs] Add Sequoia as sponsors (#5287)
1	0	README.md
1	0	docs/source/community/sponsors.md

[51a08e7d8] Philipp Moritz 2024-06-05 [Kernel] Re-tune Mixtral MoE configurations for FP8 on H100 (#5238)
2	1	benchmarks/kernels/benchmark_moe.py
52	52	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
31	31	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json

[eb8fcd266] DriverSong 2024-06-06 [BugFix] Apply get_cached_tokenizer to the tokenizer setter of LLM (#5207)
9	1	vllm/entrypoints/llm.py

[5563a4dea] Cody Yu 2024-06-05 [Model] Correct Mixtral FP8 checkpoint loading (#5231)
4	3	vllm/model_executor/layers/quantization/fp8.py
76	32	vllm/model_executor/models/mixtral.py

[ccd4f129e] Tyler Michael Smith 2024-06-05 [Kernel] Add GPU architecture guards to the CUTLASS w8a8 kernels to reduce binary size (#5157)
70	35	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu
17	2	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu

[02cc3b51a] Tyler Michael Smith 2024-06-05 [misc] benchmark_serving.py -- add ITL results and tweak TPOT results (#5263)
1	1	.buildkite/run-benchmarks.sh
22	1	benchmarks/benchmark_serving.py

[d5b1eb081] Simon Mo 2024-06-05 [CI] Add nightly benchmarks (#5260)
26	0	.buildkite/nightly-benchmarks/kickoff-pipeline.sh
39	0	.buildkite/nightly-benchmarks/sample.yaml

[f0a500545] tomeras91 2024-06-05 [Frontend] OpenAI API server: Add `add_special_tokens` to ChatCompletionRequest (default False) (#5278)
9	0	vllm/entrypoints/openai/protocol.py
3	1	vllm/entrypoints/openai/serving_chat.py
8	6	vllm/entrypoints/openai/serving_engine.py

[c65146e75] Woosuk Kwon 2024-06-05 [Misc] Fix docstring of get_attn_backend (#5271)
2	3	vllm/attention/selector.py

[41ca62cf0] Woosuk Kwon 2024-06-05 [Misc] Add CustomOp interface for device portability (#5255)
2	2	tests/kernels/test_activation.py
1	1	tests/kernels/test_layernorm.py
4	3	tests/kernels/test_pos_encoding.py
60	0	vllm/model_executor/custom_op.py
21	13	vllm/model_executor/layers/activation.py
6	4	vllm/model_executor/layers/layernorm.py
6	4	vllm/model_executor/layers/rotary_embedding.py

[974fc9b84] zifeitong 2024-06-04 [Bugfix] Fix prompt_logprobs when SamplingParams.detokenize is set to True (#5226)
18	9	tests/samplers/test_logprobs.py
4	4	vllm/engine/output_processor/single_step.py

[fee4dcc33] youkaichao 2024-06-04 [Misc] update collect env (#5261)
5	0	collect_env.py

[650a4cc55] Michael Goin 2024-06-04 [Misc] Add transformers version to collect_env.py (#5259)
2	0	collect_env.py

[9ca62d866] Simon Mo 2024-06-04 [CI] mark AMD test as softfail to prevent blockage (#5256)
3	2	.buildkite/test-template.j2

[45c35f0d5] Li, Jiang 2024-06-05 [CI/Build] Reducing CPU CI execution time (#5241)
1	1	.buildkite/run-cpu-test.sh

[9ba093b4f] Cyrus Leung 2024-06-05 [CI/Build] Simplify model loading for `HfRunner` (#5251)
16	11	tests/conftest.py
1	1	tests/models/test_embedding.py
1	1	tests/models/test_llava.py

[27208be66] Woosuk Kwon 2024-06-04 [Kernel] Add back batch size 1536 and 3072 to MoE tuning (#5242)
3	1	benchmarks/kernels/benchmark_moe.py

[87d5abef7] Jie Fu (傅杰) 2024-06-05 [Bugfix] Fix a bug caused by pip install setuptools>=49.4.0 for CPU backend (#5249)
1	1	Dockerfile.cpu
1	1	docs/source/getting_started/cpu-installation.rst

[ec784b252] Cyrus Leung 2024-06-04 [CI/Build] Add inputs tests (#5215)
7	1	.buildkite/test-pipeline.yaml
7	4	tests/multimodal/test_processor.py

[a58f24e59] zifeitong 2024-06-03 [Bugfix] Fix torch.compile() error when using MultiprocessingGPUExecutor (#5229)
3	0	vllm/executor/multiproc_gpu_executor.py

[f42a006b1] afeldman-nm 2024-06-03 [Bugfix]: During testing, use pytest monkeypatch for safely overriding the env var that indicates the vLLM backend (#5210)
10	17	tests/kernels/test_attention_selector.py
22	0	tests/kernels/utils.py

[3a434b07e] Woosuk Kwon 2024-06-03 [Kernel] Enhance MoE benchmarking & tuning script (#4921)
0	239	benchmarks/kernels/benchmark_mixtral_moe.py
319	0	benchmarks/kernels/benchmark_moe.py
27	14	vllm/model_executor/layers/fused_moe/fused_moe.py

[bd0e7802e] Zhuohan Li 2024-06-03 [Bugfix] Add warmup for prefix caching example (#5235)
4	2	examples/offline_inference_with_prefix.py

[06b2550cb] Toshiki Kataoka 2024-06-04 [Bugfix] Support `prompt_logprobs==0` (#5217)
8	4	tests/entrypoints/test_openai_server.py
1	1	vllm/entrypoints/openai/serving_completion.py
1	1	vllm/model_executor/sampling_metadata.py
1	1	vllm/worker/model_runner.py

[f775a07e3] Breno Faria 2024-06-04 [FRONTEND] OpenAI `tools` support named functions (#5032)
12	1	docs/source/serving/openai_compatible_server.md
185	0	tests/entrypoints/test_openai_server.py
2	1	tests/utils.py
55	2	vllm/entrypoints/openai/protocol.py
32	5	vllm/entrypoints/openai/serving_chat.py
28	2	vllm/model_executor/guided_decoding/__init__.py

[4f0d17c05] Kevin H. Luu 2024-06-03 New CI template on AWS stack (#5110)
59	0	.buildkite/test-template-aws.j2

[10c38e3e4] Kaiyang Chen 2024-06-04 [Misc]: Implement CPU/GPU swapping in BlockManagerV2 (#3834)
1	1	format.sh
40	9	tests/core/block/e2e/test_correctness.py
57	1	tests/core/block/test_block_manager_v2.py
17	11	vllm/config.py
4	0	vllm/core/block/block_table.py
0	1	vllm/core/block/common.py
77	5	vllm/core/block/cpu_gpu_block_allocator.py
35	1	vllm/core/block/interfaces.py
64	2	vllm/core/block/naive_block.py
78	0	vllm/core/block/prefix_caching_block.py
1	5	vllm/core/block_manager_v1.py
133	7	vllm/core/block_manager_v2.py
1	2	vllm/core/embedding_model_block_manager.py
1	2	vllm/core/interfaces.py
11	2	vllm/core/scheduler.py
9	0	vllm/engine/arg_utils.py

[cafb8e06c] Yuan 2024-06-04 [CI/BUILD] enable intel queue for longer CPU tests (#4113)
12	2	.buildkite/run-cpu-test.sh
2	0	.buildkite/test-template.j2
5	1	Dockerfile.cpu
51	50	csrc/cpu/pos_encoding.cpp
23	13	tests/conftest.py
7	4	tests/models/test_aqlm.py
8	2	tests/models/test_big_models.py
7	4	tests/models/test_fp8.py
7	4	tests/models/test_gptq_marlin.py
7	4	tests/models/test_gptq_marlin_24.py
7	4	tests/models/test_marlin.py

[cbb2f59cc] Tyler Michael Smith 2024-06-03 [Kernel] Pass a device pointer into the quantize kernel for the scales (#5159)
2	2	csrc/ops.h
9	6	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
3	1	tests/kernels/test_int8_quant.py
1	1	vllm/_custom_ops.py
1	1	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py

[0ab278ca3] Antoni Baum 2024-06-03 [Core] Remove unnecessary copies in flash attn backend (#5138)
1	1	requirements-cuda.txt
7	6	vllm/attention/backends/flash_attn.py

[7a64d24aa] Cyrus Leung 2024-06-03 [Core] Support image processor (#4197)
1	0	.github/workflows/mypy.yaml
8	6	docs/source/conf.py
51	0	docs/source/dev/multimodal/multimodal_index.rst
4	2	docs/source/index.rst
4	0	docs/source/models/supported_models.rst
56	0	docs/source/models/vlm.rst
15	14	examples/llava_example.py
1	0	format.sh
1	0	requirements-common.txt
0	3	requirements-dev.txt
24	21	tests/conftest.py
40	20	tests/models/test_llava.py
0	0	tests/multimodal/__init__.py
98	0	tests/multimodal/test_processor.py
2	1	tests/spec_decode/e2e/conftest.py
20	0	tests/tokenization/test_image_processor.py
4	2	vllm/config.py
79	29	vllm/engine/arg_utils.py
2	23	vllm/entrypoints/llm.py
46	27	vllm/model_executor/models/llava.py
7	0	vllm/multimodal/__init__.py
126	0	vllm/multimodal/base.py
141	0	vllm/multimodal/image.py
156	0	vllm/multimodal/registry.py
7	25	vllm/sequence.py
45	0	vllm/transformers_utils/image_processor.py
37	20	vllm/worker/cpu_model_runner.py
5	5	vllm/worker/embedding_model_runner.py
62	58	vllm/worker/model_runner.py

[dfbe60dc6] Cyrus Leung 2024-06-03 [Misc] Simplify code and fix type annotations in `conftest.py` (#5118)
42	50	tests/conftest.py

[a66cf40b2] Divakar Verma 2024-06-02 [Kernel][ROCm][AMD] enable fused topk_softmax kernel for moe layer (#4927)
3	5	CMakeLists.txt
1	0	Dockerfile.rocm
4	0	csrc/cuda_compat.h
17	10	csrc/moe/topk_softmax_kernels.cu
1	1	setup.py
19	27	vllm/model_executor/layers/fused_moe/fused_moe.py

[f790ad3c5] Avinash Raj 2024-06-02 [Frontend][OpenAI] Support for returning max_model_len on /v1/models response (#4643)
1	0	vllm/entrypoints/openai/protocol.py
1	0	vllm/entrypoints/openai/serving_engine.py

[ed59a7ed2] Simon Mo 2024-06-01 Update test_ignore_eos (#4898)
11	10	tests/samplers/test_ignore_eos.py

[044793d8d] Robert Shaw 2024-06-01 [BugFix] Prevent `LLM.encode` for non-generation Models  (#5184)
10	0	vllm/entrypoints/llm.py

[c2d6d2f96] Daniil Arapov 2024-06-02 [Bugfix]: Fix issues related to prefix caching example (#5177) (#5180)
37	10	examples/offline_inference_with_prefix.py

[8279078e2] Zhuohan Li 2024-06-01 [Bugfix] Remove deprecated @abstractproperty (#5174)
3	2	vllm/core/evictor_v1.py
3	2	vllm/core/evictor_v2.py
3	2	vllm/lora/worker_manager.py

[b9c0605a8] chenqianfzh 2024-06-01 [Feature][Kernel] Support bitsandbytes quantization and QLoRA (#4776)
140	0	examples/lora_with_quantization_inference.py
3	0	requirements-dev.txt
80	0	tests/quantization/test_bitsandbytes.py
8	1	vllm/config.py
35	3	vllm/engine/arg_utils.py
39	2	vllm/model_executor/layers/linear.py
3	0	vllm/model_executor/layers/quantization/__init__.py
175	0	vllm/model_executor/layers/quantization/bitsandbytes.py
246	1	vllm/model_executor/model_loader/loader.py
15	1	vllm/model_executor/model_loader/weight_utils.py
8	0	vllm/model_executor/models/llama.py

[37464a0f7] Nadav Shmayovits 2024-06-01 [Bugfix] Fix call to init_logger in openai server (#4765)
1	1	vllm/entrypoints/openai/api_server.py

[c35407282] Ye Cao 2024-06-02 [Minor] Fix the path typo in loader.py: save_sharded_states.py -> save_sharded_state.py  (#5151)
1	1	vllm/model_executor/model_loader/loader.py

[f081c3ce4] Varun Sundar Rabindranath 2024-06-01 [Kernel] Update Cutlass fp8 configs (#5144)
352	0	benchmarks/cutlass_benchmarks/w8a8_benchmarks.py
37	0	benchmarks/cutlass_benchmarks/weight_shapes.py
90	14	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
1	1	tests/kernels/test_cutlass.py

[260d119e8] Tyler Michael Smith 2024-06-01 [Kernel] Refactor CUTLASS kernels to always take scales that reside on the GPU (#5137)
28	22	csrc/quantization/cutlass_w8a8/{cutlass_visitor_2x_broadcast_epilogue.hpp => broadcast_load_epilogue_c2x.hpp}
389	0	csrc/quantization/cutlass_w8a8/broadcast_load_epilogue_c3x.hpp
4	10	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu
10	10	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
1	1	pyproject.toml
10	3	tests/kernels/test_cutlass.py
3	30	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py

[a360ff80b] Daniele 2024-06-01 [CI/Build] CMakeLists: build all extensions' cmake targets at the same time (#5034)
10	7	setup.py

[1197e0214] Tyler Michael Smith 2024-05-31 [Build] Guard against older CUDA versions when building CUTLASS 3.x kernels (#5168)
8	2	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
10	1	csrc/quantization/cutlass_w8a8/scaled_mm_dq_entry.cu

[657579113] Nick Hill 2024-05-31 [Doc] Add checkmark for GPTBigCodeForCausalLM LoRA support (#5171)
1	1	docs/source/models/supported_models.rst

[e9899fb7a] Cody Yu 2024-05-31 [Model] Enable FP8 QKV in MoE and refine kernel tuning script (#5039)
36	12	benchmarks/kernels/benchmark_mixtral_moe.py
138	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
57	51	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
42	42	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
0	9	vllm/model_executor/models/mixtral.py

[a377f0bd5] functionxu123 2024-05-31 [Misc]: optimize eager mode host time (#4196)
5	6	vllm/utils.py

[e9d3aa04f] Simon Mo 2024-05-31 Revert "[Kernel] Marlin_24: Ensure the mma.sp instruction is using the ::ordered_metadata modifier (introduced with PTX 8.5)" (#5149)
4	8	csrc/quantization/marlin/sparse/common/mma.h

[a22dea54d] SnowDist 2024-05-31 [Model] Support MAP-NEO model (#5081)
1	1	benchmarks/kernels/benchmark_paged_attention.py
1	1	benchmarks/kernels/benchmark_rope.py
6	0	csrc/attention/attention_kernels.cu
6	0	csrc/cpu/attention.cpp
1	1	tests/kernels/test_attention.py
1	1	tests/kernels/test_cache.py
1	1	tests/kernels/test_pos_encoding.py
1	1	vllm/attention/ops/paged_attn.py

[533c21779] simon-mo 2024-05-31 Fix cutlass sm_90a vesrion in CMakeList
1	1	CMakeLists.txt

[6d21fa1ca] Alexander Matveev 2024-05-30 [Kernel] Marlin_24: Ensure the mma.sp instruction is using the ::ordered_metadata modifier (introduced with PTX 8.5) (#5136)
8	4	csrc/quantization/marlin/sparse/common/mma.h

[b35be5403] Robert Shaw 2024-05-30 [Bugfix] Avoid Warnings in SparseML Activation Quantization (#5120)
20	9	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py

[45a1a69b9] Simon Mo 2024-05-30 [Build] Disable sm_90a in cu11 (#5141)
8	6	CMakeLists.txt

[87a658c81] Simon Mo 2024-05-30 Bump version to v0.4.3 (#5046)
1	1	vllm/__init__.py

[429d89720] Chansung Park 2024-05-31 add doc about serving option on dstack (#3074)
103	0	docs/source/serving/deploying_with_dstack.rst
1	0	docs/source/serving/integrations.rst

[a9bcc7afb] Cyrus Leung 2024-05-31 [Doc] Use intersphinx and update entrypoints docs (#5125)
12	1	docs/source/conf.py
0	2	vllm/engine/async_llm_engine.py
2	2	vllm/engine/llm_engine.py
18	8	vllm/entrypoints/llm.py

[d79d9eaaf] Hyunsung Lee 2024-05-30 [Misc] remove duplicate definition of `seq_lens_tensor` in model_runner.py (#5129)
0	3	vllm/worker/model_runner.py

[f758505c7] youkaichao 2024-05-30 [CI/Build] increase wheel size limit to 200 MB (#5130)
1	1	.buildkite/check-wheel-size.py

[d910816c7] Robert Shaw 2024-05-30 [Bugfix] Automatically Detect SparseML models (#5119)
14	2	vllm/config.py

[87d41c849] Breno Faria 2024-05-30 [BUGFIX] [FRONTEND] Correct chat logprobs (#5029)
4	2	tests/async_engine/test_openapi_server_ray.py
181	28	tests/entrypoints/test_openai_server.py
43	7	vllm/entrypoints/openai/protocol.py
59	9	vllm/entrypoints/openai/serving_chat.py
68	6	vllm/entrypoints/openai/serving_completion.py
6	46	vllm/entrypoints/openai/serving_engine.py

[e07aff9e5] omkar kakarparthi 2024-05-29 [CI/Build] Docker cleanup functionality for amd servers  (#5112)
28	0	.buildkite/run-amd-test.sh

[5bf185a1c] Alexander Matveev 2024-05-29 [Bugfix] gptq_marlin: Ensure g_idx_sort_indices is not a Parameter (#5108)
4	8	vllm/model_executor/layers/quantization/gptq_marlin.py

[4fbcb0f27] youkaichao 2024-05-29 [Doc][Build] update after removing vllm-nccl (#5103)
0	6	Dockerfile
1	1	docs/source/serving/deploying_with_docker.rst

[7c3604fb6] Itay Etelis 2024-05-30 [Bugfix] logprobs is not compatible with the OpenAI spec #4795 (#5031)
2	3	vllm/entrypoints/openai/protocol.py
2	2	vllm/entrypoints/openai/serving_chat.py

[b1c255630] Cyrus Leung 2024-05-30 [Core] Avoid the need to pass `None` values to `Sequence.inputs` (#5099)
0	2	tests/core/test_block_manager.py
1	6	tests/core/utils.py
1	5	tests/engine/output_processor/test_stop_checker.py
0	1	tests/test_cache_block_hashing.py
0	1	tests/tokenization/test_detokenize.py
2	2	vllm/inputs.py
2	2	vllm/sequence.py

[eb6c50cdc] Cyrus Leung 2024-05-30 [Bugfix][CI/Build] Fix codespell failing to skip files in `git diff` (#5097)
4	1	format.sh

[eecd86438] Cyrus Leung 2024-05-30 [Bugfix][CI/Build] Fix test and improve code for `merge_async_iterators` (#5096)
0	41	tests/async_engine/test_merge_async_iterators.py
56	1	tests/test_utils.py
6	3	vllm/utils.py

[ae495c74e] Ronen Schaffer 2024-05-30 [Doc]Replace deprecated flag in readme (#4526)
2	1	examples/production_monitoring/README.md

[4238bc82f] afeldman-nm 2024-05-29 [Core] Cross-attention KV caching and memory-management (towards eventual encoder/decoder model support) (#4837)
153	1	tests/core/block/test_block_manager_v2.py
217	3	tests/core/test_block_manager.py
98	1	tests/core/utils.py
56	0	vllm/core/block/utils.py
132	55	vllm/core/block_manager_v1.py
56	9	vllm/core/block_manager_v2.py
23	0	vllm/sequence.py

[594392d27] youkaichao 2024-05-29 [Core][Distributed] improve p2p access check (#4992)
2	1	vllm/distributed/device_communicators/custom_all_reduce.py
186	0	vllm/distributed/device_communicators/custom_all_reduce_utils.py
1	89	vllm/distributed/utils.py

[18c1f16d8] Cyrus Leung 2024-05-29 [Bugfix] Fix arguments passed to `Sequence` in stop checker test (#5092)
5	2	tests/engine/output_processor/test_stop_checker.py

[5bd3c6507] youkaichao 2024-05-28 [Core][Optimization] remove vllm-nccl (#5091)
0	1	.buildkite/test-pipeline.yaml
0	1	requirements-cuda.txt
2	5	setup.py
0	43	tests/distributed/test_pynccl_library.py
7	13	vllm/distributed/device_communicators/pynccl_wrapper.py
8	35	vllm/utils.py
4	2	vllm/worker/worker_base.py

[616e600e0] Marut Pandya 2024-05-28 [Misc] add gpu_memory_utilization arg (#5079)
8	1	benchmarks/benchmark_latency.py

[dfba529b4] Junichi Sato 2024-05-29 [Bugfix] Remove the last EOS token unless explicitly specified (#5077)
86	0	tests/engine/output_processor/test_stop_checker.py
5	0	vllm/engine/output_processor/stop_checker.py

[5ae5ed1e6] Cyrus Leung 2024-05-29 [Core] Consolidate prompt arguments to LLM engines (#4328)
6	3	.buildkite/test-pipeline.yaml
7	4	benchmarks/benchmark_latency.py
1	1	docs/source/{ => dev}/offline_inference/llm.rst
14	0	docs/source/dev/offline_inference/llm_inputs.rst
8	0	docs/source/dev/offline_inference/offline_index.rst
0	0	docs/source/{offline_inference => dev}/sampling_params.rst
3	8	docs/source/index.rst
2	2	docs/source/serving/openai_compatible_server.md
16	9	examples/llava_example.py
7	0	pyproject.toml
1	1	tests/async_engine/test_async_llm_engine.py
1	1	tests/async_engine/test_openapi_server_ray.py
17	6	tests/conftest.py
12	3	tests/core/test_block_manager.py
12	3	tests/core/utils.py
1	1	tests/engine/test_skip_tokenizer_init.py
4	0	tests/entrypoints/openai/test_serving_chat.py
2	0	tests/entrypoints/test_guided_processors.py
144	0	tests/entrypoints/test_llm_encode.py
120	17	tests/entrypoints/test_llm_generate.py
32	2	tests/entrypoints/test_openai_server.py
7	4	tests/entrypoints/test_server_oot_registration.py
3	5	tests/lora/test_long_context.py
4	7	tests/samplers/test_logits_processor.py
1	5	tests/samplers/test_seeded_generate.py
9	2	tests/test_cache_block_hashing.py
53	0	tests/test_inputs.py
63	0	tests/test_utils.py
5	2	tests/tokenization/test_detokenize.py
14	0	tests/utils.py
4	0	vllm/__init__.py
81	90	vllm/engine/async_llm_engine.py
179	90	vllm/engine/llm_engine.py
6	4	vllm/engine/output_processor/util.py
302	102	vllm/entrypoints/llm.py
9	3	vllm/entrypoints/openai/serving_chat.py
11	6	vllm/entrypoints/openai/serving_completion.py
26	16	vllm/entrypoints/openai/serving_embedding.py
4	2	vllm/entrypoints/openai/serving_engine.py
130	0	vllm/inputs.py
15	27	vllm/outputs.py
26	12	vllm/sequence.py
42	1	vllm/utils.py

[290f4ada2] Simon Mo 2024-05-28 [Docs] Add Dropbox as sponsors (#5089)
1	0	README.md
1	0	docs/source/community/sponsors.md

[dd8de11f0] Divakar Verma 2024-05-28 [Kernel][ROCm][AMD] Add fused_moe Triton configs for MI300X (#4951)
128	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json
110	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json
128	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json
128	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json

[9ba415588] Robert Shaw 2024-05-28 [BugFix] Fix Embedding Models with TP>1 (#5075)
4	0	vllm/worker/embedding_model_runner.py

[d4f398590] Michał Moskal 2024-05-27 [Core] Sliding window for block manager v2 (#4545)
26	0	tests/core/block/e2e/conftest.py
2	9	tests/core/block/e2e/test_correctness.py
168	0	tests/core/block/e2e/test_correctness_sliding_window.py
69	0	tests/core/block/test_block_manager_v2.py
5	1	vllm/attention/ops/prefix_prefill.py
32	2	vllm/core/block/block_table.py
74	0	vllm/core/block/cpu_gpu_block_allocator.py
9	0	vllm/core/block/interfaces.py
17	7	vllm/core/block_manager_v2.py
2	1	vllm/engine/arg_utils.py
4	1	vllm/worker/cache_engine.py
49	24	vllm/worker/model_runner.py

[890aa93d2] Isotr0py 2024-05-28 [Model] Add support for falcon-11B (#5069)
40	15	vllm/model_executor/models/falcon.py

[fbdb7b3ee] sasha0552 2024-05-27 [Core] Allow AQLM on Pascal (#5058)
1	1	vllm/model_executor/layers/quantization/aqlm.py

[1102bef21] Zhuohan Li 2024-05-27 [Bugfix / Core] Prefix Caching Guards (merged with main) (#4846)
44	0	tests/prefix_caching/test_disable_sliding_window.py
24	0	tests/test_config.py
2	1	vllm/attention/layer.py
64	3	vllm/config.py
9	3	vllm/engine/arg_utils.py
0	4	vllm/model_executor/models/llama.py
10	12	vllm/model_executor/models/mixtral.py
0	4	vllm/model_executor/models/mixtral_quant.py
14	11	vllm/model_executor/models/qwen2.py
0	2	vllm/model_executor/models/starcoder2.py
0	4	vllm/model_executor/models/xverse.py

[f17a1a8f9] Roger Wang 2024-05-25 [Misc] Make Serving Benchmark More User-friendly (#5044)
6	0	benchmarks/backend_request_func.py
26	3	benchmarks/benchmark_serving.py

[d5a169777] Lily Liu 2024-05-25 [Dynamic Spec Decoding] Minor fix for disabling speculative decoding (#5000)
41	0	tests/spec_decode/e2e/test_ngram_correctness.py
10	6	tests/spec_decode/test_dynamic_spec_decode.py
12	5	vllm/spec_decode/spec_decode_worker.py

[325c11996] youkaichao 2024-05-24 [Misc] add logging level env var (#5045)
2	0	.github/ISSUE_TEMPLATE/400-bug report.yml
5	0	vllm/envs.py
2	1	vllm/logger.py

[8e192ff96] Eric Xihui Lin 2024-05-25 [Kernel][Backend][Model] Blocksparse flash attention kernel and Phi-3-Small model (#4799)
148	37	csrc/attention/attention_kernels.cu
21	16	csrc/cpu/attention.cpp
18	17	csrc/ops.h
4	0	docs/source/models/supported_models.rst
442	0	tests/kernels/test_blocksparse_attention.py
21	9	vllm/_custom_ops.py
1	0	vllm/attention/backends/abstract.py
410	0	vllm/attention/backends/blocksparse_attn.py
4	1	vllm/attention/backends/flash_attn.py
4	1	vllm/attention/backends/rocm_flash_attn.py
4	1	vllm/attention/backends/torch_sdpa.py
4	1	vllm/attention/backends/xformers.py
7	3	vllm/attention/layer.py
0	0	vllm/attention/ops/blocksparse_attention/__init__.py
423	0	vllm/attention/ops/blocksparse_attention/blocksparse_attention_kernel.py
238	0	vllm/attention/ops/blocksparse_attention/interface.py
216	0	vllm/attention/ops/blocksparse_attention/utils.py
24	1	vllm/attention/ops/paged_attn.py
7	0	vllm/attention/selector.py
1	0	vllm/entrypoints/openai/serving_engine.py
1	0	vllm/model_executor/models/__init__.py
447	0	vllm/model_executor/models/phi3_small.py
1	1	vllm/transformers_utils/config.py

[e64fde4b0] leiwen83 2024-05-25 [Core][Bugfix]: fix prefix caching for blockv2 (#4764)
117	0	tests/core/block/test_prefix_caching_block.py
24	17	vllm/core/block/prefix_caching_block.py

[919770957] Robert Shaw 2024-05-24 [Bugfix] Fix Mistral v0.3 Weight Loading (#5005)
1	0	tests/models/test_mistral.py
15	2	vllm/model_executor/model_loader/loader.py
63	1	vllm/model_executor/model_loader/weight_utils.py

[6a50f4caf] youkaichao 2024-05-23 [Doc] add ccache guide in doc (#5012)
4	0	docs/source/getting_started/installation.rst

[e3470f875] Elisei Smirnov 2024-05-24 [Core]: Option To Use Prompt Token Ids Inside Logits Processor (#4985)
14	3	vllm/model_executor/layers/logits_processor.py
10	5	vllm/sampling_params.py

[a1242324c] Dipika Sikka 2024-05-23 [Kernel] Initial Activation Quantization Support (#4525)
1	0	CMakeLists.txt
3	0	csrc/ops.h
3	0	csrc/pybind.cpp
59	0	csrc/quantization/compressed_tensors/int8_quant_kernels.cu
31	0	tests/kernels/test_int8_quant.py
36	0	tests/quantization/test_compressed_tensors.py
18	0	vllm/_custom_ops.py
161	83	vllm/model_executor/layers/linear.py
3	0	vllm/model_executor/layers/quantization/__init__.py
0	0	vllm/model_executor/layers/quantization/compressed_tensors/__init__.py
151	0	vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py
5	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/__init__.py
33	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_scheme.py
39	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_unquantized.py
119	0	vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_statictensor.py
7	0	vllm/model_executor/model_loader/weight_utils.py
14	11	vllm/model_executor/models/llama.py

[5eda2ea02] Murali Andoorveedu 2024-05-23 [Core][1/N] Support send/recv in PyNCCL Groups (#4988)
69	6	tests/distributed/test_pynccl.py
12	6	vllm/distributed/communication_op.py
34	0	vllm/distributed/device_communicators/pynccl.py
26	0	vllm/distributed/device_communicators/pynccl_wrapper.py
29	5	vllm/distributed/parallel_state.py

[2ba80bed2] Letian Li 2024-05-23 [Bugfix] Update Dockerfile.cpu to fix NameError: name 'vllm_ops' is not defined (#5009)
1	1	.buildkite/run-cpu-test.sh
2	0	Dockerfile.cpu

[606625329] Alexander Matveev 2024-05-23 Marlin 24 prefill performance improvement (about 25% better on average) (#4983)
62	12	benchmarks/kernels/benchmark_marlin.py
40	15	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
1	1	tests/kernels/test_marlin_gemm.py
4	4	vllm/model_executor/layers/quantization/gptq_marlin_24.py

[ee3eea0a1] Cody Yu 2024-05-22 [Misc] Take user preference in attention selector (#4960)
84	0	tests/kernels/test_attention_selector.py
1	0	vllm/attention/backends/flashinfer.py
84	61	vllm/attention/selector.py

[a36de682d] Philipp Moritz 2024-05-22 [Minor] Fix small typo in llama.py: QKVParallelLinear -> QuantizationConfig (#4991)
1	1	vllm/model_executor/models/llama.py

[eb6d3c264] Nick Hill 2024-05-22 [Core] Eliminate parallel worker per-step task scheduling overhead (#4894)
9	1	vllm/engine/async_llm_engine.py
8	0	vllm/engine/llm_engine.py
97	26	vllm/executor/distributed_gpu_executor.py
8	0	vllm/executor/executor_base.py
44	29	vllm/executor/multiproc_gpu_executor.py
43	43	vllm/executor/ray_gpu_executor.py
3	1	vllm/spec_decode/ngram_worker.py
61	64	vllm/spec_decode/spec_decode_worker.py
3	2	vllm/worker/embedding_model_runner.py
3	2	vllm/worker/model_runner.py
65	38	vllm/worker/worker.py
4	3	vllm/worker/worker_base.py

[97b030005] raywanb 2024-05-23 [Model] LoRA gptbigcode implementation (#3949)
4	0	csrc/punica/bgmv/bgmv_config.h
2	0	tests/lora/test_punica.py
2	0	vllm/lora/models.py
26	5	vllm/model_executor/models/gpt_bigcode.py

[a3a73ab06] Cody Yu 2024-05-22 [Misc] Load FP8 kv-cache scaling factors from checkpoints (#4893)
6	8	benchmarks/benchmark_latency.py
5	7	benchmarks/benchmark_throughput.py
4	6	benchmarks/kernels/benchmark_paged_attention.py
52	28	tests/models/test_fp8.py
25	2	vllm/attention/layer.py
3	5	vllm/config.py
3	4	vllm/engine/arg_utils.py
45	2	vllm/model_executor/layers/quantization/fp8.py
2	1	vllm/model_executor/models/arctic.py
4	2	vllm/model_executor/models/baichuan.py
2	1	vllm/model_executor/models/bloom.py
6	7	vllm/model_executor/models/chatglm.py
6	7	vllm/model_executor/models/commandr.py
6	7	vllm/model_executor/models/dbrx.py
2	1	vllm/model_executor/models/deepseek.py
6	3	vllm/model_executor/models/falcon.py
2	1	vllm/model_executor/models/gemma.py
2	1	vllm/model_executor/models/gpt2.py
2	1	vllm/model_executor/models/gpt_bigcode.py
2	1	vllm/model_executor/models/gpt_j.py
2	1	vllm/model_executor/models/gpt_neox.py
2	1	vllm/model_executor/models/internlm2.py
6	7	vllm/model_executor/models/jais.py
18	14	vllm/model_executor/models/llama.py
2	1	vllm/model_executor/models/minicpm.py
21	8	vllm/model_executor/models/mixtral.py
7	8	vllm/model_executor/models/mixtral_quant.py
2	1	vllm/model_executor/models/mpt.py
2	1	vllm/model_executor/models/olmo.py
2	1	vllm/model_executor/models/opt.py
2	1	vllm/model_executor/models/orion.py
2	1	vllm/model_executor/models/phi.py
2	1	vllm/model_executor/models/qwen.py
2	1	vllm/model_executor/models/qwen2.py
2	1	vllm/model_executor/models/qwen2_moe.py
2	1	vllm/model_executor/models/stablelm.py
7	8	vllm/model_executor/models/starcoder2.py
2	1	vllm/model_executor/models/xverse.py
2	0	vllm/utils.py
12	5	vllm/worker/model_runner.py

[8674f9880] Tyler Michael Smith 2024-05-22 [Kernel] Fixup for CUTLASS kernels in CUDA graphs (#4954)
5	1	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu
4	1	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
41	0	tests/kernels/test_cutlass.py

[c74c913bf] SangBin Cho 2024-05-22 [misc] remove comments that were supposed to be removed (#4977)
0	1	tests/lora/conftest.py
0	2	vllm/lora/models.py

[5f6d10c14] Michael Goin 2024-05-22 [CI/Build] Enforce style for C++ and CUDA code with `clang-format` (#4722)
26	0	.clang-format
42	0	.github/workflows/clang-format.yml
64	75	csrc/activation_kernels.cu
10	9	csrc/attention/attention_generic.cuh
299	337	csrc/attention/attention_kernels.cu
6	5	csrc/attention/attention_utils.cuh
39	35	csrc/attention/dtype_bfloat16.cuh
47	45	csrc/attention/dtype_float16.cuh
33	55	csrc/attention/dtype_float32.cuh
16	16	csrc/attention/dtype_fp8.cuh
16	28	csrc/cache.h
131	157	csrc/cache_kernels.cu
28	32	csrc/cpu/activation.cpp
205	206	csrc/cpu/attention.cpp
27	26	csrc/cpu/cache.cpp
16	16	csrc/cpu/layernorm.cpp
33	33	csrc/cpu/pos_encoding.cpp
23	52	csrc/cpu/pybind.cpp
5	4	csrc/cuda_compat.h
2	5	csrc/cuda_utils.h
17	23	csrc/cuda_utils_kernels.cu
27	28	csrc/custom_all_reduce.cu
51	54	csrc/custom_all_reduce.cuh
19	19	csrc/custom_all_reduce_test.cu
20	22	csrc/dispatch_utils.h
121	121	csrc/layernorm_kernels.cu
2	1	csrc/moe/moe_ops.cpp
3	5	csrc/moe/moe_ops.h
110	101	csrc/moe_align_block_size_kernels.cu
121	209	csrc/ops.h
103	126	csrc/pos_encoding_kernels.cu
56	86	csrc/pybind.cpp
211	325	csrc/quantization/aqlm/gemm_kernels.cu
76	62	csrc/quantization/awq/dequantize.cuh
357	254	csrc/quantization/awq/gemm_kernels.cu
19	19	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu
11	11	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
24	23	csrc/quantization/cutlass_w8a8/scaled_mm_dq_entry.cu
93	123	csrc/quantization/fp8/amd/hip_float8.h
260	260	csrc/quantization/fp8/amd/hip_float8_impl.h
356	355	csrc/quantization/fp8/amd/quant_utils.cuh
38	49	csrc/quantization/fp8/common.cu
70	68	csrc/quantization/fp8/nvidia/quant_utils.cuh
35	35	csrc/quantization/gptq/compat.cuh
262	241	csrc/quantization/gptq/matrix_view.cuh
1611	1830	csrc/quantization/gptq/q_gemm.cu
48	59	csrc/quantization/gptq/qdq_2.cuh
127	119	csrc/quantization/gptq/qdq_3.cuh
91	112	csrc/quantization/gptq/qdq_4.cuh
12	22	csrc/quantization/gptq/qdq_8.cuh
27	31	csrc/quantization/gptq/qdq_util.cuh
360	336	csrc/quantization/gptq_marlin/gptq_marlin.cu
28	22	csrc/quantization/gptq_marlin/gptq_marlin.cuh
52	37	csrc/quantization/gptq_marlin/gptq_marlin_dtypes.cuh
46	48	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
229	231	csrc/quantization/marlin/dense/marlin_cuda_kernel.cu
7	5	csrc/quantization/marlin/sparse/common/base.h
34	30	csrc/quantization/marlin/sparse/common/mem.h
56	51	csrc/quantization/marlin/sparse/common/mma.h
218	228	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
27	36	csrc/quantization/squeezellm/quant_cuda_kernel.cu
11	9	csrc/reduction_utils.cuh
56	1	format.sh
1	0	requirements-dev.txt

[9b9a10d6c] sasha0552 2024-05-22 [Frontend] Dynamic RoPE scaling (#4638)
55	1	tests/test_config.py
6	1	vllm/config.py
13	5	vllm/engine/arg_utils.py
6	4	vllm/engine/llm_engine.py
9	1	vllm/transformers_utils/config.py

[99eff67ba] Isotr0py 2024-05-22 [Bugfix][Kernel] Add head size check for attention backend selection (#4944)
8	4	vllm/attention/backends/flash_attn.py
13	3	vllm/attention/selector.py

[14772eeb8] Kante Yin 2024-05-22 [Bugfix] Fix flag name for  `max_seq_len_to_capture` (#4935)
2	2	vllm/engine/arg_utils.py

[757b62c49] Michael Goin 2024-05-21 [CI/Build] Codespell ignore `build/` directory (#4945)
1	1	pyproject.toml

[e941f8858] Simon Mo 2024-05-21 [Docs] Add acknowledgment for sponsors (#4925)
25	0	README.md
24	0	docs/source/community/sponsors.md
1	0	docs/source/index.rst

[f12c3b5b3] Isotr0py 2024-05-21 [Model] Add Phi-2 LoRA support (#4886)
1	1	docs/source/models/supported_models.rst
5	0	tests/lora/conftest.py
67	0	tests/lora/test_phi.py
27	6	vllm/model_executor/models/phi.py

[d130b573a] HUANG Fei 2024-05-21 [Model] add rope_scaling support for qwen2 (#4930)
6	2	vllm/model_executor/models/qwen2.py

[65ae8c2c8] Antoni Baum 2024-05-20 [Core] Fix scheduler considering "no LoRA" as "LoRA" (#4897)
2	2	vllm/core/scheduler.py

[c3af44722] Kuntai Du 2024-05-20 [Doc]Add documentation to benchmarking script when running TGI (#4920)
4	0	benchmarks/benchmark_serving.py
1	1	benchmarks/launch_tgi_server.sh

[1937e2984] Aurick Qiao 2024-05-20 [Core] Sharded State Loader download from HF (#4889)
15	1	vllm/model_executor/model_loader/loader.py

[f0eecee61] Mor Zusman 2024-05-20 [Bugfix] Fix dummy weight for fp8 (#4916)
8	1	vllm/model_executor/model_loader/weight_utils.py

[943e72ca5] Alexei-V-Ivanov-AMD 2024-05-20 [Build/CI] Enabling AMD Entrypoints Test (#4834)
2	1	.buildkite/test-pipeline.yaml
6	2	Dockerfile.rocm
2	1	requirements-rocm.txt
6	2	tests/spec_decode/e2e/conftest.py

[546a97ef6] Wenwei Zhang 2024-05-21 [Misc]: allow user to specify port in distributed setting (#4914)
7	0	vllm/envs.py
3	0	vllm/utils.py

[da5a0b539] Alexander Matveev 2024-05-20 Remove marlin warning (#4918)
0	4	csrc/quantization/gptq_marlin/gptq_marlin.cu

[6287537a0] Cyrus Leung 2024-05-20 [Model] LLaVA model refactor (#4910)
107	30	vllm/model_executor/models/llava.py

[b57e6c594] Woosuk Kwon 2024-05-19 [Kernel] Add flash-attn back (#4907)
1	1	requirements-cuda.txt
208	0	tests/kernels/test_flash_attn.py
1	1	tests/models/test_big_models.py
5	5	tests/models/test_fp8.py
75	54	vllm/attention/backends/flash_attn.py
14	0	vllm/attention/selector.py

[27ce85476] Alexander Matveev 2024-05-19 [Kernel] Add marlin_24 unit tests (#4901)
74	13	tests/kernels/test_marlin_gemm.py
19	8	vllm/model_executor/layers/quantization/gptq_marlin_24.py
308	0	vllm/model_executor/layers/quantization/utils/format_24.py
58	0	vllm/model_executor/layers/quantization/utils/marlin_24_perms.py
58	0	vllm/model_executor/layers/quantization/utils/marlin_perms.py
132	82	vllm/model_executor/layers/quantization/utils/marlin_utils.py

[f68470e80] Cyrus Leung 2024-05-19 [Bugfix][Model] Add base class for vision-language models (#4809)
9	0	tests/models/test_registry.py
7	6	vllm/model_executor/model_loader/loader.py
25	23	vllm/model_executor/models/llava.py
12	0	vllm/model_executor/models/vlm_base.py

[2e9a2227e] SangBin Cho 2024-05-18 [Lora] Support long context lora  (#4787)
15	1	.buildkite/test-pipeline.yaml
2	3	format.sh
1	1	pyproject.toml
50	0	tests/lora/conftest.py
0	0	tests/lora/data/__init__.py
97	0	tests/lora/data/long_context_test_data.py
98	2	tests/lora/test_layers.py
292	0	tests/lora/test_long_context.py
2	1	vllm/config.py
18	10	vllm/core/scheduler.py
14	1	vllm/engine/arg_utils.py
3	1	vllm/engine/output_processor/multi_step.py
6	2	vllm/engine/output_processor/single_step.py
17	4	vllm/engine/output_processor/stop_checker.py
106	1	vllm/lora/layers.py
164	20	vllm/lora/models.py
2	0	vllm/lora/request.py
12	5	vllm/lora/utils.py
20	7	vllm/lora/worker_manager.py
47	2	vllm/model_executor/layers/rotary_embedding.py
2	0	vllm/model_executor/models/chatglm.py
2	6	vllm/model_executor/models/llama.py
2	0	vllm/transformers_utils/configs/chatglm.py
18	2	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
9	3	vllm/worker/model_runner.py

[c0724fc91] alexeykondrat 2024-05-18 [ROCm][Hardware][AMD] Adding Navi21 to fallback to naive attention if Triton is not used (#4658)
3	2	vllm/attention/backends/rocm_flash_attn.py

[86b45ae06] Michael Goin 2024-05-17 [Bugfix] Relax tiktoken to >= 0.6.0 (#4890)
1	1	requirements-common.txt

[c5711ef98] Antoni Baum 2024-05-17 [Doc] Update Ray Data distributed offline inference example (#4871)
42	6	examples/offline_inference_distributed.py

[48d5985a0] eigenLiu 2024-05-18 Sync huggingface modifications of qwen Moe model (#4774)
9	2	vllm/model_executor/models/qwen2_moe.py

[33e0823de] Jinzhen Lin 2024-05-17 [Bugfix] fix rope error when load models with different dtypes  (#4835)
43	1	tests/kernels/test_pos_encoding.py
21	12	vllm/model_executor/layers/rotary_embedding.py

[26148120b] Alexei-V-Ivanov-AMD 2024-05-16 [Build/CI] Extending the set of AMD tests with Regression, Basic Correctness, Distributed, Engine, Llava Tests (#4797)
6	5	.buildkite/run-amd-test.sh
15	3	.buildkite/test-pipeline.yaml
1	2	.buildkite/test-template.j2
5	1	tests/engine/test_stop_reason.py
1	9	vllm/config.py

[0150a1063] bofeng huang 2024-05-17 [Frontend] OpenAI API server: Do not add bos token by default when encoding (#4688)
1	1	vllm/entrypoints/openai/serving_chat.py
21	11	vllm/entrypoints/openai/serving_engine.py

[8e7fb5d43] Kante Yin 2024-05-17 Support to serve vLLM on Kubernetes with LWS (#4829)
12	0	docs/source/serving/deploying_with_lws.rst
1	0	docs/source/serving/integrations.rst

[9a31a817a] Woosuk Kwon 2024-05-16 [Bugfix] Fix FP8 KV cache support (#4869)
5	5	vllm/attention/backends/flash_attn.py
5	5	vllm/attention/backends/flashinfer.py
5	5	vllm/attention/backends/rocm_flash_attn.py
5	5	vllm/attention/backends/torch_sdpa.py
5	5	vllm/attention/backends/xformers.py
1	1	vllm/attention/layer.py

[2060e9365] Tyler Michael Smith 2024-05-16 [Kernel] Add w8a8 CUTLASS kernels (#4749)
26	1	CMakeLists.txt
8	0	csrc/ops.h
1	0	csrc/pybind.cpp
12	0	csrc/quantization/cutlass_w8a8/common.hpp
340	0	csrc/quantization/cutlass_w8a8/cutlass_visitor_2x_broadcast_epilogue.hpp
296	0	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c2x.cu
240	0	csrc/quantization/cutlass_w8a8/scaled_mm_dq_c3x.cu
65	0	csrc/quantization/cutlass_w8a8/scaled_mm_dq_entry.cu
192	0	tests/kernels/test_cutlass.py
17	1	vllm/_custom_ops.py

[8435b207a] Silencio 2024-05-17 [Kernel] Add punica dimension for Qwen1.5-32B LoRA (#4850)
2	0	csrc/punica/bgmv/bgmv_config.h
1	0	tests/lora/test_punica.py

[10fa9eea2] youkaichao 2024-05-16 [Misc] remove old comments (#4866)
0	10	vllm/worker/model_runner.py

[e08188081] youkaichao 2024-05-16 [Core][Distributed] remove graph mode  function (#4818)
3	2	tests/distributed/test_custom_all_reduce.py
2	2	tests/distributed/test_pynccl.py
40	30	vllm/distributed/communication_op.py
18	20	vllm/worker/model_runner.py

[b5853f996] Hongxia Yang 2024-05-16 [ROCm][AMD][Bugfix] adding a missing triton autotune config (#4845)
10	0	vllm/attention/ops/triton_flash_attention.py

[f09edd8a2] Simon Mo 2024-05-16 Add JSON output support for benchmark_latency and benchmark_throughput (#4848)
4	3	.buildkite/run-benchmarks.sh
18	2	benchmarks/benchmark_latency.py
17	0	benchmarks/benchmark_throughput.py

[6979ade38] Alexander Matveev 2024-05-16 Add GPTQ Marlin 2:4 sparse structured support (#4790)
2	1	CMakeLists.txt
11	0	csrc/ops.h
2	1	csrc/pybind.cpp
0	0	csrc/quantization/marlin/{ => dense}/LICENSE
0	0	csrc/quantization/marlin/{ => dense}/marlin_cuda_kernel.cu
203	0	csrc/quantization/marlin/sparse/LICENSE
49	0	csrc/quantization/marlin/sparse/common/base.h
132	0	csrc/quantization/marlin/sparse/common/mem.h
175	0	csrc/quantization/marlin/sparse/common/mma.h
1110	0	csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu
81	0	tests/models/test_gptq_marlin_24.py
10	0	vllm/_custom_ops.py
12	36	vllm/config.py
8	3	vllm/model_executor/layers/quantization/__init__.py
11	0	vllm/model_executor/layers/quantization/base_config.py
23	0	vllm/model_executor/layers/quantization/gptq_marlin.py
280	0	vllm/model_executor/layers/quantization/gptq_marlin_24.py
22	0	vllm/model_executor/layers/quantization/marlin.py

[9216b9cc3] Pierre Dulac 2024-05-16 [Bugfix] Bypass authorization API token for preflight requests (#4862)
2	0	vllm/entrypoints/openai/api_server.py

[5e0391c04] Alex Wu 2024-05-16 [Frontend] Separate OpenAI Batch Runner usage from API Server (#4851)
1	1	vllm/entrypoints/openai/run_batch.py
1	0	vllm/usage/usage_lib.py

[dbc0754dd] Alex Wu 2024-05-16 [docs] Fix typo in examples filename openi -> openai (#4864)
0	0	examples/{openi_example_batch.jsonl => openai_example_batch.jsonl}

[99caa4910] Jinzhen Lin 2024-05-16 [Kernel] add bfloat16 support for gptq marlin kernel (#4788)
173	67	csrc/quantization/gptq_marlin/gptq_marlin.cu
62	0	csrc/quantization/gptq_marlin/gptq_marlin_dtypes.cuh
7	2	tests/models/test_gptq_marlin.py
4	4	vllm/model_executor/layers/quantization/gptq_marlin.py

[5c342570d] alexm-nm 2024-05-16 Add marlin unit tests and marlin benchmark script (#4815)
183	0	benchmarks/kernels/benchmark_marlin.py
75	0	benchmarks/kernels/benchmark_shapes.py
158	0	tests/kernels/test_marlin_gemm.py
0	0	vllm/model_executor/layers/quantization/utils/__init__.py
174	0	vllm/model_executor/layers/quantization/utils/marlin_utils.py
146	0	vllm/model_executor/layers/quantization/utils/quant_utils.py

[973617ae0] Cody Yu 2024-05-16 [Speculative decoding][Re-take] Enable TP>1 speculative decoding (#4840)
1	0	.buildkite/test-pipeline.yaml
6	0	benchmarks/benchmark_latency.py
0	50	tests/spec_decode/e2e/test_compatibility.py
44	0	tests/spec_decode/e2e/test_integration.py
65	0	tests/spec_decode/e2e/test_integration_dist.py
0	37	tests/spec_decode/e2e/test_multistep_correctness.py
5	5	vllm/distributed/communication_op.py
17	54	vllm/executor/gpu_executor.py
12	7	vllm/executor/ray_gpu_executor.py
144	27	vllm/spec_decode/spec_decode_worker.py
2	1	vllm/worker/worker.py
1	1	vllm/worker/worker_base.py

[30e754390] Aurick Qiao 2024-05-16 [Core] Implement sharded state loader (#4690)
75	0	examples/save_sharded_state.py
90	0	tests/test_sharded_state_loader.py
1	0	vllm/config.py
11	0	vllm/executor/distributed_gpu_executor.py
148	0	vllm/model_executor/model_loader/loader.py
14	0	vllm/worker/model_runner.py
12	0	vllm/worker/worker.py

[52f8107cf] Alex Wu 2024-05-15 [Frontend] Support OpenAI batch file format (#4794)
172	0	examples/offline_inference_openai.md
2	0	examples/openi_example_batch.jsonl
1	0	requirements-common.txt
53	0	tests/entrypoints/test_openai_run_batch.py
41	0	vllm/entrypoints/openai/protocol.py
141	0	vllm/entrypoints/openai/run_batch.py
5	3	vllm/entrypoints/openai/serving_chat.py

[fc0d9dfc3] Cyrus Leung 2024-05-16 [Frontend] Re-enable custom roles in Chat Completions API (#4758)
30	0	tests/entrypoints/test_openai_server.py
36	2	vllm/entrypoints/openai/protocol.py
42	24	vllm/entrypoints/openai/serving_chat.py

[361c461a1] Zhuohan Li 2024-05-15 [Doc] Highlight the fourth meetup in the README (#4842)
11	1	README.md

[a5675d348] zifeitong 2024-05-15 [Bugfix] Properly set distributed_executor_backend in ParallelConfig (#4816)
1	0	vllm/config.py
7	3	vllm/engine/arg_utils.py

[e9cdd2b1e] Cyrus Leung 2024-05-15 [CI/Build] Further decouple HuggingFace implementation from ours during tests (#4166)
41	36	tests/conftest.py

[65bf2ac16] SangBin Cho 2024-05-15 [Core][2/N] Model runner refactoring part 2. Combine prepare prefill / decode to a single API (#4681)
84	39	tests/worker/test_model_runner.py
2	3	vllm/attention/__init__.py
32	36	vllm/attention/backends/abstract.py
79	16	vllm/attention/backends/flash_attn.py
28	10	vllm/attention/backends/flashinfer.py
80	18	vllm/attention/backends/rocm_flash_attn.py
22	6	vllm/attention/backends/torch_sdpa.py
77	15	vllm/attention/backends/xformers.py
2	3	vllm/attention/layer.py
5	5	vllm/attention/ops/paged_attn.py
5	0	vllm/engine/arg_utils.py
1	0	vllm/model_executor/layers/rejection_sampler.py
2	1	vllm/sequence.py
16	7	vllm/spec_decode/batch_expansion.py
1	0	vllm/spec_decode/multi_step_worker.py
3	7	vllm/worker/cpu_model_runner.py
15	115	vllm/worker/embedding_model_runner.py
323	449	vllm/worker/model_runner.py

[8a7cc254a] SangBin Cho 2024-05-15 Revert "[Kernel] Use flash-attn for decoding (#3648)" (#4820)
0	209	tests/kernels/test_flash_attn.py
1	1	tests/models/test_big_models.py
5	5	tests/models/test_fp8.py
55	73	vllm/attention/backends/flash_attn.py
0	14	vllm/attention/selector.py
4	11	vllm/worker/model_runner.py

[29bc01bf3] Simon Mo 2024-05-14 Add 4th meetup announcement to readme (#4817)
1	0	README.md

[676a99982] Nick Hill 2024-05-14 [Core] Add MultiprocessingGPUExecutor (#4539)
8	4	.buildkite/test-pipeline.yaml
10	7	tests/distributed/test_basic_distributed_correctness.py
4	0	tests/distributed/test_chunked_prefill_distributed.py
1	2	tests/lora/test_mixtral.py
29	9	vllm/config.py
12	4	vllm/engine/arg_utils.py
10	6	vllm/engine/async_llm_engine.py
7	3	vllm/engine/llm_engine.py
140	0	vllm/executor/multiproc_gpu_executor.py
2	2	vllm/executor/ray_gpu_executor.py
2	2	vllm/executor/ray_utils.py

[dc72402b5] Cyrus Leung 2024-05-15 [Bugfix][Doc] Fix CI failure in docs (#4804)
4	4	docs/source/community/meetups.rst

[ccb63a824] Kuntai Du 2024-05-14 [Core][Hash][Automatic Prefix caching] Accelerating the hashing function by avoiding deep copies (#4696)
63	0	benchmarks/overheads/benchmark_hashing.py
14	2	vllm/sequence.py

[c579b750a] Zhuohan Li 2024-05-13 [Doc] Add meetups to the doc (#4798)
12	0	docs/source/community/meetups.rst
7	0	docs/source/index.rst

[4bfa7e7f7] Cyrus Leung 2024-05-14 [Doc] Add API reference for offline inference (#4710)
7	1	docs/source/index.rst
6	0	docs/source/offline_inference/llm.rst
2	2	docs/source/{dev => offline_inference}/sampling_params.rst
2	2	docs/source/serving/openai_compatible_server.md

[ac1fbf7fd] Zhuohan Li 2024-05-13 [Doc] Shorten README by removing supported model list (#4796)
9	38	README.md
19	6	docs/source/models/supported_models.rst

[33d3914b1] Philipp Moritz 2024-05-13 [Bugfix] Fix dynamic FP8 quantization for Mixtral (#4793)
1	1	vllm/model_executor/models/mixtral.py

[1356df53b] Stephen Krider 2024-05-13 [Kernel] Use flash-attn for decoding (#3648)
209	0	tests/kernels/test_flash_attn.py
1	1	tests/models/test_big_models.py
5	5	tests/models/test_fp8.py
73	55	vllm/attention/backends/flash_attn.py
14	0	vllm/attention/selector.py
11	4	vllm/worker/model_runner.py

[ce532ff45] Cody Yu 2024-05-13 [Speculative decoding] Improve n-gram efficiency (#4724)
9	8	tests/spec_decode/test_ngram_worker.py
8	5	vllm/config.py
52	45	vllm/spec_decode/ngram_worker.py
63	0	vllm/spec_decode/top1_proposer.py

[8bc68e198] Sanger Steel 2024-05-13 [Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)
3	1	.buildkite/test-pipeline.yaml
81	119	examples/tensorize_vllm_model.py
1	1	requirements-dev.txt
1	1	setup.py
0	245	tests/tensorizer_loader/tensorize_vllm_model_for_testing.py
70	119	tests/tensorizer_loader/test_tensorizer.py
2	2	vllm/engine/arg_utils.py
1	1	vllm/envs.py
15	13	vllm/model_executor/model_loader/loader.py
85	21	vllm/model_executor/model_loader/tensorizer.py

[0fca3cdcf] Woosuk Kwon 2024-05-13 [Misc] Enhance attention selector (#4751)
0	1	tests/worker/test_model_runner.py
2	2	vllm/attention/__init__.py
2	3	vllm/attention/backends/abstract.py
7	6	vllm/attention/backends/flash_attn.py
23	10	vllm/attention/backends/flashinfer.py
9	7	vllm/attention/backends/rocm_flash_attn.py
17	11	vllm/attention/backends/torch_sdpa.py
6	6	vllm/attention/backends/xformers.py
17	2	vllm/attention/layer.py
23	5	vllm/attention/selector.py
11	8	vllm/model_executor/model_loader/__init__.py
39	22	vllm/model_executor/model_loader/loader.py
13	3	vllm/model_executor/models/arctic.py
22	7	vllm/model_executor/models/baichuan.py
11	4	vllm/model_executor/models/bloom.py
14	6	vllm/model_executor/models/chatglm.py
11	3	vllm/model_executor/models/commandr.py
13	4	vllm/model_executor/models/dbrx.py
3	1	vllm/model_executor/models/decilm.py
13	3	vllm/model_executor/models/deepseek.py
11	4	vllm/model_executor/models/falcon.py
10	4	vllm/model_executor/models/gemma.py
12	4	vllm/model_executor/models/gpt2.py
10	4	vllm/model_executor/models/gpt_bigcode.py
15	5	vllm/model_executor/models/gpt_j.py
12	4	vllm/model_executor/models/gpt_neox.py
10	3	vllm/model_executor/models/internlm2.py
9	3	vllm/model_executor/models/jais.py
13	4	vllm/model_executor/models/llama.py
4	2	vllm/model_executor/models/llava.py
10	3	vllm/model_executor/models/minicpm.py
11	2	vllm/model_executor/models/mixtral.py
21	10	vllm/model_executor/models/mixtral_quant.py
13	5	vllm/model_executor/models/mpt.py
10	4	vllm/model_executor/models/olmo.py
12	4	vllm/model_executor/models/opt.py
10	3	vllm/model_executor/models/orion.py
12	4	vllm/model_executor/models/phi.py
12	3	vllm/model_executor/models/qwen.py
10	4	vllm/model_executor/models/qwen2.py
13	3	vllm/model_executor/models/qwen2_moe.py
10	4	vllm/model_executor/models/stablelm.py
15	3	vllm/model_executor/models/starcoder2.py
10	4	vllm/model_executor/models/xverse.py
11	3	vllm/worker/cache_engine.py
11	4	vllm/worker/cpu_model_runner.py
9	1	vllm/worker/cpu_worker.py
0	1	vllm/worker/embedding_model_runner.py
11	4	vllm/worker/model_runner.py

[e7c46b952] SangBin Cho 2024-05-13 [Scheduler] Warning upon preemption and Swapping (#4647)
19	0	docs/source/models/performance.rst
43	1	tests/basic_correctness/test_preemption.py
16	0	tests/conftest.py
1	0	tests/core/test_scheduler.py
18	0	vllm/core/scheduler.py
3	1	vllm/engine/llm_engine.py
8	1	vllm/engine/metrics.py

[350f9e107] Cyrus Leung 2024-05-13 [CI/Build] Move `test_utils.py` to `tests/utils.py` (#4425)
11	13	.buildkite/test-pipeline.yaml
0	0	tests/async_engine/__init__.py
3	48	tests/async_engine/test_openapi_server_ray.py
0	0	tests/basic_correctness/__init__.py
0	0	tests/core/block/e2e/__init__.py
2	1	tests/core/block/e2e/conftest.py
0	0	tests/distributed/__init__.py
4	2	tests/distributed/test_basic_distributed_correctness.py
3	2	tests/distributed/test_comm_ops.py
3	2	tests/distributed/test_custom_all_reduce.py
0	0	tests/engine/__init__.py
0	0	tests/engine/output_processor/__init__.py
2	1	tests/engine/output_processor/test_multi_step.py
0	0	tests/entrypoints/__init__.py
2	45	tests/entrypoints/test_openai_server.py
0	0	tests/kernels/__init__.py
2	1	tests/kernels/test_activation.py
2	1	tests/kernels/test_attention.py
2	1	tests/kernels/test_pos_encoding.py
0	0	tests/metrics/__init__.py
0	0	tests/model_executor/__init__.py
0	0	tests/models/__init__.py
2	1	tests/models/test_gptq_marlin.py
2	1	tests/models/test_marlin.py
1	1	tests/models/test_mistral.py
0	0	tests/prefix_caching/__init__.py
0	0	tests/quantization/__init__.py
0	0	tests/samplers/__init__.py
2	1	tests/samplers/test_logprobs.py
2	1	tests/spec_decode/e2e/conftest.py
2	1	tests/tensorizer_loader/test_tensorizer.py
2	1	tests/test_sequence.py
89	0	tests/utils.py
0	40	vllm/test_utils.py

[702bee461] youkaichao 2024-05-12 [Core][Distributed] refactor custom allreduce to support multiple tp groups (#4754)
11	11	tests/distributed/test_comm_ops.py
58	29	tests/distributed/test_custom_all_reduce.py
2	2	tests/distributed/test_pynccl.py
34	11	vllm/distributed/communication_op.py
176	144	vllm/distributed/device_communicators/custom_all_reduce.py
3	1	vllm/distributed/device_communicators/pynccl.py
27	1	vllm/distributed/parallel_state.py
8	9	vllm/test_utils.py
4	11	vllm/worker/model_runner.py
4	7	vllm/worker/worker.py

[a7be4d007] Swapnil Parekh 2024-05-12 [CORE] Improvement in ranks code (#4718)
3	1	vllm/model_executor/layers/sampler.py

[a709e87a4] Robert Shaw 2024-05-12 [CI/Build] Tweak Marlin Nondeterminism Issues (#4713)
5	7	tests/models/test_gptq_marlin.py

[6eaccb735] Yikang Shen 2024-05-12 [Model] Add support for IBM Granite Code models (#4636)
6	2	vllm/model_executor/models/llama.py

[e254497b6] Chang Su 2024-05-11 [Model][Misc] Add e5-mistral-7b-instruct and Embedding API (#3734)
17	0	examples/offline_inference_embedding.py
23	0	examples/openai_embedding_client.py
6	3	requirements-dev.txt
30	8	tests/conftest.py
6	6	tests/engine/output_processor/test_multi_step.py
1	0	tests/entrypoints/openai/test_serving_chat.py
95	1	tests/entrypoints/test_openai_server.py
44	0	tests/models/test_embedding.py
3	3	tests/samplers/test_logits_processor.py
1	1	tests/samplers/test_seeded_generate.py
3	3	tests/spec_decode/utils.py
6	6	tests/test_sequence.py
6	1	vllm/__init__.py
15	0	vllm/config.py
84	0	vllm/core/embedding_model_block_manager.py
5	0	vllm/core/interfaces.py
8	2	vllm/core/scheduler.py
1	0	vllm/engine/arg_utils.py
130	28	vllm/engine/async_llm_engine.py
115	28	vllm/engine/llm_engine.py
121	29	vllm/entrypoints/llm.py
18	2	vllm/entrypoints/openai/api_server.py
35	1	vllm/entrypoints/openai/protocol.py
134	0	vllm/entrypoints/openai/serving_embedding.py
14	2	vllm/entrypoints/openai/serving_engine.py
5	5	vllm/executor/gpu_executor.py
56	0	vllm/model_executor/layers/pooler.py
4	3	vllm/model_executor/layers/sampler.py
11	1	vllm/model_executor/models/__init__.py
87	0	vllm/model_executor/models/llama_embedding.py
69	0	vllm/model_executor/pooling_metadata.py
81	1	vllm/outputs.py
20	0	vllm/pooling_params.py
77	12	vllm/sequence.py
3	2	vllm/spec_decode/util.py
266	0	vllm/worker/embedding_model_runner.py
18	7	vllm/worker/model_runner.py
9	5	vllm/worker/worker.py

[4e1213108] youkaichao 2024-05-10 [Core][Test] fix function name typo in custom allreduce (#4750)
2	2	tests/distributed/test_custom_all_reduce.py
4	0	vllm/distributed/device_communicators/custom_all_reduce.py

[fcc2994be] Robert Shaw 2024-05-10 [CI] Nits for bad initialization of SeqGroup in testing (#4748)
9	4	tests/core/test_block_manager.py
7	4	tests/core/utils.py

[2e7796f2c] heeju-kim2 2024-05-11 [Speculative decoding] CUDA graph support (#4295)
37	0	tests/spec_decode/e2e/test_multistep_correctness.py

[706588a77] Allen.Dou 2024-05-10 [Bugfix] Fix CLI arguments in OpenAI server docs (#4729)
1	0	docs/requirements-docs.txt

[6a0f61721] SangBin Cho 2024-05-10 [Core] Fix circular reference which leaked llm instance in local dev env (#4737)
13	0	tests/basic_correctness/test_basic_correctness.py
5	5	vllm/model_executor/model_loader/tensorizer.py
2	1	vllm/spec_decode/multi_step_worker.py
2	1	vllm/spec_decode/ngram_worker.py

[dac6a3f6e] Steve Grubb 2024-05-10 [Misc] Apply a couple g++ cleanups (#4719)
1	1	csrc/cpu/cache.cpp
0	1	csrc/cpu/pos_encoding.cpp

[64b77dfd7] Kunshang Ji 2024-05-10 [Core]fix type annotation for `swap_blocks` (#4726)
2	2	vllm/_custom_ops.py

[51d4094fd] Simon Mo 2024-05-09 chunked-prefill-doc-syntax (#4603)
21	15	docs/source/models/performance.rst

[e965d4618] Allen.Dou 2024-05-10 [Misc] Keep only one implementation of the create_dummy_prompt function. (#4716)
4	32	tests/test_sequence.py

[208b71bcc] youkaichao 2024-05-09 [Core][Distributed] refactor pynccl (#4591)
42	36	tests/distributed/test_pynccl.py
24	4	vllm/distributed/communication_op.py
78	216	vllm/distributed/device_communicators/pynccl.py
0	66	vllm/distributed/device_communicators/pynccl_utils.py
258	0	vllm/distributed/device_communicators/pynccl_wrapper.py
60	71	vllm/distributed/parallel_state.py
5	20	vllm/worker/model_runner.py
0	21	vllm/worker/worker.py

[c83310174] Cody Yu 2024-05-09 [Kernel] Refactor FP8 kv-cache with NVIDIA float8_e4m3 support (#4535)
1	1	.buildkite/check-wheel-size.py
1	1	CMakeLists.txt
2	2	cmake/utils.cmake
110	176	csrc/attention/attention_kernels.cu
11	5	csrc/attention/dtype_fp8.cuh
3	1	csrc/cache.h
70	73	csrc/cache_kernels.cu
0	0	csrc/quantization/fp8/{amd_detail => amd}/hip_float8.h
0	0	csrc/quantization/fp8/{amd_detail => amd}/hip_float8_impl.h
58	1	csrc/quantization/fp8/{amd_detail => amd}/quant_utils.cuh
0	0	csrc/quantization/fp8/{fp8_cuda_kernels.cu => common.cu}
568	0	csrc/quantization/fp8/nvidia/quant_utils.cuh
0	277	csrc/quantization/fp8_e5m2_kvcache/quant_utils.cuh
2	2	tests/kernels/test_attention.py
11	16	tests/kernels/test_cache.py
5	2	vllm/_custom_ops.py
1	1	vllm/utils.py

[379da6dcb] Philipp Moritz 2024-05-09 [Kernel] [FP8] Improve FP8 linear layer performance (#4691)
27	1	vllm/_custom_ops.py
9	4	vllm/model_executor/layers/quantization/fp8.py

[ebce310b7] Hao Zhang 2024-05-09 [Model] Snowflake arctic model implementation (#4652)
26	0	examples/offline_inference_arctic.py
3	1	vllm/model_executor/layers/fused_moe/__init__.py
90	47	vllm/model_executor/layers/fused_moe/fused_moe.py
3	0	vllm/model_executor/layers/quantization/__init__.py
194	0	vllm/model_executor/layers/quantization/deepspeedfp.py
1	0	vllm/model_executor/models/__init__.py
521	0	vllm/model_executor/models/arctic.py
204	0	vllm/transformers_utils/configs/arctic.py

[be0c5180a] Michael Goin 2024-05-09 [Bugfix] Add logs for all model dtype casting (#4717)
3	0	vllm/config.py

[cea64430f] Robert Shaw 2024-05-09 [Bugfix] Update grafana.json (#4711)
239	193	examples/production_monitoring/grafana.json

[a3c124570] Cyrus Leung 2024-05-10 [Bugfix] Fix CLI arguments in OpenAI server docs (#4709)
1	1	docs/source/serving/openai_compatible_server.md

[ff5abcd74] kliuae 2024-05-10 [ROCm] Add support for Punica kernels on AMD GPUs (#3140)
10	6	CMakeLists.txt
3	0	Dockerfile.rocm
6	0	csrc/cuda_compat.h
154	0	csrc/punica/bgmv/bgmv_impl.cuh
3	2	csrc/punica/bgmv/vec_dtypes.cuh
2	15	csrc/punica/{punica_ops.cc => punica_ops.cu}
11	0	csrc/punica/punica_ops.h
13	0	csrc/punica/punica_pybind.cpp
82	0	csrc/punica/type_convert.h
3	3	setup.py

[0ee535b29] Woosuk Kwon 2024-05-09 [Misc] Set block size at initialization & Fix test_model_runner (#4705)
32	58	tests/worker/test_model_runner.py
9	12	vllm/worker/cpu_model_runner.py
1	0	vllm/worker/cpu_worker.py
21	33	vllm/worker/model_runner.py
1	1	vllm/worker/worker.py

[190bc838e] Woosuk Kwon 2024-05-09 [Misc] Remove unnecessary ModelRunner imports (#4703)
24	57	tests/samplers/test_sampler.py
7	16	tests/test_logits_processor.py

[f12b20dec] Cyrus Leung 2024-05-09 [Frontend] Move async logic outside of constructor (#4674)
13	17	tests/async_engine/test_chat_template.py
6	2	tests/entrypoints/openai/test_serving_chat.py
1	1	vllm/engine/arg_utils.py
20	3	vllm/entrypoints/openai/api_server.py
35	37	vllm/entrypoints/openai/serving_chat.py
4	3	vllm/entrypoints/openai/serving_completion.py
17	39	vllm/entrypoints/openai/serving_engine.py

[16bc0a098] Mahmoud Ashraf 2024-05-09 [Frontend] add tok/s speed metric to llm class when using tqdm (#4400)
12	4	vllm/entrypoints/llm.py

[e288df063] alexm-nm 2024-05-08 [Bugfix] Fine-tune gptq_marlin configs to be more similar to marlin (#4626)
35	13	csrc/quantization/gptq_marlin/gptq_marlin.cu

[8b9241be3] Cade Daniel 2024-05-08 [Speculative decoding] [Bugfix] Fix overallocation in ngram + spec logprobs (#4672)
1	1	vllm/spec_decode/ngram_worker.py

[f942efb5a] Cody Yu 2024-05-08 [Dynamic Spec Decoding] Auto-disable by the running queue size (#4592)
9	4	tests/samplers/test_rejection_sampler.py
34	0	tests/spec_decode/e2e/test_multistep_correctness.py
1	1	tests/spec_decode/e2e/test_ngram_correctness.py
77	0	tests/spec_decode/test_dynamic_spec_decode.py
24	5	vllm/config.py
10	0	vllm/engine/arg_utils.py
2	0	vllm/executor/gpu_executor.py
9	2	vllm/model_executor/layers/rejection_sampler.py
6	0	vllm/sequence.py
38	21	vllm/spec_decode/spec_decode_worker.py
17	6	vllm/spec_decode/top1_proposer.py

[89579a201] Woosuk Kwon 2024-05-08 [Misc] Use vllm-flash-attn instead of flash-attn (#4686)
0	21	Dockerfile
1	0	requirements-cuda.txt
9	5	setup.py
1	1	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/flashinfer.py
4	3	vllm/attention/selector.py

[230c4b38c] youkaichao 2024-05-08 [CI/Test] fix swap test for multi gpu (#4689)
3	1	tests/kernels/test_cache.py

[20cfcdec9] youkaichao 2024-05-08 [Core][Optimization] change python dict to pytorch tensor for blocks to swap (#4659)
2	2	csrc/cache.h
11	5	csrc/cache_kernels.cu
2	2	csrc/cpu/cache.cpp
2	2	tests/core/test_block_manager.py
16	16	tests/core/test_chunked_prefill_scheduler.py
17	17	tests/core/test_scheduler.py
9	4	tests/kernels/test_cache.py
12	12	tests/worker/test_swap.py
1	1	vllm/attention/backends/abstract.py
2	2	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/flashinfer.py
2	2	vllm/attention/backends/rocm_flash_attn.py
2	2	vllm/attention/backends/torch_sdpa.py
2	2	vllm/attention/ops/paged_attn.py
8	4	vllm/core/block_manager_v1.py
2	2	vllm/core/block_manager_v2.py
3	3	vllm/core/interfaces.py
16	16	vllm/core/scheduler.py
4	4	vllm/sequence.py
3	3	vllm/worker/cache_engine.py
20	7	vllm/worker/worker.py

[ad932a221] Antoni Baum 2024-05-08 [Core] Faster startup for LoRA enabled models (#4634)
10	0	vllm/lora/models.py
22	4	vllm/lora/worker_manager.py
15	14	vllm/worker/model_runner.py

[5510cf0e8] Woosuk Kwon 2024-05-08 [Misc] Add `get_name` method to attention backends (#4685)
5	0	vllm/attention/backends/abstract.py
4	0	vllm/attention/backends/flash_attn.py
7	9	vllm/attention/backends/flashinfer.py
4	0	vllm/attention/backends/rocm_flash_attn.py
4	0	vllm/attention/backends/torch_sdpa.py
4	0	vllm/attention/backends/xformers.py
2	3	vllm/worker/model_runner.py

[0f9a6e3d2] DefTruth 2024-05-09 [Bugfix][Kernel] allow non-power-of-2 for prefix prefill with alibi  (#4573)
242	1	tests/kernels/test_prefix_prefill.py
25	16	vllm/attention/ops/prefix_prefill.py

[f6a593093] SangBin Cho 2024-05-09 [CI] Make mistral tests pass (#4596)
1	1	.buildkite/test-pipeline.yaml
62	0	tests/conftest.py
1	1	tests/models/test_big_models.py
18	15	tests/models/test_mistral.py
3	2	vllm/model_executor/layers/rotary_embedding.py

[d7740ea4d] SangBin Cho 2024-05-09 [Core] Optimize sampler get_logprobs (#4594)
68	49	vllm/model_executor/layers/sampler.py

[cc466a329] youkaichao 2024-05-07 [Core][Distributed] support cpu&device in broadcast tensor dict (#4660)
6	1	tests/distributed/test_comm_ops.py
35	21	vllm/distributed/communication_op.py

[8344f7742] leiwen83 2024-05-08 [Bug fix][Core] fixup ngram not setup correctly (#4551)
18	6	tests/spec_decode/e2e/conftest.py
4	0	vllm/executor/gpu_executor.py
7	7	vllm/spec_decode/spec_decode_worker.py

[469f85c78] youkaichao 2024-05-07 [Core][Optimization] change copy-on-write from dict[int, list] to list (#4648)
2	4	tests/core/block/test_block_table.py
5	1	tests/core/test_block_manager.py
2	2	tests/core/test_scheduler.py
10	11	vllm/core/block/common.py
4	4	vllm/core/block/cpu_gpu_block_allocator.py
3	3	vllm/core/block/interfaces.py
4	4	vllm/core/block/naive_block.py
4	4	vllm/core/block/prefix_caching_block.py
5	5	vllm/core/block_manager_v1.py
2	1	vllm/core/block_manager_v2.py
2	1	vllm/core/interfaces.py
1	4	vllm/core/scheduler.py

[10760da80] Austin Veselka 2024-05-07 [Bugfix] Fixed error in slice_lora_b for MergedQKVParallelLinearWithLora (#4609)
30	24	vllm/lora/fully_sharded_layers.py
22	8	vllm/lora/layers.py

[478aed582] Alexei-V-Ivanov-AMD 2024-05-07 [Build/CI] Fixing 'docker run' to re-enable AMD CI tests. (#4642)
1	1	.buildkite/run-amd-test.sh
4	4	.buildkite/test-pipeline.yaml
1	1	.buildkite/test-template.j2

[63575bc2e] youkaichao 2024-05-06 [Core][Optimization] change python dict to pytorch tensor (#4607)
1	1	csrc/cache.h
5	15	csrc/cache_kernels.cu
6	14	csrc/cpu/cache.cpp
4	4	tests/core/test_scheduler.py
12	9	tests/kernels/test_cache.py
1	1	tests/worker/test_swap.py
1	1	vllm/attention/backends/abstract.py
1	1	vllm/attention/backends/flash_attn.py
1	1	vllm/attention/backends/flashinfer.py
1	1	vllm/attention/backends/rocm_flash_attn.py
1	1	vllm/attention/backends/torch_sdpa.py
1	1	vllm/attention/backends/xformers.py
1	1	vllm/attention/ops/paged_attn.py
20	21	vllm/core/scheduler.py
7	0	vllm/distributed/communication_op.py
3	3	vllm/sequence.py
1	1	vllm/worker/cache_engine.py
5	2	vllm/worker/cpu_worker.py
5	3	vllm/worker/worker.py

[a98187cf7] Philipp Moritz 2024-05-06 [Kernel] Make static FP8 scaling more robust (#4570)
10	1	csrc/quantization/fp8/fp8_cuda_kernels.cu

[bd99d2262] Noam Gat 2024-05-07 Update lm-format-enforcer to 0.10.1 (#4631)
1	1	requirements-common.txt

[19cb4716e] Cade Daniel 2024-05-06 [CI] Add retry for agent lost (#4633)
4	0	.buildkite/test-template.j2

[e186d37cb] Simon Mo 2024-05-06 [CI] use ccache actions properly in release workflow (#4629)
3	0	.github/workflows/publish.yml

[323f27b90] Cyrus Leung 2024-05-07 [Bugfix] Fix `asyncio.Task` not being subscriptable (#4623)
3	3	vllm/engine/async_llm_engine.py
2	2	vllm/entrypoints/openai/api_server.py

[0650e5935] zhaoyang-star 2024-05-06 Disable cuda version check in vllm-openai image (#4530)
1	10	vllm/config.py
1	23	vllm/utils.py

[c7f2cf2b7] Simon Mo 2024-05-04 [CI] Reduce wheel size by not shipping debug symbols (#4602)
3	0	.buildkite/check-wheel-size.py
2	0	.github/workflows/publish.yml

[8d8357c8e] Simon Mo 2024-05-04 bump version to v0.4.2 (#4600)
1	1	.github/workflows/scripts/create_release.js
1	1	vllm/__init__.py

[430298706] DearPlanet 2024-05-05 [Bugfix] Fix inappropriate content of model_name tag in Prometheus metrics (#3937)
30	0	tests/metrics/test_metrics.py
25	0	vllm/config.py
18	2	vllm/engine/arg_utils.py
3	2	vllm/engine/llm_engine.py
0	10	vllm/entrypoints/openai/cli_args.py

[021b1a2ab] Simon Mo 2024-05-04 [CI] check size of the wheels (#4319)
33	0	.buildkite/check-wheel-size.py
8	4	Dockerfile

[2a052011c] Michael Goin 2024-05-04 [Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#4527)
2	2	tests/kernels/test_moe.py
120	51	vllm/model_executor/models/mixtral.py

[36fb68f94] SangBin Cho 2024-05-04 [Doc] Chunked Prefill Documentation (#4580)
1	0	docs/source/index.rst
38	0	docs/source/models/performance.rst
3	2	vllm/config.py

[bc8ad6845] Cody Yu 2024-05-03 [Misc][Refactor] Introduce ExecuteModelData (#4540)
48	50	tests/spec_decode/test_multi_step_worker.py
29	35	tests/spec_decode/test_ngram_worker.py
46	49	tests/spec_decode/test_spec_decode_worker.py
4	46	tests/spec_decode/utils.py
20	10	tests/worker/test_swap.py
4	0	vllm/core/scheduler.py
10	6	vllm/engine/async_llm_engine.py
8	4	vllm/engine/llm_engine.py
10	27	vllm/executor/cpu_executor.py
7	15	vllm/executor/executor_base.py
7	26	vllm/executor/gpu_executor.py
14	19	vllm/executor/neuron_executor.py
5	14	vllm/executor/ray_gpu_executor.py
31	1	vllm/sequence.py
10	20	vllm/spec_decode/batch_expansion.py
3	12	vllm/spec_decode/interfaces.py
21	33	vllm/spec_decode/multi_step_worker.py
21	41	vllm/spec_decode/ngram_worker.py
23	67	vllm/spec_decode/spec_decode_worker.py
11	11	vllm/spec_decode/top1_proposer.py
13	12	vllm/worker/cpu_worker.py
11	12	vllm/worker/worker.py
3	5	vllm/worker/worker_base.py

[344bf7cd2] youkaichao 2024-05-03 [Misc] add installation time env vars (#4574)
23	10	setup.py
58	8	vllm/envs.py

[ab5027511] Cade Daniel 2024-05-03 [Speculative decoding] Support target-model logprobs (#4378)
63	3	tests/spec_decode/e2e/conftest.py
335	0	tests/spec_decode/e2e/test_logprobs.py
47	16	tests/spec_decode/e2e/test_multistep_correctness.py
8	0	tests/spec_decode/test_multi_step_worker.py
24	5	tests/spec_decode/test_spec_decode_worker.py
2	0	tests/spec_decode/utils.py
12	6	vllm/engine/output_processor/multi_step.py
9	7	vllm/model_executor/layers/sampler.py
3	0	vllm/sequence.py
41	18	vllm/spec_decode/batch_expansion.py
5	0	vllm/spec_decode/interfaces.py
6	0	vllm/spec_decode/ngram_worker.py
74	26	vllm/spec_decode/spec_decode_worker.py
1	1	vllm/spec_decode/top1_proposer.py
98	5	vllm/spec_decode/util.py

[43c413ec5] Lily Liu 2024-05-03 [Kernel] Use flashinfer for decoding (#4353)
8	0	csrc/cache.h
80	0	csrc/cache_kernels.cu
4	0	csrc/pybind.cpp
11	1	tests/basic_correctness/test_basic_correctness.py
9	5	tests/distributed/test_basic_distributed_correctness.py
7	1	tests/kernels/conftest.py
77	0	tests/kernels/test_cache.py
12	0	vllm/_custom_ops.py
9	4	vllm/attention/backends/abstract.py
220	0	vllm/attention/backends/flashinfer.py
6	0	vllm/attention/selector.py
5	0	vllm/config.py
3	1	vllm/sequence.py
52	15	vllm/utils.py
97	26	vllm/worker/model_runner.py

[f8e7adda2] Sebastian Schoennenbeck 2024-05-03 Fix/async chat serving (#2727)
14	11	tests/async_engine/test_chat_template.py
37	0	tests/entrypoints/openai/test_serving_chat.py
1	1	tests/entrypoints/test_openai_server.py
9	3	vllm/entrypoints/openai/serving_chat.py
12	6	vllm/entrypoints/openai/serving_engine.py

[7e65477e5] Michael Goin 2024-05-03 [Bugfix] Allow "None" or "" to be passed to CLI for string args that default to None (#4586)
19	13	vllm/engine/arg_utils.py
15	12	vllm/entrypoints/openai/cli_args.py

[3521ba4f2] SangBin Cho 2024-05-04 [Core][Model runner refactoring 1/N] Refactor attn metadata term (#4518)
12	13	benchmarks/kernels/benchmark_paged_attention.py
38	38	csrc/attention/attention_kernels.cu
46	46	csrc/cpu/attention.cpp
4	4	csrc/ops.h
17	18	tests/kernels/test_attention.py
8	8	tests/kernels/test_prefix_prefill.py
17	17	tests/samplers/test_sampler.py
2	2	tests/spec_decode/e2e/conftest.py
14	10	tests/spec_decode/test_multi_step_worker.py
15	9	tests/spec_decode/test_ngram_worker.py
4	4	tests/spec_decode/utils.py
4	4	tests/test_logits_processor.py
48	51	tests/worker/test_model_runner.py
9	9	vllm/_custom_ops.py
22	22	vllm/attention/backends/flash_attn.py
30	30	vllm/attention/backends/rocm_flash_attn.py
18	18	vllm/attention/backends/torch_sdpa.py
32	33	vllm/attention/backends/xformers.py
17	18	vllm/attention/ops/paged_attn.py
16	7	vllm/config.py
12	2	vllm/engine/arg_utils.py
6	1	vllm/entrypoints/llm.py
3	3	vllm/model_executor/layers/sampler.py
36	27	vllm/model_executor/sampling_metadata.py
29	29	vllm/worker/cpu_model_runner.py
80	87	vllm/worker/model_runner.py
15	15	vllm/worker/neuron_model_runner.py

[2d7bce9cd] youkaichao 2024-05-02 [Doc] add env vars to the doc (#4572)
1	0	docs/source/index.rst
9	0	docs/source/serving/env_vars.rst
7	0	vllm/envs.py

[ce3f1eedf] DefTruth 2024-05-03 [Misc] remove chunk detected debug logs (#4571)
4	4	vllm/engine/llm_engine.py

[808632d3b] Yang, Bo 2024-05-02 [BugFix] Prevent the task of `_force_log` from being garbage collected (#4567)
6	1	vllm/entrypoints/openai/api_server.py

[344a5d0c3] youkaichao 2024-05-02 [Core][Distributed] enable allreduce for multiple tp groups (#4566)
39	4	tests/distributed/test_pynccl.py
0	1	vllm/distributed/communication_op.py
25	11	vllm/distributed/parallel_state.py
7	6	vllm/worker/worker.py

[0f8a91401] SangBin Cho 2024-05-03 [Core] Ignore infeasible swap requests. (#4557)
85	0	tests/basic_correctness/test_preemption.py
1	1	tests/core/test_block_manager.py
3	2	tests/core/test_chunked_prefill_scheduler.py
29	1	tests/core/test_scheduler.py
8	11	vllm/core/block/cpu_gpu_block_allocator.py
13	8	vllm/core/block/interfaces.py
4	2	vllm/core/block/naive_block.py
3	0	vllm/core/block/prefix_caching_block.py
17	2	vllm/core/block_manager_v1.py
2	2	vllm/core/block_manager_v2.py
1	1	vllm/core/interfaces.py
21	12	vllm/core/scheduler.py

[9b5c9f948] Alexei-V-Ivanov-AMD 2024-05-02 [CI/Build] AMD CI pipeline with extended set of tests. (#4267)
25	33	.buildkite/run-amd-test.sh
5	0	.buildkite/run-benchmarks.sh
14	1	.buildkite/test-pipeline.yaml
16	5	.buildkite/test-template.j2
7	6	Dockerfile.rocm

[32881f3f3] Michał Moskal 2024-05-02 [kernel] fix sliding window in prefix prefill Triton kernel (#4405)
30	4	tests/kernels/test_prefix_prefill.py
1	0	vllm/attention/backends/flash_attn.py
1	0	vllm/attention/backends/rocm_flash_attn.py
1	0	vllm/attention/backends/xformers.py
2	0	vllm/attention/ops/paged_attn.py
56	19	vllm/attention/ops/prefix_prefill.py

[5b8a7c1cb] youkaichao 2024-05-02 [Misc] centralize all usage of environment variables (#4548)
2	3	vllm/attention/backends/rocm_flash_attn.py
2	4	vllm/attention/selector.py
0	5	vllm/config.py
4	4	vllm/distributed/device_communicators/custom_all_reduce.py
2	2	vllm/distributed/parallel_state.py
5	2	vllm/distributed/utils.py
2	3	vllm/engine/async_llm_engine.py
2	2	vllm/entrypoints/openai/api_server.py
160	0	vllm/envs.py
2	3	vllm/executor/cpu_executor.py
2	3	vllm/executor/multiproc_worker_utils.py
3	5	vllm/executor/ray_gpu_executor.py
4	2	vllm/logger.py
4	3	vllm/model_executor/model_loader/loader.py
5	7	vllm/model_executor/model_loader/tensorizer.py
1	1	vllm/transformers_utils/tokenizer.py
9	7	vllm/usage/usage_lib.py
11	8	vllm/utils.py

[1ff0c73a7] Mark McLoughlin 2024-05-02 [BugFix] Include target-device specific requirements.txt in sdist (#4559)
3	0	MANIFEST.in

[5ad60b0cb] Hu Dong 2024-05-03 [Misc] Exclude the `tests` directory from being packaged (#4552)
1	1	setup.py

[fb087af52] SangBin Cho 2024-05-03 [mypy][7/N] Cover all directories (#4555)
2	0	.github/workflows/mypy.yaml
2	0	format.sh

[7038e8b80] alexm-nm 2024-05-02 [Kernel] Support running GPTQ 8-bit models in Marlin (#4533)
3	1	csrc/ops.h
377	175	csrc/quantization/gptq_marlin/gptq_marlin.cu
2	6	csrc/quantization/gptq_marlin/gptq_marlin.cuh
90	62	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
9	4	tests/models/test_gptq_marlin.py
8	6	vllm/_custom_ops.py
64	70	vllm/model_executor/layers/quantization/gptq_marlin.py

[2a85f9300] youkaichao 2024-05-01 [Core][Distributed] enable multiple tp group (#4512)
8	3	.buildkite/test-pipeline.yaml
3	0	.buildkite/test-template.j2
28	0	tests/distributed/test_pynccl.py
4	1	vllm/distributed/device_communicators/pynccl.py

[cf8cac8c7] SangBin Cho 2024-05-02 [mypy][6/N] Fix all the core subdirectory typing (#4450)
2	4	.github/workflows/mypy.yaml
1	1	format.sh
10	6	vllm/core/block/block_table.py
16	4	vllm/core/block/common.py
33	16	vllm/core/block/cpu_gpu_block_allocator.py
95	9	vllm/core/block/interfaces.py
42	10	vllm/core/block/naive_block.py
60	25	vllm/core/block/prefix_caching_block.py
6	3	vllm/core/block_manager_v2.py
10	5	vllm/core/evictor_v2.py

[5e401bce1] Ronen Schaffer 2024-05-02 [CI]Add regression tests to ensure the async engine generates metrics (#4524)
94	0	tests/metrics/test_metrics.py

[0d62fe58d] SangBin Cho 2024-05-02 [Bug fix][Core] assert num_new_tokens == 1 fails when SamplingParams.n is not 1 and max_tokens is large & Add tests for preemption (#4451)
1	0	.buildkite/test-pipeline.yaml
0	1	tests/basic_correctness/test_chunked_prefill.py
138	0	tests/basic_correctness/test_preemption.py
2	1	tests/conftest.py
3	3	tests/spec_decode/test_spec_decode_worker.py
28	8	vllm/core/scheduler.py

[b8afa8b95] Danny Guinther 2024-05-01 [MISC] Rework logger to enable pythonic custom logging configuration to be provided (#4273)
178	0	examples/logging_configuration.md
188	1	tests/test_logger.py
65	47	vllm/logger.py
5	0	vllm/logging/__init__.py
15	0	vllm/logging/formatter.py

[826b82a26] Woosuk Kwon 2024-05-01 [Misc] Fix expert_ids shape in MoE (#4517)
6	5	vllm/model_executor/layers/fused_moe/fused_moe.py

[c9d852d60] Philipp Moritz 2024-05-01 [Misc] Remove Mixtral device="cuda" declarations (#4543)
4	8	vllm/model_executor/models/mixtral.py

[6ef09b08f] youkaichao 2024-05-01 [Core][Distributed] fix pynccl del error (#4508)
4	8	vllm/distributed/device_communicators/pynccl.py

[3a922c1e7] Roy 2024-05-02 [Bugfix][Core] Fix and refactor logging stats (#4336)
9	5	vllm/engine/async_llm_engine.py
7	5	vllm/engine/llm_engine.py

[c47ba4aaa] sasha0552 2024-05-01 [Bugfix] Add validation for seed (#4529)
20	0	tests/entrypoints/test_openai_server.py
6	2	vllm/entrypoints/openai/protocol.py

[24bb4fe43] Philipp Moritz 2024-05-01 [Kernel] Update fused_moe tuning script for FP8 (#4457)
71	38	benchmarks/kernels/benchmark_mixtral_moe.py
140	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json

[a657bfc48] Nick Hill 2024-05-01 [Core] Add `multiproc_worker_utils` for multiprocessing-based workers (#4357)
176	0	tests/engine/test_multiproc_workers.py
264	0	vllm/executor/multiproc_worker_utils.py

[24750f4ca] leiwen83 2024-05-02 [Core] Enable prefix caching with block manager v2 enabled (#4142)
13	3	benchmarks/benchmark_prefix_caching.py
146	0	tests/core/block/e2e/test_correctness.py
125	0	tests/core/block/test_prefix_caching_block.py
10	2	vllm/core/block/cpu_gpu_block_allocator.py
4	0	vllm/core/block/interfaces.py
10	1	vllm/core/block/naive_block.py
134	38	vllm/core/block/prefix_caching_block.py
1	1	vllm/core/block_manager_v1.py
19	12	vllm/core/block_manager_v2.py
0	0	vllm/core/{evictor.py => evictor_v1.py}
122	0	vllm/core/evictor_v2.py

[b38e42fbc] leiwen83 2024-05-02 [Speculative decoding] Add ngram prompt lookup decoding (#4237)
58	0	tests/spec_decode/e2e/conftest.py
2	58	tests/spec_decode/e2e/{test_correctness.py => test_multistep_correctness.py}
172	0	tests/spec_decode/e2e/test_ngram_correctness.py
25	25	tests/spec_decode/test_multi_step_worker.py
206	0	tests/spec_decode/test_ngram_worker.py
59	28	vllm/config.py
18	0	vllm/engine/arg_utils.py
4	4	vllm/executor/gpu_executor.py
2	2	vllm/spec_decode/batch_expansion.py
24	185	vllm/spec_decode/multi_step_worker.py
190	0	vllm/spec_decode/ngram_worker.py
32	13	vllm/spec_decode/spec_decode_worker.py
200	0	vllm/spec_decode/top1_proposer.py
12	4	vllm/spec_decode/util.py

[8b798eec7] Travis Johnson 2024-05-01 [CI/Build][Bugfix] VLLM_USE_PRECOMPILED should skip compilation (#4534)
1	0	setup.py

[69909126a] sasha0552 2024-05-01 [Bugfix] Use random seed if seed is -1 (#4531)
4	1	vllm/sampling_params.py

[e491c7e05] Frαnçois 2024-05-01 [Doc] update(example model): for OpenAI compatible serving (#4503)
4	4	docs/source/serving/openai_compatible_server.md

[4dc8026d8] Robert Shaw 2024-05-01 [Bugfix] Fix 307 Redirect for `/metrics` (#4523)
1	1	vllm/engine/metrics.py
6	2	vllm/entrypoints/openai/api_server.py

[a88bb9b03] AnyISalIn 2024-05-02 [Bugfix] Fix the fp8 kv_cache check error that occurs when failing to obtain the CUDA version. (#4173)
2	1	vllm/config.py

[6f1df8043] SangBin Cho 2024-05-01 [Test] Add ignore_eos test  (#4519)
31	0	tests/samplers/test_ignore_eos.py

[d6f4bd7cd] Jee Li 2024-05-01 [Misc]Add customized information for models (#4132)
15	0	tests/models/test_big_models.py
15	0	tests/models/test_models.py
7	0	vllm/attention/layer.py
3	0	vllm/model_executor/layers/activation.py
5	0	vllm/model_executor/layers/layernorm.py
22	0	vllm/model_executor/layers/linear.py
6	0	vllm/model_executor/layers/logits_processor.py
6	0	vllm/model_executor/layers/rotary_embedding.py
8	0	vllm/model_executor/layers/vocab_parallel_embedding.py

[c3845d82d] Robert Caulk 2024-05-01 Allow user to define whitespace pattern for outlines (#4305)
3	1	tests/entrypoints/test_guided_processors.py
10	0	vllm/entrypoints/openai/protocol.py
5	3	vllm/model_executor/guided_decoding/outlines_decoding.py
3	4	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[a822eb341] Pastel！ 2024-05-01 [Misc] fix typo in block manager (#4453)
1	1	vllm/core/block_manager_v1.py

[f458112e8] harrywu 2024-05-01 [Misc][Typo] type annotation fix (#4495)
3	2	vllm/engine/llm_engine.py

[2e240c69a] Nick Hill 2024-04-30 [Core] Centralize GPU Worker construction (#4419)
39	44	vllm/executor/gpu_executor.py
8	24	vllm/executor/ray_gpu_executor.py

[ee37328da] fuchen.ljl 2024-05-01 Unable to find Punica extension issue during source code installation (#4494)
1	0	docs/source/getting_started/installation.rst

[6ad58f42c] fuchen.ljl 2024-05-01 fix_tokenizer_snapshot_download_bug (#4493)
1	1	vllm/transformers_utils/tokenizer.py

[dd1a50a8b] Li, Jiang 2024-05-01 [Bugfix][Minor] Make ignore_eos effective  (#4468)
2	1	vllm/sampling_params.py

[715c2d854] Alpay Ariyak 2024-04-30 [Frontend] [Core] Tensorizer: support dynamic `num_readers`, update version (#4467)
1	1	requirements-dev.txt
1	1	setup.py
10	7	vllm/model_executor/model_loader/tensorizer.py

[a49414043] Florian Greinacher 2024-05-01 [Frontend] Support complex message content for chat completions endpoint (#3467)
19	0	tests/entrypoints/test_openai_server.py
27	21	vllm/entrypoints/openai/serving_chat.py

[111815d48] Robert Shaw 2024-04-30 [Kernel] Support Fp8 Checkpoints (Dynamic + Static) (#4332)
90	0	tests/models/test_fp8.py
48	10	vllm/model_executor/layers/linear.py
169	30	vllm/model_executor/layers/quantization/fp8.py

[b31a1fb63] Prashant Gupta 2024-04-30 [Doc] add visualization for multi-stage dockerfile (#4456)
4	0	Dockerfile
-	-	docs/source/assets/dev/dockerfile-stages-dependency.png
50	0	docs/source/dev/dockerfile/dockerfile.rst
1	0	docs/source/index.rst

[4bb53e2dd] leiwen83 2024-05-01 [BugFix] fix num_lookahead_slots missing in async executor (#4165)
123	2	tests/spec_decode/e2e/conftest.py
11	4	tests/spec_decode/e2e/test_compatibility.py
16	9	tests/spec_decode/e2e/test_correctness.py
4	2	vllm/engine/async_llm_engine.py
3	1	vllm/executor/cpu_executor.py
1	0	vllm/executor/executor_base.py
3	1	vllm/executor/gpu_executor.py
1	0	vllm/executor/neuron_executor.py
1	0	vllm/executor/ray_gpu_executor.py

[26f2fb511] Kunshang Ji 2024-04-30 [Core]Refactor gptq_marlin ops (#4466)
16	0	vllm/_custom_ops.py
1	1	vllm/model_executor/layers/quantization/gptq_marlin.py

[fa3220784] Woosuk Kwon 2024-04-29 [Bugfix][Kernel] Fix compute_type for MoE kernel (#4463)
4	2	vllm/model_executor/layers/fused_moe/fused_moe.py

[d627a3d83] Michael Goin 2024-04-29 [Misc] Upgrade to `torch==2.3.0` (#4454)
1	1	.github/workflows/publish.yml
1	1	CMakeLists.txt
1	1	Dockerfile
1	1	pyproject.toml
1	1	requirements-build.txt
1	1	requirements-cpu.txt
2	2	requirements-cuda.txt

[f4f921b7f] youkaichao 2024-04-29 [Core][Distributed] use cpu group to broadcast metadata in cpu (#4444)
3	3	tests/tensorizer_loader/tensorize_vllm_model_for_testing.py
12	11	tests/worker/test_model_runner.py
48	21	vllm/distributed/communication_op.py

[ac5ccf015] Simon Mo 2024-04-29 [CI] hotfix: soft fail neuron test (#4458)
1	0	.buildkite/test-template.j2

[73c8d677e] Robert Shaw 2024-04-29 [Kernel] Marlin Expansion: Support AutoGPTQ Models with Marlin (#3922)
2	0	CMakeLists.txt
18	0	csrc/ops.h
2	0	csrc/pybind.cpp
1520	0	csrc/quantization/gptq_marlin/gptq_marlin.cu
74	0	csrc/quantization/gptq_marlin/gptq_marlin.cuh
324	0	csrc/quantization/gptq_marlin/gptq_marlin_repack.cu
93	0	tests/models/test_gptq_marlin.py
13	32	tests/models/test_marlin.py
29	0	tests/models/utils.py
0	64	tests/quantization/test_autogptq_marlin_configs.py
73	0	tests/quantization/test_configs.py
31	8	vllm/config.py
3	0	vllm/model_executor/layers/quantization/__init__.py
444	0	vllm/model_executor/layers/quantization/gptq_marlin.py

[df29793dc] SangBin Cho 2024-04-29 [mypy][5/N] Support all typing on model executor (#4427)
1	1	.github/workflows/mypy.yaml
1	1	format.sh
1	0	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
11	1	vllm/model_executor/layers/linear.py
2	2	vllm/model_executor/layers/quantization/__init__.py
11	3	vllm/model_executor/layers/quantization/base_config.py
2	3	vllm/model_executor/layers/quantization/squeezellm.py
2	2	vllm/model_executor/layers/rotary_embedding.py
27	20	vllm/model_executor/layers/sampler.py
3	1	vllm/model_executor/model_loader/tensorizer.py

[03dd7d52b] Simon Mo 2024-04-28 [CI] clean docker cache for neuron (#4441)
14	0	.buildkite/run-neuron-test.sh

[bf480c530] Ronen Schaffer 2024-04-29 Add more Prometheus metrics (#2764)
283	0	examples/production_monitoring/grafana.json
1	0	requirements-common.txt
1	1	vllm/core/scheduler.py
119	52	vllm/engine/llm_engine.py
163	58	vllm/engine/metrics.py
15	3	vllm/sequence.py

[9c7306ac1] DefTruth 2024-04-28 [Misc] fix typo in llm_engine init logging (#4428)
1	1	vllm/engine/llm_engine.py

[4ea1f9678] Robert Shaw 2024-04-27 [BugFix] Resolved Issues For LinearMethod --> QuantConfig (#4418)
0	1	vllm/model_executor/models/bloom.py
0	1	vllm/model_executor/models/falcon.py
0	1	vllm/model_executor/models/gpt2.py
0	1	vllm/model_executor/models/gpt_bigcode.py
0	1	vllm/model_executor/models/gpt_j.py
0	1	vllm/model_executor/models/gpt_neox.py
0	1	vllm/model_executor/models/mpt.py
0	1	vllm/model_executor/models/opt.py
0	1	vllm/model_executor/models/phi.py
0	1	vllm/model_executor/models/starcoder2.py

[ba4be44c3] Nick Hill 2024-04-27 [BugFix] Fix return type of executor execute_model methods (#4402)
1	1	vllm/executor/cpu_executor.py
4	3	vllm/executor/distributed_gpu_executor.py
1	1	vllm/executor/executor_base.py
1	1	vllm/executor/gpu_executor.py
1	1	vllm/executor/neuron_executor.py
1	1	vllm/executor/ray_gpu_executor.py

[d6e520e17] Prashant Gupta 2024-04-27 [Core] Support offline use of local cache for models (#4374)
29	1	tests/model_executor/weight_utils.py
4	1	vllm/model_executor/model_loader/loader.py
34	25	vllm/model_executor/model_loader/weight_utils.py
2	0	vllm/transformers_utils/tokenizer.py

[81661da7b] Nick Hill 2024-04-27 [BugFix] Fix `min_tokens` when `eos_token_id` is None (#4389)
3	6	tests/samplers/test_sampler.py
3	2	vllm/engine/llm_engine.py
6	8	vllm/model_executor/layers/sampler.py
2	2	vllm/sampling_params.py

[dfea17314] Ruoyu Qin 2024-04-28 [Bugfix] Abort requests when the connection to /v1/completions is interrupted (#4363)
41	0	tests/async_engine/test_merge_async_iterators.py
12	5	vllm/utils.py

[7134303cb] Roy 2024-04-27 [Bugfix][Core] Fix get decoding config from ray (#4335)
2	0	tests/async_engine/test_async_llm_engine.py
157	0	tests/async_engine/test_openapi_server_ray.py
9	1	vllm/engine/async_llm_engine.py
4	0	vllm/engine/llm_engine.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py

[3da24c2df] Caio Mendes 2024-04-27 [Model] Phi-3 4k sliding window temp. fix (#4380)
4	3	vllm/core/block_manager_v1.py

[eefeb1646] Austin Veselka 2024-04-27 [Kernel] Full Tensor Parallelism for LoRA Layers (#3524)
1	0	csrc/punica/bgmv/bgmv_bf16_bf16_bf16.cu
1	0	csrc/punica/bgmv/bgmv_bf16_fp32_bf16.cu
78	0	csrc/punica/bgmv/bgmv_config.h
1	0	csrc/punica/bgmv/bgmv_fp16_fp16_fp16.cu
1	0	csrc/punica/bgmv/bgmv_fp16_fp32_fp16.cu
1	0	csrc/punica/bgmv/bgmv_fp32_bf16_bf16.cu
1	0	csrc/punica/bgmv/bgmv_fp32_fp16_fp16.cu
4	1	csrc/punica/bgmv/bgmv_impl.cuh
1	0	csrc/punica/bgmv/generator.py
1	1	csrc/punica/punica_ops.cc
23	6	tests/lora/test_layers.py
49	2	tests/lora/test_punica.py
1	0	vllm/config.py
10	0	vllm/engine/arg_utils.py
262	0	vllm/lora/fully_sharded_layers.py
146	97	vllm/lora/layers.py
3	3	vllm/lora/models.py
43	0	vllm/lora/punica.py
59	1	vllm/lora/utils.py

[18d23f642] Hongxia Yang 2024-04-27 [ROCm][Hardware][AMD] Enable group query attention for triton FA (#4406)
25	28	vllm/attention/backends/rocm_flash_attn.py
11	13	vllm/attention/ops/triton_flash_attention.py

[87f545ba6] Roy 2024-04-27 [Misc] Fix logger format typo (#4396)
2	2	vllm/engine/metrics.py

[8947bc3c1] Cyrus Leung 2024-04-27 [Frontend][Bugfix] Disallow extra fields in OpenAI API (#4355)
1	0	requirements-common.txt
0	1	requirements-dev.txt
16	0	tests/entrypoints/test_openai_server.py
2	2	vllm/entrypoints/openai/cli_args.py
35	29	vllm/entrypoints/openai/protocol.py
44	11	vllm/entrypoints/openai/serving_chat.py
5	4	vllm/entrypoints/openai/serving_completion.py
10	8	vllm/entrypoints/openai/serving_engine.py

[12628d3c7] Philipp Moritz 2024-04-26 [Kernel] Optimize FP8 support for MoE kernel / Mixtral via static scales (#4343)
6	1	csrc/ops.h
2	1	csrc/pybind.cpp
24	1	csrc/quantization/fp8/fp8_cuda_kernels.cu
9	3	vllm/_custom_ops.py
9	4	vllm/model_executor/layers/fused_moe/fused_moe.py
8	1	vllm/model_executor/layers/quantization/fp8.py
37	7	vllm/model_executor/models/mixtral.py

[258a2c58d] Nick Hill 2024-04-26 [Core] Introduce `DistributedGPUExecutor` abstract class (#4348)
114	0	vllm/executor/distributed_gpu_executor.py
8	86	vllm/executor/ray_gpu_executor.py

[aba47be3f] youkaichao 2024-04-26 [Misc] add RFC issue template (#4401)
49	0	.github/ISSUE_TEMPLATE/750-RFC.yml

[a62aaf1df] Cody Yu 2024-04-26 [Misc][Refactor] Generalize linear_method to be quant_method (#4373)
1	1	tests/quantization/test_fp8.py
2	2	tests/tensorizer_loader/test_tensorizer.py
13	17	vllm/lora/layers.py
88	81	vllm/model_executor/layers/linear.py
2	2	vllm/model_executor/layers/quantization/__init__.py
8	5	vllm/model_executor/layers/quantization/aqlm.py
11	8	vllm/model_executor/layers/quantization/awq.py
28	3	vllm/model_executor/layers/quantization/base_config.py
20	40	vllm/model_executor/layers/quantization/fp8.py
11	8	vllm/model_executor/layers/quantization/gptq.py
8	5	vllm/model_executor/layers/quantization/marlin.py
14	10	vllm/model_executor/layers/quantization/squeezellm.py
18	20	vllm/model_executor/model_loader/loader.py
7	6	vllm/model_executor/model_loader/tensorizer.py
22	21	vllm/model_executor/models/baichuan.py
17	16	vllm/model_executor/models/bloom.py
19	18	vllm/model_executor/models/chatglm.py
17	16	vllm/model_executor/models/commandr.py
18	17	vllm/model_executor/models/dbrx.py
4	3	vllm/model_executor/models/decilm.py
22	23	vllm/model_executor/models/deepseek.py
17	16	vllm/model_executor/models/falcon.py
17	16	vllm/model_executor/models/gemma.py
17	16	vllm/model_executor/models/gpt2.py
17	16	vllm/model_executor/models/gpt_bigcode.py
17	16	vllm/model_executor/models/gpt_j.py
17	16	vllm/model_executor/models/gpt_neox.py
17	16	vllm/model_executor/models/internlm2.py
17	16	vllm/model_executor/models/jais.py
16	16	vllm/model_executor/models/llama.py
5	4	vllm/model_executor/models/llava.py
18	17	vllm/model_executor/models/minicpm.py
22	22	vllm/model_executor/models/mixtral.py
21	20	vllm/model_executor/models/mixtral_quant.py
17	16	vllm/model_executor/models/mpt.py
16	16	vllm/model_executor/models/olmo.py
19	18	vllm/model_executor/models/opt.py
17	16	vllm/model_executor/models/orion.py
18	17	vllm/model_executor/models/phi.py
17	16	vllm/model_executor/models/qwen.py
17	16	vllm/model_executor/models/qwen2.py
22	23	vllm/model_executor/models/qwen2_moe.py
15	14	vllm/model_executor/models/stablelm.py
16	16	vllm/model_executor/models/starcoder2.py
17	16	vllm/model_executor/models/xverse.py

[603ad8481] SangBin Cho 2024-04-26 [Core] Refactoring sampler and support prompt logprob for chunked prefill  (#4309)
41	3	tests/samplers/test_logprobs.py
31	16	tests/samplers/test_sampler.py
7	3	tests/test_logits_processor.py
13	6	tests/worker/test_model_runner.py
15	0	vllm/core/scheduler.py
1	1	vllm/engine/async_llm_engine.py
13	12	vllm/engine/llm_engine.py
6	0	vllm/engine/output_processor/interfaces.py
9	0	vllm/engine/output_processor/multi_step.py
14	8	vllm/engine/output_processor/single_step.py
4	3	vllm/engine/output_processor/util.py
12	15	vllm/model_executor/layers/logits_processor.py
366	178	vllm/model_executor/layers/sampler.py
284	65	vllm/model_executor/sampling_metadata.py
10	1	vllm/sequence.py
15	101	vllm/worker/cpu_model_runner.py
9	114	vllm/worker/model_runner.py
12	107	vllm/worker/neuron_model_runner.py

[a88081bf7] SangBin Cho 2024-04-26 [CI] Disable non-lazy string operation on logging (#4326)
3	2	docs/source/conf.py
1	0	pyproject.toml
4	3	setup.py
8	8	vllm/config.py
6	4	vllm/core/scheduler.py
4	4	vllm/distributed/device_communicators/custom_all_reduce.py
8	7	vllm/distributed/device_communicators/pynccl.py
2	2	vllm/distributed/device_communicators/pynccl_utils.py
4	2	vllm/distributed/parallel_state.py
2	2	vllm/distributed/utils.py
9	9	vllm/engine/async_llm_engine.py
36	25	vllm/engine/llm_engine.py
13	8	vllm/engine/metrics.py
2	2	vllm/entrypoints/openai/api_server.py
5	6	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/executor/cpu_executor.py
2	2	vllm/executor/gpu_executor.py
2	2	vllm/executor/ray_gpu_executor.py
3	3	vllm/executor/ray_utils.py
1	1	vllm/logger.py
3	3	vllm/lora/models.py
2	2	vllm/model_executor/layers/fused_moe/fused_moe.py
4	4	vllm/model_executor/model_loader/tensorizer.py
7	7	vllm/model_executor/model_loader/weight_utils.py
5	5	vllm/model_executor/models/__init__.py
3	3	vllm/model_executor/models/gemma.py
2	1	vllm/spec_decode/spec_decode_worker.py
7	6	vllm/transformers_utils/configs/dbrx.py
2	3	vllm/transformers_utils/tokenizer.py
11	9	vllm/utils.py
14	13	vllm/worker/model_runner.py

[2f30e7c72] Norman Mu 2024-04-25 [Frontend] Add --log-level option to api server (#4377)
2	1	vllm/entrypoints/api_server.py

[a74dee9b6] Cyrus Leung 2024-04-26 [Bugfix] Fix parameter name in `get_tokenizer` (#4107)
20	0	tests/tokenization/test_tokenizer.py
6	5	vllm/transformers_utils/tokenizer.py

[cf29b7eda] Hongxia Yang 2024-04-25 [ROCm][Hardware][AMD][Doc] Documentation update for ROCm (#4376)
65	100	docs/source/getting_started/amd-installation.rst

[efffb63f5] Nick Hill 2024-04-25 [Core] Move function tracing setup to util function (#4352)
20	1	vllm/utils.py
4	14	vllm/worker/worker_base.py

[15e7c675b] Nick Hill 2024-04-25 [Core] Add `shutdown()` method to `ExecutorBase` (#4349)
6	0	vllm/engine/llm_engine.py
7	0	vllm/executor/executor_base.py

[b6dcb4d44] Roy 2024-04-26 [Misc] Fix flash attention backend log  (#4368)
5	5	vllm/attention/selector.py

[b5b4a398a] SangBin Cho 2024-04-26 [Mypy] Typing lora folder (#4337)
3	4	.github/workflows/mypy.yaml
1	1	format.sh
22	13	vllm/lora/layers.py
17	11	vllm/lora/lora.py
34	30	vllm/lora/models.py
12	9	vllm/lora/worker_manager.py
2	2	vllm/worker/model_runner.py

[f4bc4de1b] Kunshang Ji 2024-04-25 [Core]refactor aqlm quant ops  (#4351)
1	1	benchmarks/kernels/benchmark_aqlm.py
14	0	vllm/_custom_ops.py
1	1	vllm/model_executor/layers/quantization/aqlm.py

[bd7a8eef2] Caio Mendes 2024-04-25 [Doc] README Phi-3 name fix. (#4372)
1	1	README.md

[7ee82bef1] Alexei-V-Ivanov-AMD 2024-04-25 [CI/Build] Adding functionality to reset the node's GPUs before processing. (#4213)
15	1	.buildkite/run-amd-test.sh

[fbf152d97] Isotr0py 2024-04-26 [Bugfix][Model] Refactor OLMo model to support new HF format in transformers 4.40.0 (#4324)
1	1	README.md
1	1	docs/source/models/supported_models.rst
0	1	requirements-dev.txt
1	1	vllm/model_executor/models/__init__.py
147	158	vllm/model_executor/models/olmo.py

[479d69fad] Nick Hill 2024-04-24 [Core] Move ray_utils.py from `engine` to `executor` package (#4347)
1	1	vllm/__init__.py
1	1	vllm/engine/async_llm_engine.py
1	1	vllm/engine/llm_engine.py
6	4	vllm/executor/ray_gpu_executor.py
0	0	vllm/{engine => executor}/ray_utils.py
1	1	vllm/transformers_utils/tokenizer_group/__init__.py
1	1	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py

[96e90fdeb] Caio Mendes 2024-04-25 [Model] Adds Phi-3 support (#4298)
1	0	README.md
4	0	docs/source/models/supported_models.rst
1	1	vllm/config.py
133	3	vllm/model_executor/layers/rotary_embedding.py
1	0	vllm/model_executor/models/__init__.py
9	5	vllm/model_executor/models/llama.py

[a395a638c] zifeitong 2024-04-24 [Misc] Use public API in benchmark_throughput (#4300)
13	16	benchmarks/benchmark_throughput.py

[2768884ac] youkaichao 2024-04-24 [Doc] Add note for docker user (#4340)
3	0	docs/source/serving/deploying_with_docker.rst

[aae08249a] alexm-nm 2024-04-24 [Bugfix] Fix marlin kernel crash on H100 (#4218)
8	15	csrc/quantization/marlin/marlin_cuda_kernel.cu

[7923dcad1] Roger Wang 2024-04-24 [Misc] Update ShareGPT Dataset Sampling in Serving Benchmark (#4279)
28	22	benchmarks/benchmark_serving.py

[3cd9b5bb2] youkaichao 2024-04-24 [Core][Distributed] use existing torch.cuda.device (#4318)
3	5	vllm/distributed/device_communicators/pynccl.py

[468d761b3] Woosuk Kwon 2024-04-23 [Misc] Reduce supported Punica dtypes (#4304)
0	12	CMakeLists.txt
0	4	csrc/punica/bgmv/bgmv_bf16_bf16_fp16.cu
0	4	csrc/punica/bgmv/bgmv_bf16_fp16_bf16.cu
0	4	csrc/punica/bgmv/bgmv_bf16_fp16_fp16.cu
0	4	csrc/punica/bgmv/bgmv_bf16_fp32_fp16.cu
0	4	csrc/punica/bgmv/bgmv_fp16_bf16_bf16.cu
0	4	csrc/punica/bgmv/bgmv_fp16_bf16_fp16.cu
0	4	csrc/punica/bgmv/bgmv_fp16_fp16_bf16.cu
0	4	csrc/punica/bgmv/bgmv_fp16_fp32_bf16.cu
0	4	csrc/punica/bgmv/bgmv_fp32_bf16_fp16.cu
0	4	csrc/punica/bgmv/bgmv_fp32_fp16_bf16.cu
0	4	csrc/punica/bgmv/bgmv_fp32_fp32_bf16.cu
0	4	csrc/punica/bgmv/bgmv_fp32_fp32_fp16.cu
20	0	csrc/punica/bgmv/generator.py
17	0	csrc/punica/punica_ops.cc
29	12	tests/lora/test_layers.py

[e4bf860a5] youkaichao 2024-04-23 [CI][Build] change pynvml to nvidia-ml-py (#4302)
1	1	requirements-cuda.txt

[91f50a6fe] youkaichao 2024-04-23 [Core][Distributed] use cpu/gloo to initialize pynccl (#4248)
10	5	tests/distributed/test_pynccl.py
71	51	vllm/distributed/device_communicators/pynccl.py
3	9	vllm/distributed/device_communicators/pynccl_utils.py
6	0	vllm/distributed/parallel_state.py
3	6	vllm/worker/worker.py

[79a268c4a] Robert Shaw 2024-04-23 [BUG] fixed fp8 conflict with aqlm (#4307)
3	0	.buildkite/test-pipeline.yaml
13	3	vllm/model_executor/layers/linear.py
2	1	vllm/model_executor/layers/quantization/fp8.py

[eace8bf0b] Philipp Moritz 2024-04-23 [Kernel] FP8 support for MoE kernel / Mixtral (#4244)
1	0	CMakeLists.txt
5	0	csrc/ops.h
1	0	csrc/pybind.cpp
103	0	csrc/quantization/fp8/fp8_cuda_kernels.cu
9	1	vllm/_custom_ops.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=float8.json
75	18	vllm/model_executor/layers/fused_moe/fused_moe.py
2	0	vllm/model_executor/model_loader/loader.py
1	0	vllm/model_executor/model_loader/utils.py
42	2	vllm/model_executor/models/mixtral.py

[1e8f4252a] Cyrus Leung 2024-04-24 [Bugfix][Frontend] Raise exception when file-like chat template fails to be opened (#4292)
14	5	tests/async_engine/test_chat_template.py
15	8	vllm/entrypoints/openai/serving_chat.py

[2b7949c1c] James Fleming 2024-04-23 AQLM CUDA support (#3287)
1	0	CMakeLists.txt
302	0	benchmarks/kernels/benchmark_aqlm.py
15	0	csrc/ops.h
2	0	csrc/pybind.cpp
712	0	csrc/quantization/aqlm/gemm_kernels.cu
46	0	examples/aqlm_example.py
95	0	tests/models/test_aqlm.py
34	7	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
373	0	vllm/model_executor/layers/quantization/aqlm.py
3	1	vllm/model_executor/layers/quantization/awq.py
2	1	vllm/model_executor/layers/quantization/gptq.py
2	1	vllm/model_executor/layers/quantization/marlin.py
3	1	vllm/model_executor/layers/quantization/squeezellm.py

[62b5166bd] Simon Mo 2024-04-23 [CI] Add ccache for wheel builds job (#4281)
3	0	.github/workflows/publish.yml

[d86285a4a] youkaichao 2024-04-23 [Core][Logging] Add last frame information for better debugging (#4278)
16	2	vllm/logger.py

[d87f39e9a] DefTruth 2024-04-24 [Bugfix] Add init_cached_hf_modules to RayWorkerWrapper (#4286)
2	0	vllm/executor/ray_gpu_executor.py
6	1	vllm/worker/worker_base.py

[d3c8180ac] Jack Gordley 2024-04-23 [Bugfix] Fixing max token error message for openai compatible server (#4016)
6	0	vllm/entrypoints/openai/serving_engine.py

[62b8aebc6] Cade Daniel 2024-04-23 [Speculative decoding 7/9] Speculative decoding end-to-end correctness tests. (#3951)
6	2	tests/samplers/test_rejection_sampler.py
2	1	tests/samplers/test_sampler.py
0	0	tests/spec_decode/e2e/__init__.py
35	10	tests/spec_decode/e2e/conftest.py
169	0	tests/spec_decode/e2e/test_compatibility.py
493	47	tests/spec_decode/e2e/test_correctness.py
2	2	tests/spec_decode/test_metrics.py
2	2	tests/spec_decode/test_multi_step_worker.py
23	17	tests/spec_decode/test_spec_decode_worker.py
5	2	tests/spec_decode/utils.py
65	2	vllm/config.py
15	3	vllm/engine/arg_utils.py
30	8	vllm/engine/llm_engine.py
22	1	vllm/engine/metrics.py
1	0	vllm/executor/gpu_executor.py
7	0	vllm/model_executor/layers/rejection_sampler.py
167	15	vllm/model_executor/layers/sampler.py
42	28	vllm/spec_decode/batch_expansion.py
2	2	vllm/spec_decode/interfaces.py
23	8	vllm/spec_decode/metrics.py
10	19	vllm/spec_decode/multi_step_worker.py
43	6	vllm/spec_decode/spec_decode_worker.py

[050f285ff] SangBin Cho 2024-04-23 [Core] Scheduling optimization 2 (#4280)
2	1	tests/core/test_scheduler.py
8	2	vllm/core/scheduler.py
5	0	vllm/sequence.py

[8f2ea22bd] Nick Hill 2024-04-23 [Core] Some simplification of WorkerWrapper changes (#4183)
41	49	vllm/executor/ray_gpu_executor.py
4	5	vllm/worker/worker_base.py

[0ae11f78a] SangBin Cho 2024-04-23 [Mypy] Part 3 fix typing for nested directories for most of directory (#4161)
15	14	.github/workflows/mypy.yaml
12	14	format.sh
4	2	pyproject.toml
1	1	vllm/attention/backends/abstract.py
1	0	vllm/attention/backends/rocm_flash_attn.py
2	1	vllm/attention/backends/torch_sdpa.py
1	0	vllm/attention/backends/xformers.py
1	0	vllm/core/block/block_table.py
4	2	vllm/core/block/common.py
2	4	vllm/core/block/interfaces.py
9	7	vllm/distributed/device_communicators/custom_all_reduce.py
13	9	vllm/distributed/device_communicators/pynccl.py
4	1	vllm/distributed/device_communicators/pynccl_utils.py
3	2	vllm/engine/output_processor/interfaces.py
3	2	vllm/engine/output_processor/multi_step.py
5	4	vllm/engine/output_processor/single_step.py
3	1	vllm/engine/output_processor/util.py
4	2	vllm/entrypoints/openai/api_server.py
8	5	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py
4	1	vllm/entrypoints/openai/serving_completion.py
12	7	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/lora/lora.py
2	1	vllm/model_executor/layers/ops/sample.py
2	1	vllm/model_executor/layers/rotary_embedding.py
4	2	vllm/transformers_utils/configs/jais.py
1	1	vllm/transformers_utils/tokenizer_group/__init__.py
2	0	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
2	2	vllm/transformers_utils/tokenizers/baichuan.py

[34128a697] Harry Mellor 2024-04-23 Fix `autodoc` directives (#4272)
2	3	docs/source/dev/engine/async_llm_engine.rst
3	3	docs/source/dev/engine/llm_engine.rst
2	1	docs/source/dev/sampling_params.rst

[c1b4e4157] youkaichao 2024-04-22 [Core][Distributed] use absolute path for library file (#4271)
30	2	vllm/utils.py

[ceaf4ed00] Zhanghao Wu 2024-04-22 [Doc] Update the SkyPilot doc with serving and Llama-3 (#4276)
264	23	docs/source/serving/run_on_sky.rst

[ad8d696a9] SangBin Cho 2024-04-23 [Core] Scheduler perf fix (#4270)
9	9	tests/core/test_scheduler.py
2	5	vllm/core/scheduler.py

[3d925165f] Harry Mellor 2024-04-22 Add example scripts to documentation (#4225)
2	0	.gitignore
8	1	docs/source/conf.py
61	0	docs/source/generate_examples.py
8	0	docs/source/getting_started/examples/examples_index.template.rst
1	0	docs/source/index.rst
0	0	examples/{openai_chatcompletion_client.py => openai_chat_completion_client.py}

[154368069] alexm-nm 2024-04-22 [Bugfix] Ensure download_weights_from_hf(..) inside loader is using the revision parameter (#4217)
1	1	vllm/model_executor/model_loader/loader.py

[077f0a2e8] Tao He 2024-04-22 [Frontend] Enable support for CPU backend in AsyncLLMEngine. (#3993)
5	0	vllm/engine/async_llm_engine.py
25	2	vllm/executor/cpu_executor.py

[e73ed0f1c] Woosuk Kwon 2024-04-22 [Bugfix] Fix type annotations in CPU model runner (#4256)
4	3	vllm/worker/cpu_model_runner.py

[296cdf8ac] Isotr0py 2024-04-22 [Misc] Add vision language model support to CPU backend (#3968)
1	0	vllm/executor/cpu_executor.py
37	23	vllm/worker/cpu_model_runner.py
15	9	vllm/worker/cpu_worker.py

[747b1a714] youkaichao 2024-04-21 [Core][Distributed] fix _is_full_nvlink detection (#4233)
30	18	vllm/distributed/device_communicators/custom_all_reduce.py

[95e5b087c] Hongxia Yang 2024-04-22 [AMD][Hardware][Misc][Bugfix] xformer cleanup and light navi logic and CI fixes and refactoring (#4129)
0	2	.buildkite/test-pipeline.yaml
1	4	Dockerfile.rocm
0	33	patch_xformers.rocm.sh
0	13	rocm_patch/commonpy_xformers-0.0.23.rocm.patch
0	152	rocm_patch/flashpy_xformers-0.0.23.rocm.patch
18	13	vllm/attention/backends/rocm_flash_attn.py

[a37d815b8] GeauxEric 2024-04-21 Make initialization of tokenizer and detokenizer optional (#3748)
23	0	tests/engine/test_skip_tokenizer_init.py
6	1	vllm/config.py
6	1	vllm/engine/arg_utils.py
21	8	vllm/engine/llm_engine.py
3	2	vllm/engine/output_processor/single_step.py
9	0	vllm/entrypoints/llm.py

[7f2593b16] xiaoji 2024-04-22 [Doc]: Update the doc of adding new models (#4236)
1	1	docs/source/models/adding_model.rst

[fe7d648fe] Harry Mellor 2024-04-21 Don't show default value for flags in `EngineArgs` (#4223)
3	1	docs/source/models/engine_args.rst

[cc74b2b23] Noam Gat 2024-04-20 Updating lm-format-enforcer version and adding links to decoding libraries in docs (#4222)
1	1	requirements-common.txt
5	1	vllm/engine/arg_utils.py
2	2	vllm/model_executor/layers/quantization/fp8.py

[91528575e] nunjunj 2024-04-20 [Frontend] multiple sampling params support  (#3570)
41	0	tests/entrypoints/test_llm_generate.py
20	10	vllm/entrypoints/llm.py

[a22cdea37] Cody Yu 2024-04-19 [Kernel][FP8] Initial support with dynamic per-tensor scaling (#4118)
24	0	tests/quantization/test_fp8.py
5	4	vllm/entrypoints/llm.py
8	0	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
138	0	vllm/model_executor/layers/quantization/fp8.py
4	0	vllm/model_executor/model_loader/loader.py
8	1	vllm/model_executor/model_loader/weight_utils.py

[682789d40] Harry Mellor 2024-04-20 Fix missing docs and out of sync `EngineArgs` (#4219)
2	0	docs/source/conf.py
9	125	docs/source/models/engine_args.rst
87	72	vllm/engine/arg_utils.py

[138485a82] Ayush Rautwar 2024-04-19 [Bugfix] Add fix for JSON whitespace (#4189)
14	13	tests/entrypoints/test_openai_server.py
5	0	vllm/model_executor/guided_decoding/outlines_logits_processors.py

[bc9df1571] Chirag Jain 2024-04-20 Pass `tokenizer_revision` when getting tokenizer in openai serving (#4214)
1	0	vllm/entrypoints/openai/serving_engine.py

[15b86408a] youkaichao 2024-04-19 [Misc] add nccl in collect env (#4211)
1	0	.github/ISSUE_TEMPLATE/200-installation.yml
1	0	.github/ISSUE_TEMPLATE/300-usage.yml
1	0	.github/ISSUE_TEMPLATE/400-bug report.yml
1	0	.github/ISSUE_TEMPLATE/700-performance discussion.yml
2	0	collect_env.py

[7be4f5628] Ronen Schaffer 2024-04-19 [Bugfix][Core] Restore logging of stats in the async engine (#4150)
7	1	vllm/engine/async_llm_engine.py

[8f20fc04b] Uranus 2024-04-19 [Misc] fix docstrings (#4191)
3	6	vllm/sequence.py

[221d93ecb] Simon Mo 2024-04-19 Bump version of 0.4.1 (#4177)
1	1	vllm/__init__.py

[d17c8477f] Jee Li 2024-04-19 [Bugfix] Fix LoRA loading check (#4138)
6	0	tests/lora/conftest.py
20	2	tests/lora/test_lora_checkpoints.py
3	1	vllm/lora/models.py

[a134ef6f5] Simon Mo 2024-04-18 Support eos_token_id from generation_config.json (#4182)
17	2	vllm/engine/llm_engine.py
13	1	vllm/sampling_params.py

[8a7a3e443] youkaichao 2024-04-18 [Core] add an option to log every function call to for debugging hang/crash in distributed inference (#4079)
1	1	.buildkite/test-pipeline.yaml
2	0	.github/ISSUE_TEMPLATE/400-bug report.yml
27	0	tests/test_logger.py
9	3	vllm/executor/ray_gpu_executor.py
52	0	vllm/logger.py
12	1	vllm/utils.py
17	3	vllm/worker/worker_base.py

[8f9c28fd4] Adam Tilghman 2024-04-18 [Bugfix] Fix CustomAllreduce nvlink topology detection (#3974)
4	2	vllm/distributed/device_communicators/custom_all_reduce.py

[cd2f63fb3] Liangfu Chen 2024-04-18 [CI/CD] add neuron docker and ci test scripts (#3571)
37	0	.buildkite/run-neuron-test.sh
5	0	.buildkite/test-template.j2
36	0	Dockerfile.neuron
2	1	setup.py
2	2	vllm/engine/async_llm_engine.py
21	1	vllm/executor/neuron_executor.py

[87fa80c91] Nick Hill 2024-04-18 [Misc] Bump transformers to latest version (#4176)
2	1	requirements-common.txt

[e1bb2fd52] James Whedbee 2024-04-18 [Bugfix] Support logprobs when using guided_json and other constrained decoding fields (#4149)
30	0	tests/entrypoints/test_openai_server.py
3	1	vllm/entrypoints/openai/serving_engine.py

[705578ae1] Simon Mo 2024-04-18 [Docs] document that Meta Llama 3 is supported (#4175)
1	1	README.md
2	2	docs/source/models/supported_models.rst

[e8cc7967f] Michał Moskal 2024-04-18 [Bugfix][Kernel] allow non-power-of-two head sizes in prefix prefill (#4128)
1	1	tests/kernels/test_prefix_prefill.py
27	17	vllm/attention/ops/prefix_prefill.py

[53b018edc] Michael Goin 2024-04-18 [Bugfix] Get available quantization methods from quantization registry (#4098)
2	1	benchmarks/benchmark_latency.py
3	1	benchmarks/benchmark_throughput.py
3	4	tests/models/test_marlin.py
4	3	vllm/config.py
2	1	vllm/engine/arg_utils.py
4	3	vllm/model_executor/layers/quantization/__init__.py

[66ded0306] Harry Mellor 2024-04-18 Allow model to be served under multiple names (#2894)
4	4	vllm/entrypoints/openai/api_server.py
7	3	vllm/entrypoints/openai/cli_args.py
4	4	vllm/entrypoints/openai/serving_chat.py
3	3	vllm/entrypoints/openai/serving_completion.py
8	7	vllm/entrypoints/openai/serving_engine.py

[6dc1fc9cf] youkaichao 2024-04-17 [Core] nccl integrity check and test (#4155)
1	0	.buildkite/test-pipeline.yaml
43	0	tests/distributed/test_pynccl_library.py
12	26	vllm/distributed/device_communicators/pynccl.py
51	0	vllm/utils.py

[533d2a1f3] SangBin Cho 2024-04-18 [Typing] Mypy typing part 2 (#4043)
4	4	.github/workflows/mypy.yaml
4	4	format.sh
25	19	vllm/engine/async_llm_engine.py
2	2	vllm/lora/worker_manager.py
2	2	vllm/model_executor/guided_decoding/outlines_decoding.py
5	1	vllm/model_executor/guided_decoding/outlines_logits_processors.py
9	7	vllm/model_executor/model_loader/neuron.py
1	0	vllm/model_executor/model_loader/tensorizer.py
4	0	vllm/model_executor/sampling_metadata.py
3	3	vllm/spec_decode/batch_expansion.py
2	2	vllm/spec_decode/interfaces.py
1	0	vllm/spec_decode/metrics.py
11	10	vllm/spec_decode/multi_step_worker.py
5	1	vllm/spec_decode/spec_decode_worker.py
15	10	vllm/worker/cpu_model_runner.py
6	5	vllm/worker/cpu_worker.py
56	40	vllm/worker/model_runner.py
14	8	vllm/worker/neuron_model_runner.py
7	4	vllm/worker/worker.py
4	4	vllm/worker/worker_base.py

[a53222544] Shoichi Uchinami 2024-04-18 [Kernel] Add punica dimension for Swallow-MS-7B LoRA (#4134)
1	0	csrc/punica/bgmv/bgmv_config.h
1	0	tests/lora/test_punica.py

[fe3b5bbc2] Elinx 2024-04-17 [Bugfix] fix output parsing error for trtllm backend (#4137)
1	1	benchmarks/backend_request_func.py

[8438e0569] youkaichao 2024-04-17 [Core] RayWorkerVllm --> WorkerWrapper to reduce duplication (#4024)
3	4	tests/distributed/test_pynccl.py
7	37	vllm/engine/ray_utils.py
84	72	vllm/executor/ray_gpu_executor.py
14	2	vllm/utils.py
4	1	vllm/worker/cpu_worker.py
4	0	vllm/worker/neuron_worker.py
4	0	vllm/worker/worker.py
56	0	vllm/worker/worker_base.py

[11d652bd4] Cade Daniel 2024-04-16 [CI] Move CPU/AMD tests to after wait (#4123)
8	7	.buildkite/test-template.j2

[d150e4f89] Cade Daniel 2024-04-16 [Misc] [CI] Fix CI failure caught after merge (#4126)
2	0	vllm/executor/gpu_executor.py

[e95cd8795] Cade Daniel 2024-04-16 [Speculative decoding 6/9] Integrate speculative decoding with LLMEngine  (#3894)
70	0	tests/core/block/e2e/test_correctness.py
10	7	tests/core/utils.py
270	0	tests/engine/output_processor/test_multi_step.py
114	13	tests/spec_decode/e2e/test_correctness.py
2	2	tests/spec_decode/test_multi_step_worker.py
20	12	tests/spec_decode/test_spec_decode_worker.py
1	1	tests/spec_decode/utils.py
0	1	vllm/core/block/block_table.py
2	6	vllm/core/scheduler.py
3	1	vllm/engine/async_llm_engine.py
53	318	vllm/engine/llm_engine.py
0	0	vllm/engine/output_processor/__init__.py
69	0	vllm/engine/output_processor/interfaces.py
126	0	vllm/engine/output_processor/multi_step.py
276	0	vllm/engine/output_processor/single_step.py
101	0	vllm/engine/output_processor/stop_checker.py
16	0	vllm/engine/output_processor/util.py
2	1	vllm/executor/cpu_executor.py
3	2	vllm/executor/executor_base.py
69	10	vllm/executor/gpu_executor.py
4	1	vllm/executor/neuron_executor.py
2	1	vllm/executor/ray_gpu_executor.py
13	0	vllm/sequence.py
18	5	vllm/spec_decode/batch_expansion.py
15	1	vllm/spec_decode/multi_step_worker.py
35	9	vllm/spec_decode/spec_decode_worker.py
26	0	vllm/spec_decode/util.py
5	3	vllm/worker/cpu_worker.py
7	4	vllm/worker/neuron_worker.py
8	3	vllm/worker/worker.py
7	6	vllm/worker/worker_base.py

[69e1d2fb6] Antoni Baum 2024-04-16 [Core] Refactor model loading code (#4097)
1	1	.buildkite/test-pipeline.yaml
2	2	examples/fp8/extract_scales.py
1	1	examples/tensorize_vllm_model.py
5	5	tests/lora/conftest.py
6	4	tests/lora/test_worker.py
1	1	tests/model_executor/weight_utils.py
0	4	tests/quantization/test_autogptq_marlin_configs.py
12	2	tests/samplers/test_sampler.py
1	0	tests/spec_decode/utils.py
0	0	tests/{tensorizer => tensorizer_loader}/__init__.py
2	2	tests/{tensorizer => tensorizer_loader}/tensorize_vllm_model_for_testing.py
82	57	tests/{tensorizer => tensorizer_loader}/test_tensorizer.py
0	4	tests/test_config.py
6	1	tests/test_logits_processor.py
24	15	tests/worker/test_model_runner.py
1	0	tests/worker/test_swap.py
66	135	vllm/config.py
25	34	vllm/engine/arg_utils.py
8	11	vllm/engine/llm_engine.py
1	0	vllm/executor/cpu_executor.py
5	5	vllm/executor/executor_base.py
1	1	vllm/executor/gpu_executor.py
3	2	vllm/executor/ray_gpu_executor.py
0	128	vllm/model_executor/model_loader.py
30	0	vllm/model_executor/model_loader/__init__.py
354	0	vllm/model_executor/model_loader/loader.py
0	0	vllm/model_executor/{neuron_model_loader.py => model_loader/neuron.py}
82	34	vllm/model_executor/{tensorizer_loader.py => model_loader/tensorizer.py}
40	0	vllm/model_executor/model_loader/utils.py
127	168	vllm/model_executor/{ => model_loader}/weight_utils.py
4	10	vllm/model_executor/models/baichuan.py
4	10	vllm/model_executor/models/bloom.py
4	10	vllm/model_executor/models/chatglm.py
4	12	vllm/model_executor/models/commandr.py
4	12	vllm/model_executor/models/dbrx.py
4	10	vllm/model_executor/models/decilm.py
6	14	vllm/model_executor/models/deepseek.py
4	10	vllm/model_executor/models/falcon.py
4	10	vllm/model_executor/models/gemma.py
4	10	vllm/model_executor/models/gpt2.py
4	10	vllm/model_executor/models/gpt_bigcode.py
4	10	vllm/model_executor/models/gpt_j.py
4	10	vllm/model_executor/models/gpt_neox.py
4	10	vllm/model_executor/models/internlm2.py
4	12	vllm/model_executor/models/jais.py
5	11	vllm/model_executor/models/llama.py
4	10	vllm/model_executor/models/llava.py
4	10	vllm/model_executor/models/minicpm.py
6	14	vllm/model_executor/models/mixtral.py
5	14	vllm/model_executor/models/mixtral_quant.py
4	10	vllm/model_executor/models/mpt.py
4	12	vllm/model_executor/models/olmo.py
4	10	vllm/model_executor/models/opt.py
4	10	vllm/model_executor/models/orion.py
4	10	vllm/model_executor/models/phi.py
4	10	vllm/model_executor/models/qwen.py
4	10	vllm/model_executor/models/qwen2.py
6	14	vllm/model_executor/models/qwen2_moe.py
4	10	vllm/model_executor/models/stablelm.py
4	10	vllm/model_executor/models/starcoder2.py
4	10	vllm/model_executor/models/xverse.py
20	1	vllm/transformers_utils/tokenizer.py
8	4	vllm/worker/cpu_model_runner.py
5	2	vllm/worker/cpu_worker.py
7	8	vllm/worker/model_runner.py
1	1	vllm/worker/neuron_model_runner.py
5	5	vllm/worker/worker.py

[05434764c] Noam Gat 2024-04-16 LM Format Enforcer Guided Decoding Support (#3868)
1	0	requirements-common.txt
39	3	tests/entrypoints/test_guided_processors.py
50	19	tests/entrypoints/test_openai_server.py
21	5	vllm/config.py
15	3	vllm/engine/arg_utils.py
7	3	vllm/engine/llm_engine.py
12	0	vllm/entrypoints/openai/protocol.py
5	1	vllm/entrypoints/openai/serving_chat.py
5	1	vllm/entrypoints/openai/serving_completion.py
25	0	vllm/model_executor/guided_decoding/__init__.py
69	0	vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py
3	4	vllm/model_executor/{guided_decoding.py => guided_decoding/outlines_decoding.py}
52	48	vllm/model_executor/{guided_logits_processors.py => guided_decoding/outlines_logits_processors.py}

[4e7ee664e] SangBin Cho 2024-04-16 [Core] Fix engine-use-ray broken  (#4105)
13	4	tests/async_engine/test_api_server.py
3	4	vllm/engine/async_llm_engine.py

[37e84a403] SangBin Cho 2024-04-16 [Typing] Fix Sequence type GenericAlias only available after Python 3.9. (#4092)
3	2	vllm/core/block_manager_v1.py
1	1	vllm/core/block_manager_v2.py
1	1	vllm/core/interfaces.py
4	3	vllm/utils.py

[4695397dc] Ricky Xu 2024-04-15 [Bugfix] Fix ray workers profiling with nsight  (#4095)
19	0	vllm/executor/ray_gpu_executor.py

[d619ae2d1] Sanger Steel 2024-04-15 [Doc] Add better clarity for tensorizer usage (#4090)
1	1	docs/source/models/engine_args.rst
44	16	examples/tensorize_vllm_model.py
1	5	vllm/model_executor/tensorizer_loader.py

[eb46fbfda] Nick Hill 2024-04-15 [Core] Simplifications to executor classes (#4071)
10	21	vllm/executor/cpu_executor.py
20	7	vllm/executor/executor_base.py
4	28	vllm/executor/gpu_executor.py
6	23	vllm/executor/neuron_executor.py
4	30	vllm/executor/ray_gpu_executor.py

[0003e9154] Li, Jiang 2024-04-15 [Misc][Minor] Fix CPU block num log in CPUExecutor. (#4088)
4	1	vllm/executor/cpu_executor.py

[e11e20073] Zhuohan Li 2024-04-14 [Bugfix] Fix filelock version requirement (#4075)
2	1	requirements-common.txt

[8db1bf32f] Roy 2024-04-15 [Misc] Upgrade triton to 2.2.0 (#4061)
1	1	requirements-cpu.txt
0	1	requirements-cuda.txt

[aceb17cf2] Simon Mo 2024-04-14 [Docs] document that mixtral 8x22b is supported (#4073)
1	1	README.md
18	18	docs/source/models/supported_models.rst

[563c54f76] Nick Hill 2024-04-14 [BugFix] Fix tensorizer extra in setup.py (#4072)
1	1	setup.py

[2cd6b4f36] youkaichao 2024-04-13 [Core] avoid too many cuda context by caching p2p test (#4021)
21	32	vllm/distributed/device_communicators/custom_all_reduce.py
9	0	vllm/distributed/parallel_state.py
86	1	vllm/distributed/utils.py

[711a00025] Sanger Steel 2024-04-13 [Frontend] [Core] feat: Add model loading using `tensorizer` (#3476)
3	0	.buildkite/test-pipeline.yaml
1	0	docs/source/conf.py
2	1	docs/source/models/engine_args.rst
254	0	examples/tensorize_vllm_model.py
1	1	requirements-cpu.txt
1	0	requirements-dev.txt
3	0	setup.py
0	0	tests/tensorizer/__init__.py
245	0	tests/tensorizer/tensorize_vllm_model_for_testing.py
302	0	tests/tensorizer/test_tensorizer.py
72	2	vllm/config.py
38	7	vllm/engine/arg_utils.py
7	1	vllm/engine/llm_engine.py
11	12	vllm/executor/gpu_executor.py
5	1	vllm/executor/ray_gpu_executor.py
43	18	vllm/model_executor/model_loader.py
319	0	vllm/model_executor/tensorizer_loader.py
30	4	vllm/model_executor/weight_utils.py
7	2	vllm/worker/model_runner.py
7	2	vllm/worker/worker.py

[989ae2538] Jee Li 2024-04-13 [Kernel] Add punica dimension for Baichuan-13B (#4053)
1	0	csrc/punica/bgmv/bgmv_config.h
1	1	tests/lora/test_baichuan.py
1	0	tests/lora/test_punica.py

[0a430b4ae] zspo 2024-04-13 [Bugfix] fix_small_bug_in_neuron_executor (#4051)
2	0	vllm/executor/neuron_executor.py

[ec8e3c695] zspo 2024-04-13 [Bugfix] fix_log_time_in_metrics (#4050)
1	1	vllm/engine/metrics.py

[98afde19f] youkaichao 2024-04-13 [Core][Distributed] improve logging for init dist (#4042)
6	0	vllm/distributed/parallel_state.py

[5c2e66e48] Dylan Hawk 2024-04-12 [Bugfix] More type hint fixes for py 3.8 (#4039)
1	1	vllm/executor/executor_base.py
2	2	vllm/worker/cpu_worker.py
2	2	vllm/worker/neuron_worker.py
3	3	vllm/worker/worker_base.py

[546e72116] youkaichao 2024-04-12 [CI/Test] expand ruff and yapf for all supported python version (#4037)
1	1	.github/workflows/mypy.yaml
1	1	.github/workflows/ruff.yml
1	1	.github/workflows/yapf.yml

[b8aacac31] Jee Li 2024-04-13 [Bugfix] Fix LoRA bug (#4032)
9	6	vllm/lora/layers.py

[d04973ad5] Bellk17 2024-04-12 Fix triton compilation issue (#3984)
5	1	vllm/attention/ops/triton_flash_attention.py

[fbb9d9eef] youkaichao 2024-04-12 [Core] fix custom allreduce default value (#4040)
1	1	vllm/entrypoints/llm.py

[09473ee41] SangBin Cho 2024-04-13 [mypy] Add mypy type annotation part 1 (#4006)
50	0	.github/workflows/mypy.yaml
17	5	format.sh
4	1	pyproject.toml
2	1	requirements-common.txt
1	1	requirements-dev.txt
6	3	vllm/config.py
7	5	vllm/core/block_manager_v1.py
3	1	vllm/core/block_manager_v2.py
3	1	vllm/core/interfaces.py
15	10	vllm/core/scheduler.py
5	5	vllm/distributed/communication_op.py
12	6	vllm/engine/ray_utils.py
1	0	vllm/entrypoints/api_server.py
6	2	vllm/entrypoints/llm.py
2	2	vllm/executor/cpu_executor.py
2	2	vllm/executor/gpu_executor.py
2	2	vllm/executor/neuron_executor.py
6	5	vllm/executor/ray_gpu_executor.py
3	2	vllm/sampling_params.py
4	4	vllm/sequence.py
2	1	vllm/transformers_utils/config.py
5	2	vllm/transformers_utils/detokenizer.py
2	2	vllm/transformers_utils/tokenizer.py
4	4	vllm/usage/usage_lib.py
7	5	vllm/utils.py

[d4ec9ffb9] Zhuohan Li 2024-04-12 [Misc] Fix typo in scheduler.py (#4022)
1	1	vllm/core/scheduler.py

[96b6a6d79] youkaichao 2024-04-12 [Bugfix] fix type hint for py 3.8 (#4036)
2	2	vllm/executor/executor_base.py

[36729bac1] SangBin Cho 2024-04-13 [Test] Test multiple attn backend for chunked prefill.  (#4023)
7	1	.buildkite/test-pipeline.yaml
0	6	tests/basic_correctness/test_basic_correctness.py
0	4	tests/basic_correctness/test_chunked_prefill.py
6	12	vllm/attention/backends/rocm_flash_attn.py

[7fd3949a0] Cyrus Leung 2024-04-12 [Frontend][Core] Move `merge_async_iterators` to utils (#4026)
1	37	vllm/entrypoints/openai/serving_completion.py
38	2	vllm/utils.py

[1096717ae] Jee Li 2024-04-12 [Core] Support LoRA on quantized models (#4012)
5	0	tests/lora/conftest.py
179	0	tests/lora/test_quant_model.py
6	3	vllm/config.py
44	23	vllm/lora/layers.py

[c2b4a1bce] Michael Feil 2024-04-11 [Doc] Add typing hints / mypy types cleanup (#3816)
33	29	benchmarks/backend_request_func.py
2	1	docs/source/conf.py
3	2	setup.py
19	12	vllm/core/block/interfaces.py
8	2	vllm/engine/metrics.py
7	1	vllm/logger.py
8	7	vllm/model_executor/layers/rotary_embedding.py
2	2	vllm/transformers_utils/config.py
1	1	vllm/transformers_utils/configs/dbrx.py
5	5	vllm/transformers_utils/tokenizers/baichuan.py
2	2	vllm/utils.py

[e46a60aa4] Nick Hill 2024-04-11 [BugFix] Fix handling of stop strings and stop token ids (#3672)
1	1	tests/conftest.py
1	1	tests/{samplers => engine}/test_stop_reason.py
111	0	tests/engine/test_stop_strings.py
65	33	vllm/engine/llm_engine.py
3	1	vllm/outputs.py
9	0	vllm/sampling_params.py
6	0	vllm/sequence.py
6	1	vllm/transformers_utils/detokenizer.py

[1e96c3341] Antoni Baum 2024-04-11 Add extra punica sizes to support bigger vocabs (#4015)
11	1	csrc/punica/bgmv/bgmv_config.h
7	7	csrc/punica/punica_ops.cc
44	34	tests/lora/test_layers.py
45	4	tests/lora/test_punica.py
2	2	vllm/lora/layers.py

[95e7d4a97] Dylan Hawk 2024-04-11 Fix echo/logprob OpenAI completion bug (#3441)
31	0	tests/entrypoints/test_openai_server.py
5	4	vllm/entrypoints/openai/serving_chat.py
10	5	vllm/entrypoints/openai/serving_completion.py
27	20	vllm/entrypoints/openai/serving_engine.py

[559eb852f] youkaichao 2024-04-11 [Core] init_distributed_environment align with init_process_group(#4014)
3	3	vllm/distributed/parallel_state.py

[a10d3056d] Antoni Baum 2024-04-11 [Core] Set `linear_weights` directly on the layer (#3977)
1	1	csrc/quantization/gptq/q_gemm.cu
1	1	tests/kernels/test_moe.py
4	8	vllm/lora/layers.py
40	37	vllm/model_executor/layers/linear.py
16	13	vllm/model_executor/layers/quantization/awq.py
26	21	vllm/model_executor/layers/quantization/gptq.py
13	10	vllm/model_executor/layers/quantization/marlin.py
13	11	vllm/model_executor/layers/quantization/squeezellm.py

[8afca5088] bigPYJ1151 2024-04-12 [Hardware][Intel] Isolate CPUModelRunner and ModelRunner for better maintenance (#3824)
24	48	vllm/attention/backends/torch_sdpa.py
10	0	vllm/executor/cpu_executor.py
0	1	vllm/utils.py
408	0	vllm/worker/cpu_model_runner.py
1	12	vllm/worker/cpu_worker.py

[08ccee1e8] fuchen.ljl 2024-04-11 punica fix-bgmv-kernel-640 (#4007)
1	0	csrc/punica/bgmv/bgmv_config.h

[c1dc54712] Roger Wang 2024-04-11 [Kernel] Fused MoE Config for Mixtral 8x22 (#4002)
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=NVIDIA_H100_80GB_HBM3.json

[f3d0bf758] youkaichao 2024-04-10 [Doc][Installation] delete python setup.py develop (#3989)
0	10	docs/source/getting_started/installation.rst

[e9da5a40c] Kunshang Ji 2024-04-11 [Misc] Add indirection layer for custom ops  (#3913)
1	1	benchmarks/kernels/benchmark_paged_attention.py
3	3	tests/kernels/test_attention.py
12	13	tests/kernels/test_cache.py
193	0	vllm/_custom_ops.py
5	5	vllm/attention/ops/paged_attn.py
1	1	vllm/model_executor/layers/activation.py
1	1	vllm/model_executor/layers/fused_moe/fused_moe.py
1	1	vllm/model_executor/layers/layernorm.py
1	1	vllm/model_executor/layers/quantization/awq.py
1	1	vllm/model_executor/layers/quantization/gptq.py
1	1	vllm/model_executor/layers/quantization/marlin.py
1	1	vllm/model_executor/layers/quantization/squeezellm.py
1	1	vllm/model_executor/layers/rotary_embedding.py
2	2	vllm/utils.py

[e42df7227] SangBin Cho 2024-04-11 [Test] Add xformer and flash attn tests (#3961)
6	0	tests/basic_correctness/test_basic_correctness.py
9	0	vllm/attention/selector.py

[caada5e50] youkaichao 2024-04-10 [Core][Model] torch.compile for layernorm in commandr (#3985)
15	8	vllm/model_executor/models/commandr.py

[67b4221a6] SangBin Cho 2024-04-11 [Core][5/N] Fully working chunked prefill e2e (#3884)
2	0	.buildkite/test-pipeline.yaml
1	2	benchmarks/benchmark_latency.py
38	24	benchmarks/benchmark_throughput.py
70	0	tests/basic_correctness/test_chunked_prefill.py
8	8	tests/core/test_chunked_prefill_scheduler.py
6	1	tests/distributed/test_basic_distributed_correctness.py
66	0	tests/distributed/test_chunked_prefill_distributed.py
1	1	tests/entrypoints/test_openai_server.py
1	1	tests/models/test_models.py
170	19	tests/worker/test_model_runner.py
3	1	vllm/attention/__init__.py
39	3	vllm/attention/backends/abstract.py
55	30	vllm/attention/backends/flash_attn.py
63	34	vllm/attention/backends/rocm_flash_attn.py
42	25	vllm/attention/backends/torch_sdpa.py
79	59	vllm/attention/backends/xformers.py
3	2	vllm/attention/layer.py
0	6	vllm/attention/ops/paged_attn.py
10	3	vllm/config.py
9	6	vllm/core/scheduler.py
9	1	vllm/distributed/communication_op.py
2	3	vllm/engine/arg_utils.py
4	1	vllm/engine/llm_engine.py
3	2	vllm/lora/layers.py
2	1	vllm/sequence.py
241	82	vllm/worker/model_runner.py

[63e7176f2] youkaichao 2024-04-10 [Core][Refactor] move parallel_utils into vllm/distributed (#3950)
1	2	tests/conftest.py
3	3	tests/distributed/test_comm_ops.py
6	7	tests/distributed/test_custom_all_reduce.py
2	2	tests/distributed/test_pynccl.py
1	2	tests/lora/conftest.py
3	0	vllm/distributed/__init__.py
8	6	vllm/{model_executor/parallel_utils => distributed}/communication_op.py
0	0	vllm/{model_executor/parallel_utils => distributed/device_communicators}/__init__.py
3	2	vllm/{model_executor/parallel_utils => distributed/device_communicators}/custom_all_reduce.py
0	0	vllm/{model_executor/parallel_utils => distributed/device_communicators}/pynccl.py
2	2	vllm/{model_executor/parallel_utils => distributed/device_communicators}/pynccl_utils.py
2	2	vllm/{model_executor/parallel_utils => distributed}/parallel_state.py
0	0	vllm/{model_executor/parallel_utils => distributed}/utils.py
6	7	vllm/lora/layers.py
2	3	vllm/model_executor/layers/activation.py
5	6	vllm/model_executor/layers/linear.py
1	2	vllm/model_executor/layers/logits_processor.py
3	5	vllm/model_executor/layers/vocab_parallel_embedding.py
2	2	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bloom.py
1	2	vllm/model_executor/models/chatglm.py
2	2	vllm/model_executor/models/commandr.py
3	4	vllm/model_executor/models/dbrx.py
3	4	vllm/model_executor/models/deepseek.py
3	4	vllm/model_executor/models/falcon.py
1	2	vllm/model_executor/models/gemma.py
1	2	vllm/model_executor/models/gpt2.py
1	2	vllm/model_executor/models/gpt_bigcode.py
1	2	vllm/model_executor/models/gpt_j.py
1	2	vllm/model_executor/models/gpt_neox.py
1	2	vllm/model_executor/models/internlm2.py
2	2	vllm/model_executor/models/jais.py
2	2	vllm/model_executor/models/llama.py
3	4	vllm/model_executor/models/minicpm.py
3	4	vllm/model_executor/models/mixtral.py
3	4	vllm/model_executor/models/mixtral_quant.py
2	2	vllm/model_executor/models/mpt.py
1	2	vllm/model_executor/models/olmo.py
1	2	vllm/model_executor/models/opt.py
1	2	vllm/model_executor/models/orion.py
1	2	vllm/model_executor/models/phi.py
1	2	vllm/model_executor/models/qwen.py
1	2	vllm/model_executor/models/qwen2.py
3	4	vllm/model_executor/models/qwen2_moe.py
1	2	vllm/model_executor/models/stablelm.py
1	2	vllm/model_executor/models/starcoder2.py
1	2	vllm/model_executor/models/xverse.py
0	1	vllm/model_executor/parallel_utils/README.md
2	2	vllm/test_utils.py
3	4	vllm/worker/cpu_worker.py
3	5	vllm/worker/model_runner.py
6	6	vllm/worker/worker.py

[934d3662f] Travis Johnson 2024-04-10 [Bugfix] handle hf_config with architectures == None (#3982)
3	1	vllm/config.py

[92cd2e2f2] Frαnçois 2024-04-10 [Doc] Fix getting stared to use publicly available model (#3963)
4	6	docs/source/serving/openai_compatible_server.md

[e4c4072c9] Daniel E Marasco 2024-04-10 [Bugfix] Remove key sorting for `guided_json` parameter in OpenAi compatible Server (#3945)
1	1	vllm/model_executor/guided_decoding.py

[e35397468] youkaichao 2024-04-10 [Doc] Add doc to state our model support policy (#3948)
26	0	docs/source/models/supported_models.rst

[8b317c6dd] James Whedbee 2024-04-10 [Model][AMD] ROCm support for 256 head dims for Gemma (#3972)
2	3	vllm/attention/ops/triton_flash_attention.py

[bd3c144e0] Woosuk Kwon 2024-04-10 [Bugfix][ROCm] Add numba to Dockerfile.rocm (#3962)
1	1	Dockerfile.rocm

[0258b7a94] Travis Johnson 2024-04-10 [Bugfix] handle prompt_logprobs in _apply_min_tokens_penalty (#3876)
94	22	tests/samplers/test_sampler.py
18	1	vllm/model_executor/layers/sampler.py

[b3104b2a1] 胡译文 2024-04-10 [Bugfix] Fix logits processor when prompt_logprobs is not None (#3899)
62	0	tests/samplers/test_logits_processor.py
10	1	vllm/model_executor/layers/logits_processor.py

[c2e00af52] zhaotyer 2024-04-10 [Bugfix]  fix utils.py/merge_dict func TypeError: 'type' object is not subscriptable (#3955)
3	3	vllm/utils.py

[c013d32c7] Zedong Peng 2024-04-10 [Benchmark] Add cpu options to bench scripts (#3915)
2	2	benchmarks/benchmark_latency.py
2	2	benchmarks/benchmark_throughput.py

[11dd6ebb8] Jee Li 2024-04-10 [Misc] Avoid loading incorrect LoRA config (#3777)
40	0	tests/lora/test_lora_checkpoints.py
15	2	vllm/lora/models.py
11	0	vllm/lora/worker_manager.py

[6c0b04515] Juan Villamizar 2024-04-09 [ROCm][Hardware][AMD] Use Triton Kernel for default FA on ROCm (#3643)
14	0	Dockerfile.rocm
348	0	vllm/attention/backends/rocm_flash_attn.py
2	76	vllm/attention/backends/xformers.py
809	0	vllm/attention/ops/triton_flash_attention.py
40	17	vllm/attention/selector.py

[e23a43aef] Junichi Sato 2024-04-10 [Bugfix] Fix KeyError on loading GPT-NeoX (#3925)
5	0	vllm/model_executor/models/gpt_neox.py

[e7c7067b4] Cade Daniel 2024-04-09 [Misc] [Core] Implement RFC "Augment BaseExecutor interfaces to enable hardware-agnostic speculative decoding" (#3837)
3	3	tests/core/block/e2e/test_correctness.py
6	2	tests/lora/test_worker.py
13	22	tests/spec_decode/test_spec_decode_worker.py
4	2	tests/spec_decode/utils.py
6	4	tests/worker/test_swap.py
3	3	vllm/config.py
3	3	vllm/engine/arg_utils.py
22	0	vllm/engine/llm_engine.py
22	36	vllm/executor/cpu_executor.py
23	0	vllm/executor/executor_base.py
16	44	vllm/executor/gpu_executor.py
15	13	vllm/executor/neuron_executor.py
37	59	vllm/executor/ray_gpu_executor.py
0	13	vllm/executor/utils.py
22	16	vllm/spec_decode/spec_decode_worker.py
4	5	vllm/worker/cache_engine.py
70	19	vllm/worker/cpu_worker.py
42	3	vllm/worker/neuron_worker.py
59	30	vllm/worker/worker.py
83	0	vllm/worker/worker_base.py

[6d592eb43] youkaichao 2024-04-09 [Core] separate distributed_init from worker (#3904)
60	3	vllm/model_executor/parallel_utils/parallel_state.py
6	7	vllm/test_utils.py
7	21	vllm/worker/cpu_worker.py
12	27	vllm/worker/worker.py

[d036198e2] Roy 2024-04-09 [BugFix][Model] Fix commandr RoPE max_position_embeddings (#3919)
3	1	vllm/model_executor/models/commandr.py

[59a6abf3c] Matt Wong 2024-04-08 [Hotfix][CI/Build][Kernel] CUDA 11.8 does not support layernorm optimizations (#3782)
2	0	cmake/utils.cmake
4	2	csrc/layernorm_kernels.cu

[bc0c0192d] Kiran R 2024-04-09 [Bugfix] Enable Proper `attention_bias` Usage in Llama Model Configuration (#3767)
5	1	vllm/model_executor/models/llama.py

[f46864d68] egortolmachev 2024-04-08 [Bugfix] Added Command-R GPTQ support (#3849)
10	0	vllm/model_executor/models/commandr.py

[b4543c8f6] ywfang 2024-04-08 [Model] add minicpm (#3893)
1	0	README.md
4	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
537	0	vllm/model_executor/models/minicpm.py

[0ce0539d4] Isotr0py 2024-04-07 [Bugfix] Fix Llava inference with Tensor Parallelism. (#3883)
2	0	vllm/executor/ray_gpu_executor.py

[2f1928354] youkaichao 2024-04-06 [Core] latency optimization (#3890)
1	1	vllm/core/block_manager_v1.py

[95baec828] youkaichao 2024-04-06 [Core] enable out-of-tree model register (#3871)
4	1	.buildkite/test-pipeline.yaml
27	0	docs/source/models/adding_model.rst
66	0	tests/entrypoints/test_server_oot_registration.py
32	0	tests/models/test_oot_registration.py
2	0	vllm/__init__.py
17	1	vllm/model_executor/models/__init__.py

[e4be7d70b] youkaichao 2024-04-06 [CI/Benchmark] add more iteration and use median for robust latency benchmark (#3889)
12	2	benchmarks/benchmark_latency.py

[54951ac4b] Isotr0py 2024-04-06 [Bugfix] Fix incorrect output on OLMo models in Tensor Parallelism (#3869)
12	19	vllm/model_executor/models/olmo.py

[18de88348] SangBin Cho 2024-04-06 [Chunked Prefill][4/n] Chunked prefill scheduler. (#3853)
1	1	requirements-common.txt
563	0	tests/core/test_chunked_prefill_scheduler.py
201	65	tests/core/test_scheduler.py
55	3	tests/test_sequence.py
2	1	vllm/config.py
1	3	vllm/core/policy.py
345	95	vllm/core/scheduler.py
2	3	vllm/engine/llm_engine.py
48	11	vllm/sequence.py
0	1	vllm/worker/model_runner.py

[1d7c940d7] Thomas Parnell 2024-04-05 Add option to completion API to truncate prompt tokens (#3144)
3	1	vllm/entrypoints/openai/protocol.py
8	2	vllm/entrypoints/openai/serving_completion.py
18	4	vllm/entrypoints/openai/serving_engine.py
12	1	vllm/sampling_params.py

[cfaf49a16] Woosuk Kwon 2024-04-05 [Misc] Define common requirements (#3841)
1	1	.github/workflows/publish.yml
1	1	.github/workflows/scripts/build.sh
0	1	CONTRIBUTING.md
5	3	Dockerfile
2	1	MANIFEST.in
3	9	requirements.txt => requirements-common.txt
6	15	requirements-cpu.txt
10	0	requirements-cuda.txt
4	9	requirements-neuron.txt
4	17	requirements-rocm.txt
26	20	setup.py

[9edec652e] Noam Gat 2024-04-05 [Bugfix] Fixing requirements.txt (#3865)
1	1	requirements.txt

[e0dd4d358] Cade Daniel 2024-04-04 [Misc] Fix linter issues in examples/fp8/quantizer/quantize.py (#3864)
14	16	examples/fp8/quantizer/quantize.py

[e5043a3e7] Cade Daniel 2024-04-04 [Misc] Add pytest marker to opt-out of global test cleanup (#3863)
5	1	tests/conftest.py
3	0	tests/spec_decode/test_batch_expansion.py
3	3	tests/spec_decode/test_spec_decode_worker.py

[d03d64fd2] youkaichao 2024-04-04 [CI/Build] refactor dockerfile & fix pip cache
1	1	.buildkite/test-pipeline.yaml
48	37	Dockerfile
0	3	docs/source/conf.py

[78107fa09] Sean Gallen 2024-04-04 [Doc]Add asynchronous engine arguments to documentation. (#3810)
16	0	docs/source/models/engine_args.rst

[c391e4b68] youkaichao 2024-04-04 [Core] improve robustness of pynccl (#3860)
13	8	vllm/model_executor/parallel_utils/pynccl.py

[9117f892f] Saurabh Dash 2024-04-05 [Model] Cohere CommandR+ (#3829)
40	8	vllm/model_executor/models/commandr.py

[db2a6a41e] Michael Goin 2024-04-04 [Hardware][CPU] Update cpu torch to match default of 2.2.1 (#3854)
1	1	requirements-cpu.txt

[ca81ff519] youkaichao 2024-04-04 [Core] manage nccl via a pypi package & upgrade to pt 2.2.1 (#3805)
1	1	.github/workflows/publish.yml
1	1	CMakeLists.txt
7	3	Dockerfile
1	1	pyproject.toml
1	1	requirements-build.txt
3	2	requirements.txt
10	0	setup.py
12	2	vllm/model_executor/parallel_utils/pynccl.py

[b7782002e] TianYu GUO 2024-04-04 [Benchmark] Refactor sample_requests in benchmark_throughput (#3613)
15	16	benchmarks/benchmark_throughput.py

[819a309c0] Chang Su 2024-04-04 [Bugfix] Fix args in benchmark_serving (#3836)
13	10	benchmarks/benchmark_serving.py

[aabe8f40f] Matthias Gerstgrasser 2024-04-03 [Core] [Frontend] Make detokenization optional (#3749)
32	0	tests/engine/test_detokenization.py
11	9	vllm/engine/llm_engine.py
10	0	vllm/sampling_params.py

[498eb5cfa] Woosuk Kwon 2024-04-03 [Bugfix] Add kv_scale input parameter to CPU backend (#3840)
4	2	csrc/cpu/attention.cpp
3	1	csrc/cpu/cache.cpp
4	1	vllm/attention/backends/torch_sdpa.py
1	1	vllm/attention/ops/paged_attn.py

[537ee25f4] Michael Feil 2024-04-03 [Core] Enable hf_transfer by default if available (#3817)
26	0	tests/model_executor/weight_utils.py
16	0	vllm/model_executor/weight_utils.py

[294f8f666] Tao He 2024-04-04 [BugFix] Pass tokenizer_config to local_tokenizer_group (#3754)
1	0	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py

[b95047f2d] Woosuk Kwon 2024-04-03 [Misc] Publish 3rd meetup slides (#3835)
1	11	README.md

[2ff767b51] Adrian Abeyta 2024-04-03 Enable scaled FP8 (e4m3fn) KV cache on ROCm (AMD GPU) (#3290)
1	0	.gitignore
1	1	CMakeLists.txt
16	2	benchmarks/benchmark_latency.py
19	3	benchmarks/benchmark_throughput.py
10	3	benchmarks/kernels/benchmark_paged_attention.py
1	0	cmake/utils.cmake
1	1	csrc/attention/attention_dtypes.h
79	44	csrc/attention/attention_kernels.cu
1	1	csrc/attention/{dtype_fp8_e5m2.cuh => dtype_fp8.cuh}
3	2	csrc/cache.h
42	23	csrc/cache_kernels.cu
4	2	csrc/ops.h
3	3	csrc/pybind.cpp
167	0	csrc/quantization/fp8/amd_detail/hip_float8.h
316	0	csrc/quantization/fp8/amd_detail/hip_float8_impl.h
517	0	csrc/quantization/fp8/amd_detail/quant_utils.cuh
2	1	docs/source/index.rst
49	0	docs/source/quantization/fp8_e4m3_kvcache.rst
5	2	docs/source/quantization/{fp8_e5m2_kv_cache.rst => fp8_e5m2_kvcache.rst}
96	0	examples/fp8/README.md
367	0	examples/fp8/extract_scales.py
32	0	examples/fp8/quantizer/README.md
369	0	examples/fp8/quantizer/quantize.py
4	0	pyproject.toml
90	0	tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json
42	0	tests/fp8_kv/llama2-7b-fp8-kv/kv_cache_scales.json
11	5	tests/kernels/test_attention.py
86	12	tests/kernels/test_cache.py
1	0	vllm/attention/backends/abstract.py
7	1	vllm/attention/backends/flash_attn.py
7	1	vllm/attention/backends/xformers.py
3	1	vllm/attention/layer.py
5	0	vllm/attention/ops/paged_attn.py
20	14	vllm/config.py
18	5	vllm/engine/arg_utils.py
1	0	vllm/engine/llm_engine.py
84	0	vllm/model_executor/layers/quantization/schema.py
39	3	vllm/model_executor/models/llama.py
42	1	vllm/model_executor/weight_utils.py
10	10	vllm/utils.py
21	1	vllm/worker/model_runner.py

[3dcb3e8b9] SangBin Cho 2024-04-04 [3/N] Refactor scheduler for chunked prefill scheduling (#3550)
532	6	tests/core/test_scheduler.py
13	6	tests/core/utils.py
456	241	vllm/core/scheduler.py
1	1	vllm/engine/llm_engine.py
18	1	vllm/utils.py

[c64cf3867] Michael Feil 2024-04-03 [Doc] Update contribution guidelines for better onboarding (#3819)
2	0	CONTRIBUTING.md

[76b889bf1] Robert Shaw 2024-04-02 [Doc] Update README.md (#3806)
1	1	README.md

[c9b506dad] Nick Hill 2024-04-02 [BugFix] Use different mechanism to get vllm version in `is_cpu()` (#3804)
5	3	vllm/utils.py

[5757d90e2] Cade Daniel 2024-04-02 [Speculative decoding] Adding configuration object for speculative decoding (#3706)
41	0	tests/spec_decode/e2e/conftest.py
50	0	tests/spec_decode/e2e/test_correctness.py
8	10	tests/spec_decode/utils.py
8	9	tests/worker/test_swap.py
187	1	vllm/config.py
44	11	vllm/engine/arg_utils.py
9	10	vllm/engine/async_llm_engine.py
28	16	vllm/engine/llm_engine.py
3	1	vllm/executor/executor_base.py
6	1	vllm/executor/gpu_executor.py
5	1	vllm/executor/neuron_executor.py
5	1	vllm/executor/ray_gpu_executor.py

[a3c226e7e] youkaichao 2024-04-02 [CI/Build] 0.4.0.post1, fix sm 7.0/7.5 binary (#3803)
1	1	vllm/__init__.py

[b321d4881] Michael Goin 2024-04-02 [Bugfix] Add `__init__.py` files for `vllm/core/block/` and `vllm/spec_decode/` (#3798)
0	0	vllm/core/block/__init__.py
0	0	vllm/spec_decode/__init__.py

[ad6eca408] leiwen83 2024-04-03 Fix early CUDA init via get_architecture_class_name import (#3770)
2	1	vllm/engine/llm_engine.py

[205b94942] youkaichao 2024-04-02 [CI/Build] fix TORCH_CUDA_ARCH_LIST in wheel build (#3801)
2	1	.github/workflows/scripts/build.sh

[3bec41f41] Roger Wang 2024-04-02 [Doc] Fix vLLMEngine Doc Page (#3791)
3	1	docs/requirements-docs.txt
1	0	docs/source/conf.py

[0739b1947] A-Mahla 2024-04-02 [Frontend][Bugfix] allow using the default middleware with a root path (#3788)
2	1	vllm/entrypoints/openai/api_server.py

[77a6572aa] bigPYJ1151 2024-04-02 [HotFix] [CI/Build] Minor fix for CPU backend CI (#3787)
1	1	.buildkite/run-cpu-test.sh

[0e3f06fe9] bigPYJ1151 2024-04-02 [Hardware][Intel] Add CPU inference backend (#3634)
14	0	.buildkite/run-cpu-test.sh
3	0	.buildkite/test-template.j2
16	0	CMakeLists.txt
20	0	Dockerfile.cpu
90	0	cmake/cpu_extension.cmake
148	0	csrc/cpu/activation.cpp
744	0	csrc/cpu/attention.cpp
139	0	csrc/cpu/cache.cpp
352	0	csrc/cpu/cpu_types.hpp
117	0	csrc/cpu/layernorm.cpp
199	0	csrc/cpu/pos_encoding.cpp
73	0	csrc/cpu/pybind.cpp
87	0	docs/source/getting_started/cpu-installation.rst
1	0	docs/source/index.rst
15	0	requirements-cpu.txt
17	2	setup.py
253	0	vllm/attention/backends/torch_sdpa.py
7	1	vllm/attention/selector.py
4	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
3	0	vllm/engine/llm_engine.py
154	0	vllm/executor/cpu_executor.py
10	0	vllm/utils.py
280	0	vllm/worker/cpu_worker.py

[eb69d6880] Cade Daniel 2024-04-01 [Misc] [CI/Build] Speed up block manager CPU-only unit tests ~10x by opting-out of GPU cleanup (#3783)
12	2	tests/conftest.py
12	0	tests/core/block/conftest.py
1	16	tests/core/block/e2e/conftest.py

[7d4e1b85e] Qubitium 2024-04-02 [Misc] Add support for new autogptq checkpoint_format (#3689)
68	0	tests/quantization/test_autogptq_marlin_configs.py
15	13	vllm/config.py

[93deb0b38] Cade Daniel 2024-04-01 [Speculative decoding 4/9] Lookahead scheduling for speculative decoding (#3250)
153	0	tests/core/block/e2e/test_correctness.py
103	0	tests/core/block/test_block_manager_v2.py
0	50	tests/core/block/test_block_space_manager.py
75	0	tests/core/block/test_block_table.py
12	12	tests/core/test_block_manager.py
2	2	tests/core/utils.py
16	2	vllm/config.py
52	6	vllm/core/block/block_table.py
24	9	vllm/core/block_manager_v1.py
59	23	vllm/core/block_manager_v2.py
10	6	vllm/core/interfaces.py
63	12	vllm/core/scheduler.py
10	1	vllm/engine/arg_utils.py

[ccb58b23e] Roger Wang 2024-04-01 [Misc] Fix Benchmark TTFT Calculation for Chat Completions (#3768)
3	3	benchmarks/backend_request_func.py

[49782fcb7] Nick Hill 2024-04-01 [Misc] Some minor simplifications to detokenization logic (#3670)
2	2	tests/tokenization/test_detokenize.py
156	8	vllm/transformers_utils/detokenizer.py
1	155	vllm/transformers_utils/tokenizer.py

[f03cc667a] Woosuk Kwon 2024-04-01 [Misc] Minor fixes in requirements.txt (#3769)
1	0	requirements-rocm.txt
0	1	requirements.txt

[563c1d7ec] Robert Shaw 2024-03-30 [CI/Build] Make Marlin Tests Green (#3753)
1	1	tests/models/test_marlin.py

[9c82a1bec] youkaichao 2024-03-30 [Doc] Update installation doc (#3746)
21	12	docs/source/getting_started/installation.rst

[b6d103542] mawong-amd 2024-03-30 [Kernel] Layernorm performance optimization (#3662)
5	0	cmake/utils.cmake
250	20	csrc/layernorm_kernels.cu
28	26	csrc/reduction_utils.cuh
2	1	tests/kernels/test_layernorm.py

[51c31bc10] Simon Mo 2024-03-29 CMake build elf without PTX (#3739)
4	1	cmake/utils.cmake

[3ad438c66] bnellnm 2024-03-29 Fix build when nvtools is missing (#3698)
4	3	CMakeLists.txt
11	2	cmake/utils.cmake

[203d4f82a] youkaichao 2024-03-29 [Core][Bugfix] cache len of tokenizer (#3741)
4	0	vllm/transformers_utils/tokenizer.py

[991143cfc] Nick Hill 2024-03-29 [BugFix] Use consistent logger everywhere (#3738)
2	2	vllm/lora/models.py
3	2	vllm/lora/utils.py
2	2	vllm/lora/worker_manager.py
3	2	vllm/model_executor/parallel_utils/pynccl.py
3	2	vllm/model_executor/parallel_utils/pynccl_utils.py

[8b2d3cbc1] Simon Mo 2024-03-29 usage lib get version another way (#3735)
2	2	vllm/usage/usage_lib.py

[9765b5c40] Hongxia Yang 2024-03-29 [ROCm][Bugfix] Fixed several bugs related to rccl path and attention selector logic (#3699)
1	1	Dockerfile.rocm
1	1	requirements-rocm.txt
2	2	vllm/attention/backends/xformers.py
1	1	vllm/model_executor/parallel_utils/pynccl.py

[430530fc1] Simon Mo 2024-03-29 bump version to v0.4.0 (#3712)
1	1	vllm/__init__.py

[97356f3c7] Roger Wang 2024-03-29 [Bugfix] Command-R Max Model Length (#3727)
22	9	vllm/config.py

[f510395bb] Roy 2024-03-30 [BugFix][Frontend] Fix completion logprobs=0 error (#3731)
21	0	tests/entrypoints/test_openai_server.py
1	1	vllm/entrypoints/openai/protocol.py
0	3	vllm/entrypoints/openai/serving_completion.py
3	2	vllm/model_executor/layers/sampler.py
1	1	vllm/outputs.py

[6110c39dc] Roy 2024-03-29 [BugFix] Fix tokenizer out of vocab size (#3685)
7	0	tests/tokenization/test_detokenize.py
0	8	vllm/engine/llm_engine.py
0	8	vllm/entrypoints/openai/serving_engine.py
7	3	vllm/transformers_utils/tokenizer.py

[d8658c8cc] yhu422 2024-03-28 Usage Stats Collection (#2852)
2	0	.buildkite/test-template.j2
2	0	Dockerfile
1	0	docs/source/index.rst
57	0	docs/source/serving/usage_stats.md
3	0	requirements-neuron.txt
2	0	requirements-rocm.txt
3	0	requirements.txt
18	11	vllm/engine/async_llm_engine.py
49	4	vllm/engine/llm_engine.py
3	2	vllm/entrypoints/api_server.py
3	1	vllm/entrypoints/llm.py
3	2	vllm/entrypoints/openai/api_server.py
9	4	vllm/model_executor/model_loader.py
0	0	vllm/usage/__init__.py
207	0	vllm/usage/usage_lib.py

[7bc94a0fd] Simon Mo 2024-03-28 add ccache to docker build image (#3704)
6	1	Dockerfile

[756b30a5f] youkaichao 2024-03-28 [Core][Test] move local_rank to the last arg with default value(#3711)
3	3	tests/distributed/test_comm_ops.py
2	2	tests/distributed/test_custom_all_reduce.py
2	0	tests/distributed/test_pynccl.py
6	1	vllm/model_executor/parallel_utils/pynccl.py
4	2	vllm/model_executor/parallel_utils/pynccl_utils.py
3	3	vllm/test_utils.py
4	3	vllm/worker/worker.py

[395aa823e] Woosuk Kwon 2024-03-28 [Misc] Minor type annotation fix (#3716)
2	1	vllm/attention/selector.py

[26422e477] SangBin Cho 2024-03-29 [Test] Make model tests run again and remove --forked from pytest (#3631)
6	7	.buildkite/test-pipeline.yaml
1	0	requirements-dev.txt
1	1	tests/basic_correctness/test_basic_correctness.py
30	0	tests/conftest.py
1	1	tests/distributed/test_comm_ops.py
45	0	tests/models/test_big_models.py
0	3	tests/models/test_llava.py
1	3	tests/models/test_marlin.py
4	1	tests/models/test_mistral.py
10	11	tests/models/test_models.py
1	1	tests/samplers/test_beam_search.py
1	1	tests/samplers/test_seeded_generate.py

[f342153b4] youkaichao 2024-03-28 Revert "bump version to v0.4.0" (#3708)
1	1	vllm/__init__.py

[27a57cad5] Simon Mo 2024-03-28 bump version to v0.4.0 (#3705)
1	1	vllm/__init__.py

[98a42e707] Yile (Michael) Gu 2024-03-28 [Benchmark] Change mii to use persistent deployment and support tensor parallel (#3628)
5	3	benchmarks/benchmark_throughput.py

[0267fef52] youkaichao 2024-03-28 [Core] fix del of communicator (#3702)
3	1	vllm/model_executor/parallel_utils/pynccl.py

[4716a32dd] Simon Mo 2024-03-28 fix logging msg for block manager (#3701)
3	1	vllm/attention/selector.py
1	2	vllm/core/block_manager_v1.py
1	1	vllm/model_executor/parallel_utils/pynccl_utils.py

[c0935c96d] Woosuk Kwon 2024-03-28 [Bugfix] Set enable_prefix_caching=True in prefix caching example (#3703)
1	1	examples/offline_inference_with_prefix.py

[cb40b3ab6] Woosuk Kwon 2024-03-28 [Kernel] Add MoE Triton kernel configs for A100 40GB (#3700)
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1344,device_name=NVIDIA_A100-SXM4-40GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_A100-SXM4-40GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_A100-SXM4-40GB.json

[515386ef3] Roy 2024-03-29 [Core] Support multi-node inference(eager and cuda graph) (#3686)
3	3	tests/distributed/test_comm_ops.py
2	2	tests/distributed/test_custom_all_reduce.py
0	2	vllm/executor/ray_gpu_executor.py
8	10	vllm/model_executor/parallel_utils/pynccl.py
3	1	vllm/model_executor/parallel_utils/pynccl_utils.py
5	1	vllm/test_utils.py
4	3	vllm/worker/worker.py

[a4075cba4] Simon Mo 2024-03-28 [CI] Add test case to run examples scripts (#3638)
10	0	.buildkite/test-pipeline.yaml
9	3	examples/llava_example.py

[96aa014d1] Simon Mo 2024-03-28 fix benchmark format reporting in buildkite (#3693)
3	1	.buildkite/run-benchmarks.sh

[1715056fe] Adam Boeglin 2024-03-28 [Bugfix] Update neuron_executor.py to add optional vision_language_config (#3695)
2	1	vllm/executor/neuron_executor.py

[b51c1cc9d] SangBin Cho 2024-03-29 [2/N] Chunked prefill data update (#3538)
13	1	benchmarks/benchmark_latency.py
4	0	tests/conftest.py
13	9	tests/core/test_scheduler.py
23	1	tests/test_sequence.py
20	17	tests/worker/test_model_runner.py
4	0	vllm/config.py
81	27	vllm/core/scheduler.py
15	5	vllm/engine/arg_utils.py
14	7	vllm/engine/llm_engine.py
55	0	vllm/sequence.py
30	9	vllm/worker/model_runner.py

[ce567a292] Roger Wang 2024-03-28 [Kernel] DBRX Triton MoE kernel H100 (#3692)
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1344,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=2688,device_name=NVIDIA_H100_80GB_HBM3.json

[d6ea427f0] wenyujin333 2024-03-28 [Model] Add support for Qwen2MoeModel (#3346)
1	0	README.md
4	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
457	0	vllm/model_executor/models/qwen2_moe.py

[14ccd94c8] Cade Daniel 2024-03-27 [Core][Bugfix]Refactor block manager for better testability (#3492)
0	0	tests/core/block/__init__.py
56	0	tests/core/block/e2e/conftest.py
86	0	tests/core/block/e2e/test_correctness.py
50	0	tests/core/block/test_block_space_manager.py
500	0	tests/core/block/test_block_table.py
42	0	tests/core/block/test_common.py
93	0	tests/core/block/test_cpu_gpu_block_allocator.py
102	0	tests/core/block/test_naive_block.py
384	0	tests/core/block/test_prefix_caching_block.py
40	39	tests/core/test_block_manager.py
1	1	tests/core/test_scheduler.py
38	1	tests/core/utils.py
1	1	tests/prefix_caching/test_prefix_caching.py
7	0	vllm/config.py
245	0	vllm/core/block/block_table.py
185	0	vllm/core/block/common.py
206	0	vllm/core/block/cpu_gpu_block_allocator.py
105	0	vllm/core/block/interfaces.py
275	0	vllm/core/block/naive_block.py
472	0	vllm/core/block/prefix_caching_block.py
20	22	vllm/core/{block_manager.py => block_manager_v1.py}
210	0	vllm/core/block_manager_v2.py
107	0	vllm/core/interfaces.py
20	7	vllm/core/scheduler.py
15	0	vllm/engine/arg_utils.py
0	6	vllm/engine/llm_engine.py
6	0	vllm/executor/gpu_executor.py
7	0	vllm/executor/ray_gpu_executor.py
2	0	vllm/sequence.py
10	0	vllm/utils.py

[8267b06c3] Woosuk Kwon 2024-03-27 [Kernel] Add Triton MoE kernel configs for DBRX on A100 (#3679)
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=1344,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=16,N=2688,device_name=NVIDIA_A100-SXM4-80GB.json

[3492859b6] youkaichao 2024-03-27 [CI/Build] update default number of jobs and nvcc threads to avoid overloading the system (#3675)
15	5	setup.py

[098e1776b] hxer7963 2024-03-28 [Model] Add support for xverse  (#3610)
1	0	README.md
1	0	vllm/model_executor/models/__init__.py
372	0	vllm/model_executor/models/xverse.py

[10e632228] Roy 2024-03-28 [Model] Fix and clean commandr (#3671)
4	12	vllm/model_executor/models/commandr.py

[6d9aa00fc] Woosuk Kwon 2024-03-27 [Docs] Add Command-R to supported models (#3669)
1	0	README.md
4	0	docs/source/models/supported_models.rst

[1182607e1] zeppombal 2024-03-27 Add support for Cohere's Command-R model (#3433)
1	0	vllm/model_executor/models/__init__.py
337	0	vllm/model_executor/models/commandr.py

[45b6ef651] Roger Wang 2024-03-27 feat(benchmarks): Add Prefix Caching Benchmark to Serving Benchmark (#3277)
3	2	.buildkite/run-benchmarks.sh
121	99	benchmarks/backend_request_func.py
252	51	benchmarks/benchmark_serving.py
518	0	benchmarks/sonnet.txt
1	1	pyproject.toml
2	2	tests/kernels/test_prefix_prefill.py

[195693143] AmadeusChan 2024-03-27 [Misc] add the "download-dir" option to the latency/throughput benchmarks (#3621)
16	12	benchmarks/benchmark_latency.py
16	7	benchmarks/benchmark_throughput.py

[e24336b5a] Megha Agarwal 2024-03-27 [Model] Add support for DBRX (#3660)
1	0	README.md
4	0	docs/source/models/supported_models.rst
1	0	requirements.txt
5	0	vllm/config.py
1	0	vllm/model_executor/models/__init__.py
421	0	vllm/model_executor/models/dbrx.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
277	0	vllm/transformers_utils/configs/dbrx.py

[d18f4e73f] youkaichao 2024-03-27 [Bugfix] [Hotfix] fix nccl library name (#3661)
2	2	vllm/model_executor/parallel_utils/pynccl.py

[82c540beb] Woosuk Kwon 2024-03-27 [Bugfix] More faithful implementation of Gemma (#3653)
43	3	vllm/model_executor/models/gemma.py

[8f44facdd] youkaichao 2024-03-27 [Core] remove cupy dependency (#3625)
5	2	.buildkite/test-pipeline.yaml
1	1	Dockerfile
0	20	Dockerfile.rocm
0	1	requirements.txt
0	6	setup.py
13	4	tests/distributed/test_basic_distributed_correctness.py
20	0	tests/distributed/test_comm_ops.py
90	0	tests/distributed/test_pynccl.py
2	4	vllm/executor/ray_gpu_executor.py
4	4	vllm/model_executor/parallel_utils/communication_op.py
0	130	vllm/model_executor/parallel_utils/cupy_utils.py
18	18	vllm/model_executor/parallel_utils/parallel_state.py
258	0	vllm/model_executor/parallel_utils/pynccl.py
64	0	vllm/model_executor/parallel_utils/pynccl_utils.py
1	4	vllm/test_utils.py
16	13	vllm/worker/model_runner.py
14	16	vllm/worker/worker.py

[e66b629c0] Woosuk Kwon 2024-03-26 [Misc] Minor fix in KVCache type (#3652)
2	2	docs/source/models/adding_model.rst
2	4	vllm/model_executor/models/llava.py
0	2	vllm/worker/neuron_model_runner.py

[76879342a] Jee Li 2024-03-27 [Doc]add lora support (#3649)
2	2	docs/source/models/supported_models.rst

[566b57c5c] Jee Li 2024-03-27 [Kernel] support non-zero cuda devices  in punica kernels (#3636)
3	1	csrc/punica/punica_ops.cc
26	61	tests/lora/test_punica.py

[0dc72273b] Nick Hill 2024-03-26 [BugFix] Fix ipv4 address parsing regression (#3645)
3	1	vllm/utils.py

[a979d9771] liiliiliil 2024-03-27 [Bugfix] Fix ipv6 address parsing bug (#3641)
1	1	vllm/utils.py

[8af890a86] Jee Li 2024-03-26 Enable more models to  inference based on LoRA (#3382)
6	0	csrc/punica/bgmv/bgmv_config.h
10	0	tests/lora/conftest.py
108	0	tests/lora/test_baichuan.py
57	0	tests/lora/test_chatglm3.py
10	3	tests/lora/test_layers.py
4	3	tests/lora/test_punica.py
146	22	vllm/lora/layers.py
7	4	vllm/lora/models.py
39	13	vllm/model_executor/models/baichuan.py
15	0	vllm/model_executor/models/chatglm.py

[dfeb2ecc3] Nick Hill 2024-03-25 [Misc] Include matched stop string/token in responses (#2976)
59	0	tests/samplers/test_stop_reason.py
5	2	vllm/engine/llm_engine.py
16	0	vllm/entrypoints/openai/protocol.py
3	1	vllm/entrypoints/openai/serving_chat.py
3	0	vllm/entrypoints/openai/serving_completion.py
10	4	vllm/outputs.py
1	0	vllm/sequence.py

[3a243095e] Antoni Baum 2024-03-25 Optimize `_get_ranks` in Sampler (#3623)
17	10	vllm/model_executor/layers/sampler.py

[64172a976] xwjiang2010 2024-03-25 [Feature] Add vision language model support. (#3042)
18	0	.buildkite/download-images.sh
7	1	.buildkite/test-pipeline.yaml
84	0	examples/llava_example.py
4	0	requirements-dev.txt
112	14	tests/conftest.py
110	0	tests/models/test_llava.py
1	1	tests/models/test_models.py
1	1	tests/spec_decode/utils.py
1	1	tests/worker/test_model_runner.py
1	1	tests/worker/test_swap.py
62	17	vllm/config.py
6	0	vllm/core/scheduler.py
53	3	vllm/engine/arg_utils.py
16	9	vllm/engine/async_llm_engine.py
11	6	vllm/engine/llm_engine.py
21	5	vllm/entrypoints/llm.py
2	1	vllm/executor/executor_base.py
4	1	vllm/executor/gpu_executor.py
4	1	vllm/executor/ray_gpu_executor.py
11	1	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
9	2	vllm/model_executor/models/llama.py
246	0	vllm/model_executor/models/llava.py
25	0	vllm/sequence.py
14	0	vllm/transformers_utils/config.py
10	0	vllm/utils.py
85	21	vllm/worker/model_runner.py
16	8	vllm/worker/worker.py

[f408d05c5] Simon Mo 2024-03-25 hotfix isort on logprobs ranks pr (#3622)
1	0	tests/samplers/test_ranks.py
1	1	tests/samplers/test_sampler.py

[0b4997e05] Dylan Hawk 2024-03-25 [Bugfix] API stream returning two stops (#3450)
12	0	tests/entrypoints/test_openai_server.py
13	27	vllm/entrypoints/openai/serving_completion.py

[c13ad1b7b] Travis Johnson 2024-03-25 feat: implement the min_tokens sampling parameter (#3124)
222	2	tests/samplers/test_sampler.py
18	10	vllm/engine/llm_engine.py
4	0	vllm/entrypoints/openai/protocol.py
41	0	vllm/model_executor/layers/sampler.py
14	0	vllm/sampling_params.py

[819924e74] Swapnil Parekh 2024-03-25 [Core] Adding token ranks along with logprobs (#3516)
49	0	tests/samplers/test_ranks.py
40	10	vllm/model_executor/layers/sampler.py
9	2	vllm/sequence.py

[01bfb22b4] SangBin Cho 2024-03-25 [CI] Try introducing isort.  (#3495)
5	2	.github/workflows/ruff.yml
1	2	benchmarks/benchmark_prefix_caching.py
3	6	benchmarks/benchmark_serving.py
1	1	benchmarks/benchmark_throughput.py
3	1	benchmarks/kernels/benchmark_mixtral_moe.py
2	2	benchmarks/kernels/benchmark_paged_attention.py
4	3	benchmarks/kernels/benchmark_rope.py
1	1	cmake/hipify.py
1	1	collect_env.py
2	1	docs/source/conf.py
2	1	examples/gradio_openai_chatbot_webserver.py
1	1	examples/llm_engine_example.py
2	2	examples/multilora_inference.py
3	1	examples/offline_inference_distributed.py
42	0	format.sh
4	0	pyproject.toml
1	0	requirements-dev.txt
5	5	setup.py
3	3	tests/async_engine/test_chat_template.py
1	1	tests/conftest.py
5	4	tests/core/test_block_manager.py
3	2	tests/core/test_scheduler.py
3	5	tests/distributed/test_comm_ops.py
1	1	tests/distributed/test_custom_all_reduce.py
3	3	tests/entrypoints/test_guided_processors.py
7	8	tests/entrypoints/test_openai_server.py
1	0	tests/kernels/conftest.py
1	1	tests/kernels/test_activation.py
3	4	tests/kernels/test_attention.py
1	2	tests/kernels/test_cache.py
1	1	tests/kernels/test_moe.py
2	1	tests/kernels/test_pos_encoding.py
3	2	tests/kernels/test_prefix_prefill.py
3	2	tests/kernels/test_rand.py
4	4	tests/kernels/test_sampler.py
4	4	tests/lora/conftest.py
4	2	tests/lora/test_layer_variation.py
14	18	tests/lora/test_layers.py
1	0	tests/lora/test_llama.py
4	4	tests/lora/test_lora_manager.py
3	1	tests/lora/test_tokenizer_group.py
1	1	tests/lora/test_utils.py
2	2	tests/lora/test_worker.py
3	1	tests/models/test_marlin.py
1	1	tests/samplers/test_logprobs.py
2	3	tests/samplers/test_rejection_sampler.py
1	2	tests/samplers/test_sampler.py
1	1	tests/samplers/test_seeded_generate.py
2	2	tests/spec_decode/test_batch_expansion.py
3	3	tests/spec_decode/test_metrics.py
10	9	tests/spec_decode/test_multi_step_worker.py
11	9	tests/spec_decode/test_spec_decode_worker.py
4	4	tests/spec_decode/test_utils.py
11	9	tests/spec_decode/utils.py
1	1	tests/test_cache_block_hashing.py
1	1	tests/test_sequence.py
3	1	tests/tokenization/test_cached_tokenizer.py
5	5	tests/tokenization/test_detokenize.py
4	2	tests/tokenization/test_tokenizer_group.py
1	1	tests/worker/test_swap.py
2	1	vllm/attention/__init__.py
3	2	vllm/attention/backends/flash_attn.py
2	1	vllm/attention/backends/xformers.py
1	2	vllm/attention/ops/paged_attn.py
4	2	vllm/attention/selector.py
7	6	vllm/config.py
3	3	vllm/core/block_manager.py
1	1	vllm/core/evictor.py
3	3	vllm/core/scheduler.py
2	3	vllm/engine/arg_utils.py
3	3	vllm/engine/async_llm_engine.py
5	5	vllm/engine/llm_engine.py
7	6	vllm/engine/metrics.py
2	3	vllm/engine/ray_utils.py
1	1	vllm/entrypoints/api_server.py
1	1	vllm/entrypoints/llm.py
8	9	vllm/entrypoints/openai/api_server.py
2	3	vllm/entrypoints/openai/protocol.py
8	6	vllm/entrypoints/openai/serving_chat.py
14	15	vllm/entrypoints/openai/serving_completion.py
6	6	vllm/entrypoints/openai/serving_engine.py
2	2	vllm/executor/executor_base.py
4	4	vllm/executor/gpu_executor.py
3	3	vllm/executor/neuron_executor.py
6	6	vllm/executor/ray_gpu_executor.py
1	1	vllm/logger.py
6	8	vllm/lora/layers.py
1	0	vllm/lora/lora.py
2	3	vllm/lora/models.py
2	2	vllm/lora/worker_manager.py
6	6	vllm/model_executor/guided_decoding.py
3	3	vllm/model_executor/guided_logits_processors.py
1	3	vllm/model_executor/layers/fused_moe/__init__.py
3	3	vllm/model_executor/layers/linear.py
2	2	vllm/model_executor/layers/ops/rand.py
1	1	vllm/model_executor/layers/ops/sample.py
2	2	vllm/model_executor/layers/quantization/__init__.py
1	1	vllm/model_executor/layers/quantization/gptq.py
2	1	vllm/model_executor/layers/quantization/marlin.py
2	2	vllm/model_executor/layers/rejection_sampler.py
3	5	vllm/model_executor/layers/vocab_parallel_embedding.py
2	2	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/chatglm.py
3	3	vllm/model_executor/models/deepseek.py
1	1	vllm/model_executor/models/falcon.py
1	1	vllm/model_executor/models/gemma.py
2	2	vllm/model_executor/models/gpt_j.py
2	2	vllm/model_executor/models/gpt_neox.py
2	2	vllm/model_executor/models/internlm2.py
10	16	vllm/model_executor/models/jais.py
2	2	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/mixtral.py
3	5	vllm/model_executor/models/mixtral_quant.py
10	15	vllm/model_executor/models/olmo.py
2	2	vllm/model_executor/models/orion.py
2	2	vllm/model_executor/models/phi.py
2	2	vllm/model_executor/models/qwen.py
3	3	vllm/model_executor/models/qwen2.py
2	2	vllm/model_executor/models/stablelm.py
3	3	vllm/model_executor/models/starcoder2.py
2	2	vllm/model_executor/neuron_model_loader.py
3	6	vllm/model_executor/parallel_utils/communication_op.py
3	2	vllm/model_executor/parallel_utils/custom_all_reduce.py
2	3	vllm/model_executor/sampling_metadata.py
6	6	vllm/model_executor/weight_utils.py
3	3	vllm/outputs.py
3	2	vllm/sequence.py
7	8	vllm/spec_decode/batch_expansion.py
2	2	vllm/spec_decode/interfaces.py
5	4	vllm/spec_decode/metrics.py
2	2	vllm/spec_decode/multi_step_worker.py
9	9	vllm/spec_decode/spec_decode_worker.py
5	3	vllm/spec_decode/util.py
1	1	vllm/transformers_utils/configs/__init__.py
1	0	vllm/transformers_utils/configs/mpt.py
6	4	vllm/transformers_utils/detokenizer.py
1	1	vllm/transformers_utils/tokenizer.py
2	1	vllm/transformers_utils/tokenizer_group/__init__.py
2	2	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
2	2	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
8	13	vllm/utils.py
1	1	vllm/worker/cache_engine.py
7	8	vllm/worker/model_runner.py
4	4	vllm/worker/worker.py

[e67c295b0] TianYu GUO 2024-03-25 [Bugfix] fix automatic prefix args and add log info (#3608)
5	0	vllm/core/block_manager.py
2	1	vllm/engine/arg_utils.py

[925f3332c] Woosuk Kwon 2024-03-24 [Core] Refactor Attention Take 2 (#3462)
1	2	tests/kernels/test_prefix_prefill.py
7	0	tests/samplers/test_beam_search.py
30	30	tests/worker/test_model_runner.py
10	0	vllm/attention/__init__.py
0	0	vllm/{model_executor/layers => }/attention/backends/__init__.py
85	0	vllm/attention/backends/abstract.py
238	0	vllm/attention/backends/flash_attn.py
177	55	vllm/{model_executor/layers => }/attention/backends/xformers.py
46	0	vllm/attention/layer.py
0	0	vllm/{model_executor/layers => }/attention/ops/__init__.py
217	0	vllm/attention/ops/paged_attn.py
0	0	vllm/{model_executor/layers => }/attention/ops/prefix_prefill.py
44	0	vllm/attention/selector.py
0	2	vllm/model_executor/__init__.py
0	99	vllm/model_executor/input_metadata.py
0	5	vllm/model_executor/layers/attention/__init__.py
0	85	vllm/model_executor/layers/attention/attention.py
0	139	vllm/model_executor/layers/attention/backends/flash_attn.py
0	139	vllm/model_executor/layers/attention/ops/paged_attn.py
13	17	vllm/model_executor/models/baichuan.py
14	18	vllm/model_executor/models/bloom.py
18	23	vllm/model_executor/models/chatglm.py
14	18	vllm/model_executor/models/deepseek.py
14	17	vllm/model_executor/models/falcon.py
13	17	vllm/model_executor/models/gemma.py
14	19	vllm/model_executor/models/gpt2.py
14	19	vllm/model_executor/models/gpt_bigcode.py
14	18	vllm/model_executor/models/gpt_j.py
14	18	vllm/model_executor/models/gpt_neox.py
13	17	vllm/model_executor/models/internlm2.py
15	20	vllm/model_executor/models/jais.py
13	17	vllm/model_executor/models/llama.py
14	18	vllm/model_executor/models/mixtral.py
14	18	vllm/model_executor/models/mixtral_quant.py
14	18	vllm/model_executor/models/mpt.py
13	17	vllm/model_executor/models/olmo.py
17	22	vllm/model_executor/models/opt.py
13	17	vllm/model_executor/models/orion.py
14	18	vllm/model_executor/models/phi.py
13	18	vllm/model_executor/models/qwen.py
13	17	vllm/model_executor/models/qwen2.py
13	17	vllm/model_executor/models/stablelm.py
14	18	vllm/model_executor/models/starcoder2.py
1	1	vllm/sequence.py
31	84	vllm/worker/cache_engine.py
44	41	vllm/worker/model_runner.py
3	0	vllm/worker/worker.py

[b0dfa91dd] 少年 2024-03-25 [Model] Add starcoder2 awq support (#3569)
3	2	vllm/model_executor/models/starcoder2.py

[56a8652f3] Woosuk Kwon 2024-03-24 [Bugfix] store lock file in tmp directory (#3578)" (#3599)
15	5	vllm/model_executor/weight_utils.py

[6d93d3530] Kunshang Ji 2024-03-25 [BugFix] tensor.get_device() -> tensor.device (#3604)
2	2	vllm/model_executor/layers/rotary_embedding.py

[837e18514] youkaichao 2024-03-24 [CI/Build] fix flaky test (#3602)
10	16	tests/worker/test_model_runner.py

[42bc38612] youkaichao 2024-03-24 [CI/Build] respect the common environment variable MAX_JOBS (#3600)
9	0	docs/source/getting_started/installation.rst
13	6	setup.py

[8b268a46a] youkaichao 2024-03-24 [CI] typo fix: is_hip --> is_hip() (#3595)
5	5	tests/kernels/test_cache.py

[41deac4a3] Nick Hill 2024-03-24 [BugFix] 1D query fix for MoE models (#3597)
6	4	tests/kernels/test_moe.py
3	4	vllm/model_executor/models/deepseek.py
3	4	vllm/model_executor/models/mixtral.py
3	3	vllm/model_executor/models/mixtral_quant.py

[af9e53496] Woosuk Kwon 2024-03-24 [BugFix] Fix Falcon tied embeddings (#3590)
7	7	vllm/model_executor/models/falcon.py

[f8a12ecc7] Roger Wang 2024-03-24 [Misc] Bump transformers version (#3592)
2	2	requirements-rocm.txt
1	1	requirements.txt

[3c5ab9b81] Woosuk Kwon 2024-03-23 [Misc] Fix BLOOM copyright notice (#3591)
1	1	vllm/model_executor/models/bloom.py

[743a0b740] kota-iizuka 2024-03-24 [Bugfix] use SoftLockFile instead of LockFile (#3578)
1	1	vllm/model_executor/weight_utils.py

[bfdb1ba5c] Antoni Baum 2024-03-22 [Core] Improve detokenization performance for prefill (#3469)
151	12	tests/tokenization/test_detokenize.py
9	57	vllm/engine/llm_engine.py
155	0	vllm/transformers_utils/detokenizer.py
70	20	vllm/transformers_utils/tokenizer.py

[cf2f084d5] Thomas Parnell 2024-03-22 Dynamic scheduler delay to improve ITL performance  (#3279)
34	0	tests/core/test_scheduler.py
4	0	vllm/config.py
25	1	vllm/core/scheduler.py
9	1	vllm/engine/arg_utils.py

[f721096d4] Hanzhi Zhou 2024-03-21 [BugFix] Some fixes for custom allreduce kernels (#2760)
5	5	csrc/custom_all_reduce.cu
75	152	csrc/custom_all_reduce.cuh
108	76	csrc/custom_all_reduce_test.cu
0	9	vllm/config.py
1	1	vllm/entrypoints/llm.py
43	7	vllm/model_executor/parallel_utils/custom_all_reduce.py

[e90fc21f2] Zhuohan Li 2024-03-21 [Hardware][Neuron] Refactor neuron support (#3471)
3	2	examples/offline_inference_neuron.py
1	1	tests/lora/test_worker.py
9	9	tests/spec_decode/test_spec_decode_worker.py
1	1	tests/spec_decode/utils.py
1	1	tests/worker/test_swap.py
2	15	vllm/config.py
6	1	vllm/engine/async_llm_engine.py
5	1	vllm/engine/llm_engine.py
2	16	vllm/executor/gpu_executor.py
80	0	vllm/executor/neuron_executor.py
2	16	vllm/executor/ray_gpu_executor.py
2	2	vllm/lora/layers.py
2	2	vllm/lora/lora.py
2	2	vllm/lora/models.py
1	2	vllm/model_executor/__init__.py
4	3	vllm/model_executor/input_metadata.py
5	6	vllm/model_executor/layers/logits_processor.py
1	1	vllm/model_executor/layers/sampler.py
1	14	vllm/model_executor/models/__init__.py
0	86	vllm/model_executor/models/neuron/llama.py
0	89	vllm/model_executor/models/neuron/mistral.py
89	22	vllm/model_executor/neuron_model_loader.py
4	4	vllm/model_executor/sampling_metadata.py
0	17	vllm/model_executor/utils.py
2	2	vllm/spec_decode/metrics.py
2	2	vllm/spec_decode/multi_step_worker.py
3	3	vllm/spec_decode/spec_decode_worker.py
62	1	vllm/utils.py
2	11	vllm/worker/cache_engine.py
20	62	vllm/worker/model_runner.py
287	0	vllm/worker/neuron_model_runner.py
12	153	vllm/worker/neuron_worker.py
2	2	vllm/worker/worker.py

[ea5f14e6f] Roy 2024-03-22 [Bugfix][Model] Fix Qwen2 (#3554)
1	0	tests/models/test_models.py
1	1	vllm/model_executor/models/qwen2.py

[b7050ca7d] Taemin Lee 2024-03-22 [BugFix] gemma loading after quantization or LoRA. (#3553)
4	0	vllm/model_executor/models/gemma.py

[c188ecb08] Woosuk Kwon 2024-03-21 [Misc] Bump up transformers to v4.39.0 & Remove StarCoder2Config (#3551)
1	1	requirements-rocm.txt
1	1	requirements.txt
1	7	vllm/model_executor/models/starcoder2.py
0	10	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	55	vllm/transformers_utils/configs/starcoder2.py

[865732342] Roy 2024-03-21 [Misc][Log] Add log for tokenizer length not equal to vocabulary size (#3500)
8	0	vllm/engine/llm_engine.py
8	0	vllm/entrypoints/openai/serving_engine.py

[4c07dd28c] Lalit Pradhan 2024-03-21 [🚀 Ready to be merged] Added support for Jais models (#3183)
1	0	README.md
5	1	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
1	2	vllm/model_executor/models/gpt2.py
351	0	vllm/model_executor/models/jais.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
234	0	vllm/transformers_utils/configs/jais.py

[3bbff9e5a] SangBin Cho 2024-03-21 Fix 1D query issue from `_prune_hidden_states` (#3539)
0	1	vllm/model_executor/layers/logits_processor.py

[6ebd02bde] ElizaWszola 2024-03-21 [PREFIX CACHING FOLLOW UP] OrderedDict-based evictor (#3431)
12	13	vllm/core/evictor.py

[523e30ea0] Zhuohan Li 2024-03-20 [BugFix] Hot fix in setup.py for neuron build (#3537)
1	1	setup.py

[f1c0fc391] Roy 2024-03-21 Migrate `logits` computation and gather to `model_runner` (#3233)
3	0	.buildkite/test-pipeline.yaml
5	2	tests/lora/conftest.py
36	30	tests/lora/test_layers.py
20	75	tests/samplers/test_sampler.py
94	0	tests/test_logits_processor.py
12	8	vllm/lora/layers.py
8	5	vllm/lora/models.py
106	0	vllm/model_executor/layers/logits_processor.py
1	80	vllm/model_executor/layers/sampler.py
11	4	vllm/model_executor/models/baichuan.py
11	4	vllm/model_executor/models/bloom.py
11	4	vllm/model_executor/models/chatglm.py
11	4	vllm/model_executor/models/deepseek.py
11	4	vllm/model_executor/models/falcon.py
11	4	vllm/model_executor/models/gemma.py
11	3	vllm/model_executor/models/gpt2.py
11	4	vllm/model_executor/models/gpt_bigcode.py
11	4	vllm/model_executor/models/gpt_j.py
11	4	vllm/model_executor/models/gpt_neox.py
11	4	vllm/model_executor/models/internlm2.py
14	4	vllm/model_executor/models/llama.py
12	4	vllm/model_executor/models/mixtral.py
11	4	vllm/model_executor/models/mixtral_quant.py
11	4	vllm/model_executor/models/mpt.py
11	4	vllm/model_executor/models/neuron/llama.py
11	4	vllm/model_executor/models/neuron/mistral.py
11	4	vllm/model_executor/models/olmo.py
11	4	vllm/model_executor/models/opt.py
11	4	vllm/model_executor/models/orion.py
11	5	vllm/model_executor/models/phi.py
11	4	vllm/model_executor/models/qwen.py
15	9	vllm/model_executor/models/qwen2.py
11	4	vllm/model_executor/models/stablelm.py
12	4	vllm/model_executor/models/starcoder2.py
8	1	vllm/worker/model_runner.py

[6e435de76] SangBin Cho 2024-03-21 [1/n][Chunked Prefill] Refactor input query shapes (#3236)
2	2	.buildkite/test-pipeline.yaml
3	1	tests/basic_correctness/test_basic_correctness.py
9	9	tests/core/test_scheduler.py
1	1	tests/lora/test_worker.py
2	2	tests/spec_decode/test_multi_step_worker.py
153	8	tests/worker/test_model_runner.py
0	3	vllm/config.py
3	10	vllm/core/scheduler.py
1	7	vllm/engine/arg_utils.py
0	1	vllm/engine/llm_engine.py
69	13	vllm/model_executor/input_metadata.py
2	2	vllm/model_executor/layers/activation.py
2	1	vllm/model_executor/layers/attention/attention.py
32	14	vllm/model_executor/layers/attention/backends/flash_attn.py
147	85	vllm/model_executor/layers/attention/backends/xformers.py
5	4	vllm/model_executor/layers/attention/ops/paged_attn.py
0	1	vllm/model_executor/layers/sampler.py
144	95	vllm/worker/model_runner.py

[426ec4ec6] Antoni Baum 2024-03-20 [1/n] Triton sampling kernel (#3186)
51	0	tests/kernels/test_rand.py
196	0	tests/kernels/test_sampler.py
3	3	tests/samplers/test_sampler.py
0	0	vllm/model_executor/layers/ops/__init__.py
157	0	vllm/model_executor/layers/ops/rand.py
405	0	vllm/model_executor/layers/ops/sample.py
103	6	vllm/model_executor/layers/sampler.py
123	6	vllm/model_executor/sampling_metadata.py
3	0	vllm/sequence.py
31	9	vllm/worker/model_runner.py

[80e254834] James Whedbee 2024-03-20 [Bugfix] Fix ROCm support in CMakeLists.txt (#3534)
1	1	CMakeLists.txt

[ba8ae1d84] bnellnm 2024-03-20 Check for _is_cuda() in compute_num_jobs (#3481)
6	6	setup.py

[84eaa6842] Allen.Dou 2024-03-21 Abort when nvcc command is not found in the PATH (#3527)
6	0	CMakeLists.txt

[5ee14494e] Woosuk Kwon 2024-03-20 [Misc] Remove cache stream and cache events (#3461)
77	0	tests/worker/test_swap.py
8	18	vllm/worker/cache_engine.py
1	14	vllm/worker/worker.py

[4ad521d8b] Nick Hill 2024-03-20 [Core] Add generic typing to `LRUCache` (#3511)
3	3	vllm/lora/models.py
13	6	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
2	4	vllm/transformers_utils/tokenizer_group/tokenizer_group.py
11	9	vllm/utils.py

[9474e89ba] ElizaWszola 2024-03-20 [PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)
8	6	tests/core/test_block_manager.py
3	9	tests/prefix_caching/test_prefix_caching.py
146	43	vllm/core/block_manager.py
8	63	vllm/core/evictor.py

[20478c4d3] Simon Mo 2024-03-19 Use lru_cache for some environment detection utils (#3508)
5	1	vllm/utils.py

[63e8b28a9] Jim Burtoft 2024-03-19 [Doc] minor fix of spelling in amd-installation.rst (#3506)
1	1	docs/source/getting_started/amd-installation.rst

[cc63d03fb] Simon Mo 2024-03-19 Revert "[Core] Cache some utils" (#3507)
0	5	vllm/utils.py

[2a60c9bd1] Jim Burtoft 2024-03-19 [Doc] minor fix to neuron-installation.rst (#3505)
1	0	docs/source/getting_started/neuron-installation.rst

[c614cfee5] ifsheldon 2024-03-20 Update dockerfile with ModelScope support (#3429)
1	1	Dockerfile

[7341c77d6] Nick Hill 2024-03-18 [BugFix] Avoid initializing CUDA too early (#3487)
4	4	vllm/config.py

[ef65dcfa6] Simon Mo 2024-03-18 [Doc] Add docs about OpenAI compatible server (#3288)
7	0	docs/requirements-docs.txt
3	1	docs/source/conf.py
4	0	docs/source/dev/sampling_params.rst
4	6	docs/source/index.rst
1	1	docs/source/models/lora.rst
11	0	docs/source/serving/integrations.rst
114	0	docs/source/serving/openai_compatible_server.md
2	106	vllm/entrypoints/openai/api_server.py
118	0	vllm/entrypoints/openai/cli_args.py
119	47	vllm/entrypoints/openai/protocol.py

[6a9c583e7] youkaichao 2024-03-18 [Core] print error before deadlock (#3459)
11	2	vllm/engine/ray_utils.py

[b37cdce2b] Antoni Baum 2024-03-18 [Core] Cache some utils (#3474)
5	0	vllm/utils.py

[b30880a76] Zhuohan Li 2024-03-18 [Misc] Update README for the Third vLLM Meetup (#3479)
9	0	README.md

[49eedea37] Antoni Baum 2024-03-18 [Core] Zero-copy asdict for InputMetadata (#3475)
11	2	vllm/model_executor/input_metadata.py
1	2	vllm/worker/model_runner.py

[9fdf3de34] bnellnm 2024-03-18 Cmake based build system (#2830)
279	0	CMakeLists.txt
2	0	Dockerfile
2	0	MANIFEST.in
73	0	cmake/hipify.py
334	0	cmake/utils.cmake
1	0	pyproject.toml
2	1	requirements-build.txt
1	0	requirements-rocm.txt
1	0	requirements.txt
173	301	setup.py

[c0c17d489] Zhuohan Li 2024-03-18 [Misc] Fix PR Template (#3478)
9	5	.github/PULL_REQUEST_TEMPLATE.md

[097aa0ea2] Robert Shaw 2024-03-18 [CI/Build] Fix Bad Import In Test (#3473)
1	1	tests/test_cache_block_hashing.py

[482b0adf1] Cade Daniel 2024-03-18 [Testing] Add test_config.py to CI (#3437)
1	1	.buildkite/test-pipeline.yaml

[8c654c045] Simon Mo 2024-03-18 CI: Add ROCm Docker Build (#2886)
38	0	.buildkite/run-amd-test.sh
5	0	.buildkite/test-template.j2
1	0	requirements-rocm.txt

[9101d832e] Woosuk Kwon 2024-03-18 [Bugfix] Make moe_align_block_size AMD-compatible (#3470)
2	1	csrc/moe_align_block_size_kernels.cu

[93348d945] Simon Mo 2024-03-17 [CI] Shard tests for LoRA and Kernels to speed up (#3445)
6	5	.buildkite/test-pipeline.yaml
3	0	.buildkite/test-template.j2
1	0	requirements-dev.txt

[abfc4f338] Woosuk Kwon 2024-03-17 [Misc] Use dataclass for InputMetadata (#3452)
0	1	setup.py
14	35	vllm/model_executor/input_metadata.py
10	27	vllm/worker/model_runner.py

[6b78837b2] Simon Mo 2024-03-16 Fix setup.py neuron-ls issue (#2671)
24	19	setup.py

[120157fd2] Simon Mo 2024-03-16 Support arbitrary json_object in OpenAI and Context Free Grammar (#3211)
50	0	tests/entrypoints/test_openai_server.py
9	0	vllm/entrypoints/openai/protocol.py
41	13	vllm/model_executor/guided_decoding.py
76	36	vllm/model_executor/guided_logits_processors.py

[8e67598aa] Simon Mo 2024-03-16 [Misc] fix line length for entire codebase (#3444)
1	1	.github/workflows/ruff.yml
5	3	benchmarks/backend_request_func.py
1	1	benchmarks/benchmark_prefix_caching.py
4	2	benchmarks/benchmark_serving.py
106	75	collect_env.py
1	1	csrc/punica/bgmv/generator.py
37	32	examples/multilora_inference.py
4	3	examples/offline_inference_with_prefix.py
15	10	setup.py

[ad50bf4b2] simon-mo 2024-03-15 fix lint
1	1	.github/workflows/ruff.yml
2	2	tests/kernels/test_prefix_prefill.py

[cf6ff1824] Dinghow Yang 2024-03-16 Fix Baichuan chat template (#3340)
10	19	examples/template_baichuan.jinja

[14e3f9a1b] Ronen Schaffer 2024-03-16 Replace `lstrip()` with `removeprefix()` to fix Ruff linter warning (#2958)
11	4	benchmarks/backend_request_func.py
0	2	pyproject.toml

[3123f1513] Tao He 2024-03-16 Fixes the incorrect argument in the prefix-prefill test cases (#3246)
9	2	tests/kernels/test_prefix_prefill.py

[413366e9a] youkaichao 2024-03-15 [Misc] PR templates (#3413)
60	0	.github/PULL_REQUEST_TEMPLATE.md
2	24	CONTRIBUTING.md

[10585e035] Robert Shaw 2024-03-15 Removed Extraneous Print Message From OAI Server (#3440)
0	3	vllm/entrypoints/openai/serving_completion.py

[fb96c1e98] Antoni Baum 2024-03-15 Asynchronous tokenization (#2879)
1	1	.buildkite/test-pipeline.yaml
7	9	tests/async_engine/test_api_server.py
11	0	tests/conftest.py
0	69	tests/lora/test_tokenizer.py
53	0	tests/lora/test_tokenizer_group.py
0	0	tests/tokenization/__init__.py
20	0	tests/tokenization/test_cached_tokenizer.py
0	0	tests/{engine => tokenization}/test_detokenize.py
100	0	tests/tokenization/test_tokenizer_group.py
57	0	vllm/config.py
34	9	vllm/engine/arg_utils.py
11	4	vllm/engine/llm_engine.py
38	61	vllm/transformers_utils/tokenizer.py
32	0	vllm/transformers_utils/tokenizer_group/__init__.py
48	0	vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py
166	0	vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py
80	0	vllm/transformers_utils/tokenizer_group/tokenizer_group.py

[8fa7357f2] laneeee 2024-03-16 fix document error for value and v_vec illustration (#3421)
-	-	docs/source/assets/kernel/v_vec.png
-	-	docs/source/assets/kernel/value.png
1	1	docs/source/dev/kernel/paged_attention.rst

[a7af4538c] Harry Mellor 2024-03-15 Fix issue templates (#3436)
1	1	.github/ISSUE_TEMPLATE/100-documentation.yml
1	1	.github/ISSUE_TEMPLATE/500-feature request.yml

[604f23593] youkaichao 2024-03-15 [Misc] add error message in non linux platform (#3438)
8	1	setup.py

[14b8ae02e] Tao He 2024-03-16 Fixes the misuse/mixuse of time.time()/time.monotonic() (#3220)
1	1	vllm/core/scheduler.py
1	2	vllm/engine/async_llm_engine.py
2	2	vllm/engine/llm_engine.py
2	2	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py

[03d37f244] Dan Clark 2024-03-15 [Fix] Add args for mTLS support (#3430)
7	4	vllm/entrypoints/api_server.py
14	1	vllm/entrypoints/openai/api_server.py

[a7c871680] Yang Fan 2024-03-16 Fix tie_word_embeddings for Qwen2. (#3344)
12	2	vllm/model_executor/models/qwen2.py

[429284dc3] Junda Chen 2024-03-14 Fix `dist.broadcast` stall without group argument (#3408)
1	1	vllm/model_executor/parallel_utils/communication_op.py

[253a98078] Dinghow Yang 2024-03-15 Add chat templates for ChatGLM (#3418)
18	0	examples/template_chatglm.jinja
18	0	examples/template_chatglm2.jinja

[21539e685] Dinghow Yang 2024-03-15 Add chat templates for Falcon (#3420)
15	0	examples/template_falcon.jinja
17	0	examples/template_falcon_180b.jinja

[b522c4476] youkaichao 2024-03-14 [Misc] add HOST_IP env var (#3419)
23	3	vllm/utils.py

[78b6c4845] akhoroshev 2024-03-15 Dynamically configure shared memory size for moe_align_block_size_kernel (#3376)
29	13	csrc/moe_align_block_size_kernels.cu

[b983ba35b] Enrique Shockwave 2024-03-14 fix marlin config repr (#3414)
1	1	vllm/model_executor/layers/quantization/marlin.py

[54be8a0be] 陈序 2024-03-15 Fix assertion failure in Qwen 1.5 with prefix caching enabled (#3373)
43	0	tests/test_config.py
12	2	vllm/config.py

[dfc77408b] youkaichao 2024-03-14 [issue templates] add some issue templates (#3412)
22	0	.github/ISSUE_TEMPLATE/100-documentation.yml
39	0	.github/ISSUE_TEMPLATE/200-installation.yml
37	0	.github/ISSUE_TEMPLATE/300-usage.yml
81	0	.github/ISSUE_TEMPLATE/400-bug report.yml
31	0	.github/ISSUE_TEMPLATE/500-feature request.yml
33	0	.github/ISSUE_TEMPLATE/600-new model.yml
51	0	.github/ISSUE_TEMPLATE/700-performance discussion.yml
21	0	.github/ISSUE_TEMPLATE/800-misc discussion.yml
1	0	.github/ISSUE_TEMPLATE/config.yml
1	0	.yapfignore
688	0	collect_env.py

[c17ca8ef1] Dan Clark 2024-03-14 Add args for mTLS support (#3410)
11	1	vllm/entrypoints/api_server.py

[06ec48679] Thomas Parnell 2024-03-14 Install `flash_attn` in Docker image (#3396)
24	0	Dockerfile

[8fe838659] youkaichao 2024-03-14 [Kernel] change benchmark script so that result can be directly used; tune moe kernel in A100/H100 with tp=2,4,8 (#3389)
19	11	benchmarks/kernels/benchmark_mixtral_moe.py
5	1	vllm/model_executor/layers/fused_moe/__init__.py
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=NVIDIA_H100_80GB_HBM3.json
144	18	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_H100_80GB_HBM3.json
146	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_A100-SXM4-80GB.json
144	22	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json
7	3	vllm/model_executor/layers/fused_moe/fused_moe.py

[a37415c31] Allen.Dou 2024-03-14 allow user to chose which vllm's merics to display in grafana (#3393)
88	96	examples/production_monitoring/grafana.json

[81653d968] Simon Mo 2024-03-13 [Hotfix] [Debug] test_openai_server.py::test_guided_regex_completion (#3383)
1	1	.buildkite/test-pipeline.yaml
1	1	requirements.txt

[eeab52a4f] Zhuohan Li 2024-03-13 [FIX] Simpler fix for async engine running on ray (#3371)
1	2	vllm/executor/ray_gpu_executor.py

[c33afd89f] Antoni Baum 2024-03-13 Fix lint (#3388)
2	2	vllm/model_executor/layers/rotary_embedding.py

[7e9bd08f6] Terry 2024-03-13 Add batched RoPE kernel (#3095)
120	0	benchmarks/kernels/benchmark_rope.py
10	0	csrc/ops.h
111	15	csrc/pos_encoding_kernels.cu
5	0	csrc/pybind.cpp
134	1	tests/kernels/test_pos_encoding.py
37	21	vllm/model_executor/layers/rotary_embedding.py

[ae0ccb401] Or Sharir 2024-03-13 Add missing kernel for CodeLlama-34B on A/H100 (no tensor parallelism) when using Multi-LoRA. (#3350)
1	0	csrc/punica/bgmv/bgmv_config.h
1	1	tests/lora/test_punica.py

[739c350c1] 陈序 2024-03-14 [Minor Fix] Use cupy-cuda11x in CUDA 11.8 build (#3256)
6	0	setup.py

[ba8dc958a] Hui Liu 2024-03-13 [Minor] Fix bias in if to remove ambiguity (#3259)
1	1	vllm/model_executor/layers/linear.py

[e221910e7] Ronan McGovern 2024-03-13 add hf_transfer to requirements.txt (#3031)
1	1	Dockerfile

[b167109ba] Bo-Wen Wang 2024-03-13 [Fix] Fix quantization="gptq" when using Marlin (#3319)
6	1	vllm/config.py

[602358f8a] Woosuk Kwon 2024-03-12 Add kernel for GeGLU with approximate GELU (#3337)
21	1	csrc/activation_kernels.cu
4	0	csrc/ops.h
5	1	csrc/pybind.cpp
8	3	tests/kernels/test_activation.py
11	2	vllm/model_executor/layers/activation.py

[49a3c8662] Breno Faria 2024-03-13 Fixes #1556 double free (#3347)
87	0	tests/core/test_block_manager.py
15	2	vllm/core/block_manager.py

[b0925b387] Sherlock Xu 2024-03-13 docs: Add BentoML deployment doc (#3336)
1	0	docs/source/index.rst
8	0	docs/source/serving/deploying_with_bentoml.rst

[654865e21] DAIZHENWEI 2024-03-11 Support Mistral Model Inference with transformers-neuronx (#3153)
6	4	examples/offline_inference_neuron.py
5	2	vllm/model_executor/models/__init__.py
82	0	vllm/model_executor/models/neuron/mistral.py

[c9415c19d] kliuae 2024-03-12 [ROCm] Fix warp and lane calculation in blockReduceSum (#3321)
12	2	csrc/reduction_utils.cuh

[4c922709b] Zhuohan Li 2024-03-11 Add distributed model executor abstraction (#3191)
1	1	docs/source/dev/engine/llm_engine.rst
6	2	format.sh
2	1	tests/lora/conftest.py
2	2	vllm/__init__.py
6	1	vllm/config.py
38	68	vllm/engine/async_llm_engine.py
44	402	vllm/engine/llm_engine.py
26	32	vllm/engine/ray_utils.py
0	0	vllm/executor/__init__.py
75	0	vllm/executor/executor_base.py
163	0	vllm/executor/gpu_executor.py
442	0	vllm/executor/ray_gpu_executor.py
13	0	vllm/executor/utils.py

[657061fdc] Philipp Moritz 2024-03-11 [docs] Add LoRA support information for models (#3299)
2	1	docs/source/models/lora.rst
26	1	docs/source/models/supported_models.rst

[2f8844ba0] Zhuohan Li 2024-03-10 Re-enable the 80 char line width limit (#3305)
4	2	pyproject.toml
2	2	setup.py
4	2	tests/async_engine/test_chat_template.py
2	1	tests/core/test_block_manager.py
2	2	tests/entrypoints/test_guided_processors.py
23	13	tests/entrypoints/test_openai_server.py
2	1	tests/kernels/test_moe.py
2	1	tests/kernels/test_prefix_prefill.py
3	3	tests/lora/test_layer_variation.py
9	6	tests/lora/test_layers.py
25	22	tests/lora/test_llama.py
6	6	tests/lora/test_mixtral.py
8	6	tests/metrics/test_metrics.py
8	7	tests/models/test_marlin.py
10	5	tests/prefix_caching/test_prefix_caching.py
2	2	tests/samplers/test_logprobs.py
10	7	tests/samplers/test_sampler.py
4	2	tests/spec_decode/test_metrics.py
2	1	tests/spec_decode/test_multi_step_worker.py
12	6	tests/spec_decode/test_spec_decode_worker.py
8	6	vllm/config.py
10	5	vllm/core/block_manager.py
3	3	vllm/core/evictor.py
4	4	vllm/core/scheduler.py
15	12	vllm/engine/llm_engine.py
13	9	vllm/engine/metrics.py
5	3	vllm/entrypoints/api_server.py
16	17	vllm/entrypoints/openai/api_server.py
15	10	vllm/entrypoints/openai/serving_chat.py
17	11	vllm/entrypoints/openai/serving_completion.py
8	5	vllm/entrypoints/openai/serving_engine.py
8	6	vllm/lora/layers.py
2	1	vllm/lora/models.py
3	4	vllm/lora/worker_manager.py
4	2	vllm/model_executor/guided_decoding.py
9	6	vllm/model_executor/guided_logits_processors.py
2	2	vllm/model_executor/layers/attention/attention.py
70	37	vllm/model_executor/layers/fused_moe/fused_moe.py
8	4	vllm/model_executor/layers/linear.py
2	1	vllm/model_executor/layers/quantization/__init__.py
4	2	vllm/model_executor/layers/quantization/awq.py
6	4	vllm/model_executor/layers/quantization/gptq.py
24	15	vllm/model_executor/layers/quantization/marlin.py
2	1	vllm/model_executor/layers/quantization/squeezellm.py
2	1	vllm/model_executor/layers/sampler.py
2	1	vllm/model_executor/models/baichuan.py
5	3	vllm/model_executor/models/deepseek.py
2	1	vllm/model_executor/models/gpt_j.py
2	1	vllm/model_executor/models/internlm2.py
12	7	vllm/model_executor/models/olmo.py
2	1	vllm/model_executor/models/qwen2.py
7	6	vllm/model_executor/models/stablelm.py
2	1	vllm/model_executor/models/starcoder2.py
2	1	vllm/model_executor/neuron_model_loader.py
3	2	vllm/model_executor/parallel_utils/communication_op.py
2	1	vllm/model_executor/sampling_metadata.py
2	2	vllm/sampling_params.py
2	1	vllm/sequence.py
18	11	vllm/spec_decode/batch_expansion.py
9	5	vllm/spec_decode/multi_step_worker.py
11	8	vllm/spec_decode/spec_decode_worker.py
17	72	vllm/transformers_utils/configs/mpt.py
0	72	vllm/transformers_utils/configs/starcoder2.py
42	50	vllm/transformers_utils/tokenizers/baichuan.py
6	6	vllm/utils.py
5	6	vllm/worker/model_runner.py
4	2	vllm/worker/neuron_worker.py

[4b59f00e9] Nick Hill 2024-03-10 [Fix] Fix best_of behavior when n=1 (#3298)
3	3	vllm/outputs.py

[9e8744a54] Roy 2024-03-11 [BugFix] Fix get tokenizer when using ray (#3301)
3	0	tests/async_engine/test_async_llm_engine.py
7	2	vllm/engine/async_llm_engine.py
7	1	vllm/engine/llm_engine.py
1	1	vllm/entrypoints/openai/serving_chat.py
1	1	vllm/entrypoints/openai/serving_completion.py
4	2	vllm/transformers_utils/tokenizer.py

[e4a28e531] Douglas Lehr 2024-03-10 [ROCM] Fix blockReduceSum to use correct warp counts for ROCm and CUDA (#3262)
0	8	csrc/attention/attention_kernels.cu
10	0	csrc/cuda_compat.h
3	3	csrc/reduction_utils.cuh

[0bba88df0] Terry 2024-03-09 Enhance lora tests with more layer and rank variations (#3243)
1	0	csrc/punica/bgmv/bgmv_config.h
1	0	requirements-dev.txt
104	0	tests/lora/test_layer_variation.py

[8437bae6e] Cade Daniel 2024-03-08 [Speculative decoding 3/9] Worker which speculates, scores, and applies rejection sampling (#3103)
4	1	.buildkite/test-pipeline.yaml
0	0	tests/{worker => }/spec_decode/__init__.py
95	0	tests/spec_decode/test_batch_expansion.py
157	0	tests/spec_decode/test_metrics.py
160	2	tests/{worker => }/spec_decode/test_multi_step_worker.py
591	0	tests/spec_decode/test_spec_decode_worker.py
111	0	tests/spec_decode/test_utils.py
96	19	tests/{worker => }/spec_decode/utils.py
50	0	tests/test_sequence.py
8	2	vllm/model_executor/layers/rejection_sampler.py
1	1	vllm/model_executor/layers/sampler.py
50	5	vllm/sequence.py
351	0	vllm/spec_decode/batch_expansion.py
77	0	vllm/spec_decode/interfaces.py
174	0	vllm/spec_decode/metrics.py
366	0	vllm/spec_decode/multi_step_worker.py
372	0	vllm/spec_decode/spec_decode_worker.py
99	0	vllm/spec_decode/util.py
6	5	vllm/worker/model_runner.py
0	178	vllm/worker/spec_decode/multi_step_worker.py
18	2	vllm/worker/worker.py

[f48c6791b] Zhuohan Li 2024-03-08 [FIX] Fix prefix test error on main (#3286)
0	2	vllm/model_executor/layers/attention/backends/flash_attn.py

[c2c5e0909] Michael Goin 2024-03-08 Move model filelocks from `/tmp/` to `~/.cache/vllm/locks/` dir (#3241)
5	1	vllm/model_executor/weight_utils.py

[1cb0cc297] Woosuk Kwon 2024-03-08 [FIX] Make `flash_attn` optional (#3269)
0	3	.gitignore
3	45	setup.py
7	23	vllm/__init__.py
31	6	vllm/model_executor/layers/attention/attention.py
0	1	vllm/model_executor/layers/attention/backends/flash_attn.py

[99c3cfb83] Roger Wang 2024-03-08 [Docs] Fix Unmocked Imports (#3275)
9	2	docs/source/conf.py

[1ece1ae82] TianYu GUO 2024-03-08 [Minor Fix] Fix comments in benchmark_serving (#3252)
3	3	benchmarks/benchmark_serving.py

[c59e120c5] whyiug 2024-03-08 Feature add lora support for Qwen2 (#3177)
2	0	csrc/punica/bgmv/bgmv_config.h
24	0	vllm/model_executor/models/qwen2.py

[d2339d684] Nick Hill 2024-03-07 Connect engine healthcheck to openai server (#3260)
1	0	vllm/entrypoints/openai/api_server.py

[b35cc9342] ElizaWszola 2024-03-08 Fix auto prefix bug (#3239)
34	0	tests/engine/test_computed_prefix_blocks.py
16	12	vllm/core/block_manager.py
1	0	vllm/worker/model_runner.py

[8cbba4622] jacobthebanana 2024-03-07 Possible fix for conflict between Automated Prefix Caching (#2762) and multi-LoRA support (#1804) (#3263)
31	15	tests/test_cache_block_hashing.py
2	1	vllm/sequence.py

[385da2dae] Michael Goin 2024-03-07 Measure model memory usage (#3120)
25	0	vllm/utils.py
12	6	vllm/worker/model_runner.py

[2daf23ab0] Woosuk Kwon 2024-03-07 Separate attention backends (#3005)
3	0	.gitignore
45	3	setup.py
1	1	tests/kernels/test_prefix_prefill.py
23	7	vllm/__init__.py
5	0	vllm/model_executor/layers/attention/__init__.py
59	0	vllm/model_executor/layers/attention/attention.py
0	0	vllm/model_executor/layers/{triton_kernel => attention/backends}/__init__.py
124	0	vllm/model_executor/layers/attention/backends/flash_attn.py
61	155	vllm/model_executor/layers/{attention.py => attention/backends/xformers.py}
0	0	vllm/model_executor/layers/attention/ops/__init__.py
138	0	vllm/model_executor/layers/attention/ops/paged_attn.py
0	0	vllm/model_executor/layers/{triton_kernel => attention/ops}/prefix_prefill.py
6	7	vllm/model_executor/models/baichuan.py
5	5	vllm/model_executor/models/bloom.py
2	2	vllm/model_executor/models/chatglm.py
5	5	vllm/model_executor/models/deepseek.py
14	14	vllm/model_executor/models/falcon.py
5	5	vllm/model_executor/models/gemma.py
2	4	vllm/model_executor/models/gpt2.py
5	5	vllm/model_executor/models/gpt_bigcode.py
2	2	vllm/model_executor/models/gpt_j.py
2	2	vllm/model_executor/models/gpt_neox.py
5	5	vllm/model_executor/models/internlm2.py
6	6	vllm/model_executor/models/llama.py
2	2	vllm/model_executor/models/mixtral.py
2	2	vllm/model_executor/models/mixtral_quant.py
6	6	vllm/model_executor/models/mpt.py
4	4	vllm/model_executor/models/olmo.py
4	4	vllm/model_executor/models/opt.py
5	5	vllm/model_executor/models/orion.py
2	2	vllm/model_executor/models/phi.py
2	2	vllm/model_executor/models/qwen.py
6	6	vllm/model_executor/models/qwen2.py
5	5	vllm/model_executor/models/stablelm.py
2	2	vllm/model_executor/models/starcoder2.py

[cbf4c05b1] Chen Wang 2024-03-07 Update requirements-dev.txt to include package for benchmarking scripts. (#3181)
3	0	requirements-dev.txt

[d3c04b6a3] TechxGenus 2024-03-07 Add GPTQ support for Gemma (#3200)
6	0	vllm/model_executor/models/gemma.py

[4cb3b924c] Chujie Zheng 2024-03-06 Add tqdm `dynamic_ncols=True` (#3242)
3	1	vllm/entrypoints/llm.py

[a33ce60c6] Cade Daniel 2024-03-06 [Testing] Fix core tests (#3224)
31	18	tests/core/test_block_manager.py
3	3	tests/core/test_scheduler.py
1	1	tests/core/utils.py
1	1	vllm/sequence.py

[24aecf421] SangBin Cho 2024-03-06 [Tests] Add block manager and scheduler tests (#3108)
3	0	.buildkite/test-pipeline.yaml
0	0	tests/core/__init__.py
262	0	tests/core/test_block_manager.py
170	0	tests/core/test_scheduler.py
27	0	tests/core/utils.py

[2efce05dc] Nick Hill 2024-03-05 [Fix] Avoid pickling entire LLMEngine for Ray workers (#3207)
14	7	vllm/engine/llm_engine.py

[8999ec3c1] Nick Hill 2024-03-05 Store `eos_token_id` in `Sequence` for easy access (#3166)
2	1	tests/test_cache_block_hashing.py
3	4	vllm/core/scheduler.py
13	17	vllm/engine/llm_engine.py
0	1	vllm/model_executor/layers/sampler.py
21	20	vllm/outputs.py
5	6	vllm/sequence.py

[05af6da8d] Hongxia Yang 2024-03-04 [ROCm] enable cupy in order to enable  cudagraph mode for AMD GPUs (#3123)
25	5	Dockerfile.rocm
1	3	vllm/worker/worker.py

[9a4548bae] Chen Wang 2024-03-04 Fix the openai benchmarking requests to work with latest OpenAI apis (#2992)
70	0	benchmarks/backend_request_func.py

[ff578cae5] Antoni Baum 2024-03-04 Add health check, make async Engine more robust (#3015)
16	16	tests/async_engine/test_async_llm_engine.py
15	23	tests/async_engine/test_request_tracker.py
87	26	vllm/engine/async_llm_engine.py
20	0	vllm/engine/llm_engine.py

[22de45235] Antoni Baum 2024-03-04 Push logprob generation to LLMEngine (#3065)
58	3	tests/entrypoints/test_openai_server.py
36	6	tests/samplers/test_logprobs.py
7	5	tests/worker/spec_decode/utils.py
2	0	vllm/config.py
9	1	vllm/engine/arg_utils.py
24	5	vllm/engine/async_llm_engine.py
40	2	vllm/engine/llm_engine.py
128	108	vllm/entrypoints/openai/serving_chat.py
203	188	vllm/entrypoints/openai/serving_completion.py
19	4	vllm/entrypoints/openai/serving_engine.py
11	4	vllm/model_executor/layers/sampler.py
17	8	vllm/sequence.py
1	1	vllm/worker/spec_decode/multi_step_worker.py

[76e8a7047] ttbachyinsda 2024-03-05 [Minor fix] The domain dns.google may cause a socket.gaierror exception (#3176)
1	1	vllm/utils.py

[9cbc7e5f3] Allen.Dou 2024-03-05 enable --gpu-memory-utilization in benchmark_throughput.py (#3175)
14	7	benchmarks/benchmark_throughput.py

[27a7b070d] Jialun Lyu 2024-03-04 Add document for vllm paged attention kernel. (#2978)
-	-	docs/source/assets/kernel/k_vecs.png
-	-	docs/source/assets/kernel/key.png
-	-	docs/source/assets/kernel/logits_vec.png
-	-	docs/source/assets/kernel/q_vecs.png
-	-	docs/source/assets/kernel/query.png
-	-	docs/source/assets/kernel/v_vec.png
-	-	docs/source/assets/kernel/value.png
525	0	docs/source/dev/kernel/paged_attention.rst
1	0	docs/source/index.rst

[901cf4c52] TianYu GUO 2024-03-04 [Minor Fix] Remove unused code in benchmark_prefix_caching.py (#3171)
3	9	benchmarks/benchmark_prefix_caching.py

[d0fae8811] Liangfu Chen 2024-03-03 [DOC] add setup document to support neuron backend (#2777)
135	0	docs/source/getting_started/neuron-installation.rst
1	0	docs/source/index.rst

[17c3103c5] Philipp Moritz 2024-03-03 Make it easy to profile workers with nsight (#3162)
6	0	benchmarks/benchmark_latency.py
7	0	vllm/config.py
7	1	vllm/engine/arg_utils.py
14	1	vllm/engine/llm_engine.py

[996d095c5] Zhuohan Li 2024-03-03 [FIX] Fix styles in automatic prefix caching & add a automatic prefix caching benchmark (#3158)
59	0	benchmarks/benchmark_prefix_caching.py
4	1	benchmarks/benchmark_throughput.py
4	11	vllm/core/block_manager.py
2	6	vllm/sequence.py

[d65fac273] Jason Cox 2024-03-03 Add vLLM version info to logs and openai API server (#3161)
2	1	vllm/engine/llm_engine.py
8	0	vllm/entrypoints/openai/api_server.py

[ce4f5a29f] Sage Moore 2024-03-02 Add Automatic Prefix Caching (#2762)
16	14	benchmarks/benchmark_throughput.py
4	0	docs/source/models/engine_args.rst
2	9	examples/offline_inference_with_prefix.py
69	34	tests/prefix_caching/test_prefix_caching.py
76	0	tests/test_cache_block_hashing.py
13	1	vllm/block.py
2	0	vllm/config.py
213	72	vllm/core/block_manager.py
161	0	vllm/core/evictor.py
9	6	vllm/core/scheduler.py
8	1	vllm/engine/arg_utils.py
1	13	vllm/engine/async_llm_engine.py
8	18	vllm/engine/llm_engine.py
1	5	vllm/entrypoints/api_server.py
2	12	vllm/entrypoints/llm.py
0	87	vllm/prefix.py
16	7	vllm/sequence.py
17	13	vllm/worker/model_runner.py

[baee28c46] cloudhan 2024-03-02 Reorder kv dtype check to avoid nvcc not found error on AMD platform (#3104)
3	4	vllm/config.py

[29e70e3e8] Allen.Dou 2024-03-02 allow user chose log level by --log-level instead of fixed 'info'. (#3109)
1	1	vllm/config.py
1	0	vllm/engine/metrics.py
7	1	vllm/entrypoints/openai/api_server.py

[82091b864] Woosuk Kwon 2024-03-01 Bump up to v0.3.3 (#3129)
1	1	vllm/__init__.py

[c0c2335ce] Robert Shaw 2024-03-01 Integrate Marlin Kernels for Int4 GPTQ inference (#2497)
9	0	csrc/ops.h
3	1	csrc/pybind.cpp
209	0	csrc/quantization/marlin/LICENSE
1145	0	csrc/quantization/marlin/marlin_cuda_kernel.cu
1	0	requirements-dev.txt
2	0	setup.py
32	0	tests/conftest.py
97	0	tests/models/test_marlin.py
13	5	vllm/config.py
29	0	vllm/model_executor/layers/linear.py
2	0	vllm/model_executor/layers/quantization/__init__.py
210	0	vllm/model_executor/layers/quantization/marlin.py

[90fbf1254] Huarong 2024-03-02 fix relative import path of protocol.py (#3134)
1	1	vllm/entrypoints/openai/serving_completion.py

[49d849b3a] Yuan Tang 2024-03-01 docs: Add tutorial on deploying vLLM model with KServe (#2586)
1	0	docs/source/index.rst
8	0	docs/source/serving/deploying_with_kserve.rst

[27ca23dc0] Seonghyeon 2024-03-02 Remove exclude_unset in streaming response (#3143)
3	3	vllm/entrypoints/openai/serving_completion.py

[54d354478] Sherry 2024-03-01 Fix: Output text is always truncated in some models (#3016)
4	1	vllm/engine/llm_engine.py

[703e42ee4] felixzhu555 2024-02-29 Add guided decoding for OpenAI API server (#2819)
1	0	requirements.txt
75	0	tests/entrypoints/test_guided_processors.py
237	0	tests/entrypoints/test_openai_server.py
3	0	vllm/engine/async_llm_engine.py
35	1	vllm/entrypoints/openai/protocol.py
9	0	vllm/entrypoints/openai/serving_chat.py
9	0	vllm/entrypoints/openai/serving_completion.py
99	0	vllm/model_executor/guided_decoding.py
129	0	vllm/model_executor/guided_logits_processors.py

[29a8d6a55] Nick Hill 2024-02-29 [Fix] Don't deep-copy LogitsProcessors when copying SamplingParams (#3099)
3	2	vllm/engine/llm_engine.py
15	0	vllm/sampling_params.py

[2c08ff23c] Billy Cao 2024-03-01 Fix building from source on WSL (#3112)
1	1	setup.py

[bfdcfa6a0] Seonghyeon 2024-02-29 Support starcoder2 architecture (#3089)
1	0	README.md
1	0	tests/models/test_models.py
1	0	vllm/model_executor/models/__init__.py
310	0	vllm/model_executor/models/starcoder2.py
10	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
127	0	vllm/transformers_utils/configs/starcoder2.py

[9289e577e] Allen.Dou 2024-02-29 add cache_config's info to prometheus metrics. (#3100)
4	0	vllm/config.py
1	0	vllm/engine/llm_engine.py
9	1	vllm/engine/metrics.py

[a6d471c75] Jae-Won Chung 2024-02-29 Fix: `AttributeError` in OpenAI-compatible server (#3018)
1	1	vllm/entrypoints/openai/protocol.py
1	1	vllm/entrypoints/openai/serving_chat.py

[01a5d18a5] CHU Tianxiang 2024-02-29 Add Support for 2/3/8-bit GPTQ Quantization Models (#2330)
4	2	csrc/ops.h
123	0	csrc/quantization/gptq/matrix_view.cuh
1326	126	csrc/quantization/gptq/q_gemm.cu
87	0	csrc/quantization/gptq/qdq_2.cuh
141	0	csrc/quantization/gptq/qdq_3.cuh
6	94	csrc/quantization/gptq/qdq_4.cuh
40	0	csrc/quantization/gptq/qdq_8.cuh
9	7	vllm/model_executor/layers/quantization/gptq.py

[929b4f297] Woosuk Kwon 2024-02-28 Add LoRA support for Gemma (#3050)
1	1	.buildkite/test-pipeline.yaml
2	0	csrc/punica/bgmv/bgmv_config.h
5	0	tests/lora/conftest.py
46	0	tests/lora/test_gemma.py
2	2	tests/lora/test_punica.py
25	3	vllm/model_executor/models/gemma.py
1	1	vllm/model_executor/models/llama.py

[3b7178cfa] Liangfu Chen 2024-02-28 [Neuron] Support inference with transformers-neuronx (#2569)
33	0	examples/offline_inference_neuron.py
5	3	tests/lora/conftest.py
35	6	vllm/config.py
7	9	vllm/engine/arg_utils.py
18	3	vllm/engine/llm_engine.py
4	0	vllm/lora/layers.py
1	2	vllm/model_executor/__init__.py
13	5	vllm/model_executor/layers/sampler.py
5	5	vllm/model_executor/model_loader.py
11	1	vllm/model_executor/models/__init__.py
79	0	vllm/model_executor/models/neuron/llama.py
66	0	vllm/model_executor/neuron_model_loader.py
2	2	vllm/model_executor/sampling_metadata.py
17	0	vllm/model_executor/utils.py
8	0	vllm/utils.py
9	2	vllm/worker/cache_engine.py
12	4	vllm/worker/model_runner.py
191	0	vllm/worker/neuron_worker.py

[e46fa5d52] Allen.Dou 2024-02-28 Restrict prometheus_client >= 0.18.0 to prevent errors when importing pkgs (#3070)
1	1	requirements-neuron.txt
1	1	requirements-rocm.txt
1	1	requirements.txt

[a8683102c] Ganesh Jagadeesan 2024-02-28 multi-lora documentation fix (#3064)
13	1	docs/source/models/lora.rst

[71bcaf99e] Tao He 2024-02-27 Enable GQA support in the prefix prefill kernels (#3007)
42	19	tests/kernels/test_prefix_prefill.py
18	16	vllm/model_executor/layers/attention.py
27	12	vllm/model_executor/layers/triton_kernel/prefix_prefill.py

[8b430d7de] Woosuk Kwon 2024-02-26 [Minor] Fix StableLMEpochForCausalLM -> StableLmForCausalLM (#3046)
1	1	docs/source/models/supported_models.rst

[e0ade06d6] Dylan Hawk 2024-02-26 Support logit bias for OpenAI API (#3027)
48	0	tests/entrypoints/test_openai_server.py
33	0	vllm/entrypoints/openai/protocol.py
1	7	vllm/entrypoints/openai/serving_chat.py
1	5	vllm/entrypoints/openai/serving_completion.py

[4bd18ec0c] Woosuk Kwon 2024-02-26 [Minor] Fix type annotation in fused moe (#3045)
4	3	vllm/model_executor/layers/fused_moe/fused_moe.py

[2410e320b] Jingru 2024-02-27 fix `get_ip` error in pure ipv6 environment (#2931)
19	5	vllm/utils.py

[48a8f4a7f] 张大成 2024-02-27 Support Orion model (#2539)
1	0	README.md
3	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
322	0	vllm/model_executor/models/orion.py

[4dd6416fa] Roy 2024-02-27 Fix stablelm (#3038)
1	0	vllm/model_executor/models/__init__.py
10	6	vllm/model_executor/models/stablelm.py

[c1c0d00b8] Roy 2024-02-27 Don't use cupy when `enforce_eager=True` (#3037)
4	1	vllm/engine/llm_engine.py

[d9f726c4d] Roy 2024-02-27 [Minor] Remove unused config files (#3039)
3	3	vllm/model_executor/models/baichuan.py
3	1	vllm/model_executor/models/olmo.py
4	4	vllm/model_executor/models/qwen.py
0	2	vllm/transformers_utils/config.py
0	6	vllm/transformers_utils/configs/__init__.py
0	62	vllm/transformers_utils/configs/baichuan.py
0	72	vllm/transformers_utils/configs/olmo.py
0	60	vllm/transformers_utils/configs/qwen.py

[d6e4a130b] Woosuk Kwon 2024-02-26 [Minor] Remove gather_cached_kv kernel (#3043)
0	7	csrc/cache.h
0	161	csrc/cache_kernels.cu
0	4	csrc/pybind.cpp

[cfc15a103] Philipp Moritz 2024-02-26 Optimize Triton MoE Kernel (#2979)
172	0	benchmarks/kernels/benchmark_mixtral_moe.py
3	1	setup.py
5	0	vllm/model_executor/layers/fused_moe/__init__.py
20	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=NVIDIA_A100-SXM4-80GB.json
24	0	vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=NVIDIA_H100_80GB_HBM3.json
10	0	vllm/model_executor/layers/fused_moe/configs/README
63	14	vllm/model_executor/layers/{ => fused_moe}/fused_moe.py

[70f3e8e3a] Jared Moore 2024-02-25 Add LogProbs for Chat Completions in OpenAI (#2918)
13	12	tests/entrypoints/test_openai_server.py
8	0	vllm/entrypoints/openai/protocol.py
36	2	vllm/entrypoints/openai/serving_chat.py

[ef978fe41] Harry Mellor 2024-02-25 Port metrics from `aioprometheus` to `prometheus_client` (#2730)
1	1	docs/source/conf.py
1	1	requirements-neuron.txt
1	1	requirements-rocm.txt
1	1	requirements.txt
2	0	tests/conftest.py
14	11	tests/metrics/test_metrics.py
2	1	vllm/engine/llm_engine.py
107	63	vllm/engine/metrics.py
4	8	vllm/entrypoints/openai/api_server.py

[f7c123499] Woosuk Kwon 2024-02-23 [Fix] Fissertion on YaRN model len (#2984)
0	1	vllm/model_executor/layers/rotary_embedding.py

[57f044945] zhaoyang-star 2024-02-23 Fix nvcc not found in vlm-openai image (#2781)
1	1	vllm/config.py
9	4	vllm/utils.py

[4caf7044e] Ronen Schaffer 2024-02-23 Include tokens from prompt phase in `counter_generation_tokens` (#2802)
3	0	.buildkite/test-pipeline.yaml
33	1	tests/metrics/test_metrics.py
3	0	vllm/engine/llm_engine.py

[6f32cddf1] Woosuk Kwon 2024-02-22 Remove Flash Attention in test env (#2982)
1	2	requirements-dev.txt

[c530e2cfe] 44670 2024-02-22 [FIX] Fix a bug in initializing Yarn RoPE (#2983)
2	4	vllm/model_executor/layers/rotary_embedding.py

[fd5dcc5c8] Woosuk Kwon 2024-02-21 Optimize GeGLU layer in Gemma (#2975)
48	25	csrc/activation_kernels.cu
4	0	csrc/ops.h
4	0	csrc/pybind.cpp
15	35	tests/kernels/test_activation.py
23	0	vllm/model_executor/layers/activation.py
14	17	vllm/model_executor/models/gemma.py

[93dc5a287] Massimiliano Pronesti 2024-02-22 chore(vllm): codespell for spell checking  (#2820)
4	1	.github/workflows/ruff.yml
1	1	benchmarks/benchmark_serving.py
48	3	format.sh
0	8	mypy.ini
18	0	pyproject.toml
2	0	requirements-dev.txt
1	1	tests/lora/test_layers.py
2	2	tests/lora/test_llama.py
1	1	vllm/core/block_manager.py
1	1	vllm/core/scheduler.py
1	1	vllm/lora/punica.py
1	1	vllm/model_executor/layers/triton_kernel/prefix_prefill.py
1	1	vllm/model_executor/models/decilm.py
2	2	vllm/model_executor/parallel_utils/custom_all_reduce.py
1	1	vllm/model_executor/parallel_utils/parallel_state.py
1	1	vllm/utils.py

[95529e325] Woosuk Kwon 2024-02-21 Use Llama RMSNorm custom op for Gemma (#2974)
27	33	vllm/model_executor/models/gemma.py

[344020c92] Roy 2024-02-22 Migrate MistralForCausalLM to LlamaForCausalLM (#2868)
1	1	vllm/model_executor/models/__init__.py
5	1	vllm/model_executor/models/llama.py
0	377	vllm/model_executor/models/mistral.py

[5574081c4] Mustafa Eyceoz 2024-02-21 Added early stopping to completion APIs (#2939)
4	0	vllm/entrypoints/openai/protocol.py

[d7f396486] Ronen Schaffer 2024-02-22 Update comment (#2934)
1	1	benchmarks/benchmark_serving.py

[8fbd84bf7] Zhuohan Li 2024-02-21 Bump up version to v0.3.2 (#2968)
1	1	vllm/__init__.py

[7d2dcce17] Nick Hill 2024-02-21 Support per-request seed (#2514)
147	75	tests/samplers/test_sampler.py
82	0	tests/samplers/test_seeded_generate.py
1	0	vllm/core/scheduler.py
0	1	vllm/engine/arg_utils.py
4	0	vllm/entrypoints/openai/protocol.py
22	7	vllm/model_executor/layers/sampler.py
3	0	vllm/model_executor/sampling_metadata.py
8	1	vllm/sampling_params.py
12	0	vllm/sequence.py
10	0	vllm/worker/model_runner.py

[dc903e70a] Woosuk Kwon 2024-02-21 [ROCm] Upgrade transformers to v4.38.0 (#2967)
1	1	requirements-rocm.txt

[a9c821289] Zhuohan Li 2024-02-21 [FIX] Add Gemma model to the doc (#2966)
1	0	README.md
3	0	docs/source/models/supported_models.rst

[c20ecb6a5] Woosuk Kwon 2024-02-21 Upgrade transformers to v4.38.0 (#2965)
1	1	requirements.txt

[5253edaac] Xiang Xu 2024-02-21 Add Gemma model (#2964)
1	0	vllm/model_executor/models/__init__.py
333	0	vllm/model_executor/models/gemma.py

[017d9f151] Antoni Baum 2024-02-20 Add metrics to RequestOutput (#2876)
1	1	tests/async_engine/test_request_tracker.py
1	1	vllm/core/policy.py
3	0	vllm/core/scheduler.py
5	2	vllm/engine/llm_engine.py
9	1	vllm/outputs.py
42	4	vllm/sequence.py

[181b27d88] Antoni Baum 2024-02-20 Make vLLM logging formatting optional (#2877)
7	3	vllm/logger.py

[63e2a6419] Zhuohan Li 2024-02-20 [FIX] Fix beam search test (#2930)
1	0	tests/samplers/test_beam_search.py

[264017a2b] James Whedbee 2024-02-19 [ROCm] include gfx908 as supported (#2792)
1	1	setup.py

[e433c115b] Ronen Schaffer 2024-02-19 Fix `vllm:prompt_tokens_total` metric calculation (#2869)
5	5	tests/conftest.py
33	0	tests/metrics/test_metrics.py
3	1	vllm/engine/llm_engine.py

[86fd8bb0a] Simon Mo 2024-02-18 Add warning to prevent changes to benchmark api server (#2858)
6	0	vllm/entrypoints/api_server.py

[ab3a5a825] Isotr0py 2024-02-19 Support OLMo models. (#2832)
1	0	README.md
3	0	docs/source/models/supported_models.rst
14	5	tests/models/test_models.py
1	0	vllm/model_executor/models/__init__.py
378	0	vllm/model_executor/models/olmo.py
2	0	vllm/transformers_utils/configs/__init__.py
72	0	vllm/transformers_utils/configs/olmo.py

[a61f0521b] Zhuohan Li 2024-02-18 [Test] Add basic correctness test (#2908)
10	2	.buildkite/test-pipeline.yaml
38	0	tests/basic_correctness/test_basic_correctness.py
2	0	tests/conftest.py
41	0	tests/distributed/test_basic_distributed_correctness.py

[537c9755a] Zhuohan Li 2024-02-18 [Minor] Small fix to make distributed init logic in worker looks cleaner (#2905)
4	2	vllm/worker/worker.py

[786b7f18a] Mark Mozolewski 2024-02-17 Add code-revision config argument for Hugging Face Hub (#2892)
7	1	vllm/config.py
14	7	vllm/engine/arg_utils.py
9	3	vllm/transformers_utils/config.py

[8f36444c4] jvmncs 2024-02-17 multi-LoRA as extra models in OpenAI server (#2775)
40	1	docs/source/models/lora.rst
3	1	examples/multilora_inference.py
74	15	tests/entrypoints/test_openai_server.py
23	1	vllm/entrypoints/openai/api_server.py
9	4	vllm/entrypoints/openai/serving_chat.py
11	4	vllm/entrypoints/openai/serving_completion.py
40	1	vllm/entrypoints/openai/serving_engine.py

[185b2c29e] Nick Hill 2024-02-17 Defensively copy `sampling_params` (#2881)
3	0	vllm/engine/llm_engine.py

[5f08050d8] Woosuk Kwon 2024-02-16 Bump up to v0.3.1 (#2887)
1	1	vllm/__init__.py

[64da65b32] shiyi.c_98 2024-02-16 Prefix Caching- fix t4 triton error (#2517)
3	1	vllm/model_executor/layers/triton_kernel/prefix_prefill.py

[5255d99dc] Hongxia Yang 2024-02-15 [ROCm] Dockerfile fix for flash-attention build (#2885)
3	3	Dockerfile.rocm

[4f2ad1113] Philipp Moritz 2024-02-14 Fix DeciLM (#2883)
5	1	vllm/model_executor/models/decilm.py

[d7afab6d3] Woosuk Kwon 2024-02-14 [BugFix] Fix GC bug for `LLM` class (#2882)
18	0	tests/test_regression.py
164	170	vllm/lora/punica.py

[31348dff0] Philipp Moritz 2024-02-14 Align LoRA code between Mistral and Mixtral (fixes #2875) (#2880)
10	4	vllm/model_executor/models/mixtral.py

[25e86b6a6] Woosuk Kwon 2024-02-14 Don't use cupy NCCL for AMD backends (#2855)
4	0	vllm/model_executor/parallel_utils/custom_all_reduce.py
16	6	vllm/worker/model_runner.py
3	1	vllm/worker/worker.py

[4efbac6d3] Roy 2024-02-15 Migrate AquilaForCausalLM to LlamaForCausalLM (#2867)
2	3	vllm/model_executor/models/__init__.py
0	342	vllm/model_executor/models/aquila.py
0	1	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	69	vllm/transformers_utils/configs/aquila.py

[87069ccf6] Nikola Borisov 2024-02-14 Fix docker python version (#2845)
5	16	Dockerfile
1	1	requirements.txt

[7e45107f5] Woosuk Kwon 2024-02-13 [Fix] Fix memory profiling when GPU is used by multiple processes (#2863)
5	1	vllm/worker/worker.py

[0c48b37c3] Philipp Moritz 2024-02-13 Fix internlm after https://github.com/vllm-project/vllm/pull/2860 (#2861)
2	1	vllm/model_executor/models/llama.py

[7eacffd95] Philipp Moritz 2024-02-13 Migrate InternLMForCausalLM to LlamaForCausalLM (#2860)
1	1	vllm/model_executor/models/__init__.py
0	299	vllm/model_executor/models/internlm.py
4	2	vllm/model_executor/models/llama.py

[2a543d6ef] Terry 2024-02-13 Add LoRA support for Mixtral (#2831)
5	0	tests/lora/conftest.py
47	35	tests/lora/test_lora_manager.py
53	0	tests/lora/test_mixtral.py
31	65	vllm/lora/models.py
11	10	vllm/lora/worker_manager.py
1	1	vllm/model_executor/model_loader.py
30	5	vllm/model_executor/models/llama.py
26	1	vllm/model_executor/models/mistral.py
37	3	vllm/model_executor/models/mixtral.py
10	1	vllm/worker/model_runner.py

[317b29de0] Philipp Moritz 2024-02-13 Remove Yi model definition, please use `LlamaForCausalLM` instead (#2854)
2	5	docs/source/models/supported_models.rst
0	330	vllm/model_executor/models/yi.py
0	1	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	64	vllm/transformers_utils/configs/yi.py

[a463c333d] Woosuk Kwon 2024-02-13 Use CuPy for CUDA graphs (#2811)
1	0	requirements.txt
1	1	vllm/engine/llm_engine.py
9	4	vllm/model_executor/parallel_utils/communication_op.py
130	0	vllm/model_executor/parallel_utils/cupy_utils.py
37	0	vllm/model_executor/parallel_utils/parallel_state.py
5	2	vllm/test_utils.py
39	13	vllm/worker/model_runner.py
24	2	vllm/worker/worker.py

[ea356004d] Philipp Moritz 2024-02-13 Revert "Refactor llama family models (#2637)" (#2851)
0	25	vllm/model_executor/layers/layernorm.py
5	4	vllm/model_executor/models/__init__.py
342	0	vllm/model_executor/models/aquila.py
303	32	vllm/model_executor/models/baichuan.py
299	0	vllm/model_executor/models/internlm.py
260	25	vllm/model_executor/models/internlm2.py
56	106	vllm/model_executor/models/llama.py
352	0	vllm/model_executor/models/mistral.py
236	31	vllm/model_executor/models/qwen.py
270	13	vllm/model_executor/models/stablelm.py
330	0	vllm/model_executor/models/yi.py
4	0	vllm/transformers_utils/config.py
8	0	vllm/transformers_utils/configs/__init__.py
69	0	vllm/transformers_utils/configs/aquila.py
62	0	vllm/transformers_utils/configs/baichuan.py
60	0	vllm/transformers_utils/configs/qwen.py
64	0	vllm/transformers_utils/configs/yi.py

[5c976a7e1] Roy 2024-02-13 Refactor llama family models (#2637)
25	0	vllm/model_executor/layers/layernorm.py
4	5	vllm/model_executor/models/__init__.py
0	342	vllm/model_executor/models/aquila.py
32	303	vllm/model_executor/models/baichuan.py
0	299	vllm/model_executor/models/internlm.py
25	260	vllm/model_executor/models/internlm2.py
106	56	vllm/model_executor/models/llama.py
0	352	vllm/model_executor/models/mistral.py
31	236	vllm/model_executor/models/qwen.py
13	270	vllm/model_executor/models/stablelm.py
0	330	vllm/model_executor/models/yi.py
0	4	vllm/transformers_utils/config.py
0	8	vllm/transformers_utils/configs/__init__.py
0	69	vllm/transformers_utils/configs/aquila.py
0	62	vllm/transformers_utils/configs/baichuan.py
0	60	vllm/transformers_utils/configs/qwen.py
0	64	vllm/transformers_utils/configs/yi.py

[f96449327] Simon Mo 2024-02-12 [CI] Ensure documentation build is checked in CI (#2842)
7	0	.buildkite/test-pipeline.yaml
3	1	.buildkite/test-template.j2
2	0	docs/source/conf.py
1	0	docs/source/index.rst
1	0	docs/source/quantization/fp8_e5m2_kv_cache.rst

[a4211a4dc] Roger Wang 2024-02-12 Serving Benchmark Refactoring (#2433)
10	4	.buildkite/run-benchmarks.sh
284	0	benchmarks/backend_request_func.py
258	120	benchmarks/benchmark_serving.py
1	1	benchmarks/launch_tgi_server.sh

[563836496] Rex 2024-02-12 Refactor 2 awq gemm kernels into m16nXk32 (#2723)
72	294	csrc/quantization/awq/gemm_kernels.cu
1	1	vllm/model_executor/layers/quantization/awq.py

[4ca2c358b] Philipp Moritz 2024-02-12 Add documentation section about LoRA (#2834)
1	0	docs/source/index.rst
52	0	docs/source/models/lora.rst

[0580aab02] Hongxia Yang 2024-02-11 [ROCm] support Radeon™ 7900 series (gfx1100) without using flash-attention (#2768)
12	3	Dockerfile.rocm
2	1	docs/source/getting_started/amd-installation.rst
1	1	setup.py
45	0	vllm/model_executor/layers/attention.py

[3711811b1] Woosuk Kwon 2024-02-08 Disable custom all reduce by default (#2808)
18	8	vllm/config.py

[65b89d16e] SangBin Cho 2024-02-09 [Ray] Integration compiled DAG off by default (#2471)
55	7	vllm/engine/llm_engine.py
18	0	vllm/engine/ray_utils.py

[931746bc6] Philipp Moritz 2024-02-07 Add documentation on how to do incremental builds (#2796)
10	0	docs/source/getting_started/installation.rst
5	0	setup.py

[c81dddb45] Hongxia Yang 2024-02-07 [ROCm] Fix build problem resulted from previous commit related to FP8 kv-cache support  (#2790)
1	0	Dockerfile.rocm
15	0	rocm_patch/rocm_bf16.patch

[fe6d09ae6] Lily Liu 2024-02-06 [Minor] More fix of test_cache.py CI test failure (#2750)
4	5	tests/kernels/test_cache.py
12	6	vllm/utils.py

[ed70c70ea] liuyhwangyh 2024-02-07 modelscope: fix issue when model parameter is not a model id but path of the model. (#2489)
6	3	vllm/config.py

[f0d4e1455] Woosuk Kwon 2024-02-05 Add fused top-K softmax kernel for MoE (#2769)
7	0	csrc/moe/moe_ops.cpp
9	0	csrc/moe/moe_ops.h
499	0	csrc/moe/topk_softmax_kernels.cu
1	1	csrc/pybind.cpp
11	0	setup.py
10	16	tests/kernels/test_moe.py
48	10	vllm/model_executor/layers/fused_moe.py
3	12	vllm/model_executor/models/deepseek.py
3	11	vllm/model_executor/models/mixtral.py

[2ccee3def] Douglas Lehr 2024-02-05 [ROCm] Fixup arch checks for ROCM (#2627)
0	3	Dockerfile.rocm
56	34	setup.py

[b92adec8e] Lukas 2024-02-05 Set local logging level via env variable (#2774)
2	1	vllm/logger.py

[56f738ae9] Hongxia Yang 2024-02-05 [ROCm] Fix some kernels failed unit tests (#2498)
18	0	tests/kernels/allclose_default.py
13	3	tests/kernels/test_activation.py
17	5	tests/kernels/test_attention.py
5	1	tests/kernels/test_cache.py
9	3	tests/kernels/test_pos_encoding.py

[72d3a30c6] Woosuk Kwon 2024-02-05 [Minor] Fix benchmark_latency script (#2765)
5	2	benchmarks/benchmark_latency.py

[c9b45adee] whyiug 2024-02-05 Require triton >= 2.1.0 (#2746)
1	0	requirements.txt

[5a6c81b05] Rex 2024-02-04 Remove eos tokens from output by default (#2611)
12	4	vllm/engine/llm_engine.py

[51cd22ce5] dancingpipi 2024-02-05 set&get llm internal tokenizer instead of the TokenizerGroup (#2741)
2	2	vllm/entrypoints/llm.py

[5ed704ec8] Massimiliano Pronesti 2024-02-04 docs: fix langchain (#2736)
3	3	docs/source/serving/serving_with_langchain.rst

[4abf6336e] Cheng Su 2024-02-02 Add one example to run batch inference distributed on Ray (#2696)
70	0	examples/offline_inference_distributed.py

[0e163fce1] zspo 2024-02-02 Fix default length_penalty to 1.0 (#2667)
1	1	vllm/sequence.py

[96b6f475d] Kunshang Ji 2024-02-02 Remove hardcoded `device="cuda" ` to support more devices (#2503)
7	0	benchmarks/benchmark_latency.py
9	1	benchmarks/benchmark_throughput.py
18	9	benchmarks/kernels/benchmark_paged_attention.py
21	16	tests/kernels/test_activation.py
23	28	tests/kernels/test_attention.py
21	21	tests/kernels/test_cache.py
10	7	tests/kernels/test_layernorm.py
11	11	tests/kernels/test_pos_encoding.py
20	37	tests/kernels/test_prefix_prefill.py
2	2	tests/lora/conftest.py
22	14	tests/lora/test_layers.py
3	1	tests/lora/test_worker.py
27	37	tests/samplers/test_rejection_sampler.py
24	13	tests/samplers/test_sampler.py
2	1	tests/worker/spec_decode/utils.py
1	1	tests/worker/test_model_runner.py
6	0	vllm/config.py
18	7	vllm/engine/arg_utils.py
10	2	vllm/engine/llm_engine.py
1	3	vllm/model_executor/layers/activation.py
1	1	vllm/model_executor/layers/attention.py
2	8	vllm/model_executor/layers/linear.py
0	3	vllm/model_executor/layers/quantization/awq.py
0	4	vllm/model_executor/layers/quantization/gptq.py
2	4	vllm/model_executor/layers/quantization/squeezellm.py
9	13	vllm/model_executor/layers/rotary_embedding.py
0	2	vllm/model_executor/layers/vocab_parallel_embedding.py
3	2	vllm/model_executor/model_loader.py
2	1	vllm/utils.py
2	0	vllm/worker/cache_engine.py
46	29	vllm/worker/model_runner.py
23	17	vllm/worker/worker.py

[c410f5d02] Pernekhan Utemuratov 2024-02-01 Use revision when downloading the quantization config file (#2697)
1	4	vllm/model_executor/model_loader.py
14	15	vllm/model_executor/weight_utils.py

[bb8c697ee] Simon Mo 2024-02-01 Update README for meetup slides (#2718)
1	9	README.md

[b9e96b17d] Simon Mo 2024-02-01 fix python 3.8 syntax (#2716)
14	1	Dockerfile
7	7	vllm/entrypoints/openai/serving_completion.py

[923797fea] zhaoyang-star 2024-02-02 Fix compile error when using rocm (#2648)
2	0	csrc/attention/attention_kernels.cu
7	0	csrc/cache_kernels.cu
0	1	csrc/quantization/fp8_e5m2_kvcache/quant_utils.cuh

[cd9e60c76] Fengzhe Zhou 2024-02-02 Add Internlm2 (#2666)
1	0	README.md
3	0	docs/source/models/supported_models.rst
1	0	vllm/model_executor/models/__init__.py
325	0	vllm/model_executor/models/internlm2.py

[93b38bea5] Robert Shaw 2024-01-31 Refactor Prometheus and Add Request Level Metrics (#2316)
54	0	examples/production_monitoring/README.md
19	0	examples/production_monitoring/docker-compose.yaml
931	0	examples/production_monitoring/grafana.json
10	0	examples/production_monitoring/prometheus.yaml
69	79	vllm/engine/llm_engine.py
144	22	vllm/engine/metrics.py
7	1	vllm/sequence.py

[d0d93b92b] Philipp Moritz 2024-01-31 Add unit test for Mixtral MoE layer (#2677)
6	0	Dockerfile
0	50	tests/kernels/test_fused_moe.py
104	0	tests/kernels/test_moe.py
3	1	vllm/model_executor/layers/fused_moe.py
6	4	vllm/model_executor/models/mixtral.py

[89efcf1ce] Philipp Moritz 2024-01-31 [Minor] Fix test_cache.py CI test failure (#2684)
2	2	tests/kernels/test_cache.py

[c664b0e68] zspo 2024-02-01 fix some bugs (#2689)
4	1	vllm/config.py
4	2	vllm/engine/async_llm_engine.py

[d69ff0cbb] Tao He 2024-02-01 Fixes assertion failure in prefix caching: the lora index mapping should respect prefix_len (#2688)
2	2	vllm/worker/model_runner.py

[1af090b57] Zhuohan Li 2024-01-31 Bump up version to v0.3.0 (#2656)
3	1	README.md
3	1	docs/source/index.rst
1	1	vllm/__init__.py

[3dad94448] Woosuk Kwon 2024-01-30 Add quantized mixtral support (#2673)
9	4	vllm/model_executor/model_loader.py
1	0	vllm/model_executor/models/__init__.py
412	0	vllm/model_executor/models/mixtral_quant.py

[105a40f53] Woosuk Kwon 2024-01-30 [Minor] Fix false warning when TP=1 (#2674)
4	0	vllm/model_executor/parallel_utils/custom_all_reduce.py

[bbe9bd968] Philipp Moritz 2024-01-30 [Minor] Fix a small typo (#2672)
1	1	vllm/model_executor/parallel_utils/parallel_state.py

[4f65af0e2] Vladimir 2024-01-30 Add swap_blocks unit tests (#2616)
68	0	tests/kernels/test_cache.py

[d79ced329] Wen Sun 2024-01-31 Fix 'Actor methods cannot be called directly' when using `--engine-use-ray` (#2664)
13	5	vllm/engine/async_llm_engine.py

[ab4064466] Philipp Moritz 2024-01-29 Fused MOE for Mixtral (#2542)
1	1	csrc/moe_align_block_size_kernels.cu
7	9	csrc/ops.h
3	3	csrc/pybind.cpp
104	96	vllm/model_executor/models/mixtral.py

[5d60def02] wangding zeng 2024-01-30 DeepseekMoE support with Fused MoE kernel (#2453)
11	0	csrc/dispatch_utils.h
108	0	csrc/moe_align_block_size_kernels.cu
9	0	csrc/ops.h
4	0	csrc/pybind.cpp
1	0	setup.py
50	0	tests/kernels/test_fused_moe.py
287	0	vllm/model_executor/layers/fused_moe.py
1	0	vllm/model_executor/models/__init__.py
453	0	vllm/model_executor/models/deepseek.py

[ea8489fce] Rasmus Larsen 2024-01-29 ROCm: Allow setting compilation target (#2581)
9	5	setup.py

[1b20639a4] Hanzhi Zhou 2024-01-30 No repeated IPC open (#2642)
25	18	csrc/custom_all_reduce.cuh

[b72af8f1e] zhaoyang-star 2024-01-29 Fix error when tp > 1 (#2644)
2	3	vllm/engine/llm_engine.py

[9090bf02e] zhaoyang-star 2024-01-29 Support FP8-E5M2 KV Cache (#2279)
8	0	benchmarks/benchmark_latency.py
11	1	benchmarks/benchmark_throughput.py
18	15	benchmarks/kernels/benchmark_paged_attention.py
1	0	csrc/attention/attention_dtypes.h
164	95	csrc/attention/attention_kernels.cu
35	0	csrc/attention/dtype_fp8_e5m2.cuh
7	1	csrc/cache.h
108	27	csrc/cache_kernels.cu
10	0	csrc/dispatch_utils.h
4	2	csrc/ops.h
4	0	csrc/pybind.cpp
278	0	csrc/quantization/fp8_e5m2_kvcache/quant_utils.cuh
32	0	docs/source/quantization/fp8_e5m2_kv_cache.rst
3	0	setup.py
2	39	tests/kernels/conftest.py
36	5	tests/kernels/test_attention.py
7	3	tests/kernels/test_cache.py
28	1	vllm/config.py
10	1	vllm/engine/arg_utils.py
6	0	vllm/engine/llm_engine.py
5	1	vllm/model_executor/input_metadata.py
3	0	vllm/model_executor/layers/attention.py
109	1	vllm/utils.py
12	3	vllm/worker/cache_engine.py
7	0	vllm/worker/model_runner.py
4	1	vllm/worker/worker.py

[7d648418b] Simon Mo 2024-01-28 Update Ray version requirements (#2636)
1	1	requirements-rocm.txt
1	1	requirements.txt

[89be30fa7] Murali Andoorveedu 2024-01-27 Small async_llm_engine refactor (#2618)
2	4	vllm/engine/async_llm_engine.py

[f8ecb84c0] Woosuk Kwon 2024-01-27 Speed up Punica compilation (#2632)
1	1	.buildkite/test-template.j2
0	21	csrc/punica/bgmv/bgmv_all.cu
4	0	csrc/punica/bgmv/bgmv_bf16_bf16_bf16.cu
4	0	csrc/punica/bgmv/bgmv_bf16_bf16_fp16.cu
4	0	csrc/punica/bgmv/bgmv_bf16_fp16_bf16.cu
4	0	csrc/punica/bgmv/bgmv_bf16_fp16_fp16.cu
4	0	csrc/punica/bgmv/bgmv_bf16_fp32_bf16.cu
4	0	csrc/punica/bgmv/bgmv_bf16_fp32_fp16.cu
4	0	csrc/punica/bgmv/bgmv_fp16_bf16_bf16.cu
4	0	csrc/punica/bgmv/bgmv_fp16_bf16_fp16.cu
4	0	csrc/punica/bgmv/bgmv_fp16_fp16_bf16.cu
4	0	csrc/punica/bgmv/bgmv_fp16_fp16_fp16.cu
4	0	csrc/punica/bgmv/bgmv_fp16_fp32_bf16.cu
4	0	csrc/punica/bgmv/bgmv_fp16_fp32_fp16.cu
4	0	csrc/punica/bgmv/bgmv_fp32_bf16_bf16.cu
4	0	csrc/punica/bgmv/bgmv_fp32_bf16_fp16.cu
4	0	csrc/punica/bgmv/bgmv_fp32_fp16_bf16.cu
4	0	csrc/punica/bgmv/bgmv_fp32_fp16_fp16.cu
4	0	csrc/punica/bgmv/bgmv_fp32_fp32_bf16.cu
4	0	csrc/punica/bgmv/bgmv_fp32_fp32_fp16.cu
27	0	csrc/punica/bgmv/generator.py

[5f036d2bc] Woosuk Kwon 2024-01-27 [Minor] Fix warning on Ray dependencies (#2630)
1	1	vllm/engine/ray_utils.py

[380170038] Hanzhi Zhou 2024-01-28 Implement custom all reduce kernels (#2192)
148	0	csrc/custom_all_reduce.cu
555	0	csrc/custom_all_reduce.cuh
284	0	csrc/custom_all_reduce_test.cu
22	0	csrc/ops.h
16	0	csrc/pybind.cpp
1	0	requirements.txt
4	2	setup.py
3	27	tests/distributed/test_comm_ops.py
85	0	tests/distributed/test_custom_all_reduce.py
14	0	vllm/config.py
7	1	vllm/engine/arg_utils.py
1	0	vllm/engine/llm_engine.py
3	0	vllm/entrypoints/llm.py
12	2	vllm/model_executor/parallel_utils/communication_op.py
223	0	vllm/model_executor/parallel_utils/custom_all_reduce.py
38	0	vllm/test_utils.py
31	29	vllm/worker/model_runner.py
6	4	vllm/worker/worker.py

[220a47627] Xiang Xu 2024-01-27 Use head_dim in config if exists (#2622)
2	0	vllm/config.py

[beb89f68b] Casper 2024-01-27 AWQ: Up to 2.66x higher throughput (#2566)
8	0	csrc/ops.h
1	0	csrc/pybind.cpp
108	0	csrc/quantization/awq/gemm_kernels.cu
10	1	vllm/model_executor/layers/quantization/awq.py

[390b495ff] Philipp Moritz 2024-01-26 Don't build punica kernels by default (#2605)
2	0	.github/workflows/scripts/build.sh
2	0	Dockerfile
1	1	setup.py
6	3	vllm/lora/punica.py

[3a0e1fc07] dakotamahan-stability 2024-01-26 Support for Stable LM 2 (#2598)
2	2	vllm/model_executor/models/stablelm.py

[6b7de1a03] Hongxia Yang 2024-01-26 [ROCm] add support to ROCm 6.0 and MI300 (#2274)
31	5	Dockerfile.rocm
2	1	README.md
3	0	csrc/cuda_utils.h
18	0	csrc/cuda_utils_kernels.cu
6	0	csrc/pybind.cpp
30	3	docs/source/getting_started/amd-installation.rst
2	0	setup.py
4	4	vllm/utils.py

[5265631d1] Vladimir 2024-01-26 use a correct device when creating OptionalCUDAGuard (#2583)
1	1	csrc/cache_kernels.cu

[2832e7b9f] Junyang Lin 2024-01-25 fix names and license for Qwen2 (#2589)
1	1	docs/source/models/supported_models.rst
3	2	vllm/model_executor/models/qwen2.py

[3a7dd7e36] Simon Mo 2024-01-24 Support Batch Completion in Server (#2529)
53	2	tests/entrypoints/test_openai_server.py
162	103	vllm/entrypoints/openai/serving_completion.py

[223c19224] LastWhisper 2024-01-25 Fix the syntax error in the doc of supported_models (#2584)
1	1	docs/source/models/supported_models.rst

[f1f6cc10c] Federico Galatolo 2024-01-24 Added `include_stop_str_in_output` and `length_penalty` parameters to OpenAI API (#2562)
8	0	vllm/entrypoints/openai/protocol.py

[3209b4903] Nikola Borisov 2024-01-23 [Bugfix] fix crash if max_tokens=None (#2570)
13	0	tests/test_regression.py
13	0	tests/test_sampling_params.py
2	2	vllm/sampling_params.py

[1e4277d2d] Simon Mo 2024-01-23 lint: format all python file instead of just source code (#2567)
1	1	.github/workflows/yapf.yml
7	13	benchmarks/benchmark_serving.py
0	1	examples/openai_chatcompletion_client.py
1	2	examples/openai_completion_client.py
1	1	format.sh

[9b945daaf] Antoni Baum 2024-01-24 [Experimental] Add multi-LoRA support (#1804)
3	0	.buildkite/test-pipeline.yaml
5	5	benchmarks/benchmark_latency.py
217	0	csrc/punica/LICENSE
21	0	csrc/punica/bgmv/bgmv_all.cu
59	0	csrc/punica/bgmv/bgmv_config.h
294	0	csrc/punica/bgmv/bgmv_impl.cuh
1324	0	csrc/punica/bgmv/vec_dtypes.cuh
563	0	csrc/punica/punica_ops.cc
117	0	examples/multilora_inference.py
54	5	setup.py
11	0	tests/async_engine/test_async_llm_engine.py
0	0	tests/lora/__init__.py
143	0	tests/lora/conftest.py
709	0	tests/lora/test_layers.py
144	0	tests/lora/test_llama.py
224	0	tests/lora/test_lora.py
475	0	tests/lora/test_lora_manager.py
175	0	tests/lora/test_punica.py
69	0	tests/lora/test_tokenizer.py
172	0	tests/lora/test_utils.py
61	0	tests/lora/test_worker.py
88	0	tests/lora/utils.py
7	6	tests/samplers/test_sampler.py
2	2	tests/worker/spec_decode/utils.py
2	2	tests/worker/test_model_runner.py
50	1	vllm/config.py
68	5	vllm/core/scheduler.py
50	3	vllm/engine/arg_utils.py
71	8	vllm/engine/async_llm_engine.py
87	24	vllm/engine/llm_engine.py
10	1	vllm/entrypoints/llm.py
0	0	vllm/lora/__init__.py
975	0	vllm/lora/layers.py
160	0	vllm/lora/lora.py
654	0	vllm/lora/models.py
173	0	vllm/lora/punica.py
32	0	vllm/lora/request.py
39	0	vllm/lora/utils.py
237	0	vllm/lora/worker_manager.py
18	17	vllm/model_executor/layers/sampler.py
20	6	vllm/model_executor/layers/vocab_parallel_embedding.py
15	4	vllm/model_executor/model_loader.py
25	6	vllm/model_executor/models/llama.py
27	6	vllm/model_executor/models/mistral.py
5	1	vllm/model_executor/parallel_utils/parallel_state.py
16	3	vllm/outputs.py
3	2	vllm/prefix.py
22	0	vllm/sequence.py
80	0	vllm/transformers_utils/tokenizer.py
90	0	vllm/utils.py
146	15	vllm/worker/model_runner.py
23	4	vllm/worker/worker.py

[9c1352eb5] Erfan Al-Hossami 2024-01-23 [Feature] Simple API token authentication and pluggable middlewares (#1106)
3	29	docs/source/getting_started/quickstart.rst
45	0	vllm/entrypoints/openai/api_server.py

[7a0b011dd] Jason Zhu 2024-01-22 Add a 1-line docstring to explain why calling context_attention_fwd twice in test_prefix_prefill.py (#2553)
1	0	tests/kernels/test_prefix_prefill.py

[63e835cbc] Harry Mellor 2024-01-22 Fix progress bar and allow HTTPS in `benchmark_serving.py` (#2552)
9	3	benchmarks/benchmark_serving.py

[94b5edeb5] Junyang Lin 2024-01-23 Add qwen2 (#2495)
1	0	README.md
3	0	docs/source/models/supported_models.rst
1	1	requirements-rocm.txt
1	1	requirements.txt
3	0	vllm/model_executor/models/__init__.py
335	0	vllm/model_executor/models/qwen2.py

[ab7e6006d] Philipp Moritz 2024-01-22 Fix https://github.com/vllm-project/vllm/issues/2540 (#2545)
1	1	vllm/engine/llm_engine.py

[18bfcdd05] Cade Daniel 2024-01-21 [Speculative decoding 2/9] Multi-step worker for draft model (#2424)
0	0	tests/worker/__init__.py
0	0	tests/worker/spec_decode/__init__.py
261	0	tests/worker/spec_decode/test_multi_step_worker.py
177	0	tests/worker/spec_decode/utils.py
5	3	vllm/engine/llm_engine.py
3	4	vllm/engine/ray_utils.py
25	0	vllm/model_executor/parallel_utils/parallel_state.py
4	0	vllm/utils.py
2	2	vllm/worker/model_runner.py
178	0	vllm/worker/spec_decode/multi_step_worker.py
3	3	vllm/worker/worker.py

[71d63ed72] Jannis Schönleber 2024-01-22  migrate pydantic from v1 to v2 (#2531)
1	1	requirements-neuron.txt
1	1	requirements-rocm.txt
1	1	requirements.txt
12	6	vllm/entrypoints/openai/api_server.py
3	4	vllm/entrypoints/openai/protocol.py
5	6	vllm/entrypoints/openai/serving_chat.py
3	3	vllm/entrypoints/openai/serving_completion.py

[d75c40734] Nick Hill 2024-01-20 [Fix] Keep `scheduler.running` as deque (#2523)
2	4	vllm/core/scheduler.py

[5b23c3f26] Junda Chen 2024-01-20 Add `group` as an argument in broadcast ops (#2522)
36	17	vllm/model_executor/parallel_utils/communication_op.py

[00efdc84b] Simon Mo 2024-01-19 Add benchmark serving to CI (#2505)
32	4	.buildkite/run-benchmarks.sh

[91a61da9b] Roy 2024-01-20 [Bugfix] fix load local safetensors model (#2512)
2	2	vllm/model_executor/weight_utils.py

[ef9b636e2] Zhuohan Li 2024-01-19 Simplify broadcast logic for control messages (#2501)
33	2	tests/distributed/test_comm_ops.py
68	5	vllm/model_executor/parallel_utils/communication_op.py
30	108	vllm/worker/model_runner.py
15	14	vllm/worker/worker.py

[2709c0009] Harry Mellor 2024-01-19 Support OpenAI API server in `benchmark_serving.py` (#2172)
3	0	.gitignore
48	32	benchmarks/benchmark_serving.py

[dd7e8f5f6] Simon Mo 2024-01-18 refactor complemention api for readability (#2499)
10	0	tests/entrypoints/test_openai_server.py
45	0	vllm/entrypoints/openai/protocol.py
3	25	vllm/entrypoints/openai/serving_chat.py
210	215	vllm/entrypoints/openai/serving_completion.py
15	12	vllm/entrypoints/openai/serving_engine.py
1	1	vllm/model_executor/weight_utils.py

[d2a68364c] ljss 2024-01-19 [BugFix] Fix abort_seq_group (#2463)
2	2	vllm/core/scheduler.py

[7e1081139] Nikola Borisov 2024-01-18 Don't download both safetensor and bin files. (#2480)
16	3	vllm/model_executor/weight_utils.py

[18473cf49] Liangfu Chen 2024-01-18 [Neuron] Add an option to build with neuron (#2065)
9	0	requirements-neuron.txt
51	11	setup.py
4	2	vllm/utils.py

[4df417d05] zspo 2024-01-19 fix: fix some args desc (#2487)
0	1	vllm/prefix.py
1	0	vllm/sequence.py

[5d80a9178] Jason Zhu 2024-01-18 Minor fix in prefill cache example (#2494)
10	2	examples/offline_inference_with_prefix.py

[8a25d3a71] YingchaoX 2024-01-19 fix stablelm.py tensor-parallel-size bug (#2482)
1	1	vllm/model_executor/models/stablelm.py

[d10f8e1d4] shiyi.c_98 2024-01-17 [Experimental] Prefix Caching Support (#1669)
4	0	.buildkite/test-pipeline.yaml
51	0	examples/offline_inference_with_prefix.py
168	0	tests/kernels/test_prefix_prefill.py
41	0	tests/prefix_caching/test_prefix_caching.py
12	6	tests/samplers/test_sampler.py
3	2	tests/worker/test_model_runner.py
4	0	vllm/block.py
42	5	vllm/core/block_manager.py
5	0	vllm/core/scheduler.py
13	3	vllm/engine/async_llm_engine.py
17	1	vllm/engine/llm_engine.py
5	1	vllm/entrypoints/api_server.py
14	3	vllm/entrypoints/llm.py
6	0	vllm/model_executor/input_metadata.py
55	33	vllm/model_executor/layers/attention.py
0	0	vllm/model_executor/layers/triton_kernel/__init__.py
728	0	vllm/model_executor/layers/triton_kernel/prefix_prefill.py
87	0	vllm/prefix.py
6	1	vllm/sequence.py
95	16	vllm/worker/model_runner.py

[14cc317ba] FlorianJoncour 2024-01-17 OpenAI Server refactoring (#2360)
3	0	.buildkite/test-pipeline.yaml
3	0	requirements-dev.txt
17	19	tests/async_engine/{test_openai_server.py => test_chat_template.py}
193	0	tests/entrypoints/test_openai_server.py
25	624	vllm/entrypoints/openai/api_server.py
288	0	vllm/entrypoints/openai/serving_chat.py
295	0	vllm/entrypoints/openai/serving_completion.py
130	0	vllm/entrypoints/openai/serving_engine.py

[e1957c6eb] Hyunsung Lee 2024-01-17 Add StableLM3B model (#2372)
1	0	README.md
3	0	docs/source/models/supported_models.rst
5	12	tests/models/test_models.py
2	1	vllm/model_executor/models/__init__.py
299	0	vllm/model_executor/models/stablelm.py

[8cd5a992b] Simon Mo 2024-01-16 ci: retry on build failure as well (#2457)
4	0	.buildkite/test-template.j2

[947f0b23c] Simon Mo 2024-01-16 CI: make sure benchmark script exit on error (#2449)
11	0	.buildkite/run-benchmarks.sh

[f780504d1] Chenhui Zhang 2024-01-16 fix weigit loading for GQA with TP (#2379)
4	1	vllm/model_executor/layers/linear.py

[bfc072add] Simon Mo 2024-01-15 Allow buildkite to retry build on agent lost (#2446)
4	0	.buildkite/test-template.j2

[2a18da257] Woosuk Kwon 2024-01-15 Announce the second vLLM meetup (#2444)
9	0	README.md

[6e01e8c1c] Simon Mo 2024-01-14 [CI] Add Buildkite (#2355)
24	0	.buildkite/run-benchmarks.sh
41	0	.buildkite/test-pipeline.yaml
46	0	.buildkite/test-template.j2
24	12	Dockerfile
3	1	requirements-dev.txt
6	1	setup.py
10	2	tests/async_engine/test_api_server.py
9	6	tests/async_engine/test_openai_server.py
14	12	tests/distributed/test_comm_ops.py
1	1	tests/kernels/test_attention.py
2	2	tests/kernels/test_cache.py
1	0	tests/samplers/test_logprobs.py
11	0	tests/samplers/test_sampler.py

[9f659bf07] Roy 2024-01-15 [Minor] Optimize cuda graph memory usage (#2437)
10	2	vllm/worker/model_runner.py

[35c4bc20d] Woosuk Kwon 2024-01-12 [Minor] Fix err msg (#2431)
3	3	vllm/worker/worker.py

[218dc2ccd] 陈序 2024-01-13 Aligning `top_p` and `top_k` Sampling (#1885)
63	0	tests/samplers/test_sampler.py
15	15	vllm/model_executor/layers/sampler.py

[827cbcd37] Simon 2024-01-12 Update quickstart.rst (#2369)
1	1	docs/source/getting_started/quickstart.rst

[cb7a1c1cb] Ben 2024-01-13 Suggest using dtype=half when OOM.
3	1	vllm/worker/worker.py

[7878958c0] Gary Hui 2024-01-13 Address Phi modeling update 2 (#2428)
1	1	vllm/model_executor/models/__init__.py
59	61	vllm/model_executor/models/phi.py

[ce036244c] Chirag Jain 2024-01-13 Allow setting fastapi root_path argument (#2341)
6	0	vllm/entrypoints/api_server.py
6	0	vllm/entrypoints/openai/api_server.py

[48cf1e413] 陈序 2024-01-13 fix: deque mutated during iteration in abort_seq_group (#2371)
16	13	vllm/core/scheduler.py

[97460585d] arkohut 2024-01-12 Add gradio chatbot for openai webserver (#2307)
81	0	examples/gradio_openai_chatbot_webserver.py

[f745847ef] Zhuohan Li 2024-01-11 [Minor] Fix the format in quick start guide related to Model Scope (#2425)
8	27	docs/source/getting_started/quickstart.rst

[6549aef24] Jiaxiang 2024-01-12 [DOC] Add additional comments for LLMEngine and AsyncLLMEngine (#1011)
36	7	docs/source/conf.py
7	0	docs/source/dev/engine/async_llm_engine.rst
13	0	docs/source/dev/engine/engine_index.rst
6	0	docs/source/dev/engine/llm_engine.rst
13	1	docs/source/index.rst
12	0	vllm/core/scheduler.py
45	1	vllm/engine/async_llm_engine.py
102	6	vllm/engine/llm_engine.py
8	0	vllm/worker/worker.py

[50376faa7] Woosuk Kwon 2024-01-11 Rename phi_1_5 -> phi (#2385)
0	0	vllm/model_executor/models/{phi_1_5.py => phi.py}

[4b61c6b66] Yunfeng Bai 2024-01-10 `get_ip()`: Fix ipv4 ipv6 dualstack (#2408)
3	1	vllm/utils.py

[79d64c495] Cade Daniel 2024-01-09 [Speculative decoding 1/9] Optimized rejection sampler (#2336)
392	0	tests/samplers/test_rejection_sampler.py
392	0	vllm/model_executor/layers/rejection_sampler.py

[74cd5abdd] KKY 2024-01-09 Add baichuan chat template jinjia file (#2390)
22	0	examples/template_baichuan.jinja

[28c3f1210] Woosuk Kwon 2024-01-08 [Minor] Remove unused code in attention (#2384)
9	14	vllm/model_executor/layers/attention.py

[c88481913] Woosuk Kwon 2024-01-08 Fix eager mode performance (#2377)
3	1	vllm/worker/model_runner.py

[05921a9a7] Nadav Shmayovits 2024-01-07 Changed scheduler to use deques instead of lists (#2290)
10	8	vllm/core/policy.py
14	14	vllm/core/scheduler.py
4	2	vllm/engine/llm_engine.py

[d0215a58e] Iskren Ivov Chernev 2024-01-05 Ensure metrics are logged regardless of requests (#2347)
6	0	vllm/engine/async_llm_engine.py
3	0	vllm/engine/llm_engine.py
19	1	vllm/entrypoints/openai/api_server.py

[937e7b7d7] Alexandre Payot 2024-01-04 Build docker image with shared objects from "build" step (#2237)
1	0	.dockerignore
0	1	Dockerfile

[aee8ef661] ljss 2024-01-04 Miner fix of type hint (#2340)
1	1	vllm/engine/ray_utils.py

[2e0b6e775] Woosuk Kwon 2024-01-03 Bump up to v0.2.7 (#2337)
1	1	vllm/__init__.py

[941767127] Woosuk Kwon 2024-01-03 Revert the changes in test_cache (#2335)
12	12	tests/kernels/test_cache.py

[74d8d7762] Ronen Schaffer 2024-01-04 Remove unused const TIMEOUT_TO_PREVENT_DEADLOCK (#2321)
0	1	vllm/entrypoints/api_server.py

[fd4ea8ef5] Zhuohan Li 2024-01-04 Use NCCL instead of ray for control-plane communication to remove serialization overhead (#2221)
3	4	docs/source/models/adding_model.rst
0	2	requirements-rocm.txt
0	2	requirements.txt
9	4	tests/async_engine/test_api_server.py
12	12	tests/kernels/test_cache.py
3	2	tests/worker/test_model_runner.py
33	28	vllm/engine/async_llm_engine.py
138	100	vllm/engine/llm_engine.py
22	15	vllm/engine/ray_utils.py
4	5	vllm/model_executor/input_metadata.py
14	5	vllm/model_executor/layers/sampler.py
1	1	vllm/model_executor/models/aquila.py
1	1	vllm/model_executor/models/baichuan.py
1	1	vllm/model_executor/models/bloom.py
1	1	vllm/model_executor/models/chatglm.py
1	1	vllm/model_executor/models/falcon.py
1	1	vllm/model_executor/models/gpt2.py
1	1	vllm/model_executor/models/gpt_bigcode.py
1	1	vllm/model_executor/models/gpt_j.py
1	1	vllm/model_executor/models/gpt_neox.py
1	1	vllm/model_executor/models/internlm.py
1	1	vllm/model_executor/models/llama.py
1	1	vllm/model_executor/models/mistral.py
2	2	vllm/model_executor/models/mixtral.py
1	1	vllm/model_executor/models/mpt.py
1	1	vllm/model_executor/models/opt.py
1	1	vllm/model_executor/models/phi_1_5.py
1	1	vllm/model_executor/models/qwen.py
1	1	vllm/model_executor/models/yi.py
59	0	vllm/model_executor/parallel_utils/communication_op.py
14	8	vllm/model_executor/sampling_metadata.py
11	1	vllm/utils.py
133	35	vllm/worker/model_runner.py
45	15	vllm/worker/worker.py

[1066cbd15] Ronen Schaffer 2024-01-03 Remove deprecated parameter: concurrency_count (#2315)
3	3	examples/gradio_webserver.py

[6ef00b03a] Woosuk Kwon 2024-01-03 Enable CUDA graph for GPTQ & SqueezeLLM (#2318)
12	6	csrc/quantization/gptq/q_gemm.cu
3	1	csrc/quantization/squeezellm/quant_cuda_kernel.cu
0	6	vllm/config.py

[914056105] Roy 2024-01-03 [Minor] Fix typo and remove unused code (#2305)
0	21	vllm/model_executor/layers/sampler.py
1	1	vllm/sampling_params.py

[77af974b4] Jee Li 2024-01-03 [FIX] Support non-zero CUDA devices in custom kernels (#1959)
4	1	csrc/activation_kernels.cu
3	0	csrc/attention/attention_kernels.cu
5	0	csrc/cache_kernels.cu
3	0	csrc/layernorm_kernels.cu
2	0	csrc/pos_encoding_kernels.cu
2	1	csrc/quantization/squeezellm/quant_cuda_kernel.cu
3	2	tests/kernels/conftest.py
13	3	tests/kernels/test_activation.py
15	10	tests/kernels/test_attention.py
11	6	tests/kernels/test_cache.py
6	3	tests/kernels/test_layernorm.py
7	4	tests/kernels/test_pos_encoding.py

[4934d4927] Jong-hun Shin 2023-12-31 Support GPT-NeoX Models without attention biases (#2301)
3	0	vllm/model_executor/models/gpt_neox.py

[358c328d6] Zhuohan Li 2023-12-28 [BUGFIX] Fix communication test (#2285)
1	1	tests/distributed/test_comm_ops.py

[4aaafdd28] Zhuohan Li 2023-12-27 [BUGFIX] Fix the path of test prompts (#2273)
5	4	tests/conftest.py

[66b108d14] Zhuohan Li 2023-12-27 [BUGFIX] Fix API server test (#2270)
8	6	tests/async_engine/test_api_server.py

[e0ff92000] Zhuohan Li 2023-12-26 [BUGFIX] Do not return ignored sentences twice in async llm engine (#2258)
4	6	vllm/engine/async_llm_engine.py
3	16	vllm/engine/llm_engine.py

[face83c7e] blueceiling 2023-12-25 [Docs] Add "About" Heading to README.md (#2260)
1	1	README.md

[1db83e31a] Shivam Thakkar 2023-12-23 [Docs] Update installation instructions to include CUDA 11.8 xFormers (#2246)
4	0	docs/source/getting_started/installation.rst

[a1b9cb2a3] Woosuk Kwon 2023-12-20 [BugFix] Fix recovery logic for sequence group (#2186)
3	3	vllm/core/block_manager.py
7	5	vllm/core/scheduler.py

[3a4fd5ca5] Woosuk Kwon 2023-12-20 Disable Ray usage stats collection (#2206)
5	0	vllm/engine/llm_engine.py

[c17daa9f8] Ronen Schaffer 2023-12-20 [Docs] Fix broken links (#2222)
1	1	docs/source/models/supported_models.rst
1	1	docs/source/serving/serving_with_langchain.rst

[bd29cf3d3] Antoni Baum 2023-12-20 Remove Sampler copy stream (#2209)
4	9	vllm/model_executor/layers/sampler.py

[31bff6915] Hanzhi Zhou 2023-12-19 Make _prepare_sample non-blocking and use pinned memory for input buffers (#2207)
38	17	vllm/worker/model_runner.py

[ba4f82673] Woosuk Kwon 2023-12-19 [BugFix] Fix weight loading for Mixtral with TP (#2208)
5	26	vllm/model_executor/models/mixtral.py

[de60a3fb9] avideci 2023-12-19 Added DeciLM-7b and DeciLM-7b-instruct (#2062)
1	0	README.md
3	0	docs/source/models/supported_models.rst
1	0	tests/models/test_models.py
1	0	vllm/model_executor/models/__init__.py
123	0	vllm/model_executor/models/decilm.py

[21d5daa4a] Woosuk Kwon 2023-12-18 Add warning on CUDA graph memory usage (#2182)
3	0	vllm/worker/model_runner.py

[290e015c6] Suhong Moon 2023-12-18 Update Help Text for --gpu-memory-utilization Argument (#2183)
7	5	vllm/engine/arg_utils.py

[1b7c791d6] kliuae 2023-12-19 [ROCm] Fixes for GPTQ on ROCm (#2180)
10	0	csrc/quantization/gptq/q_gemm.cu
1	0	docs/source/getting_started/amd-installation.rst
1	1	setup.py
11	15	vllm/config.py

[bbe4466fd] JohnSaxon 2023-12-18 [Minor] Fix typo (#2166)
1	1	vllm/engine/arg_utils.py

[08133c4d1] Harry Mellor 2023-12-18 Add SSL arguments to API servers (#2109)
5	1	vllm/entrypoints/api_server.py
11	1	vllm/entrypoints/openai/api_server.py

[76a7983b2] Woosuk Kwon 2023-12-17 [BugFix] Fix RoPE kernel on long sequences(#2164)
6	6	csrc/pos_encoding_kernels.cu

[8041b7305] Woosuk Kwon 2023-12-17 [BugFix] Raise error when max_model_len is larger than KV cache (#2163)
8	0	vllm/engine/llm_engine.py

[3ec8c25cd] Suhong Moon 2023-12-17 [Docs] Update documentation for gpu-memory-utilization option (#2162)
4	2	docs/source/models/engine_args.rst

[671af2b1c] Woosuk Kwon 2023-12-17 Bump up to v0.2.6 (#2157)
1	1	vllm/__init__.py

[6f41f0e37] Woosuk Kwon 2023-12-17 Disable CUDA graph for SqueezeLLM (#2161)
4	3	vllm/config.py

[2c9b63806] Woosuk Kwon 2023-12-17 [Minor] Fix a typo in .pt weight support (#2160)
1	1	vllm/model_executor/weight_utils.py

[a7347d9a6] Antoni Baum 2023-12-17 Make sampler less blocking (#1889)
123	198	vllm/model_executor/layers/sampler.py
187	0	vllm/model_executor/sampling_metadata.py

[f8c688d74] Woosuk Kwon 2023-12-17 [Minor] Add Phi 2 to supported models (#2159)
1	1	README.md
2	2	docs/source/models/supported_models.rst
2	2	tests/models/test_models.py

[c9fadda54] Woosuk Kwon 2023-12-17 [Minor] Fix xformers version (#2158)
1	1	requirements.txt

[30fb0956d] Woosuk Kwon 2023-12-17 [Minor] Add more detailed explanation on `quantization` argument (#2145)
6	1	vllm/engine/arg_utils.py
4	3	vllm/entrypoints/llm.py

[3a765bd5e] Woosuk Kwon 2023-12-17 Temporarily enforce eager mode for GPTQ models (#2154)
5	0	vllm/config.py

[26c52a5ea] Woosuk Kwon 2023-12-17 [Docs] Add CUDA graph support to docs (#2148)
2	1	README.md
2	1	docs/source/index.rst

[c3372e87b] Woosuk Kwon 2023-12-17 Remove dependency on CuPy (#2152)
0	1	requirements.txt
1	2	vllm/engine/llm_engine.py
2	8	vllm/model_executor/parallel_utils/communication_op.py
0	115	vllm/model_executor/parallel_utils/cupy_utils.py
0	37	vllm/model_executor/parallel_utils/parallel_state.py
12	18	vllm/worker/model_runner.py
4	27	vllm/worker/worker.py

[b0a1d667b] Woosuk Kwon 2023-12-17 Pin PyTorch & xformers versions (#2155)
1	1	.github/workflows/publish.yml
1	1	pyproject.toml
1	1	requirements-build.txt
2	2	requirements.txt

[e1d540223] Woosuk Kwon 2023-12-17 Fix all-reduce memory usage (#2151)
8	0	vllm/worker/worker.py

[3d1cfbfc7] Woosuk Kwon 2023-12-16 [Minor] Delete Llama tokenizer warnings (#2146)
0	17	vllm/transformers_utils/tokenizer.py

[37ca55810] Woosuk Kwon 2023-12-16 Optimize model execution with CUDA graph (#1926)
4	0	benchmarks/benchmark_latency.py
7	2	benchmarks/benchmark_throughput.py
1	0	requirements.txt
17	0	vllm/config.py
15	1	vllm/engine/arg_utils.py
6	1	vllm/engine/llm_engine.py
1	8	vllm/engine/ray_utils.py
10	0	vllm/entrypoints/llm.py
4	1	vllm/model_executor/input_metadata.py
18	22	vllm/model_executor/layers/attention.py
2	10	vllm/model_executor/models/aquila.py
2	10	vllm/model_executor/models/baichuan.py
2	10	vllm/model_executor/models/bloom.py
2	13	vllm/model_executor/models/chatglm.py
2	12	vllm/model_executor/models/falcon.py
3	10	vllm/model_executor/models/gpt2.py
3	10	vllm/model_executor/models/gpt_bigcode.py
2	10	vllm/model_executor/models/gpt_j.py
2	10	vllm/model_executor/models/gpt_neox.py
2	10	vllm/model_executor/models/internlm.py
2	10	vllm/model_executor/models/llama.py
2	10	vllm/model_executor/models/mistral.py
3	10	vllm/model_executor/models/mixtral.py
2	10	vllm/model_executor/models/mpt.py
5	14	vllm/model_executor/models/opt.py
2	10	vllm/model_executor/models/phi_1_5.py
2	10	vllm/model_executor/models/qwen.py
2	10	vllm/model_executor/models/yi.py
8	2	vllm/model_executor/parallel_utils/communication_op.py
115	0	vllm/model_executor/parallel_utils/cupy_utils.py
37	0	vllm/model_executor/parallel_utils/parallel_state.py
7	0	vllm/utils.py
225	16	vllm/worker/model_runner.py
40	12	vllm/worker/worker.py

[eed74a558] Roy 2023-12-17 Simplify weight loading logic (#2133)
4	9	vllm/config.py
5	1	vllm/model_executor/models/mixtral.py
24	27	vllm/model_executor/weight_utils.py

[2acd76f34] Woosuk Kwon 2023-12-15 [ROCm] Temporarily remove GPTQ ROCm support (#2138)
1	1	setup.py
1	1	vllm/config.py

[b81a6a6bb] Woosuk Kwon 2023-12-15 [Docs] Add supported quantization methods to docs (#2135)
2	1	README.md
2	1	docs/source/index.rst

[0fbfc4b81] CHU Tianxiang 2023-12-15 Add GPTQ support (#916)
1	1	benchmarks/benchmark_latency.py
1	1	benchmarks/benchmark_throughput.py
12	0	csrc/ops.h
2	2	csrc/pybind.cpp
64	0	csrc/quantization/gptq/compat.cuh
151	0	csrc/quantization/gptq/matrix_view.cuh
859	0	csrc/quantization/gptq/q_gemm.cu
235	0	csrc/quantization/gptq/qdq_4.cuh
60	0	csrc/quantization/gptq/qdq_util.cuh
1	0	setup.py
1	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
3	2	vllm/entrypoints/llm.py
37	23	vllm/model_executor/layers/linear.py
3	1	vllm/model_executor/layers/quantization/__init__.py
13	11	vllm/model_executor/layers/quantization/awq.py
215	0	vllm/model_executor/layers/quantization/gptq.py
8	6	vllm/model_executor/layers/quantization/squeezellm.py
8	1	vllm/model_executor/models/aquila.py
8	1	vllm/model_executor/models/baichuan.py
3	0	vllm/model_executor/models/chatglm.py
22	17	vllm/model_executor/models/falcon.py
0	1	vllm/model_executor/models/gpt2.py
8	1	vllm/model_executor/models/gpt_j.py
0	1	vllm/model_executor/models/gpt_neox.py
8	1	vllm/model_executor/models/internlm.py
8	1	vllm/model_executor/models/llama.py
8	1	vllm/model_executor/models/mistral.py
9	2	vllm/model_executor/models/mixtral.py
3	0	vllm/model_executor/models/mpt.py
8	1	vllm/model_executor/models/opt.py
3	0	vllm/model_executor/models/phi_1_5.py
8	2	vllm/model_executor/models/qwen.py
8	1	vllm/model_executor/models/yi.py
2	1	vllm/model_executor/weight_utils.py

[c06170cc8] Yunfeng Bai 2023-12-15 Add a flag to include stop string in output text (#1976)
4	3	vllm/engine/llm_engine.py
28	21	vllm/sampling_params.py

[614856da2] Mingcan Xiang 2023-12-14 Avoid multiple redefinition (#1817)
2	0	csrc/cache.h
2	0	csrc/cuda_utils.h
2	0	csrc/dispatch_utils.h
2	0	csrc/ops.h

[05bdf4eaf] TJian 2023-12-14 Fix Dockerfile.rocm (#2101)
1	1	Dockerfile.rocm

[6774bd50b] mezuzza 2023-12-14 Fix typing in AsyncLLMEngine & add toml to requirements-dev (#2100)
1	0	requirements-dev.txt
7	6	vllm/engine/async_llm_engine.py

[31c1f3255] Woosuk Kwon 2023-12-13 Bump up to v0.2.5 (#2095)
1	1	vllm/__init__.py

[21d93c140] Antoni Baum 2023-12-13 Optimize Mixtral with expert parallelism (#2090)
1	13	Dockerfile
0	4	README.md
1	2	docs/source/models/supported_models.rst
9	7	vllm/config.py
3	1	vllm/model_executor/models/__init__.py
207	307	vllm/model_executor/models/mixtral.py

[f1c852014] Woosuk Kwon 2023-12-13 [BugFix] Fix input positions for long context with sliding window (#2088)
23	11	tests/conftest.py
37	0	tests/models/test_mistral.py
8	0	tests/prompts/example.txt
1	0	tests/prompts/summary.txt
6	6	vllm/worker/model_runner.py

[096827c28] Woosuk Kwon 2023-12-13 [Docs] Add notes on ROCm-supported models (#2087)
10	3	docs/source/models/supported_models.rst

[6565d9e33] Woosuk Kwon 2023-12-13 Update installation instruction for vLLM + CUDA 11.8 (#2086)
4	3	docs/source/getting_started/installation.rst

[f375ec844] TJian 2023-12-13 [ROCm] Upgrade xformers version for ROCm & update doc (#2079)
2	2	Dockerfile.rocm
8	8	docs/source/getting_started/amd-installation.rst
17	6	patch_xformers-0.0.22.post7.rocm.sh => patch_xformers.rocm.sh
0	1	requirements-rocm.txt
0	0	rocm_patch/{commonpy_xformers-0.0.22.post7.rocm.patch => commonpy_xformers-0.0.23.rocm.patch}
57	39	rocm_patch/{flashpy_xformers-0.0.22.post7.rocm.patch => flashpy_xformers-0.0.23.rocm.patch}

[518369d78] Woosuk Kwon 2023-12-12 Implement lazy model loader (#2044)
5	57	vllm/model_executor/model_loader.py
77	38	vllm/model_executor/models/__init__.py
7	6	vllm/model_executor/models/mixtral.py

[30bad5c49] Woosuk Kwon 2023-12-12 Fix peak memory profiling (#2031)
0	5	vllm/utils.py
3	4	vllm/worker/worker.py

[3fefe271e] Simon Mo 2023-12-12 Update Dockerfile to build Megablocks (#2042)
15	3	Dockerfile

[6428f1d05] Megha Agarwal 2023-12-12 Support MPT with GQA (#1938)
8	4	vllm/model_executor/layers/attention.py
20	2	vllm/model_executor/models/mpt.py

[7e1b21daa] Woosuk Kwon 2023-12-12 Remove einops from requirements (#2049)
0	1	requirements-rocm.txt
0	1	requirements.txt

[cb3f30c60] Woosuk Kwon 2023-12-11 Upgrade transformers version to 4.36.0 (#2046)
1	1	requirements-rocm.txt
1	1	requirements.txt
3	3	vllm/model_executor/models/mixtral.py

[f3e024bec] Woosuk Kwon 2023-12-11 [CI/CD] Upgrade PyTorch version to v2.1.1 (#2045)
1	1	.github/workflows/publish.yml
1	1	pyproject.toml
2	2	requirements.txt

[31d2ab4af] Woosuk Kwon 2023-12-11 Remove python 3.10 requirement (#2040)
1	1	README.md
1	3	vllm/model_executor/models/mixtral.py

[eb1721285] Simon Mo 2023-12-11 Update Dockerfile to support Mixtral (#2027)
2	2	Dockerfile

[4dd4b5c53] Woosuk Kwon 2023-12-11 Bump up to v0.2.4 (#2034)
1	1	vllm/__init__.py

[6120e5aae] Woosuk Kwon 2023-12-11 Fix import error msg for megablocks (#2038)
4	4	vllm/model_executor/models/mixtral.py

[2eaa81b23] Ram 2023-12-12 Update README.md to add megablocks requirement for mixtral (#2033)
4	0	README.md

[81ce2a4b2] Woosuk Kwon 2023-12-11 [Minor] Fix type annotation in Mixtral (#2036)
1	1	vllm/model_executor/models/mixtral.py

[5dd80d377] Woosuk Kwon 2023-12-11 Fix latency benchmark script (#2035)
1	1	benchmarks/benchmark_latency.py

[beeee69bc] Woosuk Kwon 2023-12-11 Revert adding Megablocks (#2030)
0	1	requirements.txt

[9bf28d0b6] Ram 2023-12-12 Update requirements.txt for mixtral (#2029)
1	0	requirements.txt

[c0ce15dfb] Ikko Eltociear Ashimine 2023-12-12 Update run_on_sky.rst (#2025)
1	1	docs/source/serving/run_on_sky.rst

[b9bcdc715] Woosuk Kwon 2023-12-11 Change the load format to pt for Mixtral (#2028)
10	0	vllm/config.py

[4ff020398] Woosuk Kwon 2023-12-11 Minor fixes for Mixtral (#2015)
3	0	docs/source/models/supported_models.rst
2	6	vllm/model_executor/models/mixtral.py

[b5f882cc9] Pierre Stock 2023-12-11 Mixtral 8x7B support (#2011)
1	0	README.md
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
534	0	vllm/model_executor/models/mixtral.py

[2e8fc0d4c] Simon Mo 2023-12-10 Fix completion API echo and logprob combo (#1992)
9	4	vllm/entrypoints/openai/api_server.py

[dacaf5a40] wbn 2023-12-11 Replace head_mapping params with num_kv_heads to attention kernel. (#1997)
2	6	benchmarks/kernels/benchmark_paged_attention.py
15	16	csrc/attention/attention_kernels.cu
2	2	csrc/ops.h
2	5	tests/kernels/test_attention.py
5	8	vllm/model_executor/layers/attention.py

[24cde76a1] Woosuk Kwon 2023-12-10 [Minor] Add comment on skipping rope caches (#2004)
4	3	vllm/model_executor/models/llama.py

[1aa136151] Jin Shang 2023-12-10 Fix OpenAI server completion_tokens referenced before assignment (#1996)
3	4	vllm/entrypoints/openai/api_server.py

[fe470ae5a] Woosuk Kwon 2023-12-09 [Minor] Fix code style for baichuan (#2003)
4	3	vllm/model_executor/models/baichuan.py

[3a8c2381f] Jun Gao 2023-12-10 Fix for KeyError on Loading LLaMA (#1978)
4	0	vllm/model_executor/models/llama.py

[c85b80c2b] Simon Mo 2023-12-08 [Docker] Add cuda arch list as build option (#1950)
5	1	Dockerfile
8	0	docs/source/serving/deploying_with_docker.rst

[2b981012a] firebook 2023-12-09 Fix Baichuan2-7B-Chat (#1987)
6	2	vllm/model_executor/models/baichuan.py

[6ccc0bfff] TJian 2023-12-08 Merge EmbeddedLLM/vllm-rocm into vLLM main (#1836)
4	0	.gitignore
62	0	Dockerfile.rocm
2	0	README.md
4	3	csrc/activation_kernels.cu
21	13	csrc/attention/attention_kernels.cu
2	1	csrc/attention/attention_utils.cuh
16	3	csrc/attention/dtype_bfloat16.cuh
63	5	csrc/attention/dtype_float16.cuh
7	6	csrc/cache_kernels.cu
28	0	csrc/cuda_compat.h
3	0	csrc/cuda_utils_kernels.cu
2	0	csrc/ops.h
5	4	csrc/pos_encoding_kernels.cu
4	0	csrc/pybind.cpp
74	0	csrc/quantization/squeezellm/quant_cuda_kernel.cu
3	1	csrc/reduction_utils.cuh
143	0	docs/source/getting_started/amd-installation.rst
2	0	docs/source/index.rst
22	0	patch_xformers-0.0.22.post7.rocm.sh
17	0	requirements-rocm.txt
13	0	rocm_patch/commonpy_xformers-0.0.22.post7.rocm.patch
134	0	rocm_patch/flashpy_xformers-0.0.22.post7.rocm.patch
159	73	setup.py
35	4	vllm/config.py
7	1	vllm/engine/ray_utils.py
3	0	vllm/model_executor/layers/attention.py
9	3	vllm/model_executor/layers/quantization/squeezellm.py
24	0	vllm/model_executor/model_loader.py
5	1	vllm/utils.py

[c8e7eb1eb] Daya Khudia 2023-12-07 fix typo in getenv call (#1972)
1	1	setup.py

[24f60a54f] AguirreNicolas 2023-12-07 [Docker] Adding number of nvcc_threads during build as envar (#1893)
3	0	Dockerfile
1	1	docs/source/serving/deploying_with_docker.rst
2	1	setup.py

[42c02f589] gottlike 2023-12-07 Fix quickstart.rst typo jinja (#1964)
1	1	docs/source/getting_started/quickstart.rst

[ebede26eb] Jie Li 2023-12-08 Make InternLM follow `rope_scaling` in `config.json` (#1956)
4	1	vllm/model_executor/models/internlm.py

[d940ce497] Peter Götz 2023-12-06 Fix typo in adding_model.rst (#1947)
1	1	docs/source/models/adding_model.rst

[05ff90b69] Antoni Baum 2023-12-05 Save pytorch profiler output for latency benchmark (#1871)
25	9	benchmarks/benchmark_latency.py

[1d9b737e0] dancingpipi 2023-12-06 Support ChatGLMForConditionalGeneration (#1932)
1	0	vllm/model_executor/model_loader.py

[60dc62dc9] Roy 2023-12-04 add custom server params (#1868)
4	0	vllm/entrypoints/openai/api_server.py
4	0	vllm/entrypoints/openai/protocol.py
1	0	vllm/sampling_params.py

[0f90effc6] Woosuk Kwon 2023-12-03 Bump up to v0.2.3 (#1903)
1	1	vllm/__init__.py

[464dd985e] Woosuk Kwon 2023-12-03 Fix num_gpus when TP > 1 (#1852)
10	1	vllm/engine/async_llm_engine.py
5	1	vllm/engine/llm_engine.py

[c07a44285] Massimiliano Pronesti 2023-12-03 chore(examples-docs): upgrade to OpenAI V1  (#1785)
19	8	docs/source/getting_started/quickstart.rst
15	11	examples/openai_chatcompletion_client.py
13	9	examples/openai_completion_client.py

[cd3aa153a] Woosuk Kwon 2023-12-02 Fix broken worker test (#1900)
14	9	tests/worker/{test_worker.py => test_model_runner.py}

[9b294976a] Woosuk Kwon 2023-12-02 Add PyTorch-native implementation of custom layers (#1898)
10	17	tests/kernels/test_activation.py
24	35	tests/kernels/test_layernorm.py
24	133	tests/kernels/test_pos_encoding.py
18	0	vllm/model_executor/layers/activation.py
20	0	vllm/model_executor/layers/layernorm.py
54	0	vllm/model_executor/layers/rotary_embedding.py

[5313c2cb8] Simon Mo 2023-12-02 Add Production Metrics in Prometheus format (#1890)
1	0	docs/source/index.rst
13	0	docs/source/serving/metrics.rst
1	0	requirements.txt
13	2	vllm/engine/llm_engine.py
51	0	vllm/engine/metrics.py
10	0	vllm/entrypoints/openai/api_server.py

[5f09cbdb6] Woosuk Kwon 2023-12-02 Fix broken sampler tests (#1896)
37	20	tests/samplers/test_sampler.py
4	1	vllm/worker/model_runner.py

[4cefa9b49] Simon Mo 2023-12-02 [Docs] Update the AWQ documentation to highlight performance issue (#1883)
6	0	docs/source/quantization/auto_awq.rst

[f86bd6190] Jerry 2023-12-01 Fix the typo in SamplingParams' docstring (#1886)
1	1	vllm/sampling_params.py

[e5452ddfd] Woosuk Kwon 2023-11-30 Normalize head weights for Baichuan 2 (#1876)
1	1	README.md
1	1	docs/source/models/supported_models.rst
11	0	vllm/model_executor/models/baichuan.py

[d06980dfa] Woosuk Kwon 2023-11-30 Fix Baichuan tokenizer error (#1874)
13	0	vllm/transformers_utils/tokenizer.py
5	0	vllm/transformers_utils/tokenizers/__init__.py
263	0	vllm/transformers_utils/tokenizers/baichuan.py

[66785cc05] Adam Brusselback 2023-11-30 Support chat template and `echo` for chat API (#1756)
53	1	docs/source/getting_started/quickstart.rst
29	0	examples/template_alpaca.jinja
2	0	examples/template_chatml.jinja
30	0	examples/template_inkbot.jinja
119	0	tests/async_engine/test_openai_server.py
204	179	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/protocol.py

[05a38612b] Massimiliano Pronesti 2023-11-30 docs: add instruction for langchain (#1162)
1	0	docs/source/index.rst
31	0	docs/source/serving/serving_with_langchain.rst

[d27f4bae3] Roy 2023-12-01 Fix rope cache key error (#1867)
2	1	vllm/model_executor/layers/rotary_embedding.py

[8d8c2f6ff] aisensiy 2023-12-01 Support max-model-len argument for throughput benchmark (#1858)
10	1	benchmarks/benchmark_throughput.py

[51d3cb951] Woosuk Kwon 2023-11-30 Remove max_num_seqs in latency benchmark script (#1855)
0	1	benchmarks/benchmark_latency.py

[e74b1736a] Woosuk Kwon 2023-11-29 Add profile option to latency benchmark script (#1839)
25	14	benchmarks/benchmark_latency.py

[f07c1ceaa] Allen 2023-11-30 [FIX] Fix docker build error (#1831) (#1832)
5	0	Dockerfile
2	1	pyproject.toml
6	0	requirements-build.txt

[63b2206ad] Jee Li 2023-11-30 Avoid multiple instantiations of the RoPE class (#1828)
8	0	vllm/model_executor/layers/rotary_embedding.py

[27feead2f] Woosuk Kwon 2023-11-29 Refactor Worker & InputMetadata (#1843)
6	0	vllm/config.py
4	3	vllm/engine/arg_utils.py
0	2	vllm/engine/llm_engine.py
2	0	vllm/model_executor/__init__.py
16	65	vllm/model_executor/input_metadata.py
3	11	vllm/model_executor/layers/attention.py
60	55	vllm/model_executor/layers/sampler.py
10	2	vllm/model_executor/models/aquila.py
10	2	vllm/model_executor/models/baichuan.py
10	2	vllm/model_executor/models/bloom.py
10	2	vllm/model_executor/models/chatglm.py
10	3	vllm/model_executor/models/falcon.py
10	2	vllm/model_executor/models/gpt2.py
10	2	vllm/model_executor/models/gpt_bigcode.py
10	2	vllm/model_executor/models/gpt_j.py
10	2	vllm/model_executor/models/gpt_neox.py
10	2	vllm/model_executor/models/internlm.py
10	2	vllm/model_executor/models/llama.py
10	2	vllm/model_executor/models/mistral.py
10	2	vllm/model_executor/models/mpt.py
10	2	vllm/model_executor/models/opt.py
27	26	vllm/model_executor/models/phi_1_5.py
10	2	vllm/model_executor/models/qwen.py
10	2	vllm/model_executor/models/yi.py
43	0	vllm/model_executor/sampling_metadata.py
334	0	vllm/worker/model_runner.py
13	248	vllm/worker/worker.py

[c78219566] Michael McCulloch 2023-11-29 Disable Logs Requests should Disable Logging of requests. (#1779)
0	2	vllm/entrypoints/openai/api_server.py

[0f621c2c7] Simon Mo 2023-11-29 [Docs] Add information about using shared memory in docker (#1845)
1	1	docs/source/models/adding_model.rst
9	1	docs/source/serving/deploying_with_docker.rst

[a9e457426] Woosuk Kwon 2023-11-29 Refactor Attention (#1840)
191	361	vllm/model_executor/layers/attention.py
2	2	vllm/model_executor/layers/rotary_embedding.py
14	10	vllm/model_executor/models/aquila.py
16	16	vllm/model_executor/models/baichuan.py
5	3	vllm/model_executor/models/bloom.py
10	9	vllm/model_executor/models/chatglm.py
19	20	vllm/model_executor/models/falcon.py
11	10	vllm/model_executor/models/gpt_j.py
10	8	vllm/model_executor/models/gpt_neox.py
10	8	vllm/model_executor/models/internlm.py
15	10	vllm/model_executor/models/llama.py
17	11	vllm/model_executor/models/mistral.py
5	3	vllm/model_executor/models/mpt.py
10	8	vllm/model_executor/models/phi_1_5.py
11	9	vllm/model_executor/models/qwen.py
14	10	vllm/model_executor/models/yi.py

[0229c386c] FlorianJoncour 2023-11-29 Better integration with Ray Serve (#1821)
3	3	vllm/engine/llm_engine.py
2	2	vllm/engine/ray_utils.py

[a7b3e3307] Woosuk Kwon 2023-11-29 [Fix] Fix RoPE in ChatGLM-32K (#1841)
5	0	vllm/model_executor/models/chatglm.py

[e19a64c7e] Zhuohan Li 2023-11-28 [FIX] Fix formatting error in main branch (#1822)
[1cb4ad8de] Zhuohan Li 2023-11-29 [FIX] Fix formatting error
0	1	vllm/core/block_manager.py

[6ed068a71] explainerauthors 2023-11-28 Use the type BlockTable (#1791)
5	5	vllm/core/block_manager.py

[708e6c18b] Zhuohan Li 2023-11-28 [FIX] Fix class naming (#1803)
4	4	vllm/engine/llm_engine.py
3	3	vllm/model_executor/layers/sampler.py
10	10	vllm/sequence.py

[b94389048] Woosuk Kwon 2023-11-28 Fix OPT param names (#1819)
3	0	vllm/model_executor/models/opt.py

[a1125ad4d] explainerauthors 2023-11-28 Correct comments in parallel_state.py (#1818)
2	2	vllm/model_executor/parallel_utils/parallel_state.py

[a8b150c59] ljss 2023-11-28 Init model on GPU to reduce CPU memory footprint (#1796)
2	3	vllm/model_executor/model_loader.py

[665cbcec4] Yunmo Chen 2023-11-27 Added echo function to OpenAI API server. (#1504)
70	22	vllm/entrypoints/openai/api_server.py
1	2	vllm/entrypoints/openai/protocol.py

[7c600440f] Woosuk Kwon 2023-11-23 Fix model docstrings (#1764)
1	5	vllm/model_executor/models/aquila.py
1	5	vllm/model_executor/models/baichuan.py
1	5	vllm/model_executor/models/bloom.py
1	5	vllm/model_executor/models/chatglm.py
1	5	vllm/model_executor/models/gpt2.py
1	5	vllm/model_executor/models/gpt_bigcode.py
1	5	vllm/model_executor/models/gpt_j.py
1	5	vllm/model_executor/models/gpt_neox.py
1	5	vllm/model_executor/models/llama.py
1	5	vllm/model_executor/models/mistral.py
1	5	vllm/model_executor/models/opt.py
1	5	vllm/model_executor/models/phi_1_5.py
1	5	vllm/model_executor/models/qwen.py
1	5	vllm/model_executor/models/yi.py

[e0c6f556e] Yanming W 2023-11-23 [Build] Avoid building too many extensions (#1624)
3	3	benchmarks/kernels/benchmark_paged_attention.py
0	28	csrc/activation.cpp
0	42	csrc/attention.cpp
0	19	csrc/{cache.cpp => cache.h}
0	13	csrc/cuda_utils.cpp
5	0	csrc/cuda_utils.h
0	24	csrc/layernorm.cpp
75	0	csrc/ops.h
0	16	csrc/pos_encoding.cpp
80	0	csrc/pybind.cpp
0	19	csrc/quantization.cpp
10	72	setup.py
4	4	tests/kernels/test_activation.py
3	3	tests/kernels/test_attention.py
1	1	tests/kernels/test_cache.py
2	2	tests/kernels/test_layernorm.py
2	2	tests/kernels/test_pos_encoding.py
4	4	vllm/model_executor/layers/activation.py
4	4	vllm/model_executor/layers/attention.py
3	3	vllm/model_executor/layers/layernorm.py
2	3	vllm/model_executor/layers/quantization/awq.py
2	3	vllm/model_executor/layers/quantization/squeezellm.py
4	5	vllm/model_executor/layers/rotary_embedding.py
1	1	vllm/utils.py
1	1	vllm/worker/cache_engine.py

[de23687d1] ljss 2023-11-23 Fix repetition penalty aligned with huggingface (#1577)
47	29	vllm/model_executor/layers/sampler.py
3	3	vllm/sampling_params.py

[4cea74c73] ljss 2023-11-23 Set top_p=0 and top_k=-1 in greedy sampling (#1748)
2	4	vllm/sampling_params.py

[a921d8be9] Casper 2023-11-22 [DOCS] Add engine args documentation (#1741)
1	0	docs/source/index.rst
114	0	docs/source/models/engine_args.rst
4	0	vllm/engine/arg_utils.py

[094f716bf] 陈序 2023-11-22 Add stop_token_ids in SamplingParams.__repr__ (#1745)
1	0	vllm/sampling_params.py

[7d761fe3c] Zhuohan Li 2023-11-20 [FIX] Fix the case when `input_is_parallel=False` for `ScaledActivation` (#1737)
6	4	vllm/model_executor/layers/activation.py

[cf35d8f3d] Woosuk Kwon 2023-11-20 [BugFix] Fix TP support for AWQ (#1731)
35	11	vllm/model_executor/layers/activation.py
3	3	vllm/model_executor/models/opt.py

[4bb6b6718] boydfd 2023-11-21 fix RAM OOM when load large models in tensor parallel mode. (#1395)
2	0	vllm/config.py
9	1	vllm/engine/arg_utils.py
39	6	vllm/engine/llm_engine.py
2	0	vllm/worker/worker.py

[819b18e7b] ljss 2023-11-21 Rewrite torch.repeat_interleave to remove cpu synchronization (#1599)
20	10	vllm/model_executor/layers/attention.py

[19849db57] Zhuofan 2023-11-21 [Fix] Fix bugs in scheduler (#1727)
2	1	vllm/core/scheduler.py

[3d4ceb292] 陈序 2023-11-21 Fix hanging in the scheduler caused by long prompts (#1534)
24	3	vllm/core/block_manager.py
12	2	vllm/core/scheduler.py

[f5a37c6c6] Woosuk Kwon 2023-11-20 [BugFix] Fix a bug in loading safetensors (#1732)
1	1	vllm/model_executor/weight_utils.py

[32c927b53] Zhuohan Li 2023-11-20 [FIX] Update the doc link in README.md (#1730)
1	1	README.md

[5ffc0d13a] Simon Mo 2023-11-20 Migrate linter from `pylint` to `ruff` (#1665)
5	5	.github/workflows/{pylint.yml => ruff.yml}
0	434	.pylintrc
2	3	benchmarks/benchmark_throughput.py
8	8	format.sh
24	0	pyproject.toml
1	1	requirements-dev.txt
8	6	setup.py
0	1	tests/async_engine/api_server_async_engine.py
2	5	tests/async_engine/test_api_server.py
0	1	tests/conftest.py
3	4	tests/engine/test_detokenize.py
1	1	tests/kernels/test_attention.py
4	5	tests/samplers/test_sampler.py
0	1	tests/worker/test_worker.py
1	1	vllm/core/scheduler.py
2	2	vllm/engine/llm_engine.py
1	2	vllm/engine/ray_utils.py
8	12	vllm/entrypoints/llm.py
3	5	vllm/entrypoints/openai/api_server.py
10	11	vllm/model_executor/layers/activation.py
0	1	vllm/model_executor/layers/attention.py
1	1	vllm/model_executor/layers/quantization/awq.py
1	1	vllm/model_executor/model_loader.py
1	4	vllm/model_executor/models/aquila.py
1	4	vllm/model_executor/models/baichuan.py
1	4	vllm/model_executor/models/bloom.py
1	4	vllm/model_executor/models/chatglm.py
1	4	vllm/model_executor/models/falcon.py
1	4	vllm/model_executor/models/gpt2.py
1	4	vllm/model_executor/models/gpt_bigcode.py
2	8	vllm/model_executor/models/gpt_j.py
1	4	vllm/model_executor/models/gpt_neox.py
1	4	vllm/model_executor/models/internlm.py
1	4	vllm/model_executor/models/llama.py
1	4	vllm/model_executor/models/mistral.py
5	8	vllm/model_executor/models/mpt.py
1	4	vllm/model_executor/models/opt.py
1	4	vllm/model_executor/models/phi_1_5.py
1	4	vllm/model_executor/models/qwen.py
1	4	vllm/model_executor/models/yi.py
4	6	vllm/model_executor/weight_utils.py
1	1	vllm/transformers_utils/config.py
8	8	vllm/transformers_utils/configs/mpt.py
1	1	vllm/utils.py
1	4	vllm/worker/worker.py

[112627e8b] Wen Sun 2023-11-20 [Docs] Fix the code block's format in deploying_with_docker page (#1722)
1	1	docs/source/serving/deploying_with_docker.rst

[37c1e3c21] Simon Mo 2023-11-19 Documentation about official docker image (#1709)
16	2	docs/source/serving/deploying_with_docker.rst

[06e9ebebd] Woosuk Kwon 2023-11-18 Add instructions to install vLLM+cu118 (#1717)
19	5	docs/source/getting_started/installation.rst

[c5f7740d8] Woosuk Kwon 2023-11-18 Bump up to v0.2.2 (#1689)
1	1	vllm/__init__.py

[be66d9b12] Woosuk Kwon 2023-11-18 Fix warning msg on quantization (#1715)
3	3	vllm/config.py

[e1054247b] ljss 2023-11-19 [Optimization] Implement fused add rmsnorm (#1667)
10	0	csrc/layernorm.cpp
55	0	csrc/layernorm_kernels.cu
15	1	vllm/model_executor/layers/layernorm.py
15	10	vllm/model_executor/models/baichuan.py
15	10	vllm/model_executor/models/internlm.py
15	10	vllm/model_executor/models/llama.py
15	10	vllm/model_executor/models/mistral.py
13	10	vllm/model_executor/models/qwen.py
13	10	vllm/model_executor/models/yi.py

[8d17774f9] Woosuk Kwon 2023-11-18 Add AWQ support for all models (#1714)
47	5	vllm/model_executor/layers/activation.py
3	0	vllm/model_executor/layers/quantization/awq.py
8	0	vllm/model_executor/layers/quantization/base_config.py
3	0	vllm/model_executor/layers/quantization/squeezellm.py
3	2	vllm/model_executor/models/bloom.py
4	1	vllm/model_executor/models/falcon.py
3	1	vllm/model_executor/models/gpt2.py
3	1	vllm/model_executor/models/gpt_bigcode.py
3	1	vllm/model_executor/models/gpt_j.py
3	1	vllm/model_executor/models/gpt_neox.py
2	1	vllm/model_executor/models/mpt.py
5	3	vllm/model_executor/models/opt.py
3	1	vllm/model_executor/models/phi_1_5.py

[e946260cf] twaka 2023-11-19 use get_tensor in safe_open (#1696)
3	8	vllm/model_executor/weight_utils.py

[edb305584] liuyhwangyh 2023-11-18 Support download models from www.modelscope.cn (#1588)
27	0	docs/source/getting_started/quickstart.rst
14	0	docs/source/models/supported_models.rst
13	1	vllm/config.py
4	3	vllm/entrypoints/openai/api_server.py

[bb00f66e1] Woosuk Kwon 2023-11-17 Use `quantization_config` in hf config (#1695)
24	8	vllm/config.py
1	0	vllm/model_executor/model_loader.py
9	2	vllm/model_executor/weight_utils.py

[e87557b06] Roy 2023-11-18 Support Min P Sampler (#1642)
31	4	vllm/model_executor/layers/sampler.py
9	0	vllm/sampling_params.py

[dcc543a29] Zhuofan 2023-11-18 [Minor] Fix comment (#1704)
1	1	benchmarks/benchmark_throughput.py

[0fc280b06] Zhuohan Li 2023-11-16 Update the adding-model doc according to the new refactor (#1692)
13	10	docs/source/models/adding_model.rst

[20d0699d4] Zhuohan Li 2023-11-16 [Fix] Fix comm test (#1691)
2	1	tests/distributed/test_comm_ops.py

[686f5e321] Iskren Ivov Chernev 2023-11-17 Return usage for openai streaming requests (#1663)
26	3	vllm/entrypoints/openai/api_server.py
3	0	vllm/entrypoints/openai/protocol.py

[415d10952] Zhuohan Li 2023-11-16 [Fix] Update Supported Models List (#1690)
2	0	README.md
9	0	docs/source/models/supported_models.rst

[521b35f79] maximzubkov 2023-11-16 Support Microsoft Phi 1.5 (#1664)
1	0	README.md
1	0	requirements.txt
1	0	tests/models/test_models.py
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
314	0	vllm/model_executor/models/phi_1_5.py

[cb08cd0d7] Simon Mo 2023-11-16 [Minor] Fix duplication of ignored seq group in engine step (#1666)
27	0	tests/test_regression.py
1	1	vllm/engine/llm_engine.py

[2a2c135b4] twaka 2023-11-17 Fix loading error when safetensors contains empty tensor (#1687)
6	1	vllm/model_executor/weight_utils.py

[65ea2ddf1] Aaron Pham 2023-11-16 feat(config): support parsing torch.dtype (#1641)
18	12	vllm/config.py

[b514d3c49] Megha Agarwal 2023-11-16 Revert `MptConfig` to `MPTConfig`   (#1668)
2	2	vllm/model_executor/model_loader.py
2	2	vllm/model_executor/models/__init__.py
20	20	vllm/model_executor/models/mpt.py
2	2	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
232	0	vllm/transformers_utils/configs/mpt.py

[7076fa1c9] Zhuohan Li 2023-11-15 TP/quantization/weight loading refactor part 2 - Refactor quantized linear logic and extend quantization support to all models (#1622)
30	19	vllm/config.py
2	2	vllm/engine/async_llm_engine.py
541	0	vllm/model_executor/layers/linear.py
22	0	vllm/model_executor/layers/quantization/__init__.py
155	0	vllm/model_executor/layers/quantization/awq.py
56	0	vllm/model_executor/layers/quantization/base_config.py
121	0	vllm/model_executor/layers/quantization/squeezellm.py
0	41	vllm/model_executor/layers/quantized_linear/__init__.py
0	106	vllm/model_executor/layers/quantized_linear/awq.py
0	84	vllm/model_executor/layers/quantized_linear/squeezellm.py
139	0	vllm/model_executor/layers/vocab_parallel_embedding.py
4	16	vllm/model_executor/model_loader.py
64	102	vllm/model_executor/models/aquila.py
65	89	vllm/model_executor/models/baichuan.py
69	63	vllm/model_executor/models/bloom.py
79	111	vllm/model_executor/models/chatglm.py
93	150	vllm/model_executor/models/falcon.py
52	79	vllm/model_executor/models/gpt2.py
63	126	vllm/model_executor/models/gpt_bigcode.py
69	53	vllm/model_executor/models/gpt_j.py
70	60	vllm/model_executor/models/gpt_neox.py
62	88	vllm/model_executor/models/internlm.py
53	156	vllm/model_executor/models/llama.py
62	148	vllm/model_executor/models/mistral.py
52	59	vllm/model_executor/models/mpt.py
71	62	vllm/model_executor/models/opt.py
80	119	vllm/model_executor/models/qwen.py
53	156	vllm/model_executor/models/yi.py
0	303	vllm/model_executor/parallel_utils/layers.py
2	24	vllm/model_executor/parallel_utils/utils.py
0	22	vllm/model_executor/quantization_utils/__init__.py
0	76	vllm/model_executor/quantization_utils/awq.py
0	85	vllm/model_executor/quantization_utils/base.py
0	65	vllm/model_executor/quantization_utils/squeezellm.py
22	0	vllm/model_executor/utils.py
8	44	vllm/model_executor/weight_utils.py

[660a7fcfa] Woosuk Kwon 2023-11-14 Add DeepSpeed MII backend to benchmark script (#1649)
71	12	benchmarks/benchmark_throughput.py

[054072bee] Woosuk Kwon 2023-11-12 [Minor] Move RoPE selection logic to `get_rope` (#1633)
3	33	vllm/model_executor/layers/attention.py
44	1	vllm/model_executor/layers/rotary_embedding.py

[eb825c1e7] lirui 2023-11-13 Fix #1474 - AssertionError:assert param_slice.shape == loaded_weight.shape (#1631)
1	1	vllm/model_executor/models/gpt_j.py

[1b290ace4] Dominik Schwabe 2023-11-11 Run default _AsyncLLMEngine._run_workers_async in threadpool (#1628)
6	7	vllm/engine/async_llm_engine.py

[0d578228c] Sin 2023-11-10 config parser: add ChatGLM2 seq_length to `_get_and_verify_max_len` (#1617)
2	0	vllm/config.py

[aebfcb262] GhaziSyed 2023-11-09 Dockerfile: Upgrade Cuda to 12.1 (#1609)
4	4	Dockerfile

[ab9e8488d] forpanyang 2023-11-10 Add Yi model to quantization support (#1600)
1	0	vllm/model_executor/model_loader.py

[fd58b73a4] Woosuk Kwon 2023-11-09 Build CUDA11.8 wheels for release (#1596)
1	1	.github/workflows/publish.yml
13	2	setup.py

[8efe23f15] Yanming W 2023-11-09 Fix input_metadata.selected_token_indices in worker prepare_inputs (#1546)
44	0	tests/worker/test_worker.py
3	1	vllm/worker/worker.py

[06458a0b4] Zhuohan Li 2023-11-08 Upgrade to CUDA 12 (#1527)
4	4	.github/workflows/publish.yml
3	0	.github/workflows/scripts/build.sh
5	0	.github/workflows/scripts/cuda-install.sh
1	1	pyproject.toml
2	2	requirements.txt

[1a2bbc930] GoHomeToMacDonal 2023-11-07 ChatGLM Support (#1261)
4	0	vllm/config.py
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
408	0	vllm/model_executor/models/chatglm.py
3	2	vllm/transformers_utils/config.py
4	2	vllm/transformers_utils/configs/__init__.py
68	0	vllm/transformers_utils/configs/chatglm.py

[e7f579eb9] Roy 2023-11-07 Support Yi model (#1567)
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
426	0	vllm/model_executor/models/yi.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
64	0	vllm/transformers_utils/configs/yi.py

[851699949] Casper 2023-11-05 Add Quantization and AutoAWQ to docs (#1235)
6	0	docs/source/index.rst
69	0	docs/source/quantization/auto_awq.rst

[9f669a9a7] Antoni Baum 2023-11-03 Support YaRN models (#1264)
6	6	csrc/activation_kernels.cu
1	1	csrc/pos_encoding_kernels.cu
3	0	vllm/config.py
14	1	vllm/model_executor/layers/attention.py
104	0	vllm/model_executor/layers/rotary_embedding.py

[555bdcc5a] Noam Gat 2023-11-03 Added logits processor API to sampling params (#1469)
34	0	tests/samplers/test_sampler.py
24	0	vllm/model_executor/layers/sampler.py
12	2	vllm/sampling_params.py

[54ca1ba71] lots-o 2023-11-04 docs: add description (#1553)
1	0	vllm/outputs.py

[9738b84a0] Antoni Baum 2023-11-01 Force paged attention v2 for long contexts (#1510)
3	1	vllm/model_executor/layers/attention.py
1	28	vllm/worker/worker.py

[1fe099002] Woosuk Kwon 2023-11-01 Remove `MPTConfig` (#1529)
2	2	vllm/model_executor/model_loader.py
2	2	vllm/model_executor/models/__init__.py
20	20	vllm/model_executor/models/mpt.py
2	2	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	74	vllm/transformers_utils/configs/mpt.py

[7e90a2d11] Fluder-Paradyne 2023-11-01 Add `/health` Endpoint for both Servers  (#1540)
6	0	vllm/entrypoints/api_server.py
7	1	vllm/entrypoints/openai/api_server.py

[5687d584f] ljss 2023-11-01 [BugFix] Set engine_use_ray=True when TP>1 (#1531)
1	1	vllm/engine/async_llm_engine.py

[cf8849f2d] Wenfei Yan 2023-10-31 Add `MptForCausalLM` key in model_loader (#1526)
2	0	vllm/model_executor/model_loader.py

[e575df33b] Cade Daniel 2023-10-31 [Small] Formatter only checks lints in changed files (#1528)
35	1	format.sh
0	0	tests/__init__.py

[0ce8647dc] Woosuk Kwon 2023-10-31 Fix integer overflows in attention & cache ops (#1514)
8	2	csrc/attention/attention_kernels.cu
36	36	csrc/cache_kernels.cu
1	1	tests/kernels/test_attention.py
7	7	tests/kernels/test_cache.py
1	1	vllm/worker/worker.py

[9cabcb764] Stephen Krider 2023-10-31 Add Dockerfile (#1350)
72	0	Dockerfile
1	0	docs/source/index.rst
21	0	docs/source/serving/deploying_with_docker.rst
1	0	requirements-dev.txt
6	2	setup.py

[7b895c597] Zhuohan Li 2023-10-31 [Fix] Fix duplicated logging messages (#1524)
1	0	vllm/logger.py

[7013a8017] Dan Lord 2023-10-30 Add support for `spaces_between_special_tokens`
3	3	vllm/engine/llm_engine.py
4	0	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/protocol.py
7	1	vllm/sampling_params.py
12	3	vllm/transformers_utils/tokenizer.py

[79a30912b] Jared Roesch 2023-10-30 Add py.typed so consumers of vLLM can get type checking (#1509)
1	0	setup.py
2	0	vllm/py.typed

[2f3d36a8a] Adam Brusselback 2023-10-30 Fix logging so we actually get info level entries in the log. (#1494)
5	1	vllm/logger.py

[ac8d36f3e] iongpt 2023-10-30 Refactor LLMEngine demo script for clarity and modularity (#1413)
25	14	examples/llm_engine_example.py

[15f563236] Antoni Baum 2023-10-30 Delay GPU->CPU sync in sampling (#1337)
18	11	vllm/model_executor/input_metadata.py
3	34	vllm/model_executor/layers/sampler.py
45	2	vllm/worker/worker.py

[aa9af07ca] Woosuk Kwon 2023-10-30 Fix bias in InternLM (#1501)
4	2	vllm/model_executor/models/internlm.py

[69be658bb] ljss 2023-10-30 Support repetition_penalty (#1424)
25	7	vllm/model_executor/layers/sampler.py
10	0	vllm/sampling_params.py

[beac8dd46] Ricardo Lu 2023-10-29 fix: don't skip first special token. (#1497)
5	1	vllm/transformers_utils/tokenizer.py

[28b47d1e4] Qing 2023-10-29 Add rope_scaling to Aquila model (#1457)
5	1	vllm/model_executor/models/aquila.py

[1f24755bf] chooper1 2023-10-22 Support SqueezeLLM (#1326)
1	1	benchmarks/benchmark_latency.py
1	1	benchmarks/benchmark_throughput.py
8	4	csrc/quantization.cpp
148	0	csrc/quantization/squeezellm/quant_cuda_kernel.cu
1	0	setup.py
1	1	vllm/config.py
1	1	vllm/engine/arg_utils.py
4	0	vllm/model_executor/layers/quantized_linear/__init__.py
9	5	vllm/model_executor/layers/quantized_linear/awq.py
84	0	vllm/model_executor/layers/quantized_linear/squeezellm.py
15	9	vllm/model_executor/models/llama.py
15	9	vllm/model_executor/models/mistral.py
2	0	vllm/model_executor/quantization_utils/__init__.py
7	3	vllm/model_executor/quantization_utils/awq.py
16	6	vllm/model_executor/quantization_utils/base.py
65	0	vllm/model_executor/quantization_utils/squeezellm.py

[bf31d3606] Thiago Salvatore 2023-10-21 Pin pydantic dependency versions (#1429)
1	1	requirements.txt

[d189170b6] Wang Ran (汪然) 2023-10-20 remove useless statements (#1408)
0	1	vllm/model_executor/input_metadata.py

[f61dc8072] Light Lin 2023-10-20 Fix type hints (#1427)
1	1	vllm/core/scheduler.py

[f8a1e39fa] Woosuk Kwon 2023-10-17 [BugFix] Define `__eq__` in SequenceGroupOutputs (#1389)
6	0	vllm/sequence.py

[a13243520] Wang Ran (汪然) 2023-10-17 Fix typo (#1383)
3	3	vllm/model_executor/input_metadata.py

[952486770] Woosuk Kwon 2023-10-16 Add Mistral 7B to `test_models` (#1366)
3	2	tests/models/test_models.py

[c1376e0f8] Woosuk Kwon 2023-10-16 Change scheduler & input tensor shape (#1381)
14	14	csrc/activation_kernels.cu
7	2	csrc/cache_kernels.cu
6	6	csrc/layernorm_kernels.cu
11	11	csrc/pos_encoding_kernels.cu
3	0	vllm/config.py
11	4	vllm/core/scheduler.py
7	1	vllm/engine/arg_utils.py
5	6	vllm/model_executor/input_metadata.py
8	12	vllm/model_executor/layers/activation.py
71	95	vllm/model_executor/layers/attention.py
2	2	vllm/model_executor/layers/quantized_linear/awq.py
2	1	vllm/model_executor/layers/sampler.py
34	25	vllm/worker/worker.py

[651c614aa] Zhuohan Li 2023-10-16 Bump up the version to v0.2.1 (#1355)
1	1	vllm/__init__.py

[d3a5bd9fb] Woosuk Kwon 2023-10-16 Fix sampler test (#1379)
4	4	tests/samplers/test_sampler.py

[e8ef4c082] Woosuk Kwon 2023-10-16 Fix PyTorch index URL in workflow (#1378)
1	1	.github/workflows/scripts/pytorch-install.sh

[348897af3] Woosuk Kwon 2023-10-16 Fix PyTorch version to 2.0.1 in workflow (#1377)
3	2	.github/workflows/publish.yml
3	2	.github/workflows/scripts/pytorch-install.sh

[9d9072a06] Zhuohan Li 2023-10-16 Implement prompt logprobs & Batched topk for computing logprobs (#1328)
1	1	examples/llm_engine_example.py
1	1	tests/async_engine/test_request_tracker.py
33	0	tests/conftest.py
55	0	tests/samplers/test_logprobs.py
1	1	vllm/config.py
13	7	vllm/engine/llm_engine.py
1	1	vllm/model_executor/layers/attention.py
203	107	vllm/model_executor/layers/sampler.py
1	1	vllm/model_executor/parallel_utils/communication_op.py
1	1	vllm/model_executor/parallel_utils/layers.py
1	1	vllm/model_executor/parallel_utils/utils.py
13	8	vllm/outputs.py
12	0	vllm/sampling_params.py
35	3	vllm/sequence.py

[928de4688] Woosuk Kwon 2023-10-16 Implement PagedAttention V2 (#1348)
197	0	benchmarks/kernels/benchmark_paged_attention.py
24	4	csrc/attention.cpp
413	71	csrc/attention/attention_kernels.cu
5	0	csrc/attention/dtype_bfloat16.cuh
54	17	tests/kernels/test_attention.py
71	47	vllm/model_executor/layers/attention.py

[29678cd21] Woosuk Kwon 2023-10-15 Minor fix on AWQ kernel launch (#1356)
4	3	csrc/quantization/awq/gemm_kernels.cu

[d0740dff1] Woosuk Kwon 2023-10-14 Fix error message on `TORCH_CUDA_ARCH_LIST` (#1239)
25	12	setup.py

[de8947289] Lu Wang 2023-10-13 Fix the issue for AquilaChat2-* models (#1339)
1	0	vllm/model_executor/model_loader.py
3	2	vllm/model_executor/models/aquila.py
6	0	vllm/transformers_utils/configs/aquila.py

[e7c8555d0] Woosuk Kwon 2023-10-13 Bump up transformers version & Remove MistralConfig (#1254)
2	2	requirements.txt
1	1	vllm/model_executor/models/__init__.py
1	1	vllm/model_executor/models/mistral.py
0	9	vllm/transformers_utils/config.py
0	2	vllm/transformers_utils/configs/__init__.py
0	66	vllm/transformers_utils/configs/mistral.py

[ec3b5ce9c] Antoni Baum 2023-10-13 Improve detokenization performance (#1338)
4	3	vllm/transformers_utils/tokenizer.py

[6368e777a] ldwang 2023-10-13 Add Aquila2 to README (#1331)
1	1	README.md

[875afe38a] Woosuk Kwon 2023-10-12 Add blacklist in model checkpoint (#1325)
11	1	vllm/model_executor/weight_utils.py

[ee8217e5b] amaleshvemula 2023-10-11 Add Mistral to quantization model list (#1278)
1	0	vllm/model_executor/model_loader.py

[980dd4a2c] CHU Tianxiang 2023-10-11 Fix overflow in awq kernel (#1295)
2	2	csrc/quantization/awq/gemm_kernels.cu

[828573684] twaka 2023-10-11 workaround of AWQ for Turing GPUs (#1252)
1	1	csrc/quantization/awq/dequantize.cuh
70	2	csrc/quantization/awq/gemm_kernels.cu
2	2	vllm/model_executor/quantization_utils/awq.py

[91fce82c6] yhlskt23 2023-10-11 change the timing of sorting logits (#1309)
16	24	vllm/model_executor/layers/sampler.py

[ac5cf86aa] Wang Ran (汪然) 2023-10-11 Fix `__repr__` of `SequenceOutputs` (#1311)
2	2	vllm/sequence.py

[6a6119554] yanxiyue 2023-10-11 lock torch version to 2.0.1 (#1290)
1	1	pyproject.toml
2	2	requirements.txt

[b95ee898f] Zhuohan Li 2023-10-09 [Minor] Fix comment in mistral.py (#1303)
1	1	vllm/model_executor/models/mistral.py

[9eed4d1f3] Zhuohan Li 2023-10-08 Update README.md (#1292)
1	39	README.md
-	-	docs/source/assets/figures/perf_a100_n1_dark.png
-	-	docs/source/assets/figures/perf_a100_n1_light.png
-	-	docs/source/assets/figures/perf_a100_n3_dark.png
-	-	docs/source/assets/figures/perf_a100_n3_light.png
-	-	docs/source/assets/figures/perf_a10g_n1_dark.png
-	-	docs/source/assets/figures/perf_a10g_n1_light.png
-	-	docs/source/assets/figures/perf_a10g_n3_dark.png
-	-	docs/source/assets/figures/perf_a10g_n3_light.png

[6b5296aa3] Zhuohan Li 2023-10-08 [FIX] Explain why the finished_reason of ignored sequences are length (#1289)
3	0	vllm/sequence.py

[ee92b58b3] Antoni Baum 2023-10-07 Move bfloat16 check to worker (#1259)
0	9	vllm/config.py
14	0	vllm/worker/worker.py

[09ff7f106] Yunfeng Bai 2023-10-07 API server support ipv4 / ipv6 dualstack (#1288)
1	1	examples/gradio_webserver.py
1	1	vllm/entrypoints/api_server.py
1	4	vllm/entrypoints/openai/api_server.py

[acbed3ef4] Antoni Baum 2023-10-02 Use monotonic time where appropriate (#1249)
2	2	benchmarks/benchmark_latency.py
4	4	benchmarks/benchmark_serving.py
4	4	benchmarks/benchmark_throughput.py
1	1	vllm/core/scheduler.py
2	1	vllm/engine/async_llm_engine.py
3	3	vllm/engine/llm_engine.py
2	2	vllm/entrypoints/openai/api_server.py

[66d18a7fb] Federico Cassano 2023-10-02 add support for tokenizer revision (#1163)
5	0	vllm/config.py
10	1	vllm/engine/arg_utils.py
2	0	vllm/engine/llm_engine.py
4	0	vllm/entrypoints/llm.py
2	0	vllm/transformers_utils/tokenizer.py

[ba0bfd40e] Zhuohan Li 2023-10-02 TP/quantization/weight loading refactor part 1 - Simplify parallel linear logic (#1181)
1	1	.github/workflows/pylint.yml
1	1	.github/workflows/yapf.yml
1	1	.pylintrc
2	3	format.sh
1	0	tests/async_engine/api_server_async_engine.py
3	0	tests/async_engine/test_api_server.py
2	2	tests/async_engine/test_async_llm_engine.py
11	11	tests/async_engine/test_request_tracker.py
1	0	tests/conftest.py
82	0	tests/distributed/test_comm_ops.py
1	0	tests/engine/test_detokenize.py
6	6	tests/kernels/test_activation.py
3	3	tests/kernels/test_cache.py
1	1	tests/kernels/test_pos_encoding.py
2	1	tests/samplers/test_sampler.py
2	2	vllm/model_executor/layers/quantized_linear/__init__.py
2	2	vllm/model_executor/layers/quantized_linear/awq.py
3	3	vllm/model_executor/layers/sampler.py
22	20	vllm/model_executor/models/aquila.py
22	20	vllm/model_executor/models/baichuan.py
16	13	vllm/model_executor/models/bloom.py
15	16	vllm/model_executor/models/falcon.py
27	22	vllm/model_executor/models/gpt2.py
34	29	vllm/model_executor/models/gpt_bigcode.py
34	27	vllm/model_executor/models/gpt_j.py
29	23	vllm/model_executor/models/gpt_neox.py
24	20	vllm/model_executor/models/internlm.py
4	8	vllm/model_executor/models/llama.py
4	8	vllm/model_executor/models/mistral.py
19	17	vllm/model_executor/models/mpt.py
28	23	vllm/model_executor/models/opt.py
5	9	vllm/model_executor/models/qwen.py
0	7	vllm/model_executor/parallel_utils/__init__.py
47	0	vllm/model_executor/parallel_utils/communication_op.py
303	0	vllm/model_executor/parallel_utils/layers.py
56	376	vllm/model_executor/parallel_utils/parallel_state.py
0	50	vllm/model_executor/parallel_utils/tensor_parallel/__init__.py
0	366	vllm/model_executor/parallel_utils/tensor_parallel/layers.py
0	281	vllm/model_executor/parallel_utils/tensor_parallel/mappings.py
0	164	vllm/model_executor/parallel_utils/tensor_parallel/random.py
9	9	vllm/model_executor/parallel_utils/{tensor_parallel => }/utils.py
0	6	vllm/model_executor/utils.py

[84e4e37d1] Woosuk Kwon 2023-10-02 [Minor] Fix type annotations (#1238)
1	1	vllm/model_executor/layers/sampler.py
2	3	vllm/sampling_params.py

[a60b35300] Zhuohan Li 2023-10-02 support sharding llama2-70b on more than 8 GPUs (#1209)
29	14	vllm/model_executor/models/llama.py

[ebe4d1db3] Liang 2023-10-02 Fix boundary check in paged attention kernel (#1241)
1	1	csrc/attention/attention_kernels.cu

[b5a10eb0e] kg6-sleipnir 2023-10-01 Added `dtype` arg to benchmarks (#1228)
10	0	benchmarks/benchmark_latency.py
12	1	benchmarks/benchmark_throughput.py

[0967102c6] Usama Ahmed 2023-09-29 fixing typo in `tiiuae/falcon-rw-7b` model name (#1226)
1	1	docs/source/models/supported_models.rst

[e2fb71ec9] Woosuk Kwon 2023-09-28 Bump up the version to v0.2.0 (#1212)
1	1	vllm/__init__.py

[f936657eb] Woosuk Kwon 2023-09-28 Provide default max model length (#1224)
11	7	vllm/config.py
1	1	vllm/engine/arg_utils.py
1	0	vllm/engine/llm_engine.py
1	1	vllm/entrypoints/openai/api_server.py

[6f88f762b] Woosuk Kwon 2023-09-28 Fix OOM in attention kernel test (#1223)
5	2	tests/kernels/test_attention.py

[202351d5b] Woosuk Kwon 2023-09-28 Add Mistral to supported model list (#1221)
1	0	README.md
3	0	docs/source/models/supported_models.rst

[2e8e49fce] Woosuk Kwon 2023-09-28 [Fix] Remove false assertion (#1222)
0	2	vllm/worker/worker.py

[a8e98aee0] Woosuk Kwon 2023-09-28 Fix Mistral model (#1220)
1	1	vllm/model_executor/models/mistral.py
9	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
15	13	vllm/worker/worker.py

[bb1ba58f0] Chris Bamford 2023-09-28 [Mistral] Mistral-7B-v0.1 support (#1196)
1	1	requirements.txt
2	0	vllm/config.py
30	11	vllm/core/block_manager.py
1	1	vllm/core/scheduler.py
3	3	vllm/engine/arg_utils.py
2	0	vllm/engine/llm_engine.py
20	1	vllm/model_executor/input_metadata.py
22	5	vllm/model_executor/layers/attention.py
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
404	0	vllm/model_executor/models/mistral.py
66	0	vllm/transformers_utils/configs/mistral.py
17	3	vllm/worker/worker.py

[7bedab574] Qing 2023-09-28 Add rope_scaling to Qwen (#1210)
11	10	vllm/model_executor/models/qwen.py

[20f7cc4cd] Dan Lord 2023-09-27 Add `skip_special_tokens` sampling params (#1186)
4	3	vllm/engine/llm_engine.py
2	0	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/protocol.py
6	1	vllm/sampling_params.py

[649aa730c] Danilo Peixoto 2023-09-27 Use standard extras for uvicorn (#1166)
1	1	requirements.txt

[a19bc5c62] Woosuk Kwon 2023-09-27 Automatically configure `max_num_batched_tokens` (#1198)
34	9	vllm/config.py
1	2	vllm/engine/arg_utils.py

[28e616c4e] Qing 2023-09-28 fix qwen-14b model (#1173)
7	7	vllm/model_executor/models/qwen.py
25	36	vllm/transformers_utils/configs/qwen.py

[30e775281] Wang Ran (汪然) 2023-09-28 fix typo (#1184)
2	2	vllm/engine/llm_engine.py
3	4	vllm/engine/ray_utils.py

[21877b0d7] Lily Liu 2023-09-27 Support Longchat and RoPE scaling (#555)
11	0	vllm/config.py
25	38	vllm/model_executor/layers/attention.py
169	0	vllm/model_executor/layers/rotary_embedding.py
6	2	vllm/model_executor/models/llama.py

[cf5cb1e33] Antoni Baum 2023-09-26 Allocate more shared memory to attention kernel (#1154)
5	0	csrc/attention/attention_kernels.cu
13	0	csrc/cuda_utils.cpp
14	0	csrc/cuda_utils_kernels.cu
11	0	setup.py
7	1	tests/kernels/test_attention.py
12	1	vllm/utils.py
25	1	vllm/worker/worker.py

[03ffd0a02] Woosuk Kwon 2023-09-26 Add comments on RoPE initialization (#1176)
9	1	vllm/model_executor/layers/attention.py

[a425bd9a9] Woosuk Kwon 2023-09-26 [Setup] Enable `TORCH_CUDA_ARCH_LIST` for selecting target GPUs (#1074)
73	38	setup.py

[bbbf86565] Wen Sun 2023-09-24 Align `max_tokens` behavior with openai (#852)
2	0	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/protocol.py

[9f6be8692] Woosuk Kwon 2023-09-23 Fix config for Falcon (#1164)
6	1	vllm/config.py
2	2	vllm/worker/cache_engine.py

[f18787794] Zhuohan Li 2023-09-23 [FIX] Simplify sampler logic (#1156)
10	37	vllm/model_executor/layers/sampler.py

[947b79414] Zhuohan Li 2023-09-22 [Sampler] Vectorized sampling (simplified) (#1048)
184	0	tests/samplers/test_sampler.py
281	180	vllm/model_executor/layers/sampler.py
16	0	vllm/sampling_params.py

[8d926e91f] Woosuk Kwon 2023-09-22 Announce the First vLLM Meetup (#1148)
9	0	README.md

[4ee52bb16] Nick Perez 2023-09-22 Docs: Fix broken link to openai example (#1145)
1	1	docs/source/getting_started/quickstart.rst

[7d7e3b78a] Woosuk Kwon 2023-09-21 Use `--ipc=host` in docker run for distributed inference (#1125)
2	1	docs/source/getting_started/installation.rst

[f98b745a8] Ricardo Lu 2023-09-22 feat: support stop_token_ids parameter. (#1097)
3	0	vllm/engine/llm_engine.py
2	0	vllm/entrypoints/openai/api_server.py
2	0	vllm/entrypoints/openai/protocol.py
8	0	vllm/sampling_params.py

[2d1e86f1b] Roy 2023-09-22 clean api code, remove redundant background task. (#1102)
2	8	vllm/entrypoints/api_server.py
5	19	vllm/entrypoints/openai/api_server.py

[1ac4ccf73] Woosuk Kwon 2023-09-21 Add float16 and float32 (#1115)
3	1	vllm/engine/arg_utils.py

[2ac4d5e2b] Woosuk Kwon 2023-09-21 Replace DtypeTensor (#1123)
15	5	vllm/worker/worker.py

[3302f0aef] Antoni Baum 2023-09-20 rope_theta and max_position_embeddings from config (#1096)
39	31	vllm/config.py
11	0	vllm/model_executor/models/aquila.py
16	4	vllm/model_executor/models/baichuan.py
11	6	vllm/model_executor/models/falcon.py
11	5	vllm/model_executor/models/gpt_j.py
10	2	vllm/model_executor/models/gpt_neox.py
16	4	vllm/model_executor/models/internlm.py
13	6	vllm/model_executor/models/llama.py
13	4	vllm/model_executor/models/qwen.py

[6f2dd6c37] Tanmay Verma 2023-09-20 Add documentation to Triton server tutorial (#983)
1	0	docs/source/index.rst
6	0	docs/source/serving/deploying_with_triton.rst

[bc0644574] Woosuk Kwon 2023-09-19 Add gpu_memory_utilization and swap_space to LLM (#1090)
19	3	vllm/entrypoints/llm.py

[400b8289f] Woosuk Kwon 2023-09-18 Add pyarrow to dependencies & Print warning on Ray import error (#1094)
1	0	requirements.txt
7	1	vllm/engine/ray_utils.py

[c1026311b] Zhuohan Li 2023-09-18 [Community] Add vLLM Discord server (#1086)
3	2	README.md

[2b1c116b5] Woosuk Kwon 2023-09-18 Add minimum capability requirement for AWQ (#1064)
8	0	csrc/quantization/awq/dequantize.cuh
16	2	csrc/quantization/awq/gemm_kernels.cu
8	0	vllm/model_executor/model_loader.py
5	0	vllm/model_executor/quantization_utils/awq.py
10	0	vllm/model_executor/quantization_utils/base.py

[cc796b135] Woosuk Kwon 2023-09-18 Convert before transpose (#1073)
3	2	vllm/model_executor/models/llama.py

[f029ef94d] Zhuohan Li 2023-09-18 Fix get_max_num_running_seqs for waiting and swapped seq groups (#1068)
13	2	vllm/sequence.py

[95592fa00] Roy 2023-09-19 align llm_engine and async_engine. (#1081)
4	5	vllm/engine/async_llm_engine.py

[fbe66e1d0] orellavie1212 2023-09-18 added support for quantize on LLM module (#1080)
5	0	vllm/entrypoints/llm.py

[90979c38f] Zhuohan Li 2023-09-17 [FIX] Don't initialize parameter by default (#1067)
4	4	vllm/model_executor/parallel_utils/tensor_parallel/layers.py

[e21d7687a] 陈序 2023-09-17 Fix hanging when prompt exceeds limit (#1029)
1	1	vllm/core/scheduler.py
9	12	vllm/engine/llm_engine.py

[ff36139ff] Antoni Baum 2023-09-17 Remove AsyncLLMEngine busy loop, shield background task (#1059)
1	0	requirements-dev.txt
80	0	tests/async_engine/test_async_llm_engine.py
21	0	tests/async_engine/test_request_tracker.py
52	18	vllm/engine/async_llm_engine.py

[e3e79e9e8] Woosuk Kwon 2023-09-16 Implement AWQ quantization support for LLaMA (#1032)
4	0	.gitignore
14	4	benchmarks/benchmark_latency.py
44	30	benchmarks/benchmark_throughput.py
15	0	csrc/quantization.cpp
79	0	csrc/quantization/awq/dequantize.cuh
477	0	csrc/quantization/awq/gemm_kernels.cu
14	0	setup.py
16	0	vllm/config.py
9	3	vllm/engine/arg_utils.py
1	0	vllm/engine/llm_engine.py
37	0	vllm/model_executor/layers/quantized_linear/__init__.py
102	0	vllm/model_executor/layers/quantized_linear/awq.py
28	2	vllm/model_executor/model_loader.py
91	34	vllm/model_executor/models/llama.py
47	130	vllm/model_executor/parallel_utils/tensor_parallel/layers.py
20	0	vllm/model_executor/quantization_utils/__init__.py
67	0	vllm/model_executor/quantization_utils/awq.py
65	0	vllm/model_executor/quantization_utils/base.py
48	5	vllm/model_executor/weight_utils.py

[b9fe4616f] Jerry Yang 2023-09-15 Abort when coroutine is cancelled (#1020)
3	2	vllm/engine/async_llm_engine.py

[64ca424e7] Woosuk Kwon 2023-09-14 Fix warning message on LLaMA FastTokenizer (#1037)
5	5	vllm/transformers_utils/tokenizer.py

[b5f93d063] Lukas Kreussel 2023-09-15 Only fail if logit_bias has actual values (#1045)
2	2	vllm/entrypoints/openai/api_server.py

[a58936966] Woosuk Kwon 2023-09-14 Add pandas to requirements.txt (#1047)
1	0	requirements.txt

[dd54a4b02] Antoni Baum 2023-09-14 Fix detokenization leaving special tokens (#1044)
11	4	tests/engine/test_detokenize.py
3	3	vllm/transformers_utils/tokenizer.py

[eda1a7cad] Woosuk Kwon 2023-09-13 Announce paper release (#1036)
14	1	README.md
1	0	docs/source/index.rst

[f04908cae] Zhuohan Li 2023-09-13 [FIX] Minor bug fixes (#1035)
3	2	vllm/model_executor/layers/sampler.py
1	1	vllm/sequence.py

[ab019eea7] Jasmond L 2023-09-14 Add Model Revision Support (#1014)
6	1	vllm/config.py
10	1	vllm/engine/arg_utils.py
3	1	vllm/engine/llm_engine.py
2	0	vllm/entrypoints/llm.py
1	1	vllm/model_executor/model_loader.py
3	2	vllm/model_executor/models/aquila.py
3	2	vllm/model_executor/models/baichuan.py
3	2	vllm/model_executor/models/bloom.py
3	2	vllm/model_executor/models/falcon.py
3	2	vllm/model_executor/models/gpt2.py
3	2	vllm/model_executor/models/gpt_bigcode.py
3	2	vllm/model_executor/models/gpt_j.py
3	2	vllm/model_executor/models/gpt_neox.py
3	2	vllm/model_executor/models/internlm.py
3	2	vllm/model_executor/models/llama.py
3	2	vllm/model_executor/models/mpt.py
3	2	vllm/model_executor/models/opt.py
2	1	vllm/model_executor/models/qwen.py
8	3	vllm/model_executor/weight_utils.py
7	3	vllm/transformers_utils/config.py

[9841d48a1] Antoni Baum 2023-09-13 Use TGI-like incremental detokenization (#984)
55	0	tests/engine/test_detokenize.py
16	9	vllm/engine/llm_engine.py
6	1	vllm/sequence.py
62	28	vllm/transformers_utils/tokenizer.py

[3272d7a0b] Ikko Eltociear Ashimine 2023-09-14 Fix typo in README.md (#1033)
3	3	README.md

[0bb1e885a] Antoni Baum 2023-09-12 Make `max_model_len` configurable (#972)
15	0	vllm/config.py
7	1	vllm/engine/arg_utils.py

[d6545ad22] leiwen83 2023-09-13 add option to shorten prompt print in log (#991)
7	0	vllm/engine/arg_utils.py
13	2	vllm/engine/async_llm_engine.py

[90eb3f43c] Woosuk Kwon 2023-09-11 Bump up the version to v0.1.7 (#1013)
1	1	vllm/__init__.py

[e67b4f2c2] Woosuk Kwon 2023-09-11 Use FP32 in RoPE initialization (#1004)
3	2	tests/kernels/test_pos_encoding.py
4	4	vllm/model_executor/layers/attention.py

[d6770d1f2] Woosuk Kwon 2023-09-10 Update setup.py (#1006)
35	14	setup.py

[b9cecc263] Woosuk Kwon 2023-09-10 [Docs] Update installation page (#1005)
11	19	docs/source/getting_started/installation.rst

[898285c9b] Kyujin Cho 2023-09-10 fix: CUDA error when inferencing with Falcon-40B base model (#992)
2	1	vllm/config.py

[a62de9ecf] Antoni Baum 2023-09-09 Fix wrong dtype in PagedAttentionWithALiBi bias (#996)
11	4	vllm/model_executor/layers/attention.py

[4042d192f] Jingru 2023-09-09 fix "tansformers_module" ModuleNotFoundError when load model with `trust_remote_code=True` (#871)
1	1	vllm/engine/llm_engine.py
5	1	vllm/engine/ray_utils.py

[1117aa141] Zhuohan Li 2023-09-08 Bump up the version to v0.1.6 (#989)
1	1	vllm/__init__.py

[080438477] Antoni Baum 2023-09-08 Start background task in `AsyncLLMEngine.generate` (#988)
1	2	tests/async_engine/api_server_async_engine.py
13	9	vllm/engine/async_llm_engine.py
1	5	vllm/entrypoints/api_server.py
1	8	vllm/entrypoints/openai/api_server.py

[4b5bcf890] Robert Irvine 2023-09-08 faster startup of vLLM  (#982)
3	2	vllm/model_executor/layers/attention.py

[852ef5b4f] Woosuk Kwon 2023-09-08 Bump up the version to v0.1.5 (#944)
1	1	vllm/__init__.py

[db09d4ad8] Zhuohan Li 2023-09-07 [FIX] Fix Alibi implementation in PagedAttention kernel (#945)
1	1	csrc/attention/attention_kernels.cu
3	2	tests/kernels/test_attention.py

[c957c741d] Zhuohan Li 2023-09-07 Enable safetensors loading for all models (#974)
23	7	vllm/config.py
18	13	vllm/engine/arg_utils.py
1	2	vllm/engine/llm_engine.py
2	2	vllm/model_executor/model_loader.py
2	2	vllm/model_executor/models/aquila.py
6	4	vllm/model_executor/models/baichuan.py
2	2	vllm/model_executor/models/bloom.py
5	3	vllm/model_executor/models/falcon.py
6	4	vllm/model_executor/models/gpt2.py
5	4	vllm/model_executor/models/gpt_bigcode.py
2	2	vllm/model_executor/models/gpt_j.py
2	2	vllm/model_executor/models/gpt_neox.py
2	2	vllm/model_executor/models/internlm.py
2	3	vllm/model_executor/models/llama.py
5	4	vllm/model_executor/models/mpt.py
2	2	vllm/model_executor/models/opt.py
5	2	vllm/model_executor/models/qwen.py
53	23	vllm/model_executor/weight_utils.py

[c07ece5ca] Antoni Baum 2023-09-07 Make `AsyncLLMEngine` more robust & fix batched abort (#969)
51	0	tests/async_engine/api_server_async_engine.py
86	0	tests/async_engine/test_api_server.py
54	0	tests/async_engine/test_request_tracker.py
4	1	vllm/core/scheduler.py
148	54	vllm/engine/async_llm_engine.py
1	0	vllm/entrypoints/api_server.py
1	0	vllm/entrypoints/openai/api_server.py

[7a9c20c71] Woosuk Kwon 2023-09-08 Bum up transformers version (#976)
1	1	requirements.txt

[005ba458b] Antoni Baum 2023-09-06 Set torch default dtype in a context manager (#971)
24	15	vllm/model_executor/model_loader.py

[320a622ec] Woosuk Kwon 2023-09-06 [BugFix] Implement RoPE for GPT-J (#941)
6	5	csrc/pos_encoding.cpp
68	45	csrc/pos_encoding_kernels.cu
38	18	tests/kernels/test_pos_encoding.py
5	2	vllm/model_executor/layers/attention.py
5	2	vllm/model_executor/models/gpt_j.py

[c9927c1a6] Antoni Baum 2023-09-05 Use queue for finished requests (#957)
2	2	tests/conftest.py
11	7	vllm/engine/async_llm_engine.py

[fbd80ad40] Woosuk Kwon 2023-09-06 Clean up kernel unit tests (#938)
43	0	tests/kernels/conftest.py
31	28	tests/kernels/test_activation.py
159	198	tests/kernels/test_attention.py
80	130	tests/kernels/test_cache.py
24	22	tests/kernels/test_layernorm.py
27	21	tests/kernels/test_pos_encoding.py

[22379d551] Wen Sun 2023-09-05 fix: typo (#948)
1	1	vllm/engine/async_llm_engine.py

[169672587] Antoni Baum 2023-09-04 Initialize AsyncLLMEngine bg loop correctly (#943)
11	5	vllm/engine/async_llm_engine.py
6	1	vllm/entrypoints/api_server.py
8	1	vllm/entrypoints/openai/api_server.py

[002800f08] Zhuohan Li 2023-09-04 Align vLLM's beam search implementation with HF generate (#857)
1	1	docs/source/models/adding_model.rst
56	10	tests/conftest.py
46	0	tests/samplers/test_beam_search.py
2	6	vllm/core/block_manager.py
45	69	vllm/core/scheduler.py
260	65	vllm/engine/llm_engine.py
31	42	vllm/model_executor/layers/sampler.py
3	3	vllm/model_executor/models/aquila.py
3	3	vllm/model_executor/models/baichuan.py
3	3	vllm/model_executor/models/bloom.py
3	3	vllm/model_executor/models/falcon.py
3	3	vllm/model_executor/models/gpt2.py
3	3	vllm/model_executor/models/gpt_bigcode.py
3	3	vllm/model_executor/models/gpt_j.py
3	3	vllm/model_executor/models/gpt_neox.py
3	3	vllm/model_executor/models/internlm.py
3	3	vllm/model_executor/models/llama.py
3	3	vllm/model_executor/models/mpt.py
3	3	vllm/model_executor/models/opt.py
3	3	vllm/model_executor/models/qwen.py
6	4	vllm/outputs.py
34	3	vllm/sampling_params.py
77	22	vllm/sequence.py
2	2	vllm/worker/worker.py

[e15932bb6] Nelson Liu 2023-09-04 Only emit warning about internal tokenizer if it isn't being used (#939)
2	1	vllm/transformers_utils/tokenizer.py

[ce741ba3e] Antoni Baum 2023-09-03 Refactor AsyncLLMEngine (#880)
9	4	vllm/core/scheduler.py
212	109	vllm/engine/async_llm_engine.py
46	32	vllm/engine/llm_engine.py

[bf87484ef] Woosuk Kwon 2023-09-04 [BugFix] Fix NaN errors in paged attention kernel (#936)
12	0	csrc/attention/attention_kernels.cu
10	0	csrc/attention/dtype_bfloat16.cuh
5	5	csrc/attention/dtype_float16.cuh
5	0	csrc/attention/dtype_float32.cuh

[8ce9c50d4] Woosuk Kwon 2023-09-02 Avoid compiling kernels for double data type (#933)
4	6	csrc/activation_kernels.cu
5	9	csrc/cache_kernels.cu
14	0	csrc/dispatch_utils.h
2	3	csrc/layernorm_kernels.cu
3	3	csrc/pos_encoding_kernels.cu

[32b6816e5] Woosuk Kwon 2023-09-01 Add tests for models (#922)
1	0	requirements-dev.txt
132	0	tests/conftest.py
45	0	tests/models/test_models.py

[c128d6985] Zhuohan Li 2023-08-31 Fix README.md Link (#927)
1	1	README.md

[55b28b1ee] Woosuk Kwon 2023-09-01 [Docs] Minor fixes in supported models (#920)
3	3	docs/source/models/supported_models.rst

[e11222333] Dong-Yong Lee 2023-09-01 fix: bug fix when penalties are negative (#913)
1	1	vllm/model_executor/layers/sampler.py

[28873a279] Aman Gupta Karmani 2023-08-31 Improve _prune_hidden_states micro-benchmark (#707)
2	1	vllm/model_executor/layers/sampler.py

[0080d8329] Zhuohan Li 2023-08-30 Add acknowledgement to a16z grant
1	0	README.md

[0d93f1569] JFDuan 2023-08-30 Accelerate LLaMA model loading (#234)
9	14	vllm/model_executor/models/aquila.py
12	17	vllm/model_executor/models/baichuan.py
7	11	vllm/model_executor/models/gpt2.py
7	11	vllm/model_executor/models/gpt_bigcode.py
7	14	vllm/model_executor/models/internlm.py
13	16	vllm/model_executor/models/llama.py
8	11	vllm/model_executor/models/qwen.py
128	19	vllm/model_executor/weight_utils.py

[becd7a56f] lplcor 2023-08-29 Enable request body OpenAPI spec for OpenAI endpoints (#865)
3	4	vllm/entrypoints/openai/api_server.py

[75471386d] Aman Gupta Karmani 2023-08-30 use flash-attn via xformers (#877)
0	2	tests/kernels/test_attention.py
0	3	vllm/model_executor/layers/attention.py

[d2b2eed67] Zhuohan Li 2023-08-27 [Fix] Fix a condition for ignored sequences (#867)
6	6	vllm/core/scheduler.py

[4b6f069b6] Antoni Baum 2023-08-25 Add support for CodeLlama (#854)
6	0	vllm/model_executor/models/llama.py

[791d79de3] Woosuk Kwon 2023-08-25 Bump up the version to v0.1.4 (#846)
1	1	vllm/__init__.py

[94d2f5989] Woosuk Kwon 2023-08-25 Set replacement=True in torch.multinomial (#858)
3	1	vllm/model_executor/layers/sampler.py

[75c0ca9d4] wenjun93 2023-08-24 Clean up code (#844)
0	3	vllm/core/scheduler.py

[2a4ec9085] Woosuk Kwon 2023-08-23 Fix for breaking changes in xformers 0.0.21 (#834)
1	1	requirements.txt
3	2	vllm/model_executor/layers/attention.py

[85ebcda94] ldwang 2023-08-23 Fix typo of Aquila in README.md (#836)
1	1	README.md

[d64bf1646] Woosuk Kwon 2023-08-23 Implement approximate GELU kernels (#828)
16	0	csrc/activation.cpp
68	0	csrc/activation_kernels.cu
43	1	tests/kernels/test_activation.py
37	17	vllm/model_executor/layers/activation.py

[a41c20435] Woosuk Kwon 2023-08-23 Add compute capability 8.9 to default targets (#829)
10	1	setup.py

[eedac9dba] Wen Sun 2023-08-23 fix: revert code to avoid no attribute problem (#827)
4	3	vllm/model_executor/models/gpt_bigcode.py

[14f9c72bf] Zhuohan Li 2023-08-22 Update Supported Model List (#825)
3	0	README.md
9	0	docs/source/models/supported_models.rst

[ad5f2fe34] shunxing1234 2023-08-22 Add support for aquila (#663)
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
362	0	vllm/model_executor/models/aquila.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
63	0	vllm/transformers_utils/configs/aquila.py

[4f8584756] zhaoyang-star 2023-08-22 Fix mqa is false case in gpt_bigcode (#806)
11	8	vllm/model_executor/models/gpt_bigcode.py

[65fc1c312] Xudong Zhang 2023-08-22 set default coompute capability according to cuda version (#773)
12	6	setup.py

[c393af6cd] Daniel 2023-08-21 [Feature | CI] Added a github action to build wheels (#746)
101	0	.github/workflows/publish.yml
15	0	.github/workflows/scripts/build.sh
20	0	.github/workflows/scripts/create_release.js
18	0	.github/workflows/scripts/cuda-install.sh
56	0	.github/workflows/scripts/env.sh
14	0	.github/workflows/scripts/pytorch-install.sh

[0c04ce323] wangcx18 2023-08-18 Fix typo in sampling_params.py (#788)
2	2	vllm/sampling_params.py

[73b3de79e] Xinyu Yang 2023-08-18 explicitly del state (#784)
2	0	vllm/model_executor/weight_utils.py

[d1744376a] Abraham-Xu 2023-08-16 Align with huggingface Top K sampling (#753)
23	24	vllm/model_executor/layers/sampler.py

[805de738f] Ikko Eltociear Ashimine 2023-08-15 Fix typo in tokenizer.py (#750)
1	1	vllm/transformers_utils/tokenizer.py

[1b151ed18] Uranus 2023-08-14 Fix baichuan doc style (#748)
1	1	docs/source/models/supported_models.rst

[e06f504a7] WanMok 2023-08-11 Supports tokens and arrays of tokens as inputs to the OpenAI completion API (#715)
43	15	vllm/entrypoints/openai/api_server.py
2	1	vllm/entrypoints/openai/protocol.py

[462ae5220] WRH 2023-08-12 [Fix] unwantted bias in InternLM Model (#740)
6	6	vllm/model_executor/models/internlm.py

[66c54aa9c] Nicolas Basile 2023-08-08 Check the max prompt length for the OpenAI completions API (#472)
12	5	vllm/entrypoints/openai/api_server.py

[735ecfff6] Jia Guoqing 2023-08-09 add internlm model (#528)
1	0	vllm/model_executor/model_loader.py
14	4	vllm/model_executor/models/__init__.py
299	0	vllm/model_executor/models/internlm.py

[a57d13cc9] Qing 2023-08-09 add QWen-7b (#685)
1	0	vllm/model_executor/model_loader.py
5	11	vllm/model_executor/models/__init__.py
316	0	vllm/model_executor/models/qwen.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
71	0	vllm/transformers_utils/configs/qwen.py

[79af7e96a] Dean Leitersdorf 2023-08-04 [OPTIMIZATION] Optimizes the single_query_cached_kv_attention kernel (#420)
7	4	csrc/attention/attention_kernels.cu

[621980bdc] Wen Sun 2023-08-05 fix: incorrect bigcode attention heads num (#676)
5	3	vllm/config.py

[aa84c92ef] Zhuohan Li 2023-08-02 Bump up version to 0.1.3 (#657)
1	1	vllm/__init__.py

[f7389f476] Zhuohan Li 2023-08-02 [Doc] Add Baichuan 13B to supported models (#656)
1	1	README.md
2	2	docs/source/models/supported_models.rst
2	1	vllm/model_executor/models/__init__.py

[55fe8a81e] Woosuk Kwon 2023-08-02 Refactor scheduler (#658)
1	1	examples/llm_engine_example.py
96	129	vllm/core/scheduler.py
91	9	vllm/engine/llm_engine.py
17	5	vllm/model_executor/layers/attention.py

[e8ddc08ec] YHPeter 2023-08-02 [BUG FIX] upgrade fschat version to 0.2.23 (#650)
10	2	vllm/entrypoints/openai/api_server.py

[1b0bd0fe8] Zhuohan Li 2023-08-02 Add Falcon support (new) (#592)
1	0	README.md
29	13	csrc/pos_encoding_kernels.cu
3	0	docs/source/models/supported_models.rst
2	1	examples/llm_engine_example.py
7	2	vllm/config.py
24	12	vllm/model_executor/layers/attention.py
2	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
496	0	vllm/model_executor/models/falcon.py
1	72	vllm/model_executor/parallel_utils/parallel_state.py
2	1	vllm/model_executor/parallel_utils/tensor_parallel/__init__.py
16	15	vllm/model_executor/parallel_utils/tensor_parallel/layers.py
2	0	vllm/transformers_utils/config.py
5	0	vllm/transformers_utils/configs/__init__.py
87	0	vllm/transformers_utils/configs/falcon.py
1	6	vllm/worker/worker.py

[20044cab7] Lily Liu 2023-08-02 Fix log message in scheduler (#652)
5	5	vllm/core/scheduler.py

[64f23c290] Song 2023-08-02 fix baichuan for different position embedding for 7b and 13b models (#643)
2	1	vllm/model_executor/model_loader.py
2	1	vllm/model_executor/models/__init__.py
72	15	vllm/model_executor/models/baichuan.py

[d4c7755ca] Qing 2023-08-02 fix biachuan-7b tp (#598)
37	10	vllm/model_executor/models/baichuan.py

[aa39e42c5] Chaofan Lin 2023-08-01 fix doc (#622)
1	1	vllm/config.py
1	1	vllm/engine/llm_engine.py

[953f28cf9] Fang li 2023-07-30 fix ModuleNotFoundError (#599)
7	3	vllm/engine/llm_engine.py

[c0d00f5be] Xudong Zhang 2023-07-28 [Fix] fix import error of RayWorker (#604) (#605)
1	0	vllm/engine/ray_utils.py

[58a072be1] Zhuohan Li 2023-07-25 [Fix] Add model sequence length into model config (#575)
20	0	vllm/config.py
2	3	vllm/engine/arg_utils.py
5	15	vllm/entrypoints/openai/api_server.py

[82ad323de] Zhuohan Li 2023-07-25 [Fix] Add chat completion Example and simplify dependencies (#576)
33	0	examples/openai_chatcompletion_client.py
7	7	examples/{openai_client.py => openai_completion_client.py}
0	1	requirements.txt
12	3	vllm/entrypoints/openai/api_server.py

[df5dd3c68] Zhuohan Li 2023-07-25 Add Baichuan-7B to README (#494)
1	0	README.md
3	0	docs/source/models/supported_models.rst

[2d867b55f] MoeedDar 2023-07-25 fixed tensor parallel is not defined (#564)
0	3	vllm/model_executor/parallel_utils/__init__.py

[d7a1c6d61] Tao Peng 2023-07-25 Fix paged attention testing. (#495)
9	0	tests/kernels/test_attention.py

[7d5a155e4] Zhuohan Li 2023-07-24 [Fix] Fix GPTBigcoder for distributed execution (#503)
62	24	vllm/model_executor/models/gpt_bigcode.py

[1dde34e0f] leegohi04517 2023-07-25 GPTJConfig has no attribute rotary.  (#532)
1	1	vllm/model_executor/models/gpt_j.py

[6fc2a38b1] Zhuohan Li 2023-07-20 Add support for LLaMA-2 (#505)
2	1	README.md
13	8	csrc/pos_encoding_kernels.cu
2	2	docs/source/models/supported_models.rst
1	1	requirements.txt
6	1	vllm/config.py
9	6	vllm/model_executor/layers/attention.py
34	19	vllm/model_executor/models/llama.py

[c487a221e] Antoni Baum 2023-07-19 Fix bad assert in initialize_cluster if PG already exists (#526)
5	3	vllm/engine/ray_utils.py

[9925c1794] Antoni Baum 2023-07-19 Ray placement group support (#397)
1	1	requirements.txt
2	2	vllm/engine/async_llm_engine.py
78	27	vllm/engine/llm_engine.py
56	63	vllm/engine/ray_utils.py
48	21	vllm/worker/worker.py

[8c4b2592f] Ricardo Lu 2023-07-20 fix: enable trust-remote-code in api server & benchmark. (#509)
3	0	benchmarks/benchmark_latency.py
3	1	benchmarks/benchmark_serving.py
6	4	benchmarks/benchmark_throughput.py
2	1	vllm/entrypoints/openai/api_server.py

[cf21a9bd5] WRH 2023-07-20 support trust_remote_code in benchmark (#518)
9	3	benchmarks/benchmark_throughput.py

[16c3e295a] Massimiliano Pronesti 2023-07-20 fix(ray_utils): ignore re-init error (#465)
1	1	vllm/engine/ray_utils.py

[bda41c70d] Song 2023-07-19 hotfix attn alibi wo head mapping (#496)
2	0	tests/kernels/test_attention.py
1	0	vllm/model_executor/layers/attention.py

[453bafb96] Lily Liu 2023-07-18 Merge pull request #498 from MoeedDar/main
[328d231c1] MoeedDar 2023-07-18 Fixed old name reference for max_seq_len
1	1	vllm/core/scheduler.py

[b4b195b36] Lily Liu 2023-07-17 fix max seq len (#489)
2	2	vllm/config.py
3	1	vllm/core/scheduler.py
2	3	vllm/engine/arg_utils.py
1	2	vllm/engine/llm_engine.py

[20b0d88d1] codethazine 2023-07-17 Add support for baichuan (#365)
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
293	0	vllm/model_executor/models/baichuan.py
1	0	vllm/transformers_utils/config.py
2	0	vllm/transformers_utils/configs/__init__.py
62	0	vllm/transformers_utils/configs/baichuan.py

[2bdea7ac1] Zhuohan Li 2023-07-17 [Fix] Fix the condition of max_seq_len (#477)
1	1	vllm/core/scheduler.py
1	1	vllm/engine/llm_engine.py

[58df2883c] Zhanghao Wu 2023-07-16 [Doc] Add doc for running vLLM on the cloud (#426)
1	0	docs/source/index.rst
69	0	docs/source/serving/run_on_sky.rst

[6d7d95a70] Zhangir Azerbayev 2023-07-16 Offload port selection to OS (#467)
9	3	vllm/engine/ray_utils.py

[96853af5a] Zhuohan Li 2023-07-14 Optimize MQA Kernel (#452)
1	0	csrc/attention.cpp
22	9	csrc/attention/attention_kernels.cu
7	0	vllm/config.py
32	11	vllm/model_executor/layers/attention.py
22	52	vllm/model_executor/models/gpt_bigcode.py

[dbed69058] Wen Sun 2023-07-14 Fix the `KeyError` when loading bloom-based models (#441)
10	3	vllm/model_executor/models/bloom.py

[7b6ae9405] panda 2023-07-14 add vocab padding for LLama(Support WizardLM) (#411)
17	4	vllm/model_executor/models/llama.py

[c6dfc3cdb] xcnick 2023-07-12 Fix handling of special tokens in decoding. (#418)
3	2	vllm/engine/llm_engine.py
3	1	vllm/transformers_utils/tokenizer.py

[51be36514] Keming 2023-07-12 fix: freeze pydantic to v1 (#429)
1	1	requirements.txt

[c89483610] Andre Slavescu 2023-07-08 [Model] Add support for GPT-J (#226)
1	0	README.md
4	4	csrc/attention/attention_kernels.cu
3	0	docs/source/models/supported_models.rst
2	2	tests/kernels/test_attention.py
1	1	vllm/model_executor/layers/attention.py
3	0	vllm/model_executor/layers/sampler.py
1	0	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
251	0	vllm/model_executor/models/gpt_j.py
1	0	vllm/model_executor/models/mpt.py

[75beba29b] Fazlul Shahriar 2023-07-08 Don't try to load training_args.bin (#373)
4	1	vllm/model_executor/weight_utils.py

[ddfdf470a] Woosuk Kwon 2023-07-08 Add trust_remote_code arg to get_config (#405)
1	1	vllm/config.py
15	2	vllm/transformers_utils/config.py
4	3	vllm/transformers_utils/tokenizer.py

[b6fbb9a56] Woosuk Kwon 2023-07-08 Sort the outputs before return (#402)
4	0	vllm/entrypoints/llm.py

[2179e4f4c] Lily Liu 2023-07-08 avoid python list copy in sequence initialization (#401)
5	3	vllm/sequence.py

[a945fcc2a] codethazine 2023-07-07 Add trust-remote-code flag to handle remote tokenizers (#364)
4	0	vllm/config.py
8	3	vllm/engine/arg_utils.py
4	1	vllm/engine/llm_engine.py
4	0	vllm/entrypoints/llm.py
19	2	vllm/transformers_utils/tokenizer.py

[be54f8e5c] Nicolas Frenay 2023-07-06 [Fix] Change /generate response-type to json for non-streaming (#374)
2	2	vllm/entrypoints/api_server.py

[b396cb499] Ricardo Lu 2023-07-07 fix: only response [DONE] once when streaming response. (#378)
2	2	vllm/entrypoints/openai/api_server.py

[1c395b4ea] Woosuk Kwon 2023-07-04 Bump up the version (#300)
1	1	vllm/__init__.py

[3d64cf019] akxxsb 2023-07-05 [Server] use fastchat.model.model_adapter.get_conversation_template method to get model template (#357)
5	17	vllm/entrypoints/openai/api_server.py

[98fe8cb54] Zhuohan Li 2023-07-03 [Server] Add option to specify chat template for chat endpoint (#345)
1	0	requirements.txt
22	6	vllm/entrypoints/openai/api_server.py

[ffa6d2f9f] Woosuk Kwon 2023-07-03 [Docs] Fix typo (#346)
1	1	docs/source/models/supported_models.rst

[404422f42] Woosuk Kwon 2023-07-03 [Model] Add support for MPT (#334)
1	0	README.md
3	0	csrc/attention/attention_kernels.cu
3	0	docs/source/models/supported_models.rst
3	2	vllm/config.py
1	1	vllm/model_executor/layers/attention.py
2	1	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
279	0	vllm/model_executor/models/mpt.py
15	0	vllm/transformers_utils/config.py
5	0	vllm/transformers_utils/configs/__init__.py
74	0	vllm/transformers_utils/configs/mpt.py

[7717d0838] coolcloudcol 2023-07-04 Fix an endless loop issue when engine_step throws a RuntimeError (#339)
5	1	vllm/engine/async_llm_engine.py

[42e0c1df7] Zhuohan Li 2023-07-03 [Quality] Add CI for formatting (#343)
31	0	.github/workflows/pylint.yml
31	0	.github/workflows/yapf.yml
8	0	vllm/engine/async_llm_engine.py
4	0	vllm/engine/llm_engine.py
37	30	vllm/entrypoints/openai/api_server.py
2	1	vllm/model_executor/models/bloom.py

[e41f06702] Woosuk Kwon 2023-07-03 Add support for BLOOM (#331)
1	0	README.md
3	1	csrc/attention.cpp
19	6	csrc/attention/attention_kernels.cu
3	0	docs/source/models/supported_models.rst
1	0	tests/kernels/test_attention.py
4	2	vllm/model_executor/input_metadata.py
128	5	vllm/model_executor/layers/attention.py
2	3	vllm/model_executor/model_loader.py
2	0	vllm/model_executor/models/__init__.py
316	0	vllm/model_executor/models/bloom.py
0	1	vllm/model_executor/models/gpt_neox.py

[d6fa1be3a] Zhuohan Li 2023-07-03 [Quality] Add code formatter and linter (#326)
434	0	.pylintrc
4	1	CONTRIBUTING.md
5	2	examples/api_client.py
16	9	examples/gradio_webserver.py
7	2	examples/llm_engine_example.py
0	1	examples/offline_inference.py
7	2	examples/openai_client.py
108	0	format.sh
11	1	requirements-dev.txt
33	20	tests/kernels/test_attention.py
61	29	tests/kernels/test_cache.py
4	2	tests/kernels/test_layernorm.py
20	15	tests/kernels/test_pos_encoding.py
2	0	vllm/__init__.py
2	1	vllm/block.py
12	14	vllm/config.py
9	5	vllm/core/block_manager.py
35	30	vllm/core/scheduler.py
78	51	vllm/engine/arg_utils.py
26	17	vllm/engine/async_llm_engine.py
18	15	vllm/engine/llm_engine.py
7	6	vllm/engine/ray_utils.py
8	9	vllm/entrypoints/api_server.py
1	2	vllm/entrypoints/llm.py
67	46	vllm/entrypoints/openai/api_server.py
4	2	vllm/entrypoints/openai/protocol.py
3	3	vllm/logger.py
0	1	vllm/model_executor/__init__.py
13	2	vllm/model_executor/input_metadata.py
9	10	vllm/model_executor/layers/activation.py
81	32	vllm/model_executor/layers/attention.py
45	37	vllm/model_executor/layers/sampler.py
6	7	vllm/model_executor/model_loader.py
0	2	vllm/model_executor/models/__init__.py
53	33	vllm/model_executor/models/gpt2.py
66	38	vllm/model_executor/models/gpt_bigcode.py
52	33	vllm/model_executor/models/gpt_neox.py
57	39	vllm/model_executor/models/llama.py
69	46	vllm/model_executor/models/opt.py
10	11	vllm/model_executor/weight_utils.py
4	2	vllm/outputs.py
8	2	vllm/sampling_params.py
70	19	vllm/sequence.py
3	2	vllm/transformers_utils/tokenizer.py
3	2	vllm/utils.py
5	6	vllm/worker/cache_engine.py
13	10	vllm/worker/worker.py

[0ffded812] Zhuohan Li 2023-07-03 [Fix] Better error message for batched prompts (#342)
7	1	vllm/entrypoints/openai/api_server.py

[0bd2a573a] Michele Catalano 2023-07-03 Allow send list of str for the Prompt on openai demo endpoint /v1/completions (#323)
5	1	vllm/entrypoints/openai/api_server.py
1	1	vllm/entrypoints/openai/protocol.py

[49b26e2ce] Ricardo Lu 2023-07-03 feat: add ChatCompletion endpoint in OpenAI demo server. (#330)
236	3	vllm/entrypoints/openai/api_server.py
48	3	vllm/entrypoints/openai/protocol.py

[dafd924c1] Lily Liu 2023-06-30 Raise error for long prompt (#273)
4	0	vllm/config.py
20	7	vllm/core/scheduler.py
5	1	vllm/engine/arg_utils.py
9	3	vllm/engine/llm_engine.py
4	0	vllm/sequence.py

[598dc4b79] Zhuohan Li 2023-06-29 [Fix] Weight loading for GPTBigCode (#313)
15	3	vllm/model_executor/models/gpt_bigcode.py
3	1	vllm/model_executor/weight_utils.py

[85de09347] Zhuohan Li 2023-06-29 [Fix] Do not pin memory when in WSL (#312)
5	0	vllm/utils.py
12	2	vllm/worker/cache_engine.py

[f72297562] Zhanghao Wu 2023-06-29 Add news for the vllm+skypilot example (#314)
1	1	README.md

[9d27b09d1] Bayang 2023-06-29 Update README.md (#306)
2	2	docs/README.md

[998d9d150] Woosuk Kwon 2023-06-28 [Tokenizer] Add tokenizer mode (#298)
14	1	vllm/config.py
9	2	vllm/engine/arg_utils.py
3	1	vllm/engine/llm_engine.py
4	0	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/api_server.py
7	0	vllm/transformers_utils/tokenizer.py

[425040d4c] Lily Liu 2023-06-28 remove floats == 0 comparison  (#285)
6	5	vllm/model_executor/layers/sampler.py
5	4	vllm/sampling_params.py

[4338cc475] Woosuk Kwon 2023-06-28 [Tokenizer] Add an option to specify tokenizer (#284)
2	0	benchmarks/benchmark_latency.py
2	9	benchmarks/benchmark_serving.py
15	20	benchmarks/benchmark_throughput.py
3	0	vllm/config.py
6	1	vllm/engine/arg_utils.py
4	2	vllm/engine/llm_engine.py
3	0	vllm/entrypoints/llm.py
1	1	vllm/entrypoints/openai/api_server.py
0	0	vllm/transformers_utils/__init__.py
25	27	vllm/{engine/tokenizer_utils.py => transformers_utils/tokenizer.py}

[bdd6b4c8b] Jishnu Ray Chowdhury 2023-06-28 Add LLM.set_tokenizer (#283)
6	0	vllm/entrypoints/llm.py

[2b7d3aca2] Cody Yu 2023-06-27 Update setup.py (#282)
2	3	setup.py

[4026a049d] twaka 2023-06-27 expand coverage of gpt2 model loading (#271)
4	2	vllm/model_executor/models/gpt2.py

[43710e8d0] Zhuohan Li 2023-06-26 [Fix] Fix default port number in benchmark scripts (#265)
1	1	benchmarks/benchmark_serving.py
1	1	benchmarks/launch_tgi_server.sh

[526df28fb] Woosuk Kwon 2023-06-26 [BugFix] Fix a bug in counting running sequences (#266)
6	2	vllm/core/scheduler.py

[2cf1a333b] Zhuohan Li 2023-06-26 [Doc] Documentation for distributed inference  (#261)
3	0	.gitignore
1	1	README.md
12	2	docs/source/index.rst
38	0	docs/source/serving/distributed_serving.rst

[0b7db411b] Zhuohan Li 2023-06-26 [Bug] Fix the OOM condition for CPU cache (#260)
1	1	vllm/engine/llm_engine.py
2	0	vllm/worker/worker.py

[471a7a456] BasicCoder 2023-06-27 Compatible with Decapoda Research llama hf version (#251)
1	0	vllm/model_executor/model_loader.py

[6214dd6ce] Lianmin Zheng 2023-06-25 Update README.md (#236)
1	1	README.md

[060337986] metacryptom 2023-06-25 fix wrong using getattr to get dict value (#232)
2	2	vllm/engine/tokenizer_utils.py

[665c48963] Woosuk Kwon 2023-06-22 [Docs] Add GPTBigCode to supported models (#213)
2	1	README.md
3	0	docs/source/models/supported_models.rst

[298695b76] Michael Feil 2023-06-22 GPTBigCode (StarCoder, SantaCoder Support) (#209)
1	0	vllm/model_executor/layers/activation.py
2	1	vllm/model_executor/model_loader.py
4	1	vllm/model_executor/models/__init__.py
291	0	vllm/model_executor/models/gpt_bigcode.py

[83658c8ac] Zhuohan Li 2023-06-22 Bump up version to 0.1.1 (#204)
1	1	vllm/__init__.py

[1d24ccb96] Zhuohan Li 2023-06-22 [Fix] Better error message when there is OOM during cache initialization (#203)
6	0	vllm/engine/llm_engine.py
1	0	vllm/outputs.py

[14f0b39cd] Woosuk Kwon 2023-06-22 [Bugfix] Fix a bug in RequestOutput.finished (#202)
1	1	examples/llm_engine_example.py
1	1	vllm/engine/async_llm_engine.py
1	1	vllm/entrypoints/llm.py
7	5	vllm/outputs.py

[2e0d31438] Zhuohan Li 2023-06-22 fix-ray (#193)
1	1	vllm/engine/llm_engine.py

[67d96c29f] Woosuk Kwon 2023-06-19 Use slow tokenizer for open llama models (#168)
6	1	vllm/engine/tokenizer_utils.py

[033f5c78f] Zhuohan Li 2023-06-20 Remove e.g. in README (#167)
4	4	README.md

[794e578de] Woosuk Kwon 2023-06-19 [Minor] Fix URLs (#166)
6	6	README.md
1	1	docs/source/index.rst
1	1	docs/source/models/adding_model.rst

[caddfc14c] Woosuk Kwon 2023-06-19 [Minor] Fix icons in doc (#165)
4	3	docs/source/index.rst

[fc72e39de] Zhuohan Li 2023-06-20 Change image urls (#164)
7	7	README.md

[b7e62d345] Woosuk Kwon 2023-06-19 Fix repo & documentation URLs (#163)
6	6	README.md
1	1	docs/source/conf.py
1	1	docs/source/getting_started/installation.rst
5	5	docs/source/getting_started/quickstart.rst
3	3	docs/source/index.rst
5	5	docs/source/models/adding_model.rst
1	1	docs/source/models/supported_models.rst

[364536acd] Woosuk Kwon 2023-06-19 [Docs] Minor fix (#162)
1	1	README.md
1	1	docs/source/index.rst

[0b32a987d] Zhuohan Li 2023-06-20 Add and list supported models in README (#161)
7	0	README.md
2	0	docs/source/conf.py
6	1	docs/source/models/supported_models.rst

[570fb2e9c] Woosuk Kwon 2023-06-19 [PyPI] Fix package info in setup.py (#158)
4	5	setup.py

[a255885f8] Zhuohan Li 2023-06-19 Add logo and polish readme (#156)
167	10	.gitignore
54	20	README.md
-	-	{assets => docs/source/assets}/figures/perf_a100_n1_dark.png
-	-	{assets => docs/source/assets}/figures/perf_a100_n1_light.png
-	-	{assets => docs/source/assets}/figures/perf_a100_n3_dark.png
-	-	{assets => docs/source/assets}/figures/perf_a100_n3_light.png
-	-	{assets => docs/source/assets}/figures/perf_a10g_n1_dark.png
-	-	{assets => docs/source/assets}/figures/perf_a10g_n1_light.png
-	-	{assets => docs/source/assets}/figures/perf_a10g_n3_dark.png
-	-	{assets => docs/source/assets}/figures/perf_a10g_n3_light.png
-	-	docs/source/assets/logos/vllm-logo-only-light.png
-	-	docs/source/assets/logos/vllm-logo-text-dark.png
-	-	docs/source/assets/logos/vllm-logo-text-light.png
37	12	docs/source/index.rst

[5822ede66] Woosuk Kwon 2023-06-18 Add performance figures for dark mode (#160)
-	-	assets/figures/perf_a100_n1.png
-	-	assets/figures/perf_a100_n1_dark.png
-	-	assets/figures/perf_a100_n1_light.png
-	-	assets/figures/perf_a100_n3.png
-	-	assets/figures/perf_a100_n3_dark.png
-	-	assets/figures/perf_a100_n3_light.png
-	-	assets/figures/perf_a10g_n1.png
-	-	assets/figures/perf_a10g_n1_dark.png
-	-	assets/figures/perf_a10g_n1_light.png
-	-	assets/figures/perf_a10g_n3.png
-	-	assets/figures/perf_a10g_n3_dark.png
-	-	assets/figures/perf_a10g_n3_light.png

[0370afa2e] Zhuohan Li 2023-06-19 Remove benchmark_async_llm_server.py (#155)
0	60	benchmarks/benchmark_async_llm_server.py

[7e2a913c6] Woosuk Kwon 2023-06-18 [Minor] Fix CompletionOutput.__repr__ (#157)
1	1	vllm/outputs.py

[3f92038b9] Woosuk Kwon 2023-06-18 Add comments on swap space (#154)
2	1	benchmarks/benchmark_serving.py
6	1	vllm/core/scheduler.py

[dcda03b4c] Woosuk Kwon 2023-06-18 Write README and front page of doc (#147)
39	51	README.md
-	-	assets/figures/perf_a100_n1.png
-	-	assets/figures/perf_a100_n3.png
-	-	assets/figures/perf_a10g_n1.png
-	-	assets/figures/perf_a10g_n3.png
8	5	docs/source/getting_started/installation.rst
15	1	docs/source/index.rst
2	2	docs/source/models/supported_models.rst
1	1	setup.py

[bf5f121c0] Zhuohan Li 2023-06-18 Reduce GPU memory utilization to make sure OOM doesn't happen (#153)
1	1	vllm/engine/arg_utils.py

[bec7b2dc2] Zhuohan Li 2023-06-18 Add quickstart guide (#148)
2	0	docs/source/getting_started/installation.rst
105	8	docs/source/getting_started/quickstart.rst
2	0	docs/source/index.rst
20	1	vllm/outputs.py

[0b98ba15c] Woosuk Kwon 2023-06-17 Change the name to vLLM (#150)
6	6	CONTRIBUTING.md
5	5	README.md
1	1	benchmarks/README.md
1	1	benchmarks/benchmark_async_llm_server.py
1	1	benchmarks/benchmark_latency.py
5	5	benchmarks/benchmark_serving.py
8	7	benchmarks/benchmark_throughput.py
0	18	cacheflow/__init__.py
0	10	cacheflow/model_executor/__init__.py
0	12	cacheflow/model_executor/models/__init__.py
3	3	csrc/activation_kernels.cu
3	3	csrc/attention/attention_generic.cuh
4	4	csrc/attention/attention_kernels.cu
3	3	csrc/attention/attention_utils.cuh
3	3	csrc/attention/dtype_bfloat16.cuh
3	3	csrc/attention/dtype_float16.cuh
3	3	csrc/attention/dtype_float32.cuh
9	9	csrc/cache_kernels.cu
3	3	csrc/layernorm_kernels.cu
3	3	csrc/pos_encoding_kernels.cu
3	3	csrc/reduction_utils.cuh
1	1	docs/README.md
4	4	docs/source/conf.py
11	11	docs/source/getting_started/installation.rst
1	1	docs/source/getting_started/quickstart.rst
2	2	docs/source/index.rst
11	11	docs/source/models/adding_model.rst
6	6	docs/source/models/supported_models.rst
1	1	examples/api_client.py
2	2	examples/gradio_webserver.py
1	1	examples/llm_engine_example.py
1	1	examples/offline_inference.py
1	1	examples/openai_client.py
2	2	mypy.ini
13	13	setup.py
1	1	tests/kernels/test_activation.py
1	1	tests/kernels/test_attention.py
1	1	tests/kernels/test_cache.py
1	1	tests/kernels/test_layernorm.py
1	1	tests/kernels/test_pos_encoding.py
21	0	vllm/__init__.py
1	1	{cacheflow => vllm}/block.py
3	3	{cacheflow => vllm}/config.py
0	0	{cacheflow => vllm}/core/__init__.py
3	3	{cacheflow => vllm}/core/block_manager.py
1	1	{cacheflow => vllm}/core/policy.py
7	7	{cacheflow => vllm}/core/scheduler.py
0	0	{cacheflow => vllm}/engine/__init__.py
5	5	{cacheflow => vllm}/engine/arg_utils.py
9	9	{cacheflow => vllm}/engine/async_llm_engine.py
13	14	{cacheflow => vllm}/engine/llm_engine.py
1	1	{cacheflow => vllm}/engine/ray_utils.py
1	1	{cacheflow => vllm}/engine/tokenizer_utils.py
0	0	{cacheflow => vllm}/entrypoints/__init__.py
4	4	{cacheflow => vllm}/entrypoints/api_server.py
5	5	{cacheflow => vllm}/entrypoints/llm.py
0	0	{cacheflow => vllm}/entrypoints/openai/__init__.py
13	13	{cacheflow => vllm}/entrypoints/openai/api_server.py
3	3	{cacheflow => vllm}/entrypoints/openai/protocol.py
1	1	{cacheflow => vllm}/logger.py
10	0	vllm/model_executor/__init__.py
2	2	{cacheflow => vllm}/model_executor/input_metadata.py
0	0	{cacheflow => vllm}/model_executor/layers/__init__.py
1	1	{cacheflow => vllm}/model_executor/layers/activation.py
8	8	{cacheflow => vllm}/model_executor/layers/attention.py
1	1	{cacheflow => vllm}/model_executor/layers/layernorm.py
4	4	{cacheflow => vllm}/model_executor/layers/sampler.py
4	4	{cacheflow => vllm}/model_executor/model_loader.py
12	0	vllm/model_executor/models/__init__.py
16	16	{cacheflow => vllm}/model_executor/models/gpt2.py
13	12	{cacheflow => vllm}/model_executor/models/gpt_neox.py
14	14	{cacheflow => vllm}/model_executor/models/llama.py
12	12	{cacheflow => vllm}/model_executor/models/opt.py
0	0	{cacheflow => vllm}/model_executor/parallel_utils/README.md
2	2	{cacheflow => vllm}/model_executor/parallel_utils/__init__.py
1	1	{cacheflow => vllm}/model_executor/parallel_utils/parallel_state.py
0	0	{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/__init__.py
2	2	{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/layers.py
2	2	{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/mappings.py
2	2	{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/random.py
1	1	{cacheflow => vllm}/model_executor/parallel_utils/tensor_parallel/utils.py
2	2	{cacheflow => vllm}/model_executor/utils.py
0	0	{cacheflow => vllm}/model_executor/weight_utils.py
1	1	{cacheflow => vllm}/outputs.py
0	0	{cacheflow => vllm}/sampling_params.py
2	2	{cacheflow => vllm}/sequence.py
0	0	{cacheflow => vllm}/utils.py
0	0	{cacheflow => vllm}/worker/__init__.py
2	2	{cacheflow => vllm}/worker/cache_engine.py
8	9	{cacheflow => vllm}/worker/worker.py

[e5464ee48] Zhuohan Li 2023-06-17 Rename servers to engines (#152)
1	1	benchmarks/benchmark_latency.py
1	1	benchmarks/benchmark_serving.py
2	2	benchmarks/benchmark_throughput.py
5	5	cacheflow/__init__.py
1	1	cacheflow/core/scheduler.py
0	0	cacheflow/{server => engine}/__init__.py
24	24	cacheflow/{server => engine}/arg_utils.py
53	53	cacheflow/{server/async_llm_server.py => engine/async_llm_engine.py}
21	21	cacheflow/{server/llm_server.py => engine/llm_engine.py}
6	6	cacheflow/{server => engine}/ray_utils.py
0	0	cacheflow/{server => engine}/tokenizer_utils.py
8	8	cacheflow/entrypoints/api_server.py
15	15	cacheflow/entrypoints/llm.py
18	27	cacheflow/entrypoints/openai/api_server.py
10	10	examples/{llmserver_example.py => llm_engine_example.py}

[bab8f3dd0] Woosuk Kwon 2023-06-16 [Minor] Fix benchmark_throughput.py (#151)
1	0	benchmarks/benchmark_throughput.py

[eedb46bf0] Zhuohan Li 2023-06-17 Rename servers and change port numbers to reduce confusion (#149)
1	1	benchmarks/benchmark_async_llm_server.py
2	2	cacheflow/__init__.py
4	4	cacheflow/entrypoints/{simple_fastapi_frontend.py => api_server.py}
3	3	cacheflow/entrypoints/llm.py
2	2	cacheflow/entrypoints/openai/{openai_frontend.py => api_server.py}
16	16	cacheflow/server/async_llm_server.py
3	3	cacheflow/server/llm_server.py
3	1	examples/{simple_fastapi_client.py => api_client.py}
3	2	examples/gradio_webserver.py
4	3	examples/{simple_server.py => llmserver_example.py}

[311490a72] Woosuk Kwon 2023-06-14 Add script for benchmarking serving throughput (#145)
5	3	benchmarks/benchmark_async_llm_server.py
1	0	benchmarks/benchmark_latency.py
237	0	benchmarks/benchmark_serving.py
0	255	benchmarks/benchmark_text_completion.py
138	28	benchmarks/benchmark_throughput.py
16	0	benchmarks/launch_tgi_server.sh
0	116	benchmarks/trace.py
3	0	cacheflow/server/arg_utils.py
13	7	cacheflow/server/async_llm_server.py
8	6	examples/simple_fastapi_client.py

[da5ddcd54] Woosuk Kwon 2023-06-10 Remove redundant code in ColumnParallelLinear (#146)
1	1	cacheflow/model_executor/parallel_utils/tensor_parallel/layers.py

[5020e1e80] Zhuohan Li 2023-06-11 Non-streaming simple fastapi server (#144)
1	1	cacheflow/entrypoints/openai/openai_frontend.py
29	10	cacheflow/entrypoints/simple_fastapi_frontend.py
31	9	examples/simple_fastapi_client.py

[429837426] Zhuohan Li 2023-06-07 Add docstrings for LLMServer and related classes and examples (#142)
39	3	cacheflow/config.py
12	0	cacheflow/entrypoints/openai/openai_frontend.py
6	0	cacheflow/entrypoints/simple_fastapi_frontend.py
2	0	cacheflow/server/arg_utils.py
67	7	cacheflow/server/async_llm_server.py
65	4	cacheflow/server/llm_server.py
17	2	cacheflow/server/ray_utils.py
1	0	cacheflow/server/tokenizer_utils.py
6	5	examples/openai_client.py
1	1	examples/simple_server.py

[e38074b1e] Woosuk Kwon 2023-06-07 Support FP32 (#141)
3	4	cacheflow/config.py
5	4	cacheflow/entrypoints/llm.py
3	5	cacheflow/model_executor/layers/attention.py
4	4	cacheflow/server/arg_utils.py
37	32	csrc/attention/attention_kernels.cu
3	0	docs/source/getting_started/installation.rst
5	0	setup.py
5	5	tests/kernels/test_attention.py

[376725ce7] Woosuk Kwon 2023-06-05 [PyPI] Packaging for PyPI distribution (#140)
1	0	.gitignore
4	0	MANIFEST.in
2	0	cacheflow/__init__.py
0	0	cacheflow/core/__init__.py
0	0	cacheflow/entrypoints/__init__.py
0	0	cacheflow/entrypoints/openai/__init__.py
0	0	cacheflow/model_executor/layers/__init__.py
0	0	cacheflow/server/__init__.py
0	0	cacheflow/worker/__init__.py
1	2	docs/source/getting_started/installation.rst
9	0	pyproject.toml
55	5	setup.py

[456941cfe] Woosuk Kwon 2023-06-05 [Docs] Write the `Adding a New Model` section (#138)
88	1	docs/source/models/adding_model.rst

[1a956e136] Zhuohan Li 2023-06-05 Fix various issues of async servers (#135)
58	0	benchmarks/benchmark_async_llm_server.py
3	3	cacheflow/config.py
6	3	cacheflow/core/block_manager.py
13	1	cacheflow/core/scheduler.py
23	7	cacheflow/entrypoints/openai/openai_frontend.py
17	8	cacheflow/entrypoints/simple_fastapi_frontend.py
9	1	cacheflow/sequence.py
72	59	cacheflow/server/arg_utils.py
71	22	cacheflow/server/async_llm_server.py
7	9	cacheflow/server/llm_server.py
10	8	cacheflow/server/ray_utils.py

[8274ca23a] Woosuk Kwon 2023-06-04 Add docstrings for LLM (#137)
2	2	benchmarks/benchmark_latency.py
5	3	benchmarks/benchmark_throughput.py
57	4	cacheflow/entrypoints/llm.py
2	1	cacheflow/server/llm_server.py

[62ec38ea4] Woosuk Kwon 2023-06-02 Document supported models (#127)
3	1	cacheflow/entrypoints/llm.py
1	2	docs/README.md
7	0	docs/source/index.rst
7	0	docs/source/models/adding_model.rst
40	0	docs/source/models/supported_models.rst

[0eda2e095] Woosuk Kwon 2023-06-02 Add .readthedocs.yaml (#136)
21	0	.readthedocs.yaml
3	3	docs/requirements-docs.txt

[211318d44] Woosuk Kwon 2023-05-28 Add throughput benchmarking script (#133)
0	165	benchmark/benchmark_attention.py
0	81	benchmark/benchmark_cache.py
8	0	benchmarks/README.md
0	0	{benchmark => benchmarks}/benchmark_latency.py
0	0	{benchmark => benchmarks}/benchmark_text_completion.py
104	0	benchmarks/benchmark_throughput.py
0	0	{benchmark => benchmarks}/trace.py
2	1	cacheflow/__init__.py
3	0	cacheflow/core/scheduler.py
24	8	cacheflow/entrypoints/llm.py
3	0	cacheflow/server/llm_server.py
1	2	examples/simple_server.py

[337871c6f] Woosuk Kwon 2023-05-28 Enable LLaMA fast tokenizer (#132)
1	1	cacheflow/sampling_params.py
10	6	cacheflow/server/tokenizer_utils.py

[56b7f0efa] Woosuk Kwon 2023-05-27 Add a doc for installation (#128)
41	1	docs/source/getting_started/installation.rst

[d72116844] Woosuk Kwon 2023-05-27 Improve setup script & Add a guard for bfloat16 kernels (#130)
0	3	csrc/attention/attention_dtypes.h
0	2	csrc/attention/attention_kernels.cu
44	0	csrc/attention/dtype_bfloat16.cuh
46	11	setup.py

[4a151dd45] Woosuk Kwon 2023-05-25 Add activation registry (#126)
1	1	cacheflow/entrypoints/llm.py
15	0	cacheflow/model_executor/layers/activation.py
2	6	cacheflow/model_executor/models/gpt2.py
2	4	cacheflow/model_executor/models/gpt_neox.py
2	2	cacheflow/model_executor/models/opt.py

[057daef77] Zhuohan Li 2023-05-23 OpenAI Compatible Frontend (#116)
3	3	cacheflow/core/block_manager.py
4	2	cacheflow/core/scheduler.py
300	0	cacheflow/entrypoints/openai/openai_frontend.py
126	0	cacheflow/entrypoints/openai/protocol.py
51	0	cacheflow/entrypoints/simple_fastapi_frontend.py
3	3	cacheflow/model_executor/layers/sampler.py
18	11	cacheflow/outputs.py
2	2	cacheflow/sampling_params.py
21	5	cacheflow/sequence.py
21	46	cacheflow/{entrypoints/fastapi_server.py => server/async_llm_server.py}
8	5	cacheflow/server/llm_server.py
5	0	cacheflow/utils.py
7	6	gradio_webserver.py => examples/gradio_webserver.py
22	0	examples/openai_client.py
48	0	examples/simple_fastapi_client.py
5	4	examples/simple_server.py
0	20	playground/http_client.py
0	40	playground/streaming_fastapi_worker.py
1	0	requirements.txt
0	23	test_cli_client.py

[e86717833] Woosuk Kwon 2023-05-23 Incrementally decode output tokens (#121)
1	1	cacheflow/core/scheduler.py
8	3	cacheflow/sequence.py
12	12	cacheflow/server/llm_server.py
62	1	cacheflow/server/tokenizer_utils.py

[aedba6d5e] Woosuk Kwon 2023-05-23 Print warnings/errors for large swap space (#123)
31	0	cacheflow/config.py
1	0	cacheflow/server/llm_server.py
2	0	cacheflow/utils.py

[a283ec2ee] Woosuk Kwon 2023-05-23 Add contributing guideline and mypy config (#122)
74	0	CONTRIBUTING.md
1	1	cacheflow/core/scheduler.py
2	2	cacheflow/model_executor/layers/attention.py
1	1	cacheflow/model_executor/layers/sampler.py
3	1	cacheflow/model_executor/model_loader.py
4	4	cacheflow/model_executor/models/gpt2.py
6	6	cacheflow/model_executor/models/gpt_neox.py
6	6	cacheflow/model_executor/models/llama.py
7	7	cacheflow/model_executor/models/opt.py
1	1	cacheflow/outputs.py
5	2	cacheflow/sequence.py
1	1	cacheflow/server/ray_utils.py
6	11	cacheflow/worker/worker.py
8	0	mypy.ini
2	0	requirements-dev.txt
1	1	tests/kernels/test_pos_encoding.py

[3f942acfe] Woosuk Kwon 2023-05-22 Fix latency benchmark script (#118)
33	29	benchmark/benchmark_latency.py
10	2	cacheflow/entrypoints/llm.py

[19d289943] Woosuk Kwon 2023-05-22 Add initial sphinx docs (#120)
1	0	.gitignore
20	0	docs/Makefile
20	0	docs/README.md
35	0	docs/make.bat
3	0	docs/requirements-docs.txt
65	0	docs/source/conf.py
10	0	docs/source/getting_started/installation.rst
34	0	docs/source/getting_started/quickstart.rst
12	0	docs/source/index.rst

[655a5e48d] Woosuk Kwon 2023-05-21 Introduce LLM class for offline inference (#115)
5	9	cacheflow/__init__.py
5	1	cacheflow/config.py
3	4	cacheflow/entrypoints/fastapi_server.py
62	0	cacheflow/entrypoints/llm.py
5	5	cacheflow/outputs.py
97	54	cacheflow/server/arg_utils.py
16	2	cacheflow/server/llm_server.py
23	0	examples/offline_inference.py
5	5	examples/simple_server.py

[f746ced08] Woosuk Kwon 2023-05-21 Implement stop strings and best_of (#114)
6	6	cacheflow/core/block_manager.py
15	48	cacheflow/core/scheduler.py
3	2	cacheflow/entrypoints/fastapi_server.py
8	8	cacheflow/model_executor/layers/sampler.py
23	21	cacheflow/outputs.py
29	11	cacheflow/sampling_params.py
21	5	cacheflow/sequence.py
55	13	cacheflow/server/llm_server.py
2	2	examples/simple_server.py

[c3442c1f6] Woosuk Kwon 2023-05-20 Refactor system architecture (#109)
8	5	README.md
19	0	cacheflow/__init__.py
165	0	cacheflow/config.py
80	84	cacheflow/core/scheduler.py
0	302	cacheflow/core/server.py
128	0	cacheflow/entrypoints/fastapi_server.py
0	201	cacheflow/frontend/fastapi_frontend.py
0	72	cacheflow/frontend/simple_frontend.py
1	3	cacheflow/model_executor/__init__.py
1	1	cacheflow/model_executor/layers/attention.py
12	43	cacheflow/model_executor/model_loader.py
0	35	cacheflow/model_executor/utils.py
79	0	cacheflow/outputs.py
1	1	cacheflow/sampling_params.py
9	6	cacheflow/sequence.py
74	0	cacheflow/server/arg_utils.py
198	0	cacheflow/server/llm_server.py
90	0	cacheflow/server/ray_utils.py
0	1	cacheflow/{frontend/utils.py => server/tokenizer_utils.py}
37	21	cacheflow/worker/cache_engine.py
0	130	cacheflow/worker/controller.py
73	93	cacheflow/worker/worker.py
44	0	examples/simple_server.py
0	38	simple_server.py

[7297fa6f7] Zhuohan Li 2023-05-20 Remove unused parts in Megatron-LM code and add copyright notice (#110)
0	2	cacheflow/model_executor/parallel_utils/__init__.py
2	25	cacheflow/model_executor/parallel_utils/parallel_state.py
0	6	cacheflow/model_executor/parallel_utils/tensor_parallel/__init__.py
2	0	cacheflow/model_executor/parallel_utils/tensor_parallel/layers.py
2	0	cacheflow/model_executor/parallel_utils/tensor_parallel/mappings.py
2	91	cacheflow/model_executor/parallel_utils/tensor_parallel/random.py
15	53	cacheflow/model_executor/parallel_utils/tensor_parallel/utils.py
0	120	cacheflow/model_executor/parallel_utils/utils.py

[b7955ef17] Zhuohan Li 2023-05-19 Fix timeout error in the FastAPI frontend (#34)
5	2	cacheflow/frontend/fastapi_frontend.py

[f756799b8] Zhuohan Li 2023-05-19 Use runtime profiling to replace manual memory analyzers (#81)
30	22	cacheflow/core/server.py
7	3	cacheflow/frontend/fastapi_frontend.py
4	3	cacheflow/model_executor/__init__.py
29	21	cacheflow/model_executor/layers/attention.py
1	1	cacheflow/model_executor/layers/sampler.py
0	370	cacheflow/model_executor/memory_analyzer.py
0	37	cacheflow/model_executor/model_loader.py
2	1	cacheflow/model_executor/models/gpt2.py
2	1	cacheflow/model_executor/models/gpt_neox.py
2	1	cacheflow/model_executor/models/llama.py
2	1	cacheflow/model_executor/models/opt.py
12	0	cacheflow/model_executor/utils.py
36	9	cacheflow/worker/controller.py
84	8	cacheflow/worker/worker.py

[825d8892b] Woosuk Kwon 2023-05-17 Use pytest format for unit tests (#107)
4	4	tests/kernels/{activation.py => test_activation.py}
14	14	tests/kernels/{attention.py => test_attention.py}
18	12	tests/kernels/{cache.py => test_cache.py}
3	3	tests/kernels/{layernorm.py => test_layernorm.py}
4	4	tests/kernels/{pos_encoding.py => test_pos_encoding.py}

[b322fd160] Woosuk Kwon 2023-05-14 Add docstrings to some modules and classes (#100)
9	2	cacheflow/block.py
20	15	cacheflow/core/block_manager.py
2	2	cacheflow/core/server.py
5	0	cacheflow/model_executor/layers/activation.py
28	1	cacheflow/model_executor/layers/attention.py
6	0	cacheflow/model_executor/layers/layernorm.py
14	0	cacheflow/model_executor/layers/sampler.py
2	2	cacheflow/model_executor/model_loader.py
5	1	cacheflow/model_executor/models/gpt2.py
6	1	cacheflow/model_executor/models/gpt_neox.py
5	1	cacheflow/model_executor/models/llama.py
5	1	cacheflow/model_executor/models/opt.py
6	5	cacheflow/model_executor/utils.py
8	0	cacheflow/model_executor/weight_utils.py
30	0	cacheflow/sampling_params.py
8	0	cacheflow/worker/cache_engine.py
7	0	cacheflow/worker/worker.py

[667ba3995] Woosuk Kwon 2023-05-14 Add copyright headers to source files adapted from FT (#104)
1	2	cacheflow/model_executor/models/gpt2.py
1	2	cacheflow/model_executor/models/gpt_neox.py
1	2	cacheflow/model_executor/models/llama.py
1	2	cacheflow/model_executor/models/opt.py
17	0	csrc/attention/attention_generic.cuh
17	0	csrc/attention/attention_kernels.cu
17	0	csrc/attention/attention_utils.cuh
18	0	csrc/attention/dtype_bfloat16.cuh
18	0	csrc/attention/dtype_float16.cuh
18	0	csrc/attention/dtype_float32.cuh
17	0	csrc/reduction_utils.cuh

[707ec647b] Woosuk Kwon 2023-05-14 Add copyright headers for HF models (#103)
18	0	cacheflow/model_executor/models/gpt2.py
17	0	cacheflow/model_executor/models/gpt_neox.py
22	0	cacheflow/model_executor/models/llama.py
17	0	cacheflow/model_executor/models/opt.py

[89988ec8c] Woosuk Kwon 2023-05-14 Add Apache-2.0 license (#102)
201	0	LICENSE

[6208d622c] Woosuk Kwon 2023-05-12 Minor code cleaning for SamplingParams (#99)
50	49	cacheflow/sampling_params.py

[42f1042e1] Woosuk Kwon 2023-05-11 Enhance SamplingParams (#96)
11	11	benchmark/benchmark_latency.py
1	1	cacheflow/core/scheduler.py
2	2	cacheflow/frontend/fastapi_frontend.py
2	2	cacheflow/model_executor/layers/sampler.py
18	36	cacheflow/sampling_params.py
1	1	gradio_webserver.py
1	1	simple_server.py

[55f8b0a5d] Woosuk Kwon 2023-05-10 Implement presence and frequency penalties (#95)
7	16	cacheflow/core/scheduler.py
1	1	cacheflow/frontend/fastapi_frontend.py
3	2	cacheflow/frontend/simple_frontend.py
6	4	cacheflow/model_executor/input_metadata.py
106	8	cacheflow/model_executor/layers/sampler.py
26	9	cacheflow/sampling_params.py
46	23	cacheflow/sequence.py
18	17	cacheflow/worker/worker.py
2	2	simple_server.py

[9f88db35d] Woosuk Kwon 2023-05-10 Support top-k sampling (#94)
37	16	cacheflow/model_executor/layers/sampler.py
38	25	cacheflow/sampling_params.py
3	2	simple_server.py

[ae356774a] Woosuk Kwon 2023-05-10 Avoid sorting waiting queue & Minor code cleaning (#93)
10	13	cacheflow/core/scheduler.py
0	1	cacheflow/core/server.py
0	1	cacheflow/frontend/simple_frontend.py
0	1	cacheflow/model_executor/memory_analyzer.py

[e33195778] Woosuk Kwon 2023-05-10 Log system stats (#90)
48	129	cacheflow/core/scheduler.py
5	6	cacheflow/core/server.py
0	1	cacheflow/worker/worker.py

[8d66a7b6d] Woosuk Kwon 2023-05-10 Rename variables and methods (#91)
1	1	cacheflow/block.py
4	4	cacheflow/core/block_manager.py
12	12	cacheflow/core/scheduler.py
2	10	cacheflow/sampling_params.py
13	12	cacheflow/sequence.py
3	15	cacheflow/worker/controller.py
29	29	cacheflow/worker/worker.py

[ce26e57fd] Woosuk Kwon 2023-05-09 Update sample prompts in `simple_server.py` (#89)
3	3	simple_server.py

[85eb63183] Woosuk Kwon 2023-05-09 Use slow tokenizer for LLaMA (#84)
2	2	cacheflow/frontend/fastapi_frontend.py
2	3	cacheflow/frontend/simple_frontend.py
22	0	cacheflow/frontend/utils.py

[add055e15] Woosuk Kwon 2023-05-09 Enhance model loader (#83)
1	1	cacheflow/core/server.py
55	41	cacheflow/model_executor/model_loader.py

[7c041ab57] Woosuk Kwon 2023-05-09 Refactor system architecture (#82)
0	0	cacheflow/{master => core}/block_manager.py
0	0	cacheflow/{master => core}/policy.py
2	2	cacheflow/{master => core}/scheduler.py
5	4	cacheflow/{master => core}/server.py
7	7	cacheflow/{http_frontend => frontend}/fastapi_frontend.py
0	0	cacheflow/{master => frontend}/simple_frontend.py
11	0	cacheflow/model_executor/__init__.py
0	0	cacheflow/{models => model_executor}/input_metadata.py
0	0	cacheflow/{models => model_executor/layers}/activation.py
1	1	cacheflow/{models => model_executor/layers}/attention.py
0	0	cacheflow/{models => model_executor/layers}/layernorm.py
4	3	cacheflow/{models/sample.py => model_executor/layers/sampler.py}
1	1	cacheflow/{models => model_executor}/memory_analyzer.py
8	11	cacheflow/{models/model_utils.py => model_executor/model_loader.py}
12	0	cacheflow/model_executor/models/__init__.py
10	14	cacheflow/{ => model_executor}/models/gpt2.py
16	19	cacheflow/{ => model_executor}/models/gpt_neox.py
13	16	cacheflow/{ => model_executor}/models/llama.py
10	14	cacheflow/{ => model_executor}/models/opt.py
0	0	cacheflow/{ => model_executor}/parallel_utils/README.md
12	0	cacheflow/model_executor/parallel_utils/__init__.py
0	0	cacheflow/{ => model_executor}/parallel_utils/parallel_state.py
0	0	cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/__init__.py
1	1	cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/layers.py
1	1	cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/mappings.py
2	2	cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/random.py
2	2	cacheflow/{ => model_executor}/parallel_utils/tensor_parallel/utils.py
1	1	cacheflow/{ => model_executor}/parallel_utils/utils.py
41	0	cacheflow/model_executor/utils.py
27	35	cacheflow/{models/utils.py => model_executor/weight_utils.py}
0	10	cacheflow/models/__init__.py
0	12	cacheflow/parallel_utils/__init__.py
1	17	cacheflow/utils.py
1	1	cacheflow/worker/controller.py
5	8	cacheflow/worker/worker.py
0	0	cacheflow/http_frontend/gradio_webserver.py => gradio_webserver.py
0	212	plot/plot_normalized_latency.py
0	52	plot/plot_stats.py
1	1	simple_server.py
0	0	cacheflow/http_frontend/test_cli_client.py => test_cli_client.py

[8917782af] Woosuk Kwon 2023-05-08 Add a system logger (#85)
51	0	cacheflow/logger.py
16	3	cacheflow/master/server.py
6	2	cacheflow/master/simple_frontend.py
11	7	cacheflow/models/memory_analyzer.py
1	1	simple_server.py

[7addca593] Woosuk Kwon 2023-05-07 Specify python package dependencies in requirements.txt (#78)
3	8	README.md
10	0	requirements.txt
44	28	setup.py

[c84e92428] Woosuk Kwon 2023-05-06 [Minor] Fix a dtype bug (#79)
5	1	cacheflow/models/model_utils.py

[c9d5b6d4a] Woosuk Kwon 2023-05-05 Replace FlashAttention with xformers (#70)
1	5	README.md
1	1	cacheflow/master/server.py
16	38	cacheflow/models/attention.py
9	12	cacheflow/models/input_metadata.py
2	2	cacheflow/models/llama.py
6	6	cacheflow/models/memory_analyzer.py
2	2	cacheflow/models/opt.py
0	8	cacheflow/worker/worker.py
1	1	tests/kernels/activation.py
35	44	tests/kernels/attention.py
10	9	tests/kernels/cache.py
3	2	tests/kernels/layernorm.py
1	1	tests/kernels/pos_encoding.py

[189ae2313] Woosuk Kwon 2023-05-04 Use dtype from model config & Add Dolly V2 (#63)
5	1	cacheflow/master/server.py
28	6	cacheflow/models/model_utils.py

[e548c1488] Woosuk Kwon 2023-05-04 Add support for GPT-2 (#60)
265	0	cacheflow/models/gpt2.py
5	5	cacheflow/models/gpt_neox.py
1	1	cacheflow/models/llama.py
70	0	cacheflow/models/memory_analyzer.py
4	0	cacheflow/models/model_utils.py
1	1	cacheflow/models/opt.py
4	1	cacheflow/models/sample.py

[130d5fd8c] Woosuk Kwon 2023-05-04 Fix a bug in attention kernel (#68)
1	1	csrc/attention/attention_kernels.cu

[e070829ae] Woosuk Kwon 2023-05-03 Support bfloat16 data type (#54)
2	2	cacheflow/master/server.py
1	0	cacheflow/models/utils.py
3	1	csrc/activation_kernels.cu
4	0	csrc/attention/{attention_dtypes.cuh => attention_dtypes.h}
6	2	csrc/attention/attention_kernels.cu
1	1	csrc/attention/attention_utils.cuh
361	0	csrc/attention/dtype_bfloat16.cuh
1	1	csrc/attention/dtype_float32.cuh
55	43	csrc/cache_kernels.cu
3	1	csrc/layernorm_kernels.cu
3	1	csrc/pos_encoding_kernels.cu
15	1	setup.py

[436e523bf] Woosuk Kwon 2023-05-03 Refactor attention kernels (#53)
5	0	csrc/attention/attention_dtypes.cuh
47	0	csrc/attention/attention_generic.cuh
451	0	csrc/attention/attention_kernels.cu
38	0	csrc/attention/attention_utils.cuh
426	0	csrc/attention/dtype_float16.cuh
250	0	csrc/attention/dtype_float32.cuh
0	896	csrc/attention_kernels.cu
0	165	csrc/attention_utils.h
0	1340	csrc/cuda_primitives.h
1	1	csrc/layernorm_kernels.cu
34	0	csrc/reduction_utils.cuh
0	76	csrc/reduction_utils.h
1	1	setup.py
0	90	tests/kernels/attention.py

[27f1410d0] Zhuohan Li 2023-05-03 New weight loader without np copy (#52)
4	42	benchmark/benchmark_latency.py
4	43	benchmark/benchmark_text_completion.py
10	7	cacheflow/http_frontend/fastapi_frontend.py
58	7	cacheflow/master/server.py
15	59	cacheflow/models/gpt_neox.py
51	73	cacheflow/models/llama.py
5	6	cacheflow/models/model_utils.py
34	69	cacheflow/models/opt.py
94	1	cacheflow/models/utils.py
5	3	cacheflow/worker/controller.py
5	3	cacheflow/worker/worker.py
4	44	simple_server.py

[4858f3bb4] Zhuohan Li 2023-04-30 Add an option to launch cacheflow without ray (#51)
3	0	.gitignore
8	4	benchmark/benchmark_latency.py
10	6	benchmark/benchmark_text_completion.py
15	3	cacheflow/http_frontend/fastapi_frontend.py
39	4	cacheflow/master/server.py
21	9	cacheflow/worker/controller.py
6	2	simple_server.py

[a96d63c21] Woosuk Kwon 2023-04-28 Add support for GPT-NeoX (Pythia) (#50)
11	5	cacheflow/models/attention.py
278	0	cacheflow/models/gpt_neox.py
1	1	cacheflow/models/llama.py
107	50	cacheflow/models/memory_analyzer.py
6	1	cacheflow/models/model_utils.py
1	1	cacheflow/models/opt.py
1	0	csrc/pos_encoding.cpp
9	6	csrc/pos_encoding_kernels.cu
22	7	tests/kernels/pos_encoding.py

[aa50b17ca] Woosuk Kwon 2023-04-17 Change plotting script
11	2	plot/plot_normalized_latency.py

[0f4b32199] Woosuk Kwon 2023-04-15 Support various block sizes & Change default block size to 16 (#38)
1	0	benchmark/benchmark_text_completion.py
0	3	cacheflow/master/block_manager.py
2	1	cacheflow/master/scheduler.py
2	2	cacheflow/master/server.py
0	16	csrc/attention.cpp
557	579	csrc/attention_kernels.cu
40	18	csrc/cuda_primitives.h

[84eee24e2] Woosuk Kwon 2023-04-12 Collect system stats in scheduler & Add scripts for experiments (#30)
4	0	.gitignore
1	0	benchmark/benchmark_latency.py
289	0	benchmark/benchmark_text_completion.py
116	0	benchmark/trace.py
6	0	cacheflow/master/block_manager.py
146	1	cacheflow/master/scheduler.py
8	1	cacheflow/master/server.py
3	1	cacheflow/master/simple_frontend.py
1	0	cacheflow/sequence.py
203	0	plot/plot_normalized_latency.py
52	0	plot/plot_stats.py
1	0	simple_server.py

[e3cec88aa] Siyuan (Ryans) Zhuang 2023-04-10 Memcpy kernel for flash attention (#29)
81	0	benchmark/benchmark_cache.py
11	0	csrc/cache.cpp
157	0	csrc/cache_kernels.cu
44	0	tests/kernels/cache.py

[b9926f7f6] Woosuk Kwon 2023-04-09 Support block size 32 (#35)
2	2	cacheflow/master/block_manager.py
1	1	cacheflow/master/server.py
44	0	csrc/attention_kernels.cu
2	2	tests/kernels/attention.py

[ee88a7e5f] Woosuk Kwon 2023-04-08 Add an option to use dummy model weights (#33)
1	0	benchmark/benchmark_latency.py
1	0	cacheflow/http_frontend/fastapi_frontend.py
3	0	cacheflow/master/server.py
4	0	cacheflow/models/llama.py
17	6	cacheflow/models/model_utils.py
4	0	cacheflow/models/opt.py
2	0	cacheflow/worker/controller.py
3	2	cacheflow/worker/worker.py
1	0	simple_server.py

[c267b1a02] Woosuk Kwon 2023-04-08 Add query stride to multi_query_cached_kv_attention & Add kernel benchmark script (#27)
165	0	benchmark/benchmark_attention.py
11	5	csrc/attention_kernels.cu
5	3	tests/kernels/attention.py

[0f40557af] Woosuk Kwon 2023-04-07 Implement block copy kernel to optimize beam search (#32)
6	3	benchmark/benchmark_latency.py
3	2	cacheflow/models/sample.py
4	20	cacheflow/worker/cache_engine.py
2	2	csrc/cache.cpp
82	22	csrc/cache_kernels.cu
58	0	tests/kernels/cache.py

[a490aafa3] Zhuohan Li 2023-04-06 Fix potential bugs in FastAPI frontend and add comments (#28)
30	5	cacheflow/http_frontend/fastapi_frontend.py

[12659a0bd] Woosuk Kwon 2023-04-05 Add CUDA graph-based all reduce launcher (#26)
3	2	benchmark/benchmark_latency.py
7	6	cacheflow/master/server.py
71	0	cacheflow/parallel_utils/parallel_state.py
13	6	cacheflow/parallel_utils/tensor_parallel/layers.py
2	0	cacheflow/worker/controller.py
6	1	cacheflow/worker/worker.py
1	1	simple_server.py

[21b3671bb] Siyuan (Ryans) Zhuang 2023-04-04 Basic attention kernel that supports cached KV + (multi-)prompts (#24)
16	0	csrc/attention.cpp
463	0	csrc/attention_kernels.cu
143	0	tests/kernels/attention.py

[897cb2ae2] Woosuk Kwon 2023-04-02 Optimize data movement (#20)
20	0	cacheflow/models/activation.py
46	46	cacheflow/models/attention.py
5	0	cacheflow/models/input_metadata.py
7	12	cacheflow/models/llama.py
2	5	cacheflow/models/opt.py
8	0	cacheflow/worker/worker.py
12	0	csrc/activation.cpp
46	0	csrc/activation_kernels.cu
13	9	csrc/attention_kernels.cu
17	8	csrc/cache_kernels.cu
0	2	csrc/pos_encoding.cpp
22	27	csrc/pos_encoding_kernels.cu
7	0	setup.py
30	0	tests/kernels/activation.py
31	15	tests/kernels/attention.py
5	5	tests/kernels/cache.py
4	6	tests/kernels/pos_encoding.py

[1f01a18d3] Zhuohan Li 2023-04-02 Merge QKV into one linear layer (#15)
52	47	cacheflow/models/llama.py
38	36	cacheflow/models/opt.py

[2c5cd0def] Woosuk Kwon 2023-04-01 Add ninja to dependency (#21)
2	1	README.md

[a90c97d72] Woosuk Kwon 2023-03-31 Use FP32 for log probabilities (#19)
2	1	cacheflow/models/sample.py

[e3f00d191] Zhuohan Li 2023-04-01 Modify README to include info on loading LLaMA (#18)
16	0	README.md

[09e924547] Woosuk Kwon 2023-03-31 Add custom kernel for RMS normalization (#16)
26	0	cacheflow/models/layernorm.py
4	19	cacheflow/models/llama.py
1	0	csrc/attention_kernels.cu
0	39	csrc/attention_utils.h
14	0	csrc/layernorm.cpp
61	0	csrc/layernorm_kernels.cu
76	0	csrc/reduction_utils.h
8	0	setup.py
53	0	tests/kernels/layernorm.py

[c45f3c3ab] Zhuohan Li 2023-04-01 Optimize tensor parallel execution speed (#17)
99	0	benchmark/benchmark_latency.py
0	3	cacheflow/parallel_utils/tensor_parallel/__init__.py
4	284	cacheflow/parallel_utils/tensor_parallel/layers.py

[7a7929abe] Woosuk Kwon 2023-03-30 Implement preemption via recomputation & Refactor scheduling logic (#12)
2	1	cacheflow/http_frontend/fastapi_frontend.py
2	1	cacheflow/master/block_manager.py
45	0	cacheflow/master/policy.py
221	120	cacheflow/master/scheduler.py
2	1	cacheflow/master/server.py
3	1	cacheflow/master/simple_frontend.py
4	2	cacheflow/sequence.py

[88c0268a1] Woosuk Kwon 2023-03-30 Implement custom kernel for LLaMA rotary embedding (#14)
71	2	cacheflow/models/attention.py
5	59	cacheflow/models/llama.py
1	2	cacheflow/models/memory_analyzer.py
1	2	cacheflow/models/opt.py
1	1	cacheflow/models/sample.py
3	3	csrc/cache_kernels.cu
16	0	csrc/pos_encoding.cpp
83	0	csrc/pos_encoding_kernels.cu
8	0	setup.py
129	0	tests/kernels/pos_encoding.py

[80a2f812f] Woosuk Kwon 2023-03-29 Implement LLaMA (#9)
4	2	README.md
1	0	cacheflow/master/simple_frontend.py
357	0	cacheflow/models/llama.py
130	26	cacheflow/models/memory_analyzer.py
5	1	cacheflow/models/model_utils.py
2	5	cacheflow/models/opt.py
1	1	cacheflow/models/sample.py

[a1b3de86c] Woosuk Kwon 2023-03-29 Refactor the test code for attention kernels (#13)
53	19	tests/kernels/attention.py

[64e0e3831] Woosuk Kwon 2023-03-29 Add cache watermark to avoid frequent cache eviction (#11)
8	2	cacheflow/master/block_manager.py

[721fa3df1] Zhuohan Li 2023-03-29 FastAPI-based working frontend (#10)
39	2	README.md
152	0	cacheflow/http_frontend/fastapi_frontend.py
43	0	cacheflow/http_frontend/gradio_webserver.py
23	0	cacheflow/http_frontend/test_cli_client.py
14	12	cacheflow/master/scheduler.py
89	87	server.py => cacheflow/master/server.py
8	23	cacheflow/master/{frontend.py => simple_frontend.py}
9	9	cacheflow/models/memory_analyzer.py
4	1	cacheflow/models/model_utils.py
0	11	cacheflow/models/utils.py
14	1	cacheflow/sampling_params.py
10	0	cacheflow/utils.py
20	0	playground/http_client.py
40	0	playground/streaming_fastapi_worker.py
71	0	simple_server.py

[d359cda5f] Woosuk Kwon 2023-03-26 Minor
3	3	cacheflow/master/block_manager.py
1	0	cacheflow/models/sample.py
1	1	cacheflow/sequence.py

[2f49f1558] Zhuohan Li 2023-03-22 Support tensor parallel (#2)
2	1	README.md
0	2	cacheflow/models/__init__.py
1	1	cacheflow/models/attention.py
5	1	cacheflow/models/input_metadata.py
18	16	cacheflow/models/memory_analyzer.py
16	6	cacheflow/models/model_utils.py
131	70	cacheflow/models/opt.py
2	1	cacheflow/models/sample.py
0	8	cacheflow/models/utils.py
1	0	cacheflow/parallel_utils/README.md
12	0	cacheflow/parallel_utils/__init__.py
522	0	cacheflow/parallel_utils/parallel_state.py
58	0	cacheflow/parallel_utils/tensor_parallel/__init__.py
719	0	cacheflow/parallel_utils/tensor_parallel/layers.py
279	0	cacheflow/parallel_utils/tensor_parallel/mappings.py
253	0	cacheflow/parallel_utils/tensor_parallel/random.py
108	0	cacheflow/parallel_utils/tensor_parallel/utils.py
120	0	cacheflow/parallel_utils/utils.py
6	0	cacheflow/sequence.py
17	0	cacheflow/utils.py
4	6	cacheflow/worker/cache_engine.py
42	19	cacheflow/worker/controller.py
51	22	cacheflow/worker/worker.py
115	23	server.py

[cfae35b86] Woosuk Kwon 2023-03-13 Add miscellaneous updates (#8)
8	7	cacheflow/master/scheduler.py
5	6	cacheflow/models/attention.py
14	4	cacheflow/models/memory_analyzer.py
1	1	cacheflow/models/sample.py
7	0	cacheflow/worker/worker.py
5	2	csrc/cache_kernels.cu
4	2	server.py

[e9d3f2ff7] Woosuk Kwon 2023-03-11 Add memory analyzer & utomatically configure KV cache size (#6)
1	1	README.md
3	3	cacheflow/master/scheduler.py
4	2	cacheflow/models/__init__.py
125	0	cacheflow/models/memory_analyzer.py
23	21	cacheflow/models/model_utils.py
43	0	cacheflow/models/utils.py
17	7	server.py

[1a7eb7da6] Woosuk Kwon 2023-03-10 Support beam search & parallel generation (#7)
4	0	cacheflow/block.py
28	5	cacheflow/master/frontend.py
55	36	cacheflow/master/scheduler.py
3	1	cacheflow/models/__init__.py
11	7	cacheflow/models/input_metadata.py
10	0	cacheflow/models/model_utils.py
2	1	cacheflow/models/opt.py
258	17	cacheflow/models/sample.py
54	19	cacheflow/sampling_params.py
72	6	cacheflow/sequence.py
27	8	cacheflow/worker/cache_engine.py
7	10	cacheflow/worker/controller.py
76	43	cacheflow/worker/worker.py
14	2	csrc/cache.cpp
31	1	csrc/cache_kernels.cu
10	7	server.py

[04e5acc08] Woosuk Kwon 2023-03-06 Fix a bug in 1D input shape (#5)
8	3	cacheflow/models/attention.py
1	1	cacheflow/models/input_metadata.py
2	2	server.py

[3e9f991d6] Woosuk Kwon 2023-03-01 Use FlashAttention for `multi_query_kv_attention` (#4)
1	0	README.md
37	30	cacheflow/models/attention.py
5	2	server.py
65	2	tests/kernels/attention.py

[0deacbce6] Woosuk Kwon 2023-03-01 Implement `single_query_cached_kv_attention` kernel (#3)
3	1	cacheflow/master/block_manager.py
19	35	cacheflow/models/attention.py
12	8	cacheflow/worker/cache_engine.py
1	1	cacheflow/worker/worker.py
19	0	csrc/attention.cpp
400	0	csrc/attention_kernels.cu
204	0	csrc/attention_utils.h
5	5	csrc/cache_kernels.cu
1318	0	csrc/cuda_primitives.h
9	2	setup.py
142	0	tests/kernels/attention.py
8	8	tests/{kernels.py => kernels/cache.py}

[cbf8779af] Woosuk Kwon 2023-02-24 Fix a bug in tying OPT embeddings (#1)
3	2	cacheflow/models/model_utils.py
22	0	cacheflow/models/opt.py

[c84c708a1] Woosuk Kwon 2023-02-24 Add README
13	0	README.md

[fa16389a2] Woosuk Kwon 2023-02-24 Clean up the server script
25	64	server.py

[6aef2278f] Woosuk Kwon 2023-02-24 [Minor] Fix printing format
1	1	cacheflow/master/frontend.py

[1132fae0c] Woosuk Kwon 2023-02-24 Add Frontend
56	0	cacheflow/master/frontend.py
24	17	cacheflow/master/scheduler.py

[46ce1356f] Woosuk Kwon 2023-02-24 Add max_num_steps to SamplingParams
3	0	cacheflow/sampling_params.py

[b39f149a0] Woosuk Kwon 2023-02-24 Add is_finished
3	0	cacheflow/sequence.py

[ef6098ec5] Woosuk Kwon 2023-02-24 Merge pre_step and step
4	13	cacheflow/master/scheduler.py

[53f70e733] Woosuk Kwon 2023-02-24 Reduce the number of states in scheduler
44	29	cacheflow/master/scheduler.py

[762fd1c3f] Woosuk Kwon 2023-02-24 Refactor and annotate types for attention
40	32	cacheflow/models/attention.py

[7f22f90e8] Woosuk Kwon 2023-02-24 Remove xformers
20	16	cacheflow/models/attention.py

[afdbe5d37] Woosuk Kwon 2023-02-24 [WIP] Add server script
6	0	cacheflow/master/scheduler.py
107	0	server.py

[932844f1c] Woosuk Kwon 2023-02-23 Fix attention
10	6	cacheflow/models/attention.py
11	0	cacheflow/models/input_metadata.py

[ba84b8728] Woosuk Kwon 2023-02-23 Fix attention
5	1	cacheflow/models/attention.py

[87e0bcd42] Woosuk Kwon 2023-02-23 Fix attention
12	7	cacheflow/models/attention.py

[1ce133357] Woosuk Kwon 2023-02-23 Set default dtype to half
19	2	cacheflow/models/model_utils.py
2	0	cacheflow/worker/controller.py
2	1	cacheflow/worker/worker.py

[de0fabbc5] Woosuk Kwon 2023-02-23 Fix sampler
3	2	cacheflow/models/opt.py
3	6	cacheflow/models/sample.py

[fdd0f2f47] Woosuk Kwon 2023-02-23 Minor
1	1	cacheflow/master/scheduler.py
2	2	cacheflow/models/sample.py
1	0	cacheflow/worker/worker.py

[7f985166f] Woosuk Kwon 2023-02-23 Consider pempty tensor
4	1	cacheflow/models/input_metadata.py

[86f9eb6d3] Woosuk Kwon 2023-02-23 Fix typo
0	1	cacheflow/models/opt.py

[1f6c7ef43] Woosuk Kwon 2023-02-23 Add controller
75	0	cacheflow/worker/controller.py

[d4bc1a4d2] Woosuk Kwon 2023-02-23 Add unoptimized OPT Attention
118	0	cacheflow/models/attention.py
59	14	cacheflow/models/opt.py

[b56b6ca0d] Woosuk Kwon 2023-02-23 Add greedy sampler
45	0	cacheflow/models/sample.py

[343cea3db] Woosuk Kwon 2023-02-23 Add seq_ids to input metadata
3	0	cacheflow/models/input_metadata.py
3	3	cacheflow/worker/worker.py

[4f6f4967f] Woosuk Kwon 2023-02-23 Add get_block_table
4	0	cacheflow/master/block_manager.py

[331fa0b04] Woosuk Kwon 2023-02-23 Implement scheduler.step & Add a threshold for batch size
59	21	cacheflow/master/scheduler.py

[501c4bd0c] Woosuk Kwon 2023-02-23 decoding.py -> sampling_params.py
0	0	cacheflow/{decoding.py => sampling_params.py}

[86c682cd3] Woosuk Kwon 2023-02-23 DecodingParams -> SamplingParams
1	1	cacheflow/decoding.py

[af16c0507] Woosuk Kwon 2023-02-23 Add get_len
3	0	cacheflow/sequence.py

[d09451229] Woosuk Kwon 2023-02-23 Move max_context_len
5	2	cacheflow/decoding.py
0	2	cacheflow/sequence.py

[4b1ac23f5] Woosuk Kwon 2023-02-23 Fix slot mapping
1	1	cacheflow/models/input_metadata.py
15	5	cacheflow/worker/worker.py

[8290fce47] Woosuk Kwon 2023-02-22 Add Worker class
169	0	cacheflow/worker/worker.py

[7b6844e59] Woosuk Kwon 2023-02-22 Add input metadata
3	2	cacheflow/models/__init__.py
25	0	cacheflow/models/input_metadata.py

[608f74ffe] Woosuk Kwon 2023-02-22 Minor
1	1	cacheflow/models/model_utils.py

[709a69176] Woosuk Kwon 2023-02-22 Move worker/models -> models
0	0	cacheflow/{worker => }/models/__init__.py
0	0	cacheflow/{worker => }/models/model_utils.py
0	0	cacheflow/{worker => }/models/opt.py

[af68ec1c5] Woosuk Kwon 2023-02-18 Add tests for kernels
55	0	tests/kernels.py

[c413c41cd] Woosuk Kwon 2023-02-18 Add reshape_and_cache op
11	0	csrc/cache.cpp
72	0	csrc/cache_kernels.cu

[ffad4e1e0] Woosuk Kwon 2023-02-16 cache_kernel -> cache_kernels
0	0	csrc/{cache_kernel.cu => cache_kernels.cu}
1	1	setup.py

[6d2f74efb] Woosuk Kwon 2023-02-16 Remove redundant fn
1	8	csrc/cache.cpp

[3b41f1659] Woosuk Kwon 2023-02-16 Add gitignore
2	0	.gitignore

[6f058c7ba] Woosuk Kwon 2023-02-16 Implement cache ops
23	6	cacheflow/worker/cache_engine.py
20	0	csrc/cache.cpp
43	0	csrc/cache_kernel.cu
23	0	setup.py

[a1c67e6db] Woosuk Kwon 2023-02-16 Minor
3	2	cacheflow/worker/cache_engine.py

[9e68a6827] Woosuk Kwon 2023-02-16 Fix return type error
2	2	cacheflow/worker/cache_engine.py

[8edcabc73] Woosuk Kwon 2023-02-16 Add warning
3	0	cacheflow/worker/cache_engine.py

[2f4887de7] Woosuk Kwon 2023-02-16 Fix KVCache shape
43	32	cacheflow/worker/cache_engine.py

[3363c27d1] Woosuk Kwon 2023-02-14 Add __repr__
5	0	cacheflow/block.py
9	3	cacheflow/sequence.py

[2729087ef] Woosuk Kwon 2023-02-14 Fix a ref count bug in swapping
2	0	cacheflow/master/block_manager.py

[c128c2ed0] Woosuk Kwon 2023-02-14 Fix double free
2	2	cacheflow/master/block_manager.py

[c80ed212a] Woosuk Kwon 2023-02-14 Minor
1	1	cacheflow/master/block_manager.py

[263e91b38] Woosuk Kwon 2023-02-14 Minor
4	4	cacheflow/master/block_manager.py

[be1e2163c] Woosuk Kwon 2023-02-14 Fix memory leak in swapping
9	9	cacheflow/master/block_manager.py

[7e5f604e6] Woosuk Kwon 2023-02-14 Fix bugs in scheduler
12	6	cacheflow/master/scheduler.py

[1f739f9b0] Woosuk Kwon 2023-02-14 Fix a bug in swap_in and swap_out
27	9	cacheflow/master/block_manager.py

[b1644f764] Woosuk Kwon 2023-02-14 Fix can_swap_in
2	2	cacheflow/master/block_manager.py

[c574f1950] Woosuk Kwon 2023-02-14 Fix typo
1	1	cacheflow/utils.py

[e40fa136a] Woosuk Kwon 2023-02-14 Fix typo
1	1	cacheflow/block.py

[ee9442518] Woosuk Kwon 2023-02-13 Fix get_model
4	3	cacheflow/worker/models/model_utils.py

[531e1c74e] Woosuk Kwon 2023-02-13 Fix typo
1	1	cacheflow/master/scheduler.py

[d3d317665] Woosuk Kwon 2023-02-13 Fix scheduler
38	11	cacheflow/master/scheduler.py

[fffa2e1f4] Woosuk Kwon 2023-02-13 Add model_utils
4	2	cacheflow/worker/models/__init__.py
13	0	cacheflow/worker/models/model_utils.py

[bb59a3e73] Woosuk Kwon 2023-02-13 Fix cache engine
15	34	cacheflow/worker/cache_engine.py

[5a309bb58] Woosuk Kwon 2023-02-13 Add scheduler
170	0	cacheflow/master/scheduler.py

[0961f5a49] Woosuk Kwon 2023-02-13 Add find method to sequence group
6	0	cacheflow/sequence.py

[eb52db1be] Woosuk Kwon 2023-02-13 Fix can_swap_in
10	4	cacheflow/master/block_manager.py

[a2a9869cb] Woosuk Kwon 2023-02-12 SERVING -> RUNNING
1	1	cacheflow/master/block_manager.py
1	1	cacheflow/sequence.py

[46958cf94] Woosuk Kwon 2023-02-12 BlockAllocator -> BlockManager
3	3	cacheflow/master/block_manager.py

[3be29a110] Woosuk Kwon 2023-02-09 Add blank setup file
0	0	setup.py

[38ed06a84] Woosuk Kwon 2023-02-09 Minor
3	3	cacheflow/master/block_manager.py

[0a11a2e5c] Woosuk Kwon 2023-02-09 Add gitignore
4	0	.gitignore

[e7bee2aa8] Woosuk Kwon 2023-02-09 Add cache engine
109	0	cacheflow/worker/cache_engine.py

[aa78aeaa0] Woosuk Kwon 2023-02-09 Add block manager
204	0	cacheflow/master/block_manager.py

[3c2b47fca] Woosuk Kwon 2023-02-09 Add decoding params
30	0	cacheflow/decoding.py

[5e644b912] Woosuk Kwon 2023-02-09 Add utils
20	0	cacheflow/utils.py

[d904350a2] Woosuk Kwon 2023-02-09 Add sequence
78	0	cacheflow/sequence.py

[6680129ba] Woosuk Kwon 2023-02-09 Add blocks
51	0	cacheflow/block.py

[39161c98a] Woosuk Kwon 2023-02-09 Add OPT
5	0	cacheflow/worker/models/__init__.py
202	0	cacheflow/worker/models/opt.py

[e7d9d9c08] Woosuk Kwon 2023-02-09 Initial commit
1	0	README.md
